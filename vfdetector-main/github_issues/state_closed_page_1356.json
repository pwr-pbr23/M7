[{"number": 12398, "title": "Better documentation for tf.contrib.layers.optimize_loss", "body": "- `tf.contrib.layers.optimize_loss` references `OPTIMIZER_SUMMARIES` in\r\n   its docstring, so we should expose that in the API\r\n- Correct the documentation for what `summaries` `optimize_loss` uses by default.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed the CLA", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "Mind fixing the new test failures?", "@jhseu: I can't seem to reproduce the test failure locally. AFAICT, the only test that failed was `//bazel_pip/tensorflow/contrib/learn:run_config_test`, but `bazel test //tensorflow/contrib/learn:run_config_test` passes for me.", "Yeah, looks like a flake."]}, {"number": 12397, "title": "Fix bug on the gradients graph computation caused by an Assign node - C++ API", "body": "The gradient computation in the C++ API works as follow.\r\nLet's say that we have the following graph:\r\n```\r\n                Tanh\r\n                  |\r\n         Assign MatMul\r\n         /    \\ /    \\\r\n      Const   Var   Const\r\n```\r\nHere our Output is Tanh and our Input is Var. The gradient method does a BFS from Var to Tanh to count for each node how many backprop we should expect. If a node has two outgoing edges, it will be ready only when both would have been backpropagated and the gradient summed. In our case, Var has 2, Assign 0, MatMul 1. These values are saved into a pending array.\r\n\r\nThen the gradient method does a BFS from Tanh to Var, this is the actual backpropagation, the error is backpropagated until we reach Var. When a Node is reached, pending is decreased by one and if pending == 0, the Node is ready and is added to the queue of Nodes to be processed. If it is not ready, it will be reached again in the future and will be ready at some point.\r\n\r\nIn our case, doing a BFS from Var to Tanh give us 2 expected gradients, one from Assign and one from MatMul, whereas doing a BFS from Tanh to Var, we will reach Var only once, because we can't reach Assign from Tanh. In that case, the pending count will never reach Zero, Var will never be ready and the BFS will end. At the end, a check is done and if pending nodes are still there, an error is raised.\r\n\r\nThis PR updates the gradient method to ignore nodes that have 0 outgoing edges and are not in the list of Output (Tanh is the only one in the list of outputs in our case).\r\n\r\nThe unit tests have been updated to use Variable with Assign nodes and not Const, because differentiating w.r.t Const make less sense and the error would have been catched before.", "comments": ["Can one of the admins verify this patch?", "@skye: Yes the bug also manifest in this situation. I'll open an issue and work on a solution.", "@skye: I made changes in the gradient computation method to handle the case you outlined, I create a vector of boolean called `reachable_nodes` that I initialize doing a BFS from output to input. When counting how many node should be backpropagated for a given node, I only count it if `reachable_nodes[node id]` is equals to true. The same thing is done in the python gradient method.\r\n\r\nI added two more test cases for the case you outlined.\r\n\r\nI did not remove the stuff about `ThreeNodeVariable` because even if Const is simpler to use, it is particularly because we used Const and not Variable that we didn't see this bug before. I'll wait for your comment on that last point.", "@skye: Normally all your comments have been followed.", "@skye Updated following your comments.", "Jenkins, test this please", "@jhseu The failing tests seem unrelated to the PR content no?", "@theflofly I had those same two checks fail and fixed them just by merging master back into my branch. Worth a try?", "@bpiel done, let's see.", "@martinwicke why does it say \"TF Code Review \u2014 Waiting for status to be reported\"? Do I need to do something else?", "It means someone has to run the tests.\r\n\r\nJenkins, test this please!", "One test failing. https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/6212/console\r\n\r\n```\r\n//bazel_pip/tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test FAILED in 19.4s\r\n  /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu-python3/bazel-ci_build-cache/.cache/bazel/_bazel_jenkins/eab0d61a99b6696edb3d2aff87b585e8/execroot/workspace/bazel-out/local-py3-opt/testlogs/bazel_pip/tensorflow/contrib/timeseries/python/timeseries/input_pipeline_test/test.log\r\n\r\nExecuted 703 out of 703 tests: 702 tests pass and 1 fails locally.\r\nPIP tests-on-install FAILED\r\n```", "@bpiel I merged with master once again... If someone has some insights?", "I have a branch with this branch merged into it. I attempted to run the failing test (from above), but it passed for me. Not sure what that means.\r\n\r\n```\r\n$ sudo tensorflow/tools/ci_build/ci_build.sh CPU bazel test //tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test\r\n\r\n(...lots of stuff...)\r\n\r\n[3,132 / 3,133] Testing //tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test\r\nPASS: //tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test\r\nTarget //tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test up-to-date:\r\n  bazel-bin/tensorflow/contrib/timeseries/python/timeseries/input_pipeline_test\r\nINFO: Elapsed time: 1113.296s, Critical Path: 65.63s\r\n//tensorflow/contrib/timeseries/python/timeseries:input_pipeline_test    PASSED in 6.4s\r\n```\r\n", "@bpiel Thanks for your help on the issue. Let's wait to hear from the google team but it would be great if this could be merged.", "@theflofly Thank you for taking this on! I'm relying on the C API `TF_AddGradients` function and would be completely blocked if I didn't have access to your code. It fixes a **MAJOR** bug.", "@bpiel Thanks for the kind words. Does it work if us contributor are writing:\r\nJenkins, test this please! ", "Jenkins, test this please.\r\n\r\nNo it doesn't work. Sorry.", ":tada: thanks!", "Thanks! :)"]}, {"number": 12396, "title": "tf Dataset with tf.py_func doesn't work as the tutorial says", "body": "### System information\r\n-  Ubuntu 16.04\r\n- TF 1.3 (released version)\r\n- CUDA 8.0\r\n- CUDNN 6.0\r\n- Python 3.5\r\n\r\n### Describe the problem\r\nDataset with tf.py_func() doesn't work as the dataset tutorial says in (https://www.tensorflow.org/programmers_guide/datasets)\r\n\r\n\r\n### Source code / logs\r\nminimum code to reproduce the problem.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.data.python.ops.dataset_ops import Dataset\r\nimport numpy as np\r\nimport glob\r\nimport sys\r\nimport os\r\n\r\ndef _read_py_function(filename):\r\n    filename = filename.decode(sys.getdefaultencoding())\r\n    data = np.load(filename)\r\n    # print(data.shape)\r\n    label = np.random.randint(5)\r\n    # print(label)\r\n    return data.astype(np.float32), np.cast[np.float32](label)\r\n\r\n\r\nsample_size = 5\r\nseq_len = 3\r\nnum_samples = 7\r\n\r\n\r\ndata_dir = 'test_toy_data/'\r\n\r\nif not os.path.exists(data_dir):\r\n    os.makedirs(data_dir)\r\n\r\nfor i in range(num_samples):\r\n    np.save('%s/toy_%d.npy' % (data_dir, i),\r\n            np.random.rand(seq_len, sample_size))\r\n\r\n\r\ntr_filenames = glob.glob('%s/toy*.npy' % (data_dir))\r\n\r\ntr_dataset = Dataset.from_tensor_slices((tr_filenames))\r\ntr_dataset = tr_dataset.map(\r\n    lambda filename: tf.py_func(_read_py_function,\r\n                                [filename],\r\n                                [tf.float32, tf.float32]))\r\ntr_dataset = tr_dataset.shuffle(buffer_size=20)\r\ntr_dataset = tr_dataset.batch(2)\r\n\r\n\r\niterator = tr_dataset.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nsess = tf.Session()\r\n\r\nfor _ in range(1):\r\n    # training\r\n    sess.run(iterator.initializer)\r\n\r\n    while True:\r\n        try:\r\n            feats, labs = sess.run(next_element)\r\n            print(feats.shape)\r\n            print(labs)\r\n        except tf.errors.OutOfRangeError:\r\n            break\r\n```", "comments": ["Thanks for reporting this problem. It's definitely a bug, and we're working on a fix.", "In the meantime, you can work around it with the following tweak: \r\n\r\n```python\r\ntr_dataset = tr_dataset.map(\r\n    lambda filename: tuple(tf.py_func(_read_py_function,\r\n                                [filename],\r\n                                [tf.float32, tf.float32])))\r\n```\r\n\r\nThe problem stems from treating the list returned by `tf.py_func()` as an invitation to auto-stack the returned tensors into a single tensor. Converting it to tuple prevents that conversion from being attempted. The root cause is that the interpretation of lists in a `Dataset` nested structure changed between TF 1.2 and 1.3....", "Thanks for pointing out this trick. It works now when converting it to tuple. :+1: "]}, {"number": 12395, "title": "Does StreamExecutor support OpenCL?", "body": "I only see TENSORFLOW_STREAM_EXECUTOR_MACHINE_MANAGER_PREFER_OPENCL option in machine_manager.cc. However I don't see where SE links to OpenCL headers and libs. So I wonder of SE supports OpenCL?\r\n\r\nThanks,\r\nRLE", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12394, "title": "[bug] CUDA messes up after improperly closing Tensorflow session ", "body": "Whenever I run Keras sessions on PyCharm, and I \"forget\" to properly close the session (example: I log off the computer, or close session). It seems like the CUDA driver stops working. I can still run `$nvcc -V` and `$nvidia-smi` successfully but running the `./deviceQuery` sanity check from the CUDA samples fails, which means the system has stopped detecting the graphics card.\r\n\r\nHas anyone else had this problem? I definitely recall that this is not the first time this has happened to me using tensorflow vs say using torch, pytorch that also use the graphics card and \"forgetting\" to close my session.\r\n\r\nPerhaps I am just being spoiled? Note that this has happened both using tensorflow directly and using keras with tensorflow as backend (where I do not have direct control over closing sessions directly as far as I know).", "comments": ["@ArturoDeza does this also occur if you kill the TensorFlow process, or just when you log out?\r\n\r\n@zheng-xq is this a known problem? Since it happens in other ML libraries, it sounds like a bug in Cuda or the Nvidia drivers.", "@reedwm I have never had this issue happen to me on PyTorch or Torch, either by me not closing the session properly or just killing the process. So I think this is more of a bug on the tensorflow side.\r\n\r\n**Edit**: When I mean session: I mean both logging out, as well as a tensorflow session itself.", "Whoops sorry, I misinterpreted your comment. If it only occurs in TensorFlow, it is less likely to be a bug in Cuda.\r\n\r\nI cannot reproduce by creating a TensorFlow `tf.Session` then closing it or killing the process. I haven't tried logging out yet since that will close all my windows which is inconvenient, but I'll try that later.\r\n\r\n@zheng-xq do you know what the issue is?", "Thank you @reedwm for looking into this. Once I find the exact cause of this issue again, I will repost it. I guess I felt a quite frustrated since I did a mix of these things, and had to re-install the CUDA driver three times in the last 2 days because of running a process on keras with tensorflow in the backend, and all of a sudden the next day (or hour) the driver stopped working. But this has definitely happened to me before with pure tensorflow processes on other computers.", "@reedwm @zheng-xq **Bug Reproduced:** I started an interactive python keras program with tensorflow in the backend, and \"closed my laptop\". I left it closed overnight at work, and got back in the morning, and when I still had the interactive python console on, the GPU didn't seem to respond. I could still get a positive response by typing in `$nvcc -V` and `$nvidia-smi `, but couldn't run the `./deviceQuery` and `./bandwidthTest` samples from the NVIDIA CUDA Samples folder.\r\n\r\n**Solution:** Restart computer. It seems like the Driver gets messed up when you leave a process hanging in an interactive console and don't close the console before logging out of the session or \"making the computer sleep\" (the case of the laptop when you close it). It works fine now as if nothing has happened.\r\n\r\n"]}, {"number": 12393, "title": "Anaconda installation example doesn't match the description on web site", "body": "On this page:\r\n\r\nhttps://www.tensorflow.org/install/install_linux#installing_with_anaconda\r\n\r\nStep 4 says: \r\n\r\n> where tfBinaryURL is the URL of the TensorFlow Python package. For example, the following command installs the CPU-only version of TensorFlow for Python 2.7:\r\n\r\nHowever, the example command below it actually installs TensorFlow for Python 3.4, not 2.7:\r\n\r\n> pip install --ignore-installed --upgrade \\\r\n>  https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.3.0-cp34-cp34m-linux_x86_64.whl", "comments": ["This was fixed in https://github.com/tensorflow/tensorflow/commit/17f9320b33069ecda689bc71a917f5df5911666f at head (see #11708), however it seems that that change didn't make it to the r1.3. branch and the website content is now from the r1.3 branch.\r\n\r\n@av8ramit : Could you take a look - should that change be merged into the r1.3 branch and the website re-pushed?\r\n", "I'll CP this and push the docs, thanks @asimshankar and @pulzar ", "This is live."]}, {"number": 12392, "title": "Update link to doc for global_variables_initializer.", "body": "Update link to doc for global_variables_initializer.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 12391, "title": "Implement L2Loss gradient.", "body": "This branch implements L2LossGrad, the gradient for L2Loss.\r\nIt is a port of the python implementation:\r\nhttps://github.com/tensorflow/tensorflow/blob/c0198fd8d491a83576b75d1bc9becaa5910a19fe/tensorflow/python/ops/nn_grad.py#L752\r\n\r\nThis is my first PR to TensorFlow. When giving feedback, please assume that I barely know what I'm doing.\r\n\r\nSide question: I plan to do a few more gradients. Is it ok if I batch a few simple ones (like this one) into a single PR?", "comments": ["Can one of the admins verify this patch?", "@suharshs Do you want to be pinged on PRs like this? (I don't have rights to assign)", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please", "@gunan @yifeif for some reason, tests aren't triggering for this pull request (even after 3 attempts). It only runs the sanity checks.", "Looks like it just started https://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/6594/. Jenkins is having a long queue now.", "Jenkins, test this please", "Jenkins, test this please", "I merged master and now tests are passing again.", "@tensorflow-jenkins test this please", "The WIndows Cmake Tests aborted. Unrelated to the code changes?\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/4079/console\r\n```\r\n18:00:45 289/290 Test #205: C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/tensorflow/python/kernel_tests/tensordot_op_test.py .....................................   Passed  326.71 sec\r\n19:25:20 Build was aborted\r\n19:25:20 Aborted by unknown\r\n19:25:20 [Set GitHub commit status (universal)] PENDING on repos [] (sha:4d472fa) with context:tensorflow-pr-win-cmake-py\r\n19:25:20 Unable to get pull request builder trigger!!\r\n19:25:20 Setting status of e5ac94c2c4f5002679f3cd1734ac1cdaebf2d090 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pr-win-cmake-py/4079/ and message: 'FAILURE\r\n19:25:20  '\r\n19:25:20 Using context: Windows Cmake Tests\r\n19:25:21 Finished: ABORTED\r\n```", "The windows tests timed out after 30 minutes. We have been seeing this occasionally. Let's try again:\r\nJenkins, test this please.", "I wonder what happens if I type:\r\nJenkins, test this please", "Hmmmm.... doesn't trigger *ALL* the tests, I guess", "Jenkins, test this please", "I don't know what these failures mean.\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/6532/console\r\n```\r\nStarted by upstream project \"tensorflow-pull-requests-multijob\" build number 6653\r\noriginally caused by:\r\n GitHub pull request #12391 of commit 02a593886e5e1ac815bb887ac61417736cbd8564, no merge conflicts.\r\nSetting status of 02a593886e5e1ac815bb887ac61417736cbd8564 to PENDING with url https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/6532/ and message: 'BUILDING'\r\nUsing context: Linux CPU Tests\r\n[EnvInject] - Loading node environment variables.\r\nBuilding remotely on cpu42-slave (slave cpu-slave) in workspace /var/lib/jenkins/workspace/tensorflow-pull-requests-cpu\r\n > git rev-parse --is-inside-work-tree # timeout=10\r\nFetching changes from the remote Git repository\r\n > git config remote.origin.url https://github.com/tensorflow/tensorflow.git # timeout=10\r\nFetching upstream changes from https://github.com/tensorflow/tensorflow.git\r\n > git --version # timeout=10\r\n > git fetch --tags --progress https://github.com/tensorflow/tensorflow.git +refs/pull/*:refs/remotes/origin/pr/*\r\nERROR: Error fetching remote repo 'origin'\r\nhudson.plugins.git.GitException: Failed to fetch from https://github.com/tensorflow/tensorflow.git\r\n\tat hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:812)\r\n\tat hudson.plugins.git.GitSCM.retrieveChanges(GitSCM.java:1079)\r\n\tat hudson.plugins.git.GitSCM.checkout(GitSCM.java:1110)\r\n\tat hudson.scm.SCM.checkout(SCM.java:496)\r\n\tat hudson.model.AbstractProject.checkout(AbstractProject.java:1281)\r\n\tat hudson.model.AbstractBuild$AbstractBuildExecution.defaultCheckout(AbstractBuild.java:604)\r\n\tat jenkins.scm.SCMCheckoutStrategy.checkout(SCMCheckoutStrategy.java:86)\r\n\tat hudson.model.AbstractBuild$AbstractBuildExecution.run(AbstractBuild.java:529)\r\n\tat hudson.model.Run.execute(Run.java:1728)\r\n\tat hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)\r\n\tat hudson.model.ResourceController.execute(ResourceController.java:98)\r\n\tat hudson.model.Executor.run(Executor.java:405)\r\nCaused by: hudson.plugins.git.GitException: Command \"git fetch --tags --progress https://github.com/tensorflow/tensorflow.git +refs/pull/*:refs/remotes/origin/pr/*\" returned status code 128:\r\nstdout: \r\nstderr: error: RPC failed; curl 18 transfer closed with outstanding read data remaining\r\nfatal: The remote end hung up unexpectedly\r\n\r\n\tat org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:1903)\r\n\tat org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:1622)\r\n\tat org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$300(CliGitAPIImpl.java:71)\r\n\tat org.jenkinsci.plugins.gitclient.CliGitAPIImpl$1.execute(CliGitAPIImpl.java:348)\r\n\tat org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$1.call(RemoteGitImpl.java:153)\r\n\tat org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler$1.call(RemoteGitImpl.java:146)\r\n\tat hudson.remoting.UserRequest.perform(UserRequest.java:153)\r\n\tat hudson.remoting.UserRequest.perform(UserRequest.java:50)\r\n\tat hudson.remoting.Request$2.run(Request.java:336)\r\n\tat hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:68)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat hudson.remoting.Engine$1$1.run(Engine.java:94)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\tat ......remote call to JNLP4-connect connection from 118.56.196.104.bc.googleusercontent.com/104.196.56.118:47590(Native Method)\r\n\tat hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1545)\r\n\tat hudson.remoting.UserResponse.retrieve(UserRequest.java:253)\r\n\tat hudson.remoting.Channel.call(Channel.java:830)\r\n\tat org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler.execute(RemoteGitImpl.java:146)\r\n\tat sun.reflect.GeneratedMethodAccessor380.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.jenkinsci.plugins.gitclient.RemoteGitImpl$CommandInvocationHandler.invoke(RemoteGitImpl.java:132)\r\n\tat com.sun.proxy.$Proxy83.execute(Unknown Source)\r\n\tat hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:810)\r\n\t... 11 more\r\nERROR: Error fetching remote repo 'origin'\r\nERROR: [GitHub Commit Status Setter] - Cannot retrieve Git metadata for the build, setting build result to FAILURE\r\nUnable to get pull request builder trigger!!\r\nSetting status of 02a593886e5e1ac815bb887ac61417736cbd8564 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/6532/ and message: 'FAILURE\r\n '\r\nUsing context: Linux CPU Tests\r\nFinished: FAILURE\r\n```", "That is an infrastructure issue where our executors could not properly fetch information from github. We should retry.\r\nJenkins, test this please.", "try *again*?", "Something is causing our builds on windows to get stuck. But as far as I can tell, this change is not causing an issue. OK to merge. We will investigate windows issues separately.", "@gunan Works for me, thanks!"]}, {"number": 12390, "title": "Running ops with restored model gives FailedPreconditionError", "body": "I'm trying running some ops with a pre-trained model and the model was restored successfully. When running some ops that depend on one node in the graph, TF returns me \r\n```\r\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_213\r\n\t [[Node: Variable_213/read = Identity[T=DT_FLOAT, _class=[\"loc:@Variable_213\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_213)]]\r\n```\r\nIt might be that I missed something , but this didn't show up before I updated my Tensorflow to 1.2. I'm running TF on MacOS X, Python 2.7. Below is the source code:\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport csv\r\nimport os\r\nimport glob\r\nfrom PIL import Image\r\nfrom datetime import datetime\r\n\r\ndef read_image_file(filename):\r\n    image = tf.image.decode_png(tf.read_file(filename), channels=3)\r\n    image = tf.cast(image, tf.float32)\r\n    image = tf.reshape(image, [224, 112, 3])\r\n    return image\r\n\r\ndef labelFiles(filename):\r\n    label_list = []\r\n    for name in filename:\r\n        label_list.append(getLabel(name))\r\n    return label_list\r\n\r\ndef readCSV():\r\n    with open('label.csv', 'r') as infile:\r\n        reader = csv.reader(infile)\r\n        label = {rows[0]:rows[1] for rows in reader}\r\n        return label\r\n\r\ndef getWord(filename):\r\n    word = ''\r\n    (name, extension) = os.path.splitext(filename)\r\n    (path, name) = os.path.split(name)\r\n    for i in name:\r\n        if i.isalpha():\r\n            start = name.index(i)\r\n            word = word + name[start:]\r\n            #print(word)\r\n            break\r\n    return word\r\n\r\ndef getLabel(filename):\r\n    label = [0]*len(labels)\r\n    index =  int(labels.get(getWord(str(filename)), 0))\r\n    label[index] = 1\r\n    label = tf.constant(label, dtype = tf.float32)\r\n    tf.reshape(label, [-1])\r\n    print(label)\r\n    return label\r\n\r\ndef weight(shape):\r\n    initial = tf.truncated_normal(shape, stddev=0.01)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_0(shape):\r\n    initial = tf.constant(0.0, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef conv2d(image, weight, stride):\r\n    return tf.nn.conv2d(image, weight, strides=[1,stride,stride,1], padding='SAME')\r\n\r\ndef max_pool_3x3(image, stride):\r\n    return tf.nn.max_pool(image, ksize=[1, 3, 3, 1], strides=[1, stride, stride, 1], padding='SAME')\r\n\r\ndef batch_norm(image, out_size):\r\n    mean, var = tf.nn.moments(image, [0, 1, 2])\r\n    scale = tf.Variable(tf.ones([out_size]))\r\n    beta = tf.Variable(tf.zeros([out_size]))\r\n    epsilon = 0.001\r\n    bn = tf.nn.batch_normalization(image,mean,var,beta,scale,epsilon)\r\n    return bn\r\n\r\ndef inception(image, conv1_size, conv2_1_size, conv2_2_size, conv3_1_size, conv3_2_size, conv4_2_size):\r\n    conv1_weight = weight(conv1_size)\r\n    conv1_bias = bias_0([conv1_size[3]])\r\n    conv1 = tf.nn.relu(batch_norm(conv2d(image, conv1_weight, 1) + conv1_bias, conv1_size[3]))\r\n\r\n    conv2_1_weight = weight(conv2_1_size)\r\n    conv2_1_bias = bias_0([conv2_1_size[3]])\r\n    conv2_1 = tf.nn.relu(batch_norm(conv2d(image, conv2_1_weight, 1) + conv2_1_bias, conv2_1_size[3]))\r\n    conv2_2_weight = weight(conv2_2_size)\r\n    conv2_2_bias = bias_0([conv2_2_size[3]])\r\n    conv2_2 = tf.nn.relu(batch_norm(conv2d(conv2_1, conv2_2_weight, 1) + conv2_2_bias, conv2_2_size[3]))\r\n\r\n    conv3_1_weight = weight(conv3_1_size)\r\n    conv3_1_bias = bias_0([conv3_1_size[3]])\r\n    conv3_1 = tf.nn.relu(batch_norm(conv2d(image, conv3_1_weight, 1) + conv3_1_bias, conv3_1_size[3]))\r\n    conv3_2_weight = weight(conv3_2_size)\r\n    conv3_2_bias = bias_0([conv3_2_size[3]])\r\n    conv3_2 = tf.nn.relu(batch_norm(conv2d(conv3_1, conv3_2_weight, 1) + conv3_2_bias, conv3_2_size[3]))\r\n\r\n    pool4_1 = max_pool_3x3(image, 1)\r\n    conv4_2_weight = weight(conv4_2_size)\r\n    conv4_2_bias = bias_0([conv4_2_size[3]])\r\n    conv4_2 = tf.nn.relu(batch_norm(conv2d(pool4_1, conv4_2_weight, 1) + conv4_2_bias, conv4_2_size[3]))\r\n    return tf.concat([conv1, conv2_2, conv3_2, conv4_2], 3)\r\n\r\n\r\nnow = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\r\n\r\nroot_logdir = \"/home/tensorflow/tf_logs\"\r\nlogdir = \"{}/run-{}/\".format(root_logdir, now)\r\nsavedir = \"/home/tensorflow/saves\"\r\n\r\nlabels = readCSV()\r\nnum_labels = len(labels)\r\ntrainpath = '/home/resized/trainset/'\r\ntestpath = '/home/resized/testset/'\r\n\r\ntrainnames = glob.glob(trainpath + '*.png')\r\ntestnames = glob.glob(testpath + '*.png')\r\n\r\n\r\ntrain_label = labelFiles(trainnames)\r\ntest_label = labelFiles(testnames)\r\n\r\nbatch_size = 256\r\nepochs = 50\r\nnum_batch = int(len(trainnames)/batch_size)\r\ninitial_learning_rate = 0.001\r\ndecay_steps = num_batch*8\r\ndecay_rate = 0.96\r\nglobal_step = tf.Variable(0, trainable = False)\r\nlearning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate, staircase=True)\r\n\r\n\r\n# TRAIN QUEUE\r\ntrain_queue = tf.RandomShuffleQueue(len(trainnames)*1.5, 0, [tf.string, tf.float32], shapes=[[],[len(labels),]])\r\n\r\nenqueue_train = train_queue.enqueue_many([trainnames, train_label])\r\n\r\ntrain_image, train_image_label = train_queue.dequeue()\r\n\r\ntrain_image = read_image_file(train_image)\r\n\r\ntrain_batch, train_label_batch = tf.train.batch(\r\n    [train_image, train_image_label],\r\n    batch_size=batch_size,\r\n    num_threads=1,\r\n    capacity=10*batch_size,\r\n    enqueue_many=False,\r\n    shapes=[[224,112,3], [len(labels),]],\r\n    allow_smaller_final_batch=True\r\n)\r\n\r\ntrain_close = train_queue.close()\r\n\r\n# TEST QUEUE\r\n\r\ntest_queue = tf.FIFOQueue(len(testnames)*1.5, [tf.string, tf.float32], shapes=[[],[len(labels),]])\r\n\r\nenqueue_test = test_queue.enqueue_many([testnames, test_label])\r\n\r\ntest_image, test_image_label = test_queue.dequeue()\r\n\r\ntest_image = tf.expand_dims(read_image_file(test_image), 0)\r\n\r\ntest_close = test_queue.close()\r\n\r\n# MODEL\r\n\r\nkeep_prob = tf.placeholder(tf.float32)\r\n\r\nweights_conv1 = weight([7, 7, 3, 64])\r\n\r\nbias_conv1 = bias_0([64])\r\n\r\nweights_conv2 = weight([1, 1, 64, 64])\r\n\r\nbias_conv2 = bias_0([64])\r\n\r\nweights_conv3 = weight([3, 3, 64, 192])\r\n\r\nbias_conv3 = bias_0([192])\r\n\r\nweights_fc13 = weight([1024, num_labels])\r\n\r\nbias_fc13 = bias_0([num_labels])\r\n\r\n\r\ndef GoogleNet(data):\r\n\r\n    # First Conv. Layer\r\n    conv1 = tf.nn.relu(batch_norm(conv2d(data, weights_conv1, 2) + bias_conv1, 64))\r\n\r\n    pool1 = max_pool_3x3(conv1, 2)\r\n\r\n    lrn1 = tf.nn.local_response_normalization(pool1, 2, 1, 0.00002, 0.75)\r\n\r\n    # Second Conv. Layer\r\n    conv2 = tf.nn.relu(batch_norm(conv2d(lrn1, weights_conv2, 1) + bias_conv2, 64))\r\n\r\n    conv3 = tf.nn.relu(batch_norm(conv2d(conv2, weights_conv3, 1) + bias_conv3, 192))\r\n\r\n    lrn2 = tf.nn.local_response_normalization(conv3, 2, 1, 0.00002, 0.75)\r\n\r\n    pool2 = max_pool_3x3(lrn2, 2)\r\n\r\n    # First Inception Layer\r\n    incep4 = inception(\r\n        pool2,\r\n        [1, 1, 192, 64],\r\n        [1, 1, 192, 96],\r\n        [3, 3, 96, 128],\r\n        [1, 1, 192, 16],\r\n        [5, 5, 16, 32],\r\n        [1, 1, 192, 32]\r\n    )\r\n\r\n    incep5 = inception(\r\n        incep4,\r\n        [1, 1, 256, 128],\r\n        [1, 1, 256, 128],\r\n        [3, 3, 128, 192],\r\n        [1, 1, 256, 32],\r\n        [5, 5, 32, 96],\r\n        [1, 1, 256, 64]\r\n    )\r\n\r\n    pool3 = max_pool_3x3(incep5, 2)\r\n\r\n    # Second Inception Layer\r\n    incep6 = inception(\r\n        pool3,\r\n        [1, 1, 480, 192],\r\n        [1, 1, 480, 96],\r\n        [3, 3, 96, 208],\r\n        [1, 1, 480, 16],\r\n        [5, 5, 16, 48],\r\n        [1, 1, 480, 64]\r\n    )\r\n\r\n    incep7 = inception(\r\n        incep6,\r\n        [1, 1, 512, 160],\r\n        [1, 1, 512, 112],\r\n        [3, 3, 112, 224],\r\n        [1, 1, 512, 24],\r\n        [5, 5, 24, 64],\r\n        [1, 1, 512, 64]\r\n    )\r\n\r\n    incep8 = inception(\r\n        incep7,\r\n        [1, 1, 512, 128],\r\n        [1, 1, 512, 128],\r\n        [3, 3, 128, 256],\r\n        [1, 1, 512, 24],\r\n        [5, 5, 24, 64],\r\n        [1, 1, 512, 64]\r\n    )\r\n\r\n    incep9 = inception(\r\n        incep8,\r\n        [1, 1, 512, 112],\r\n        [1, 1, 512, 144],\r\n        [3, 3, 144, 288],\r\n        [1, 1, 512, 32],\r\n        [5, 5, 32, 64],\r\n        [1, 1, 512, 64]\r\n    )\r\n\r\n    incep10 = inception(\r\n        incep9,\r\n        [1, 1, 528, 256],\r\n        [1, 1, 528, 160],\r\n        [3, 3, 160, 320],\r\n        [1, 1, 528, 32],\r\n        [5, 5, 32, 128],\r\n        [1, 1, 528, 128]\r\n    )\r\n\r\n    pool4 = max_pool_3x3(incep10, 2)\r\n\r\n    # Third Inception Layer\r\n    incep11 = inception(\r\n        pool4,\r\n        [1, 1, 832, 256],\r\n        [1, 1, 832, 160],\r\n        [3, 3, 160, 320],\r\n        [1, 1, 832, 32],\r\n        [5, 5, 32, 128],\r\n        [1, 1, 832, 128]\r\n    )\r\n\r\n    incep12 = inception(\r\n        incep11,\r\n        [1, 1, 832, 384],\r\n        [1, 1, 832, 192],\r\n        [3, 3, 192, 384],\r\n        [1, 1, 832, 48],\r\n        [5, 5, 48, 128],\r\n        [1, 1, 832, 128]\r\n    )\r\n\r\n    pool5 = tf.nn.avg_pool(incep12, ksize = [1, 7, 4, 1], strides = [1, 1, 1, 1], padding = 'VALID')\r\n\r\n    pool5_flat = tf.reshape(pool5, [-1, 1024])\r\n\r\n    # FC Layers\r\n    fc13_drop = tf.nn.dropout(pool5_flat, keep_prob)\r\n\r\n    fc13 = tf.matmul(fc13_drop, weights_fc13) + bias_fc13\r\n\r\n    return fc13\r\n\r\n#Training\r\nwith tf.name_scope(\"cost_function\") as scope:\r\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=train_label_batch, logits=GoogleNet(train_batch)))\r\n    train_step = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(cost, global_step=global_step)\r\n\r\n\r\n#Accuracy\r\nwith tf.name_scope(\"accuracy\") as scope:\r\n    correct_prediction = tf.equal(tf.argmax(GoogleNet(test_image), 1), tf.argmax(test_image_label, 0))\r\n    accuracy = tf.cast(correct_prediction, tf.float32)\r\n\r\ncost_summary = tf.summary.scalar(\"cost_function\", cost)\r\nfile_writer = tf.summary.FileWriter(logdir)\r\n\r\n#Session\r\nwith tf.Session() as sess:\r\n    saver = tf.train.Saver()\r\n    saver.restore(sess, \"/Users/aleex/Dropbox/TensorFlow/GoogleNet/saves/GoogleNet.ckpt\")\r\n    print(\"Model restored...\")\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord, start=True)\r\n\r\n    sess.run(enqueue_test)\r\n    accuracy_vector = []\r\n   \r\n    print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))\r\n    \r\n    for num in range(len(testnames)):\r\n        accuracy_vector.append(sess.run(accuracy, feed_dict={keep_prob: 1.0}))\r\n    mean_accuracy = sess.run(tf.divide(tf.add_n(accuracy_vector), len(testnames)))\r\n\r\n    print(\"test accuracy %g\"%mean_accuracy)\r\n    sess.run(test_close)\r\n\r\n    coord.request_stop()\r\n    coord.join(threads)\r\n\r\n    file_writer.close()\r\n```\r\n\r\nIt was the line `print(tf.argmax(GoogleNet(test_image), 1).eval(feed_dict={keep_prob: 1.0}))` that messed this up.\r\nA side question here: the model was trained well, but it gives me a poor test accuracy of 0.0004, which basically shows that the predictions with the test samples are all wrong. Is this a bug?", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12389, "title": "Python quit unexpectedly while using the _batch_ops.so plug-in.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra 10.12.6\r\n- **TensorFlow installed from (source or binary)**: both source and binary\r\n- **TensorFlow version (use command below)**: ('v1.3.0-0-g9e76bf324', '1.3.0')\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: 0.5.3-homebrew\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**:pydoc modules\r\n\r\n\r\n\r\n### Describe the problem\r\n\r\n\r\nEvery time I use _pydoc modules_, it crashes with this message:\r\n_python(9152,0x7fffbdeb33c0) malloc: *** error for object 0x11531fb58: pointer being freed was not allocated_\r\n\r\nThe same result for both 1.2.1 and 1.3.0, official build or custom builds.\r\n\r\n\r\n### Source code / logs\r\n\r\nThe backtrace:\r\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\r\n0   libsystem_kernel.dylib        \t0x00007fffb50cdd42 __pthread_kill + 10\r\n1   libsystem_pthread.dylib       \t0x00007fffb51bb457 pthread_kill + 90\r\n2   libsystem_c.dylib             \t0x00007fffb5033420 abort + 129\r\n3   libsystem_malloc.dylib        \t0x00007fffb5122fe7 free + 530\r\n4   _batch_ops.so                 \t0x0000000119fb853c tensorflow::OpDef::SharedDtor() + 108\r\n5   _batch_ops.so                 \t0x0000000119fb838c tensorflow::OpDef::~OpDef() + 28\r\n6   _batch_ops.so                 \t0x0000000119f4b274 _GLOBAL__sub_I_batch_ops.cc + 388\r\n7   dyld                          \t0x000000010ea9aa1b ImageLoaderMachO::doModInitFunctions(ImageLoader::LinkContext const&) + 385\r\n8   dyld                          \t0x000000010ea9ac1e ImageLoaderMachO::doInitialization(ImageLoader::LinkContext const&) + 40\r\n9   dyld                          \t0x000000010ea964aa ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 338\r\n10  dyld                          \t0x000000010ea95524 ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 138\r\n11  dyld                          \t0x000000010ea955b9 ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 75\r\n12  dyld                          \t0x000000010ea8a7cd dyld::runInitializers(ImageLoader*) + 87\r\n13  dyld                          \t0x000000010ea923ec dlopen + 556\r\n14  libdyld.dylib                 \t0x00007fffb4f9c832 dlopen + 59\r\n15  _pywrap_tensorflow_internal.so\t0x0000000110c3165f tensorflow::internal::LoadLibrary(char const*, void**) + 47\r\n16  _pywrap_tensorflow_internal.so\t0x0000000110c30b74 tensorflow::(anonymous namespace)::PosixEnv::LoadLibrary(char const*, void**) + 20\r\n17  _pywrap_tensorflow_internal.so\t0x0000000110b27f17 tensorflow::LoadLibrary(char const*, void**, void const**, unsigned long*) + 1431\r\n18  _pywrap_tensorflow_internal.so\t0x000000010edaff44 TF_LoadLibrary + 52\r\n19  _pywrap_tensorflow_internal.so\t0x000000010eb2b452 _wrap_TF_LoadLibrary(_object*, _object*) + 162\r\n20  org.python.python             \t0x00000001096fd4d4 PyEval_EvalFrameEx + 14624\r\n21  org.python.python             \t0x00000001096f99be PyEval_EvalCodeEx + 1617\r\n22  org.python.python             \t0x00000001097003e2 0x109677000 + 562146\r\n23  org.python.python             \t0x00000001096fce4e PyEval_EvalFrameEx + 12954\r\n24  org.python.python             \t0x00000001096f99be PyEval_EvalCodeEx + 1617\r\n25  org.python.python             \t0x00000001097003e2 0x109677000 + 562146\r\n26  org.python.python             \t0x00000001096fce4e PyEval_EvalFrameEx + 12954\r\n27  org.python.python             \t0x00000001096f99be PyEval_EvalCodeEx + 1617\r\n28  org.python.python             \t0x00000001096f9367 PyEval_EvalCode + 48\r\n29  org.python.python             \t0x000000010970e6bd PyImport_ExecCodeModuleEx + 241\r\n30  org.python.python             \t0x00000001097113c7 0x109677000 + 631751\r\n31  org.python.python             \t0x0000000109710e2c 0x109677000 + 630316\r\n32  org.python.python             \t0x0000000109710a00 0x109677000 + 629248\r\n33  org.python.python             \t0x000000010970fc10 PyImport_ImportModuleLevel + 1185\r\n34  org.python.python             \t0x00000001096f5006 0x109677000 + 516102\r\n35  org.python.python             \t0x00000001096816fb PyObject_Call + 99\r\n36  org.python.python             \t0x00000001096ffdbb PyEval_CallObjectWithKeywords + 165\r\n37  org.python.python             \t0x00000001096fbc0f PyEval_EvalFrameEx + 8283\r\n38  org.python.python             \t0x00000001096f99be PyEval_EvalCodeEx + 1617\r\n39  org.python.python             \t0x00000001096f9367 PyEval_EvalCode + 48\r\n40  org.python.python             \t0x000000010970e6bd PyImport_ExecCodeModuleEx + 241\r\n41  org.python.python             \t0x00000001097113c7 0x109677000 + 631751\r\n42  org.python.python             \t0x000000010971164f 0x109677000 + 632399\r\n43  org.python.python             \t0x0000000109710e2c 0x109677000 + 630316\r\n44  org.python.python             \t0x0000000109710a00 0x109677000 + 629248\r\n45  org.python.python             \t0x000000010970fc10 PyImport_ImportModuleLevel + 1185\r\n46  org.python.python             \t0x00000001096f5006 0x109677000 + 516102\r\n47  org.python.python             \t0x00000001096fd4d4 PyEval_EvalFrameEx + 14624\r\n48  org.python.python             \t0x0000000109696b00 0x109677000 + 129792\r\n49  org.python.python             \t0x00000001096fa0ec PyEval_EvalFrameEx + 1336\r\n50  org.python.python             \t0x0000000109696b00 0x109677000 + 129792\r\n51  org.python.python             \t0x00000001096fa0ec PyEval_EvalFrameEx + 1336\r\n52  org.python.python             \t0x0000000109696b00 0x109677000 + 129792\r\n53  org.python.python             \t0x00000001096fa0ec PyEval_EvalFrameEx + 1336\r\n54  org.python.python             \t0x00000001096f99be PyEval_EvalCodeEx + 1617\r\n55  org.python.python             \t0x00000001097003e2 0x109677000 + 562146\r\n56  org.python.python             \t0x00000001096fce4e PyEval_EvalFrameEx + 12954\r\n57  org.python.python             \t0x00000001096f99be PyEval_EvalCodeEx + 1617\r\n58  org.python.python             \t0x00000001097003e2 0x109677000 + 562146\r\n59  org.python.python             \t0x00000001096fce4e PyEval_EvalFrameEx + 12954\r\n60  org.python.python             \t0x0000000109700475 0x109677000 + 562293\r\n61  org.python.python             \t0x00000001096fce4e PyEval_EvalFrameEx + 12954\r\n62  org.python.python             \t0x0000000109700475 0x109677000 + 562293\r\n63  org.python.python             \t0x00000001096fce4e PyEval_EvalFrameEx + 12954\r\n64  org.python.python             \t0x00000001096f99be PyEval_EvalCodeEx + 1617\r\n65  org.python.python             \t0x00000001096f9367 PyEval_EvalCode + 48\r\n66  org.python.python             \t0x00000001097195dd 0x109677000 + 665053\r\n67  org.python.python             \t0x0000000109719680 PyRun_FileExFlags + 133\r\n68  org.python.python             \t0x00000001097191d1 PyRun_SimpleFileExFlags + 702\r\n69  org.python.python             \t0x000000010972ab6a Py_Main + 3094\r\n70  libdyld.dylib                 \t0x00007fffb4f9f235 start + 1", "comments": ["I can reproduce by running the command `pydoc modules` from the root TensorFlow directory. I get the error message\r\n\r\n```\r\n--------------------------------------------------------------------------\r\nIt looks like opal_init failed for some reason; your parallel process is\r\nlikely to abort.  There are many reasons that a parallel process can\r\nfail during opal_init; some of which are due to configuration or\r\nenvironment problems.  This failure appears to be an internal failure;\r\nhere's some additional information (which may only be relevant to an\r\nOpen MPI developer):\r\n\r\n  opal_error_register failed\r\n  --> Returned value -2 instead of OPAL_SUCCESS\r\n--------------------------------------------------------------------------\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n@gunan Do you know why this happens? \r\n\r\n", "@reedwm I believe your error is from the mpi package, not tensorflow.", "This sounds a little bit like the known caveat of protobuf that one shouldn't share objects from protobuf across library boundaries, lest the global `fixed_address_empty_string` might get freed? \r\n\r\n(This may be no longer an issue in the protobuf being used for tensorflow - i may be out of date..) ?\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "@jhseu or @allenlavoie may have better insight into this issue.", "I see, `pydoc modules` imports everything, and \"everything\" in this case causes what I'm guessing is a symbol conflict. Starting with 1.4 TensorFlow will not cause these symbol conflicts (no RTLD_GLOBAL), but it's possible another package is exporting symbols which cause issues for TensorFlow . Without knowing which other package is causing the issue it's hard to say more (and even then the answer is probably \"file a bug with them to not export the problematic symbol globally\").\r\n\r\nFWIW `pydoc modules` is working on my machine with TF 1.7ish.\r\n\r\nI'm going to close this. Feel free to reopen or comment if you figure out which specific Python package(s) are causing issues."]}, {"number": 12388, "title": "Why my tensorflow-gpu runs only on cpu? Ubuntu", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from **: `pip3 install tensorflow-gpu`\r\n- **TensorFlow version**: ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0')\r\n- **Python version**:  Python 3.5.2\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61\r\n\r\n\r\nMy tensorflow-gpu runs only on cpu, how to fix it?\r\nI've already tried to set \r\n`CUDA_DEVICE_VISIBLE=all my gpus`  But it didn't work.\r\n\r\nThen I tried to use this code:\r\n`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))`\r\nThe output is following:\r\n`Device mapping: no known devices.`\r\n`2017-08-18 16:44:44.654177: I tensorflow/core/common_runtime/direct_session.cc:300] Device mapping:`\r\nThen nothing. It doesn't output the map.\r\nI've also tried to install it with virtualenv and run the program in virtualenv or reinstall, still not working. \r\nWhy is this happening? How can I fix it? \r\n\r\n\r\n\r\n", "comments": ["It is possible that you haven't installed the CUDA driver properly.\r\n\r\nWhat is the output of:\r\n`$nvcc -V`\r\nand \r\n`$nvidia-smi`\r\n\r\nGo to your CUDA Samples folder and run the `./deviceQuery` script to check if your system is finding your graphics card.", "I've noticed that using `pip install tensorflow-gpu` also installs `tensorflow` as a dependency, but `pip install tensorflow-gpu==1.2.1` does not demonstrate this behavior. Is this an issue with the 1.3 rollout, or am I completely missing something?", "Something also to check: install Tensorflow from source (with Bazel). That's what I did, and it started running processes on GPU.", "@ArturoDeza \r\n\r\nI don't think it is the problem. Because tensorflow worked properly all the time before I uninstalled it for some reason.(The reason is somehow only SGD optimizer doesn't work in keras while other optimizers work well.) However, when I try to reinstall tensorflow, this problem shows up.\r\nThe output of `$nvcc -V`:\r\n`nvcc: NVIDIA (R) Cuda compiler driver`\r\n`Copyright (c) 2005-2016 NVIDIA Corporation`\r\n`Built on Tue_Jan_10_13:22:03_CST_2017`\r\n`Cuda compilation tools, release 8.0, V8.0.61`\r\n\r\nThe output of `nvidia-smi`:\r\n`| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 0000:04:00.0     Off |                  N/A |\r\n|  0%   24C    P8    11W / 180W |    113MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1080    Off  | 0000:05:00.0     Off |                  N/A |\r\n|  0%   25C    P8     6W / 180W |      2MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 1080    Off  | 0000:08:00.0     Off |                  N/A |\r\n|  0%   26C    P8     6W / 180W |      2MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce GTX 1080    Off  | 0000:09:00.0     Off |                  N/A |\r\n|  0%   26C    P8     6W / 180W |      2MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  GeForce GTX 1080    Off  | 0000:85:00.0     Off |                  N/A |\r\n|  0%   24C    P8     6W / 180W |      2MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  GeForce GTX 1080    Off  | 0000:86:00.0     Off |                  N/A |\r\n|  0%   28C    P8     6W / 180W |      2MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  GeForce GTX 1080    Off  | 0000:89:00.0     Off |                  N/A |\r\n|  0%   26C    P8     6W / 180W |      2MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  GeForce GTX 1080    Off  | 0000:8A:00.0     Off |                  N/A |\r\n|  0%   27C    P8     6W / 180W |      2MiB /  8114MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n`", "@mykalu \r\nOh my god! It works! Tensorflow runs on gpus.\r\nThank you very much.\r\nMaybe the latest version of tensorflow(1.3.1) isn't compatible with my CUDA", "I have encountered the same problem (ubuntu16.04).\r\nAs mykalu-san said, my 'pip install tensorflow-gpu' also installed tensorflow.", "same problem to me, I am using tensorflow(1.3.0)", "@vell001 \r\ntry to use this\r\n`pip install tensorflow-gpu==1.2.1`", "Just as @mykalu said, it's really confusing that `pip3 install tensorflow-gpu` also install `tensorflow`. And it seems that this makes gpu version not work.", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@DNXie @skyoung @mykalu I had the same problem, but then found that I had `tensorflow-tensorboard` 0.1.4 installed which depends on `tensorflow`. That seemed to be the source of the erroneous dependency, and updating it to 0.1.5 made it all work for me because `tensorflow` wasn't installed anymore.", "@srowen Nice catch, that worked for me. Thanks!", "@srowen \r\nThat's true. I have tensorboard on that version too. Thank you!", "@mykalu when I run `pip install tensorflow-gpu==1.3.0` I also have a folder called tensorflow. Is this correct? I am having the same issue - my GPU is not being utilized. ", "uninstalling tensorflow-gpu==1.3.0 and installing tensorflow-gpu==1.2.1 worked for me! thanks ", "As of writing the latest versions of TF only support the Titan V on the consumer side \ud83d\ude11\r\n\r\nCrosscheck what software works for what hardware\r\n    https://developer.nvidia.com/cuda-gpus\r\n    https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nIf you need to update CUDA, use the binary provided:\r\n    https://developer.nvidia.com/cuda-downloads\r\n\r\nCheck what cuDNN version you need here (requires login):\r\n    https://developer.nvidia.com/rdp/cudnn-download\r\n\r\nFor cuDNN download usually to `/usr/local` and extract.\r\n\r\nFinally, reinstall. TF version depends on GPU and Cuda version.\r\n\r\n    pip uninstall tensorflow\r\n    pip install tensorflow==1.4.1\r\n    pip install tensorflow-gpu==1.4.1\r\n\r\n\r\n\r\n", "For me tf 1.4.1 works properly.", "@jiforcen Could you tell me which method you used to install the CUDA toolkit, CuDNN and TensorFlow(GPU)? I've been trying several methods for days now and I just can't make tensorflow detect my GPU.", "@caiosuzuki  I've followed the nvidia guide. [http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#introduction](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#introduction)\r\nDon't forget to setup the environment variables!! [http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#post-installation-actions)\r\nCheck that your nvidia driver is configured and cuda toolkit is installed properly.\r\nAfter that I've installed tensorflow gpu in a conda environment. I've problems with tensorflow gpu 1.3 but tensorflow gpu 1.4 works fine for me.", "My code was running on the CPU, so I changed tensorflow-gpu from version 1.4.1 to 1.2.1 and now it shows me the following error. Please help me out here.\r\n----------------------------------------------------------------------------------------------------------------\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"extract_features.py\", line 16, in <module>\r\n    from data import DataSet\r\n  File \"/home/shayan/five-video-classification-methods/data.py\", line 12, in <module>\r\n    from processor import process_image\r\n  File \"/home/shayan/five-video-classification-methods/processor.py\", line 4, in <module>\r\n    from keras.preprocessing.image import img_to_array, load_img\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import activations\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/activations.py\", line 3, in <module>\r\n    from . import backend as K\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.py\", line 64, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcusolver.so.8.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "@shayan09 Have you fixed the problem? I have got into the same trouble.", "The current version of CUDA is not supported by tensorflow. So just downgrade your CUDA version to 7.0 and then reinstall the normal tensorflow and then tensorflow-gpu. Worked for me. ", "Having the same trouble and none of the advice works. Can't downgrade CUDA, tensorflow-gpu package looks for 9.0 DLLs explicitly. I can watch my CPU/GPU usage while its running and TF *says* its running through the GPU, but the CPU is pegged at 100% and the GPU usage hovers around 5%. I tried adjusting the batch size as high as I can get it without hitting OOM errors, but it seemed to make no difference. (elsewhere I read small batch sizes might make the GPU throughput suffer).\r\n\r\nAre there known current issues with the windows binaries for the Tensorflow1.5.0/CUDA 9.0/cuDNN 7.0 combination? If there are known issues, is there a previous combination that might work? If there are other steps that might be overlooked to run on the GPU, are there guides for common missteps?", "Having similar problems as @ironfroggy with  Win10/Tensorflow1.5.0/CUDA 9.0/cuDNN 7.0 combination", "I have the same problem with Tensorflow 1.5 and Cuda 9.0 :-(", "@ironfroggy \r\n@hyperh \r\n\r\nI have the same problem as yours. Could you find a solution?", "@ironfroggy\r\n@hyperh\r\n@VanitarNordic\r\n\r\nLikewise, same problem. tensorflow-gpu==1.5.0/Cuda9.0/cuDNN7.0/windows 7. \r\nI'll add that when I first encountered the problem I was getting an error importing tensorflow stating that it couldn't find cudart64_90.dll (it was in my path). After I restarted my computer, tensorflow successfully imported, but it just uses cpu.\r\n\r\n\r\nUPDATE: It seemed to be a driver issue for me: I uninstalled, re-downloaded, and re-installed my gpu drivers, restarted my computer, and it seems to be working fine now!", "I encountered the same problem. \r\nFor me a simple reboot of the system did the trick, my tensorflow is running on GPU\r\n`reboot`\r\n\r\n ", "Same issue with Tensorflow 1.8.0 and Cuda 8.0\r\n", "I have busted my head and eyes last few days to figure out why TF 1.8 is not running on gpu with cuda 9.2 installation and still have no answer  but close so I want to share my experience in case anyone have solved the problem.  I installed from scratch cuda 9.2 with cudnn 7.14 and ncc2.2.13 per nvidia website then installed TF from the source via bazel all were successful as I ran a toy code Hello and worked but when I checked devices it was only CPU and not GPU which was a disappointment.  Then per nvidia suggestion I downloaded the coda 9.2 samples and compiled and ran. /deviceQuery and I got the table which declares that cuda 9.2 and cunn 7.14 are configured correctly as it shows the GPU device and its compatibility with the runtime of Ubuntu software and finally s pass on the test!  So I don't know why on earth I cannot get TF 1.8 run on GPU.  This leaves me with suspiciousion that there is an incompatibility here between TF 1.8 gpu and cuda 9.2 that tensorflow website is not telling us.   I hope I am wrong and there is something I am missing to get gpu working. Let me know if anyone has ideas or suggestions.  Thank you ", "FYI, it works for me with CUDA 9.0 and tensorflow 1.8.0, when I install `tensorflow` and `tensorflow-gpu` separately:\r\n```\r\npip uninstall tensorflow\r\npip uninstall tensorflow-gpu\r\npip install tensorflow==1.8.0\r\npip install tensorflow-gpu==1.8.0\r\n```\r\n", "@erobic thank you! that work for me", "I had the same problem where only the CPU was used. You need to uninstall tensorflow, and only have tensorflow-gpu installed", "@mmawada Try this link, it worked for me - TF==1.8, Cuda 9.2\r\nhttps://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-9-2-for-python-on-ubuntu/", "Thank you!\r\nInstall:\r\nUbuntu 18.04\r\nDriver Version: 390.77\r\ncuda:\r\n```\r\nnvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n```\r\ncudnn: 7.1\r\n\r\nand this is what I installed:\r\n```\r\nconda create -n tt  anaconda python\r\nsource activate tt\r\n\r\nconda install tensorflow-gpu==1.11 cudatoolkit==9.0 cudnn==7.1.2 h5py\r\npip install pillow h5py keras\r\npip uninstall tensorflow\r\npip install tensorflow-gpu\r\n\r\ngit clone https://github.com/fchollet/keras.git\r\ncd keras/examples\r\npython mnist_cnn.py\r\n```\r\n\r\nit worked for me!!!\r\ntuananhktmt", "same problem here. I use ubuntu 16.4, tensorflow 1.12.0, gtx 1080 and cuda 9.0 with cudnn7.3.1.\r\nMy network was training normally when I went back home last friday. But in monday morning I found it unable to run evaluation on gpu, even if I set \"with tf.device('/device:GPU:0')\". I tried \"nvidia-smi\" in terminal, there is no problem with my gpu and driver", "> same problem here. I use ubuntu 16.4, tensorflow 1.12.0, gtx 1080 and cuda 9.0 with cudnn7.3.1.\r\n> My network was training normally when I went back home last friday. But in monday morning I found it unable to run evaluation on gpu, even if I set \"with tf.device('/device:GPU:0')\". I tried \"nvidia-smi\" in terminal, there is no problem with my gpu and driver\r\n\r\nMee too, just reinstall ubuntu :))", "This solved for me, while training a Transformer Translation Model:\r\n\r\n`pip install tf-nightly-gpu`", "> FYI, it works for me with CUDA 9.0 and tensorflow 1.8.0, when I install `tensorflow` and `tensorflow-gpu` separately:\r\n> \r\n> ```\r\n> pip uninstall tensorflow\r\n> pip uninstall tensorflow-gpu\r\n> pip install tensorflow==1.8.0\r\n> pip install tensorflow-gpu==1.8.0\r\n> ```\r\n\r\nDo you guys think using cache would be a problem? Do I need use `--no_cache` option?", "hey guys it worked for me with tensorflow 2.0+ with: \r\nexport CUDA_VISIBLE_DEVICES='0'\r\n\r\nif you have more GPUs you need to set '0','1', etc", "Hi, guys! I have same problem. My OS is \r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 20.04 LTS\r\nRelease:\t20.04\r\nI have this driver:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\nI have this packages:\r\ntb-nightly              2.3.0a20200615     \r\ntensorboard             2.2.2              \r\ntensorboard-plugin-wit  1.6.0.post3        \r\ntensorflow              2.2.0              \r\ntensorflow-estimator    2.2.0              \r\ntensorflow-gpu          2.2.0\r\nWhy I can not use GPU in processing?"]}, {"number": 12387, "title": "The precision difference between tensorflow and numpy when using tf.reduce_mean and np.mean", "body": "```\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n== cat /etc/issue ===============================================\r\nLinux quad6 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.1 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.1-2ubuntu1~16.04) 5.4.1 20160904\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux quad6 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\ntensorflow-fold (0.0.1)\r\ntensorflow-gpu (1.2.1)\r\ntensorflow-tensorboard (0.1.4)\r\n\r\n== check for virtualenv =========================================\r\nTrue\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Aug 18 16:14:35 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN Xp            Off  | 0000:05:00.0     Off |                  N/A |\r\n| 28%   51C    P0    67W / 250W |      0MiB / 12188MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  TITAN Xp            Off  | 0000:06:00.0     Off |                  N/A |\r\n| 31%   55C    P0    66W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  TITAN Xp            Off  | 0000:09:00.0     Off |                  N/A |\r\n| 54%   84C    P2   130W / 250W |  11655MiB / 12189MiB |     52%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  TITAN Xp            Off  | 0000:0A:00.0     Off |                  N/A |\r\n| 23%   33C    P0    61W / 250W |      0MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    2      5854    C   python3                                      11651MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n\r\n\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.2.0-5-g435cdfc 1.2.1\r\n```\r\n### Describe the problem\r\nWhen using `tf.reduce_mean`, i found the behaviour between `tensorflow` and `numpy` was different when dtype was `float32`. The experiment shows that this maybe caused by the precision problem in `tensorflow`. When I changed the dtype to `tf.float64`, the problem fixed. \r\nOr if I calculate the mean axis by axis, the problem also disappear.\r\nHowever, if I use `numpy` array, the result was right no matter the dtype is  `float32` or `float64`.\r\nIs this a normal behaviour or will be fixed later?\r\n\r\nThe test code I used as following:\r\n### Source code / logs\r\n```Python\r\nIn [61]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float32)\r\n\r\nIn [62]: y = tf.reduce_mean(x)\r\n\r\nIn [63]: sess.run(y)\r\nOut[63]: 0.26278782\r\n\r\nIn [64]: y = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(x, axis=2), axis=-1))\r\n\r\nIn [65]: sess.run(y)\r\nOut[65]: 0.693142\r\n\r\nIn [66]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float32)\r\n\r\nIn [67]: y = tf.reduce_mean(x)\r\n\r\nIn [68]: sess.run(y)\r\nOut[68]: 0.26278782\r\n\r\nIn [69]: x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float64)\r\n\r\nIn [70]: y = tf.reduce_mean(x)\r\n\r\nIn [71]: sess.run(y)\r\nOut[71]: 0.6931000008030902\r\n\r\nIn [72]: y = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(x, axis=2), axis=-1))\r\n\r\nIn [73]: sess.run(y)\r\nOut[73]: 0.69310000000034644\r\n\r\nIn [74]: xn = np.full((32, 100, 79084), 0.6931, dtype=np.float64)\r\n\r\nIn [75]: yn = np.mean(xn)\r\n\r\nIn [76]: yn\r\nOut[76]: 0.69310000000032723\r\n\r\nIn [77]: xn = np.full((32, 100, 79084), 0.6931, dtype=np.float32)\r\n\r\nIn [78]: yn = np.mean(xn)\r\n\r\nIn [79]: yn = np.mean(xn)\r\n\r\nIn [80]: yn\r\nOut[80]: 0.69321907\r\n```", "comments": ["@ekelsen do you know why this issue is occurring?", "This is on CPU?  \r\n\r\nThere are likely two issues:\r\n\r\n1) The CPU reductions use a numerically terrible algorithm for doing sums.  Summing into one or few accumulators - this leads to inaccurate sums for large N.  Breaking the sum in 3 parts ameliorates this problem.  This problem has come up multiple times before.\r\n\r\n2) The mean reductions are also done by dividing the inputs by N and then summing which losing some accuracy in the mantissa (likely not the big issue here).\r\n\r\nThe GPU uses a more numerically stable sum algorithm, so (1) is not a problem there.  And the GPU mean calculation will soon also fix (2).  Using float64 is less of a cost on CPU than GPU, so if accurate reductions are important, the best workaround for now is probably to change the type before the reduction to float64 and after back to float32.  Breaking along multiple dimensions also works if you have a multi-dimensional tensor.", "I did a test on GPU, the test code is as following:\r\n```Python\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n    x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float32)\r\n    y = tf.reduce_mean(x)\r\n    sess = tf.Session()\r\n    res = sess.run(y)\r\n    print(res)\r\n    x = tf.constant(0.6931, shape=[32, 100, 79804], dtype=tf.float64)\r\n    y = tf.reduce_mean(x)\r\n    res = sess.run(y)\r\n    print(res)\r\n```\r\nThe Result is just the same:\r\n```\r\nCUDA_VISIBLE_DEVICES=1 python3 test.py\r\n2017-08-19 11:05:45.390445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties:\r\nname: TITAN Xp\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.582\r\npciBusID 0000:09:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.75GiB\r\n2017-08-19 11:05:45.390474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0\r\n2017-08-19 11:05:45.390479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y\r\n2017-08-19 11:05:45.390495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN Xp, pci bus id: 0000:09:00.0)\r\n0.262788\r\n0.693100000803\r\n```\r\nIs the test right?", "Then my intuition that the problem was due to (1) was wrong - it looks it is due to (2).  This should already be fixed for the GPU internally.", "@wenjunpku @ekelsen \r\nI have met the same problems. tf.reduce_mean() gets different results from numpy.mean() on GPU (float32 and float16). But code with tensorflow + keras gets the right answer.  \r\nHow to get the same results as the numpy.mean() with tensorflow?", "you can try do the mean part by part, just like ```y=tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(x, axis=2), axis=-1))```\r\nwhich fix my problem:)", "I can confirm this is fixed internally on the GPU - it now returns .6931 as expected", "Any updates on this?", "This issue no longer occurs. If you have a different precision problem, file a new issue."]}, {"number": 12386, "title": "program hung when trying to use the result of ConditionalAccumulator's take_grad method after The dequeue operator", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 & Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI tried to extend `SyncReplicasOptimizer` where I need to use the result of `ConditionalAccumulator`'s `take_grad` method after `sync_token_queue.dequeue()`, the program blocked. Then I looked into the code and thought that it might be caused  by  the step state in `ConditionalAccumulator`, but it still blocked.\r\n\r\n### Source code / logs\r\nI could modify the official example a bit to reproduce the problem.\r\nThe cmds I used are as below\uff1a\r\n`python mnist_replica.py --job_name ps --task_index 0 --sync_replicas True`\r\n`python mnist_replica.py --job_name worker --task_index 0 --sync_replicas True`\r\n`python mnist_replica.py --job_name worker --task_index 1 --sync_replicas True`\r\n\r\nWhere `mnist_replica.py` only changed a line of importing module, [link](https://gist.github.com/yaochengji/455b541988920134df4260c954ad4985#file-mnist_replica-py). \r\nAnd I edited `sync_replicas_optimizer.py` a bit, two versions in [link1](https://gist.github.com/yaochengji/455b541988920134df4260c954ad4985#file-my_sync_replicas_optimizer_0-py) and in [link2](https://gist.github.com/yaochengji/455b541988920134df4260c954ad4985#file-my_sync_replicas_optimizer_1-py).", "comments": ["We're trying to move to the `dataset` API which doesn't have these freezing issues. Typically it's either some racing condition or the eval queue not being started.\r\n\r\n@nealwu @mrry @tfboyd do we have plans to convert our examples with the new API? I think that would remove a lot of confusion.", "@drpngx I think the freezing issue is probably in the `tf.train.SyncReplicasOptimizer` rather than the queue-based input pipeline.\r\n\r\n@josh11b and @isaprykin are working on a replacement that shouldn't have the same problems.", "Sorry for the lack of updates, we do not at this time have a direct replacement for SyncReplicaOptimizer. On one machine you can use tf.contrib.distribute.MirroredStrategy to do sync replication across multiple GPUs. Further sync replication options will be coming, but I'm not certain of the time frame.", "Nagging Assignee @josh11b: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 12385, "title": "Slim.learning.train support tf-debug", "body": "tfdbg is a great tool for debugging TensorFlow programs. But if I use slim for training(like in TensorFlow object detection API), it seems not support to use tf-dbg, and brings inconvenience.", "comments": ["cc @sguada\r\n\r\nOne way I can see we solve this problem is by adding a keyword argument, maybe called `tf_debug` or `debug` to the `slim.learning.train()` method. @sguada, please let me know what you think. Thanks.", "I haven't used tf-dgb myself, but if the change is minor and defaults to False it should be ok."]}, {"number": 12384, "title": "tf.train.batch does not preserve the order of data and miss some data.", "body": "I use tf.train.batch to produce data in tensorflow but get unexpected data order as following: \r\n```\r\n# Create a queue that produces the filenames to read.\r\nfilename_queue = tf.train.string_input_producer(string_tensor=filenames, num_epochs=1, shuffle=False)\r\n\r\n......\r\n\r\nnum_preprocess_threads = 1 \r\nimages_x, images_y, label_batch = tf.train.batch(\r\n            [image[0], image[1], label],\r\n            batch_size=1,\r\n            num_threads=num_preprocess_threads,\r\n            capacity=1)\r\n```\r\n\r\nthe label produced by tf.train.batch: [ 6.03125     5.2734375   4.03125   5.84375     5.15625   ....]. A total of 33 labels.\r\n\r\nthe label of original data: [ 6.03125     5.2734375   **4.546875**    4.03125     **6.1875**  5.8438 ...]. A total of 68 labels.\r\n\r\n\r\nIt seems that the generated data differs from the original data in the order and is less. Why?\r\n", "comments": ["seems that I met the same problem", "@qinglintian Any solutions?", "Nope. Actually, I met this problem yesterday and haven't got time to explore further.", "@qinglintian \r\nWhen I delete some specific variables in the parameter fetch of sess.run(), it works as expected. It seems that the computing graph has been evaluated more than one time in a round. ", "@HC-2016 Can you share a complete reproduction for the problem?  Based on your last comment, it sounds like you are fetching tensors from both \"before\" and \"after\" the `tf.train.batch()` in the same `sess.run()` call. This will lead to some input values being consumed before they get to be batched. Unfortunately this kind of error is hard for TensorFlow to detect, since the `sess.run()`  call doesn't know anything about the queue-runner structure.\r\n\r\n(As an aside, the `tf.contrib.data` API might be useful in preventing this kind of bug. It also has better facilities for getting a deterministic execution order, even when shuffling is used.)", "@mrry sorry, I can not share the codes for some reason now. And I think you are right. I have fetched tensors from both \"before\" and \"after\" the tf.train.batch() in the same sess.run() call. Stopping to fetch the tensor before tf.train.batch() indeed solves the problem."]}, {"number": 12383, "title": "Update install_windows.md to mention cuDNN 6", "body": "This PR updates the r1.3 docs, which are currently the default on the website.\r\n\r\nFixes #12366.", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @yifeif, @tensorflower-gardener and @av8ramit to be potential reviewers.", "@av8ramit Amit, is this the best way to submit doc updates for the 1.3 branch? Let me know if not, and I'll submit some other way....", "@mrry this is fine. Thanks! I'll push this soon."]}, {"number": 12382, "title": "using two different models for prediction", "body": "Im using the following code to import graph def since i want to use two model files individually.\r\n\r\n```\r\ndef initialSetup():\r\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\n    start_time = timeit.default_timer()\r\n\r\n    # This takes 2-5 seconds to run\r\n    # Unpersists graph from file\r\n    with tf.gfile.FastGFile('age/output_graph.pb', 'rb') as f:\r\n        age_graph_def = tf.GraphDef()\r\n        age_graph_def.ParseFromString(f.read())\r\n        tf.import_graph_def(age_graph_def, name='')\r\n\r\n    with tf.gfile.FastGFile('output_graph.pb', 'rb') as f:\r\n        gender_graph_def = tf.GraphDef()\r\n        gender_graph_def.ParseFromString(f.read())\r\n        tf.import_graph_def(gender_graph_def, name='')\r\n\r\n    print ('Took {} seconds to unpersist the graph'.format(timeit.default_timer() - start_time))\r\n\r\n```\r\n\r\nbut how do I use them after importing individually ? I tried adding name ='age' and name = 'gender' to differentiate but it is not working out as well. ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@reedwm I have asked but no one is responding. Can you please check [this](https://stackoverflow.com/questions/45747769/using-two-different-models-in-tensorflow)"]}, {"number": 12381, "title": "Limit GPU memory usage not working in distributed training of inception", "body": "I'm using the example provided for distributed trainig of [inception](https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py). I have three hosts, with one for parameter server and two for workers. All hosts have TensorFlow 1.2 installed with CUDA 8.0, cuDNN 5.1 and Titan X Pascal GPU running Ubuntu 14.04.\r\n\r\nI followed the instruction for distributed training as provided in the [readme](https://github.com/tensorflow/models/blob/master/inception/README.md) and it runs successfully. But when I try to limit the GPU usage by making the following changes.\r\n\r\nChange\r\n\r\nsess_config = tf.ConfigProto(\r\n          allow_soft_placement=True,\r\n          log_device_placement=FLAGS.log_device_placement)\r\n\r\nto\r\n\r\nsess_config = tf.ConfigProto(\r\n          allow_soft_placement=True,\r\n          log_device_placement=FLAGS.log_device_placement,\r\n          gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.6))\r\n\r\nin [inception_distributed_train.py](https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py#L255)\r\n\r\nI recompiled the example using 'bazel build //inception:imagenet_distributed_train' and run the example. But the process still occupies all GPU memory available observed using nvidia-smi.", "comments": ["The device options are interpreted somewhat differently in the distributed version of TensorFlow, because they are properties of the [`tf.train.Server`](https://www.tensorflow.org/api_docs/python/tf/train/Server), rather than an individual `tf.Session`. If you pass the same `sess_config` object as the optional `config` arg to the `tf.train.Server` constructor, you should get the desired behavior.\r\n\r\nI'll close this issue for now, but feel free to reopen if it doesn't solve the problem.", "Thank you @mrry \uff0cthat solves the problem.", "Thanks for confirming!"]}, {"number": 12380, "title": " Who can explain the usage of this c++ api", "body": "Who can explain the usage of this api\uff1a\r\nStatus Run(\r\n  const FeedType & inputs,\r\n  const std::vector< Output > & fetch_outputs,\r\n  const std::vector< Operation > & run_outputs,\r\n  std::vector< Tensor > *outputs\r\n) const \r\n\r\n\r\nI read the doc but i still can not understand it ....\r\nI don\u2018t know  the role of each parameter.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12379, "title": "[Feature] InputStream variant for ReadBinaryProto and ReadTextProto", "body": "### System information\r\n\r\nN/A\r\n\r\n### Describe the problem\r\nCurrently in master branch both ReadBinaryProto and ReadTextProto accepts only file path to load model proto, while in some cases we may want to load from a network stream or any other input stream of model proto file that input stream is our best option.\r\n\r\nSo I think adding two variants accepts stream input instead of file path should be good. As both these ReadBinaryProto and ReadTextProto are in C++ public API, may I know is it feasible to add? Or any other security or implementation difficulty that is blocking it? Or design conflict? \r\n\r\nIf it is just lacking of people, sure I can help.\r\n\r\n### Source code / logs\r\n\r\nN/A\r\n", "comments": ["@tatatodd what do you think?", "@resec What is the signature of your proposed API?  I.e. what exactly do you mean by 'input stream'?\r\n\r\nNote that Read{Binary,Text}Proto are pretty short helper functions, and only exist because there are multiple uses in tests and command-line tools, when reading from files.  The proto library already has APIs to perform streaming input, which could be used directly without any TensorFlow-side wrapper.", "@tatatodd actually yesterday I tried to implement this feature, and than I found that I do agree with you that TensorFlow-side wrapper for this feature would be kind of redundant.\r\n\r\nWe can always load the model proto with protobuf lib only nicely. So just feel free to close this, thanks.", "@resec Cool, closing this out."]}, {"number": 12378, "title": "ssd_mobilenets model faild using in android demo", "body": "I am glad to see android demo is updated to be available for ssd_mobilenets models. I tried the demo on my phone, and it works well. But when I tried my own model, the app can not work normally. \r\n1.I used  `export_inference_graph.py` in object_detection to convert .ckpt to .pb files\r\n2.I replaced the original model by my .pb file\r\n3. Also, I changed the label_list.txt. \r\nAfter doing this, I generate the app, but the app crushed. Is there anything wrong with my handle? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12377, "title": "Branch 165646100", "body": "Pushing internal commits", "comments": ["@tensorflow-jenkins Test this, please."]}, {"number": 12376, "title": "Remove tensorboard if we are building tf_nightly.", "body": "Once there are consistent nightly tensorboard builds I'll have the update_version script just update the tensorboard dependency tag. ", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @vrv to be potential reviewers.", "Jenkins, test this please."]}, {"number": 12375, "title": "Eager tensor execution not inferring dtype attributes", "body": "Hi,\r\n\r\nI've noticed that when I use the \"StridedSlice\" op, for example, with the new eager tensor execution API, the type arguments (\"T\" and \"Index\" in this case) are not automatically inferred and the library complains that they are not provided. Also, when I do provide them, I get an error saying `NodeDef mentions attr '`, which I cannot figure out. @alextp maybe you have some idea of what's happening.\r\n\r\nThanks,\r\nAnthony", "comments": ["I have noticed that if I manually set eager op attributes one by one, I get the error whenever I add type attributes. More specifically, I managed to find cases where I get a more complete error message, such as:\r\n\r\n```\r\nNodeDef mentions attr '\u00dd' not in Op<name=Add; signature=x:T, y:T -> z:T; \r\nattr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, \r\nDT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_STRING]>; \r\nNodeDef: Add = Add[\u00dd=DT_FLOAT](). (Check whether your GraphDef-interpreting \r\nbinary is up to date with your GraphDef-generating binary.).\r\n```\r\nNote that when I set the attribute in the op, the name I use is `\"T\"` for the above example. Therefore, something goes wrong, but I cannot figure out what.\r\n\r\n@alextp The only changes I've made to how I compile the TensorFlow dynamic libraries are the following (I had to make them so I could include the eager API in the built dynamic library and use it):\r\n\r\n  1. In the `tensorflow/BUILD` file, add `\"//tensorflow/c/eager:c_api\"` \r\n     to the dependencies of the `\"libtensorflow.so\"` `cc_binary` \r\n     target.\r\n  2. In the `tensorflow/c/exported_symbols.lds` file, add a new line \r\n     containing: `_TFE_*`.\r\n  3. In the `tensorflow/c/eager/BUILD` file, change the visibility of \r\n     the `c_api` `cc_library` and the `runtime` `cc_library` target to \r\n     `\"//visibility:public\"`.\r\n", "First, can you send a pull request with these visibility and BUILD changes?\nThere's no reason not to have them.\n\nSecond, the corrupted attribute name is definitely a sign that something is\nbroken. Can you share a gist or some other way of reproducing this so I can\nattach a debugger and see what's up?\n\nThanks!\n\nOn Thu, Aug 17, 2017 at 8:33 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> I have noticed that if I manually set eager op attributes one by one, I\n> get the error whenever I add type attributes. More specifically, I managed\n> to find cases where I get a more complete error message, such as:\n>\n> NodeDef mentions attr '\u00dd' not in Op<name=Add; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_COMPLEX64, DT_COMPLEX128, DT_STRING]>; NodeDef: Add = Add[\u00dd=DT_FLOAT](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n>\n> Note that when I set the attribute in the op, the name I use is \"T\" for\n> the above example. Therefore, something goes wrong, but I cannot figure out\n> what.\n>\n> @alextp <https://github.com/alextp> The only changes I've made to how I\n> compile the TensorFlow dynamic libraries are the following (I had to make\n> them so I could include the eager API in the built dynamic library and use\n> it):\n>\n>    1. In the tensorflow/BUILD file, add \"//tensorflow/c/eager:c_api\"\n>    to the dependencies of the \"libtensorflow.so\" cc_binary\n>    target.\n>    2. In the tensorflow/c/exported_symbols.lds file, add a new line\n>    containing: _TFE_*.\n>    3. In the tensorflow/c/eager/BUILD file, change the visibility of\n>    the c_api cc_library and the runtime cc_library target to\n>    \"//visibility:public\".\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-323250120>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZzmBMpGfp9CsRa2WkZU3wIgW9uQks5sZQX-gaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp Sounds good! I'll try to get those changes in today. One question I have related to this is whether to use `tf_cuda_library` and `tf_cc_test`, instead of `cc_library` and `cc_test`, as is done for the C API. What do you think?\r\n\r\nRegarding the attribute name, I'll try to get some code that reproduces it in C++ so you can test. One thing I've noticed is that so far it only happens with attributes that have data types as values.", "Follow what the C API does.\n\nTo me this sounds like there's a bug in the java swig code somewhere which\nis confusing enums and string dtype names.\n\nOn Fri, Aug 18, 2017 at 8:35 AM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> @alextp <https://github.com/alextp> Sounds good! I'll try to get those\n> changes in today. One question I have related to this is whether to use\n> tf_cuda_library and tf_cc_test, instead of cc_library and cc_test, as is\n> done for the C API. What do you think?\n>\n> Regarding the attribute name, I'll try to get some code that reproduces it\n> in C++ so you can test. One thing I've noticed is that so far it only\n> happens with attributes that have data types as values.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-323385927>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZ4njNryVgn5HqYJalEwc72l7q9Uks5sZa9QgaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n", "Ok, will do so soon!\r\n\r\nI don't use swig at all. I have my own JNI bindings which work fine for executing ops within sessions outside the eager API. And the code is pretty much the same for my eager API bindings. What strikes me is that it only happens for data type attributes and not for others, while the way I handle attribute names is the same for all types of attributes.", "It could be something as simple as swapping the order of arguments for a\ncall to TFE_OpSetAttrType\n\nOn Fri, Aug 18, 2017 at 8:48 AM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> Ok, will do so soon!\n>\n> I don't use swig at all. I have my own JNI bindings which work fine for\n> executing ops within sessions outside the eager API. And the code is pretty\n> much the same for my eager API bindings. What strikes me is that it only\n> happens for data type attributes and not for others, while the way I handle\n> attribute names is the same for all types of attributes.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-323388712>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxXO-eD2egRcenV49_1YNF6oax8bDks5sZbJHgaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n", "I use this code to generate the bindings for the set attribute functions:\r\n\r\n```\r\n#define DEFINE_SET_ATTR(atype, jtype, ctype)                                            \\\r\n  JNIEXPORT void JNICALL Java_org_platanios_tensorflow_jni_Tensor_00024_eagerSetOpAttr##atype( \\\r\n      JNIEnv* env, jobject object, jlong handle, jstring name, jtype value) {                  \\\r\n    static_assert(                                                                             \\\r\n        sizeof(ctype) >= sizeof(jtype),                                                        \\\r\n        \"Information loss when converting between Java and C types\");                          \\\r\n    TFE_Op* op = require_eager_op_handle(env, handle);                                         \\\r\n    if (op == nullptr) return;                                                                 \\\r\n    const char *c_name = env->GetStringUTFChars(name, nullptr);                                \\\r\n    TFE_OpSetAttr##atype(op, c_name, static_cast<ctype>(value));                               \\\r\n    env->ReleaseStringUTFChars(name, c_name);                                                  \\\r\n  }\r\n\r\nDEFINE_SET_ATTR(Int, Long, jlong, int64_t);\r\nDEFINE_SET_ATTR(Float, Float, jfloat, float);\r\nDEFINE_SET_ATTR(Bool, Boolean, jboolean, unsigned char);\r\nDEFINE_SET_ATTR(Type, Int, jint, TF_DataType);\r\n#undef DEFINE_SET_ATTR\r\n```\r\n\r\nCould the `ReleaseStringUTFChars` method be interfering with the name passed to the set attribute function? Is the attribute name in that function handled differently than the one used in the normal op set attribute functions (i.e., not eager)? I guess those are the questions I'm looking into now. I feel that the name string may be deallocated by the JVM and then the pointer passed to the set attribute function may be invalid. Although, this doesn't seem to happen with the normal op set attribute functions.", "I think (thought not sure) TensorFlow keeps a reference to the attr name\nstring around until TFE_Execute is called. I think that's what you're\nstumbling against.\n\nOn Fri, Aug 18, 2017 at 9:06 AM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> I use this code to generate the bindings for the set attribute functions:\n>\n> #define DEFINE_SET_ATTR(atype, jtype, ctype)                                            \\\n>   JNIEXPORT void JNICALL Java_org_platanios_tensorflow_jni_Tensor_00024_eagerSetOpAttr##atype( \\\n>       JNIEnv* env, jobject object, jlong handle, jstring name, jtype value) {                  \\\n>     static_assert(                                                                             \\\n>         sizeof(ctype) >= sizeof(jtype),                                                        \\\n>         \"Information loss when converting between Java and C types\");                          \\\n>     TFE_Op* op = require_eager_op_handle(env, handle);                                         \\\n>     if (op == nullptr) return;                                                                 \\\n>     const char *c_name = env->GetStringUTFChars(name, nullptr);                                \\\n>     TFE_OpSetAttr##atype(op, c_name, static_cast<ctype>(value));                               \\\n>     env->ReleaseStringUTFChars(name, c_name);                                                  \\\n>   }\n>\n> DEFINE_SET_ATTR(Int, Long, jlong, int64_t);\n> DEFINE_SET_ATTR(Float, Float, jfloat, float);\n> DEFINE_SET_ATTR(Bool, Boolean, jboolean, unsigned char);\n> DEFINE_SET_ATTR(Type, Int, jint, TF_DataType);\n> #undef DEFINE_SET_ATTR\n>\n> Could the ReleaseStringUTFChars method be interfering with the name\n> passed to the set attribute function? Is the attribute name in that\n> function handled differently than the one used in the normal op set\n> attribute functions (i.e., not eager)? I guess those are the questions I'm\n> looking into now. I feel that the name string may be deallocated by the JVM\n> and then the pointer passed to the set attribute function may be invalid.\n> Although, this doesn't seem to happen with the normal op set attribute\n> functions.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-323393167>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxardMJGjOzEt49AttTd-b8DS-D3Nks5sZbaCgaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n", "Actually yeah, if I hardcode `\"T\"` instead of \"c_name\" in the `TFE_OpSetAttr##atype(op, c_name, static_cast<ctype>(value));` line above, it works. So I guess it does have to do with that space being potentially deallocated. In the case of `TF_SetAttrType`, does it not keep a reference to that string?", "In either case, I have an idea for how to deal with that and make it faster altogether so I'll give it a try and update you.", "@alextp Made a pull request for the visibility changes at #12399.", "I figured it out and it is indeed what we have been discussing. It turns out that one big difference between the eager api and the existing op construction api is that in the latter, the calls to the op builder functions can be made independently without having issues if the name of an attribute passed to such a function is reallocated after the function call. In the eager api, the op must be constructed and executed, before any of these arguments are deallocated.\n\nThis is an important distinction that I think we should find a way to document well (maybe we should have a C API design and maybe tutorial page, covering such things). This issue in particular is very important for language bindings where the memory is managed separately. For Java or Scala, for example, one cannot keep memory and deallocate it later, across different calls to a JNI function (or maybe it can be done in a convoluted way that requires one to keep track of multiple deallocators and keep passing them around somehow). This is important because it implies that for the eager api, for example, one has to construct and execute an op, within a single jni call. This forces the user (in order to have reasonable performance) to implement a jni function for each op, separately. In this case, I think this is good because we gain lots of performance (JNI can be incur a large overhead), but it should be documented as some of the errors caused are hard to trace.\n\nWhat are your thoughts on this?\n\nOn Aug 18, 2017, 12:23 PM -0400, Alexandre Passos <notifications@github.com>, wrote:\n> I think (thought not sure) TensorFlow keeps a reference to the attr name\n> string around until TFE_Execute is called. I think that's what you're\n> stumbling against.\n>\n> On Fri, Aug 18, 2017 at 9:06 AM, Anthony Platanios <notifications@github.com\n> > wrote:\n>\n> > I use this code to generate the bindings for the set attribute functions:\n> >\n> > #define DEFINE_SET_ATTR(atype, jtype, ctype) \\\n> > JNIEXPORT void JNICALL Java_org_platanios_tensorflow_jni_Tensor_00024_eagerSetOpAttr##atype( \\\n> > JNIEnv* env, jobject object, jlong handle, jstring name, jtype value) { \\\n> > static_assert( \\\n> > sizeof(ctype) >= sizeof(jtype), \\\n> > \"Information loss when converting between Java and C types\"); \\\n> > TFE_Op* op = require_eager_op_handle(env, handle); \\\n> > if (op == nullptr) return; \\\n> > const char *c_name = env->GetStringUTFChars(name, nullptr); \\\n> > TFE_OpSetAttr##atype(op, c_name, static_cast<ctype>(value)); \\\n> > env->ReleaseStringUTFChars(name, c_name); \\\n> > }\n> >\n> > DEFINE_SET_ATTR(Int, Long, jlong, int64_t);\n> > DEFINE_SET_ATTR(Float, Float, jfloat, float);\n> > DEFINE_SET_ATTR(Bool, Boolean, jboolean, unsigned char);\n> > DEFINE_SET_ATTR(Type, Int, jint, TF_DataType);\n> > #undef DEFINE_SET_ATTR\n> >\n> > Could the ReleaseStringUTFChars method be interfering with the name\n> > passed to the set attribute function? Is the attribute name in that\n> > function handled differently than the one used in the normal op set\n> > attribute functions (i.e., not eager)? I guess those are the questions I'm\n> > looking into now. I feel that the name string may be deallocated by the JVM\n> > and then the pointer passed to the set attribute function may be invalid.\n> > Although, this doesn't seem to happen with the normal op set attribute\n> > functions.\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-323393167>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/AAATxardMJGjOzEt49AttTd-b8DS-D3Nks5sZbaCgaJpZM4O68WV>\n> > .\n> >\n>\n>\n>\n> --\n> - Alex\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "I think generating the JNI code automatically like we do the python and C++\nAPIs is the way to go. But please file an issue about this poor\ndocumentation.\n\nOn Sat, Aug 19, 2017 at 1:41 AM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> I figured it out and it is indeed what we have been discussing. It turns\n> out that one big difference between the eager api and the existing op\n> construction api is that in the latter, the calls to the op builder\n> functions can be made independently without having issues if the name of an\n> attribute passed to such a function is reallocated after the function call.\n> In the eager api, the op must be constructed and executed, before any of\n> these arguments are deallocated.\n>\n> This is an important distinction that I think we should find a way to\n> document well (maybe we should have a C API design and maybe tutorial page,\n> covering such things). This issue in particular is very important for\n> language bindings where the memory is managed separately. For Java or\n> Scala, for example, one cannot keep memory and deallocate it later, across\n> different calls to a JNI function (or maybe it can be done in a convoluted\n> way that requires one to keep track of multiple deallocators and keep\n> passing them around somehow). This is important because it implies that for\n> the eager api, for example, one has to construct and execute an op, within\n> a single jni call. This forces the user (in order to have reasonable\n> performance) to implement a jni function for each op, separately. In this\n> case, I think this is good because we gain lots of performance (JNI can be\n> incur a large overhead), but it should be documented as some of the errors\n> caused are hard to trace.\n>\n> What are your thoughts on this?\n>\n>\n> On Aug 18, 2017, 12:23 PM -0400, Alexandre Passos <\n> notifications@github.com>, wrote:\n> > I think (thought not sure) TensorFlow keeps a reference to the attr name\n> > string around until TFE_Execute is called. I think that's what you're\n> > stumbling against.\n> >\n> > On Fri, Aug 18, 2017 at 9:06 AM, Anthony Platanios <\n> notifications@github.com\n> > > wrote:\n> >\n> > > I use this code to generate the bindings for the set attribute\n> functions:\n> > >\n> > > #define DEFINE_SET_ATTR(atype, jtype, ctype) \\\n> > > JNIEXPORT void JNICALL Java_org_platanios_tensorflow_jni_Tensor_00024_eagerSetOpAttr##atype(\n> \\\n> > > JNIEnv* env, jobject object, jlong handle, jstring name, jtype value)\n> { \\\n> > > static_assert( \\\n> > > sizeof(ctype) >= sizeof(jtype), \\\n> > > \"Information loss when converting between Java and C types\"); \\\n> > > TFE_Op* op = require_eager_op_handle(env, handle); \\\n> > > if (op == nullptr) return; \\\n> > > const char *c_name = env->GetStringUTFChars(name, nullptr); \\\n> > > TFE_OpSetAttr##atype(op, c_name, static_cast<ctype>(value)); \\\n> > > env->ReleaseStringUTFChars(name, c_name); \\\n> > > }\n> > >\n> > > DEFINE_SET_ATTR(Int, Long, jlong, int64_t);\n> > > DEFINE_SET_ATTR(Float, Float, jfloat, float);\n> > > DEFINE_SET_ATTR(Bool, Boolean, jboolean, unsigned char);\n> > > DEFINE_SET_ATTR(Type, Int, jint, TF_DataType);\n> > > #undef DEFINE_SET_ATTR\n> > >\n> > > Could the ReleaseStringUTFChars method be interfering with the name\n> > > passed to the set attribute function? Is the attribute name in that\n> > > function handled differently than the one used in the normal op set\n> > > attribute functions (i.e., not eager)? I guess those are the questions\n> I'm\n> > > looking into now. I feel that the name string may be deallocated by\n> the JVM\n> > > and then the pointer passed to the set attribute function may be\n> invalid.\n> > > Although, this doesn't seem to happen with the normal op set attribute\n> > > functions.\n> > >\n> > > \u2014\n> > > You are receiving this because you were mentioned.\n> > > Reply to this email directly, view it on GitHub\n> > > <https://github.com/tensorflow/tensorflow/issues/\n> 12375#issuecomment-323393167>,\n> > > or mute the thread\n> > > <https://github.com/notifications/unsubscribe-\n> auth/AAATxardMJGjOzEt49AttTd-b8DS-D3Nks5sZbaCgaJpZM4O68WV>\n> > > .\n> > >\n> >\n> >\n> >\n> > --\n> > - Alex\n> > \u2014\n> > You are receiving this because you authored the thread.\n>\n> > Reply to this email directly, view it on GitHub, or mute the thread.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-323502371>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxVA_5yOioaZshYJpPioSTLfmuYNyks5sZnWPgaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp Can this issue be closed?", "@alextp Sounds good! I'll do that. :) Do you plan to add automatic inference for the type attributes or should I assume I need to set them?\r\n\r\n@andydavis1 Yes, this issue can be closed. :)", "Re automatic inference of type attributes this is something we do at the\nother language bindings now, so I think you should add it to the scala\nbindings.\n\nOn Wed, Aug 23, 2017 at 6:20 AM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> @alextp <https://github.com/alextp> Sounds good! I'll do that. :) Do you\n> plan to add automatic inference for the type attributes or should I assume\n> I need to set them?\n>\n> @andydavis1 <https://github.com/andydavis1> Yes, this issue can be\n> closed. :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-324327696>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdrVSlvi1v_9WdgapS1QFGWxieGWks5sbCc7gaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp I'm currently doing that at the Scala side. I was just wondering why it's not done on the C side for the eager API, but it is done for the standard op creation API (i.e., for symbolic execution).", "Oh, I didn't realize that.\n\nOn Wed, Aug 23, 2017 at 1:40 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> @alextp <https://github.com/alextp> I'm currently doing that at the Scala\n> side. I was just wondering why it's not done on the C side for the eager\n> API, but it is done for the standard op creation API (i.e., for symbolic\n> execution).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-324408345>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcpOiEmXsqjbTE7nXNaJ6Rvra9Vpks5sbGQbgaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n", "@alextp Also, a small comment about efficiency: currently eager execution also performs shape inference. Wouldn't it be better to avoid running shape inference completely when executing an op eagerly, since the shape will be known when the execution finishes?", "I believe eager execution is not performing shape inference. If it is, it's\na bug. What made you think that?\n\nOn Wed, Aug 23, 2017 at 3:23 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> @alextp <https://github.com/alextp> Also, a small comment about\n> efficiency: currently eager execution also performs shape inference.\n> Wouldn't it be better to avoid running shape inference completely when\n> executing an op eagerly, since the shape will be known when the execution\n> finishes?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-324436356>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxV3iVDOgccTYS7z77NNbcxvMnlTQks5sbHwrgaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n", "It\u2019s because when I try to use the strided assign op, I get an error at `ValidateStridedSliceOp` which is called during shape inference for that op. It may as well be called from somewhere else that I am not aware of, so there may not be a bug.\n\nOn Aug 23, 2017, 4:13 PM -0400, Alexandre Passos <notifications@github.com>, wrote:\n> I believe eager execution is not performing shape inference. If it is, it's\n> a bug. What made you think that?\n>\n> On Wed, Aug 23, 2017 at 3:23 PM, Anthony Platanios <notifications@github.com\n> > wrote:\n>\n> > @alextp <https://github.com/alextp> Also, a small comment about\n> > efficiency: currently eager execution also performs shape inference.\n> > Wouldn't it be better to avoid running shape inference completely when\n> > executing an op eagerly, since the shape will be known when the execution\n> > finishes?\n> >\n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-324436356>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/AAATxV3iVDOgccTYS7z77NNbcxvMnlTQks5sbHwrgaJpZM4O68WV>\n> > .\n> >\n>\n>\n>\n> --\n> - Alex\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "It's called here:\nhttps://github.com/tensorflow/tensorflow/blob/593dc8e5d65f4db93e8f5fced772abb3531a9752/tensorflow/core/kernels/strided_slice_op.cc#L100\n\nOn Wed, Aug 23, 2017 at 3:08 PM, Anthony Platanios <notifications@github.com\n> wrote:\n\n> It\u2019s because when I try to use the strided assign op, I get an error at\n> `ValidateStridedSliceOp` which is called during shape inference for that\n> op. It may as well be called from somewhere else that I am not aware of, so\n> there may not be a bug.\n>\n> On Aug 23, 2017, 4:13 PM -0400, Alexandre Passos <notifications@github.com>,\n> wrote:\n> > I believe eager execution is not performing shape inference. If it is,\n> it's\n> > a bug. What made you think that?\n> >\n> > On Wed, Aug 23, 2017 at 3:23 PM, Anthony Platanios <\n> notifications@github.com\n> > > wrote:\n> >\n> > > @alextp <https://github.com/alextp> Also, a small comment about\n> > > efficiency: currently eager execution also performs shape inference.\n> > > Wouldn't it be better to avoid running shape inference completely when\n> > > executing an op eagerly, since the shape will be known when the\n> execution\n> > > finishes?\n> > >\n> > > \u2014\n> > > You are receiving this because you were mentioned.\n> > > Reply to this email directly, view it on GitHub\n> > > <https://github.com/tensorflow/tensorflow/issues/\n> 12375#issuecomment-324436356>,\n> > > or mute the thread\n> > > <https://github.com/notifications/unsubscribe-auth/\n> AAATxV3iVDOgccTYS7z77NNbcxvMnlTQks5sbHwrgaJpZM4O68WV>\n> > > .\n> > >\n> >\n> >\n> >\n> > --\n> > - Alex\n> > \u2014\n> > You are receiving this because you authored the thread.\n>\n> > Reply to this email directly, view it on GitHub, or mute the thread.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12375#issuecomment-324475667>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxe05NxWS2xg5SBFomLDFYLcvA2JGks5sbKLBgaJpZM4O68WV>\n> .\n>\n\n\n\n-- \n - Alex\n"]}, {"number": 12374, "title": "Tensor Assignment", "body": "Hi,\r\n\r\nI've been wondering why there are no simple tensor assignment ops. Currently one can only assign values to tensors by creating variables. Why is there no simple assignment op for tensors directly? An example would be a case where we simply zero-out a column of a tensor, for example, at some point in execution. In order to do that now we would either need to create a variable, or a series of slicing and concatenation ops (along with creating a new tensor containing the zero-ed out column).\r\n\r\nThanks!", "comments": ["@alextp This would also be useful in the context of eager execution, for supporting more of the numpy functionality.", "A reason why we don't have assignment ops is that graphs using them tend to not be differentiable.\r\n\r\nAnother reason is that they make it much harder for the compiler to know what kinds of rewrites are safe to do on the graph.\r\n\r\nAnother is that they are really hard to use correctly as a user, as tensorflow has no easy way to say that the old value of a tensor is no longer available.\r\n\r\nFinally, though, note that whenever possible if tensorflow knows that a given tensor is no longer going to be referenced it can and will reuse its memory and perform an in-place update.\r\n\r\nMaybe we need ops which do this kind of operation you want (like zeroing out a column) and make them such that they'll reuse memory if possible, or otherwise return a freshly allocated tensor?", "@alextp I see. I guess you follow a sort of purely functional approach where if your data structures are immutable, you have more well-defined behavior and can optimize more. I was sort of expecting that response and it makes sense. Thanks for making it clear!:)\r\n\r\nI guess the next question would be to figure out in which use cases (potentially other than zeroing out a column), do users want to assign values to tensor slices. My use case came in the context of creating tensors using the new eager tensor execution API. In this case I was trying to pre-allocate the tensor buffer and then I was thinking of using assignment ops to fill it with values. The current approach to do this is to allocate the buffer in host memory, set the values manually, and then create the tensor object. Do you have any better ideas for this?\r\n\r\nI guess one reason for thinking of this as useful functionality was that I was thinking about it in the context of the eager execution API. In this case, we need no notions of graph or rewrites and optimizations and that kind of functionality feels natural.", "I actually think a non-destructive tensor slice assignment (which can return a new tensor) is a great op to add, and we can probably make one by reusing the code of StridedSliceAssign. Can you rewrite this issue to make that clear?\r\n\r\n(though this approach won't work very well for eager yet as eager confuses tensorflow's reference counting a little and doesn't make it clear when a tensor will no longer be needed; we're still working on this and I think there is a solution)", "Yeah that sounds good actually! I'll do that sometime later today. :)", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12373, "title": "tf.contrib.data.Dataset behaves strangely when re-defined.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 on GCP\r\n- **TensorFlow installed from (source or binary)**: pip (implies binary install I think).\r\n- **TensorFlow version (use command below)**: GIT Version: v1.2.0-5-g435cdfc, Version: 1.2.1\r\n- **Python version**: 2.7 and 3.5 (I checked with both)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Run the attached script.\r\n[tf_repro.txt](https://github.com/tensorflow/tensorflow/files/1232707/tf_repro.txt)\r\n\r\n### Describe the problem\r\nIt seems like once we start initializing and using datasets, redefining them even in minor ways leads to errors? I think? This is rather unlike any other error in python. Normally I wouldn't expect redefinitions of python variables to cause problems.\r\n\r\nI've noticed the following:\r\n* Using separate variables for the datasets does not help (regardless of whether the old dataset is kept around or not).\r\n* Creating the datasets first and using them after does work (again regardless of whether separate variables.\r\n\r\nIf this is intended behavior, it might be worth documenting it. It's easy enough to work around I suppose. I don't know enough about TensorFlow internals to know what might be causing it.\r\n\r\n### Source code / logs\r\nHere's the output I get when I run the attached script:\r\n```\r\n2017-08-17 21:56:55.536140: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-17 21:56:55.536187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-17 21:56:55.536194: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-17 21:56:55.536199: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-08-17 21:56:55.536203: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nFirst run works.\r\n2017-08-17 21:57:00.353667: W tensorflow/core/framework/op_kernel.cc:1158] Not found: Function tf_map_func_065aa1a3 is not defined.\r\nSecond run failed...\r\nFunction tf_map_func_065aa1a3 is not defined.\r\n         [[Node: MapDataset_1 = MapDataset[Targuments=[], f=tf_map_func_065aa1a3[], output_shapes=[[]], output_types=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](RepeatDataset_1)]]\r\n\r\nCaused by op 'MapDataset_1', defined at:\r\n  File \"/home/prakhar/scratch/tf_repro.py\", line 22, in <module>\r\n    iterator = ds.make_initializable_iterator()\r\n  File \"/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 396, in make_initializable_iterator\r\n    return Iterator.from_dataset(self, shared_name)\r\n  File \"/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 98, in from_dataset\r\n    initializer = gen_dataset_ops.make_iterator(dataset.make_dataset_resource(),\r\n  File \"/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py\", line 1457, in make_dataset_resource\r\n    output_shapes=nest.flatten(self.output_shapes))\r\n  File \"/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 297, in map_dataset\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/mnt/eph/user-pg-r0033ed652607/mypyenv/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): Function tf_map_func_065aa1a3 is not defined.\r\n         [[Node: MapDataset_1 = MapDataset[Targuments=[], f=tf_map_func_065aa1a3[], output_shapes=[[]], output_types=[DT_INT32], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](RepeatDataset_1)]]\r\n\r\nThird run works.\r\n```\r\n\r\n-- PG", "comments": ["This bug should was fixed by 23caae7408cf6bb961d1bab0c8b819d88ccf5f3c, which is part of TF 1.3. I tested your repro script with the released version of 1.3 and it works as intended.\r\n\r\n(There was previously an issue where it was not possible to define new \"TensorFlow functions\" after the first use of a `tf.Session`. Since defining a new dataset often involves defining new functions, this limited how you could use datasets. Thanks are due to @skye for fixing the underlying issue!)", "You're right. Thanks a lot.\r\n\r\n-- PG."]}, {"number": 12372, "title": "tf.contrib.data.Dataset does not correctly handle nested dictionaries", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n`tf.contrib.data.Dataset` objects do not correctly deal with nested dictionary structures. When using a dataset with a nested dictionary, the inner dictionaries are replaced with the first tensor in that inner dictionary, and following tensors are restored for incorrect keys.\r\n\r\nThis is not an issue with `tf.contrib.framework.nest`, only with datasets, which appear to instead use `tensorflow.contrib.data.python.util.nest`. The particular difference causing the bug appears to be https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/util/nest.py#L279 vs. https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/data/python/util/nest.py#L184\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.data.python.util import nest as data_nest\r\n\r\ntest_value = {\r\n    \"a\": {\r\n        \"aa\": tf.constant(1),\r\n        \"ab\": tf.constant([2,2]),\r\n    },\r\n    \"b\": tf.constant([3, 3, 3]),\r\n}\r\n\r\n\r\nprint tf.contrib.framework.nest.map_structure(lambda t: t.shape, test_value)\r\n# {\r\n#   'a': {\r\n#       'aa': TensorShape([]),\r\n#       'ab': TensorShape([Dimension(2)])\r\n#   },\r\n#   'b': TensorShape([Dimension(3)])\r\n# } <- these are the correct shapes\r\n\r\n\r\nd = tf.contrib.data.Dataset.from_tensors(test_value)\r\nprint d.output_shapes\r\n# {\r\n#   'a': TensorShape([]),\r\n#   'b': TensorShape([Dimension(2)])\r\n# } <- incorrect\r\n\r\nprint data_nest.map_structure(lambda t: t.shape, test_value)\r\n# {\r\n#   'a': TensorShape([]),\r\n#   'b': TensorShape([Dimension(2)])\r\n# } <- incorrect\r\n```\r\n", "comments": ["Thanks for pointing out this bug, and helping to narrow down the cause! We're working on a fix presently."]}, {"number": 12371, "title": "tf.contrib.distributions.NegativeBinomial numerical stability (inf)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI'm working on a negative binomial regression model and ran into `inf`/`nan` values during training. I was able to trace it back to the NegativeBinomial implementation, and specificially when dealing with relatively large probability logits.\r\n\r\nExample to demonstrate the issue:\r\n```python\r\nimport tensorflow as tf\r\n\r\nlogits = tf.constant([1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0])\r\nnb = tf.contrib.distributions.NegativeBinomial(10.0, logits)\r\n\r\nsess = tf.Session()\r\nx = sess.run(nb.log_prob(10.0))\r\nprint(x)\r\n```\r\nResults in\r\n```\r\n[  -4.83159161  -38.70070267  -88.56268311 -137.00408936 -147.99020386\r\n          -inf          -inf          -inf]\r\n```\r\n\r\nLooking at the values before going to `-inf` (somewhere between 16.0 and 17.0) it doesn't seem to approaching any sort of limit, so I figured something else was going on.\r\n\r\nInside the `NegativeBinomial` class, the provides logits are first transformed to real probabilities by passing them through a sigmoid, which is `1.0 / (exp(-x) + 1.0)`. Later, in `_log_unnormalized_prob`, these probabilities go through a `log1p(-x)` function. The problem is that for big values and limited floating point precision, sigmoid will output exactly 1.0, which results in taking the log of -1.0 + 1.0, and the log of 0 yields negative infinity. \r\n\r\nI first tried to fix it by simplifying the combination of functions and getting rid of the sigmoid.\r\n\r\nI replaced ` math_ops.log1p(-self.probs)` with `tf.log(1.0 / (tf.exp(self.logits) + 1.0))`, which is equivalent (`log((1 / (1 + exp(-x)) + 1) = log(1 / (exp(x) + 1))` but gives better numerical stability. For the same script but with higher values (1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0, 70.0, 80.0, 90.0), the output now is:\r\n```\r\n[  -4.8315897   -38.70066452  -88.567276   -138.56636047 -148.56636047\r\n -158.56636047 -188.56636047 -488.56634521 -688.56634521 -788.56634521\r\n          -inf]\r\n```\r\n\r\nThis is better, but it still goes to -inf. Finally, I inspected the raw values of the function I changed, and noticed that it converges to `-x` somewhere after 15.0 as input, so my final solution looks like this:\r\n```python\r\ntf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)\r\n```\r\n\r\nSo ultimately I changed from\r\n```python\r\ndef _log_unnormalized_prob(self, x):\r\n    if self.validate_args:\r\n        x = distribution_util.embed_check_nonnegative_integer_form(x)\r\n    return (self.total_count * math_ops.log1p(-self.probs)\r\n            + x * math_ops.log(self.probs))\r\n```\r\nto\r\n```python\r\ndef _log_unnormalized_prob(self, x):\r\n    if self.validate_args:\r\n        x = distribution_util.embed_check_nonnegative_integer_form(x)\r\n    return (self.total_count\r\n            * tf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)\r\n            + x * math_ops.log(self.probs))\r\n```\r\n\r\nwhich gives the correct probabilities for arbitrarily large probability logits.\r\n\r\nI don't feel like creating a PR since I'm not convinced this is the most elegant solution, and not sure whether this problem might exist at other placed in the code base, for example other distribution types. For now, I made my own copy of BinomialDistribution with this fix included, and I'd like to invite you to verify the issue, check the math and perhaps come up with a better solution.\r\n\r\nThanks.", "comments": ["@jvdillon can you comment?", "I ran into a similar but different issue with large negative logits. In this case, `probs` goes to 0.0 and the last log term of the expression goes to -inf. Similarly to the term I updated, the log converges to `logits`, so this case can be solved with a similar `where` op, like this.\r\n\r\n```python\r\ndef _log_unnormalized_prob(self, x):\r\n    if self.validate_args:\r\n        x = distribution_util.embed_check_nonnegative_integer_form(x)\r\n    return (self.total_count\r\n            * tf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)\r\n            + x * tf.where(self.logits >= -20.0, x=tf.log(self.probs), y=self.logits))\r\n```", "Sorry for just now seeing this. Ill push this change right now.", "That's okay, I'm fine with my own version until this lands.", "Fix submitted using the following logic:\r\n\r\n  def _log_unnormalized_prob(self, x):\r\n    if self.validate_args:\r\n      x = distribution_util.embed_check_nonnegative_integer_form(x)\r\n    return (self.total_count * math_ops.log_sigmoid(-self.logits)\r\n            + x * math_ops.log_sigmoid(self.logits))", "Nice, I knew there had to be a much better way. `log(sigmoid(x))` is just `-softplus(-x)`, and I can see in `softplus_op.h` that it deals with the same underflow / overflow issue I tried to solve here, but of course much more efficiently. Thanks!"]}, {"number": 12370, "title": "Patching the missing nightly GPU build links.", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @tensorflower-gardener to be potential reviewers."]}, {"number": 12369, "title": "Add compression support to RecordInput", "body": "This fix tries to fix the request raised in #12344 so that it is possible to process RecordInput with compressions.\r\n\r\nAn attr of `compression_type` has been added.\r\n\r\nAdditional tests have been created to cover the changes.\r\n\r\nThis fix fixes #12344.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks @drpngx for the review. The current PR utilizes the existing implementation of `io::RecordReader`which allows a compression type as the flag.\r\n\r\nTo add compression to the payload but expose the header might be possible. It may require additional implementations on `io::RecordReader`.\r\n\r\nLet me play with it and see if I could create another PR for that.", "OK, I think that's the same as other behavior in TF, but I'm curious to see what you'll find.\r\n\r\n/CC @skye ", "@yongtang any update? What should we do with this PR?", "@martinwicke Given that the original issue #12344 was to support gzipped tfrecord files, I think it probably make sense to limit the scope of this PR and merge if there is no outstanding issues.\r\n\r\nIn the review @drpngx raised an interesting point of having payload compressed while exposing the header. I haven't spend much time on that though I think it is worth at least exploring.\r\n\r\nMaybe we could move forward to merge with this PR (assuming no other outstanding issue) so that the original issue #12344 could be closed? I could open a separate issue to track the alternative approach. You can assign the new issue to me and I will spend time on it.", "Added a new issue #14342 to cover the payload only compression. ", "I see no API change in this PR which is surprising. Are you sure it does what it's intended to? Or do you just need to update the golden files for the API test?", "Strange, there should be a `compression_type`. Maybe there's some indirection of some kind.", "It looks like the op is only available through [here](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py#L2267), @yongtang can you check?", "@martinwicke @drpngx The C++ kernel ops update is through:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/12369/files#diff-9dd26225c3129302e6f60e2b119f48d2R2478\r\n\r\nand for python, it is through the `__init__()` method of the class `RecordInput` and `TFRecordInput`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/12369/files#diff-156604dc2cdee8c1c0cd03edb1e8a7fdR2155\r\n\r\nhttps://github.com/tensorflow/tensorflow/pull/12369/files#diff-156604dc2cdee8c1c0cd03edb1e8a7fdR2178\r\n\r\nAs it is through class constructor, it is not a python ops like `decode_raw`.\r\n\r\nMaybe that is the reason there is no api compatibility issue?", "Not able to see the build error. Maybe it is an infrastructure failure?", "Ah, there is an API change detected, good.\r\n\r\n```\r\n2017-12-15 18:38:23.754138: I tensorflow/core/api_def/api_test.cc:233] Skipping GenerateBigQueryReaderPartitions\r\ntensorflow/core/api_def/api_test.cc:298: Failure\r\n      Expected: golden_api_defs_str\r\n      Which is: \"op {\\n  graph_op_name: \\\"RecordInput\\\"\\n  out_arg {\\n    name: \\\"records\\\"\\n    description: <<END\\nA tensor of shape [batch_size].\\nEND\\n  }\\n  attr {\\n    name: \\\"file_pattern\\\"\\n    description: <<END\\nGlob pattern for the data files.\\nEND\\n  }\\n  attr {\\n    name: \\\"file_random_seed\\\"\\n    description: <<END\\nRandom seeds used to produce randomized records.\\nEND\\n  }\\n  attr {\\n    name: \\\"file_shuffle_shift_ratio\\\"\\n    description: <<END\\nShifts the list of files after the list is randomly\\nshuffled.\\nEND\\n  }\\n  attr {\\n    name: \\\"file_buffer_size\\\"\\n    description: <<END\\nThe randomization shuffling buffer.\\nEND\\n  }\\n  attr {\\n    name: \\\"file_parallelism\\\"\\n    description: <<END\\nHow many sstables are opened and concurrently iterated over.\\nEND\\n  }\\n  attr {\\n    name: \\\"batch_size\\\"\\n    description: <<END\\nThe batch size.\\nEND\\n  }\\n  summary: \\\"Emits randomized records.\\\"\\n}\\n\"\r\nTo be equal to: new_api_defs_str\r\n      Which is: \"op {\\n  graph_op_name: \\\"RecordInput\\\"\\n  out_arg {\\n    name: \\\"records\\\"\\n    description: <<END\\nA tensor of shape [batch_size].\\nEND\\n  }\\n  attr {\\n    name: \\\"file_pattern\\\"\\n    description: <<END\\nGlob pattern for the data files.\\nEND\\n  }\\n  attr {\\n    name: \\\"file_random_seed\\\"\\n    description: <<END\\nRandom seeds used to produce randomized records.\\nEND\\n  }\\n  attr {\\n    name: \\\"file_shuffle_shift_ratio\\\"\\n    description: <<END\\nShifts the list of files after the list is randomly\\nshuffled.\\nEND\\n  }\\n  attr {\\n    name: \\\"file_buffer_size\\\"\\n    description: <<END\\nThe randomization shuffling buffer.\\nEND\\n  }\\n  attr {\\n    name: \\\"file_parallelism\\\"\\n    description: <<END\\nHow many sstables are opened and concurrently iterated over.\\nEND\\n  }\\n  attr {\\n    name: \\\"batch_size\\\"\\n    description: <<END\\nThe batch size.\\nEND\\n  }\\n  attr {\\n    name: \\\"compression_type\\\"\\n    description: <<END\\nThe type of compression for the file. Currently ZLIB and\\nGZIP are supported. Defaults to none.\\nEND\\n  }\\n  summary: \\\"Emits randomized records.\\\"\\n}\\n\"\r\nWith diff:\r\n@@ +42,11 @@\r\n     description: <<END\r\n The batch size.\r\n+END\r\n+  }\r\n+  attr {\r\n+    name: \\\"compression_type\\\"\r\n+    description: <<END\r\n+The type of compression for the file. Currently ZLIB and\r\n+GZIP are supported. Defaults to none.\r\n END\r\n   }\r\n\r\nTo update golden API files, run tensorflow/core/api_def/update_api_def.sh.\r\n```", "Thanks @drpngx. The PR has been updated and pushed with changes from `tensorflow/core/api_def/update_api_def.sh`.\r\n\r\nMost of the tests passed except `Linux CPU Tests (Python 3)`, which seems to be an infrastructure issue:\r\n```\r\nFATAL: command execution failed\r\njava.nio.channels.ClosedChannelException\r\n\tat org.jenkinsci.remoting.protocol.impl.ChannelApplicationLayer.onReadClosed(ChannelApplicationLayer.java:208)\r\n```\r\n\r\nPlease take a look.\r\n", "Jenkins, test this please.", "Double-checking with @martinwicke for API if it's OK to merge.", "Yes, these classes are not part of the public API at all, so this is good to go. "]}]