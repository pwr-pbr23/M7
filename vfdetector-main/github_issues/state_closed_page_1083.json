[{"number": 20783, "title": "Tensorflow issue with libcublas.so.9.0", "body": "Hi,\r\n\r\nI receive the following error while I want to import tensorflow. The python version is 3.5 and Cuda version is 9.2. \r\n\r\n/usr/bin/python3.5 /home/usr/Desktop/Project1/test.py\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/solale/Desktop/LSTM/lstm/PhasedLSTM-Keras-master/test.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1", "comments": ["1. Latest TensorFlow __supports__ CUDA 8-9.2. CuDNN 6-7, or perhaps earlier versions as well.\r\n2. Each TensorFlow binary only works with the version of CUDA and CuDNN it is built with.\r\n3. Official latest TensorFlow binaries (the one downloadable from pypi or conda) are built with CUDA 9.0, CuDNN 7\r\n4. If you don't like to install a different version of Nvidia software, you can:\r\n(1) Use non-official binaries built by others. e.g.: https://github.com/mind/wheels/releases, https://github.com/hadim/docker-tensorflow-builder#builds\r\n(2) Build the binaries by yourself from source with your version of Nvidia software", "Hi\n\nThanks for following up.\nI could not resolve the issue and the only option which worked for me was\ninstalling Cuda 8!\n\nOn Tue, Jul 31, 2018 at 3:13 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> It has been 14 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20783#issuecomment-409332880>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AMimrKGpyXGwPqJz9IJmc8slFxNB4p1Wks5uMKxdgaJpZM4VPLCC>\n> .\n>\n-- \n\nRegards\n\nSolale Tabarestani\n\n------------------------\n\nPh.D. student, Electrical and Computer Engineering\n\nCenter for Advanced Technology and Education\n\nFlorida International University\n\n10555 West Flagler Street, Room EC 2220\n\nMiami, FL 33174\n\nEmail: staba006@fiu.edu\n\nPhone: (786) 479-6517\n", "Nagging Assignee @poxvoculi: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20782, "title": "Feature requested by issues 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter", "body": "Feature requested by issues 18354: No gradient defined for operation DepthwiseConv2dNativeBackpropFilter", "comments": ["Not super familiar with depthwise convolutions. @chsigg, can you review?", "Assign to Amit since this PR is trying to merge to R1.9", "1.9 has already been released and we are not taking cherrypicks at this time. Please merge to master to make it for 1.10.", "Ok,thanks", "Is there a follow up of this PR?", "Also would really like 2nd order grads for these!", "It looks  functionality of this PR was merged in 6d96cc4d05fd09e5663853a795bcd9a5b01f1732.", "Right, but I'm getting this error on 1.13.1, I've made a workaround by adding this code to `__init__.py` in a project where I needed second order gradients:\r\n\r\n```\r\n@ops.RegisterGradient(\"DepthwiseConv2dNativeBackpropInput\")\r\ndef _DepthwiseConv2DNativeBackpropInputGrad(op, grad):\r\n  \"\"\"The derivatives for depth-wise deconvolution.\r\n  Args:\r\n    op: the depth-wise deconvolution op.\r\n    grad: the tensor representing the gradient w.r.t. the output\r\n  Returns:\r\n    the gradients w.r.t. the input and the filter\r\n  \"\"\"\r\n  return [None,\r\n          nn_ops.depthwise_conv2d_native_backprop_filter(\r\n              grad,\r\n              array_ops.shape(op.inputs[1]),\r\n              op.inputs[2],\r\n              op.get_attr(\"strides\"),\r\n              op.get_attr(\"padding\"),\r\n              data_format=op.get_attr(\"data_format\")),\r\n          nn_ops.depthwise_conv2d_native(\r\n              grad,\r\n              op.inputs[1],\r\n              op.get_attr(\"strides\"),\r\n              op.get_attr(\"padding\"),\r\n              data_format=op.get_attr(\"data_format\"))]\r\n```\r\nI know this makes very little sense, but for some reason this PR, while merged, must have regressed recently, could op renaming for v2 be the culprit?\r\n\r\nThese ops really need tests for 2nd order grads so these regression do not happen so frequently.", "The commit didn't make it into 1.13.1, but will be in the next stable release. The commit was merged in 5da4b304fad211a4191a59f6cba64c5330329ff4, which was merged too late to be in 1.13."]}, {"number": 20781, "title": "Feature request: preserve cycle order of open iterators in tf.data.Dataset.interleave", "body": "I'm trying to train RNNs with truncated BPTT with `tf.data` (a great API by the way!) but got tripped up by [these lines](https://github.com/tensorflow/tensorflow/blob/744cf3d3e06fb63ffa40086766137daedc01a5ba/tensorflow/core/kernels/data/interleave_dataset_op.cc#L190-L195) as I've assumed an exhausted iterator would result in a new element being opened directly at the same position in the cycle (in order to pass around RNN states reliably).\r\n\r\nInstead what seems to be happening is that my sequences are accidentally shifted in in the subsequent `.batch()` call whenever a sequence is done. Could the default be changed so that a new element is consumed directly as long as there are any left, such that consecutive dataset elements can be batched in a more straightforward way for RNN training.\r\n\r\nOr could we have a `tf.contrib.data.batched_interleave` or similar?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.9.0-rc2-271-g8955c28 1.9.0-rc0\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A", "Not sure why @tensorflowbutler auto-assigned @tatatodd but this seems like a more relevant feature request for @mrry according to commit history (https://github.com/tensorflow/tensorflow/commit/1fa7aae058f3d4ee85966163b1b27d21ca209f5f)!", "@carlthome,\r\nSorry for the delayed response. [The lines](https://github.com/tensorflow/tensorflow/blob/744cf3d3e06fb63ffa40086766137daedc01a5ba/tensorflow/core/kernels/data/interleave_dataset_op.cc#L190-L195) you highlighted are no longer present in the [Latest Tensorflow Version (2.5)](https://github.com/tensorflow/tensorflow/blob/c536fdd9de6abcb24922f9cb8d969c043704391a/tensorflow/core/kernels/data/interleave_dataset_op.cc#L184-L195). \r\n\r\nCan you please confirm if we can close this issue? Thanks!", "Yes, let's close and make a new feature request if the need resurfaces."]}, {"number": 20780, "title": "TensorRT inference error: TensorFlow device was not registered", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nlast commit from pull request #20350 \r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.15.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc 5.4.0\r\n- **CUDA/cuDNN version**:\r\nCuda 9.0/cuDNN 7\r\n- **GPU model and memory**:\r\n2 x Tesla P40 (22919MiB)\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nUsing tensorflow.contrib.tensorrt.create_inference_graph results in an error:\r\n`Non-OK-status: GpuIdManager::TfToCudaGpuId(tf_gpu_id, &cuda_gpu_id) status: Not found: TensorFlow device GPU:0 was not registered`\r\nI use custom frozen graph to convert (2 x conv->3 x gru rnn->dense->softmax).\r\nOther Tensorflow scripts (which are not using tensorrt) works correctly with both GPU.\r\n\r\n### Source code / logs\r\n**Source code:**\r\n\r\n```\r\ndef load_graph(frozen_graph_filename):\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n    return graph_def\r\n\r\ngraph_def = load_graph(model_path)\r\n\r\nimport tensorflow.contrib.tensorrt as trt2\r\ntrt_graph = trt2.create_inference_graph(\r\n    input_graph_def=graph_def,\r\n    outputs=['softmax_var/output'],\r\n    max_batch_size=5,\r\n    max_workspace_size_bytes=5 << 25,\r\n    precision_mode='FP16',\r\n    minimum_segment_size=1\r\n)\r\nwith tf.Graph().as_default() as infer_graph:\r\n    tf.import_graph_def(trt_graph, name=\"\")\r\n```\r\n\r\n**Traceback:**\r\n```\r\nINFO:tensorflow:Running against TensorRT version 4.0.1\r\n[INFO 2018-07-13 10:40:36,867 tf_logging.py:115] Running against TensorRT version 4.0.1\r\n2018-07-13 10:40:38.625151: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 2\r\n2018-07-13 10:40:41.120095: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:747] MULTIPLE tensorrt candidate conversion: 428\r\n2018-07-13 10:40:41.120688: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2858] Segment @scope 'rnn/RNN_2/BGRU_2/bw/bw/while/custom_gru_cell/', converted to graph\r\n<...many segment conversion messages...>\r\n2018-07-13 10:41:02.943808: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:711]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-07-13 10:41:02.943813: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:833] Can't identify the cuda device. Running on device 0\r\n2018-07-13 10:41:02.946091: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:846] Engine creation for segment 108, composed of 1 nodes failed: Invalid argument: Output node 'rnn/RNN/BGRU_0/bw/bw/TensorArrayUnstack/strided_slice/stack_2' is weights not tensor. Skipping...\r\n2018-07-13 10:41:02.946118: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:711]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-07-13 10:41:02.946123: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:833] Can't identify the cuda device. Running on device 0\r\n2018-07-13 10:41:02.948399: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:846] Engine creation for segment 109, composed of 1 nodes failed: Invalid argument: Output node 'rnn/RNN/BGRU_0/bw/custom_gru_cell/BatchNorm/moving_mean/read' is weights not tensor. Skipping...\r\n2018-07-13 10:41:02.948426: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:711]  Can't find a GPU device to work with. Please instantiate a session to initialize devices\r\n2018-07-13 10:41:02.948430: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:833] Can't identify the cuda device. Running on device 0\r\n2018-07-13 10:41:02.950709: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:846] Engine creation for segment 110, composed of 1 nodes failed: Invalid argument: Output node 'rnn/RNN_1/BGRU_1/fw/fw/ExpandDims/dim' is weights not tensor. Skipping...\r\n2018-07-13 10:41:02.950719: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:715] Can't determine the device, constructing an allocator at device 0\r\n2018-07-13 10:41:02.950731: F ./tensorflow/core/common_runtime/gpu/gpu_id_utils.h:50] Non-OK-status: GpuIdManager::TfToCudaGpuId(tf_gpu_id, &cuda_gpu_id) status: Not found: TensorFlow device GPU:0 was not registered\r\nAborted (core dumped)\r\n\r\n```", "comments": ["I have also faced the same issue with PR #20350 ", "This is a bug in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensorrt/convert/convert_graph.cc#L731, and will be fixed by #20318. Please wait for that PR to be merged and try again later.\r\n\r\nThanks.", "Nagging Assignee @aaroey: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I tested the script provided by @k-anastasia and it works well.", "@aaroey I still have the same issue with this PR merged. Although not core dumped, it end up with not changing the graph. The log information shows failed in TensorRT engine creation.\r\n\r\n> 2018-08-07 15:19:10.464436: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:814] Can't identify the cuda device. Running on device 0                                                        \r\n> 2018-08-07 15:19:10.469900: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         \r\n> 2018-08-07 15:19:10.470166: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger runtime.cpp (16) - Cuda Error in allocate: 2                                                         \r\n> 2018-08-07 15:19:10.470190: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:827] Engine creation for segment 1, composed of 22 nodes failed: Internal: Failed to build TensorRT engine. Skipping...", "@qinyao-he since it's different from this issue, would you please file a separate issue for that? \r\nThanks."]}, {"number": 20779, "title": "AttributeError: module 'pandas' has no attribute 'rolling_count'", "body": "Hi,\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nNO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Hugh Sierra\r\n- **TensorFlow installed from (source or binary)**: I did pip install tensor flow\r\n- **TensorFlow version (use command below)**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nPlease find below the error am having.\r\n\r\nBuilding the network\r\nTraceback (most recent call last):\r\n  File \"./src/NeirbiLSTM-occupancy.py\", line 49, in <module>\r\n    trainoccupancy.run()\r\n  File \"/Users/jasonachonu/git/SeqHub_Summer2018/src/nblib/trainoccupancy.py\", line 173, in run\r\n    config.cost_type)\r\n  File \"/Users/jasonachonu/git/SeqHub_Summer2018/src/nblib/network.py\", line 34, in train_net\r\n    from tensorflow.contrib import rnn\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 39, in <module>\r\n    from tensorflow.contrib import distributions\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distributions/__init__.py\", line 40, in <module>\r\n    from tensorflow.contrib.distributions.python.ops.estimator import *\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/estimator.py\", line 21, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/__init__.py\", line 95, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/__init__.py\", line 28, in <module>\r\n    from tensorflow.contrib.learn.python.learn import *\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/__init__.py\", line 30, in <module>\r\n    from tensorflow.contrib.learn.python.learn import estimators\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/__init__.py\", line 302, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 35, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 36, in <module>\r\n    from tensorflow.contrib.learn.python.learn.estimators import estimator\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 52, in <module>\r\n    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/__init__.py\", line 26, in <module>\r\n    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data\r\n  File \"/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_io/dask_io.py\", line 33, in <module>\r\n    import dask.dataframe as dd\r\n  File \"/anaconda3/lib/python3.6/site-packages/dask/dataframe/__init__.py\", line 12, in <module>\r\n    from .rolling import (rolling_count, rolling_sum, rolling_mean, rolling_median,\r\n  File \"/anaconda3/lib/python3.6/site-packages/dask/dataframe/rolling.py\", line 200, in <module>\r\n    rolling_count = wrap_rolling(pd.rolling_count, 'count')\r\nAttributeError: module 'pandas' has no attribute 'rolling_count'\r\n\r\n", "comments": ["uninstall and reinstall dask worked for me...", "Thank you chabir, I did as you recommended and it worked.\r\n\r\nRegards.\r\n\r\nOn Fri, Jul 13, 2018 at 11:04 PM, Xavier Capdepon <notifications@github.com>\r\nwrote:\r\n\r\n> uninstall and reinstall dask worked for me...\r\n>\r\n> \u2014\r\n> You are receiving this because you authored the thread.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/20779#issuecomment-404994056>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/Ag8LYoCDe73VOoLmIjnKPgw1Av0pP3tIks5uGV-6gaJpZM4VO2MW>\r\n> .\r\n>\r\n", "Similarly to what happened [here](https://github.com/pyviz/pyviz/issues/56), this might be likely due to the fact that pandas 0.23 no longer has `rolling_x` methods.", "Nagging Assignee @jart: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing as this is no longer relevant"]}, {"number": 20778, "title": "ImportError: cannot import name 'abs'", "body": "Windows 10 ,  CUDA 9 + CUDNN 7,  Python3.6(Anaconda5.2)\r\ninstall the TensorFlow 1.9:\r\n`pip install --upgrade tensorflow-gpu`\r\nwhen I import the TensorFlow in ipyhon:\r\n` from tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name 'abs'`\r\n![image](https://user-images.githubusercontent.com/37954865/42689854-ab49a7c8-86d4-11e8-9a14-43e05d4e3f90.png)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@gunan @case540 @amitpatankar\r\n\r\nI this is a widespread problem we should have seen it in the windows tests, no? Or do we not have Windows pip tests? Maybe we should for release. :/\r\n\r\n", "We do have pip tests on windows, not sure how we missed this.\r\n", "Do we only have pip tests for Bazel builds (release we only look at CMake Windows builds)", "I guess that means patch release time. Only needed for Windows, I guess?", "This issue seems to suggest a fix...\r\nhttps://github.com/tensorflow/probability/issues/46\r\n\r\n1. uninstalling tensorflow\r\n2. uninstalling protobuf\r\n3. reinstalling tensorlfow (which should come along with the correct protobuf version.\r\n\r\nNot sure if these issues have same root cause, but maybe something to try before a patch release", "Nice find! @RyenAng can you check whether that solution works for you? We did upgrade our protobuf dependency, and while it's regrettable that the failure is so obscure, it's somewhat beyond our control.", "@case540 This didn't work for me. I tried this on Windows 7", "We should try to repro this. We'll have to, for any fix anyway. And we\nshould find out which platforms are affected.\n", "I just reproduced the problem on my windows laptop. I am really surprised, because all the python files are there!\r\nI can manually run that exact line (which fails with a very similar issue) and it is found, but it is not found during `import tensorflow`. Also, it looks to be specific to keras.\r\n@annarev is it possible we have another API generation problem?\r\n@case540 did we do anything special for keras API generation?\r\n\r\nI will see why our tests did not catch this. Maybe our unit tests just do `from tensorflow.python.foo import bar` and these all pass?\r\n", "Hm we had a change a couple months ago that removed _impl directory. I don't see _impl directory under\r\nhttps://github.com/tensorflow/tensorflow/tree/r1.9/tensorflow/python/keras\r\nSo, it is strange that an import fails on upgrade.\r\nCould it be that some old file is there after upgrade?", "Could it be an issue similar to https://github.com/pypa/pip/issues/5020?\r\nWe have --ignore-installed in documentation: https://www.tensorflow.org/install/install_windows\r\nAccording to the thread at that link, if this argument was passed some time in the past it could cause files from old versions to stay in site-packages.", "You are exactly right!\r\nOn my personal machine, even when I did \"pip uninstall tensorflow\" it did not fix things.\r\nI had to go into my site-packages directory, and delete all \"tensorflow\" directory manually and then reinstall.\r\nThen it just worked.\r\n\r\nSo, Id say this is a pip bug. the workaround is to purge all TF files your python distribution has. then reinstall TF.", "I've dealt with this problem (in conda env on Ubuntu 18.04):\r\n\r\n```\r\npip uninstall tensorflow protobuf --yes\r\nfind $CONDA_PREFIX -name \"tensorflow\" | xargs -Ipkg rm -rfv pkg\r\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.9.0-cp36-cp36m-linux_x86_64.whl --no-cache-dir\r\n```", "Grrrr pip.\n\nCan we either add a short (python?) snippet to our install docs that people\ncan run to upgrade safely?\n\nAlternatively, could we (going forward) put the python files into a\nversioned directory, and have \"tensorflow\" be a link in site-packages?\nWould that work on Windows? Would it work in pip?\n", "Having links on windows, I am not so sure.\n\nHow about this, without using a symlink?\n`tensorflow/__init__.py` includes only a line that says `from\ntensorflow.v1.10.0 import *`\n\nThen all our versions go to `tensorflow/v1.10.0` (or another version).\nOne downside is with similar pip issues, we will accumulate a ton of files\non users disks, potentially GBs of stale files.\n\n\nOn Sat, Jul 14, 2018 at 7:46 AM Martin Wicke <notifications@github.com>\nwrote:\n\n> Grrrr pip.\n>\n> Can we either add a short (python?) snippet to our install docs that people\n> can run to upgrade safely?\n>\n> Alternatively, could we (going forward) put the python files into a\n> versioned directory, and have \"tensorflow\" be a link in site-packages?\n> Would that work on Windows? Would it work in pip?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20778#issuecomment-405027903>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOREu1gItVIy393mL04rtQuewUNwCks5uGgRDgaJpZM4VOwSJ>\n> .\n>\n", "#Got the same issue\r\nWin 10 + CUDA 9.2  + Anaconda 1.8.7\r\nUpgrated from Tensorflow 1.8 to Tensorflow 1.9\r\nUninstalling/reinstalling doesn't work.\r\n\r\nEven complete remove Tensorflow GPU witn only CPU version left doesn't help. \r\nSeems that this is not just GPU ver. but the general tensorflow/keras api problem. \r\nWhat ver of keras should be installed with this release?\r\n\r\nNow i'm thinking of complete remove an Anaconda to rebuild everything again (((\r\n\r\n\r\nUPD: \r\nFound partial solution for win 10 - Anaconda:\r\n1) run Anaconda under Admin\r\n2) via \"environements\" uninstall Keras\r\n3) Run Anaconda prompt under Admin and pip uninstall tensorflow and then pip uninstall tensorflow-gpu\r\n4) reboot your comp\r\n5) run Anaconda under Admin and via \"environements\" install Keras 2.2.0 and all available keras packages \r\n6) while solving packages, it'll propose you to install all necessary Tensorflow 1.8. packages. \r\n7) Apply. \r\n\r\n Now i have tensorflow GPU devices listed and Keras finally found proper GPU backend \r\nChecked it via monitoring devices usage. \r\n\r\nHope it'll be helpful for someone ", "I don't think that running something under admin could be a solution. Did you try to remove tensorflow from filesystem manually (I mean w/o using pip just all folders in anaconda/env)?", "Running under Admin is important cause Anaconda itself does't allow (at least in my case) \r\nto change the environement with simple user rights\r\n\r\nAnaconda prompt under user also is not allowed to make changes in some folders.\r\n\r\nAlso sometimes people have several different environements (as me,  for example) so, searching every folder for manually deleting becomes a problem itself. \r\n\r\nMy way is totally worked for me. \r\nIt is simple, effective, and looks to be the same for everyone. ", "When I update my python version from 3.6.2 to 3.6.6, I meet this problem.", "pip install --upgrade keras\r\ndid the trick (especially in conda installed tensorflow-gpu)\r\nEnv: Windows 10 , CUDA 9 + CUDNN 7, Python3.6(Anaconda5.2)", "All the solutions above have been tried and none of them worked , I just downgraded to tensorflow 1.8.0 and the problem is solved...:(", "The full solution to this issue is to clean up all TF installation files from your system. Here is how to do that. Run the following first:\r\n```\r\n$ pyhthon\r\n> import tensorflow as tf\r\n> tf.__file__\r\n'path/to/your/python/installation/site-packages/tensorflow/__init__.pyc'\r\n```\r\n\r\nFrom the above command, you need to go and remove the folder\r\n```\r\npath/to/your/python/installation/site-packages/tensorflow\r\n```\r\n\r\nFinally, you need to run\r\n```\r\n$ pip uninstall tensorflow # Also add -gpu if you installed the GPU version\r\n$ pip install tensorflow # Also add -gpu if you installed the GPU version\r\n```\r\nThen it should work.", "@gunan's solution worked for me with WinPython 3.5 x64 , TensorFlow 1.10.0 and keras 2.2.2\r\n", "For me it worked too for my anaconda3 environment!!!\r\nI have deleted \\anaconda3\\envs\\test\\Lib\\site-packages\\tensorflow folder\r\nand run:\r\n`pip uninstall tensorflow`\r\n`pip uninstall tensorflow-gpu`\r\n`conda install tensorflow-gpu`\r\n", "The problem is that you updated the tensorflow but not tensorflow_gpu.\r\n\r\npip install --upgrade tensorflow\r\npip install --upgrade tensorflow-gpu\r\n\r\nThe above-mentioned commands will uninstall and reinstall the tensorflow correctly.\r\nBest wishes.", "In my windows laptop, i get fixed this error by uninstalling tensorflow, remove tensorflow things' files in site-packages and install tensorflow again", "Installing 1.8.0 instead of the (currently) latest 1.11.0 worked on the Udacity AMI (Ubuntu 16.0.4).", "I got this error after \u201cpip3 install --upgrade tensorflow\u201d .", "After I upgraded tensorflow to the latest version 1.12 my keras was still left at 2.1.5 I got this error\r\nSo I switched to another conda environment with TF 1.9 and Keras 2.2.4 and it just worked", "I got this error after upgrade tensorflow. I uninstall tensorflow after that install tensorflow but I cloudn't fix this problem. Finally I uninstall tensorflow and delete tensorflow folder in python site package ( C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python36_64\\Lib\\site-packages ). By the way, The problem is fixed.(OS:Windows 10 ; Using Visual Studio Code)\r\n ", "Hi, I have a similar problem. I am using tf and keras. I found out that do not have some models from keras and decided to install keras in tensorflow environment. After installation, during compilation of the program I've got this  ;\r\n\"from tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name 'abs' \"\r\nSystem: Win10x64, python 3.6, tensorflow 1-12.0, keras 2.2.4-0", "Hi, just had the same problem while installing tensorflow 1.12.0 and keras 2.2.4 on Windows 10. I managed the installations with Anaconda3.\r\n\r\nPROBLEM SOLVED after reading the answers above and doing the following steps:\r\n1. Uninstalled all packages that begin with \"tensorflow\" and \"keras\".\r\n2. Re-installed \"keras\" and installed the dependencies Anaconda offered to install, which include tensorflow.\r\n", "@case540 \r\nyour suggestion worked for me just fine on Windows 10. Thank you!\r\n", "This happened to me on Ubuntu Linux.", "When I have not install xlearn on my Ubuntu, my tensorflow is ok. \r\nI think the error is relation to the package xlearn and tensorflow.", "I also encountered this awful error this afternoon, luckily I made it following the steps:\r\n1. Uninstall everythin about tensorflow in your python env or virtual env(anaconda maybe). The way you check can be: in cmd, input \"python3\", then input \"import tensorflow\", if no module named ... is reported, then you finish this step.\r\n2. Then install tensorflow in gpu version like nothin' happens. follow this official site:   \r\nhttps://anaconda.org/anaconda/tensorflow-gpu  \r\n(input the command \"conda install -c anaconda tensorflow-gpu\"), then if you try again, you may be make through it. good luck.", "Am getting the following error trying to run ChatLearner chatbot. Not sure where to get the 'abs' for keras?  How to fix this any ideas? \r\n\r\nfrom tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name 'abs'", "try uninstalling all dependencies of keras(including some base), and reinstall the keras(pip install). My successful running version of keras is 2.2.4. Plus, I suppose my fault is caused from the version conflict of keras base file and some of its dependencies.\n\n\n| |\n\u738b\u5c24\u5609\n|\n|\n15640162499@163.com\n|\n\u7b7e\u540d\u7531\u7f51\u6613\u90ae\u7bb1\u5927\u5e08\u5b9a\u5236\n\n\nOn 03/21/2019 10:43\uff0cBrian Vogl<notifications@github.com> wrote\uff1a\n\nAm getting the following error trying to run ChatLearner chatbot. Not sure where to get the 'abs' for keras? How to fix this any ideas?\n\nfrom tensorflow.python.keras._impl.keras.backend import abs\nImportError: cannot import name 'abs'\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.", "This happend to me on ubuntu 16.04", "It happens on win10. I tried to uninstall tensorflow and reinstall it but this doesn't work.", "When i got the error on win10 it helped to uninstall tensorflow and then install version 1.8. this removed the error. I could then without any problem update to version 1.12.0 and 1.13.1", "Had the same issue with Ubuntu 18.04 when trying to install 2.0.0-beta1\r\nfixed by uninstalling tensorflow, removing the remaining tensorflow folder from site-packages and reinstalling.", "start a new virtualenv without side package  and reinstall tensorflow,will solve this problem without wasting your time.", "What I did on my Ubuntu container was this, and it worked!\r\n```\r\npip3 uninstall tensorflow\r\npip3 uninstall protobuf\r\npip3 install https://github.com/lakshayg/tensorflow-build/releases/download/tf1.13.1-ubuntu16.04-py3/tensorflow-1.13.1-cp35-cp35m-linux_x86_64.whl\r\n```", "> All the solutions above have been tried and none of them worked , I just downgraded to tensorflow 1.8.0 and the problem is solved...:(\r\n\r\nYou just saved my life. Thanks for this \ud83d\ude02 "]}, {"number": 20777, "title": " Fix an broken URL in security/index.md", "body": " Fix an broken URL in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/index.md", "comments": ["Hi @frankchn this pull request fixed the invalid url format in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/security/index.md\r\nWould you mind help to review it?", "This looks like it is already fixed in https://github.com/tensorflow/tensorflow/pull/20760"]}, {"number": 20776, "title": "bug about boosted_trees", "body": "In tensorflow 1.9 , win10, python36,  I run the example \"tensorflow-master\\tensorflow\\contrib\\boosted_trees\\examples\\boston.py\", report :\r\n\r\n  File \"<ipython-input-1-0c2875deaba9>\", line 6, in <module>\r\n    from tensorflow.contrib.boosted_trees.estimator_batch import custom_export_strategy\r\n\r\n  File \"D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\boosted_trees\\estimator_batch\\custom_export_strategy.py\", line 25, in <module>\r\n    from tensorflow.contrib.boosted_trees.python.training.functions import gbdt_batch\r\n\r\n  File \"D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\boosted_trees\\python\\training\\functions\\gbdt_batch.py\", line 26, in <module>\r\n    from tensorflow.contrib.boosted_trees.lib.learner.batch import categorical_split_handler\r\n\r\nModuleNotFoundError: No module named 'tensorflow.contrib.boosted_trees.lib'\r\n++++++++++++++++++++++++++++++++++++++++++++++\r\nAnd when use tensorflow 1.8 ,python 36 ,centos5 or win10, run the same example \"tensorflow-master\\tensorflow\\contrib\\boosted_trees\\examples\\boston.py\", report :\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 225, in run\r\n    return _execute_schedule(experiment, schedule)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 52, in _execute_schedule\r\n    return task()\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 666, in train_and_evaluate\r\n    self.train(delay_secs=0)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 389, in train\r\n    saving_listeners=self._saving_listeners)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 879, in _call_train\r\n    input_fn=input_fn, steps=steps, max_steps=max_steps, monitors=hooks)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 524, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1080, in _train_model\r\n    scaffold=scaffold)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py\", line 421, in __init__\r\n    self._save_path = os.path.join(checkpoint_dir, checkpoint_basename)\r\n  File \"/usr/local/python36/lib/python3.6/posixpath.py\", line 92, in join\r\n    genericpath._check_arg_types('join', a, *p)\r\n  File \"/usr/local/python36/lib/python3.6/genericpath.py\", line 151, in _check_arg_types\r\n    raise TypeError(\"Can't mix strings and bytes in path components\") from None\r\nTypeError: Can't mix strings and bytes in path components\r\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\nand  I run other example again ,\"tensorflow-master\\tensorflow\\contrib\\boosted_trees\\examples\\mnist.py\" ,report:\r\n\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 225, in run\r\n    return _execute_schedule(experiment, schedule)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\", line 52, in _execute_schedule\r\n    return task()\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 666, in train_and_evaluate\r\n    self.train(delay_secs=0)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 389, in train\r\n    saving_listeners=self._saving_listeners)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\", line 879, in _call_train\r\n    input_fn=input_fn, steps=steps, max_steps=max_steps, monitors=hooks)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 524, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1041, in _train_model\r\n    model_fn_ops = self._get_train_ops(features, labels)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1264, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1227, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py\", line 116, in model_builder\r\n    logits=logits)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1085, in create_model_fn_ops\r\n    enable_centered_bias=self._enable_centered_bias)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 669, in _create_model_fn_ops\r\n    batch_size, loss_fn, weight_tensor)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1944, in _train_op\r\n    train_op = train_op_fn(loss)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py\", line 105, in _train_op_fn\r\n    update_op = gbdt_model.train(loss, predictions_dict, labels)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py\", line 696, in train\r\n    control_flow_ops.no_op))\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 432, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2063, in cond\r\n    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1913, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py\", line 932, in _update_bias_stats\r\n    ensemble_stamp, partition_ids, feature_ids, grads_sum, hess_sum)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/stats_accumulator_ops.py\", line 117, in add\r\n    partition_ids, feature_ids, gradients, hessians))\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/stats_accumulator_ops.py\", line 154, in _make_summary\r\n    partition_ids, feature_ids, gradients, hessians)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/gen_stats_accumulator_ops.py\", line 1232, in stats_accumulator_tensor_make_summary\r\n    name=name)\r\n  File \"/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 533, in _apply_op_helper\r\n    (prefix, dtypes.as_dtype(input_arg.type).name))\r\nTypeError: Input 'partition_ids' of 'StatsAccumulatorTensorMakeSummary' Op has type int64 that does not match expected type of int32.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Added a PR #20807 for the fix in boston.py."]}, {"number": 20775, "title": "tf1.2 windows dll error", "body": "\u4e25\u91cd\u6027\t\u4ee3\u7801\t\u8bf4\u660e\t\u9879\u76ee\t\u6587\u4ef6\t\u884c\t\u7981\u6b62\u663e\u793a\u72b6\u6001\r\n\u9519\u8bef\tLNK2001\t\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: void __cdecl google::protobuf::internal::LogFinisher::operator=(class google::protobuf::internal::LogMessage &)\" (??4LogFinisher@internal@protobuf@google@@QEAAXAEAVLogMessage@123@@Z)\twin_trainer\tH:\\2018\\001_TFopencv\\002_demo\\TF1.2\\DemoTF1.2\\win_trainer\\example_trainer.obj\t1\t\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@craigtao  Thank you for reporting the error. Please try using a more recent version of TensorFlow and see if it helps.", "if you use Anaconda, you can use  `conda install -c hesi_m tensorflow ` to install tensorflow 1.9.0 for CPU \r\n\r\nif you specifically use Keras also, you can use  `conda install -c hesi_m keras` which installs Keras 2.2.0 in accompany with everything including tensorflow 1.9.0\r\n\r\nIt is better to make a fresh anaconda environment for your experiments", "my project just need tf1.2 \uff0c its enough,", "1.2 is an old version and might have bugs that have been fixed since then. If you see erroneous behavior in the latest version please let us know and reopen the issue. Please also provide full details about your system  using the standard template. "]}, {"number": 20774, "title": "[tftrt update]", "body": "  Added python tests for converter functions\r\n  Added BUILD for python tests", "comments": ["@aaroey Here's the unit testing scripts. I was using yapf with {google style + indent_width: 2}\r\nAlso a reminder that this scripts depends on functionalities provided from PR #20350 ", "Thanks @jjsjann123, as mentioned in #20350 I'll merge this with tf_trt_integration_test.py"]}, {"number": 20772, "title": "Fix the gradient update in the word2vec example", "body": "The gradient update should be `x <- x - alpha * gradient`, not `x <- x + alpha * gradient`.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 20771, "title": "Where is the nightly-android ?", "body": "--------------------\r\n\r\n### Describe the problem\r\nThe link https://ci.tensorflow.org/view/Nightly/job/nightly-android/ (prebuilt android libraries) is broken but is still used in various files in documentation :\r\n- https://www.tensorflow.org/mobile/android_build#android_inference_library\r\n- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/README.md\r\n\r\nI use it for my app, but obviously I can't download the new versions. \r\n\r\nIs there a new link from which we could retrieve built binaries for Android ? \r\nWhy has the nightly-android stopped running and will it restart eventually ?\r\nCan someone fix the documentation files ?\r\n\r\nThanks. \r\n\r\n### UPDATE\r\nHave I written custom code N/A\r\nOS Platform and Distribution N/A\r\nTensorFlow installed from N/A\r\nTensorFlow version N/A\r\nBazel version N/A\r\nCUDA/cuDNN version N/A\r\nGPU model and memory N/A\r\nExact command to reproduce N/A\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "https://ci.tensorflow.org/view/Nightly/job/nightly-android/ this link is not working and mentioned everywhere is there any other link to get pre built binaries\r\n", "@gunan Can you comment on this? Where do these files live now?", "ci.tensorflow.org is deprecated.\r\nWe are working with tf mobile team to get the android builds back to healthy state and export the artifacts.", "@gunan this process is really slow kindly provide an alternative solution for this\r\n", "you could use the aar package from here:\r\nhttps://bintray.com/google/tensorflow/tensorflow/1.13.0-rc0", "the aar package is for maven. I am building my app in gradle through studio, is there an alternate link to the aar package ?", "This is now TF Lite?"]}, {"number": 20770, "title": "CUDA 9.2 + cuDNN 7.1.4 Support for Windows", "body": "I installed CUDA 9.2 and cuDNN 7xx on windows 10. when I import tensorflow in python, it says that tensorflow works only with CUDA 9.0, so when wil the support for CUDA 9.2 come.", "comments": ["prebuilt binaries has been built with 9.0, you need to build tensorflow from source by yourself", "As @alanpurple mentioned, and as documented at https://www.tensorflow.org/install/install_windows\r\nOur prebuilt binaries are built for cuda 9.0.\r\nThe sources can run on cuda 9.2, but you just need to build from sources tourself.\r\nAccording to our discussions, we will probably jump straight to cuda 10."]}, {"number": 20769, "title": "Configure Tensorflow on GPU using Java API", "body": "Hello,\r\n\r\nHow to set following config \"gpu_options.allow_growth = True\" for Tensorflow, using Java API?\r\nI tried using this code but it doesn't work:\r\n\r\n```\r\n        ConfigProto config = ConfigProto.newBuilder()\r\n                  .setGpuOptions(GPUOptions.newBuilder()\r\n                                         .setAllowGrowth(true)\r\n                                         .setPerProcessGpuMemoryFraction(0.04)\r\n                                         .build()\r\n                                        ).build();\r\n           model.session().runner()\r\n                    .setOptions(config.toByteArray())\r\n                    .feed(\"image_tensor\", input).fetch(\"detection_scores\")\r\n                    .fetch(\"detection_classes\").fetch(\"detection_boxes\").fetch(\"num_detections\").run();\r\n```\r\n\r\nI know the method setOption is an experimental method. \r\nIs there another away to achieve it?\r\n\r\nThanks", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Could you elaborate on what you mean by \"it does not work\". Can you provide a [minimal, complete, verifiable example](https://stackoverflow.com/help/mcve) to reproduce the problem?\r\n\r\nThanks", "1) Used this code [code](https://github.com/tensorflow/models/tree/master/samples/languages/java/object_detection),\r\n2) Coco [Models ](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for Inference: faster_rcnn_nas\r\n2) I ran in AWS using AWS EC2, type p2.xlarge, with Tesla K80,\r\n3) I chose a Linux instance for Deep Learning,\r\n4) Tensorflow GPU version: 1.8.0 and 1.9 rc\r\n5) Java 8\r\n\r\n The process reserve the whole memory but the actual utilization is under 40% - 50%. I used [nvidia-smi queries](http://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries) for monitoring the GPU.", "Oh, I see.\r\n\r\n`Session.runner().setOptions()` is supposed to be provided with a [`RunOptions`](http://static.javadoc.io/org.tensorflow/proto/1.9.0-rc2/org/tensorflow/framework/RunOptions.html) protocol buffer (see [javadoc](https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/Session.Runner.html#setOptions(byte[]))), not a `ConfigProto` message.\r\n\r\nThe `ConfigProto` message needs to be provided when creating the `Session` object and currently `SavedModelBundle.load()` doesn't make this possible.  PR #18716 was supposed to make this possible, but it seems like the PR author didn't get around to updating the PR to fit into the master branch. I can try to fix that up.\r\n\r\nIdeally, this error would have been caught by the compiler if the API accepted Java objects for the protocol buffer messages instead of a `byte[]` of serialized data. However, in an effort to ensure that\r\nthis lowest layer of the Java API has no dependencies, we avoid a dependency on the Java protocol buffer libraries, which comes at the cost of this confusion (since the serialized `ConfigProto` message provided can be parsed into a `RunOptions` message without error).\r\n\r\nI'll make a PR to allow a serialized `ConfigProto` to be provided when loading a `SavedModelBundle` which would then address your use case.", "Thank you!", " Hello,\nHow can I test it?Can I use this version 1.9.0-rc2?\nThank youAdrian    On Saturday, July 14, 2018, 12:28:05 AM GMT+3, Asim Shankar <notifications@github.com> wrote:  \n \n \nOh, I see.\n\nSession.runner().setOptions() is supposed to be provided with a RunOptions protocol buffer (see javadoc), not a ConfigProto message.\n\nThe ConfigProto message needs to be provided when creating the Session object and currently SavedModelBundle.load() doesn't make this possible. PR #18716 was supposed to make this possible, but it seems like the PR author didn't get around to updating the PR to fit into the master branch. I can try to fix that up.\n\nIdeally, this error would have been caught by the compiler if the API accepted Java objects for the protocol buffer messages instead of a byte[] of serialized data. However, in an effort to ensure that\nthis lowest layer of the Java API has no dependencies, we avoid a dependency on the Java protocol buffer libraries, which comes at the cost of this confusion (since the serialized ConfigProto message provided can be parsed into a RunOptions message without error).\n\nI'll make a PR to allow a serialized ConfigProto to be provided when loading a SavedModelBundle which would then address your use case.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n   ", "@AdrianXIA : You'll have to build from source. This change isn't included in 1.9 or 1.10 (whose branch was cut just before the change was merged).\r\n\r\nInstructions for building from source: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java#building-from-source", "Hello,\r\n\r\nI actually built from source the version with the Loader in SavedModelBundle.\r\nHowever it seems that using SavedModelBundle.withConfigProto(config) doesn't impact anything compared to the \"default\" configuration.\r\nCode is below :\r\n```\r\nConfigProto configProto = ConfigProto.newBuilder()\r\n                .setAllowSoftPlacement(true) // allow less GPUs than configured\r\n                .setGpuOptions(GPUOptions.newBuilder().setPerProcessGpuMemoryFraction(0.01).build())\r\n                .build();\r\n\r\nSavedModelBundle  bundle = SavedModelBundle.loader(modelDir).withTags(\"serve\").withConfigProto(configProto.toByteArray()).load();\r\n```\r\n\r\nAm I missing something ?", "@DutAlex : Just to confirm: When you say there is \"no impact\" you mean that you see all of the GPU memory being allocated by the JVM process (as reported by `nvidia-smi`)? Could you post some more detail on how you're running the program and observing the change?", "@asimshankar Yes, all the GPU memory is allocated to the JVM process (using `nvidia-smi` ). To run the program, I am building the .jar file using `mvn package` and running it with using `java -cp jar-file-with-dependencies.jar`.\r\n\r\nIn the main file, I am loading a model and open a file where I make prediction on 10000 inputs (1-by-1). I am using `System.nanoTime()` to profile how long did the prediction of those 10000 examples take.\r\nI see no difference in execution time whether I am setting the GPUOptions (`GPUOptions.newBuilder().setPerProcessGpuMemoryFraction(0.01).build()`) or using the historical version of the SavedModelBundle (`SavedModelBundle.load(modelDir, \"serve\")`).\r\nIn addition to that, I am also profiling my GPU activity every second while the model is making predictions (using `gpustat`) and see that its activity during the predictions is around ~65% whether I am setting the GPUOptions or not.", "@DutAlex : There seem to be two things in your comment\r\n\r\n1. Even after setting `per_process_gpu_memory_fraction`, all GPU memory is being allocated.\r\n2. Setting that option doesn't change the time it takes to run predictions.\r\n\r\nI don't quite see why setting the memory fraction option should change the latency of predictions. The 65% utilization of GPU cores is not something this option would address. Most likely there is some other limitation (in the model) that precludes better GPU utilization.\r\n\r\nSo, of 1. and 2., I think only 1. is concerning here :)\r\nCould you create a [minimal, verifiable, complete](https://stackoverflow.com/help/mcve) example to help reproduce the problem?\r\n\r\nFor example, I tried the following  - https://gist.github.com/asimshankar/2d65be7ef53f1888cf7dc93758a565c9 - which suggests to me that the change is working (in that it is respecting options provided to the `ConfigProto` message).\r\n\r\nIn particular, note that without the `setPerProcessGpuMemoryFraction()` line, `nvidia-smi` shows that the JVM process has allocated all ~12GB of GPU memory:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0    241863      C   /usr/lib/jvm/java-8-openjdk-amd64/bin/java 11615MiB |\r\n\r\n```\r\n\r\nbut with that line, it allocates ~370MB:\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0    241613      C   /usr/lib/jvm/java-8-openjdk-amd64/bin/java   371MiB |\r\n\r\n```\r\n\r\nA way to reproduce the problem will be helpful, otherwise it may be hard to figure out what's going on in your particular setup. Thanks!", "@asimshankar I managed to reproduced the results of your gist. I found out that I was using a wrong version of the tensorflow-jni library... . It actually works well now.\r\n\r\nThank you for your help.\r\nLooking forward to see the version in a stable release !"]}, {"number": 20768, "title": "Fix tf.Print summarized format bug", "body": "This fix fixes the issue raised in #20751 where tf.Print may miss `...` at the end with `summarize`.\r\n\r\nBelow is the output after the fix:\r\n```\r\n$ python\r\nPython 2.7.12 (default, Dec  4 2017, 14:50:18)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> v = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\r\n>>> a = tf.Print(v, [v], message=\"summarize(3): \")\r\n>>> b = tf.Print(v, [v], message=\"summarize(10): \", summarize=10)\r\n>>> sess = tf.Session()\r\n>>> sess.run(a)\r\nsummarize(3): [[1 2 3...]...]\r\narray([[ 1,  2,  3,  4],\r\n       [ 5,  6,  7,  8],\r\n       [ 9, 10, 11, 12]], dtype=int32)\r\n>>> sess.run(b)\r\nsummarize(10): [[1 2 3 4][5 6 7 8][9 10...]...]\r\narray([[ 1,  2,  3,  4],\r\n       [ 5,  6,  7,  8],\r\n       [ 9, 10, 11, 12]], dtype=int32)\r\n```\r\n\r\nThis fix fixes #20751\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Please also fix the failed test cases.", "@benoitsteiner @qlzh727 The test cases have been fixed. Please take a look.", "Nagging Reviewer @benoitsteiner: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 20767, "title": "How to access the tensorflow model directly from PC?", "body": "The main issue I am facing is that I am not able to access to object_detection_models even though it has been saved offline. I have described my problem in the comment below:\r\nThe main error I am having is this:\r\n**The filename, directory name, or volume label syntax is incorrect.**", "comments": ["Here is the problem I am facing... I found object detection script in jupyter notebook and made necessary changes to read objects in real time and form object detection. But the problem I am facing is each time I run the script I have to download the model again and again. Here is the code:\r\n`import numpy as np\r\nimport os\r\nimport six.moves.urllib as urllib\r\nimport sys\r\nimport tarfile\r\nimport tensorflow as tf\r\nimport zipfile\r\n\r\nfrom collections import defaultdict\r\nfrom io import StringIO\r\n#from matplotlib import pyplot as plt\r\nfrom PIL import Image\r\nprint('Stage-1 Done')\r\n##################################################################\r\nimport cv2\r\ncap = cv2.VideoCapture(0)\r\nsys.path.append(\"..\")\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\nprint('Stage-2 Done')\r\n##################################################################\r\nMODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'\r\nMODEL_FILE = MODEL_NAME + '.tar.gz'\r\nDOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\r\n\r\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\r\nPATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\r\n\r\n# List of the strings that is used to add correct label for each box.\r\nPATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\r\n\r\nNUM_CLASSES = 90\r\nprint('Stage-3 Done, NUM_CLASSES = %d'%NUM_CLASSES)\r\n################################################################\r\nopener = urllib.request.URLopener()\r\nopener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\r\ntar_file = tarfile.open(MODEL_FILE)\r\nfor file in tar_file.getmembers():\r\n  file_name = os.path.basename(file.name)\r\n  if 'frozen_inference_graph.pb' in file_name:\r\n    tar_file.extract(file, os.getcwd())\r\nprint('Stage 4 Done')\r\n##############################################################\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n  od_graph_def = tf.GraphDef()\r\n  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n    serialized_graph = fid.read()\r\n    od_graph_def.ParseFromString(serialized_graph)\r\n    tf.import_graph_def(od_graph_def, name='')\r\n\r\n\r\n# ## Loading label map\r\n# Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine\r\n\r\n# In[7]:\r\n\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\nprint('Almost done! Stage 5 also completed...')\r\n##################################################################\r\n# In[8]:\r\n\r\ndef load_image_into_numpy_array(image):\r\n  (im_width, im_height) = image.size\r\n  return np.array(image.getdata()).reshape(\r\n      (im_height, im_width, 3)).astype(np.uint8)\r\n\r\n\r\n# # Detection\r\n\r\n# In[9]:\r\n\r\n# For the sake of simplicity we will use only 2 images:\r\n# image1.jpg\r\n# image2.jpg\r\n# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\r\nPATH_TO_TEST_IMAGES_DIR = 'test_images'\r\nTEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\r\n\r\n# Size, in inches, of the output images.\r\nIMAGE_SIZE = (12, 8)\r\n\r\n\r\n# In[10]:\r\nprint('And now we start it... Enjoy!!!')\r\nwith detection_graph.as_default():\r\n  with tf.Session(graph=detection_graph) as sess:\r\n    while True:\r\n      ret, image_np = cap.read()\r\n      print(ret)\r\n      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\r\n      image_np_expanded = np.expand_dims(image_np, axis=0)\r\n      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n      # Each box represents a part of the image where a particular object was detected.\r\n      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n      # Each score represent how level of confidence for each of the objects.\r\n      # Score is shown on the result image, together with the class label.\r\n      scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\n      classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n      num_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n      # Actual detection.\r\n      (boxes, scores, classes, num_detections) = sess.run(\r\n          [boxes, scores, classes, num_detections],\r\n          feed_dict={image_tensor: image_np_expanded})\r\n      # Visualization of the results of a detection.\r\n      vis_util.visualize_boxes_and_labels_on_image_array(\r\n          image_np,\r\n          np.squeeze(boxes),\r\n          np.squeeze(classes).astype(np.int32),\r\n          np.squeeze(scores),\r\n          category_index,\r\n          use_normalized_coordinates=True,\r\n          line_thickness=8)\r\n\r\n      cv2.imshow('object detection', cv2.resize(image_np, (800,600)))\r\n      if cv2.waitKey(25) & 0xFF == ord('q'):\r\n        cv2.destroyAllWindows()\r\n        break\r\n\r\n`\r\nNow the changes I made was removed all the tar downloading part and instead of path_to_ckpt i added the path where my frozen_inference_graph.pb is stored. Its throwing the following error.\r\nShould I use os.path.append() ? because I am already using it to import files from object_detection folder.. also I have downloaded model from here: https://github.com/tensorflow/models\r\n\r\nAnd here is the error I get:\r\nTraceback (most recent call last):\r\n  File \"E:\\SAHIL\\RW\\project_earth\\Computer_vision_object_detection\\primary_image\\models\\research\\object_detection\\obdet.py\", line 51, in <module>\r\n    serialized_graph = fid.read()\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 125, in read\r\n    self._preread_check()\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 85, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512, status)\r\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 519, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NewRandomAccessFile failed to Create/Open: E:\\SAHIL\\RW\\project_earth\\Computer_vision_object_detection\\primary_image\\models\r\nesearch\\object_detection\\ssd_mobilenet_v1_coco_11_06_2017\frozen_inference_graph.pb : The filename, directory name, or volume label syntax is incorrect.\r\n\r\n; Unknown error", "Okay, I got the solution... Instead of directly using\r\npath='path_to_pb_file'\r\nuse\r\npath=r'path_to_pb_file'\r\nthat pretty much solved the issue easily.", "> path='path_to_pb_file'\r\n\r\ni did not get it. can you say it again please\r\n"]}, {"number": 20766, "title": "Python 3.7 fixes", "body": "### Reserved word: ```async```\r\n\"async\" became a reserved word in Python 3.7. I replaced with \"is_async\", which was already used for such flags in other places. An alternative suggestion, if the ```is_``` construct is deemed too ugly, might be to use the full term, \"asynchronous\".\r\n\r\n### Python C API changes\r\n\r\nPython C API now returns ```const``` types for `PyUnicode_AsUTF8AndSize``` and similar. Fixed analogous to https://github.com/google/protobuf/commit/0a59054c30e4f0ba10f10acfc1d7f3814c63e1a7.\r\n\r\nFixes #20690", "comments": ["This change only changes C++ files and one python comment to not use a python keyword. So nothing in here is necessary, so I'm closing this PR.", "@alextp - We'd still want to address the change in the return type of `PyUnicode_AsUTF8AndSize` in 3.7. So reopening the PR.\r\n\r\n@MatthiasWinkelmann : Can you reduce the PR to handle just that change?", "Any update ? :D \r\n\r\nEdit:\r\nIf someone does not want to wait...\r\n\r\nPatch `_pywrap_tensorflow_internal.pyd` (its PE file) strings in `Lib\\site-packages\\tensorflow\\python` using `https://hexed.it/`\r\n`python36` -> `python37` (single match)\r\n`64_90` -> `64_92` (multiple matches)\r\n\r\nPatch `build_info.py` strings in `Lib\\site-packages\\tensorflow\\python\\platform`\r\n`64_90` -> `64_92` (single match)\r\n`9.0` -> `9.2` (single match)\r\n\r\nPatch `pywrap_tensorflow_internal.py` in `Lib\\site-packages\\tensorflow\\python`:\r\n`async)` -> `is_async)` (should be only 4 matches)\r\n\r\nYea i know, it's dirty way but works :D", "Closing due to inactivity, and since #21202 seems like it subsumes this."]}, {"number": 20765, "title": "Failed:  bazel build tensorflow/tools/graph_transforms:transform_graph", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:    Mac OS 10.13.6\r\n- **TensorFlow installed from (source or binary)**:   Installed from source by git clone\r\n- **TensorFlow version (use command below)**:     version 1.8\r\n- **Python version**: version 3.6     \r\n- **Bazel version (if compiling from source)**:  0.15.0-homebrew\r\n- **CUDA/cuDNN version**:  NO\r\n- **GPU model and memory**: NO\r\n- **Exact command to reproduce**: \r\n\r\n\r\n### Describe the problem\r\nI want to quantize my object detection model trained by tensorflow object detection api. But when I build transform_graph, there comes an error.\r\n\r\n### Source code / logs\r\n` bazel build tensorflow/tools/graph_transforms:transform_graph`\r\nError log is as follows:\r\n\r\n> Starting local Bazel server and connecting to it...\r\n..........\r\nINFO: Analysed target //tensorflow/tools/graph_transforms:transform_graph (67 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /private/var/tmp/_bazel_lee/f925e5225c4f573b45c7ce2445e70455/external/protobuf_archive/BUILD:260:1: Linking of rule '@protobuf_archive//:js_embed' failed (Exit 1)\r\nld: unknown option: -no-as-needed\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/graph_transforms:transform_graph failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 5.752s, Critical Path: 0.20s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\nI see [https://github.com/tensorflow/tensorflow/issues/19525](url), and I tried tensorflow branch r1.8 and r1.7, both return the same error as above.\r\n", "comments": ["Nagging Assignee @shivaniag: It has been 94 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20764, "title": "Can't build custom ops in tensorflow-gpu 1.9.0 if include \"cuda_kernel_helper.h\" with C++14", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:5.5.0\r\n- **CUDA/cuDNN version**:9.0/7.0\r\n- **GPU model and memory**:GTX1060\r\n- **Exact command to reproduce**:N?A\r\n\r\n### Describe the problem\r\nIf I build a custom ops with C++ 14 and \"cuda_kernel_helper.h\", following errors are produced:\r\n```bash\r\n>> nvcc -std=c++14 -c -o fill_functor.cu.o fill_functor.cu.cc    -arch=sm_61 ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -I/usr/local --expt-relaxed-constexpr\r\n\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = stream_executor::dnn::VersionInfo]\u2019:\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = stream_executor::dnn::VersionInfo]\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]\u2019:\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnSequenceTensorDescriptor>]\u2019:\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnSequenceTensorDescriptor>]\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnStateTensorDescriptor>]\u2019:\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnStateTensorDescriptor>]\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\r\n```\r\nIf I build it with `std=c++11` or tensorflow 1.8.0, there is no problem. But I need auto lambda to use static loop in my host code which only available in c++14.\r\n### Source code / logs\r\nThe following code is a minimal example to reproduce that error.\r\n```C++\r\n// fill_functor.h\r\n#define EIGEN_USE_THREADS\r\n\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"tensorflow/core/framework/tensor_types.h\"\r\n#include \"tensorflow/core/framework/types.h\"\r\n\r\nnamespace tensorflow {\r\nnamespace functor {\r\n\r\ntemplate <typename Device, typename T>\r\nstruct FillFunctor {\r\n  // Computes on device \"d\": out = out.constant(in(0)),\r\n  void operator()(const Device& d, typename TTypes<T>::Flat out,\r\n                  typename TTypes<T>::ConstScalar in);\r\n  void operator()(const Device& d, typename TTypes<T>::Flat out,\r\n                  T in);\r\n};\r\n\r\ntemplate <typename Device, typename T>\r\nstruct SetZeroFunctor {\r\n  // Computes on device \"d\": out = out.setZero(),\r\n  void operator()(const Device& d, typename TTypes<T>::Flat out);\r\n};\r\n\r\n\r\n}  // namespace functor\r\n}  // namespace tensorflow\r\n\r\n// fill_functor.cu.cc\r\n#if GOOGLE_CUDA\r\n\r\n#define EIGEN_USE_GPU\r\n\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/tensor_types.h\"\r\n#include \"fill_functor.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/util/cuda_kernel_helper.h\"\r\nnamespace Eigen {\r\nnamespace internal {\r\n\r\ntemplate <typename T>\r\nstruct scalar_const_op {\r\n  typedef typename packet_traits<T>::type Packet;\r\n\r\n  const T* val;\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE\r\n  scalar_const_op(const scalar_const_op& x)\r\n      : val(x.val) {}\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_const_op(const T* v) : val(v) {}\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const T operator()() const {\r\n    return *val;\r\n  }\r\n\r\n  template <typename PacketType = Packet>\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const PacketType packetOp() const {\r\n    return internal::pset1<PacketType>(*val);\r\n  }\r\n};\r\n\r\ntemplate <typename T>\r\nstruct functor_traits<scalar_const_op<T> > {\r\n  enum {\r\n    Cost = 1,\r\n    PacketAccess = packet_traits<T>::Vectorizable,\r\n    IsRepeatable = true\r\n  };\r\n};\r\n\r\n}  // end namespace internal\r\n}  // end namespace Eigen\r\n\r\nnamespace tensorflow {\r\n\r\nnamespace functor {\r\n\r\ntypedef Eigen::GpuDevice GPUDevice;\r\n\r\n// Partial specialization FillFunctor<Device=GPUDevice, T>\r\ntemplate <typename T>\r\nstruct FillFunctor<GPUDevice, T> {\r\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out,\r\n                  typename TTypes<T>::ConstScalar in) {\r\n    Eigen::internal::scalar_const_op<T> f(in.data());\r\n    To32Bit(out).device(d) = To32Bit(out).nullaryExpr(f);\r\n  }\r\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out,\r\n                  T in) {\r\n    To32Bit(out).device(d) = To32Bit(out).constant(in);\r\n  }\r\n};\r\n\r\n#define DEFINE_FILL_GPU(T) template struct FillFunctor<GPUDevice, T>;\r\nTF_CALL_NUMBER_TYPES(DEFINE_FILL_GPU);\r\nTF_CALL_bool(DEFINE_FILL_GPU);\r\n#undef DEFINE_FILL_GPU\r\n\r\n// Partial specialization of FillFunctor<Device=GPUDevice, T>.\r\ntemplate <typename T>\r\nstruct SetZeroFunctor<GPUDevice, T> {\r\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out) {\r\n    To32Bit(out).device(d) = To32Bit(out).constant(T(0));\r\n  }\r\n};\r\n\r\n#define DEFINE_SETZERO_GPU(T) template struct SetZeroFunctor<GPUDevice, T>;\r\nTF_CALL_NUMBER_TYPES(DEFINE_SETZERO_GPU);\r\nTF_CALL_bool(DEFINE_SETZERO_GPU);\r\n#undef DEFINE_SETZERO_GPU\r\n\r\n\r\n}  // end namespace functor\r\n}  // end namespace tensorflow\r\n\r\n#endif  // GOOGLE_CUDA\r\n```\r\n", "comments": ["my understanding was that all templating code needed to be in the header (.h) file and there were issues if they were defined in the cpp file. ", "@rohan100jain this issue only appears when compile with C++14 and tf 1.9.0, no problem with C++11(1.9.0) or C++14(1.8.0).", "Nagging Assignee @zheng-xq: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ymodak: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Is this an issue? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@traveller59 , Hello, I met the same problem and didn't find any other issues. \r\nIf you have solved the problem, would you mind sharing your method for solving this problem ?\r\n\r\nmy enviroment is:\r\n```\r\ntensorflow == 2.3.0\r\ng++ == 5.4.0\r\ncuda == 9.0\r\n``` "]}, {"number": 20763, "title": "New Feature: #12686 SoftmaxCrossEntropyWithLogits gradient function", "body": "Committing in a new PR as the old PR has too many commit files", "comments": ["@suharshs This is the new PR taking over from the old one. Please let me know if there are any issues merging this. ", "Awesome ! Hey thank you everyone for helping me in pulling this together. I took an awfully long time to get this done. This is my first ever PR to Tensorflow so it means a lot ! Thanks again."]}, {"number": 20762, "title": "Add a donate button to README.md", "body": "The **donate** button is a link to [`https://boost-lab.app/tensorflow/tensorflow`](https://boost-lab.app/tensorflow/tensorflow), which provides potential donors the freedom to financially support the developers of `tensorflow`.\r\n\r\nAt Boost Lab, our mission is to give developers the ability to receive continuous income for their open source work with minimal impact to their workflow.\r\n\r\nFor a more detailed explanation of how our platform works, we would love it if you check out our [web app](https://boost-lab.app/about).", "comments": ["Assigning to @wicke for more input.", "While appreciating the spirit of the idea, I don't think it's appropriate to do something like this: the majority of our developers are employed by Google or some other corporation. Please see https://opensource.google.com/community/ for some of the ways in which Google supports open source and its developers.", "\ud83d\ude4f @ewilderj thank you for your reply. And you are absolutely right. Just by looking at the repo statistics alone, I saw that around 90 - 95% of the contributions are made by Google employees (my estimations are probably off haha). Therefore, financial support is most likely not an issue for those developers.\r\n\r\nHowever, the **donate** button could be a way for other parties to show appreciation & support. For example, one of the many companies that use `tensorflow`, or even a YouTuber that teaches a course on this technology. Those parties could want to support `tensorflow`'s development in ways other than just engineering hours.\r\n\r\nBut, I can see where you are coming from. Because `tensorflow` is backed by Google, external financial support is not a requirement.\r\n\r\nAnyways, we appreciate your feedback. And perhaps Boost Lab is only tailored towards projects that does not have the backing of a tech giant.\r\n\r\nBTW, we tried out `tensorflow.js` and we love what Google is doing with the technology \ud83d\udc4d.", "Thanks @jing-c! Looks like Boost Lab is a new initiative, so if you want any further conversation on this feel free to email me, and I can go into more detail. ewj at google. ", "@ewilderj thank you. We will definitely take you up on your offer. Have a great weekend!"]}, {"number": 20760, "title": "Fix invalid link in security advisories page", "body": "The link in security advisories page was invalid and caused incorrect rendering in markdown, should be `[SECURITY.md](https://...)` instead of `(https://...)[SECURITY.md]`. This fix correct the link issue.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 20759, "title": "[XLA:GPU] let GpuTransferManager take different StreamExecutor and LLVM Compiler", "body": "GpuTransferManager is tied to CUDA StreamExecutor and LLVM NVPTX backend thus\r\nfar. In this commit refactor its constructor so it's possible to take different\r\nStreamExecutor instances and LLVM backends.", "comments": ["@jlebar this is the other patch to lay the basic foundation to support AMDGPU on XLA.\r\n\r\nsubsequent PRs would be trickier as they'd involve changes in IR emitters.", "When I try to leave review comments on this, I get an error from github, `start commit oid is not part of this pull request`.  :-/\r\n\r\nI think this may be because you have a merge commit in this PR.  Can you try rebasing that way?\r\n\r\nAnyway here's my review comment, looks good other than that.\r\n\r\nPlease use style `/*id=*/stream_executor::cuda::kPlatformId`.\r\n\r\nWe have linters internally that recognize this style and will raise an error if the comment does not match the parameter name.\r\n\r\n(The `0 /*default address space*/` comment is unfortunate but the correct way to do it, because the comment does not match the parameter name.)", "updated the PR per review comment", "@qlzh727, can you help us land this when you have a chance?"]}, {"number": 20758, "title": "Add cyclical learning rate", "body": "Add cyclical learning rate as reported by `Leslie N Smith. Cyclical learning rates for training neural networks. arXiv preprint arXiv:1506.01186v3, 2016.`\r\n\r\nSee [this example](https://postimg.cc/image/vzwtaxzhj/) to understand what a cyclical learning rate is about\r\n\r\nThe function can be easily coupled to others like `tf.train.exponential`\r\n\r\n  ```python\r\n  ...\r\n  cosine_lr = tf.train.cosine_cyclical(starter_learning_rate, global_step,\r\n                                            decay_steps, end_learning_rate, T)\r\n  cosine_exp_lr = tf.train.exponential_decay(cosine_lr,global_step,300,0.7,staircase=False)\r\n\r\n  ```\r\n\r\n  For the a better comprehension, see [this example](https://postimg.cc/image/8tcgv2xjb/)  of how the snippet above looks like in practice.", "comments": ["@wicke, didn't find a good reviewer for this change based on the change history, do u mind reroute this if you know anyone worked on this?", "I'm happy that another contributor proposed another cyclical learning rate on PR #20785 based on the same reference. I just want to make it clear that the cyclical learning rate I've coded is based on a cosine curve, not linear as proposed by #20785", "Ping @martinwicke for review.", "Have you compared the experiment results with cosine_decay_restarts and https://github.com/tensorflow/tensorflow/pull/20785?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/483913a32ef0807791a9b5edfb9628cd66c786a2/tensorflow/python/training/learning_rate_decay.py#L710", "Nope.\r\n\r\nBut as I mentioned before, the cyclical learning rate I've coded is based on a cosine curve, not linear as proposed by #20785", "Nagging Reviewer @martinwicke: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 45 days with no activity and the `awaiting review` label has been applied.", "The image linked in the opening post shows SGDR and it is reproducible with cosine_decay_restarts (https://github.com/tensorflow/tensorflow/pull/11749).", "> The image linked in the opening post shows SGDR and it is reproducible with cosine_decay_restarts (#11749).\r\n\r\nI agree. There is no need to duplicate a function. I haven't seen this before. My bad =P . I'm gonna close this PR."]}, {"number": 20757, "title": "[XLA:GPU] terminology change in GpuExecutable", "body": "To cope with upcoming introduction of AMDGPU support in XLA, change terms used\r\nin GpuExecutable.", "comments": ["@jlebar this is another patch to prepare for upcoming AMDGPU support in XLA. I've made it to be independent from #20752 ", "@jlebar I think it's fine to use `std::pair<int, int>` for AMDGPU LLVM backend and leave the second value be 0.\r\n\r\nHowever there is another complexity imposed in the interface of `DeviceDescription::cuda_compute_capability()`. Certainly it's not ideal to call such interface in a generic `GpuExecutable`.\r\n\r\nIn PR #20709 , I amended an member function `DeviceDescription::rocm_amdgpu_isa_version()` for ROCm path, and let AMDGPU subclass of `GpuExecutable`, `AMDGPUExecutable` , use this new interface.\r\n\r\nDo you see it feasible to merge these 2 interfaces into one in StreamExecutor and give it a more generic name?", "> Do you see it feasible to merge these 2 interfaces into one in StreamExecutor and give it a more generic name?\r\n\r\nYes.  And/or you could provide a third function.", "@jlebar It turns out the rabbit hole is quite deep. In order to ditch `std::pair<int, int>` in XLA, and adopt a common `DeviceVersion` class for NVPTX and AMDGPU, the following components have to be changed:\r\n\r\n- GPU common runtime : move `CudaVersion` to StreamExecutor and rename it to `DeviceVersion`\r\n- StreamExecutor : instead of using `cc_major` and `cc_minor`, use `DeviceVersion`\r\n- XLA : forefeit interfaces which take `std::pair<int, int>` and switch to `DeviceVersion`\r\n\r\nI'll submit one PR for this soon and will then revise this PR.", "@jlebar per our discussion I've revised the PR. Now it only contains terminology changes in GpuExecutable. Please focus on the content in commit:\r\nhttps://github.com/tensorflow/tensorflow/pull/20757/commits/49f225f8e45b53e9d124670a483dab1fd6dc406e", "rebase after changing #20786 ", "Not sure if this PR is still alive or not. Please resolve the merge conflict before proceeding.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 20756, "title": "Add cyclical learning rate", "body": "Add cyclical learning rate as reported by `Leslie N Smith. Cyclical learning rates for training neural networks. arXiv preprint arXiv:1506.01186v3, 2016.`\r\n\r\nSee [this example](https://postimg.cc/image/vzwtaxzhj/) to understand what a cyclical learning rate is about\r\n\r\nThe function can be easily couple to others like `tf.train.exponential`\r\n\r\n  ```python\r\n  ...\r\n  cosine_lr = tf.train.cosine_cyclical(starter_learning_rate, global_step,\r\n                                            decay_steps, end_learning_rate, T)\r\n  cosine_exp_lr = tf.train.exponential_decay(cosine_lr,global_step,300,0.7,staircase=False)\r\n\r\n  ```\r\n\r\n  For the a better comprehension, see [this example](https://postimg.cc/image/8tcgv2xjb/)  of how the snippet above looks like in practice.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Add rodrigofp-cit on PR", "rodrigofp-cit Added"]}, {"number": 20755, "title": "Let segmenter able to detect invalid input/output segment nodes beforehand.", "body": "Changes include:\r\n\r\n1. Added two candidate selector parameters to SegmentGraph, so it can use them to detect input/output edges that are not suitable for conversion and remove corresponding input/output nodes beforehand, instead of delaying the detection to conversion time by when the whole conversion will fail. The detail removal algorithm is documented as comments in the code. We have multiple internal use case for this, and all conversion fails without this removal logic.\r\n2. Implemented the selector in convert_nodes.cc as InputEdgeValidator and OutputEdgeValidator. I encapsulate related logic into helper methods like GetInputProperties/GetOutputProperties/ValidateInputProperties and reuse them in both the selector and the conversion code, and also fix a bug in the GetOutputProperties() logic where it used `graph->FindNodeId(connection.inside_id)->output_type(connection.outside_port);` previously.\r\n3. Simplify segment_test.cc and add tests for the selector.\r\n\r\nPlease let me know if you have any questions.\r\nAlso +@benbarsdell for the segmenter changes (since I'm not able to add you as reviewer..).", "comments": ["@samikama I merged the conflicts with upstream and fixed review comments, and it requires reapproval, so PTAL. Thanks.", "Friendly ping @samikama @jjsjann123 @benbarsdell. ", "Thanks for the review! @benbarsdell I totally agree we should put as much checks in the nodes candidate fn, I'll fix that after making sure it works for common models."]}, {"number": 20754, "title": "[XLA][AMDGPU] use AddrSpaceCast to access GlobalVariable on AMDGPU", "body": "In XLA kernels pointers are in default address space (0). On LLVM AMDGPU\r\nbackend, all GlobalVariable instances must be in global address space (1).\r\nTherefore an AddrSpaceCast is required for AMDGPU backend. To cope with\r\nexisting NVPTX backend, use ConstantExpr::getPointerBitCastOrAddrSpaceCast\r\nin this commit.", "comments": ["@jlebar this is a patch in XLA to cope with the special property held by LLVM AMDGPU backend", "@qlzh727 can you help merge this?", "@qlzh727 the error in \"Ubuntu Sanity\" target seems to have nothing to do with this PR but a configuration in the test environment. is it possible to force restart the test?", "I will attempt to merge this."]}, {"number": 20753, "title": "[tf.keras] Fix input shape for 1D output using tf.data.Dataset", "body": "Using keras models with one dimensional output\u00a0together with the `tf.data.Dataset` API will fail due to the shape validation introduced in 677b4cf7539af0cf5741d12dfe7e142c586d4567 since the dataset will have output shape `(None,)` but keras expects shape `(None, 1)`.\r\n\r\nHere is a small notebook describing the problem: https://colab.research.google.com/drive/1h3FUGBhVsXnj6oEE3JDnC0WRFF-Zu__c\r\n\r\nThis PR fixes the bug by expanding the last dimension of the data tensors the same way it is done with numpy arrays. Alternatively we could loosen the input validation to account for this case.\r\n\r\n/cc @rohan100jain @fchollet \r\n\r\nFixes #20698\r\n\r\n<details>\r\n <summary><b>Traceback</b></summary>\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-11-d79ea2a94b4d> in <module>()\r\n      3 \r\n      4 history = model.fit(dataset, epochs=300,\r\n----> 5                     verbose=0, steps_per_epoch=len(train_data) // 32)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1261         steps_name='steps_per_epoch',\r\n   1262         steps=steps_per_epoch,\r\n-> 1263         validation_split=validation_split)\r\n   1264 \r\n   1265     # Prepare validation data.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\r\n    905           feed_output_shapes,\r\n    906           check_batch_axis=False,  # Don't enforce the batch size.\r\n--> 907           exception_prefix='target')\r\n    908 \r\n    909       # Generate sample-wise weight values given the `sample_weight` and\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    180                            ': expected ' + names[i] + ' to have ' +\r\n    181                            str(len(shape)) + ' dimensions, but got array '\r\n--> 182                            'with shape ' + str(data_shape))\r\n    183         if not check_batch_axis:\r\n    184           data_shape = data_shape[1:]\r\n\r\nValueError: Error when checking target: expected dense_5 to have 2 dimensions, but got array with shape (None,)\r\n</details>", "comments": ["This can be a serious bug for some users, it would be great get a review on this.\r\n\r\nI've been using this patch in production for the past two weeks and haven't had any problems.", "Ping @fchollet for review again.", "@fchollet Thanks for the thorough review.\r\n\r\nI addressed your comments \ud83d\udc4d ", "@lgeiger can you resolve the merge conflict here?", "> @lgeiger can you resolve the merge conflict here?\r\n\r\nSorry for the delay. I forgot to push the rebased version \ud83d\ude04 \r\n\r\nThis is now good to go \ud83c\udf89 ", "What's the timeline for merging this PR? We are also facing the issue menitioned in https://github.com/tensorflow/tensorflow/issues/20698", "Nagging Reviewer @fchollet: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 59 days with no activity and the `awaiting review` label has been applied.", "This issue was fixed by an internal commit. I verified that it works as intended in Tensorflow 1.12"]}, {"number": 20752, "title": "[XLA:GPU] rename GpuCompiler to NVPTXCompiler", "body": "To cope with upcoming introduction of AMDGPU support in XLA, rename GpuCompiler\r\nand associated gpu_backend_lib module to NVPTXCompiler and nvptx_backend_lib.", "comments": ["@jlebar this is the 2nd patch to prepare for upcoming AMDGPU support in XLA. I'll submit proposed changes to `gpu_executable` and `gpu_transfer_manager` in subsequent PRs.", "@jlebar ping. There are rationales behind this PR because there would be quite a few differences between NVPTX and AMDGPU paths:\r\n\r\n- NVPTXCompiler and to-be-introduced AMDGPUCompiler uses different triple and datalayout string. Also different StreamExecutor platform ID would be used.\r\n- how libdevice on NVPTX and ROCm Device Libs on AMDGPU are located / linked is slightly different\r\n- HLO module optimization pipeline could be different between NVPTX and AMDGPU as some passes may only be applicable for NVPTX for now\r\n- NVPTX has to deal with ptxas and it's not needed for AMDGPU. On the other hand, AMDGPU uses `lld` to emit ELF", "I hesitate primarily because keeping the pipeline up to date is going to be a pain.  In particular, it's going to be a long time until we have testing for AMDGPU inside of google, so it's going to be hard to keep it right for correctness and performance.\r\n\r\nOTOH I also think you're probably right and it should be split out.  I'd like to take another look tomorrow, though.", "Can you please rebase this to HEAD?  Then I can try submitting upstream.", "rebase done", "Please resolve the merge conflict.", "@qlzh727 rebase done", "@jlebar / @qlzh727 ping. may need your approval on this PR once again after rebase."]}]