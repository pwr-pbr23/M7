[{"number": 41780, "title": "ResourceExhaustedError (out of memory) when training a Saved Model", "body": "**System information**\r\n- **OS Platform and Distribution:** Linux Ubuntu 18.04.4 64-bit\r\n- **TensorFlow installed from:** pip\r\n- **TensorFlow version:** 2.3.0-rc2 and 2.4.0-dev20200727\r\n- **Python version:** 3.7.6\r\n- **CUDA/cuDNN version:** CUDA 10.1, cuDNN 7.6.4.38\r\n- **GPU model and memory:** RTX 2060 Super (8 GB)\r\n\r\n**Describe the current behavior**\r\nWhen building a model with a pretrained Xception net as base, than saving it in the Saved Model format, loading and running fit(), ResourceExhaustedError (out of memory) occurs.\r\n\r\n**Describe the expected behavior**\r\nWhen training the model with the same dataset, but without saving or if the Keras H5 format is used, everything is alright, so it looks like there is actually enough GPU memory and the loaded Saved Model should also train without errors. I've also tried replacing Xception with InceptionV3 and this removes the error.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/OlegPonomaryov/xception-saved-model-bug-demo/blob/master/model.ipynb\r\n", "comments": ["@OlegPonomaryov \r\nI am unable to access the link shared for the code, please provide with simple stand alone code for us to replicate the issue or if possible share a colab gist with the error faced.", "@Saduf2019 \r\nSorry, I've made the repository private by mistake, please, try the link again, now it works.\r\n\r\n", "@OlegPonomaryov\r\nI ran the code shared and do not face any error as in the repository, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d9eb0bcb587dbcbb2010495950cb968e/untitled309.ipynb)\r\n\r\nPlease try[ limiting GPU memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if you are still facing the same issue. \r\n\r\nBelow are few issues with same error:\r\n[link](https://stackoverflow.com/questions/59394947/how-to-fix-resourceexhaustederror-oom-when-allocating-tensor) [link1](https://github.com/balancap/SSD-Tensorflow/issues/267) [link2](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/discussion/97660)\r\n\r\n", "@Saduf2019 \r\nThank you for the response.\r\n\r\n> I ran the code shared and do not face any error as in the repository, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d9eb0bcb587dbcbb2010495950cb968e/untitled309.ipynb)\r\n\r\nWere you using GPU or CPU for training? 26s/step when training looks more like a CPU performance, however, I think, a GPU is needed to reproduce this issue. Maybe the issue is even more specific, like occurring only on GPUs with the same memory amount as mine or maybe even of the same series. I understand, that this, unfortunately, makes the issue much harder to reproduce.\r\n\r\n> Please try[ limiting GPU memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) and check if you are still facing the same issue.\r\n\r\nThank you, I've tried setting memory_limit=4096, and there is still this error with the loaded Saved Model, but everything is OK with the other models. This is quiet interesting, because while the other models can work even with 4 GB of GPU memory, the loaded Saved Model throws ResourceExhaustedError even with all 8 GB available.\r\n\r\n> Below are few issues with same error:\r\n> [link](https://stackoverflow.com/questions/59394947/how-to-fix-resourceexhaustederror-oom-when-allocating-tensor) [link1](https://github.com/balancap/SSD-Tensorflow/issues/267) [link2](https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/discussion/97660)\r\n\r\nThese and other discussions of similar issues I've stumbled upon suggest to reduce either a model or an input size. However, in my case exactly the same models work well with exactly the same input data, but only the loaded Saved Model causes OOM. This makes me think, that there might be some bug with memory allocation specific for the loaded Saved Model.\r\n", "Hi @OlegPonomaryov, I have tried the code you shared in colab on the GPU runtime, and I'm not seeing an issue (to reproduce this, you can use the gist provided by @Saduf2019 but change the runtime type to GPU). I've also run this in GCP with a K80 and still do not see the error. \r\nStrangely the error message in your notebook seems to be referencing TF1 `report_tensor_allocations_upon_oom`. But that being said did you try and follow what was suggested in the error message?\r\n\r\nAs a first experiment, since you have the saved model, can you try (in a new notebook environment) just loading the model and training? Do you still see the error in that case?\r\n", "Hi @nikitamaia, sorry for late reply. Thank you for hints, I've tried just loading the Saved Model  and have encountered the same error. When loading just Keras H5 model, everything is still alright.\r\n\r\nAs for `report_tensor_allocations_upon_oom`, I haven't found a way to set `RunOptions` for `tf.keras` models. However, I've tried profiling memory usage with Tensorboard (using `tensorboard-plugin-profile`).\r\n\r\nHere is [a branch]( https://github.com/OlegPonomaryov/xception-saved-model-bug-demo/tree/saved-model-only), that contains the notebook modified to just load the Saved Model and to collect Tensorboard logs when running fit() (logs are also there).\r\n\r\nHere is [a similar branch](https://github.com/OlegPonomaryov/xception-saved-model-bug-demo/tree/h5-model-only) but with the Keras H5 model.\r\n\r\nUnfortunately, sharing the profile plugin logs through [tensorboard.dev](https://tensorboard.dev/) currently is not supported, so I guess the only way to view them is to download them and open in your local instance of Tensorboard. Alternatively, here are [screenshots](https://imgur.com/a/UDDBhwG) of memory profiling tabs both for the Saved Model and Keras H5 model.\r\n\r\nI notice differences in Peak Heap Usage and Peak Memory Usage, but I don't see any operations in the Memory Breakdown Table that consume too much memory. However, I have zero experience with Tensorboard profiling, so might just be missing some obvious things.", "Thanks for sharing these screenshots. I also ran some experiments using the colab GPU runtime, which is a T4 with ~15GB. I ran for 3 epochs and noticed the peak memory usage for the H5 model was about the same as what you saw (~3.6GB) but close to 10GB for the savedmodel, which probably explains why you're getting the resource exhausted error on an 8GB device. \r\n\r\nCan you run some similar experiments for InceptionV3 since you said that worked for you? I'm curious what the difference will be between H5 and savedmodel peak memory usage, since the almost 3x difference for Xception does seem awfully high.\r\n", "@nikitamaia So I've tested this with 3 other models (including InceptionV3). In all 3 cases there was no OOM error, so Tensorboard was able to log peak memory usage through all training process. Here are the results:\r\n\r\n## InceptionV3\r\n### Saved Model\r\nPeak Heap Usage 5.13 GiBs\r\nPeak Memory Usage 5.13 GiBs\r\n### Keras H5\r\nPeak Heap Usage 2.63 GiBs\r\nPeak Memory Usage 2.63 GiBs\r\n\r\n## MobileNetV2\r\n### Saved Model\r\nPeak Heap Usage 4.41 GiBs\r\nPeak Memory Usage 4.41 GiBs\r\n### Keras H5\r\nPeak Heap Usage 2.11 GiBs\r\nPeak Memory Usage 2.11 GiBs\r\n\r\n\r\n## ResNet50V2\r\n### Saved Model\r\nPeak Heap Usage 6.62 GiBs\r\nPeak Memory Usage 6.62 GiBs\r\n### Keras H5\r\nPeak Heap Usage 3.62 GiBs\r\nPeak Memory Usage 3.62 GiBs\r\n\r\nIn all experiments a model loaded from tjhe Keras H5 used approximately 2 times less GPU memory than a model loaded from the Saved Model. So it looks like a performance issue with the Saved Model in general rather then with the Xception model (of course if there is no mistake in my code that I've missed).", "A little bit more experiments. I\\ve tried replacing `.fit(dataset, epochs=1)` with `.predict(dataset)` in both [\"Saved Model only\"](https://github.com/OlegPonomaryov/xception-saved-model-bug-demo/tree/saved-model-only) and [\"Keras H5 only\"](https://github.com/OlegPonomaryov/xception-saved-model-bug-demo/tree/h5-model-only) notebooks. Everything runs OK, Peak Heap Usage and Peak Memory Usage are 3.62 GiBs for the Saved Model and 3.63 GiBs for the Keras H5. So the problem seems to be specifically with the `fit()` method.\r\n\r\nI've also tried to make a \"copy\" of the loaded Saved Model by replacing\r\n```\r\nwith tf.profiler.experimental.Profile(SAVED_MODEL_PROFILING_LOG):\r\n    saved_model.fit(dataset, epochs=1)\r\n```\r\nwith\r\n```\r\nsaved_model_copy = tf.keras.models.Sequential(saved_model.layers)\r\nsaved_model_copy.compile(loss=\"categorical_crossentropy\")\r\nwith tf.profiler.experimental.Profile(SAVED_MODEL_PROFILING_LOG):\r\n    saved_model_copy.fit(dataset, epochs=1)\r\n```\r\nin the \"Saved Model only\" notebook.\r\nI've actually done it already in the initial notebook, just wanted to check memory usage with Tensorboard. Everything also runs OK, Peak Heap Usage and Peak Memory Usage are 3.62 GiBs.", "And another update, I've also tried the VGG16 and here are the results:\r\n### Saved Model\r\n**Peak Heap Usage:** 0.20 GiBs\r\n**Peak Memory Usage:** 5.83 GiBs\r\n### Keras H5\r\n**Peak Heap Usage:** 5.22 GiBs\r\n**Peak Memory Usage:** 5.22 GiBs\r\n\r\nSo in contrast to other models, in this case Peak Memory Usage of the Saved Model is just a little bit higher compared to the Keras H5. The Saved Model also has an oddly low  Peak Heap Usage, but this happens sometimes, I think it's caused by some Tensorboard behavior rather than by the model itself.", "I have tried to limit the GPU memory to 4096 and run the `Xception` model and I did not get any `ResourceExhaustedErrror` please find the Tensorflow 2.7 gist [here](https://colab.research.google.com/gist/sachinprasadhs/2bbf5d1bdf2a5d7b2fbae9e5a78ad921/untitled309.ipynb). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41779, "title": "While trying to run 3D Unet of mesh-tensorflow ", "body": "**System information**\r\n- OS Platform : ubuntu 18.04 : x86_64 \r\n- TensorFlow version 2.2.0:\r\n- Python version 3.6:\r\n- Installed using  pip:\r\n- CUDA/cuDNN : **10.1.243** / **7.6** :\r\n- GPU model and memory  :  pciBusID: 0000:1c:00.0 name: TITAN V computeCapability: 7.0\r\ncoreClock: 1.455GHz coreCount: 80 deviceMemorySize: 11.75GiB deviceMemoryBandwidth: 607.97GiB/s\r\n\r\n\r\n\r\nI was trying to run 3D Unet model in the experimental/ of mesh-tensorflow on GPUs.     \r\n **python model_executor.py --use_tpu=False --write_summary=False --train_file_pattern=media/tfrecords64/train/*.tfrecords  --eval_file_pattern=media/tfrecords64/eval/*.tfrecords   --batch_size_train=2  --ct_resolution=64 --mesh_shape=\"rows:2, columns:2, cores:2\" --checkpoint_dir=checkpoint_dir_8gpu &**   \r\n\r\nI removed tensorflow.contrib etc, in the code as contrib does not belong to 2.2.0. I installed tensorflow_probability. I adjusted/removed a part of code that depends on TPU, to run the above. \r\n\r\n\r\nHowever this is in the following in the logs. I can also see that GPU memory is all being used up as shown by nvidia-smi. The training seems to be slow too.\r\n\r\n**tensorflow/core/common_runtime/colocation_graph.cc:1017] Failed to place the graph without changing the devices of some resources. Some of the operations (that had to be colocated with resource generating operations) are not supported on the resources' devices. Current candidate devices are [\r\n/job:localhost/replica:0/task:0/device:CPU:0].\r\nSee below for details of this colocation group:\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=-1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nAssignVariableOp: CPU XLA_CPU XLA_GPU\r\nRandomUniform: CPU XLA_CPU XLA_GPU\r\nVarIsInitializedOp: GPU CPU XLA_CPU XLA_GPU\r\nConst: GPU CPU XLA_CPU XLA_GPU\r\nMul: CPU XLA_CPU XLA_GPU\r\nReadVariableOp: GPU CPU XLA_CPU XLA_GPU\r\nSub: CPU XLA_CPU XLA_GPU\r\nVarHandleOp: CPU XLA_CPU XLA_GPU\r\nAdd: CPU XLA_CPU XLA_GPU** \r\n\r\n\r\n", "comments": ["@ved27 \r\n\r\nRequest you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!\r\n", "@ravikyram  please find the updated details Thank you! ", "@ved27 Can you please try running it using a tensorflow-gpu than using mesh-tensorflow and let me know if the problem still persists\r\n/", "Hi ,\r\n\r\nI was using tensorflow-gpu 2.2.0 and then mesh tensorflow. Problem exists still ", "@ved27 Please provide the reproducible code for us to reproduce the issue. Thanks!", "Hi @ved27 This is the duplicate of this [issue](https://github.com/tensorflow/serving/issues/1701) and as you mentioned here that its because of the datatype and as its being resolved, I am closing this issue here"]}, {"number": 41778, "title": "Models with Conv2D layer cause segmentation fault when invoked in C++", "body": "**System information**\r\n- Linux Ubuntu 18.04\r\n- TensorFlow Binary (via `pip`)\r\n- Tested on 2.2.0 & 2.3.0rc2\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv2D, Flatten, Input, Dense\r\nfrom tensorflow.keras.models import Model\r\n\r\ninputs = Input(shape=[10, 5, 1])\r\nx = inputs\r\nx = Conv2D(32, (3, 3))(x) # *** Functions correctly when removed\r\nx = Flatten()(x)\r\nx = Dense(1)(x)\r\n    \r\nmodel = Model(inputs, x)\r\nmodel.compile()\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\n# Save the TF Lite model.\r\nwith tf.io.gfile.GFile(\"min_model.tflite\", \"wb\") as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\nNone\r\n\r\nModels in `SavedModel` & `tflite` formats as well as `minimal` example build for `linux_x86_64`:\r\n\r\n```\r\nhttps://www.dropbox.com/sh/2w67cix39onn28m/AAD0O3OR4Z_y_zMJJJdGBdtQa?dl=0\r\n```\r\n\r\n**Failure details**\r\n* Conversion successful\r\n* Causes segmentaion fault on `invoke()` when run via [tflite c++ minimal example](https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_c)\r\n  * Invokes fine without `Conv2D` layer\r\n\r\n```c++\r\n#include \"tensorflow/lite/interpreter.h\"\r\n#include \"tensorflow/lite/kernels/register.h\"\r\n#include \"tensorflow/lite/model.h\"\r\n#include \"tensorflow/lite/optional_debug_tools.h\"\r\n#include <cstdio>\r\n\r\nusing namespace tflite;\r\n\r\n#define TFLITE_MINIMAL_CHECK(x)                                                \\\r\n  if (!(x)) {                                                                  \\\r\n    fprintf(stderr, \"Error at %s:%d\\n\", __FILE__, __LINE__);                   \\\r\n    exit(1);                                                                   \\\r\n  }\r\n\r\nint main(int argc, char *argv[]) {\r\n  if (argc != 2) {\r\n    fprintf(stderr, \"minimal <tflite model>\\n\");\r\n    return 1;\r\n  }\r\n  const char *filename = argv[1];\r\n\r\n  // Load model\r\n  std::unique_ptr<tflite::FlatBufferModel> model =\r\n      tflite::FlatBufferModel::BuildFromFile(filename);\r\n  TFLITE_MINIMAL_CHECK(model != nullptr);\r\n\r\n  // Build the interpreter\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  InterpreterBuilder builder(*model, resolver);\r\n  std::unique_ptr<Interpreter> interpreter;\r\n  builder(&interpreter);\r\n  TFLITE_MINIMAL_CHECK(interpreter != nullptr);\r\n\r\n  // Allocate tensor buffers.\r\n  TFLITE_MINIMAL_CHECK(interpreter->AllocateTensors() == kTfLiteOk);\r\n  printf(\"=== Pre-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  // Run inference\r\n  TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk); // *** Fails here before return\r\n  printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n  tflite::PrintInterpreterState(interpreter.get());\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\nCalled via `minimal min_model.tflite`.\r\n\r\n\r\n", "comments": ["Closing in favour of [42057](https://github.com/tensorflow/tensorflow/issues/42057#issue-673420638) which more accurately represents the problem.\r\n\r\nIt appears the issue lies with static linking and thread generation rather than with `conv2d` or model conversion.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41778\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41778\">No</a>\n"]}, {"number": 41777, "title": "Adagrad colocation fitting error", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Nvidia Quadro V100\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nCompilation and fitting of a network that uses tf.keras.layers.DenseFeatures and an Adagrad optimizer results in a co-location error that does not resolve when allowing soft placement and/or turning off XLA JIT. Changing the optimizer to SGD resolves the issue.\r\n\r\n**Describe the expected behavior**\r\nAdagrad should not incite a colocation error at model fitting when used with DenseFeatures.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.feature_column import feature_column_v2 as fc\r\nmetadata = {'m1': tf.ones(shape=(100,1)), 'm2': tf.ones(shape=(100,1)),'label':tf.ones(shape=(100,1))}\r\nnum_samples = 100\r\ndnn_optimizer = tf.keras.optimizers.Adagrad(\r\n                learning_rate=1\r\n            )\r\ndef meta_dict_gen():\r\n    for i in range(num_samples):\r\n        ls = {}\r\n        for key, val in metadata.items():\r\n            ls[key] = val[i]\r\n        yield ls\r\n# DATASET CREATION\r\nd = tf.data.Dataset.from_generator(\r\n    meta_dict_gen,\r\n    output_types={k: tf.float32 for k in metadata},\r\n    output_shapes={'m1': (1,), 'm2': (1),'label':(1)})\r\nd = d.shuffle(\r\n        buffer_size=10 * 8\r\n    )\r\n\r\n\r\nfeatures = {'m1':1,'m2':1}\r\ndef label_map(d):\r\n    \r\n    label = d.pop('label')\r\n    reshaped_label = tf.reshape(label, [-1, label.shape[-1]])\r\n    reshaped_elem = {\r\n        key: tf.reshape(d[key], [-1, d[key].shape[-1]])\r\n        for key in d if key in features.keys()\r\n    }\r\n    \r\n    return reshaped_elem, reshaped_label\r\nd = d.map(map_func=label_map)\r\n\r\n# CREATING DENSE FEATURE LAYER\r\nd_columns = [tf.feature_column.embedding_column(fc.categorical_column_with_hash_bucket(key='m1', hash_bucket_size=2, dtype=tf.int64),dimension=1,combiner='mean'),\r\ntf.feature_column.numeric_column(\r\n                    'm2', shape=(1,))]\r\n\r\nd_features = {}\r\nd_features['m1'] = tf.keras.Input(shape=(1,), name='m1', dtype=tf.int64, sparse=False)\r\nd_features['m2'] = tf.keras.Input(shape=(1,), name='m2', dtype=tf.int64, sparse=False)\r\n\r\n\r\n#CREATING MODEL\r\n\r\nd_input = tf.keras.layers.DenseFeatures(d_columns, name='d_embedded')(d_features)\r\nd_output = tf.keras.layers.Dense(1)(d_input)\r\nd_model = tf.keras.Model(d_features,d_output)\r\nd_model.compile()\r\nd_model.compile(optimizer = dnn_optimizer,loss= 'binary_crossentropy',metrics = ['binary_crossentropy'])\r\nd_model.fit(d)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n@DEKHTIARJonathan @nluehr \r\n\r\n", "comments": ["@kkranen \r\nI ran the code shared and did not face any error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/65890558a70bd7bb814c9b7250af061f/untitled300.ipynb).\r\nPlease share a  colab gist with the error and the error logs for us to analyse.", "Hmm, I see the same results you achieved with colab on my side as well. Locally, my stack trace looks like this when I attempt to run the block:\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-476fdc9b6732> in <module>\r\n     52 d_model.compile()\r\n     53 d_model.compile(optimizer = dnn_optimizer,loss= 'binary_crossentropy',metrics = ['binary_crossentropy'])\r\n---> 54 d_model.fit(d)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    846                 batch_size=batch_size):\r\n    847               callbacks.on_train_batch_begin(step)\r\n--> 848               tmp_logs = train_function(iterator)\r\n    849               # Catch OutOfRangeError for Datasets of unknown size.\r\n    850               # This blocks until the batch has finished executing.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    578         xla_context.Exit()\r\n    579     else:\r\n--> 580       result = self._call(*args, **kwds)\r\n    581 \r\n    582     if tracing_count == self._get_tracing_count():\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\r\n    642         # Lifting succeeded, so variables are initialized and we can run the\r\n    643         # stateless function.\r\n--> 644         return self._stateless_fn(*args, **kwds)\r\n    645     else:\r\n    646       canon_args, canon_kwds = \\\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2418     with self._lock:\r\n   2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n   2422   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n   1663          if isinstance(t, (ops.Tensor,\r\n   1664                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1665         self.captured_inputs)\r\n   1666 \r\n   1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1744       # No tape is watching; skip to running the function.\r\n   1745       return self._build_call_outputs(self._inference_function.call(\r\n-> 1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n   1748         args,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\r\n    596               inputs=args,\r\n    597               attrs=attrs,\r\n--> 598               ctx=ctx)\r\n    599         else:\r\n    600           outputs = execute.execute_with_cancellation(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: Cannot assign a device for operation model/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup: Could not satisfy explicit device specification '' because the node {{colocation_node model/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. \r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nResourceSparseApplyAdagradV2: CPU \r\nUnsortedSegmentSum: GPU CPU XLA_CPU XLA_GPU \r\nStridedSlice: GPU CPU XLA_CPU XLA_GPU \r\nConst: GPU CPU XLA_CPU XLA_GPU \r\nResourceGather: GPU CPU XLA_CPU XLA_GPU \r\nUnique: GPU CPU \r\nShape: GPU CPU XLA_CPU XLA_GPU \r\n_Arg: GPU CPU XLA_CPU XLA_GPU \r\nIdentity: GPU CPU XLA_CPU XLA_GPU \r\nVariableShape: GPU CPU XLA_CPU XLA_GPU \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  model_d_embedded_m1_embedding_m1_embedding_weights_embedding_lookup_sparse_embedding_lookup_651 (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  adagrad_adagrad_update_resourcesparseapplyadagradv2_accum (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  model/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup (ResourceGather) \r\n  model/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup/Identity (Identity) \r\n  gradient_tape/model/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup/VariableShape (VariableShape) \r\n  Adagrad/Adagrad/update/Unique (Unique) \r\n  Adagrad/Adagrad/update/Shape (Shape) \r\n  Adagrad/Adagrad/update/strided_slice/stack (Const) \r\n  Adagrad/Adagrad/update/strided_slice/stack_1 (Const) \r\n  Adagrad/Adagrad/update/strided_slice/stack_2 (Const) \r\n  Adagrad/Adagrad/update/strided_slice (StridedSlice) \r\n  Adagrad/Adagrad/update/UnsortedSegmentSum (UnsortedSegmentSum) \r\n  Adagrad/Adagrad/update/ResourceSparseApplyAdagradV2 (ResourceSparseApplyAdagradV2) \r\n\r\n\t [[{{node model/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup}}]] [Op:__inference_train_function_913]\r\n```", "Inside tensorflow/tensorflow:nightly-gpu container:\r\n\r\n```\r\nColocation Debug Info:\r\nColocation group had the following types and supported devices:\r\nRoot Member(assigned_device_name_index_=2 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nResourceSparseApplyAdagradV2: CPU\r\nUnsortedSegmentSum: GPU CPU XLA_CPU XLA_GPU\r\nStridedSlice: GPU CPU XLA_CPU XLA_GPU\r\nConst: GPU CPU XLA_CPU XLA_GPU\r\nResourceGather: GPU CPU XLA_CPU XLA_GPU\r\nUnique: GPU CPU\r\nShape: GPU CPU XLA_CPU XLA_GPU\r\n_Arg: GPU CPU XLA_CPU XLA_GPU\r\nIdentity: GPU CPU XLA_CPU XLA_GPU\r\nVariableShape: GPU CPU XLA_CPU XLA_GPU\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  functional_1_d_embedded_m1_embedding_m1_embedding_weights_embedding_lookup_sparse_embedding_lookup_689 (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  adagrad_adagrad_update_resourcesparseapplyadagradv2_accum (_Arg)  framework assigned device=/job:localhost/replica:0/task:0/device:GPU:0\r\n  functional_1/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup (ResourceGather) /job:localhost/replica:0/task:0/device:GPU:0\r\n  functional_1/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup/Identity (Identity) /job:localhost/replica:0/task:0/device:GPU:0\r\n  gradient_tape/functional_1/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup/VariableShape (VariableShape)\r\n  Adagrad/Adagrad/update/Unique (Unique) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Adagrad/Adagrad/update/Shape (Shape) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Adagrad/Adagrad/update/strided_slice/stack (Const) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Adagrad/Adagrad/update/strided_slice/stack_1 (Const) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Adagrad/Adagrad/update/strided_slice/stack_2 (Const) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Adagrad/Adagrad/update/strided_slice (StridedSlice) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Adagrad/Adagrad/update/UnsortedSegmentSum (UnsortedSegmentSum) /job:localhost/replica:0/task:0/device:GPU:0\r\n  Adagrad/Adagrad/update/ResourceSparseApplyAdagradV2 (ResourceSparseApplyAdagradV2) /job:localhost/replica:0/task:0/device:GPU:0\r\nOp: ResourceGather\r\nNode attrs: dtype=DT_FLOAT, batch_dims=0, Tindices=DT_INT64, validate_indices=true, _class=[\"loc:@funct...lookup/689\"]\r\nRegistered kernels:\r\n  device='XLA_GPU'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='CPU'; dtype in [DT_QINT32]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_QINT32]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_QUINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_QUINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_QINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_QINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_RESOURCE]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_STRING]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_BOOL]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_BFLOAT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT16]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT32]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT32]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_INT64]; Tindices in [DT_INT32]\r\n  device='CPU'; dtype in [DT_UINT64]; Tindices in [DT_INT64]\r\n  device='CPU'; dtype in [DT_UINT64]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_VARIANT]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_BOOL]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_BOOL]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='GPU'; dtype in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='GPU'; dtype in [DT_INT64]; Tindices in [DT_INT32]\r\n  device='XLA_GPU_JIT'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_CPU_JIT'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n  device='XLA_CPU'; Tindices in [DT_INT32, DT_INT64]; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]\r\n         [[{{node functional_1/d_embedded/m1_embedding/m1_embedding_weights/embedding_lookup_sparse/embedding_lookup}}]] [Op:__inference_train_function_952]\r\n```", "@kkranen I can reproduce the issue when I select `GPU` in the colab. [Here](https://colab.research.google.com/gist/jvishnuvardhan/5d41769e7eb3aa69e70fb70c53d97fb7/untitled300.ipynb) is the gist for our reference. Thanks!\r\n\r\n\r\n", "Is it correct to say that the root cause is that `SparseApplyAdagradV2` does not have a GPU kernel?", "That would also be my bet: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L1938-L1951\r\n\r\n```cpp\r\n#define REGISTER_KERNELS(T, Tindices)                                \\\r\n  REGISTER_KERNEL_BUILDER(Name(\"SparseApplyAdagradV2\")               \\\r\n                              .Device(DEVICE_CPU)                    \\\r\n                              .TypeConstraint<T>(\"T\")                \\\r\n                              .TypeConstraint<Tindices>(\"Tindices\"), \\\r\n                          SparseApplyAdagradV2Op<T, Tindices>);      \\\r\n  REGISTER_KERNEL_BUILDER(Name(\"ResourceSparseApplyAdagradV2\")       \\\r\n                              .Device(DEVICE_CPU)                    \\\r\n                              .TypeConstraint<T>(\"T\")                \\\r\n                              .TypeConstraint<Tindices>(\"Tindices\"), \\\r\n                          SparseApplyAdagradV2Op<T, Tindices>);\r\n\r\n#define REGISTER_CPU_KERNELS(T) \\\r\n  REGISTER_KERNELS(T, int32);   \\\r\n  REGISTER_KERNELS(T, int64);\r\n```", "Quickly reading the issue, why is this related to `DenseFeatures` -- it will also error out w/o it, correct?", "Initially I thought this was related to dense features but it seems to error without it.", "> Initially I thought this was related to dense features but it seems to error without it.\r\n\r\nThat'd my guess as well. And yes missing GPU kernels for those sparse ops is the main reason I believe.", "Was able to reproduce the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5801f85168ed86a1eaa6bfcb5b2a95cf/untitled139.ipynb?authuser=1)..Thanks !", "I haven't tried reproducing this issue, but I'll note that a GPU kernel for SparseApplyAdagradV2 was [added](https://github.com/tensorflow/tensorflow/commit/28162ecc0cf0993fa72e4d3b72f864300188ccd5) in TF 2.5.", "I ran the gist on Colab and locally on my machine with TF 2.5 and 2.6 and in all cases it worked without error. I do see the error with TF 2.4. So afaict this issue is fixed as of TF 2.5.\r\n@sushreebarsa can you confirm that you reproduced the error with TF 2.5?", "@benbarsdell \r\nI ran the code on tf 2.5 and it works as expected, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d8d47fc4b28fc5cc64b48b74388800a0/untitled139.ipynb?authuser=1), can you please move this to closed status as its resolved.", "Closing as fixed since TF 2.5.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41777\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41777\">No</a>\n"]}, {"number": 41776, "title": "Add s3_filesystem_test init", "body": "@mihaimaruseac \r\nThis PR adds some preparation for `s3_filesystem_test`.", "comments": []}, {"number": 41775, "title": "Added a \"note\" in tf.where as workaround for issue #38349", "body": "", "comments": ["Can you fix build errors?\r\n\r\n```\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128\r\n    \t; NodeDef: {{node Sqrt}}; Op<name=Sqrt; signature=x:T -> y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:Sqrt]\r\n```", "@codeadmin-peritiae Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "@codeadmin-peritiae, Any update on this PR? Please. Thanks!", "> @codeadmin-peritiae, Any update on this PR? Please. Thanks!\r\n\r\n@gbaned, I really don't understand why a couple of build fail because of a string definition... In any case, I'm still working on it... thank you.  ", "The docstring also has testable code. That code runs and can fail", "> The docstring also has testable code. That code runs and can fail\r\n\r\nI red the error details and I found the issue was probably a type mismatching. \r\nI was trying to check my code fixing before to submit a PR update. This was time consuming...\r\n\r\nI take the opportunity to ask a suggestion:\r\nIs it reasonable to try to make code checks \"locally\" on my machine, before to update PR code; or there is no alternative and code checks have to be completed by the TF continuous integration platform after a PR update?\r\nThank you  ", "You can always test locally, `bazel test //tensorflow/path/to:test`", "@codeadmin-peritiae Can you please check @mihaimaruseac's comments and keep us posted ? Thanks!", "> You can always test locally, `bazel test //tensorflow/path/to:test`\r\n\r\n... not so easy :-)\r\nIn any case, I updated the PR.\r\nThank you.\r\n"]}, {"number": 41774, "title": "why can i not install tensorflow library at python 3.8?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:2.2\r\n- Python version:3.8\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow==2.2\r\npip install tensorflow\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Motasem2000 \r\n\r\nWhat is the output of\r\n\r\n```\r\npip list\r\npip --version\r\npython -vv\r\npython -w\r\n```\r\nIn general you don't need to download the pip from storage.googleapis.com, a pip install tensorflow should be able to get it, assuming your Python is on 64 bits, your pip is up to date, you have the needed MSVC redistributable installed and that Python is installed from the Python website, not from Microsoft Marketplace (apparently, there is a bug with that).\r\nMake sure your python was installed 64 bit.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41774\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41774\">No</a>\n"]}, {"number": 41773, "title": "MirroredStrategy with train_on_batch: Method requires being in cross-replica context, use get_replica_context().merge_call()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.4.0-dev20200727\r\n- Python version: 3.8.3\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: 2x GTX 1060 6Go\r\n\r\n**Describe the current behavior**\r\nWhen using **MirroredStrategy** with **train_on_batch**, I get the error that the **Method requires being in cross-replica context**\r\nI have the same behavior with TF2.2, 2.3\r\n\r\nThe code is taken from issue #39270, and I added the wrapper `strategy.run`\r\n\r\n**Describe the expected behavior**\r\nTo have the workload distributed across 2 GPUs\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\nwith strategy.scope():\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dense(10),\r\n    ])\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy'])\r\n\r\ndef train_step(x, y):\r\n    model.train_on_batch(x, y)\r\n    \r\nfor x, y in train_dataset:\r\n    strategy.run(train_step, args=(x, y,))\r\n```\r\n\r\n\r\n**Other info / logs** \r\n```\r\n~/anaconda3/envs/distributed/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py in __enter__(self)\r\n    385     # Allow this scope to be entered if this strategy is already in scope.\r\n    386     if distribution_strategy_context.has_strategy():\r\n--> 387       _require_cross_replica_or_default_context_extended(\r\n    388           self._context.strategy.extended)\r\n    389       self._same_scope_again_count += 1\r\n\r\n~/anaconda3/envs/distributed/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py in _require_cross_replica_or_default_context_extended(extended)\r\n    309     _wrong_strategy_scope(strategy, context)\r\n    310   assert cross_replica is None\r\n--> 311   raise RuntimeError(\"Method requires being in cross-replica context, use \"\r\n    312                      \"get_replica_context().merge_call()\")\r\n    313 \r\n\r\nRuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()\r\n```\r\n\r\n[error.log](https://github.com/tensorflow/tensorflow/files/4983379/error.log)\r\n", "comments": ["@OdysseeT,\r\nCould you please take a look at [this](https://stackoverflow.com/a/61091830) StackOverflow comment form a similar issue and let us know if it helps. Thanks!", "@amahendrakar \r\nI spent 2 days on that post already... none of the proposed solutions worked for me. \r\nThe major differences with my code are:\r\n- I compile my model - where in the SO post he doesn't\r\n- I'm using `train_on_batch` method - where in the SO post he's using the `__call__` method", "I took the same code as in the post (the working version), and I replaced the line:\r\n`predictions = model(images, training=True)`\r\nby\r\n`predictions = model.train_on_batch(images)`\r\n\r\nand I get the same error:\r\n\r\n`RuntimeError: Method requires being in cross-replica context, use get_replica_context().merge_call()\r\n`\r\n\r\nPlease note that I got intermediate errors before getting this one:\r\n```RuntimeError: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.```\r\n--> So I added `model.compile`\r\n\r\n```RuntimeError: Detected a call to `Model.train_on_batch` inside a `tf.function`. `Model.train_on_batch is a high-level endpoint that manages its own `tf.function`. Please move the call to `Model.train_on_batch` outside of all enclosing `tf.function`s. Note that you can call a `Model` directly on `Tensor`s inside a `tf.function` like: `model(x)```\r\n--> So I remove the `tf.function`", "My code worked with `model((x,y), training=True)`. So, it is coming from the `train_on_batch` method. Is it supported with **MirroredStrategy**?", "Hi @OdysseeT, `train_on_batch` indeed now works with MirroredStrategy as noted in the issue #39270. I'm curious why you added `strategy.run`, as you shouldn't need that unless you're using a custom training loop (and not using the Keras API)\r\n\r\n", "Hi @nikitamaia, thanks for your answer.\r\nI have simplified the code here to reproduce the issue, but the full code is using a custom training loop.\r\nI might be mistaken here. I thought I could use the loss returned by `train_on_batch` to compute the average loss from the global batch size. But from what you say, `train_on_batch` should not be used in `strategy.run`. Is that correct? Thanks", "`Model.train_on_batch` is a high level endpoint so I don't think it makes sense to use with `strategy.run` just like how you wouldn't wrap model.fit within `strategy.run`.  ", "Clear, thanks for the clarification."]}, {"number": 41771, "title": "can we convert back tflite model to keras h5 format?", "body": "hi there, thanks for this powerful tool.\r\ncan we convert back tflite model to keras h5 format?\r\ni find a solution [here](https://stackoverflow.com/questions/59559289/is-there-any-way-to-convert-the-tflite-file-back-to-keash5), but the weight in bn layer in tflite is null, how can we convert back this?", "comments": ["@feiwofeifeixiaowo \r\n\r\nI think there is no way to convert tflite model to keras h5 format as some information will be lost after conversion.\r\n\r\nCan you refer the [link](https://stackoverflow.com/questions/53664279/converting-tflite-to-pb) and see if it helps you.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n", "ok, thanks."]}, {"number": 41769, "title": "Simple TFLite UNet slower on mobile GPU than CPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04, Android 8.0.0\r\n- Mobile device if the issue happens on mobile device: Huawei P20 Lite (ANE-LX1)\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: CUDA 10.1\r\n- GPU model: Mali-T830 MP2\r\n\r\n**Describe the current behavior**\r\n\r\nI have written a simple image segmentation model based on UNet in Keras, which I want to use on an Android device (Huawei P20 Lite). It consists solely of Conv2D, Conv2DTranspose, and Concatenate layers, which should all be supported on mobile GPUs by TFLite according to the [documentation](https://www.tensorflow.org/lite/performance/gpu_advanced#supported_ops). The model has ~1.9M parameters and expects input tensors of shape [1, 224, 224, 3] and type float32. It is not trained yet, as I want to check its performance first before addressing the accuracy. I exported the model using\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nwith tf.io.gfile.GFile('trained models/keras_unet.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nYou can find the exported TFLite model attached further down.\r\n\r\nOn the CPU of my device, inference takes about 1.3s on average using 6 threads. However, when employing the GPU delegate using\r\n\r\n```\r\ntfliteModel = FileUtil.loadMappedFile(this, MODEL_NAME_TF);\r\nGpuDelegate gpu_del = new GpuDelegate();\r\ntfliteOptions.addDelegate(gpu_del);\r\ntflite = new Interpreter(tfliteModel, tfliteOptions);\r\n```\r\n\r\nthe performance drops to about 3.5s per inference, which is more than 2.5 times slower compared to CPU. \r\n\r\n**Describe the expected behavior**\r\nAs the UNet model only consists of supported operations, the mobile GPU supports OpenGL ES 3.2, and model export and import as well as GPU delegate creation have been done as suggested in the [TFLite Android Tutorial](https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#0) and the [GPU delegate Guide](https://www.tensorflow.org/lite/performance/gpu#trying_the_gpu_delegate_on_your_own_model), I would expect the UNet to infer faster on the GPU than on the CPU.\r\n\r\nI would be glad if you could point out any implementation issues on my side that could potentially cause the comparably worse performance on the GPU. The guides and tutorials unfortunately do not offer any deeper insight. Is it possible that the model is simply too large for my GPU (and would there be an error if this was the case)?\r\n\r\n**Standalone code to reproduce the issue**\r\nThe following model exhibits the described behavior: [tflite_model.zip](https://github.com/tensorflow/tensorflow/files/4982040/tflite_model.zip)\r\n\r\n\r\n**Other info / logs**\r\nI have previously implemented the exact same model in PyTorch, the performance on the CPU is about the same. I then switched to Keras to utilize TFLite's GPU delegate. I used a subclassed approach to develop the UNet-like model in Keras.\r\n\r\nI have read [here](https://www.tensorflow.org/lite/performance/gpu#tips_for_optimization) that it might be beneficial to use a Tensor with c=4 instead to avoid memory copies. I tried adding a fourth channel to the model and the input images (shape [1, 224, 224, 4]), which made performance even worse.\r\n\r\nThe performance has been tested using a custom Android application. The timings mentioned above refer to the following call:\r\n`tflite.run(inputTensorBuffer.getBuffer(), outputProbabilityBuffer.getBuffer().rewind());`\r\nI also attempted to profile the TFLite model using the [TFLite Model Benchmark Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark), but the Tool failed to build correctly using Bazel. If this is of relevance for solving the problem at hand, I will file another issue for that.\r\n\r\nFinally, here are the outputs produced by the Python script for exporting the model and the Android test application:\r\n[python_out.txt](https://github.com/tensorflow/tensorflow/files/4982006/python_out.txt)\r\n[android_out.txt](https://github.com/tensorflow/tensorflow/files/4982010/android_out.txt)", "comments": ["Hi Chao, can you help take a look?\r\n\r\nthanks", "Hi Jakob, could you download the pre-built model benchmark tool from [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/mobile/android_lite/nightly/806/20200726-221609/benchmark_tflite_model), then run it on your device and share detailed profiling info here?\r\n\r\nOn my side, I tried this benchmark tool with this model (keras_unet.tflite) on a Pixel 3 and found that gpu gives much better perf. than the cpu. If using the OpenGL backend, it's about ~150ms. If using the OpenCL backend, it's about ~32ms.", "Hi Chao, thanks for looking into that issue!\r\n\r\nYour results seem to suggest that the model should be alright and the bad performance is either due to some error I have in my Android application or because my phone can't keep up.\r\n\r\nI used the pre-built benchmark tool you linked and the results are similar to my Android application. While inference is a bit faster in total, GPU inference is still slower by the same factor relative to the CPU.\r\n\r\nAverage Inference time on **CPU**: ~1s\r\n\r\n```\r\nWARNING: linker: \"/data/data/org.picsart.bgremoval/tf_test/benchmark_tflite_model\" unused DT entry: type 0xf arg 0x930\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nLog parameter values verbosely: [0]\r\nNum threads: [6]\r\nGraph: [keras_unet.tflite]\r\nEnable op profiling: [1]\r\nCSV File to export profiling data to: [benchmark_cpu.csv]\r\n#threads used for CPU inference: [6]\r\nLoaded model keras_unet.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nThe input model file size (MB): 7.70576\r\nInitialized session in 3.487ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=1 curr=1289876\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=1064709 curr=1013050 min=997794 max=1064709 avg=1.01013e+06 std=10693\r\n\r\nInference timings in us: Init: 3487, First inference: 1289876, Warmup (avg): 1.28988e+06, Inference (avg): 1.01013e+06\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=2.11719 overall=314.234\r\n```\r\n\r\nAverage Inference time on **GPU** using OpenGL backend: ~2.4s\r\n\r\n```\r\nWARNING: linker: \"/data/data/org.picsart.bgremoval/tf_test/benchmark_tflite_model\" unused DT entry: type 0xf arg 0x930\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nLog parameter values verbosely: [0]\r\nGraph: [keras_unet.tflite]\r\nEnable op profiling: [1]\r\nCSV File to export profiling data to: [benchmark_gpu.csv]\r\nUse gpu: [1]\r\nLoaded model keras_unet.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: TensorDescriptor don't have selector with name - Slice\r\nERROR: Falling back to OpenGL\r\nINFO: Initialized OpenGL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nApplied GPU delegate, and the model graph will be completely executed by the delegate.\r\nThe input model file size (MB): 7.70576\r\nInitialized session in 2101.73ms.\r\nRunning benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.\r\ncount=1 curr=2503969\r\n\r\nRunning benchmark for at least 50 iterations and at least 1 seconds but terminate if exceeding 150 seconds.\r\ncount=50 first=2345432 curr=2325535 min=2284622 max=2461721 avg=2.35429e+06 std=31681\r\n\r\nInference timings in us: Init: 2101733, First inference: 2503969, Warmup (avg): 2.50397e+06, Inference (avg): 2.35429e+06\r\nNote: as the benchmark tool itself affects memory footprint, the following is only APPROXIMATE to the actual memory footprint of the model at runtime. Take the information at your discretion.\r\nPeak memory footprint (MB): init=102.875 overall=102.875\r\n```\r\n\r\nHowever, I could not get it to run using the OpenCL GPU backend you mentioned. I have read that this should be the default behavior, but if no backend is specified, it falls back to the OpenGL backend as seen above.\r\nI have also tried enforcing the OpenCL backend by using `--gpu_backend=\"cl\"`, which failed:\r\n\r\n```\r\nWARNING: linker: \"/data/data/org.picsart.bgremoval/tf_test/benchmark_tflite_model\" unused DT entry: type 0xf arg 0x930\r\nSTARTING!\r\nDuplicate flags: num_threads\r\nLog parameter values verbosely: [0]\r\nGraph: [keras_unet.tflite]\r\nUse gpu: [1]\r\nGPU backend: [cl]\r\nLoaded model keras_unet.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nERROR: TfLiteGpuDelegate Init: TensorDescriptor don't have selector with name - Slice\r\nINFO: Created 0 GPU delegate kernels.\r\nERROR: TfLiteGpuDelegate Prepare: delegate is not initialized\r\nERROR: Node number 24 (TfLiteGpuDelegateV2) failed to prepare.\r\n\r\nERROR: Restored original execution plan after delegate application failure.\r\nFailed to apply GPU delegate.\r\nBenchmarking failed.\r\n```\r\n\r\nHere is the OP Profiling Output in case it provides any additional insight:  [benchmark_op_profiling.zip](https://github.com/tensorflow/tensorflow/files/4989590/benchmark_op_profiling.zip)\r\n\r\nSo it looks like both the Model and the Android application are fine, and the issue is somehow related to OpenCL on my device, as it works properly on yours.", "Hi Jakob,\r\n\r\nBased on outputs of the benchmark tool you pasted above (i.e. \"ERROR: TfLiteGpuDelegate Init: TensorDescriptor don't have selector with name - Slice\"), I agree with you that the OpenCL lib on your device is the culprit to the failed OpenCL GPU delegate run.  Could you tell the OpenCL version on your device?\r\n\r\nIt's also not clear to me why the OpenGL GPU performance is worse than that of CPU (which used 6 threads based on the output). It might be related to the OpenGL library on your device as well. Just wondering do you have another different Android phone to check the performance? \r\n\r\nAdded @impjdi to shed additional lights on this issue.", "Hi,\r\n\r\nyes, I could get my hands on a Samsung Galaxy S6 for comparison. Unfortunately, I do not have root access and also cannot use run-as, so I could not use your benchmark tool on the S6. However, my Android test-app behaves as expected:\r\n~710ms/frame on GPU vs. ~850ms/frame on CPU\r\n\r\nSo the S6 seems to be faster than the P20 Lite in general and inference is faster on the GPU than on the CPU. On the other hand, it also seems to use OpenGL instead of OpenCL according to the Logs:\r\n\r\n```\r\nI/tflite: Created TensorFlow Lite delegate for GPU.\r\nI/tflite: Initialized TensorFlow Lite runtime.\r\nD/libEGL: loaded /vendor/lib64/egl/libGLES_mali.so\r\nE/libEGL: validate_display:99 error 3008 (EGL_BAD_DISPLAY)\r\nE/libEGL: call to OpenGL ES API with no current context (logged once per thread)\r\nI/tflite: Initialized OpenGL-based API.\r\nI/tflite: Created 1 GPU delegate kernels.\r\n```\r\n\r\nThe OpenGL ES/OpenCL versions of the two phones (Huawei P20 Lite with Mali-T830 GPU vs. Samsung Galaxy S6 with Mali-T760 GPU) are almost identical. I used [this app](https://play.google.com/store/apps/details?id=nl.svdree.glesinfo&hl=en) and [this app](https://play.google.com/store/apps/details?id=com.robertwgh.opencl_z_android&hl=en) for checking:\r\n\r\n### **P20 Lite:**\r\n\r\n![P20Lite Drivers](https://user-images.githubusercontent.com/44022022/88913407-1ee1da00-d261-11ea-8831-24e7113a28fd.png)\r\n\r\n### **Galaxy S6:**\r\n\r\n![S6 Drivers](https://user-images.githubusercontent.com/44022022/88913425-26a17e80-d261-11ea-8260-950b8ea61b75.png)\r\n\r\nThe main difference is that the S6 has 8 compute units while the P20 Lite has only 2. According to the Performance tab, the S6 (right) also seems to have a higher float vec4 throughput:\r\n\r\n![Performance Comparison](https://user-images.githubusercontent.com/44022022/88917154-9155b880-d267-11ea-900a-9b966932720e.jpg)\r\n\r\n", "Jakob, many thanks for pasting raw perf. specs of these two phones! As you said, the GPU on S6 has a higher float vec4 throughput (>6X over the P20 Lite). I also noted that the host<->device memory BW on S6 is ~2X higher as well. Therefore, I think it's expected that the TFLite GPU performs much better on S6 than that on P20 Lite. It might be safe to conclude that it's not a good option to use GPU delegate on P20 Lite generally.", "Hi Chao, thanks for checking! Yes, that seems reasonable. Guess I have to stick with the CPU for now.\r\n\r\nOne last question before I close this issue: Which specs of the mobile GPU do I have to pay special attention to when considering to use a GPU delegate? Does TFLite extensively utilize vec4 computations in particular? ", "> Hi Chao, thanks for checking! Yes, that seems reasonable. Guess I have to stick with the CPU for now.\r\n> \r\n> One last question before I close this issue: Which specs of the mobile GPU do I have to pay special attention to when considering to use a GPU delegate? \r\nThis is a great question! And we are working to resolve this issue w.r.t. whether to recommend using GPU delegate. At the moment, I really don't have a good answer to this. But I think GPU delegate generally delivers better performance on most recent mid-range+ phones with Android 9+. \r\n\r\nDoes TFLite extensively utilize vec4 computations in particular?\r\nI don't know. @impjdi, could you help w/ this question? Thx!\r\n\r\n", "Which spec is difficult to answer.  The performance depends on many factors such as what kind of GPU you have (typically Adreno vs Mali), how large your tensors are (both runtime and trained weights), how fast your ops are (if you have too many small ops, performance hurts), and how many ops you have etc.  You can take a quick look at the kernel implementations under `gpu/delegates/{gl,cl}/kernels`, but yes, we use `vec4` a lot."]}, {"number": 41768, "title": "[TF 2.2.0] Error on creating optimizer slot variable under multi GPU training with tf.distribute.MirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Red Hat Enterprise Linux Server release 7.6 (Maipo)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): tensorflow/2.2.0--cuda--10.1\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): [GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 4x Tesla V100-SXM2-16GB\r\n\r\n\r\n**Describe the current behavior**\r\nI am using Tensorflow 2.2.0 GPU with Tensorflow's Keras API for training a GAN model with multi GPU using Distributed Tensorflow MirroredStrategy. Under the mirrored strategy scope, I've:\r\n- instantiated the model;\r\n- defined the optimizer (tf.keras.optimizer.RMSProp);\r\n- compiled the model with that optimizer.\r\n\r\nIn the train_step function I've defined the computational graph in Pythonic-way. I'm using Lazy mode (as opposed to the default TF2.0 eager mode and Keras graph mode) because TF warned me that running MirroredStrategy with eager execution still has overhead problems.\r\nWhen the execution arrives to the apply_gradients call on the optimizer of my model, TF crashes reporting me this error:\r\n\r\n`    ValueError: Trying to create optimizer slot variable under the scope for tf.distribute.Strategy (<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x20079d15d4f0>), which is different from the scope used for the original variable (MirroredVariable:{\r\n      0: <tf.Variable 'conv2d_20/kernel:0' shape=(3, 3, 2, 2) dtype=float32>,\r\n      1: <tf.Variable 'conv2d_20/kernel/replica_1:0' shape=(3, 3, 2, 2) dtype=float32>,\r\n      2: <tf.Variable 'conv2d_20/kernel/replica_2:0' shape=(3, 3, 2, 2) dtype=float32>,\r\n      3: <tf.Variable 'conv2d_20/kernel/replica_3:0' shape=(3, 3, 2, 2) dtype=float32>\r\n    }). Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope\r\n`\r\n\r\n**Describe the expected behavior**\r\nThe apply gradient should execute fine, because I've correctly defined the model and the optimizer under the MirroredStrategy object, as reported in: https://www.tensorflow.org/guide/distributed_training\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://pastebin.com/MyT6p1T8\r\n\r\n**Other info / logs**\r\n", "comments": ["Hi @DekraN, I've tried to run the code you've shared but am getting syntax errors. If I fix the syntax errors, I get the error `'Model' object has no attribute 'generator'` Can you please double check the code runs and provide it in Colab?\r\n\r\nAdditionally, although I am not able to run the code yet, there are a few things that stand out. Firstly the use of `tf.compat.v1.disable_eager_execution()` as we don't support custom losses in legacy graph mode (which is what you have if you disable eager mode). It is true that running MirroredStrategy with eager execution still has overhead problems, but that just means you need to wrap your distributed training step in a @tf.function (which it looks like you have done), and not disable eager execution all together.\r\n\r\nLastly, it doesn't look like you've distributed your dataset. As explained [here](https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_custom_training_loops), you'll need `tf.distribute.Strategy.experimental_distribute_dataset` when using MirroredStrategy with a custom training loop.\r\nHappy to provide further support if you can share reproducible code.", "Greatly thanks for the answer! I was looking for a confirm regarding the execution modality to use in TF2.2.0. In fact, in my code I use custom losses, therefore I should use normal eager execution instead of legacy graph mode.\r\nNow, after some other issues, I've been successful in executing with multi-GPU my code in eager execution\r\nSorry for the not reproducible code.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41768\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41768\">No</a>\n", "@DekraN Could you please tell me how exactly do you fix the code? I am encountering the same issues."]}, {"number": 41767, "title": "quantize deeplab model trained on PASCAL VOC\uff0cTypeError: unsupported operand type(s) for +: 'NoneType' and 'float'", "body": "When I change quantize_delay_step=-1 to quantize_delay_step=0, I run the following command, \r\npython deeplab/train.py \\\r\n    --logtostderr \\\r\n    --training_number_of_steps=3000 \\\r\n    --train_split=\"train\" \\\r\n    --model_variant=\"mobilenet_v2\" \\\r\n    --output_stride=16 \\\r\n    --train_crop_size=\"513,513\" \\\r\n    --train_batch_size=8 \\\r\n    --base_learning_rate=3e-5 \\\r\n    --dataset=\"pascal_voc_seg\" \\\r\n    --quantize_delay_step=0 \\\r\nI get an error\uff1a\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 464, in <module>\r\n    tf.app.run()\r\n  File \"/home/sherry/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/home/sherry/.local/lib/python3.6/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/home/sherry/.local/lib/python3.6/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"train.py\", line 391, in main\r\n    quant_delay=FLAGS.quantize_delay_step)\r\n  File \"/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py\", line 122, in create_training_graph\r\n    freeze_bn_delay=freeze_bn_delay)\r\n  File \"/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/quantize_graph.py\", line 73, in _create_graph\r\n    is_training=is_training)\r\n  File \"/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 53, in FoldBatchNorms\r\n    graph, is_training, freeze_batch_norm_delay=freeze_batch_norm_delay)\r\n  File \"/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 98, in _FoldFusedBatchNorms\r\n    freeze_batch_norm_delay=freeze_batch_norm_delay))\r\n  File \"/home/sherry/.local/lib/python3.6/site-packages/tensorflow/contrib/quantize/python/fold_batch_norms.py\", line 384, in _ComputeBatchNormCorrections\r\n    match.moving_variance_tensor + match.batch_epsilon)\r\nTypeError: unsupported operand type(s) for +: 'NoneType' and 'float'\r\n", "comments": ["@shakey-cuimiao,\r\nCould you please provide the TensorFlow version you are using and the complete code or the sequence of commands to reproduce the issue reported here. Thanks!\r\n\r\n", "@amahendrakar\r\nTensorflow Version\uff1a1.14.0\r\nI required to quantize DeepLab model and convert it to TFLite\u3002The main steps: Quantization-aware training\u3002\r\nWhen I change quantize_delay_step=-1 to quantize_delay_step=0, I run the following command\uff1a\r\npython train.py\r\n--logtostderr\r\n--training_number_of_steps=3000\r\n--train_split=\"train\"\r\n--model_variant=\"mobilenet_v2\"\r\n--output_stride=16\r\n--train_crop_size=\"513,513\"\r\n--train_batch_size=8\r\n--base_learning_rate=3e-5\r\n--dataset=\"pascal_voc_seg\"\r\n--quantize_delay_step=0", "@shakey-cuimiao,\r\nTensorFlow v1.14 is not actively supported. Please update TensorFlow to 2.x or v1.15 and check if you are facing the same issue. \r\n\r\nAlso, please take a look at [this similar](https://github.com/tensorflow/tensorflow/issues/20867#issuecomment-422246723) issue and let us know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I was able to solve this issue by upgrading tensorflow version from 1.15 to 1.15.3.", "@balazsbanto,\r\nThank you for the update. Marking this issue as close since it is resolved."]}, {"number": 41766, "title": "TypeError: '<' not supported between instances of 'NoneType' and 'int'", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n![pye1](https://user-images.githubusercontent.com/66160761/88523988-69234b00-d016-11ea-9c74-0b00bb25d371.PNG)\r\n", "comments": ["@aamina-cybex \r\nPlease provide with simple indented stand alone code and tf version such that we could replicate the issue faced or if possible share colab gist with the error faced.\r\n\r\nWith respect to the error reported refer to below links and let us know if it helps:\r\n#36358 #29917 [link](https://stackoverflow.com/questions/43708541/python-typeerror-typeerror-not-supported-between-instances-of-nonetype) ", "Hi\r\n\r\nI am new in GitHub so can you please guide me how to share complete code here. I did this python code in PyCharm. ", "@aamina-cybex\r\nplease let us know if you referred to the links, you can attach files  as mention in below box \"drag and drop or click and attach\", or update the issue template, share a colab gist if possible with the error faced.", "Hi\r\nI have shared these codes as you advised. \r\n\r\n![p1](https://user-images.githubusercontent.com/66160761/88759391-529d0100-d188-11ea-894a-a9d3f1a180a1.PNG)\r\n![p2](https://user-images.githubusercontent.com/66160761/88759392-53ce2e00-d188-11ea-8d72-51d30b171063.PNG)\r\n\r\n\r\n", "@aamina-cybex\r\nPlease provide with simple indented stand alone code and tf version such that we could replicate the issue faced in text format or if possible share colab gist with the error faced.\r\nAlso have you referred to the links shared with similar error.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41766\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41766\">No</a>\n"]}, {"number": 41765, "title": "Build windows x86 c++ library failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  NA\r\n- TensorFlow installed from (source or binary):  source\r\n- TensorFlow version:    r2.2\r\n- Python version:  3.7.6\r\n- Installed using virtualenv? pip? conda?:   conda \r\n- Bazel version (if compiling from source):    2.0.0\r\n- GCC/Compiler version (if compiling from source):  vs2019 \r\n- CUDA/cuDNN version:   NA\r\n- GPU model and memory:   NA\r\n\r\n\r\n**Describe the problem**\r\nI build tensorflow r2.2 branch, I use this commands\r\n> bazel build -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow\r\nbazel build -c opt //tensorflow/lite:tensorflowlite\r\n\r\nthis is right, but this is x64 lib. I want to build x86 lib, then I use:\r\n> bazel build --cpu=x86 -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow\r\n\r\nbut get tolchain does not contain a toolchain for cpu 'x86'.\r\n\r\n> D:\\IdeaProjects\\tensorflow>bazel build --cpu=x86 -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=168\r\nINFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/ProgramData/Anaconda3/python.exe\r\nINFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages --python_path=C:/ProgramData/Anaconda3/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file d:\\ideaprojects\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file d:\\ideaprojects\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:opt in file d:\\ideaprojects\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:windows in file d:\\ideaprojects\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\ideaprojects\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Build options --copt and --define have changed, discarding analysis cache.\r\nERROR: C:/users/administrator/_bazel_administrator/rqkipcky/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86'\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\nINFO: Elapsed time: 1.693s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (11 packages loaded, 118 targets configured)\r\n    Fetching @nsync; fetching\r\n\r\n", "comments": ["@Bluewind001 \r\n\r\nThis issue is more suitable on Continuum [Anaconda repo](https://github.com/ContinuumIO/anaconda-issues/issues) since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).Thanks!", "I change to my pure python env, get same error.\r\n\r\n> D:\\IdeaProjects\\tensorflow> bazel build --cpu=x86 -c opt  --config=opt --local_ram_resources=2048  //tensorflow/lite:tensorflowlite\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=168\r\nINFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=D:/mypython/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow\\.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=D:/mypython/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=D:/mypython/Python/Python37/lib/site-packages --python_path=D:/mypython/Python/Python37/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file d:\\ideaprojects\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file d:\\ideaprojects\\tensorflow\\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:opt in file d:\\ideaprojects\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\nINFO: Found applicable config definition build:windows in file d:\\ideaprojects\\tensorflow\\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file d:\\ideaprojects\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz failed: class java.io.IOException connect timed out\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/bazel-toolchains/archive/92dd8a7a518a2fb7ba992d47c8b38299fe0be825.tar.gz failed: class javax.net.ssl.SSLProtocolException Read timed out\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at C:/users/administrator/_bazel_administrator/rqkipcky/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):\r\n \\- C:/users/administrator/_bazel_administrator/rqkipcky/external/bazel_toolchains/repositories/repositories.bzl:37:9\r\n \\- D:/ideaprojects/tensorflow/WORKSPACE:37:1\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/bazel-skylib/releases/download/0.9.0/bazel_skylib-0.9.0.tar.gz failed: class java.io.IOException connect timed out\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_swift/releases/download/0.12.1/rules_swift.0.12.1.tar.gz failed: class java.io.IOException connect timed out\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd.tar.gz failed: class java.io.IOException connect timed out\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_apple/releases/download/0.18.0/rules_apple.0.18.0.tar.gz failed: class java.io.IOException connect timed out\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/apple_support/releases/download/0.7.1/apple_support.0.7.1.tar.gz failed: class java.io.IOException connect timed out\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_cc/archive/01d4a48911d5e7591ecb1c06d3b8af47fe872371.zip failed: class java.io.IOException connect timed out\r\nERROR: C:/users/administrator/_bazel_administrator/rqkipcky/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86'\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz failed: class java.io.IOException connect timed out\r\nWARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_java/archive/7cf3cefd652008d0a64a419c34c13bdca6c8f178.zip failed: class java.io.IOException connect timed out\r\nERROR: Analysis of target '//tensorflow/lite:tensorflowlite' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\nINFO: Elapsed time: 0.323s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n", "The error is a timeout issue during downloading the packages. Please check your network connection.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> _Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template_\r\n> \r\n> **System information**\r\n> \r\n> * OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Windows 10\r\n> * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  NA\r\n> * TensorFlow installed from (source or binary):  source\r\n> * TensorFlow version:    r2.2\r\n> * Python version:  3.7.6\r\n> * Installed using virtualenv? pip? conda?:   conda\r\n> * Bazel version (if compiling from source):    2.0.0\r\n> * GCC/Compiler version (if compiling from source):  vs2019\r\n> * CUDA/cuDNN version:   NA\r\n> * GPU model and memory:   NA\r\n> \r\n> **Describe the problem**\r\n> I build tensorflow r2.2 branch, I use this commands\r\n> \r\n> > bazel build -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow\r\n> > bazel build -c opt //tensorflow/lite:tensorflowlite\r\n> \r\n> this is right, but this is x64 lib. I want to build x86 lib, then I use:\r\n> \r\n> > bazel build --cpu=x86 -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow\r\n> \r\n> but get tolchain does not contain a toolchain for cpu 'x86'.\r\n> \r\n> > D:\\IdeaProjects\\tensorflow>bazel build --cpu=x86 -c opt  --config=opt --local_ram_resources=2048  //tensorflow/tools/lib_package:libtensorflow\r\n> > INFO: Options provided by the client:\r\n> > Inherited 'common' options: --isatty=1 --terminal_columns=168\r\n> > INFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow.bazelrc:\r\n> > Inherited 'common' options: --experimental_repo_remote_exec\r\n> > INFO: Options provided by the client:\r\n> > 'build' options: --python_path=C:/ProgramData/Anaconda3/python.exe\r\n> > INFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow.bazelrc:\r\n> > 'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\n> > INFO: Reading rc options for 'build' from d:\\ideaprojects\\tensorflow.tf_configure.bazelrc:\r\n> > 'build' options: --action_env PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/lib/site-packages --python_path=C:/ProgramData/Anaconda3/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0\r\n> > INFO: Found applicable config definition build:v2 in file d:\\ideaprojects\\tensorflow.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n> > INFO: Found applicable config definition build:xla in file d:\\ideaprojects\\tensorflow.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\n> > INFO: Found applicable config definition build:opt in file d:\\ideaprojects\\tensorflow.tf_configure.bazelrc: --copt=/arch:AVX --define with_default_optimizations=true\r\n> > INFO: Found applicable config definition build:windows in file d:\\ideaprojects\\tensorflow.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false\r\n> > INFO: Found applicable config definition build:monolithic in file d:\\ideaprojects\\tensorflow.bazelrc: --define framework_shared_object=false\r\n> > INFO: Build options --copt and --define have changed, discarding analysis cache.\r\n> > ERROR: C:/users/administrator/_bazel_administrator/rqkipcky/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86'\r\n> > ERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed; build aborted\r\n> > INFO: Elapsed time: 1.693s\r\n> > INFO: 0 processes.\r\n> > FAILED: Build did NOT complete successfully (11 packages loaded, 118 targets configured)\r\n> > Fetching @NSYNC; fetching\r\n\r\nWhy this tell my x86 error?", "@Bluewind001, We are checking to see if you still need help on this issue. Can you try building the latest stable version of TF i.e `2.6.0` and let us know if the issue persists? You can use this [guide](https://www.tensorflow.org/install/source_windows) for your reference and take a look at this [link](https://www.tensorflow.org/install/source_windows#tested_build_configurations) for the tested build configs.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41765\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41765\">No</a>\n"]}, {"number": 41764, "title": "Run benchmark models get hanged when use grpc+gdr communication.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r1.15\r\n- Python version: python3.6\r\n- Bazel version (if compiling from source): 0.26.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: CUDA10.1\r\n- GPU model and memory: V100 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I compile tensorflow r1.15 version from source code with the gdr support. The compile command line is as following.\r\n`bazel build --config=opt --config=cuda --config=gdr //tensorflow/tools/pip_package:build_pip_package`\r\nAfter I run the benchmark models, I get the process hang with the following output.\r\n\r\n> WARNING:tensorflow:From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nnon-resource variables are not supported in the long term\r\n2020-07-27 15:27:24.230627: I tensorflow/contrib/gdr/gdr_memory_manager.cc:220] RDMA server is listening on 10.0.9.2:10001\r\n2020-07-27 15:27:24.230719: I tensorflow/contrib/gdr/gdr_memory_manager.cc:86] NUMA node for device: mlx5_0 is 0\r\n2020-07-27 15:27:24.230763: I tensorflow/contrib/gdr/gdr_memory_manager.cc:250] Instrumenting CPU allocator(s)\r\n2020-07-27 15:27:24.245919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz\r\n2020-07-27 15:27:24.246089: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f6a630 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-27 15:27:24.246112: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-27 15:27:24.248568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-27 15:27:26.772045: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5f72cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-27 15:27:26.772110: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2020-07-27 15:27:26.776200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties:\r\nname: Tesla V100-SXM2-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:89:00.0\r\n2020-07-27 15:27:26.776621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-27 15:27:26.780467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-27 15:27:26.784188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-27 15:27:26.784806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-27 15:27:26.787654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-27 15:27:26.788934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-27 15:27:26.793107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-27 15:27:26.795799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\r\n2020-07-27 15:27:26.795862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-27 15:27:26.798030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-27 15:27:26.798050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0\r\n2020-07-27 15:27:26.798057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N\r\n2020-07-27 15:27:26.800718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 30584 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)\r\n2020-07-27 15:27:26.806113: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> 10.0.9.2:10009, 1 -> localhost:10001}\r\nbegin a gdr server\r\n2020-07-27 15:27:26.810425: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:10001\r\nWARNING:tensorflow:From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.keras.layers.Conv2D` instead.\r\nW0727 15:27:26.848110 139800816277312 deprecation.py:323] From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.keras.layers.Conv2D` instead.\r\nWARNING:tensorflow:From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nW0727 15:27:26.850684 139800816277312 deprecation.py:323] From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `layer.__call__` method instead.\r\nWARNING:tensorflow:From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.MaxPooling2D instead.\r\nW0727 15:27:26.874744 139800816277312 deprecation.py:323] From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.MaxPooling2D instead.\r\nWARNING:tensorflow:From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:408: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.dropout instead.\r\nW0727 15:27:26.954734 139800816277312 deprecation.py:323] From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:408: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse keras.layers.dropout instead.\r\nWARNING:tensorflow:From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nW0727 15:27:27.005284 139800816277312 deprecation.py:323] From /home/cengguang/zcg/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\nWARNING:tensorflow:From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2268: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\nW0727 15:27:27.223294 139800816277312 deprecation.py:323] From /home/cengguang/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2268: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\nINFO:tensorflow:Running local_init_op.\r\nI0727 15:27:28.615225 139800816277312 session_manager.py:500] Running local_init_op.\r\n2020-07-27 15:27:28.770876: I tensorflow/contrib/gdr/gdr_memory_manager.cc:572] RDMA endpoint connected to rdma://10.0.9.2:10009\r\nINFO:tensorflow:Done running local_init_op.\r\nI0727 15:27:29.256539 139800816277312 session_manager.py:502] Done running local_init_op.\r\n2020-07-27 15:27:29.479763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-27 15:27:29.801486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-07-27 15:27:31.455157: I tensorflow/contrib/gdr/gdr_memory_manager.cc:279] Accepted new RDMA connection\r\n\r\n**Describe the expected behavior**\r\nExpected behavior should run the code smoothly and exit, BUT get stucked.\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nAfter compile the source code with gdr support, you can use the following launch scripts:\r\n`export CUDA_VISIBLE_DEVICES='0'\r\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64/\r\npython tf_cnn_benchmarks.py \\\r\n        --controller_host=controller_ip \\\r\n        --worker_hosts=worker1_ip,worker2_ip\\\r\n        --variable_update=collective_all_reduce \\\r\n        --all_reduce_spec=collective \\\r\n        --job_name=worker \\\r\n        --use_fp16 \\\r\n        --batch_size=128 \\\r\n        --num_gpus=1 \\\r\n        --model=alexnet \\\r\n        --task_index=0 \\\r\n        --force_gpu_compatible \\\r\n        --server_protocol=grpc+gdr 2>&1| tee output1.txt`\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@junshi15 , @shamoya , @byronyi  can you give me some suggestions?", "When I check the Mellanox mlx5 Linux Counters and Status Parameters, I find that the following counters increase when the training process is hanged.\r\nContent of rnr_nak_retry_err is: 220 -> 282 still increasing\r\nContent of out_of_buffer is: 220 -> 282 still increasing\r\nYou can check the counters using following scripts:\r\n```\r\nCOUNTERS=`ls /sys/class/infiniband/mlx5_0/ports/1/hw_counters/`\r\nARRAY=(${COUNTERS///})\r\nfor var in ${ARRAY[@]}\r\ndo\r\n  echo \"Content of $var is:\"\r\n  cat /sys/class/infiniband/mlx5_0/ports/1/hw_counters/$var\r\n  echo \"\"\r\ndone\r\n```", "I have not tracked this piece of work for a while, maybe @shamoya , @byronyi are more up-to-date on this.", "Same here. also not a Mellanox employee anymore.\r\nFrom a short glimpse of the description, looks like you're running on a lossy Ethernet fabric. \r\nTry moving to lossless ethernet configuration (global pause / PFC) and see it reproduces.\r\n\r\nCheers", "@lalalapotter Did you try moving to lossless ethernet configuration (global pause / PFC) and were you still able to reprodce this issue? Thanks!", "@gowthamkpr Hi, I can still reproduce the issue, it seems the problem is not related to the ethernet configuration. Do you have any other suggestions? Or can you try the grpc+gdr protocol?", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41764\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41764\">No</a>\n"]}, {"number": 41763, "title": "Please list XLA flags", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/xla\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nIt's hard to find a list of XLA environment variables / flags, and examples of how/when to use them, like: \r\n\r\n```\r\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\r\nos.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\r\n```\r\n\r\n### Usage example\r\n\r\nUsage examples are buried across many different open/closed github issues\r\n\r\nDesired usage:\r\n\r\n```\r\ndef run_agent_on_env(agent, env_name):\r\n  env = gym.make(env_name)\r\n  obs = env.reset()\r\n  action = agent(obs) # how do you make this the only thing to run on XLA?\r\n  obs, reward, done, info = env.step(action)\r\n  # ... loop ... \r\n  return total_reward\r\n\r\ndef main():\r\n  agents = get_agents()\r\n  with futures.ProcessPoolExecutor() as pool:\r\n      jobs = [pool.submit(run_agent_on_env, (agent, name)) for agent, name in ...]\r\n      for future in futures.as_completed(jobs):\r\n           # optimize stuff\r\n           # log stuff\r\n           # submit more jobs\r\n```\r\nProblem is, XLA runs OOM\r\n\r\n### Request visuals, if applicable\r\n\r\nXLA configuration often makes the difference between running, and not running code, given memory issues etc. A simple table of XLA environment variables / \"flags\" would be hugely useful for users \r\n\r\n### Submit a pull request?\r\n\r\nI can't", "comments": ["Can I submit a PR to fix this? Thank you! @r4nt \r\nWhere do you prefer to add the usage example? Could you be more specific on that? Such as before which section, which line... @bionicles ", "A question, where is the source code of this issue? I couldn't find where is the code pattern that I can create PR with. @ymodak ", "Not sure about where to put it, it would be epic to have a function in tensorflow which lists the xla flags, or just a simple page of the docs with a table of them, I don't mind where it is, but XLA has a lot of feature flags and it's good if we can find the list!", "hi, @bionicles,\r\nwith reference to your above reply, what I understood is need of doc with listed XLA feature flags\r\ncorrect me if I am wrong, as I am new to open source contribution\r\ncan I work on this if this is still open\r\nthanks", "All the flags which are allowed are currently listed in the source code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/jit/flags.cc#L82-L142) along with the description. You can refer the same.\r\nIf you want the same to be included in the document let us know, will create a PR for the same. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41762, "title": "[doc] Enrich doc for jit_scope for clarification", "body": "This pr adds more doc to `tf.xla.experimental.jit_scope` to clarify possibly confusion mentioned in #41626.\r\n\r\nThank you for your time on reviewing this pr.\r\n\r\nGently ping @cheshire :)", "comments": []}, {"number": 41761, "title": "XLA_CPU_JIT unsupports  NonMaxSuppressionV2 ?", "body": "\r\n- Model: mtcnn downloaded [here](https://github.com/blaueck/tf-mtcnn)\r\n- OS: Linux WSL 4.4.0-19041-Microsoft\r\n- TensorFlow: Source code [@a50de3e](https://github.com/tensorflow/tensorflow/tree/a50de3e381ccb6d0b37a0bfe5d033cf312b53955)\r\n- Python version: 2.7\r\n- Bazel version: 0.11.0 (binary installer installed)\r\n\r\nAfter hacking according to [this repo](https://github.com/nuchi/tf-to-xla-to-wasm)\r\n\r\ncmd\r\n```bash\r\n\r\nbazel-bin/tensorflow/compiler/aot/tfcompile \\\r\n    --target_triple=\"wasm32-unknown-unknown-wasm\" \\\r\n    --target_cpu=\"generic\" \\\r\n    --xla_cpu_multi_thread_eigen=false \\\r\n    --graph=\"./mtcnn.pbtxt\" \\\r\n    --config=\"./mtcnn_config.pbtxt\" \\\r\n    --out_function_object=\"out_model.o\" \\\r\n    --out_header=\"out_header.h\" \\\r\n    --out_metadata_object=\"out_helper.o\" \\\r\n    --cpp_class=\"MyClass\"\r\n```\r\n\r\nlog\r\n``` bash\r\nINVALID ARGUMENTS: Detected unsupported operations when trying to compile graph tfcompile on XLA_CPU_JIT:NonMaxSuppressionV2\r\n```\r\n\r\n\r\n", "comments": ["@chaozju I think you are using an old version of tensorflow. Can you pleas use tensorflow 1.15 or 1.14 and let me know if the problem still persists. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41761\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41761\">No</a>\n"]}, {"number": 41760, "title": "Can't change model attribute inside @tf.function with custom training loop.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n@tf.function with custom training loop can't change the attribute of the model. For example, after 10k steps, I want to set some layer to non-trainable by `model.layer[0].trainable = False`, or in case training GAN, after 10k steps, the discriminator start to train. \r\n\r\n**Describe the expected behavior**\r\nWithout @tf.function everything is fine but the training will slow compared with graph mode.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nHere is a simple example that I take from custom_training_walkthough colab and modify a bit. (https://colab.research.google.com/drive/1Rv0YTBq6qISvt-EFebYr4U_RUdjdSWDe?usp=sharing). \r\n\r\nThe main code I changed is here: \r\n```\r\n    if steps == 1:\r\n      print(\"trainable=True\")\r\n      model.layer0.trainable = True\r\n      model.layer1.trainable = True\r\n      model.layer2.trainable = True\r\n\r\n    if steps == 100:\r\n      print(\"trainable=False\")\r\n      model.layer0.trainable = False\r\n      model.layer1.trainable = False\r\n      model.layer2.trainable = False  \r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n```\r\nValueError: tf.function-decorated function tried to create variables on non-first call.\r\n```\r\n\r\nI also have an example in training GAN where I enable discriminator training after N steps but nothing happened, just like the code inside the if condition is not run. See (https://github.com/TensorSpeech/TensorflowTTS/blob/master/examples/multiband_melgan/train_multiband_melgan.py#L157-L166). The workaround is that I MUST to training generator with N steps then resume to training both G and D. ", "comments": ["I was able to reproduce the issue with TF v2.2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/385c4e588b00f47665899b95e261ff2d/41760.ipynb). ", "@dathudeptrai,\r\nCould you please check [this workaround](https://github.com/tensorflow/tensorflow/issues/27120#issuecomment-540071844) from a similar issue and let us know if it helps. Thanks!", "@amahendrakar I agree that the native solution is create multiple forward functions for each condition.  But don't you think this solution is not good interms of coding style?. Especially when you want to apply very complicated training procedures such as GAN, etc. Is there any good solution to replace this native solution ? ", "I had a look, I think there are two issues highlighted in this bug:\r\n\r\n1. calling `model(x, training=False)` and then `model(x, training=True)` leads to error. This looks like a bug, but it can be worked around by decorating `call` with `@tf.function`. Ideally, Keras should rebuild the graph when the `training` argument changes.\r\n2. it's unclear whether changing the layer's trainable attribute has the expected effect - @tomerk will Keras rebuild the graph when the `trainable` attribute of various layers is being changed? I think that would be the expected behavior.\r\n\r\nI think both cases should be handled without the need for extra workarounds, although they may require additional smarts inside Keras to handle that correctly. I do agree that users should not need to resort to building multiple versions of the same model.", "As far as I can tell this doesn't have anything to do with Keras specifically. (and it doesn't even use Keras's own tf.functioning logic in fit/evaluate)\r\n\r\ntf.functions will only capture python state at the time when they are first traced, so changing any python attribute after the first time you call the `tf.function`'d `grad` won't do anything.\r\n@mdanatg I suppose the tf.function guide might be insufficient, because it seems to only talk about python side effects w/in the tf.function ( https://www.tensorflow.org/guide/function#python_side_effects ) but doesn't mention that changes to python state that happen outside the function will only be reflected in new traces.\r\n\r\nIf you want the trainable state to be reflected w/in your tf.function, you need to make sure to use different traces when you want to set the model.trainable to True and when you wan't to set it to False. You could do this by making your `grad` method take a `trainable` python boolean argument, then set the model's state at the start of `grad` inside of the tf.function tracing. (Just make sure to reset trainable at the end of your trace)", "From your notebook, this is the problem:\r\n\r\n```\r\n@tf.function\r\ndef grad(model,...):\r\n  ...\r\n\r\ngrad(model, ...)\r\n\r\nmodel = SmallModel()\r\n\r\ngrad(model, ...)\r\n```\r\nFor now, if you need to call a `tf.function` on multiple objects (that initialize their variables in the first call) you need to create a separate `@tf.function` for each instance. Or define it as a method.\r\n\r\nSee: https://github.com/keras-team/keras-io/pull/334\r\n\r\n```\r\ndense1 = tf.keras.layers.Dense(3)\r\ndense2 = tf.keras.layers.Dense(3)\r\n\r\n@tf.function\r\ndef fun(dense, input):\r\n  return dense(input)\r\n  \r\nfun(dense1, input)\r\nfun(dense2, input)\r\n```\r\n```\r\nValueError: tf.function-decorated function tried to create variables on non-first call.\r\n```\r\n\r\n\r\nThat explains the `non-first call.` error. But everything else is still true. You shouldn't change an object's state and expect an old trace to pick up the difference.", "Was able to reproduce the issue in TF v2.5,please find the gist[ here](https://colab.research.google.com/gist/sushreebarsa/cdf0d9d892362efc329004c7405cde4d/untitled111.ipynb)..Thanks !", "@dathudeptrai\r\nDid you figure out a way to switch between training generator and discriminator based on an attribute? I'm working on a similar problem where I also want to train one network for a given number of steps and then switch between training the first and the second. A minimal example can be found [here](https://colab.research.google.com/drive/1qhz0bTGwYvwTSVxNr7WC7kCOgYXXNV0p?usp=sharing). Any suggestions?", "@kaijennissen you need use self.optimizer.iterations % 2 == 0 for if condition for example. Do not use any python scalar, boolean :D. ", "@dathudeptrai That was fast, thanks!", "@dathudeptrai Unfortunately that doesn't work. Using f.e. `if self.w_optimizer.iterations < 50:` yields the following error message \r\n```OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.```\r\nThis is strange because from my understand of the [docs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/control_flow.md#if-statements)  this should be converted.\r\n", "@kaijennissen you need wrap `train_step` by @tf.function or use tf.cond rather than if. BTW, you need to use `Tensor` to perform if condition, do not use any python object in the if condition. For example, if the condition is `if A <= 5`, A must be a tensor object such as optimizer.iterations, or tf.Variable.\r\n\r\n```\r\n@tf.function\r\ndef train_step(self, data):\r\n....\r\n```", "> For now, if you need to call a `tf.function` on multiple objects (that initialize their variables in the first call) you need to create a separate `@tf.function` for each instance. Or define it as a method.\r\n\r\n@MarkDaoust your arguments about graph construction are clear. And your example is quite obvious. But can you, please, point the piece of @dathudeptrai code where variables are going to be created? \r\n\r\nAs I can see, \r\n```\r\n@tf.function\r\ndef grad(model, inputs, targets, optimizer):\r\n  with tf.GradientTape() as tape:\r\n    loss_value = loss(model, inputs, targets, training=True)\r\n  grads = tape.gradient(loss_value, model.trainable_variables)\r\n  optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n```\r\nall variables should be created once and used every time you call `grad` function. Is there any hidden variable creation?", "> all variables should be created once and used every time you call grad function. Is there any hidden variable creation?\r\n\r\nRight. For any particular model & optimizer the variables will only be created once. But,  a separate trace is created for each new combinations of python object-args. The problem is that for a stand-alone function the \"no variables created after the first call\" logic is global, it's not per trace.\r\n\r\nSo the way to fix this is to move this to a method on the Model class. For methods the \"no variables created after the first call\" logic is tracked per object instead of per function.\r\n\r\n(It looks like you're writing a [\"train_step\" method](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit))\r\n\r\n```\r\nclass MyModel(keras.Model):\r\n    ...\r\n    @tf.function\r\n    def train_step(self, inputs, targets):\r\n      with tf.GradientTape() as tape:\r\n        loss_value = self.loss(inputs, targets, training=True)\r\n      grads = tape.gradient(loss_value, model.trainable_variables)\r\n      self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\r\n    \r\n    \r\nmodel = MyModel(...)\r\nmodel.compile(loss = loss, optimizer=optimizer)\r\nmodel.train_step(...)\r\n\r\nmodel2 = MyModel(...)\r\nmodel2.compile(loss = loss, optimizer=optimizer)\r\nmodel2.train_step(...)\r\n```\r\n\r\nYou _can_ also create a separate function for each, but this is messier:\r\n\r\n```\r\ndef get_step_function()\r\n    @tf.function\r\n    def grad(model, inputs, targets, optimizer):\r\n        with tf.GradientTape() as tape:\r\n          loss_value = loss(model, inputs, targets, training=True)\r\n        grads = tape.gradient(loss_value, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n    return grad\r\n\r\nmodel1 = ...\r\nmodel2 = ...\r\n\r\nf1 = get_step_function()\r\nf1(model1,...)\r\n\r\nf2 = get_step_function()\r\nf2(model1,...)\r\n```\r\n\r\n> I also have an example in training GAN where I enable discriminator training after N steps but nothing happened, just like the code inside the if condition is not run. See (https://github.com/TensorSpeech/TensorflowTTS/blob/master/examples/multiband_melgan/train_multiband_melgan.py#L157-L166). The workaround is that I MUST to training generator with N steps then resume to training both G and D.\r\n\r\nhttps://github.com/TensorSpeech/TensorFlowTTS/blob/a280b619ec399c37609d8d9db335ffdb0816dd2e/examples/multiband_melgan/train_multiband_melgan.py#L157\r\n\r\ntf.function writes down all the operations that it encounters to create a Graph. It only picks up control flow ops if they act on a TenosrFlow object (Tensor or Variable) so my bet is: on that line both `if self.steps >= self.config[\"discriminator_train_start_steps\"]:` arguments to `>=` are python ints, not tensors. If you change a python value after tracing, tensorflow can't see the new value.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41760\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41760\">No</a>\n"]}, {"number": 41759, "title": "GPU memory issue with triton server 20.03 ", "body": "I know Tf uses Best-Fit with Coalescing to deal GPU memory. \r\nSome oom issues always happen when I inferences lots of models at the same time. So I just take some tests to check the issue. \r\n\r\nI have two models A/B trained by tenforflow1.13 and I inference by  triton server 20.03.\r\n\r\nWhen I only load A model with warm up in triton sever, I find GPU memory is 3G.\r\nWhen I only load B model with warm up in triton sever, I find GPU memory is 4G.\r\nWhen I load A model and B model with warm up at the same time in triton sever, I find GPU memory is 4.1G. Why not 7G?\r\n\r\nIf I inference A/B at the same time, should GPU memory increase from 4.1G to 7G?\r\n\r\n\r\n", "comments": ["How are you measuring the amount of allocated memory?  And does Triton share the same memory allocator between different inference models?", "> How are you measuring the amount of allocated memory?  And does Triton share the same memory allocator between different inference models?\r\n\r\nI just only run or load with warm-up one model and check gpu memory usage by nvidia-smi.\r\nIn general, the gpu memory usage remains unchanged after models inference once.\r\n\r\nAbout triton, it seems to deal gpu memory by different backends and some triton info likes the follows link:\r\nhttps://github.com/NVIDIA/triton-inference-server/issues/1507#issuecomment-630926105\r\nhttps://github.com/NVIDIA/triton-inference-server/issues/1499", "> I just only run or load with warm-up one model and check gpu memory usage by nvidia-smi.\r\n\r\nDoes Triton run the inference models on different streams?  Or does it run them on a single stream?  If the latter then this is not surprising -- the TF memory allocator must be reusing memory allocated and freed by the first model for the second model.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information."]}, {"number": 41758, "title": "Encountered unresolved custom op: Slice.", "body": "I have converted pb to tflite,When I transplant algorithms,it appears the following error:\r\njava.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Encountered unresolved custom op: Slice.\r\n    Node number 0 (Slice) failed to prepare.\r\nplease help me.", "comments": ["@zhaojc001 \r\n\r\nCan you add below 3 steps before converting would help.\r\n```\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\nconverter.experimental_new_converter =True\r\ntflite_model = converter.convert()\r\n```\r\nAlso, request you to fill issue template  and share reproducible code in colab if you are still facing the issue.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41757, "title": "[Wsign-compare] warning resolutions, by directory, 10", "body": "Resolutions affecting directories:\r\n`tensorflow/lite/`\r\n`tensorflow/python/`\r\n`tensorflow/stream_executor/`", "comments": ["@mihaimaruseac "]}, {"number": 41756, "title": "[Wsign-compare] warning resolutions, by directory, 9", "body": "Resolution affecting directories:\r\n`tensorflow/core/`\r\n\r\nset 3 of 3", "comments": ["@mihaimaruseac ", "> Please remove unrelated changes\r\n\r\nUnrelated changes removed. ", "updates applied.", "updates applied, 2. ", "recommended updates applied."]}, {"number": 41755, "title": "[Wsign-compare] warning resolutions, by directory, 8", "body": "Resolution affecting directories:\r\n`tensorflow/core/`\r\n\r\nset 2 of 3", "comments": ["All recommended updates applied. ", "updates applied, 2.\r\n\r\n"]}, {"number": 41754, "title": "[Wsign-compare] warning resolutions, by directory, 7", "body": "Resolution affecting directories:\r\n`tensorflow/core/`\r\n\r\nset 1 of 3", "comments": ["@mihaimaruseac ", "All updates applied"]}, {"number": 41753, "title": "[Wsign-compare] warning resolutions, by directory, 6", "body": "Resolution affecting directories:\r\n`tensorflow/compiler/xla`,\r\n`tensorflow/compiler/xla/service`\r\n\r\n@mihaimaruseac ", "comments": ["`tensorflow/compiler/xla` is also covered y #41752.", "> `tensorflow/compiler/xla` is also covered y #41752.\r\n\r\nYes, they are essentially 2 batches. ", "updates applied.", "updates applied, 2. ", "This seems to cause build failures. Can you investigate please?", "> This seems to cause build failures. Can you investigate please?\r\n\r\n++, will investigate."]}, {"number": 41752, "title": "[Wsign-compare] warning resolutions, by directory, 5", "body": "Resolution affecting directories:\r\n`tensorflow/compiler/xla/`", "comments": ["updates applied.", "updates applied, 2. ", "updates applied, 2. ", "updates applied, 3. ", "update applied, 4. ", "update applied, 5. "]}, {"number": 41751, "title": "[Wsign-compare] warning resolutions, by directory, 4", "body": "Resolution affecting directories:\r\n`tensorflow/compiler/tf2xla`", "comments": ["@mihaimaruseac \r\n", "updates applied. ", "recommended updates applied.", "updates applied, 3."]}, {"number": 41750, "title": "[Wsign-compare] warning resolutions, by directory, 3", "body": "Resolution affecting directory:\r\n`tensorflow/compiler/mlir/`\r\n\r\n@mihaimaruseac ", "comments": ["updates applied."]}, {"number": 41749, "title": "[Wsign-compare] warning resolutions, by directory, 2", "body": "Resolution affecting directory:\r\n`tensorflow/compiler/mlir/lite`\r\n\r\n@mihaimaruseac ", "comments": ["all recommended updates applied.", "updated, 3. "]}]