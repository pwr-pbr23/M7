[{"number": 23304, "title": "Add a line break when logging the graph definition", "body": "This PR adds a line break to make the log of the graph proto easier to read. \r\n\r\nWithout the line break, the log info will be like this:\r\n```\r\n2018-10-26 22:09:38.402045: I tensorflow/core/common_runtime/graph_execution_state.cc:79] Graph proto is node {\r\n  name: \"start\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT64\r\n    }\r\n  }\r\n.......\r\n```\r\n\r\nThis PR makes it as below:\r\n```\r\n2018-10-26 22:09:38.402045: I tensorflow/core/common_runtime/graph_execution_state.cc:79] Graph proto is \r\nnode {\r\n  name: \"start\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_INT64\r\n    }\r\n  }\r\n.......\r\n```\r\n\r\n\r\n\r\n", "comments": ["@ymodak @mrry The test failures seem to be unrelated to this PR. "]}, {"number": 23303, "title": "Error while running python code in pymanoid and openrave", "body": "Hello. I am trying to use pymanoid which uses openrave to visualize robots. Whenever I run a python file i get something like:\r\n[0;34mIn [[1;34m1[0;34m]: [0m\r\n\r\nI feel like this is an error. Can anyone help me with this?Is this an error? How can I fix it?\r\n\r\nThank You", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n\r\n"]}, {"number": 23302, "title": "Autotune seems to be \"forgetting\" after a short delay within the same session.", "body": "**System information**\r\n- I have written custom code in Microsoft Visual Studio Pro 2017 - C# (using TensorFlowSharp v1.11 wrapper classes).\r\n- Windows 7.\r\n- TensorFlow installed from binary.\r\n- TensorFlow version:  I think Nuget automatically gives me v1.4 with TensorFlowSharp v1.11?\r\n- Python version: Not using Python.  C# instead.\r\n- Laptop CPU only, no GPU used.\r\n\r\n**Describe the current behavior**\r\nI'm trying to process 128x128 pixel images real-time on a low-powered PC.  I've been profiling my code by programmatically recording millisecond timings.  We've created a CAE.pb file and it seems to run properly, and the time that the session.run() call is made in rapid succession (using the same session each time), the time it takes decreases (up to a certain point).  Specifically, when I process the first image takes 1400ms, second image 1150ms, third image 900ms... at the 7th our 8th image it levels off at 350ms per image processing.  The problem is that this speed increase only increases if I push images to TensorFlow in rapid succession.  If I pause for more than approx 350ms, then the time it takes to process the next image (with the *same session* being reused the whole time), jumps back up to 1400ms.  I'm assuming that the decreasing time is due to autotune running.  However, autotune seems to \"forget\" it's session values if I don't keep it running and fed with new data constantly.  Am I doing something wrong or is this a bug?  Is there a RunOption or SessionOption that I'm unaware of that controls how long until autotune (or what I'm presuming is autotune) \"forgets\"? :P\r\n\r\n**Describe the expected behavior**\r\nI would have expected that autotune could pause and not \"start from scratch\" (and would stay running quickly) so long as I kept the session object around.\r\n\r\n**Code to reproduce the issue**\r\nThis is tricky as I'm using C#, and the code is large.  Seeing as TensorFlowSharp is a pretty thin wrapper class, this seemed to be more related to the internals/usage of TensorFlow itself.\r\n", "comments": ["EXAMPLE PROGRAM:\r\n\r\n    using System;\r\n    using System.Collections.Generic;\r\n    using System.Diagnostics;\r\n    using System.IO;\r\n    using System.Linq;\r\n    using System.Text;\r\n    using System.Threading.Tasks;\r\n    using TensorFlow;\r\n    \r\n    namespace TensorFlow_Test\r\n    {\r\n        class Program\r\n        {\r\n            public static Stopwatch oTimer = new Stopwatch();                                 // Stopwatch for elapsed program execution time in ms.\r\n            private static long lLastElapsedTicks = 0;                                        // Number of ticks elapsed at the last timing fence.\r\n            public static long lTicksPerMicrosecond = Stopwatch.Frequency / (1000L * 1000L);  // Number of ticks per microsecond.\r\n\r\n            static void Main(string[] args)\r\n            {\r\n                TFSession m_oCaeSession;\r\n                TFGraph m_oCaeGraph;\r\n    \r\n                oTimer.Start();\r\n\r\n                m_oCaeGraph = new TFGraph();\r\n                byte[] acCaeModel = File.ReadAllBytes(\"CAE.pb\");\r\n                m_oCaeGraph.Import(acCaeModel, \"\");\r\n                TFSessionOptions oTFOptions = new TFSessionOptions();\r\n                m_oCaeSession = new TFSession(m_oCaeGraph, oTFOptions);\r\n\r\n                float[,,,] afSpectrogram = new float[1, 128, 128, 1];\r\n                var oCaeInputTensor = new TFTensor(afSpectrogram);\r\n                LogSpeed(\"Setup\");\r\n\r\n                for (int iLoop = 1; iLoop < 1000; iLoop++)\r\n                {\r\n                    TFBuffer oWhatever = new TFBuffer();\r\n                    var oCaeOutputTensor = m_oCaeSession.Run(new TFOutput[] { new TFOutput(m_oCaeGraph[\"input/X\"]) }, new TFTensor[] { oCaeInputTensor }, new TFOutput[] { new TFOutput(m_oCaeGraph[\"MaxPool2D_4/MaxPool\"]) }, null, oWhatever, new TFBuffer(new byte[] { 0x08, 0x03 }), null);\r\n                    LogSpeed(\"PerformCAE #\" + iLoop.ToString());\r\n    \r\n                    if (iLoop%10==0)\r\n                    {\r\n                        System.Threading.Thread.Sleep(1000);\r\n                        LogSpeed(\"Delay\");\r\n                    }\r\n                }\r\n            }\r\n\r\n            public static void LogSpeed(String p_sFenceName)\r\n            {\r\n                long lElapsedTicks = oTimer.ElapsedTicks;\r\n                Console.WriteLine(((lElapsedTicks - lLastElapsedTicks) / lTicksPerMicrosecond) + \" microseconds to \" + p_sFenceName);\r\n                lLastElapsedTicks = lElapsedTicks;\r\n            }\r\n        }\r\n    }\r\n\r\nEXAMPLE OUTPUT:\r\n\r\n    2018-10-30 14:57:02.646680: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n    668414 microseconds to Setup\r\n    1470936 microseconds to PerformCAE #1\r\n    874811 microseconds to PerformCAE #2\r\n    761587 microseconds to PerformCAE #3\r\n    658079 microseconds to PerformCAE #4\r\n    551307 microseconds to PerformCAE #5\r\n    378162 microseconds to PerformCAE #6\r\n    370983 microseconds to PerformCAE #7\r\n    390558 microseconds to PerformCAE #8\r\n    376005 microseconds to PerformCAE #9\r\n    383651 microseconds to PerformCAE #10\r\n    1267764 microseconds to Delay\r\n    1302333 microseconds to PerformCAE #11\r\n    1137831 microseconds to PerformCAE #12\r\n    864681 microseconds to PerformCAE #13\r\n    713534 microseconds to PerformCAE #14\r\n    635954 microseconds to PerformCAE #15\r\n    ...\r\n", "What is this autotune thing you refer to here?\r\n\r\nIn general, though, TF will only work with high throughput if you can get data into it at high throughput", "@MindMusic  -  Any update ?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing for now, please reopen with answers to my earlier question.", "Sorry for the delayed response.  This seems to have been an issue with one particular laptop.  I've been unable to replicate it anywhere else.  I just gave up and used a different laptop.  Thanks!"]}, {"number": 23301, "title": "Fix T-distribution sampling parameter", "body": "To sample a chi-square (df=n) distribution requires a gamma distribution with shape = df/2 and rate = 2. I believe this is a mistype by the previous contributor. For reference: https://stats.stackexchange.com/questions/118676/relationship-between-gamma-and-chi-squared-distribution", "comments": ["@jart Can you please take a look at this PR? Thanks!", "Thanks jvdillon and ymodak, I have proposed a pull request in: https://github.com/tensorflow/probability/pull/231\r\nPlease if you could, remove the pull request and close the underlying issue", "Note, as per the discussion on https://github.com/tensorflow/probability/pull/231, this change may be erroneous.", "> Thanks jvdillon and ymodak, I have proposed a pull request in: [tensorflow/probability#231](https://github.com/tensorflow/probability/pull/231)\r\n> Please if you could, remove the pull request and close the underlying issue\r\n\r\nClosing this pull request. Thanks!"]}, {"number": 23300, "title": "AttributeError: module '_pywrap_tensorflow_internal' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT_swigconstant'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNA\r\n- TensorFlow installed from (source or binary):\r\nFrom pip install\r\n- TensorFlow version (use command below):\r\n1.11.0\r\n- Python version:\r\n3.6.6\r\n- Bazel version (if compiling from source):\r\nNA\r\n- GCC/Compiler version (if compiling from source):\r\nNA\r\n- CUDA/cuDNN version:\r\n???\r\n- GPU model and memory:\r\nNA\r\n\r\n**Describe the current behavior**\r\nAfter installing 1.11.0 from pip, whenever I do `import tensorflow as tf`, I get the error in title.  If I downgrade to 1.10.0, the installation appears to be successful, or at least the import command works.\r\n\r\n**Describe the expected behavior**\r\n1.11.0 should work out of the box when doing a pip install\r\n\r\n**Code to reproduce the issue**\r\nAll you need is `import tensorflow as tf` to see the problem\r\n\r\n**Other info / logs**\r\nNA", "comments": ["@demongolem  - Try the following.\r\n- Reinstall tensorflow 1.11 and try restarting the system.\r\n- Make sure you install the Microsoft Visual C++ 2015 Redistributable Update 3\r\n- Also you may refer [this.](https://www.tensorflow.org/install/#pip_installation_on_windows)", "I have since made it up to 1.11.0 successfully, so I suppose issue resolved..  Perhaps it is as simple as bulletpoint #1, but I don't recall the exact moment when I did reinstall it or all the other dependencies that were involved in updating other libraries I use alongside tensorflow such as keras and spacy.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=23300)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=23300)\r\n"]}, {"number": 23299, "title": "`tf.keras.layers.Bidirectional` does not work under eager execution mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.12.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO\r\n- TensorFlow installed from (source or binary): BINARY\r\n- TensorFlow version (use command below): v1.12.0-rc0-17-g7b08198113 1.12.0-rc1\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\nA very simple model with `Bidirectional` layer will stop work after we enable the eager execution mode.\r\n\r\n**Describe the expected behavior**\r\n\r\nCode should work with eager execution mode.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe following code will be able to reprorduce this problem:\r\n\r\n```py\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution()\r\n\r\nx = tf.keras.layers.Input(shape=(2, 1))\r\ny = tf.keras.layers.Bidirectional(\r\n    tf.keras.layers.LSTM(1, return_state=True)\r\n)(x)\r\n\r\nmodel = tf.keras.Model(inputs=x, outputs=y)\r\n\r\ndata = np.array([0.1, 0.2]).reshape((1, -1, 1))\r\nprint(model.predict(data))\r\n```\r\nHowever, if we comment out the `tf.enable_eager_execution()`, then it will work as expected:\r\n\r\n**Other info / logs**\r\n\r\nThe error message(with eager execution enabled) is:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"src/t.py\", line 11, in <module>\r\n    )(x)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 473, in __call__\r\n    return super(Bidirectional, self).__call__(inputs, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 769, in __call__\r\n    output_shapes = self.compute_output_shape(input_shapes)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\", line 149, in wrapper\r\n    output_shape = fn(instance, input_shape)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 444, in compute_output_shape\r\n    input_shape).as_list())\r\nAttributeError: 'list' object has no attribute 'as_list'\r\n```\r\n\r\nAfter we disable the eager execution, then the above code works as expected.\r\n\r\n```\r\n[array([[0.01535364, 0.04371894]], dtype=float32), array([[0.01535364]], dtype=float32), array([[0.03121366]], dtype=float32), array([[0.04371894]], dtype=float32), array([[0.09061633]], dtype=float32)]\r\n```", "comments": ["### Update\r\n\r\nIt's related with the following code:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/47c84f6053faebf0c9e3b5fb54da315e89ad37ec/tensorflow/python/keras/layers/wrappers.py#L443-L445\r\n\r\nWhen in eager execution mode, the `child_input_shape` is a `list` already.", "@zixia @alextp I submitted a [PR](https://github.com/tensorflow/tensorflow/pull/23441) to fix this issue. Could you help review it?", "@feihugis awesome, it seems should work. \r\n\r\nLGTM, and thank you very much!", "Hi @zixia, I think the issue has been recently fixed on tensorflow side, and I cannot reproduce the error with latest nightly build.\r\n\r\nCan u try \"pip install -U tf-nightly\" or \"pip install -U tf-nightly-gpu\" to verify that the error has been fixed? ", "@qlzh727 Thank you for pointing out that, I can confirm that this issue had been fixed with the `tensorflow/tensorflow:nightly-devel-gpu-py3` docker image.\r\n\r\nI think we have another repository inside Google and the latest code is not synced with GitHub in time, am I right?\r\n\r\nIf so, I'd like to suggest that we should keep them automatic in sync so that we can see the latest code.\r\n\r\nHave a nice day!", "Hi @huan, the repository within google is fully synced with github. The issue here is that the fix is currently only available in the latest nightly and has not reach the release yet. We usually have monthly release cycle, so the fix should reach stable release within a month.", "@qlzh727 It's great that we had fully synced. Thanks for the clarification!"]}, {"number": 23298, "title": "Student t distribution: beta parameter in gamma", "body": "I think the beta parameter in gamma sampling for student t distribution should be 2 instead of 0.5 : \r\n[https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/ops/distributions/student_t.py](url)\r\n\r\n\r\n`gamma_sample = random_ops.random_gamma(\r\n        [n],\r\n        0.5 * df,\r\n        beta=0.5,\r\n        dtype=self.dtype,\r\n        seed=distribution_util.gen_new_seed(seed, salt=\"student_t\"))`", "comments": ["Hi,\r\nPlease check pull request #23301\r\n", "@missmagnum  - Hi, a PR has been raised for this and please wait until it's merged.", "Nagging Assignee @harshini-gadige: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As commented by @nickyungyung  in the PR, closing this issue. "]}, {"number": 23297, "title": "what's the default activation function of cudnnlstm in tensorflow? How can I set an activation function such as relu?", "body": "what's the default activation function of cudnnlstm in tensorflow? How can I set an activation function such as relu? Maybe it's just linear model? I read the document, but I did not find it.\r\nfor example, the code is below:\r\n\r\nlstmcell=tf.contrib.cudnn_rnn.CudnnLSTM(1,encoder_size,direction=\"bidirectional\")\r\n\r\nhq,_ =lstmcell(query)\r\n\r\nand i read the document in tensorflow https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnLSTM?hl=zh-cn the functon is below\r\n\r\ninit(\r\n\r\nnum_layers,\r\nnum_units,\r\ninput_mode=CUDNN_INPUT_LINEAR_MODE,\r\ndirection=CUDNN_RNN_UNIDIRECTION,\r\ndropout=0.0,\r\nseed=None,\r\ndtype=tf.float32,\r\nkernel_initializer=None,\r\nbias_initializer=None,\r\nname=None\r\n)\r\n\r\nAnd no keyword to set a parameter such as \"activation = \"tanh\" just like tf.nn.rnn_cell.LSTMell.\r\n\r\nSo what's the default activation function of cudnnlstm in tensorflow, and how to change it to leaky_relu.", "comments": ["You may refer [this.](https://github.com/keras-team/keras/issues/8510)\r\n\r\n\r\nThese type of questions are better asked on [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 23296, "title": "Issue building C++ Tensorflow r1.6 and r1.7 on 64bit Windows 7 with CMake", "body": "**System information**\r\n- OS: Windows 7 Enterprise\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r1.6 and r1.7\r\n- Python version: 3.5\r\n- Installed using virtualenv? pip? conda?: Cmake\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): MSVC 19.14\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\nThe compilation of TensorFlow fails.\r\nThis issue occurs when I try to build Tensorflow with the C++ API on my machine running Windows 7 Enterprise (x64) using the provided CMake files since I need this for a standalone project.\r\nI have tested this with the releases r1.6, r1.7, which I have pulled from the repository (for versions above r1.7 the compilation procedure fails due to other reasons; r1.12 cannot be compiled because it fails during CMake).\r\nI have used the official documentation provided [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md), as well as other guides found on the internet (e.g. the two articles found [here](https://joe-antognini.github.io/machine-learning/build-windows-tf) and [here](https://joe-antognini.github.io/machine-learning/windows-tf-project) and another one [here](https://medium.com/@shiweili/building-tensorflow-c-shared-library-on-windows-e79c90e23e6e)).\r\n\r\nFirst I initialise the shell by executing in a VS2017 developer command prompt:\r\n\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\VC\\Auxiliary\\Build.\r\n\r\nThen I do\r\n\r\ncmake .. -A x64 -T host=x64 -DCMAKE_BUILD_TYPE=Release ^\r\n-DSWIG_EXECUTABLE=\"C:\\swigwin-3.0.12\\swig.exe\" ^\r\n-DPYTHON_EXECUTABLE=\"C:\\anaconda3\\envs\\tensorflow\\python.exe\" ^\r\n-DPYTHON_LIBRARIES=\"C:\\anaconda3\\envs\\tensorflow\\libs\\python35.lib\" ^\r\n-Dtensorflow_BUILD_PYTHON_BINDINGS=OFF ^\r\n-Dtensorflow_ENABLE_GRPC_SUPPORT=OFF ^\r\n-Dtensorflow_BUILD_SHARED_LIB=ON \r\n\r\nand start the build process with\r\n\r\n\"C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\MSBuild\\15.0\\Bin\\amd64\\MSBuild.exe\" ^\r\n/m:1 ^\r\n/p:CL_MPCount=1 ^\r\n/p:Configuration=Release ^\r\n/p:Platform=x64 ^\r\n/p:PreferredToolArchitecture=x64 ALL_BUILD.vcxproj ^\r\n/filelogger\r\n\r\n\r\nThe build fails, the error messages are taken from msbuild.log:\r\n\r\n  Creating directories for 'eigen'\r\n  Performing download step (download, verify and extract) for 'eigen'\r\n  -- Downloading...\r\n     dst='C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/downloads/14e1418fcf12.tar.gz'\r\n     timeout='none'\r\n  -- Using src='https://bitbucket.org/eigen/eigen/get/14e1418fcf12.tar.gz'\r\n  -- [download 1% complete]\r\n  [...]\r\n  -- [download 100% complete]\r\n  -- Downloading... done\r\n  -- extracting...\r\n       src='C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/downloads/14e1418fcf12.tar.gz'\r\n       dst='C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/eigen/src/eigen'\r\n  -- extracting... [tar xfz]\r\n  -- extracting... [analysis]\r\n  -- extracting... [rename]\r\n  CMake Error at eigen-stamp/extract-eigen.cmake:51 (file):\r\n    file RENAME failed to rename\r\n  \r\n   C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/eigen/src/ex-eigen1234/eigen-eigen-14e1418fcf12\r\n    to\r\n  \r\n   C:/tensorflow_r1.6/tensorflow/contrib/cmake/build/eigen/src/eigen\r\n  \r\n because: No such file or directory\r\n  \r\n  \r\n  \r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 1. [C:\\tensorflow_r1.6\\tensorflow\\contrib\\cmake\\build\\eigen.vcxproj]\r\nDone executing task \"CustomBuild\" -- FAILED.\r\nDone building target \"CustomBuild\" in project \"eigen.vcxproj\" -- FAILED.\r\nDone Building Project \"C:\\tensorflow_r1.6\\tensorflow\\contrib\\cmake\\build\\eigen.vcxproj\" (default targets) -- FAILED.\r\n\r\nCheers.", "comments": ["Can you use the latest version of TensorFlow and build by any chance?", "> Can you use the latest version of TensorFlow and build by any chance?\r\n\r\nUnfortunately, when I try to build release 1.11 I get 48 errors of the type C1083. Specifically tf_core_lib.vcxproj complains that stringpiece.h cannot find a file, namely \"\"absl/strings/string_view.h\": No such file or directory\".\r\n\r\nCheers!", "Cmake has not been supported for quite a while (even when this issue was reported).\r\nIt is possible they were simply broken.\r\nNow that they are fully deleted. I will close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23296\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23296\">No</a>\n"]}, {"number": 23295, "title": "[Keras tensorflow] float16 doesn't work with conv2d ?", "body": "Hello,\r\nI have a rtx card to use RT cores (dedicated for NN, uses half-precision to my understanding) I'd like using float16 so I : \r\n\r\n    from tensorflow.keras import backend\r\n    os.environ['KERAS_FLOATX'] = 'float16'\r\n    os.environ['TF_FP16_CONV_USE_FP32_COMPUTE'] = '0'\r\n    backend.set_floatx('float16')\r\n\r\nbut this triggers error : \r\n   model = Sequential([\r\n        Conv2D(32, kernel_size=(5, 5), input_shape=(32, 32, 3), strides=(1, 1))\r\n    ])\r\n    model.compile(optimizer=Adam(), loss='mse')\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 510, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1107, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 944, in _TensorTensorConversionFunction\r\n    (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: 'Tensor(\"conv2d_sample_weights:0\", shape=(?,), dtype=float32)'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"float16.py\", line 17, in <module>\r\n    init()\r\n  File \"float16.py\", line 15, in init\r\n    model.compile(optimizer=Adam(), loss='mse')\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 442, in compile\r\n    output_loss = weighted_loss(y_true, y_pred, sample_weight, mask)\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\", line 453, in weighted\r\n    score_array *= weights\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 847, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1091, in _mul_dispatch\r\n    return gen_math_ops.mul(x, y, name=name)\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 4758, in mul\r\n    \"Mul\", x=x, y=y, name=name)\r\n  File \"C:\\Users\\xd111\\Miniconda3\\envs\\evo\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 546, in _apply_op_helper\r\n    inferred_from[input_arg.type_attr]))\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\r\n\r\nI'm not a professional I'm a newbie to NN so excuse me if my question is not relevant.\r\n\r\n", "comments": ["@NitroBAY Can you provide more information about the version, os, etc, and a minimal complete code sample that could reproduce the error you encountered, as was requested when you open the issue?\r\n\r\nThe  code shown in your error log (e.g., tensorflow\\python\\keras\\engine\\training_utils.py) has been updated a lot recently and it does not match your log. Without additional information it is hard to debug.", "Nagging Assignee @fchollet: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this as it is in \"awaiting response\" status for more than 7 days. Feel free to add your comments and we will reopen(if required)."]}, {"number": 23293, "title": "Merge pull request #1 from tensorflow/master", "body": "Updata my repo", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "There are no changes to merge , so closing the PR."]}, {"number": 23292, "title": "Dockerfile of tensorflow container tag \"1.10.1-gpu\"", "body": "Dear all,\r\nwhere can i find the Docker file of the docker container with tag \"1.10.1-gpu\" ?\r\n\r\nBest regards. Giusy ", "comments": ["We currently maintain two Docker container images:\r\ntensorflow/tensorflow - TensorFlow with all dependencies - CPU only!\r\ntensorflow/tensorflow:latest-gpu - TensorFlow with all dependencies and support for NVidia CUDA\r\n\r\nWe store all our containers on [Docker Hub](https://hub.docker.com/r/tensorflow/tensorflow/tags/).\r\n\r\nAlso please note that these type of questions are better asked on [StackOverflow](https://stackoverflow.com/questions/tagged//tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. We encourage you to post it there. Thank you !"]}, {"number": 23291, "title": "EMA's support for DistributionStrategy introduces worse performance", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cent OS 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: master\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n**Describe the current behavior**\r\nThis [PR](https://github.com/benjamintanweihao/tensorflow/commit/70af5e7ddcef853adb8af6355341bb8a97dcf736) add DistributionStrategy support to moving average APIs. However, **in MirroredStrategy**, this patch's implementation will introduce expensive MEMCPYPtoP cost, as `strategy.reduce(MEAN, value, v)` will eventually call strategy.broadcast(reduced, devices), and this get even worse when there are more devices used. Is this as expected as this implementation is just prepared for GENERAL distribution strategy? Thanks in advance. \r\n\r\n\r\n", "comments": ["I saw \r\n\r\n``TensorFlow version (use command below): 1.4``\r\n\r\nin your checklist. I am not aware of publicly available TF-1.4 package that provides DistributionStrategy. Is this a mistake?", "@byronyi Sorry for that typo, I have corrected it.", "Better to give a git hash for easier reproducing.", "Reassigning this to Josh, since it's a distribution strategy issue.", "Cross device communication does have some overhead, but is necessary for correctness when running distributed. What is the magnitude of the performance degradation?", "@josh11b Let's take a translation model with was trained offline on one machine with **four** 1080Ti cards as an example, the consumption time of EMA related is about **20%** and this gets worse when there are more devices, which is really a huge overhead. \r\nMore specifically, there overhead include addN, AssignSubVariableOp, MEMCPYPtoP which introduced by broadcast etc. And these are where we wish to do some job to optimize.\r\n\r\n![image](https://user-images.githubusercontent.com/3927162/47850863-b6f32b00-de11-11e8-860b-cc69b5751175.png)\r\n", "@fanshiqing which broadcast are you looking at? Does your another PR(https://github.com/tensorflow/tensorflow/pull/23396) fix it?", "Yes, @yuefengz PR(#23396) fix the case when MirroredVariable comes and no reduce-mean-broadcast is performed, it's exactly fix the timeline as shown above.", "Nagging Assignee @yuefengz: It has been 31 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 23290, "title": "Object detection and vibration", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.9.0\r\n- Are you willing to contribute it (Yes/No):NO\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI want to Object detection, if  Object name is equal to stop vibration , else cancel.\r\n\r\n**Will this change the current api? How?**  NO change\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n\r\nmy code\uff1a\r\n\r\n Vibrator vb = (Vibrator) getSystemService(Context.VIBRATOR_SERVICE);\r\n                if (getLocalClassName() == \"STOP\")\r\n                  vb.vibrate(5000);\r\n                else if (getLocalClassName() == \"turnright\")\r\n                  vb.cancel();\r\n\r\nmy IDE\uff1aandroid studio\r\n\r\ncode source\uff1ahttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java\r\n\r\nI have build APK, but detection \"STOP\"  no vibration.\r\n", "comments": ["This seems to be an android development question instead of model feature. ", "Nagging Assignee @harshini-gadige: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "As mentioned by @pkulzc, this looks like support question on android development to implement object detection. Please ask support questions in [Stackoverflow](https://stackoverflow.com/) community. Closing this issue due to lack of recent activity. Please open new ticket if you see similar issue. Thanks!\r\n\r\n"]}, {"number": 23289, "title": "[XLA] Remove dependency on having a GPU to run jit tests", "body": "This test is set up to require the GPU device, but not all systems have a GPU device in them.\r\n\r\nThis change removes the need for a GPU device to be present, both at compile time (where the xla_gpu_device was compiled in and therefore required CUDA support to be compiled in), and also at run time, where the GPU device was being targeted in a test, when it might not have been registered.\r\n", "comments": ["Hi David,\r\n\r\nCan you give me more details how this test fails at runtime without a GPU?  If the test truly needs a GPU then I'm not sure how it passes internally.", "hello\r\n\r\nit seems that the test requires a GPU, because it creates a GPU target, and does some work with it.  i can't remember why it fails to run, except that it does.\r\n\r\nit appears to require the GPU to build (because it requires the GPU device).  this drags in a lot of CUDA stuff.\r\n\r\nif I can find some spare time then I will remove the changes from our code base and see why it is failing.\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on."]}, {"number": 23288, "title": "Added org.tensorflow.Tensor.create() methods that are an order of mag\u2026", "body": "\u2026nitude faster on android.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "@w-adamski : Thanks for the contribution! Did you see the note about the CLA? We can only look at the change after the CLA has been signed. Thanks!", "Yes, I've seen the note about CLA, but apparently signing it by a corporation takes more time than signing by an individual.", "@w-adamski : Wasn't sure if your last comment meant that you do intend to figure out the CLA with your employer or if you're abandoning this PR. Could you clarify? Thanks!", "As far as I know the CLA will be signed. Possibly (but it's not certain) this month.", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F23288) for more info**.\n\n<!-- ok -->", "@sjamesr : Mind taking a look? Seems reasonable to me", "Can one of the admins verify this patch?", "This PR has currently very low priority for us so if anyone needs that feel free to update it / use it in your own PR.\r\n\r\nAnyway I will add it into the bottom of our backlog so maybe some day we'll go back to it if it makes sense.", "@herbakamil thank you for letting us know , please let us know once you are ready with new changes i can reopen it ."]}, {"number": 23287, "title": "Adding quantize fake quant per channel to TFlite", "body": "Hello! I was faced with the fact that in the current version of TFlite the operation FakeQuantWithMinMaxVarsPerChannel was not implemented when quantized by means of the utility TFLiteConverter.\r\nIn this regard, the question is, is it planned to implement this operation and this type of quantization in TFlite?\r\nAnd are there any known difficulties in realizing this quantization?", "comments": ["@synchro10  - Hi, please refer to this issue #21526 for tracking the information related to the operations need to be added to TFLite. Also please refer [this link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md) which gives information on TFLite supported operations. Thank you !"]}, {"number": 23286, "title": "Tensorflow (cpu) missing DLL on import", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- TensorFlow installed from (source or binary): pip install tensorflow; pip install --upgrade tensorflow;\r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.11.0-cp36-cp36m-win_amd64.whl etc.)\r\n- TensorFlow version: Tried multiple from 1.4.0 - 1.11.0\r\n- Python version: Python 3.6\r\n- Installed using virtualenv? pip? conda?: Yes to all 3\r\n- CUDA/cuDNN version: As far as I know I don't need it for the CPU Version.\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\nCan't import Tensorflow, missing DLL:\r\n`\r\nTraceback (most recent call last):\r\n  File \"<input>\", line 1, in <module>\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"venv\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\n  File \"venv\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\n  File \"C:\\Program Files\\JetBrains\\PyCharm Community Edition 2018.2.4\\helpers\\pydev\\_pydev_bundle\\pydev_import_hook.py\", line 20, in do_import\r\n    module = self._system_import(name, *args, **kwargs)\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Installed vc_redist64.exe\r\n- Open Python\r\n`import tensorflow`\r\n\r\n**Any other info / logs**\r\n- I already reinstalled Python completly \r\n- reinstalled vc_redist multiple times \r\n- and tried everything I found over google. Not sure if I am messing something up.\r\n", "comments": [" https://www.youtube.com/watch?v=HJ9bTO5yYw0\r\nFrom this source, you can find a solution to the problem.", "@BunnyJumps  You may also refer this #17386", "> @BunnyJumps You may also refer this #17386\r\n\r\nI already, referred to it, it didn't help sadly. I am currently working on a vm running Linux to use tensorflow because I just can't get it to work on Windows", "> https://www.youtube.com/watch?v=HJ9bTO5yYw0\r\n> From this source, you can find a solution to the problem.\r\n\r\nI think it's not a problem connected to pycharm. I tried to install it over pip and it didn't work. Even though I can install everything else without any problems.", "@BunnyJumps Can you please try again with the nightly build (`pip install tf-nightly`)? We've added some diagnostics to the loading process that should help to track down the cause of errors like this.", "@BunnyJumps Is this still an issue? Did you get a chance to try ?\r\n\r\n> @BunnyJumps Can you please try again with the nightly build (`pip install tf-nightly`)? We've added some diagnostics to the loading process that should help to track down the cause of errors like this.\r\n\r\n", "After running `py -m pip install tf-nightly`\r\nI still get on import the dll missing problem. But I am not sure if I should import something else than tensorflow like import tf-nightly or so.\r\n\r\n`Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 52, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\", line 6, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.`", "This looks like an issue in the version of `protobuf` that's installed, which seems to be incompatible with the version used by TensorFlow.\r\n\r\nCan you share the output of running `pip show protobuf` on your machine with us?", "@BunnyJumps  Any update ?", "Sorry for the late answer.\r\n\r\nCurrently I get\r\n`\r\npy -m pip show protobuf\r\nName: protobuf\r\nVersion: 3.6.1\r\nSummary: Protocol Buffers\r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: protobuf@googlegroups.com\r\nAuthor-email: protobuf@googlegroups.com\r\nLicense: 3-Clause BSD License\r\nLocation: c:\\users\\\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\r\nRequires: setuptools, six\r\nRequired-by: tf-nightly, tensorflow, tensorflow-tensorboard, tensorboard, tb-nightly\r\n`\r\n\r\nThing is I already tried multiple protobuf versions from downgrading down to 3.0 etc.", "Hmm, that's the version I would have expected. It looks like there might be something wrong with that protobuf installation though. If you open a Python shell and execute `from google import protobuf`, do you get a similar error?\r\n\r\n(Also, is it possible that there are multiple versions of `protobuf` installed in different virtual environments, and there's a conflict between them?)", "No, everything works fine. On\r\n`\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from google import protobuf\r\n>>>\r\n`", "Can you also try executing `from google.protobuf import descriptor` and let us know if that produces the same `ImportError`?", "Yeah that one creates the error:\r\n`\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from google.protobuf import descriptor\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in <module>\r\n    from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.\r\n>>>\r\n`", "Thanks for confirming. It looks like this is a bug in the protobuf package, which is probably related to https://github.com/protocolbuffers/protobuf/issues/5046. According to a [comment](https://github.com/protocolbuffers/protobuf/issues/5046#issuecomment-439736098) on that thread, upgrading Python from 3.6.0 to 3.6.1 may solve the problem.", "For tensorflow i'm using `pip3 install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.2.1-cp36-cp36m-win_amd64.whl`\r\n\r\nmy pip show protobuf is \r\n`Version: 3.6.1` \r\n\r\nI can confirm upgrading from python 3.6.0 to 3.6.1 works on my machine!\r\nThanks alot @mrry ", "Sorry for the late answer mrry and fufuninjas reply fixed the issues! Thanks."]}, {"number": 23285, "title": "Can't import tensorflow", "body": " - Windows 10 1709\r\n - Tensorflow version 1.11 install via pip\r\n - Python 3.6.6 64bit\r\n - Have I written custom code: No\r\n - Bazel version : I don't have\r\n - CUDA/cuDNN version : I don't have\r\n - GPU model and memory :N\\A\r\n - Exact command to reproduce : import tensorflow\r\n\r\n**The problem**\r\nCan't import tensorflow\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Failed to initialize a DLL.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\latit\\Desktop\\LanParty\\LANParty1.py\", line 27, in <module>\r\n    import tensorflow\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Failed to initialize a DLL.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Use Python 3.5.X 64bit on Windows.\r\n\r\nFor me, this version is working with Tensorflow, I had this same issue, but installing an older python solved it:\r\n```\r\nPython 3.5.4 (Aug  8 2017) 64 bit\r\nTensorflow 1.11.0\r\n```\r\nSo python must be 64 bit and 3.5.X at the same time.", "We also build and distribute python 3.6 binaries.\r\nThis can be missing MSVC redistributable, or CPU not supporting AVX instruction set.", "> \r\n> \r\n> Use Python 3.5.X 64bit on Windows.\r\n> \r\n> For me, this version is working with Tensorflow, I had this same issue, but installing an older python solved it:\r\n> \r\n> ```\r\n> Python 3.5.4 (Aug  8 2017) 64 bit\r\n> Tensorflow 1.11.0\r\n> ```\r\n> \r\n> So python must be 64 bit and 3.5.X at the same time.\r\n\r\nGetting the same error in 3.5.4", "In case of [Missing MSVC redistributable](https://www.microsoft.com/en-us/download/details.aspx?id=53587),\r\nor [CPU not supporting AVX (Install GPU version)](https://github.com/rohit-patel/Install_Instructions-Win10-Deeplearning-Keras-Tensorflow).", "I have already installed the MSVC download, and I don't have a NVIDIA gpu", "I tried to debug this or a related issue a little more: https://github.com/tensorflow/tensorflow/issues/27035#issuecomment-479392652", "Apologies for the delay in response.\r\n*TensorFlow release binaries (CPU/GPU) version 1.6 and higher are prebuilt with AVX instruction sets.*  \r\nSee [hardware requirements][1] to know more.  \r\n\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:  \r\n\r\n* Try Google Colab to use TensorFlow.    \r\n * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.  \r\n    * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.  \r\n* Try to build TF from sources by changing CPU optimization flags.\r\n\r\n\r\n  [1]: https://www.tensorflow.org/install/pip#hardware-requirements", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23285\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23285\">No</a>\n"]}, {"number": 23284, "title": " bazel-bin/$examples_dir/cifar10/cifar10_train --   in model_pruning  for No such file or directory", "body": "Hi,\r\nbazel-bin/$examples_dir/cifar10/cifar10_train --pruning_hparams=name=cifar10_pruning,begin_pruning_step=10000,end_pruning_step=100000,target_sparsity=0.9,sparsity_function_begin_step=10000,sparsity_function_end_step=100000\r\nbash: bazel-bin/contrib/model_pruning/examples/cifar10/cifar10_train: No such file or directory\r\n\r\n\r\nThank you for your help!", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n"]}, {"number": 23283, "title": "CollectiveAllReduceStrategy RecvBufResponse error in ResNet model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): binary(modify the code to fix the bug, pull request [link](https://github.com/tensorflow/estimator/pull/3) )\r\n- TensorFlow version (use command below): 1.12.0-rc0\r\n- Python version: 2.7.10\r\n- Bazel version (if compiling from source): 0.16.1\r\n- GCC/Compiler version (if compiling from source):  LLVM version 9.1.0(clang-902.0.39.2)\r\n- CUDA/cuDNN version: no installed\r\n- GPU model and memory: Intel Iris Plus Graphics\r\n\r\n**Describe the current behavior**\r\nusing the collective_all_reduce_strategy to train cifar10_Resnet model will raise an error, but the ParameterServerStrategy won't.\r\n\r\n**Describe the expected behavior**\r\nThis cifar10_ResNet has been updated to r1.11 and used tf.estimator.train_and_evaluate to train model, with the new feature of using distribute strategy.\r\nIt was found that the ParameterServerStrategy ran normally, but the CollectiveAllReduceStrategy raised an error in RecvBufResponse.\r\n\r\n**Code to reproduce the issue**\r\nSource code is attached.\r\n[cifar10_estimator.zip](https://github.com/tensorflow/tensorflow/files/2517601/cifar10_estimator.zip)\r\n\r\nTo run the ParameterServerStrategy, type these two commands in different shells:\r\n\r\n```\r\npython PS_ps0.py --data-dir=./cifar-10-data --job-dir=./model_dir/ --num-gpus=0 --train-steps=1000\r\n```\r\n\r\n```\r\npython PS_worker0.py --data-dir=./cifar-10-data --job-dir=./model_dir/ --num-gpus=0 --train-steps=1000\r\n```\r\n\r\nTo run the CollectiveAllReduceStrategy(just modify TF_CONFIG and strategy), type these two commands in different shells:\r\n\r\n```\r\npython Collective_worker0.py --data-dir=./cifar-10-data --job-dir=./model_dir/ --num-gpus=0 --train-steps=1000\r\n```\r\n\r\n```\r\npython Collective_worker1.py --data-dir=./cifar-10-data --job-dir=./model_dir/ --num-gpus=0 --train-steps=1000\r\n```\r\n\r\n**Other info / logs**\r\n\r\nError message:RecvBufResponse returned 128 bytes where to_tensor expected 64\r\n[[node resnet/BatchNorm/beta/Momentum/Initializer/CollectiveBcastRecv (defined at /Library/Python/2.7/site-packages/tensorflow/contrib/distribute/python/collective_all_reduce_strategy.py:186) ]]\r\n\r\n", "comments": ["Thanks! It would be better if you could give a minimal reproduce and more verbose logging.\r\n", "An error in CollectiveBcastRecv indicates the problem is probably during model initialization.  The size mismatch suggests the model has been set up differently at each worker.   @yuefengz may be able to pinpoint the problem more exactly.  ", "@CheukNgai Seems your code has logic that only can apply for parameter servers such as `tf.contrib.training.GreedyLoadBalancingStrategy`. Could you remove these logic and try again? It is not recommended to control device placement when you are using distribution strategies.", "@clustarycx Could you pls help following this issue? If not, i will close this issue because i have left my intern job.", "@CheukNgai I cannot find the recreating procedure. I think you can just closet this, and we will open a new issue or reopen this one if similar problem is hit again. thanks."]}, {"number": 23282, "title": "Just some tweaks in the readme files", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "The \"Files changed\" view doesn't show any commits. Closing this PR. "]}, {"number": 23281, "title": "Delete legacy Input class", "body": "For some strange reasons, the legacy `Input` class, which has been replaced by `Operand` a long time ago, reappeared in the main repository.\r\n\r\nProbably due from a bad merge that occurred [here](https://github.com/tensorflow/tensorflow/commit/90fc5e3819ed62e93228a9c2c29dede0f0f8cfd6)", "comments": []}, {"number": 23280, "title": "1.12.0-rc2 cherry-pick request: Fixed the issue that each invocation of model.fit/evaluate/predict\u2026", "body": "\u2026 modifies the graph.\r\n\r\nPiperOrigin-RevId: 218793646", "comments": []}, {"number": 23279, "title": "Prevent memory leak by storing strings instead of StringPiece in vector", "body": "This fixes a memory leak where a `StringPiece` stored in a vector, hence the `AttrVec` here, outlives the `const char*` buffer he has been initialized with.\r\n\r\nI observed this behaviour by adding an attribute value to the `AttrBuilder` from JNI code (Java Native), where the buffer has to be released before switching back to the JVM, e.g.\r\n```\r\nconst char* cname = env->GetStringUTFChars(attr_name, nullptr);\r\nTFE_OpSetAttrInt(op, cname, static_cast<int64_t>(value));\r\nenv->ReleaseStringUTFChars(attr_name, cname);\r\n```\r\nThis bug was only observable for the `Int/Float/Bool/DataType` attributes, others were falling directly into the `SetInAttrValueMap` which was already converting the `StringPiece` into a `string` before adding it into the `AttrValueMap`.\r\n", "comments": ["Nagging Reviewer @asimshankar: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied.", "Hey @asimshankar , anything I can do to help you with this one?", "Hmm...naively I'm seeing a noticeable (~10-15ns) overhead with this copy in `BM_InitOp`\r\n\r\n```sh\r\nbazel build -c opt //tensorflow/c/eager:c_api_test\r\n./bazel-bin/tensorflow/c/eager/c_api_test --benchmarks=InitOp\r\n```\r\n\r\nHaven't dug into it yet, just a heads up in case you have cycles to figure out if that is real and if there is a way to avoid that overhead.", "That was the simplest way I found to fix it as it mimics what was already done for attributes not on the optimized path (i/f/b/dt).\r\n\r\nTo avoid the overhead, quickly I guess the Java client could keep track of all strings passed to the `AttrBuilder` and release them only after the op has been executed. Or we could collect the attributes on the JVM side and build the op in C++ only at execution time.\r\n\r\nBut IMHO, and I think you agree, it is a bit unusual for an API to require that strings being passed in parameter should have a certain lifetime beyond that call, normally they act pretty much as primitives. If we want to keep the current behaviour, we should probably document it and explain it.", "Yeah, I agree the API contract is silly/confusing and should be fixed. Just haven't given enough thought to how we can avoid the performance penalty. I have some ideas, will try those out when I get a chance."]}, {"number": 23278, "title": "absl warning/error when compiling from source", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.11\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 0.18\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nTensorflow cannot find absl when tensorflow is included in external project. Warning is given when compiling tensorflow.\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n_bazel build --verbose_failures -c opt --copt=-msse4.1 --copt=-msse4.2 //tensorflow:libtensorflow_cc.so_\r\n^this gives the following warning:\r\n```\r\nWARNING: /opt/tensorflow/tensorflow/core/BUILD:2463:1: in includes attribute of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /opt/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: /opt/tensorflow/tensorflow/core/BUILD:2548:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /opt/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\n\r\n```\r\nWhen compiling external project which includes tensorflow, I get the following error: \r\n```\r\nIn file included from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor_shape.h:26:0,\r\n                 from /usr/local/include/google/tensorflow/tensorflow/core/framework/tensor.h:21,\r\n                 from /usr/local/include/google/tensorflow/tensorflow/core/public/session.h:24,\r\n                 from /share/C++/modules/xyz/include/abc/abc.h:8,\r\n                 from /share/C++/modules/xyz/src/abc/abc.cpp:1:\r\n/usr/local/include/google/tensorflow/tensorflow/core/lib/gtl/array_slice.h:19:29: fatal error: absl/types/span.h: No such file or directory\r\n\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["I alse meet this problem in my ubuntu 16.04 with tensorflow-master,py-3.5,cuda8.0,cudnn7.0 and bazel0.18.0. SO i changed the tensorflow version to 1.5.0 also the bazel version to 0.9.0,it works.", "Bazel cannot include workspaces tranasitively.\r\nSo you will need to import workspace.bzl in your WORKSPACE file,  and call tf_workspace() to add all TF dependencies when including TF as an external project.\r\n\r\nYou can check servo or tf estimator for example usage.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 23277, "title": "Tensorflow installation", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\nAlienware \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n      Windows 10\r\n- TensorFlow installed from (source or binary):\r\n  not sure...from source though. I do pip install tensorflow-gpu\r\n- TensorFlow version:\r\n   tensorflow-gpu 1.11\r\n- Python version:\r\n      3.6\r\n- Installed using virtualenv? pip? conda?:\r\n    pip\r\n- Bazel version (if compiling from source):\r\n    no bazel. Would this be easier?\r\n- GCC/Compiler version (if compiling from source):\r\n    \r\n- CUDA/cuDNN version:\r\n    CUDA: 9\r\n    cuDNN: 7.3\r\n- GPU model and memory:\r\n   laptop\r\n   GPU: NVIDIA GTX 1070\r\n   CPU: I7-7700 HQ \r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nThe installation is just not working for some reason. I am sure that I have followed the steps pretty well.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1)cmd\r\n2) \"python\"\r\n3) \"import tensorflow\"\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n\r\n[Tensorflow_Issues.txt](https://github.com/tensorflow/tensorflow/files/2517308/Tensorflow_Issues.txt)\r\n\r\n", "comments": ["You need to add the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environmental variable if haven't already. Please refer to these [windows setup instructions](https://www.tensorflow.org/install/gpu#windows_setup).", "@ymodak What is CUPTI?? Maybe that is the problem. I thought that was something that came with CUDA though\r\nI have followed the instructions on Tensorflow's site stating to open cmd and then setting the three variables. I created the tools directory in my c:/  drive as well. ", "@Vilchis-Joshua Can you please try running [Dependency Walker](http://www.dependencywalker.com/) on \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd\" and let us know if it detects any missing DLLs?\r\n", "@mrry \r\nI ran it, and I am not sure what I am supposed to be looking for. \r\n\r\nTHese are the erros that I get:\r\n\"Error: At least one required implicit or forwarded dependency was not found.\r\nError: At least one module has an unresolved import due to a missing export function in an implicitly dependent module.\r\nError: Modules with different CPU types were found.\r\nWarning: At least one delay-load dependency module was not found.\r\nWarning: At least one module has an unresolved import due to a missing export function in a delay-load dependent module.\"\r\n\r\nHowever, it also stopped at one point because it said something about an error and not being able to contineu.", "@Vilchis-Joshua It should show a list of DLLs with details about ones that are missing, like in [this screenshot](https://goo.gl/images/hdcaiz). If you can find such a list, please paste in the list of missing DLLs, and that should help to diagnose the problem.", "\r\n[dlllist.txt](https://github.com/tensorflow/tensorflow/files/2540174/dlllist.txt)\r\n\r\n![image](https://user-images.githubusercontent.com/24929856/47881227-edbe4680-ddfb-11e8-8ac3-b4af0ad864fa.png)\r\n\r\nThis is a really long list....but is this what you mean? They all have a list of error opening file", "Great, thanks! AIUI the API-MS-WIN-*.DLL entries are spurious (at least, according to this [Stack Overflow answer](https://stackoverflow.com/a/36244483/3574081)). The list seems to contain \"CUDNN64_7.DLL\", which seems more suspicious. Can you check that that file is in a directory that's in your `%PATH%` environment variable? ", "Does the path variable need to go straight tot he bin folder? Right now it doesn't go straight there, but one directory before it.\r\n\r\n\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\"", "Yes, you may need to add \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\bin\" to your path.", "When doing that, I still get errors, but there seem to be fewer:\r\nPython 3.6.5 (v3.6.5:f59c0932b4, Mar 28 2018, 17:00:18) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\joshs\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.", "[dlllist.txt](https://github.com/tensorflow/tensorflow/files/2540687/dlllist.txt)\r\n\r\nHere is the new one", "It looks like \"CUDNN64_7.DLL\" is still in the list. Is it showing up in Dependency Walker as not found?", "Looking through, I do not see it. I don't really understand since I have added the path to my environment variables.\r\n\r\n![image](https://user-images.githubusercontent.com/24929856/47889576-068d2300-de21-11e8-8134-8a4be37f7871.png)\r\n", "I added them to my system variables, but maybe that's not correct? Should I add them to the variables for my user?", "Could there be a conflict if I have those variable paths, but also this is in just the path variable:\r\n\r\n![image](https://user-images.githubusercontent.com/24929856/47889747-deea8a80-de21-11e8-9805-95abdb44dfe8.png)\r\n", "No wait, you are totally right!\r\n\r\n![image](https://user-images.githubusercontent.com/24929856/47890140-f88cd180-de23-11e8-8327-1cf429e469ac.png)\r\n", "@mrry \r\nThank you so much for the help. I have been trying to get this working for weeks....and now it finally does.\r\n\r\nsteps I took to fix:\r\n1) searched my c drive for that exact name\r\n2) it was located in my CUDA 10v bin, but NOT in my CUDA 9v. \r\n3) I copied and pasted, and everything is working alright.", "Am I supposed to close the issue?\r\n", "Thanks for confirming that that worked! I'll close the issue now."]}, {"number": 23276, "title": "1.12.0-rc2 cherry-pick request: Upgrade setuptools before clean pip install pulls in absl-py.", "body": "absl-py recently added a version dependency to the package, causing\r\ninstall to fail on the old setuptools\r\n\r\nPiperOrigin-RevId: 218783878", "comments": []}, {"number": 23275, "title": "1.12.0-rc2 cherry-pick request: Allow empty GCS tokens to be cached.", "body": "PiperOrigin-RevId: 217159671", "comments": ["\ud83d\udc4d "]}, {"number": 23274, "title": "differentiable type casting", "body": "There seems to be no way to do differentiable type casting in tensorflow. tf.cast is not differentiable and so is tf.image.convert_image_dtype() to cast image tensors. How to incorporate casting in a loss function if there are no options ?", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n"]}]