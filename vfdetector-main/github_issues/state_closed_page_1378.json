[{"number": 11727, "title": "tf.one_hot indices out of bound", "body": "Hi.\r\n\r\nI have ran into some nasty indexing bugs, which weren't caught by TF. I investigated and found the following behaviour:\r\n\r\n```python\r\n>>> sess=tf.Session()\r\n>>> sess.run(tf.one_hot(tf.constant(3), 4))\r\narray([ 0.,  0.,  0.,  1.], dtype=float32)\r\n>>> sess.run(tf.one_hot(tf.constant(0), 4))\r\narray([ 1.,  0.,  0.,  0.], dtype=float32)\r\n>>> sess.run(tf.one_hot(tf.constant(4), 4))\r\narray([ 0.,  0.,  0.,  0.], dtype=float32)\r\n>>> sess.run(tf.one_hot(tf.constant(-1), 4))\r\narray([ 0.,  0.,  0.,  0.], dtype=float32)\r\n```\r\n\r\nBoth overflow and underflow result in a zeroed array. Should this be caught, and errors be thrown? If not, why and when would this zeroed behaviour make sense? Or is it costly to check and throw assertions?\r\n", "comments": ["This is indeed intentional. See the examples on the [C++ op documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/array_ops.cc#L4494). We cannot change this without breaking backwards comptability."]}, {"number": 11726, "title": "Error while generating executable from obj file created with XLA aot for CPU", "body": "Hi,\r\nI m trying to generate executable (currently x86 for testing) using xla aot. I built the test file and generated .o using bazel build. Now I am trying to generate executable using clang (gcc also gives the similar error). I get the following error as undefined reference to **tensorflow::tfcompile::runtime::MallocContiguousBuffers** and\r\n**tensorflow::tfcompile::runtime::FreeContiguous**\r\n```\r\n**clang  ../aot_build/9fb1b9df97c5e4f9c88d55c740dbcaad/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/aot/tests/_objs/test_graph_binary/tensorflow/compiler/aot/tests/test_graph.o -lstdc++ -v**\r\nclang version 3.8.0 (tags/RELEASE_380/final)\r\nTarget: x86_64-unknown-linux-gnu\r\nThread model: posix\r\nInstalledDir: /pkg/qct/software/llvm/build_tools/llvm38_160329/bin\r\nFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.8\r\nFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.8.4\r\nFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.9\r\nFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.9.3\r\nSelected GCC installation: /usr/lib/gcc/x86_64-linux-gnu/4.8\r\nCandidate multilib: .;@m64\r\nSelected multilib: .;@m64\r\n \"/usr/bin/ld\" -z relro --hash-style=gnu --build-id --eh-frame-hdr -m elf_x86_64 -dynamic-linker /lib64/ld-linux-x86-64.so.2 -o a.out /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crt1.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crti.o /usr/lib/gcc/x86_64-linux-gnu/4.8/crtbegin.o -L/usr/lib/gcc/x86_64-linux-gnu/4.8 -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu -L/lib/x86_64-linux-gnu -L/lib/../lib64 -L/usr/lib/x86_64-linux-gnu -L/usr/lib/gcc/x86_64-linux-gnu/4.8/../../.. -L/afs/localcell/cm/gv2.6/sysname/pkg.@sys/qct/software/llvm/build_tools/llvm38_160329/bin/../lib -L/lib -L/usr/lib ../aot_build/9fb1b9df97c5e4f9c88d55c740dbcaad/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/aot/tests/_objs/test_graph_binary/tensorflow/compiler/aot/tests/test_graph.o -lstdc++ -lgcc --as-needed -lgcc_s --no-as-needed -lc -lgcc --as-needed -lgcc_s --no-as-needed /usr/lib/gcc/x86_64-linux-gnu/4.8/crtend.o /usr/lib/gcc/x86_64-linux-gnu/4.8/../../../x86_64-linux-gnu/crtn.o\r\n../aot_build/9fb1b9df97c5e4f9c88d55c740dbcaad/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/compiler/aot/tests/_objs/test_graph_binary/tensorflow/compiler/aot/tests/test_graph.o: In function `main':\r\ntest_graph.cc:(.text.startup.main+0xb4): undefined reference to `tensorflow::tfcompile::runtime::MallocContiguousBuffers(long const*, unsigned long, void**, bool)'\r\ntest_graph.cc:(.text.startup.main+0xda): undefined reference to `tensorflow::tfcompile::runtime::MallocContiguousBuffers(long const*, unsigned long, void**, bool)'\r\ntest_graph.cc:(.text.startup.main+0xf8): undefined reference to `xla::ExecutableRunOptions::set_intra_op_thread_pool(Eigen::ThreadPoolDevice const*)'\r\ntest_graph.cc:(.text.startup.main+0x1dc): undefined reference to `__tensorflow_compiler_aot_tests__test_graph_tfmatmul'\r\ntest_graph.cc:(.text.startup.main+0x22e): undefined reference to `tensorflow::tfcompile::runtime::FreeContiguous(void*)'\r\ntest_graph.cc:(.text.startup.main+0x23a): undefined reference to `tensorflow::tfcompile::runtime::FreeContiguous(void*)'\r\ntest_graph.cc:(.text.startup.main+0x289): undefined reference to `tensorflow::tfcompile::runtime::FreeContiguous(void*)'\r\ntest_graph.cc:(.text.startup.main+0x295): undefined reference to `tensorflow::tfcompile::runtime::FreeContiguous(void*)'\r\nclang-3.8: error: linker command failed with exit code 1 (use -v to see invocation)\r\n\r\nI am running it on  Ubuntu 14.04.5\r\nTensorflow version: 1.2.1\r\nInstalled tensorflow from source with XLA enabled and everything else disabled.\r\nbazel: release 0.5.1\r\nGPU: N/A\r\nPython: 2.7.6 (although I thik it's not applicable here)\r\nclang version 3.8.0\r\n\r\nI am following this: https://www.tensorflow.org/performance/xla/tfcompile\r\nInitially I was getting error in build\r\n\r\nERROR: /local/mnt/workspace/ankitac/virtual/tensorflow/tensorflow/compiler/aot/tests/aot_project/BUILD:12:1: undeclared inclusion(s) in rule '//tensorflow/compiler/aot/tests/aot_project:test_graph_binary':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/compiler/aot/tests/aot_project/test_graph.cc':\r\n  '/local/mnt/workspace/ankitac/virtual/tensorflow/tensorflow/compiler/aot/tests/aot_project/test_graph_tfmatmul.h'.\r\nTarget //tensorflow/compiler/aot/tests/aot_project:test_graph_binary failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 5.092s, Critical Path: 1.34s\r\n```\r\nI have BUILD file as:\r\n```\r\nload(\"//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\ntf_library(\r\n    name = \"test_graph_tfmatmul\",\r\n    cpp_class = \"foo::MatMulComp\",\r\n    graph = \"test_graph_tfmatmul.pb\",\r\n    config = \"test_graph_tfmatmul.config.pbtxt\",\r\n)\r\n\r\ncc_binary(\r\n    name = \"test_graph_binary\",\r\n    srcs = [\r\n        \"test_graph.cc\",  # include test_graph_tfmatmul.h to access the generated header\r\n    ],\r\n    deps = [\r\n        \":test_graph_tfmatmul\",  # link in the generated object file\r\n        \"//third_party/eigen3\",\r\n    ],\r\n    linkopts = [\r\n          \"-lpthread\",\r\n    ]\r\n)\r\n```\r\nBuilding that using\r\n```\r\nbazel --output_user_root=/local/mnt/workspace/ankitac/virtual/aot_build/ build  //tensorflow/compiler/aot/tests/aot_project:test_graph_binary\r\n```\r\n**For now I resolved this issue by adding my cc_binary to aot/test BUILD and building it from there works. But crating executable is still the issue.**\r\n\r\nAny help is highly appreciated!\r\n\r\nThanks & regards\r\n\r\n\r\n", "comments": ["Generally speaking AOT still requires TensorFlow runtime symbols which can be done by finding the right .cc's to add to your compile command, but yes, the easiest way is to use bazel and a cc_binary command (as the XLA tutorial indicates). At Google we haven't used other build systems for XLA, so we can't provide much guidance on that, but perhaps others in the community have.", "@tatatodd, do you have any quick suggestions?", "@aselle Thanks! I've found workaround for the issue as stated adding cc_binary inside test/BUILD works. Now I am trying to generate executable for hexagon. I see that someone has already tried that at Google. I replaced the triple and data format in cpu_compiler.cc. Still it's generating x86 executable. Ideally it should pick up the appropriate platform based on triple isn't it? I've added appropriate additions in BUILD files of llvm/BUILD and cpu/BUILD. It's building successfully. But the binary generated isn't hexagon compatible.", "@satok16, do you have any suggestions?", "@aselle I have one doubt regarding the example given in https://developers.googleblog.com/2017/03/xla-tensorflow-compiled.html video. where exactly does it picks up the operation for the following graph:\r\n# Text form of tensorflow.tfcompile.Config proto.\r\nfeed {\r\n  id { node_name: \"x_hold\" }\r\n  shape {\r\n    dim { size: 2 }\r\n    dim { size: 3 }\r\n  }\r\n}\r\nfeed {\r\n  id { node_name: \"y_hold\" }\r\n  shape {\r\n    dim { size: 3 }\r\n    dim { size: 2 }\r\n  }\r\n}\r\nfetch {\r\n  id { node_name: \"x_y_prod\" }\r\n}\r\n\r\nI don't see any operation defined for this. and in tf_library, we just pass the config file and define the class names. I don't understand where exactly does it specifies that the given operation associated with the graph is matmul.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Maybe you can try adding the one line in cc_library.\r\n> testonly = 1,\r\n\r\nSuch as\r\n>cc_binary(\r\n    name = \"my_binary\",\r\n    testonly = 1,\r\n", "Nagging Assignee @tatianashp: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 45 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tatianashp: It has been 60 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 11725, "title": "tf_cnn_benchmarks.py stuck when running with multiple GPUs and ImageNet data with protocol grpc+verbs", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, running tf_cnn_benchmarks.py from benchmarks repo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: Unmodified source with RDMA Verbs enabled\r\n- **TensorFlow version (use command below)**: 1.3.0-rc0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**: NVIDIA Tesla P100 PCIe 16GB (8 per node)\r\n- **Exact command to reproduce**: \r\n\r\nPS: CUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=ps --task_index=0 --server_protocol grpc+verbs\r\n\r\nWorker0: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=0 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs\r\n\r\nWorker1: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=1 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs\r\n\r\n- **RDMA driver version**: MLNX_OFED_LINUX-4.1-1.0.2.0\r\n\r\n### Describe the problem\r\nWhen running the above commands (Inception V3 synchronized data parallelism training with 2 workers and 1 external ps), the tf_cnn_benchmarks application hangs forever after some iterations (usually in warm up).\r\n\r\nIt happens only when real data is involved (ImageNet), and with >4 GPUs. (More GPUs, less iterations before it hangs). Doesn't happen with grpc protocol, or when running with \"synthetic\" data.\r\n\r\nThe master_service in the workers is stuck [here](\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L608), which I guess means some operations in the computation have not been completed.\r\n\r\nThe RDMA protocol looks valid and clean, all messages corresponds to the protocol (see below logs). \r\nThere some tensors requested by the workers which they don't receive, but they are passed by the RDMA Verbs transport to the BaseRendezvoudMgr with RecvLocalAsync in a valid way, and for some reason the higher level worker service doesn't trigger the Send kernel on those tensors.\r\n\r\nAny help is much appreciated!\r\nIf there are some debug mechanisms I can use to understand which tensors/operations have not been completed it can greatly help. I was mostly debugging this from the RDMA Verbs layer till now, without much success, and I feel I don't have enough information there to understand what's missing.\r\nAlso I feel we don't have enough knowledge on how the step_id acts (diving into this in the code now, but there's some higher level documentation it can greatly help).\r\n\r\nMy initial guess was an occurrence of a racy condition when loading the data, since it creates a gap in execution time (worker0 starts the first training step 30-60 seconds after worker1, since it does the preprocessing of the data twice for a reason I couldn't understand yet), but after the first iteration (which usually passes successfully) the time is synchronized between workers.\r\n\r\n### Source code / logs\r\nThose are the logs of the runtime after moving the logging in [rdma.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc) to VLOG(0) (also adding Tensor name and step id for all cases, in some cases the step_id doesn't mean anything like BUFFER_REQUEST/RESPONSE for example), and also some VLOG in [master_session.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc)\r\n\r\n[worker0](https://gist.github.com/shamoya/15a42f421e088473b8f02bf00c16d0fc)\r\n[worker1](https://gist.github.com/shamoya/dd3126c02c73990a6e28b534d9a9ddf6)\r\n[ps](https://gist.github.com/shamoya/0c856365802ae4d42b38baf988149574)\r\n\r\nUnfortunately they are fairly large, but it's better then to cut the log files IMO.\r\nExample for analysis I did in the verbs layer, comparing the Sent Tensor requests to the actual received tensors writes in both workers:\r\n\r\nworker 0:\r\n -  /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:0/gpu:0;edge_116943_group_deps_2/NoOp_1;0:0 80661058974090965\r\n-  /job:worker/replica:0/task:1/cpu:0;1a50d5c51cd9c5d1;/job:worker/replica:0/task:0/gpu:0;edge_116947_group_deps_3/NoOp_1;0:0 80661058974090965\r\n- /job:worker/replica:0/task:1/gpu:2;7f00fadabfe781f5;/job:worker/replica:0/task:0/gpu:0;edge_111078_group_deps_1/NoOp_2;0:0 80661058974090965\r\n- /job:worker/replica:0/task:1/gpu:4;b07185dd19f62088;/job:worker/replica:0/task:0/gpu:0;edge_111080_group_deps_1/NoOp_4;0:0 80661058974090965\r\n\r\nworker 1:\r\n- /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:1/cpu:0;edge_155113_AssignAdd;0:0 80661058974090965\r\n- /job:worker/replica:0/task:0/gpu:0;f3df8abf03739fe8;/job:worker/replica:0/task:1/cpu:0;edge_116948_group_deps_3;0:0 80661058974090965\r\n\r\nThe tensors requests received well by the other side and passed to RecvLocalAsync, but are not called later.\r\n\r\nThanks a lot.", "comments": ["I was able to reproduce the issue. I also tried 'alexnet', it hung as well. I will take a close look in the coming days. Thanks for reporting.", "Thank you for looking into this @junshi15!", "@mrry @jhseu @poxvoculi , just a small question.\r\nI'm trying to understand why execution hangs in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L608).\r\nWhat (I think) I need is to get the Executor(s) which doesn't end in [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/graph_mgr.cc#L541).\r\nI tried passing tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) to sess.run but then I get a huge file I can't really understand.  Is this the right way ? ", "I have met stuck problem(I think it may happen in warm up if I'm right.) when all workers finished all steps. I think `global_step` should be protected by a lock.I raised [an issue](https://github.com/tensorflow/benchmarks/issues/38). I'm not sure if tensorflow has some special code for `global_step`.", "When I'm trying to debug a hung concurrent program the first step is\nusually to try to find a small case that exhibits the problem, i.e. try to\nfind the least number of workers and GPUs that still hangs.  It's much\neasier to debug a small case than a large one.  Then, if it's a really\nsmall case you might be able to set the logging level high and read all the\nlog files.  Usually though, I form some hypotheses about where the problem\nmight be (e.g. a missing lock, a race that might result in deadlock, logic\nerror that always deadlocks) and start putting in LOG(INFO) statements\naround the suspicious points to confirm or refute each hypothesis.\n\nOn Tue, Jul 25, 2017 at 4:53 AM, Ziming Dong <notifications@github.com>\nwrote:\n\n> I think global_step should be protected by a lock.I raised an issue\n> <https://github.com/tensorflow/benchmarks/issues/38>. I'm not sure if\n> tensorflow has some special code for global_step.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11725#issuecomment-317713980>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AO818aHj9Ak6IS6emES-wdAhL-n1M4oUks5sRdcmgaJpZM4OhqyQ>\n> .\n>\n", "@tfboyd I'm unassigning myself because I know very little about the RPC layer and know nothing about VERBs. I don't think this is a bug with tf_cnn_benchmarks, because #11416 also has the same issue with a different model.\r\n\r\n@suiyuan2009 I don't think tensorflow/benchmarks#38 is the same issue, since this issue only occurs with verbs, and it occurs with another model with verbs. I'll take a look at tensorflow/benchmarks#38 though.", "Thanks @poxvoculi \r\nSadly the smallest case which repro the issue is 3 nodes (2 workers 1 ps) with InveptionV3 and ImageNet, with the tf_cnn_benchmark.py (haven't checked others).\r\nI had ~15 hypotheses which turn out to be false.\r\nAdded a lot of prints, but still couldn't understand which tensor is the rebel.\r\nI feel the trace_log mechanism (tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)) can help me here, but I have no idea how to parse it. Is there a parsing tool or format for it ?", "after adding  a lock to `global step`, I didn't meet stuck problem.\r\nbut I got error when running vgg benchmark code, maybe my network environment is a little unstable.\r\n```\r\n2017-07-26 21:21:26.691844: F tensorflow/contrib/verbs/rdma.cc:683] Check failed: status.ok() RecvLocalAsync was not ok, key/job:worker/replica:0/task:2/gpu:1;b0903effc6e4881e;/job:ps/replica:0/task:0/cpu:0;edge_2493_Mul_1;0:0;138139939197237012 error message: Step 138139939197237012\r\n```", "@suiyuan2009 Your error might be caused by another process (PS or worker) in your job. We have met the same problem and it turns out when one process terminated the connection, its peers will print out such odd error log.", "@shamoya For a simpler model, you could try `--model=trivial` (the default one) and set your number of batches to a smaller value. \r\n\r\nSorry that I am busying working on migrating my own patch and have little time to reproduce this particular bug right now.", "@shamoya Can you try what @suiyuan2009 and @byronyi suggested.\r\n\r\n1) try the simple model to see if it hangs `--model=trivial`.\r\n\r\n2) With inception3, add a lock around global_step. something like\r\n```\r\n  with self.lock:\r\n        inc_global_step = global_step.assign_add(1)\r\n        fetches.append(inc_global_step)\r\n```\r\n\r\n  some where in the init function, you define the lock:\r\n       `self.lock = threading.Lock()`\r\n\r\nI was unable to get an infiniband setup yesterday. I will try again today.", "@shamoya \r\nThe tracing data is available in a pre-parsed (protobuf) format.  See the run_metadata option to Session.run, which returns tracing data in StepStats.", "@junshi15 @shamoya \r\nnote that `global_step` is also a part of computing graph, `assign_add` op has a `use_locking` arg.", "@byronyi , you're the best ! it's reproduced on trivial case (with ImageNet data).\r\nNow it will be much easier to debug, going full VLOG now - let's hope I'll have update soon.\r\n\r\n@suiyuan2009, it's not related to the global_step.\r\nI changed to have only the chief increment it and it doesn't have any affect.\r\n", "@byronyi, do your GDR patch have met same problem like stuck or failure?", "No I have not met such kind of problems, at least not during our internal testing. But my patch uses vastly different design compared to current verbs implementation so there is little I can tell about this particular issue.", "My observation agrees with @shamoya 's.\r\n\r\n1) the program got stuck with the trivial model, but it needed 8 GPUs per worker. Even 7 GPUs do not result in hang (after running 1000 iterations).\r\n2) the program got stuck either at warm-up or early iterations (< 100).\r\n3) locking global_step does not help.\r\n4) At the very beginning, some tensors changed size as shown below. This may not be an issue, but it will be good to find out why.\r\n\r\n```\r\ntensor and buffer size do not agree! buffer_size = 653 requested tensor size = 593 Tensor<type: int64 shape: [0,1] values: > key = /job:ps/replica:0/task:0/cpu:0;ccc0db7aa974ba53;/job:worker/replica:0/task:0/gpu:0;edge_37_report_uninitialized_variables/boolean_mask/Where;0:0\r\n\r\ntensor and buffer size do not agree! buffer_size = 649 requested tensor size = 589 Tensor<type: int64 shape: [0] values: > key = /job:worker/replica:0/task:0/gpu:0;62e354fc58da5afa;/job:ps/replica:0/task:0/cpu:0;edge_39_report_uninitialized_variables/boolean_mask/Squeeze;0:0\r\n```", "@katyakats @bkovalev\r\nOk, after reviewing the full logs, this is what we think is the root cause: \r\nA single GPU (in worker1) doesn't complete loading of the model parameters from the CPU.\r\nFor this GPU we don't see \"Async kernel done\" for the SEND/RECV operation (CPU:0 -> GPU:x locally).\r\n\r\nThe reason why it happens only with RDMA (and not with gRPC) is not known yet.\r\nThought about possible interrupts issue due to excessive interrupts (8 GPUs + NIC + NVMe drive which holds the ImageNet data, all on the same PCIe bus). however polling mode of Process_CQ (no interrupts from the NIC at all) didn't resolve the issue.\r\n\r\n[This](https://gist.github.com/shamoya/824d452be527d95902f20b59f868b391) is the problematic GPU relevant logs.\r\n\r\nThis is what I get from grepping one of the model parameters tensors  in the problematic GPU log (as above) VS one of the other valid GPUs:\r\n\r\nidos@MTR-IDOS $cat gpu2.log | grep \"affine1/biases\"\r\n2017-07-27 16:04:48.740960: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G109 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:2\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_609_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:2\"]() is dead: 0\r\n2017-07-27 16:04:48.748463: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35506 step 286 v/affine1/biases/read_G108 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:2\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_609_v/affine1/biases/read\", _device=\"/job:worker/replica:0/task:1/cpu:0\"](v/affine1/biases/read) is dead: 0\r\n\r\nidos@MTR-IDOS $cat gpu4.log | grep \"affine1/biases\"\r\n2017-07-27 16:04:48.740332: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G105 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"]() is dead: 0\r\n2017-07-27 16:04:48.748482: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35508 step 286 v/affine1/biases/read_G104 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", _device=\"/job:worker/replica:0/task:1/cpu:0\"](v/affine1/biases/read) is dead: 0\r\n2017-07-27 16:04:48.764311: I tensorflow/core/common_runtime/executor.cc:1612] 0x7f1fc4e67ab0 Async kernel done: v/affine1/biases/read_G105 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:4\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=-5325932350616196133, tensor_name=\"edge_605_v/affine1/biases/read\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"]()\r\n2017-07-27 16:04:48.765221: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 69 step 286 v_4/tower_4/gradients/v_4/tower_4/L2Loss_3_grad/mul = Mul[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v/affine1/biase\r\n/read_G105, v_4/tower_4/gradients/v_4/tower_4/mul_1_grad/Reshape_1) is dead: 0\r\n2017-07-27 16:04:48.765199: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 9 step 286 v_4/tower_4/L2Loss_3 = L2Loss[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v/affine1/biases/read_G105) is dead: 0\r\n2017-07-27 16:04:48.765338: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 38 step 286 v_4/tower_4/affine1/add = Add[T=DT_FLOAT, _device=\"/job:worker/replica:0/task:1/gpu:4\"](v_4/tower_4/affine1/MatMul, v/affine1/biase\r\n/read_G105) is dead: 0", "Just a wild guess, if the callback passed into the local async recv kernel is an erroneous remote send, would it appear to be stuck at the local recv? What's the downstream operator of that local recv?", "@shamoya intriguing discovery. Local tensor transfer is handled by BaseRemoteRendezvous. There is one difference between gRPC and RDMA: tolerate_dup_recv is set to `false` in [the former](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/distributed_runtime/rpc/rpc_rendezvous_mgr.cc#L42) and `true` in [the latter](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc#L34). The reason: gRPC only receives a tensor once, with verbs we have multiple receive attempts, since the tx/rx buffer may not be ready at early attempts. I ran some experiments by changing that to `true` in gPRC model, no hang was observed. However [this patch](https://github.com/tensorflow/tensorflow/commit/cbfd50ff0f01e1825922230a8bc6e5766da98dd7#diff-b9ae16e68ba80801fe243bb5e19bac51) totally broke the verbs code. I had to raise an issue [here](https://github.com/tensorflow/tensorflow/issues/11825). Something to worry about after this debug.\r\n\r\nA correction to what I said early, I finally saw hang with 7 GPUs per worker, after 2000 steps.\r\n\r\n\r\n", "Thanks @junshi15 for noticing this commit.\r\nNot sure I understand why you say BaseRemoteRendezvous is used in the case of local tensor transfer between GPU and CPU on the same worker.\r\n[This](https://gist.github.com/shamoya/36beb1d093d4b95a523e27d4deda16ea) is a more detailed log of the all the occurences of affine1/biases in the last iteration in worker1. We can see the RdmaRemoteRendezvous has completed successfully from the PS.\r\nI'm not so sure anymore this issue is even related to the Verbs code.", "@shamoya RdmaRemoteRendezvous is a derived class of BaseRemoteRendezvous. The local send/receive functions are defined in the base class. RdmaRemoteRendezvous only overrides RecvFromRemoteAsync().", "I noticed the date of that commit is 6 days ago, but I remember, if correctly, the issue came out earlier than that.\n\nPlease ignore me if I'm wrong. Maybe you guys could revert that commit locally first and test if the issue could still be reproduced.", "That patch is unrelated to this bug.  I am testing code prior to that. With that patch, the RDMA path won't run, and you will get an immediate crash.", "I digged a little deeper into the bug.\r\nOne of the NoOps from a single GPU is never sent (the thread is stuck in  CopyGPUTensorToCPUSync method). It appears that this method is not called from the grpc, but only from the verbs code. \r\nReverting commit  626d8d905aa412aaca02d171e5e0b4a1c407656b (Improve RDMA rendezvous speed) that made changes in this code area did not resolve the issue. \r\nDoes anyone have any ideas?\r\n\r\n", "CopyGPUTensorToCPUSync  is a synchronous wrapper for GPUUtil::CopyGPUTensorToCPU.\r\nIt was stuck only because CopyGPUTensorToCPU did not complete or did not send notification.\r\nBut grpc uses CopyGPUTensorToCPU, not sure why this is not an issue.", "thanks @junshi15 \r\nDo you have any thoughts on why CopyGPUTensorToCPU does not complete or send notification?", "I am looking at [CopyGPUTensorToCPU](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/core/common_runtime/gpu/gpu_util.cc#L304-L347). It eventually calls [ThenMemcpy](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/stream_executor/stream.h#L1492-L1497). The comment says the host memory needs to be registered. I am not familiar with this piece of code, but this is not done in the verbs code. If it is indeed required, then it is what's missing.", "Did you check for the 3 odd edge cases mentioned [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/rpc/grpc_worker_service.cc#L338)? Just a wild guess, if it is a `NoOps` it might be related to the `is_dead` parameter or zero-sized tensors.", "@byronyi I don't see all 3 odd cases are checked in your link (I don't see zero-size and is_dead is checked). Equivalent code in verbs is [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/verbs/rdma.cc#L707-L719), pretty much follows that link. is_dead is fed to SetProtoFromGPUSync, but zero-size is not checked in both cases.", "@katyakats When you reverted commit 626d8d9 (Improve RDMA rendezvous speed), where did the code hang? SetProtoFromGPUSync?", "@junshi15 thank you! we will check this direction! \r\nI did not check what method is stuck when I reverted the commit. Will check this as well", "Hi @junshi15 , following the discussion [here](https://github.com/tensorflow/tensorflow/issues/11825) and in this bug and after I review the code, I think we have a serious issue with tolerate_dup_recv mechanism (maybe this is why it was removed ? ).\r\n\r\nI think [this](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L209) is wrong in case we don't do a duplicate receive (if we have a buffer already in some step), becasue no one will UnRef it, in case the Recv will not be called.\r\nDo u think it might be the issue ?", "@shamoya UnRef is in [Recv](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L264). In what case Recv will not be called? In verbs case, Send only is only called once, but Recv is called multiple times.", "If Recv was called before the SEND ? \r\nIt doesn't have to be called multiple times (only first tensor appearance)", "If the first Recv is called before Send, then the waiting table does not have the tensor yet, Recv will put a callback in the table and return. Now, if a second Recv is called and still before Send, I think there is an issue since we allow multiple Recvs. In this case, [an empty tensor will be received](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L246). We will have trouble in the downstream. Is this what you are referring to?", "No @junshi15, you didn't follow me, I'll try to be more clear.\r\n\r\nLet's say RECV is called before SEND, and puts [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L678) callback in the table as u said. And let's say there's already a buffer for this tensor, meaning this callback will be called once (trigger TENSOR_WRITE).\r\nnow SEND flow performs [this](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L209) Ref() before calling the callback, since it assumes the RECV **can** be duplicated and called again.\r\nBut in this case it doesn't, and no one will do [this](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L264) Unref.\r\n\r\nWhen we are stuck in the bug, I noticed there's a huge diff between Refs and Unref on the GPU that is \"stuck\" (hundernds).\r\n\r\nNow, I do see [this](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L323) Unref, in the Item destructor, which might do the missing Unref, but I need to check if it happens.", "@shamoya Thanks for the explanation. A Recv-Send sequence does increase the reference count by one for send_dev_context. This can be a problem. I tried adding Unref right after [this](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L219), it crashed right away.\r\n\r\n@katyakats mentioned hang at `CopyGPUTensorToCPU`. I see similar issue. Here is a [stack trace](https://gist.github.com/junshi15/af3063e5bd22c911c30fcaafee060fa1) I captured with gdb when the program hung. It was stuck while trying to lock a mutex. Frame #2 is `SetProtoFromGPUSync` and Frame #20 is `CopyCPUTensorToGPU`. Note both of them call `ThenExecute`, which uses the same lock. So a nesting call like this tries to acquire a already obtained lock, a recipe for hang. Not sure what caused this situation.", "Wowww @junshi15, looks like this is it ! \r\n\r\nI've ran the test with inter/intra_op_parallelism_threads = 500 and it passed (multiple times).\r\nIt workaround the problem by reducing the probability to have this scenario (Schedule of the threadpool more likely to choose an idle thread).\r\nIt also explains why it happens when working with real data (queue readers also use the same threads)!\r\n\r\n@poxvoculi\r\nReally not clear to me how Schedule is possible on a thread which holds a mutex.\r\nCan't understand also where the context switch happens in this thread while it performs ThenExecute.\r\n\r\nThanks @junshi15 !\r\n", "By reducing the number of threads to 1 inter_thread I managed to reproduce the bug in a system with 2 GPUs per host and 2 hosts only  (2ps + 2 workers). I believe its the same issue because it occurs only when using verbs and I am getting the same backtrace as @junshi15 did. so I think we are close.\r\n\r\nBut I don't think **ThenExecute's lock** is the problem.\r\nI've added VLOG prints before and after the critical section, And counted them afterward. No one seems to be 'stuck' inside (locks and unlocks were equal).", "@shamoya Glad we are making some progress.\r\n@yanivbl6  Thanks for your help. This [line](https://gist.github.com/junshi15/af3063e5bd22c911c30fcaafee060fa1#file-gistfile1-txt-L2) seems to be waiting for a mutex. We need to identify the mutex inside `SetProtoFromGPUSync`.", "This [line](https://gist.github.com/junshi15/af3063e5bd22c911c30fcaafee060fa1#file-gistfile1-txt-L37) is not `CopyCPUTensorToGPU` itself. It looks like a lambda function (callback?) inside `CopyCPUTensorToGPU`.", "I think this line relate to the condition variable of \"Notification n\", which requires a mutex. (for waiting).\r\nIf I got this right The condition variable is waiting for the callback, that was scheduled after the stream in ThenExecute.", "I think the problem rises because the Sync deviceToDevice operation blocks the thread, preventing the earlier Async Device to Device operation from finishing- which, for some reason, blocks the later operation.\r\n\r\nWhat I did to check this was change the [call to the wrapper sync function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L724) to a call to the async function, and inserted the rest of the code (that which is executed after the sync wrapper) into then lambda callback function (\"done\"). With this setting, the application no longer hangs (Tested multipile times, few of which in 8 gpus system). \r\n@junshi15 , is there an additional motivation for the usage of the sync wrappers, beside the pending operations we execute after the operation is completed?", "@yanivbl6 Thanks for looking into it. There is another sync wrapper function, [SetProtoFromGPUSync](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L730). We should convert that back to async as well. \r\n\r\nThe motivation for using sync wrapper is mainly for simplicity. Otherwise, you end up with chained call-backs, which is actually fairly common in tensorflow code.\r\n\r\nPlease submit a PR if you have a fix.", "Great job @yanivbl6 , looks like this is it.\r\nWaiting for the PR with the fix.\r\n\r\nLooking now in the gRPC code, they are doing this flow aysnc, this is probably why we didn't see it when running with gRPC. ", "Thanks to all to solve it! Great job!  Go to check it !\r\nNeed to put it in r1.3 and master. \r\n", "@poxvoculi We found some of [the wrapper functions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/verbs_util.cc#L23-L70) caused hang. Do you know whether we are not supposed to use the sync version of these functions? I know CPU/GPU transfer can be slow,  but I did not expect it to hang. Thanks.", "I would avoid using any sync version of a function where an async version with callback is available.  The sync versions suspend a thread which could lead to a deadlock if you run out of threads.  It may be that by blocking you've also introduced a deadlock some other way, where the callback is waiting on something whose execution would have been triggered after the async invocation returned.   ", "@poxvoculi What we saw matches your description. Hang happens when 4+ GPUs are used. It goes away when we increase the number of threads. @yanivbl6 is working on a [fix](https://github.com/tensorflow/tensorflow/pull/12361).", "#12361 merged to master", "> @poxvoculi What we saw matches your description. Hang happens when 4+ GPUs are used. It goes away when we increase the number of threads. @yanivbl6 is working on a [fix](https://github.com/tensorflow/tensorflow/pull/12361).\r\n\r\n@junshi15  Cool thanks, I hit a very similar problem and got stuck during the warm up. As you suggested, I increased `inter_op_parallelism_threads` to work around the problem. \r\n\r\nbefore changing the number of threads, I couldn't get this far. \r\n`2018-10-20 01:54:02.736996: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.2 locally`\r\n", "@nicolefinnie - did you experience an hang with grpc+verbs?\r\nThis issue was (At least, to my knowledge) fixed already.\r\nWhat TF version are you experiencing this issue with?", "@yanivbl6  Thanks for your response. It's tensorflow `1.11` and I didn't specify the server protocol so it's `grpc` by default. ", "@nicolefinnie The thread is about an issue related to grpc+verbs, and it has been fixed by @yanivbl6.\r\nI do not know why you are experiencing the issue with grpc only. ", "I experienced an hang with the master branch and vanilla grpc about a week ago, and it was already fixed in master. I would suggest trying to pull the latest commit. "]}, {"number": 11724, "title": "Importing TensorFlow breaks numpy.linalg.matrix_power()", "body": "#10771 \r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: bazel release 0.5.1\r\n- **CUDA/cuDNN version**: CUDA V8.0.61 / \r\n- **GPU model and memory**: NVIDIA Titan X Fall 2016\r\n- **Exact command to reproduce**:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(2000, 2000)\r\nY = np.linalg.matrix_power(X, 30)\r\n```\r\n\r\n### Describe the problem\r\nThis seems to be a bug where importing tensorflow breaks numpy.linalg.matrix_power()\r\n\r\nThe two following code snippets are producing different results on my system when entered directly into the python interpreter. Note, I am restarting the python kernel between running each block.\r\n\r\n**This works:**\r\n```python\r\nimport numpy as np\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(2000, 2000)\r\nY = np.linalg.matrix_power(X, 30)\r\n>>> X \r\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\r\n        -1.14190142, -1.31097037],\r\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\r\n         1.57708821, -0.8128021 ],\r\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\r\n         0.39344443,  0.28651827],\r\n       ..., \r\n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\r\n         1.28756456, -0.6953778 ],\r\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\r\n         0.51428298,  0.72148202],\r\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\r\n         0.95170926, -1.15237806]])\r\n>>> Y\r\narray([[ -1.04752205e+48,   2.10841282e+47,  -4.54826843e+47, ...,\r\n         -7.84526353e+46,  -4.45185369e+47,  -1.96340973e+47],\r\n       [ -5.40802471e+46,   1.12546832e+48,  -1.88764494e+47, ...,\r\n         -3.72182046e+47,   7.97461852e+47,  -5.04546546e+46],\r\n       [ -3.59835691e+47,  -6.90559050e+46,  -8.78538707e+47, ...,\r\n          7.67940928e+47,   2.10052546e+47,   1.75193723e+47],\r\n       ...,\r\n       [  2.20970288e+46,   3.60679821e+47,   5.76631889e+47, ...,\r\n          1.21938369e+48,  -8.61048462e+47,  -3.93610572e+47],\r\n       [ -1.19116636e+48,   2.47318954e+48,   2.65693291e+46, ...,\r\n          9.18513286e+47,   3.91490216e+47,  -7.08113716e+47],\r\n       [ -2.25527724e+47,  -4.94088618e+46,  -2.69359430e+47, ...,\r\n         -4.07174632e+47,   7.38250907e+47,   5.86758288e+46]])\r\n```\r\n**This produces the wrong result:**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(2000, 2000)\r\nY = np.linalg.matrix_power(X, 30)\r\n>>> X\r\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\r\n        -1.14190142, -1.31097037],\r\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\r\n         1.57708821, -0.8128021 ],\r\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\r\n         0.39344443,  0.28651827],\r\n       ..., \r\n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\r\n         1.28756456, -0.6953778 ],\r\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\r\n         0.51428298,  0.72148202],\r\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\r\n         0.95170926, -1.15237806]])\r\n>>> Y\r\narray([[ -3.40382764e+91,   2.85027458e+91,   1.14039870e+91, ...,\r\n          3.32682992e+91,   6.00166234e+91,   2.33233825e+91],\r\n       [  3.86088264e+91,   1.15500453e+92,   2.83821815e+91, ...,\r\n         -6.16959058e+91,  -1.91501705e+91,  -4.67672849e+91],\r\n       [  5.05026067e+91,   7.72796711e+91,  -4.70112473e+91, ...,\r\n          8.41553063e+90,  -2.66176140e+91,   8.50899233e+90],\r\n       ...,\r\n       [ -2.08592878e+91,   2.28435173e+91,   9.15188619e+90, ...,\r\n         -1.25550051e+91,   1.85247259e+91,  -8.73231986e+90],\r\n       [ -4.73923534e+91,   1.61385540e+92,   1.26364668e+92, ...,\r\n          2.83667716e+91,   5.06236372e+90,  -5.18395025e+91],\r\n       [ -1.52984791e+91,  -5.57421948e+90,   3.27657918e+91, ...,\r\n         -7.08972359e+91,  -1.58912068e+91,  -1.22216698e+91]])\r\n```\r\n\r\nThe values for X are exactly the same. Interestingly, if you run the first code block, then import tensorflow without restarting the kernel and recompute the matrix power, the correct result is returned.\r\n\r\nI haven't been able to find a reference to this anywhere online, and it's a big problem not being able to use tensorflow and numpy in the same script.\r\n", "comments": ["Hmm, I am not able to reproduce it. These are  large numbers. Does this occur when you use say matrix power 2? Could you print out the X's in both cases (at least as many as you do for Y) just so we can verify they are exactly the same.\r\n\r\nMy guess would be TensorFlow configures the floating point flag settings different from default.", "I do still get the wrong result if I use matrix power 2:\r\n\r\n```python\r\nIn [1]:\r\nimport numpy as np\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(1000, 1000)\r\nY = np.linalg.matrix_power(X, 2)\r\nIn [2]:\r\nX\r\nOut[2]:\r\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.0941923 ,\r\n        -1.14761094, -0.35811408],\r\n       [ 0.55596268,  0.89247389, -0.42231482, ...,  0.15843385,\r\n        -1.14190142, -1.31097037],\r\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.19240421,\r\n        -1.21251574, -0.08059852],\r\n       ..., \r\n       [ 0.43624889, -1.2972301 ,  1.99717648, ..., -0.42071168,\r\n        -0.10894278, -0.45209518],\r\n       [-0.10034023, -0.13437346, -0.15060322, ...,  0.35111034,\r\n         1.36105416, -1.41088647],\r\n       [-1.24468854, -1.20417642,  0.99255852, ..., -0.79728264,\r\n         0.87475609,  1.37183066]])\r\nIn [3]:\r\n\r\nY\r\nOut[3]:\r\narray([[-20.90456457,  63.66601763,   4.60530742, ...,  22.82832076,\r\n        -79.92214929,  37.8851418 ],\r\n       [-16.69474231,  13.14948903,  -5.72608797, ...,  65.28426251,\r\n         19.15823444, -15.81612538],\r\n       [ -2.13591144, -62.87672722,   7.74009454, ..., -21.37731197,\r\n         22.43966432, -23.05161539],\r\n       ..., \r\n       [ 53.28579179,  56.25607449, -16.17015098, ..., -21.17820313,\r\n        -15.59329817, -10.84431817],\r\n       [  9.36027737, -38.47373171, -13.87875906, ...,  80.51944872,\r\n        -34.13373385, -33.20098989],\r\n       [-32.40022125,   8.00801434, -20.68091457, ..., -44.3388419 ,\r\n         69.68672123,  10.77702875]])\r\n```\r\n\r\n\r\n```python\r\nIn [1]:\r\nimport numpy as np\r\nimport tensorflow as tf\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(2000, 2000)\r\nY = np.linalg.matrix_power(X, 2)\r\nIn [2]:\r\nX\r\nOut[2]:\r\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\r\n        -1.14190142, -1.31097037],\r\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\r\n         1.57708821, -0.8128021 ],\r\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\r\n         0.39344443,  0.28651827],\r\n       ..., \r\n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\r\n         1.28756456, -0.6953778 ],\r\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\r\n         0.51428298,  0.72148202],\r\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\r\n         0.95170926, -1.15237806]])\r\nIn [3]:\r\nY\r\nOut[3]:\r\narray([[  896.31054468,  2315.19199227,   106.53963366, ...,\r\n         2919.22268215,   194.34572697,   698.64701546],\r\n       [  688.43757414,  1574.75771259,  1761.10054579, ...,\r\n         1757.49918001,  1817.00223281,   777.72532453],\r\n       [ 1412.42877031, -1482.4939304 ,  1034.41817963, ...,\r\n          494.08789763,   537.5932247 ,   371.50003778],\r\n       ..., \r\n       [ -427.99247673,   942.29074458, -2296.56391085, ...,\r\n         -833.13985797,  1251.9797819 , -1913.47322301],\r\n       [ -307.34497424,  -981.15390609,  -204.04086649, ...,\r\n         -207.92168833,   218.47627601,    67.95123058],\r\n       [ -530.56347763,  -281.01241822,  -398.9338794 , ...,\r\n         -912.01258927,   692.58480244,  -921.82876347]])\r\n```\r\n\r\nInterestingly, if the matrix is smaller, I can get the correct result. Unfortunately I'm not working with small matrices, I'm working with 100,000 x 20,000 matrices:\r\n\r\n```python\r\nIn [1]:\r\nimport numpy as np\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(5, 5)\r\nY = np.linalg.matrix_power(X, 2)\r\n\r\nIn [2]:\r\nX\r\nOut[2]:\r\narray([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\r\n       [-0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ],\r\n       [ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323],\r\n       [ 0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574],\r\n       [-2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462]])\r\nIn [3]:\r\n\r\nY\r\nOut[3]:\r\narray([[ -1.15833119,   7.07817798,   3.56548186,   3.34635217,\r\n          6.21816086],\r\n       [ -3.75696704,   0.40564612,  -0.83937785,  -2.64350512,  -0.4820835 ],\r\n       [ -2.15008482,   3.01799485,   0.85877337,  -0.02605015,\r\n          2.10746742],\r\n       [  1.3838972 ,   1.16416823,  -0.85823278,   1.30044014,\r\n         -1.06041697],\r\n       [-11.06016005,   1.23122545,   0.17454991,  -7.60014337,\r\n          1.66987577]])\r\n```\r\n\r\nAlso correct:\r\n\r\n```python\r\nIn [1]:\r\nimport numpy as np\r\nimport tensorflow as tf\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(5, 5)\r\nY = np.linalg.matrix_power(X, 2)\r\nIn [2]:\r\nX\r\nOut[2]:\r\narray([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799],\r\n       [-0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ],\r\n       [ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323],\r\n       [ 0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574],\r\n       [-2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462]])\r\nIn [3]:\r\nY\r\nOut[3]:\r\narray([[ -1.15833119,   7.07817798,   3.56548186,   3.34635217,\r\n          6.21816086],\r\n       [ -3.75696704,   0.40564612,  -0.83937785,  -2.64350512,  -0.4820835 ],\r\n       [ -2.15008482,   3.01799485,   0.85877337,  -0.02605015,\r\n          2.10746742],\r\n       [  1.3838972 ,   1.16416823,  -0.85823278,   1.30044014,\r\n         -1.06041697],\r\n       [-11.06016005,   1.23122545,   0.17454991,  -7.60014337,\r\n          1.66987577]])\r\n```", "Can you reproduce this outside of Jupyter or Ipython?", "Yes.\r\n\r\n```python\r\n[dan@orkney ~]$ python\r\nPython 3.6.1 (default, Mar 27 2017, 00:27:06)\r\n[GCC 6.3.1 20170306] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> np.random.seed(seed=0)\r\n>>> X = np.random.randn(2000, 2000)\r\n>>> Y = np.linalg.matrix_power(X, 2)\r\n>>> X\r\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\r\n        -1.14190142, -1.31097037],\r\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\r\n         1.57708821, -0.8128021 ],\r\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\r\n         0.39344443,  0.28651827],\r\n       ...,\r\n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\r\n         1.28756456, -0.6953778 ],\r\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\r\n         0.51428298,  0.72148202],\r\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\r\n         0.95170926, -1.15237806]])\r\n>>> Y\r\narray([[  910.34426793,  2319.0172763 ,    63.12608588, ...,\r\n         2958.19091355,   -27.49231812,   567.53132384],\r\n       [  769.19726704,  1604.25576828,  1787.40414157, ...,\r\n         1687.0179502 ,  1923.88228017,   832.52806049],\r\n       [ 1491.82177556, -1398.59394668,  1029.47973498, ...,\r\n          603.01417689,   446.60541119,   484.82890574],\r\n       ...,\r\n       [ -452.5080765 ,   840.10603188, -2303.87826274, ...,\r\n         -763.55186962,  1181.29736415, -1825.21634485],\r\n       [ -199.92457917,  -952.89987624,  -279.06374383, ...,\r\n         -146.58255678,   174.25003392,   183.98291186],\r\n       [ -554.67074632,  -223.03466867,  -479.93292745, ...,\r\n         -950.28529065,   800.70076349,  -898.34404592]])\r\n```\r\n\r\nOr with a larger power:\r\n```python\r\n[dan@orkney ~]$ python\r\nPython 3.6.1 (default, Mar 27 2017, 00:27:06)\r\n[GCC 6.3.1 20170306] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy as np\r\n>>> import tensorflow as tf\r\n>>> np.random.seed(seed=0)\r\n>>> X = np.random.randn(2000, 2000)\r\n>>> Y = np.linalg.matrix_power(X, 30)\r\n>>> X\r\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\r\n        -1.14190142, -1.31097037],\r\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\r\n         1.57708821, -0.8128021 ],\r\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\r\n         0.39344443,  0.28651827],\r\n       ...,\r\n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\r\n         1.28756456, -0.6953778 ],\r\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\r\n         0.51428298,  0.72148202],\r\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\r\n         0.95170926, -1.15237806]])\r\n>>> Y\r\narray([[ -1.69238825e+91,   1.72147982e+91,   1.09701486e+91, ...,\r\n          8.96734224e+90,   2.21217699e+91,   2.31904059e+91],\r\n       [ -7.07657012e+89,   4.85685830e+91,   1.74187144e+91, ...,\r\n         -1.61727709e+91,   7.45551485e+90,  -2.72503249e+91],\r\n       [  2.97126997e+91,   2.33421956e+91,  -1.95668005e+91, ...,\r\n          8.42491285e+90,  -2.30603740e+91,  -9.68175387e+90],\r\n       ...,\r\n       [ -3.91887830e+91,   6.66524075e+90,   1.44916935e+91, ...,\r\n         -1.31096552e+91,  -6.69442115e+90,  -2.46114967e+88],\r\n       [ -1.88415868e+91,   8.90475448e+91,   6.85311519e+91, ...,\r\n         -3.61608344e+90,   8.49098335e+90,  -3.72585092e+91],\r\n       [ -2.17044444e+91,   1.01337006e+91,   3.26124960e+90, ...,\r\n         -2.58026794e+91,  -5.82000633e+89,  -5.54466196e+90]])\r\n```\r\nIf I don't import tensorflow, then I get the same result as posted above.", "Could you try with the latest stock build and see if it still occurs?", "Sorry, could you please clarify what you mean by stock build? Do you just mean the precompiled binary?", "Yeah that's what I mean. Unfortunately I can't really reproduce this on my ubuntu machine and I don't have an Arch machine. This is definitely bizarre.\r\n", "@dburkhardt are you using MKL? It comes by default if you use Anaconda distro. There is some weird interaction between tensorflow and scipy when MKL is used (I reproduced one here https://github.com/tensorflow/tensorflow/issues/10005)", "I am indeed using Intel MKL, @yaroslavvb. Actually the timing of discovering this bug coincided with compiling numpy with MKL.\r\n\r\nI would be a shame if it turns out that Tensorflow isn't compatible with MKL. I've been thinking about switching to OpenBLAS, perhaps this is the time.", "If you can make a reliable repro it could make sense to report it with Intel. I recently found another MKL-caused bug in linalg and they fixed it promptly after [reporting](https://software.intel.com/en-us/forums/intel-distribution-for-python/topic/628049#comment-1904231)", "Thanks for the insight @yaroslavvb! ", "@dburkhardt as a possible work-around, can you try lowering the number of MKL threads?\r\nSet env var MKL_NUM_THREADS ", "If I run `export MKL_NUM_THREADS=\"1\"` before opening python, then I get the correct result, but if I set MKL_NUM_THREADS=\"2\" or greater, then I get the wrong answer again.", "Sounds like an MKL bug. I also saw a bug where MKL's SVD gave incorrect results for certain numbers of MKL_NUM_THREADS (however it was order of magnitude faster than openblas version)", "But why would this bug only appear after importing Tensorflow? I don't have any problems with MKL outside of trying to use it with Tensorflow.", "Dunno....my first suspicion would be that it's related to dynamic linking. Both MKL and TensorFlow are .so files loaded with dlopen, at which point dynamic linker tries to resolve all the symbols, remap all internal references to global variables and do other dark magic. If .so libraries were compiled with incompatible flags, bad things happen, like https://github.com/tensorflow/models/issues/523", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I had an issue very similar to this. Running Numpy 1.11.1 with MKL 2017 and TF 1.2.0.\r\n\r\nTest script:\r\n```python\r\nimport numpy as np\r\n\r\nbad = True\r\nif bad:\r\n    import tensorflow as tf\r\n\r\nif __name__ == '__main__':\r\n    rows = 100\r\n    cols = 100\r\n    X = np.ones((rows, cols))\r\n    w = np.ones(cols)\r\n\r\n    print np.dot(X, w).sum(), np.dot(X, w).sum(), np.dot(X, w).sum()\r\n    print 'correct:', X.sum()\r\n```\r\n\r\nWhen ``bad`` is True, it outputs incorrect numbers. The exact numbers are non-deterministic, but they're almost always wrong. Example output:\r\n```\r\n15300.0 30000.0 26800.0\r\ncorrect: 10000.0\r\n```\r\n\r\nIf I either don't import TF or I set ``MKL_NUM_THREADS=1``, then all works correctly\r\n```\r\n10000.0 10000.0 10000.0\r\ncorrect: 10000.0\r\n```\r\n\r\nIt would be nice for this to work so that I could use multi-threaded Numpy and TF in the same Python process without postponing TF import as much as possible.", "@eamartin 1.2 is really old, this kind of issue should have been fixed by changing how TF symbols get linked in https://github.com/tensorflow/tensorflow/commit/5c7f9e316d8c7735308a217310350d416d7498cc\r\n", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am going to mark this as closed unless someone has a report from TF 1.8.  No disrespect.  It just pops up in my queue and without a repro for TF 1.8 and there are PRs that claim to have possibly fixed the problem it is really stale.  If the issue still exists with MKL then we can reopen and then assign to @tatianashp and we can get Intel to address it.   \r\n\r\n**Edit:** that indicated we can reopen I meant to include that as an option."]}, {"number": 11723, "title": "tf.nn.sparse_softmax_cross_entropy_with_logits raise Segmentation fault", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\ncode from tensorflow/nmt\r\n- **OS Platform and Distribution**:centos 7\r\n- **TensorFlow installed from**:pip install\r\n- **TensorFlow version**:('v1.2.0-5-g435cdfc', '1.2.1')\r\n- **Python version**: 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:without gpu\r\n- **GPU model and memory**:None\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n### Describe the problem\r\nAs the code pointed, tf.nn.sparse_softmax_cross_entropy_with_logits seems supporting less size tensor than tf.nn.softmax_cross_entropy_with_logits. At least, do shape check rather than Segmenation fault error.\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nlabels=tf.placeholder(tf.int32, [None,128])\r\nlogits=tf.placeholder(tf.float32, [None,128,600000])\r\ncrossent1=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\r\nlabels_one_hot=tf.one_hot(labels, 600000, dtype=tf.int32)\r\ncrossent2 = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\r\nlogits_data=np.random.rand(39,128,600000)\r\nlabels_data=np.random.randint(600000,size=(39,128))\r\nfd={logits:logits_data, labels:labels_data}\r\nwith tf.Session() as sess:\r\n    sess.run([crossent2], feed_dict=fd)\r\n    print 'OK2' # OK2\r\nwith tf.Session() as sess:\r\n    sess.run([crossent1], feed_dict=fd)\r\n    print 'OK1' #Segmentation fault\r\n```\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "@reedwm i have updated the problem, with my experiment code.", "I cannot reproduce. I ran that code after replacing the \"600000\"s with \"6000\"s so it would finish in a reasonable amount of time, and the code worked fine. \r\n\r\n[`tf.nn.sparse_softmax_cross_entropy_with_logits` ](https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits) requires `labels` to have one less dimension as `logits`. [`tf.nn.softmax_cross_entropy_with_logits`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits) requires `labels` and `logits` to have the same shape. You do this in your sample code, so I'm not sure what the problem is.", "@reedwm 6000 work fine, but 600000 not.", "With 600000, `logits_data` is 12 GB of data. Using this in a feed dict will then take even more memory. You are probably getting the segmentation fault due to a lack of memory.", "but tf.nn.softmax_cross_entropy_with_logits works"]}, {"number": 11722, "title": "Update wide_n_deep_tutorial.py", "body": "-mkdtemp is low-level, and creates a temporary file somewhere out of sight that requires users to delete the file themselves. For the use case of a tutorial, it is best to use a method that cleanups the file when finished\r\n-TemporaryFile does such as described above, but not available in Python 2", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11721, "title": "Memory Leak from Training Step", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, although my code is somewhat based on the MNIST deep learning tutorial.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04 VERSION=\"14.04.5 LTS, Trusty Tahr\"\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled via the VirtualEnv method\r\n- **TensorFlow version (use command below)**:\r\n1.1 and 1.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA Version 8.0.44\r\n- **GPU model and memory**:\r\nGeForce GTX 780M 4GB\r\n- **Exact command to reproduce**:\r\n`self.sess.run(self.train_step, feed_dict={self.x: trainingdata, self.y_true: traininglabels, self.keepratio: self.training_keep_rate})`\r\n\r\n### Describe the problem\r\n\r\nThis is very similar to [the bug report I submitted here](https://github.com/tensorflow/tensorflow/issues/9590), but is a bit of a slower leak and is present in both TF 1.1 and 1.2.  I have finalized my graph.  Using the architecture described by [Zeiler et al., 2013 (ZF Net)](https://arxiv.org/pdf/1311.2901v3.pdf), batch sizes of 64, and 224x224 grayscale (1 channel) input, it leaks approximately 4GB after approximately 3000 batches.  This makes it unworkable, for say, 80 epochs of ImageNet training.  I have confirmed that the leak either does not occur or is much less severe (hard to tell which) if I comment out the training line (i.e. still do all of my preprocessing and loading).\r\n\r\n\r\nAs directed in that last linked issue, I tried to call sess.run with \r\noptions=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE), run_metadata=run_metadata\r\nand start the program with \r\nenv TF_CPP_MIN_VLOG_LEVEL=1 python deep_learning_main.py\r\n\r\nbut the amount of spew was enormous, and it won't respond to keyboard interrupts (I have to kill the job).  If that info would be helpful, how do I go about recording/saving this information properly to upload and help you all debug?\r\n\r\n[1_08.zip](https://github.com/tensorflow/tensorflow/files/1170372/1_08.zip)", "comments": ["Write the log (with TF_CPP_MIN_VLOG_LEVEL=1) to a file and upload it somewhere so that we can download it.\r\n\r\nAlso, see if you can reproduce the problem with a simpler program. The current program is over 1000 lines, making it hard to debug a memory leak. Additionally, I cannot run the program because you have some hardcoded paths, and the matplotlib dependency makes it hard to run internally. See if synthetic data can be used to remove the dependency on the imagenet data.", "I apologize for not getting back to this and being able to help; I've run out of time and had to move on to a new project.  However, I have had a similar issue with PyTorch; apparently it is an issue with CUDA and maybe my GPU.  Perhaps the TF leak is caused by same issue?  See\r\n\r\nhttps://discuss.pytorch.org/t/memory-usage-leak/5645", "@reedwm btw, regarding using TF_CPP_MIN_VLOG_LEVEL to debug memory leaks, it became less useful between 1.0.1 and 1.1\r\n\r\nSome of the deallocation messages started missing allocation ids, so you can't tell how much memory got reclaimed.\r\n\r\nie for some of my scripts I see messages like this in TF 1.1\r\n`'2017-05-09 15:31:16.865982: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: \"cpu\" }'`\r\n\r\ninstead of following in TF 1.0.1\r\n`I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocation_id: 235 allocator_name: \"cpu\" }\r\n`", "@JamesKostas That's ok. Respond to this post if you ever get time to upload the logs or reproduce the problem with a simpler program.\r\n\r\n@yaroslavvb Can you file a separate bug? allocation_id might be only printed if its not 0, but I'm not sure.", "@reedwm -- filed under https://github.com/tensorflow/tensorflow/issues/13087", "Without a reproducible test case, it seems unlikely we are going to make much headway, especially if you are working on a new project. So I am closing for now.", "@zheng-xq, FYI."]}, {"number": 11720, "title": "how to use the trained model(build in OOP schema) to predict new samples?", "body": "Hi, there\r\n   I write a OOP schema cnn model, but there is some error when predict. I know that if define Variable and write the code like `c` style in one file, it is easy to restore the value, but in OOP schema, it is something wrong. Here is my class:\r\n```python\r\nclass TextCNN(object):\r\n    \"\"\"\r\n    A CNN for text classification.\r\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n                 is_training,\r\n                 sequence_length,\r\n                 num_classes,\r\n                 vocab_size,\r\n                 embedding_size,\r\n                 filter_sizes,\r\n                 num_filters,\r\n                 l2_reg_lambda=0.0,\r\n                 drop_out=1.0):\r\n        self.is_training = is_training\r\n        self.sequence_length = sequence_length\r\n        self.num_classes = num_classes\r\n        self.vocab_size = vocab_size\r\n        self.embedding_size = embedding_size\r\n        self.filter_sizes = filter_sizes\r\n        self.num_filters = num_filters\r\n        self.l2_reg_lambda = l2_reg_lambda\r\n        self.input_x_test = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x_text')\r\n        self.input_y_test = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y_text')\r\n        if self.is_training:\r\n            self.dropout_keep_prob = drop_out\r\n        else:\r\n            self.dropout_keep_prob = 1.0\r\n\r\n        # Embedding layer\r\n        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\r\n            self.W = tf.Variable(\r\n                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\r\n                name=\"W\")\r\n        self.l2_loss = tf.constant(0.0)\r\n\r\n    def inference(self, input_x):\r\n        if not self.is_training:\r\n            input_x = self.input_x_test\r\n        embedded_chars = tf.nn.embedding_lookup(self.W, input_x)\r\n        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\r\n\r\n        # Create a convolution + maxpool layer for each filter size\r\n        pooled_outputs = []\r\n        for i, filter_size in enumerate(self.filter_sizes):\r\n            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\r\n                # Convolution Layer\r\n                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\r\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\r\n                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\r\n                conv = tf.nn.conv2d(\r\n                    embedded_chars_expanded,\r\n                    W,\r\n                    strides=[1, 1, 1, 1],\r\n                    padding=\"VALID\",\r\n                    name=\"conv\")\r\n                # Apply nonlinearity\r\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\r\n                # Maxpooling over the outputs\r\n                pooled = tf.nn.max_pool(\r\n                    h,\r\n                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\r\n                    strides=[1, 1, 1, 1],\r\n                    padding='VALID',\r\n                    name=\"pool\")\r\n                pooled_outputs.append(pooled)\r\n\r\n        # Combine all the pooled features\r\n        num_filters_total = self.num_filters * len(self.filter_sizes)\r\n        h_pool = tf.concat(pooled_outputs, 3)\r\n        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\r\n\r\n        # Add dropout\r\n        with tf.name_scope(\"dropout\"):\r\n            h_drop = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob)\r\n\r\n        # Final (unnormalized) scores and predictions\r\n        with tf.name_scope(\"output\"):\r\n            W = tf.get_variable(\r\n                \"W\",\r\n                shape=[num_filters_total, self.num_classes],\r\n                initializer=tf.contrib.layers.xavier_initializer())\r\n            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\r\n            self.l2_loss += tf.nn.l2_loss(W)\r\n            self.l2_loss += tf.nn.l2_loss(b)\r\n            logits = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\r\n            return logits\r\n\r\n    def loss(self, logits, input_y):\r\n        # CalculateMean cross-entropy loss\r\n        with tf.name_scope(\"loss\"):\r\n            if not self.is_training:\r\n                input_y = self.input_y_test\r\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\r\n            loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\r\n            return loss\r\n\r\n    def training(self, loss, learning_rate):\r\n        tf.summary.scalar('loss', loss)\r\n        optimizer = tf.train.AdamOptimizer(learning_rate)\r\n        global_step = tf.Variable(0, name='global_step', trainable=False)\r\n        train_op = optimizer.minimize(loss, global_step=global_step)\r\n        return train_op\r\n\r\n    def evaluation(self, logits, input_y):\r\n        if not self.is_training:\r\n            input_y = self.input_y_test\r\n        predictions = tf.argmax(logits, 1, name=\"predictions\")\r\n        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\r\n        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\r\n        return accuracy\r\n```\r\n\r\n## and this is my train file:\r\n```python\r\ncnn = TextCNN(...)\r\nlogits = cnn.inference(x_batch)\r\nloss = cnn.loss(logits, y_batch)\r\ntrain_op = cnn.training(loss, FLAGS.learning_rate)\r\neval_correct = cnn.evaluation(logits, y_batch)\r\n_, loss_value, accuracy = sess.run([train_op, loss, eval_correct])\r\nif step % FLAGS.checkpoint_every == 0 and step != 0:\r\n    path = saver.save(sess, checkpoint_prefix, global_step=step)\r\n```\r\n\r\n\r\n## and this is my predict file:\r\n```python\r\nwith graph.as_default():\r\n    session_conf = tf.ConfigProto(\r\n        allow_soft_placement=FLAGS.allow_soft_placement,\r\n        log_device_placement=FLAGS.log_device_placement)\r\n    sess = tf.Session(config=session_conf)\r\n\r\n    data_set_test = DataSet(...)\r\n\r\n    with sess.as_default():\r\n        cnn = TextCNN(\r\n            is_training=False,\r\n            sequence_length=data_set_test.max_seq_len,\r\n            num_classes=len(data_set_test.label_dict),\r\n            vocab_size=len(data_set_test.vocab),\r\n            embedding_size=FLAGS.embedding_dim,\r\n            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\r\n            num_filters=FLAGS.num_filters,\r\n            l2_reg_lambda=FLAGS.l2_reg_lambda,\r\n            drop_out=FLAGS.dropout_keep_prob\r\n        )\r\n\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, checkpoint_file)\r\n        x_batch, y_batch = data_set_test.next_batch(-1, True, True)\r\n        logits = cnn.inference(x_batch)\r\n        results = sess.run(logits, feed_dict={cnn.input_x_test: x_batch})\r\n        pass\r\n```\r\n\r\n\r\n## error is:\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value output/b_1\r\n\t [[Node: output/b_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@output/b_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](output/b_1)]]\r\n```\r\n\r\nDo you know how to load the Variable like OOP schema, is there a demo projects?\r\nThanks!\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11719, "title": "Add a custom tag for use by fusion ops", "body": "Currently when replacing an op with a fusion of many, the information which is carried by that fusion op is limited to a small fixed enumeration.\r\n\r\nI have previously added a 'custom' entry to that enumeration, however it isn't really enough to label an arbitrary and non-fixed set of fuse results.\r\n\r\nThis adds a general integer that can be used by any back end to label fused ops arbitrarily.\r\n\r\nThere are other ways to achieve this:\r\n- replacing the FusionKind enumeration with an integer field\r\n- allowing read-write access to the metadata structure, and replacing it with something like a map\r\n- some other thing I can't think of right now.\r\n\r\nI'm happy to go with a different scheme, but I do think that the HloInstruction needs an annotation interface of some kind.\r\n\r\n\r\n\r\n", "comments": ["actually - i've decided to abandon using fusing as a method for representing custom blocks of code - i will use sub-computations and Call ops instead.   this problem is no longer an issue.", "actually - maybe this is a thing anyway.\r\n", "Can one of the admins verify this patch?", "no - sorry.  changed my mind again.  the code which looked promising re using the fusion op is the same as the code which I write - it cannot correctly fuse a set of nodes where one is referred to by 2 others.", "once again sorry - i have found a way of getting fuse to behave the way I want,  so I would like this change (or something similar to allow a bit of meta-data to be attached to a fusion instruction or the internal computation).\r\n\r\nanother idea - perhaps - is to allow the API to set the name of the internal instruction.  I don't like using strings as a place to store enumerations, but it is an alternative.\r\n\r\nyet another is to allow the API to set the name of the instruction itself - same problem with using a string as an enumeration.\r\n", "Can one of the admins verify this patch?", "@vrv I may have confused things by opening and closing this one over and over.  I am now sure that I would like some sort of annotation mechanism for fusion instructions (and ideally also computations).\r\n\r\n", "(I have very little knowledge about XLA at this point, so assigning to others!)", "@DavidNorman Can you please explain the motivation behind FusionKind::kCustom and this custom tag? I'd like to better understand the limitations in the current scheme.", "sure thing.\r\n\r\ncurrently, when a fusion instruction is created it can get one of the enumeration values to indicate its type.  I added 'kCustom' because there are ops in our device which are not really fusions that fit the current set of enumerations.\r\n\r\nI really need lots of kCustom enumeration values though, because I have more than one specialized ops. \r\n\r\nI added another 'sub_type' kind of field for this.   However, I think I could static_cast<> in and out of an int - which perhaps would be nicer.    A sub_type field makes the code neater. Or maybe it doesn't.   I'll make the casting change and see if it feels ok.\r\n\r\n\r\n", "ok - i will just do it with the existing enum and some type casting.   closing this off :)\r\n"]}, {"number": 11718, "title": "gather_nd InvalidArgumentError", "body": "2017-07-24 10:35:25.247357: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).\r\n\t [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims, conv-maxpool-3/stack)]]\r\n2017-07-24 10:35:25.247422: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).\r\n\t [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims, conv-maxpool-3/stack)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1039, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1021, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n    next(self.gen)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).\r\n\t [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims, conv-maxpool-3/stack)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"mix.py\", line 66, in <module>\r\n    autoen_nn.clf_train_epoch(x_train,y_train,batch_size,dropout_rate = 0.5)\r\n  File \"/Users/MilesZhao/Desktop/ORNL/data/epath_data/yong/cnn.py\", line 248, in clf_train_epoch\r\n    feed_dict = feed_dict)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).\r\n\t [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims, conv-maxpool-3/stack)]]\r\n\r\nCaused by op 'conv-maxpool-3/GatherNd', defined at:\r\n  File \"mix.py\", line 64, in <module>\r\n    num_classes)\r\n  File \"/Users/MilesZhao/Desktop/ORNL/data/epath_data/yong/cnn.py\", line 125, in __init__\r\n    filter_size)\r\n  File \"/Users/MilesZhao/Desktop/ORNL/data/epath_data/yong/cnn.py\", line 217, in conv_layer\r\n    local_in_doc_mat = tf.gather_nd(in_doc_mat,temp)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1321, in gather_nd\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): flat indices[12287, :] = [127, 42] does not index into param (shape: [32,512,100,1]).\r\n\t [[Node: conv-maxpool-3/GatherNd = GatherNd[Tindices=DT_INT32, Tparams=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](ExpandDims, conv-maxpool-3/stack)]]\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks, in particular detailed instructions to reproduce the problem. Thank you."]}, {"number": 11717, "title": "Model file example in Tensorboard repository", "body": "[`graph_run_run2.pbtxt`](https://github.com/tensorflow/tensorboard/blob/master/tensorboard/demo/data/graph_run_run2.pbtxt) example was moved to tensorboard repository.", "comments": ["Can one of the admins verify this patch?"]}, {"number": 11716, "title": "Fix for invalid Darkflow command", "body": "The command \"`./flow --model cfg/tiny-yolo-voc.cfg --load bin/tiny-yolo-voc.weights --savepb --verbalise=True`\" is not a valid Darkflow command and will result in the following error:\r\n\r\n```\r\nERROR - Invalid argument: --verbalise=True\r\nTry running flow --help\r\n```\r\n\r\nChanging the command to `./flow --model cfg/tiny-yolo-voc.cfg --load bin/tiny-yolo-voc.weights --savepb --verbalise` will fix the issue", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 11715, "title": "Using higher order operators within tf.contrib.data.Dataset.map raises a segmentation fault", "body": "### System information\r\n\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: `('v1.2.0-2323-geaa7a8e', '1.3.0-rc0')`\r\n- **Python version**: 2.7.6\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: GTX 1080 (8GB)\r\n- **Exact command to reproduce**: see the code snippet below\r\n\r\n### Describe the problem\r\n\r\nWhen using a higher order operator (such as `tf.map_fn`) within the `tf.contrib.data.Dataset.map` method, the program exits with a `Segmentation fault`.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndataset = tf.contrib.data.Dataset.from_tensor_slices([\r\n  tf.constant([1, 2, 3, 4, 5, 6])])\r\n\r\ndataset = dataset.map(lambda elems: tf.map_fn(lambda x: x * x, elems))\r\n\r\niterator = dataset.make_one_shot_iterator()\r\nnext_element = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n  print(sess.run(next_element))\r\n```\r\n\r\n---\r\n\r\nMy particular use case is to split a sentence into tokens and then map a function on each token to build tensors of characters.", "comments": ["/CC @mrry Can you debug this?", "+1 In my case I added a tf.map_fn to decode a set of images. I noticed that the simple fact of adding tf.map_fn in the graph causes this problem.", "Looks like the problem is a missing `ScopedStepContainer` in the context when you run the function in `Dataset.map()`. The same problem also afflicts `Dataset.filter()`, although perhaps it would be less common for a predicate to use `tf.map_fn()`. I have a fix in the works!"]}, {"number": 11714, "title": "run silent /.configure", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.3\r\n\r\n- **Python version**: \r\nintel python\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.5\r\n\r\n- **CUDA/cuDNN version**:\r\nnone\r\n- **GPU model and memory**:\r\nnone\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am building a docker container, compiling tensorflow from source, would like to use MKL, and need to run ```configure``` script silently in order to set up the build correctly. Cannot figure out where, when and how the MKL libraries are being installed.\r\n\r\n### Source code / logs\r\nnone\r\n\r\nIs there a way to run the ```configure``` script silently? alternatively,\r\nDoes someone have a Docker script that will compile tensorflow USING the MKL?  ", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\n(That said, there are two options, if you want to use all the defaults, you can run `yes \"\" | ./configure\"`. Alternatively, if you need to set individual options, you can use the environment variables. For example, the scripts that run the builds for continuous integration such as [`libtenorflow_cpu.sh`](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/tools/ci_build/osx/libtensorflow_cpu.sh#L26) set various variables. Unfortunately, the full list is not documented in one place, so you'll have to look at the source of the [`configure` script](https://github.com/tensorflow/tensorflow/blob/master/configure))"]}, {"number": 11713, "title": "decode_csv is extremely slow", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nI tested on both MacOS 10, Centos 7, https://pastebin.com/LsADQ89s\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nTensorflow 1.2.1, also tried with tensorflow compiled from sources\r\n\r\n- **TensorFlow version (use command below)**:\r\n('v1.2.0-5-g435cdfc', '1.2.1'), also compiled and tested from master\r\n\r\n- **Python version**: \r\n2.7.13\r\n\r\n- **CUDA/cuDNN version**:\r\nno\r\n\r\n- **GPU model and memory**:\r\nno\r\n\r\n- **Exact command to reproduce**:\r\nsee above\r\n\r\n### The problem\r\nI noticed that a huge amount of training time CPU is idle. I isolated the problem to the code snippet bellow, where I create a string tensor using tf.constant with N rows each of them contains comma separated list of randomly generated numberical values, then I decode these string tensor to rank2 tensor of floats using 3 different approaches. \r\n\r\n\r\nI discovered following:\r\ndecoding tfrecords with *tf.parse_example* is quite fast\r\ndecoding csv with *decode_csv* is ~20 (4 cores) 60 (16 cores) times slower than parse_example and it utilises only 1 core on my machine\r\ndecoding csv with *tf.string_split* + *tf.string_to_number()* is ~20-60 times slower than parse_example. \r\nIt seems *tf.string_to_number()* makes it slow, if I comment out the line with *tf.string_to_number* it is 2-3 times slower than parse example\r\n\r\n\r\n\r\n\r\nThe code I used for that:\r\nhttps://pastebin.com/ZR152y5M\r\n\r\n\r\n\r\n### Source code / logs\r\nRunning parse_example\r\nTotal time: 2.01925897598\r\n\r\nRunning string_split\r\nTotal time: 5.52524709702\r\n\r\nRunning decode_csv\r\nTotal time: 22.6595089436\r\n\r\n", "comments": ["It makes sense for `tf.decode_csv` to be slower than `tf.parse_example`, since `tf.train.Example`s are designed to be decoded fast while CSVs are not. When training, I would recommend first converting your data into `tf.train.Example`s.\r\n\r\nIf you find that TensorFlow decodes CSVs significantly slower than some other CSV decoder implementation, then reopen this issue.", "Hi,\r\n\r\nI implemented decoding csv using _tf.py_func_ and numpy (test_numpy_csv function in the code below). It seems even reading with numpy is faster than decode_csv. \r\n\r\nhttps://pastebin.com/rTZjCwGH\r\n\r\nSo far parsing csv using py_func+numpy is ~5 times faster than using decode_csv on this particular synthetic data (1000 examples with 100 float columns)\r\n", "Thank you for performing that benchmark. Since `tf.decode_csv` is significantly slower than numpy's `np.fromstring`, I'm reopening and marking as contributions welcome.", "I would like to reserve it if I can.", "If what Valgrind has said me is true, the biggest problem is a slow string to float conversion in [core/lib/strings/numbers.cc](https://github.com/tensorflow/tensorflow/blame/master/tensorflow/core/lib/strings/numbers.cc). \r\nIt seems to be true. There are two string copying, unordered_map using and so on. I can adjust the [double-conversion](https://github.com/google/double-conversion) library here. It essential faster.\r\n\r\nThere are some inconsistencies with TF str-to-double functions. Double-conversion library has case sensitive way to specify special values ```NaN``` and ```INF```. I can create a simple patch to fix case sensibility. Another problem is a length of ```infinity``` word. In TF you can write ```inf``` and ```infinity```, but double-conversion supports only one. Numpy support only one too (```inf```). And may be TF can do the same.\r\n\r\nWhat do you think about it? Can I use this lib here?", "@cwhipkey any thoughts on using the double-conversion library? The current function used, locale_independent_strtonum, is not intended to be fast.", "I've made some changes here for the sake of experiments. With the double-conversion library, decode_csv works almost the same time as numpy csv parsing. I'll send PR, please look at it.", "Has the PR sent sufficiently improved performance?", "@Androbin I looked the git history. The PR #12102 by @akindyakov (\ud83d\udc4d ) was merged, and later reverted in PR #15133.\r\n\r\nLater the change was re-introduced in PR #18746 and finally merged. For that I think the issue is resolved.\r\n\r\nDuring the above mentioned process, the version used by cmake and bazel became out of sync. I created a PR #19726 to address the out of sync issue.\r\n\r\nOther than that I think this issue could be closed."]}, {"number": 11712, "title": "nn", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 11711, "title": "First time of inference very slow on iOS", "body": "### Describe the problem\r\nI tried to run the inference graph on iOS. However, the first run was very slow. It gets faster at the subsequent runs. My test inference graph is very simple, and just with two features as input, and run a logistic regression on it. The first run on `session.run(...)` will take about 15ms to run. The subsequent runs (starting from the second run) can go downs as low as 0.2ms.\r\n\r\nI have seen a similar issue raise here: https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/PDIBnp1ftxk\r\n\r\nIs it a common problem on the c++ runtime? what cause this problem? Is it the lib level initialization issue or the graph level? I was not able to find any support about this on stackoverflow. \r\n\r\nCan we have an option to prepare the graph in optimized state in order to eliminate  problem with slow first run?", "comments": ["Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.\r\n\r\nIn particular, can you share the complete code that generates the inference graph and runs it twice, with the first run being slow?", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11710, "title": "Disable failing tests in Windows Bazel build", "body": "Fix http://ci.tensorflow.org/job/tf-master-win-bzl/1278/console\r\n@gunan @caisq ", "comments": ["can you rebase please?", "It's done! Please take another look."]}, {"number": 11709, "title": "Most of embedding_lookup/embedding_lookup_sparse computations are on CPU,  Most of the GPU time is transferring data.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Centos 7\r\n- **TenorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\nr.12\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.5.2\r\n- **CUDA/cuDNN version**:\r\n8.0\r\n- **GPU model and memory**:\r\nP40\r\n\r\n\r\n### Describe the problem\r\nI have done some profiling about *embedding_lookup_sparse*, i have use `tf.device('\\gpu:0')` to specify where the ops run. But the results show:\r\n\r\n- Most of the operations are performed on the CPU.\r\n- GPU was transferring data at most of time.\r\n\r\nAfter I looked into the codes, there are some Ops without GPU version, including `dynamic_partition`,  `segment_sum`, `strided_slice`. So I'm a little confused.\r\n\r\nIs this intended behavior? Or are these Ops not implemented yet? Thanks for the explanations.\r\n\r\nHere is some profiling result:\r\n### tfprof\r\n```\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] hidden/embed/embedding_lookup_sparse/embedding_lookup/DynamicPartition:(DynamicPartition)/job:localhost/replica:0/task:0/cpu:0\r\nhidden/embed/embedding_lookup_sparse: (SegmentSum): /job:localhost/replica:0/task:0/cpu:0\r\nhidden/embed/embedding_lookup_sparse/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0\r\n```\r\n### nvprof\r\n```\r\n==4762== Profiling result:\r\nTime(%)      Time     Calls       Avg       Min       Max  Name\r\n 44.07%  130.029s    240276  541.17us     768ns  8.1151ms  [CUDA memcpy DtoH]\r\n 25.20%  74.3504s    480619  154.70us     992ns  8.9016ms  [CUDA memcpy HtoD]\r\n  9.84%  29.0339s    330000  87.981us  61.922us  610.68us  void tensorflow::GatherOpKernel<float, int>(float const *, int const *, tensorflow::GatherOpKernel<float, int>*, __int64, __int64, __int64)\r\n```\r\n", "comments": ["Can you provide the code you ran that demonstrates `embedding_lookup_sparse` performance, and the tfprof commands you used to generate the output? Thank you", "Code: [embedding_lookup_sparse](https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624). \r\n\r\ntfprof code to do the profiling: \r\n```\r\n  tf.contrib.tfprof.model_analyzer.print_model_analysis(\r\n    tf.get_default_graph(),\r\n    run_meta=run_metadata,\r\n    tfprof_options=tf.contrib.tfprof.model_analyzer.PRINT_ALL_TIMING_MEMORY)\r\n````", "/CC @alextp Can you take a look? When I run, all ops are put on the CPU.", "@reedwm there are some ops in GPU like `gather`. did you use gpu? By the way I add `with tf.device('/gpu:0')`, maybe you can have another try. Thanks.", "`embedding_lookup` and friends should only rely on dynamic partition and other cpu-heavy ops when you have partitioned embedding variables.\r\n\r\nMost of the use cases I've seen for partitioning embedding variables put them on remote parameter servers (since there's no point in sharding locally) and most setups I know of involving parameter servers are more efficient when the partitioned variables are on the CPU than when they are on the GPU.\r\n\r\nAre you using GPU-placed partitioned variables? If so, why?\r\n\r\nI don't think it's easy using our current setup to get rid of dynamic_partition for partitioned embedding lookup.", "I just tested the local situation. I got your point, I'll have a try. Thank you very much for the detailed answers.", "@alextp\r\n### System information\r\n* ***Code***\r\n[embedding_lookup_sparse](https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624)\r\n* ***Params***\r\n100M * 1K\r\n* ***Id***\r\n256 * 1000\r\n* ***CPU***\r\n32-cores\r\n* ***Tensorflow***\r\nr1.3\r\n### Conclusion\r\nOn a single machine,  profiling the `embedding_lookup_sparse` with tfprof, the result shows that `gather` Op takes a lot of time. I checked the code and found that the CPU version `gather` op is single-thread. \r\nThen I modify the `gather` Op to multi-threads, the result shows about 6x speedup, which benifites a lot of cases. If neccessary, I can post a PR.\r\n\r\n### Profiling result\r\n```\r\nnode name | requested bytes | total execution time | accelerator execution t    ime | cpu execution time`\r\n# old `gather`\r\nGather     2048.00MB (100.00%, 2.27%),   622.96ms (99.99%, 22.97%),    0us (0.00%, 0.00%),  622.96ms (99.99%, 22.97%)\r\n# multi-thread `gather`\r\nGather    2048.00MB (100.00%, 49.96%),   107.91ms (99.99%, 5.50%),     0us (0.00%, 0.00%),   107.91ms (99.99%, 5.50%)\r\n```", "Please send a PR!\n\nOn Aug 12, 2017 6:24 AM, \"nolan liu\" <notifications@github.com> wrote:\n\n> @alextp <https://github.com/alextp>\n> System information\n>\n>    - *Code*\n>    embedding_lookup_sparse\n>    <https://gist.github.com/nolanliou/c00af5938b2aecfdc5ea1189426b8624>\n>    - *Params*\n>    100M * 1K\n>    - *Id*\n>    256 * 1000\n>    - *CPU*\n>    32-cores\n>    - *Tensorflow*\n>    r1.3\n>\n> Conclusion\n>\n> On a single machine, profiling the embedding_lookup_sparse with tfprof,\n> the result shows that gather Op takes a lot of time. I checked the code\n> and found that the CPU version gather op is single-thread.\n> Then I modify the gather Op to multi-threads, the result shows about 6x\n> speedup, which benifites a lot of cases. If neccessary, I can post a PR.\n> Profiling result\n>\n> node name | requested bytes | total execution time | accelerator execution t    ime | cpu execution time`\n> # old `gather`\n> Gather     2048.00MB (100.00%, 2.27%),   622.96ms (99.99%, 22.97%),    0us (0.00%, 0.00%),  622.96ms (99.99%, 22.97%)\n> # multi-thread `gather`\n> Gather    2048.00MB (100.00%, 49.96%),   107.91ms (99.99%, 5.50%),     0us (0.00%, 0.00%),   107.91ms (99.99%, 5.50%)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11709#issuecomment-321980774>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxbWYcp_2ZsE-T5iTNVSBOCA_d7fhks5sXaefgaJpZM4Og3Qr>\n> .\n>\n", "@alextp I have sent a PR, please review it if you have time. Thanks."]}, {"number": 11708, "title": "error in installation instructions", "body": "In instructions specified at: https://www.tensorflow.org/install/install_linux#ValidateYourInstallation\r\n\r\nit states:\r\n\r\n For example, the following command installs the CPU-only version of TensorFlow for Python 2.7:\r\n\r\n (tensorflow)$ pip install --ignore-installed --upgrade \\\r\n https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.2.1-cp34-cp34m-linux_x86_64.whl\r\n\r\nHowever it is for Python **3.4** (not 2.7)", "comments": ["Thanks for pointing this out, will fix it."]}, {"number": 11707, "title": "error in tensorflow tutorial", "body": "In tensorflow tutorial at: https://www.tensorflow.org/get_started/get_started\r\n\r\nunder 'A custom model' section it is missing an assignment for eval_input_fn as:\r\n\r\neval_input_fn = tf.contrib.learn.io.numpy_input_fn({\"x\": x_eval}, y_eval, 4, num_epochs=1000)\r\n", "comments": ["Ah, I already fixed that in https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/docs_src/get_started/get_started.md\r\n\r\nSince version 1.2 is frozen, you will have to wait until 1.3 for the website to update with the correct and updated documentation. Unless you have any other concern, please close this issue."]}, {"number": 11706, "title": "no input tensor in mobilenet_v1_1.0_224.ckpt", "body": "I am newbie in TF and slim. Sorry if this a basic question. I downloaded mobilenet_v1_1.0_224.ckpt, built latest TF and TF-slim tree and ran tensorflow/tensorflow/python/tools/inspect_checkpoint.py on it.\r\n\r\nI dont find input tensors, that are usually 224,224,3 or 112,112,3 etc size. Am I missing something here?\r\n\r\n[mobilenet.txt](https://github.com/tensorflow/tensorflow/files/1169140/mobilenet.txt)\r\n", "comments": ["The checkpoint is not sufficient to get the input placeholders. It is only the saved variables (which would be the weights and biases). You need to load a model from the graphdef and then load the checkpointed variables into that loaded graph.\r\n\r\nSee...\r\nhttps://github.com/tensorflow/models/tree/master/slim\r\n\r\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 11705, "title": "fix issues serving#421 Symbol not found __ZN6google8protobuf8internal10LogMessageC1ENS0_8LogLevelEPKci", "body": "fix issues [serving#421](https://github.com/tensorflow/serving/issues/421)\r\nthe problem reason is ```single_image_random_dot_stereograms.cc<<kernels>>``` be dependent on ```TensorShapeProto```\r\nit define on protobuf, use class LogMessage. but no deps protobuf on BUILD rule ```python/ops/_single_image_random_dot_stereograms.so ```.\r\nso ```Symbol not found: __ZN6google8protobuf8internal10LogMessageC1ENS0_8LogLevelEPKci```(google::protobuf::internal::LogMessage::LogMessage)", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "this fix worked for me, thank you so much for fixing it!", "If you can sign the CLA we should be able to accept this, thanks!", "Can one of the admins verify this patch?", "I signed it! ", "CLAs look good, thanks!\n\n<!-- ok -->", "ok @vrv please accept, thanks!", "@tensorflow-jenkins test this please", "Would the better thing to have done to modify https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L957 to add the tf_custom_op_library_additional_deps, instead of adding it to the leaf target here? ", "Actually I don't know how this did anything, \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L936 was already adding the protobuf header library to the cc_binary rule, if I am understanding correctly.", "Oh, we added protobuf here, not protobuf_headers.  My bad.", "I get the same issue. Can some one guide me with the set of steps to replicate to fix this issue. Thanks in advance. "]}, {"number": 11704, "title": "ERROR: Failed to load Skylark extension '//tensorflow:workspace.bzl'.", "body": "Environment info\r\n\r\nOperating System: Ubuntu 16.04\r\nBazel version : 0.5.2\r\npython version : 3.5.2\r\ntensorflow version : 1.2.0\r\nHello,\r\n\r\nWhen I try this command :\r\n\r\nbazel build tensorflow/examples/label_image:label_image && bazel-bin/tensorflow/examples/label_image/label_image --graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt --output_layer=final_result --image=$HOME/piscine/nvxpool/cad_image_182022_259650_1987_3303.jpg --input_layer=Mul --output_layer='final_result'\r\nI have this error :\r\n\r\nERROR: Failed to load Skylark extension '//tensorflow:workspace.bzl'. It usually happens when the repository is not defined prior to being used. Maybe repository '' was defined later in your WORKSPACE file? ERROR: cycles detected during target parsing.\r\n\r\nI do not understand because before restarting my computer this command worked. Do you have any ideas ?\r\n\r\n(For information before restarting my computer I try this tutorial https://github.com/DigitalGlobe/mltools/tree/master/examples/polygon_classify_cnn [The section Getting Started]. Do you think the problem comes from there?)\r\n\r\nThanks\r\n", "comments": ["Maybe try running ./configure again?\r\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "@aurorelaurent Hi I also have this problem. Did you fix your problem?", "Hi @aselle I have same error how to resolve it\r\nError\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\nERROR: Failed to load Skylark extension '//tensorflow:workspace.bzl'.\r\nIt usually happens when the repository is not defined prior to being used.\r\nMaybe repository '' was defined later in your WORKSPACE file?\r\nERROR: cycles detected during target parsing\r\nINFO: Elapsed time: 0.035s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n", "Same problem here... anyone have a resolution?"]}, {"number": 11703, "title": "Fix missing spaces for several errors", "body": "", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 11702, "title": "syntaxnet: Global training is worse than greedy trainning when the training data size is 1 millon data", "body": "As you can see the following is my experimental configuration parameters and experiment results,  I'd like to know on a fairly large amount of data\uff0c how to adjust the super parameters can get a result that global better than greedy;\r\nwhen the trainning data size is 20,000\uff0c We got the global trainning better than greedy trainning using default configuration parameters, But when the training data increased to 1 million\uff0c We can't reproduce the results;\r\nThis is our experimental result:\r\n![image](https://user-images.githubusercontent.com/21216639/28505738-14a4b908-7059-11e7-8288-ff7ac89abc95.png)\r\nThis is our experimental configuration: \r\ngreedy trainging parameters: \r\n![image](https://user-images.githubusercontent.com/21216639/28505764-4063d40c-7059-11e7-99d1-417cef636d1c.png)\r\nglobal trainging parameters: \r\n![image](https://user-images.githubusercontent.com/21216639/28505797-5ba4f318-7059-11e7-8acc-fe101383b909.png)\r\n\r\nI guess that's the parameter setting problem\uff0c Anyone who has experience can help me with this problem? Thanks\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11701, "title": "tf.contrib.learn.KMeansClustering squeeze problem when using predict()", "body": "This is a problem with every version of tensorflow I've tried (1.1, 1.2, 1.3).\r\n\r\nAfter using the fit() method train a KMeansClustering model, if the predict() method is used to predict a *single* data point's cluster then there is an internal error (predicting two data points works fine)...\r\n\r\nIn ./tensorflow/contrib/learn/python/learn/estimators/estimator.py in _predict_generator() the batch_length is rightly determined by asking for the first dimension of the shape, but the problem is (at least for KMeansClustering) ./tensorflow/contrib/factorization/python/ops/clustering_ops.py _infer_graph() uses tf.squeeze to remove all dimensions with size of 1 for the distances and indices.  An error occurs when computing batch_size because the value will have rank 0 after the squeeze.  This appears to have been there since gardener@ originally imported that code for computing kmeans.\r\n\r\nA) This certainly does not need to squeeze the first dimension as, in my case, if predict() is called for a single value, then the distances and indices are both just one element.\r\nB) I cannot really see why a squeeze should be done there in the first place.  Perhaps flatten? (that doesn't make sense either)\r\n\r\nI have a pull request ready which just removes the squeeze.  But I wanted to file an issue to which the original developer might be able to comment.\r\n\r\nOr I can try to make it squeeze all but the first dimension, though the shape of those tensors are unknown when the graph is built. I'll have to figure out how to do that. I'd prefer to just not squeeze if it's not necessary.\r\n\r\nHere is a script that demonstrates the problem:\r\n```python\r\n#/usr/bin/python3\r\n\r\nimport tensorflow as tf\r\n\r\nK = 4    # K classes\r\nkm = tf.contrib.learn.KMeansClustering(K, '/tmp/test-kmeans-tf-model')\r\n\r\nX = [[1.1, 1.2],\r\n     [2.2, 2.5],\r\n     [3.3, 3.6],\r\n     [2.4, 2.7],\r\n     [14.1, 3.2],\r\n     [4.12, 15.2],\r\n     [3.14, 3.6],\r\n     [2.4, 13.7],\r\n     [3.1, 3.2],\r\n     [4.1, 4.2],\r\n     [13.1, 13.2],\r\n     [4.11, 14.2],\r\n     [5.1, 15.2]]\r\n\r\ndef train_input_fn():\r\n    data = tf.constant(X, tf.float32)\r\n    return (data, None)\r\n\r\n\r\n# train\r\nkm.fit(input_fn=train_input_fn, steps=1000)\r\n\r\ndef predict_input_fn():\r\n    ### WORKS FINE\r\n    #data = tf.constant([[2.1, 3.1], [1.1, 1.2]])\r\n\r\n    ### FAILS TO PREDICT\r\n    data = tf.constant([[1.1, 1.2]])\r\n    return data\r\n\r\ngen = km.predict(input_fn=predict_input_fn, as_iterable=True)\r\nfor t in gen:\r\n    print(\"predict returned: %s\" % t['cluster_idx'])\r\n\r\n\r\n```\r\nRemoving the squeeze in clustering_ops.py fixes this script's problem (or give it two data points to predict).\r\n", "comments": ["The squeeze doesn't seem to do anything useful for either dimension. @agarwal-ashish, any thoughts?\r\n", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A fix has been submitted and should be out in the nightly build. Please reopen if this is still broken."]}, {"number": 11700, "title": " tf.import_graph_def  load quantized graph error", "body": "[root@A01-R04-I221-14 export_dir]# python predict.py \r\nTraceback (most recent call last):\r\n  File \"predict.py\", line 70, in <module>\r\n    load_graph('quantized_graph.pb')\r\n  File \"predict.py\", line 33, in load_graph\r\n    tf.import_graph_def(graph_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 365, in import_graph_def\r\n    % (input_name,)))\r\nValueError: graph_def is invalid at node u'while/add_6/y': More inputs specified ('while/Switch:1') than the op expects..", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks, in particular detailed instructions on reproducing the problem. Thank you.", "Hi @guoying1030 \r\nI have the same error, Did you solve it?", "I have the same problem, Have you solve it ?", "I hope @petewarden can answer this question.", "I have the same problem ,  have you solved?"]}, {"number": 11699, "title": "Branch 162842890", "body": "", "comments": ["Investigating GPU build failure: https://ci.tensorflow.org/job/tensorflow-pull-requests-gpu/5878/", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 11698, "title": "Exposed more graph io methods", "body": "I think these should probably be exposed since some of these are being used in examples, such as parsing utilities [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/parsing_utils.py#L86). ", "comments": ["Jenkins, test this please.", "@tensorflow-jenkins test this please"]}]