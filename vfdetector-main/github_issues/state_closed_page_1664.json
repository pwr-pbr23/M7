[{"number": 2985, "title": "fix word2vec_test's tmp path", "body": "fix word2vec_test's tmp path:\n'/tmp/word2vec_testtest-text.txt' to '/tmp/word2vec_test/test-text.txt'\n'/tmp/word2vec_testeval-text.txt' to '/tmp/word2vec_test/eval-text.txt'\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "Thanks for the cleanup.\n"]}, {"number": 2984, "title": "Multiple Towers Not Applying Updates", "body": "### Environment info\n\nAll 4 machines: 4.1.13-100.fc21.x86_64\n\nSetting 1:\n1 machine with 1x 960 GTX (one of the workers from below)\n\nSetting 2:\n1 machine with cpu (parameter server)\n3 machines with 1x 960 GTX (worker)\n\ntensorflow 0.9.0, cudNN v4\n### Problem\n\nI can train a mnist example on both settings while using the same input pipeline as in the following scripts.\n\nThe following two scripts are almost identical but one is made to work on Setting 1 while the other one is made for Setting 2 to work in a distributed, parameter sharing, data parallelism manner. The scripts operates on a small set with coin images with 2 classes.\n\nSingle Machine Script (Setting 1): https://gist.github.com/ischlag/96b10519a45727bd17fe0cce01c1bd15\nDistributed Script (Setting 2): https://gist.github.com/ischlag/d9fc4429971ce7c1957798de30c56372\n\nThe Single machine script works as expected and can generalize well:\n'''\nSession started!\nPartial-Epoch Avg Error:  0.692565  AvgMsPerBatch: 0.56 ms\nPartial-Epoch Avg Error:  0.686972  AvgMsPerBatch: 0.49 ms\nPartial-Epoch Avg Error:  0.671962  AvgMsPerBatch: 0.46 ms\nPartial-Epoch Avg Error:  0.645295  AvgMsPerBatch: 0.47 ms\nPartial-Epoch Avg Error:  0.59795  AvgMsPerBatch: 0.47 ms\nPartial-Epoch Avg Error:  0.607252  AvgMsPerBatch: 0.47 ms\nPartial-Epoch Avg Error:  0.548216  AvgMsPerBatch: 0.49 ms\nPartial-Epoch Avg Error:  0.5107  AvgMsPerBatch: 0.49 ms\nPartial-Epoch Avg Error:  0.492883  AvgMsPerBatch: 0.47 ms\nPartial-Epoch Avg Error:  0.466268  AvgMsPerBatch: 0.48 ms\nPartial-Epoch Avg Error:  0.431923  AvgMsPerBatch: 0.49 ms\nPartial-Epoch Avg Error:  0.407919  AvgMsPerBatch: 0.48 ms\nPartial-Epoch Avg Error:  0.387163  AvgMsPerBatch: 0.47 ms\nPartial-Epoch Avg Error:  0.340534  AvgMsPerBatch: 0.47 ms\nPartial-Epoch Avg Error:  0.349155  AvgMsPerBatch: 0.47 ms\nPartial-Epoch Avg Error:  0.327694  AvgMsPerBatch: 0.47 ms\nPartial-Epoch Avg Error:  0.244313  AvgMsPerBatch: 0.46 ms\nPartial-Epoch Avg Error:  0.256759  AvgMsPerBatch: 0.46 ms\nPartial-Epoch Avg Error:  0.206276  AvgMsPerBatch: 0.46 ms\nPartial-Epoch Avg Error:  0.184809  AvgMsPerBatch: 0.46 ms\nPartial-Epoch Avg Error:  0.187335  AvgMsPerBatch: 0.46 ms\n'''\nThe distributed script is quite slower and fails to update the parameters.\n''' \nSession started!\nPartial-Epoch Avg Error:  0.69339  AvgMsPerBatch: 3.25 ms\nPartial-Epoch Avg Error:  0.692113  AvgMsPerBatch: 3.27 ms\nPartial-Epoch Avg Error:  0.693958  AvgMsPerBatch: 3.27 ms\nPartial-Epoch Avg Error:  0.688354  AvgMsPerBatch: 3.26 ms\nPartial-Epoch Avg Error:  0.692994  AvgMsPerBatch: 3.25 ms\nPartial-Epoch Avg Error:  0.692903  AvgMsPerBatch: 3.24 ms\nPartial-Epoch Avg Error:  0.691708  AvgMsPerBatch: 3.29 ms\nPartial-Epoch Avg Error:  0.691477  AvgMsPerBatch: 3.35 ms\nPartial-Epoch Avg Error:  0.69129  AvgMsPerBatch: 3.37 ms\nPartial-Epoch Avg Error:  0.691391  AvgMsPerBatch: 3.35 ms\nPartial-Epoch Avg Error:  0.691415  AvgMsPerBatch: 3.30 ms\nPartial-Epoch Avg Error:  0.69209  AvgMsPerBatch: 3.30 ms\nPartial-Epoch Avg Error:  0.691746  AvgMsPerBatch: 3.32 ms\nPartial-Epoch Avg Error:  0.690423  AvgMsPerBatch: 3.31 ms\nPartial-Epoch Avg Error:  0.692738  AvgMsPerBatch: 3.30 ms\n''' \n\nThe same distributed script works well with the mnist dataset in a distributed manner. All the lines necessary to run it with mnist are there but commented out.\n\nWhy is my distributed script not working in this case?\n", "comments": []}, {"number": 2983, "title": "Multiple CPU usage ineffective: CPU utilization only 200% on a 8 core VM", "body": "This is the same as #583. Opening a new issue since CPU utilization is still low. I built tensorflow with `-mavx2` flag and ran `examples/tutorials/word2vec/word2vec_basic.py`. TensorFlow is recognizing num of cores as 8, yet still only 200% of CPU is used during training.\n### Environment info\n\nOperating System:\n14.04.1-Ubuntu x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \nNo CUDA or cuDNN installed.\n\nIf installed from sources, provide the commit hash:\nCommit hash:\n\n```\n840c4ac\na2d6cf7\n5b66275\n27b83b7\nb77f607\nce330a7\n2c33855\n451f18c\naf794ed\n921b709\n```\n### Steps to reproduce\n1. Enable logging of `inter_op_parallelism_threads` and `intra_op_parallelism_threads`.\n2. Configure, build tensorflow with AVX2 with this command\n   `bazel build -c opt --copt=-mavx2 //tensorflow/tools/pip_package:build_pip_package`\n3. Build wheel, install package and run `python examples/tutorials/word2vec/word2vec_basic.py`.\n", "comments": ["This is not a bug.  \n\nThe word2vec_**basic**.py tutorial is an unoptimized example designed to illustrate a point. It feeds training examples via a sequential and high-overhead mechanism, contains numerous small ops which are expensive to dispatch and there are parts of the model where there is not much opportunity for parallelism.    \n\nPlease read the [tutorial](https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html) section entitled \"**Optimizing the Implementation**\" which states:\n\n> For example, the naive code we used in this tutorial would suffer compromised speed because we use Python for reading and feeding data items -- each of which require very little work on the TensorFlow back-end\n\nThe tutorial provides two faster variants. \nhttps://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/models/embedding/word2vec.py\nhttps://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/models/embedding/word2vec_optimized.py\n", "Just for future references, models have been moved to a separate repo:\r\nhttps://github.com/tensorflow/models/tree/master/tutorials/embedding"]}, {"number": 2982, "title": "Branch 125491759", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Greetings @maciekcc @maciekcc. Is this 'sync to master' inthis PR\n"]}, {"number": 2981, "title": "Update build file to fix #2703", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2980, "title": "Setting CUDA_VISIBLE_DEVICES=\"\" crashes TensorFlow on Mac", "body": "Something changed in the last 3 days so that\u00a0setting CUDA_VISIBLE_DEVICES to \"\" makes TensorFlow crash on Mac with SIGSERV\n\n```\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000000000000\nException Note:        EXC_CORPSE_NOTIFY\n\nVM Regions Near 0:\n--> \n    __TEXT                 0000000107d49000-0000000107d4a000 [    4K] r-x/rwx SM=COW  /System/Library/Frameworks/Python.framework/Versions/2.7/Resources/Python.app/Contents/MacOS/Python\n\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\n0   libsystem_c.dylib               0x00007fff97ebc152 strlen + 18\n1   _pywrap_tensorflow.so           0x000000010c9e7a2c perftools::gputools::cuda::Diagnostician::FindKernelDriverVersion() + 204\n2   _pywrap_tensorflow.so           0x000000010c9e7220 perftools::gputools::cuda::Diagnostician::LogDriverVersionInformation() + 512\n3   _pywrap_tensorflow.so           0x000000010c9e6f8f perftools::gputools::cuda::Diagnostician::LogDiagnosticInformation() + 671\n4   _pywrap_tensorflow.so           0x000000010c9f7d69 perftools::gputools::cuda::CUDADriver::Init() + 569\n5   _pywrap_tensorflow.so           0x000000010ca0e56f perftools::gputools::cuda::CudaPlatform::VisibleDeviceCount() const + 15\n6   _pywrap_tensorflow.so           0x000000010c79d1f5 tensorflow::GPUMachineManager() + 261\n7   _pywrap_tensorflow.so           0x000000010c79ac4e tensorflow::BaseGPUDeviceFactory::GetValidDeviceIds(std::__1::vector<int, std::__1::allocator<int> >*) + 46\n8   _pywrap_tensorflow.so           0x000000010c79aa79 tensorflow::BaseGPUDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<tensorflow::Device*, std::__1::allocator<tensorflow::Device*> >*) + 345\n9   _pywrap_tensorflow.so           0x000000010c91d1b5 tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::vector<tensorflow::Device*, std::__1::allocator<tensorflow::Device*> >*) + 245\n10  _pywrap_tensorflow.so           0x000000010b5a7175 _wrap_DeviceFactory_AddDevices(_object*, _object*) + 133\n11 \n```\n\nOn main window I see following\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\n\n```\n\nI used to be able to do this, also, if I run NVidia's `deviceQuery` that comes with CUDA drivers, it fails without a SIGSERV\n\n```\ncd /usr/local/cuda/samples\nsudo make -C 1_Utilities/deviceQuery\n./bin/x86_64/darwin/release/deviceQuery\n\n./bin/x86_64/darwin/release/deviceQuery Starting...\n CUDA Device Query (Runtime API) version (CUDART static linking)\n\ncudaGetDeviceCount returned 38\n-> no CUDA-capable device is detected\nResult = FAIL\n```\n", "comments": ["It seems to be some edge-case when trying to obtain \"kernel reported version\" \n[stream_executor/cuda/cuda_diagnostics.cc:189](https://github.com/tensorflow/tensorflow/blob/d42facc3cc9611f0c9722c81551a7404a0bd3f6b/tensorflow/stream_executor/cuda/cuda_diagnostics.cc#L189)\n\nIt seems to be tricky to reproduce, I get it only for some combination of LD_LIBRARY_PATH/DYLD_LIBRARY_PATH and not for others, I'll close it for now since I can't make a good repro\n", "BTW, I'm experiencing this issue again in latest version. To reproduce on MacOS:\r\nexport CUDA_VISIBLE_DEVICES=\r\nunset LD_LIBRARY_PATH\r\n\r\nIt's probably similar issue as what was fixed in https://github.com/tensorflow/tensorflow/pull/3448"]}, {"number": 2979, "title": "Branch 125447159", "body": "", "comments": []}, {"number": 2978, "title": "CIFAR-10 init routine breaks simple usages", "body": "This is partly breaking the CIFAR-10 example, and it doesn't make sense.\n\nThe files `cifar10.py` and `cifar10_input_test.py` already import `cifar10_input`, so the initialization doesn't seem to be needed at all.\nIf one tries to copy the code of `cifar10.py` and run it, it will break, since `__init__.py` imports cifar10, which defines the flags `batch_size` and `data_dir`, causing an error when your `cifar10.py` tries to redefine them.\n\nTo reproduce the problem, create a file exactly like `cifar10.py` and try to run it.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2977, "title": "Complex Number Calculations Not Available On GPU -- No Supported Kernels", "body": "Hey Guys,\n\nHave been trying to implement unitary RNN in tensorflow. Big thanks to @khaotik as he has done much work on implementing unitary RNN's in his repo here:\n\nhttps://github.com/sherjilozair/char-rnn-tensorflow\n\nWhen I try to implement this on a GPU I run into several kernel support errors. Here are some operations that do not currently have kernels for the `complex64` dtype:\n- `tf.conj`\n- `tf.matmul` (which is crucial)\n\nI'm unsure if `batch_fft` has supported GPU kernels. \n\nRegardless, having these kernels would give tensorflow the ability to explore entirely new architectures as several new RNN papers use complex numbers now. I'm not sure how difficult it would be to write these kernels but much thanks to those who could do it. \n", "comments": ["I would be happy to have a look at this issue unless someone else has started\n", "@LeavesBreathe can you kindly test with my branch? Thanks!\n", "@kashif thanks for doing this. Right now my gpu's are tied up so it will be at least a few days before I can give it a test. Will post back here upon testing. Thanks again. \n", "tf.matmul/tf.batch_matmul support for complex types is now in: https://github.com/tensorflow/tensorflow/commit/6c7681fbbcc3c244f3e406abc4ea1287fd717752\n", "can this issue be closed?\n", "@kashif  i'm testing urnn's right now on the gpu and it seems that the implementation is pretty slow right now. I hesitate to close the issue right now but I can if you feel that its justified. It does seem that the unitary rnn is about 10x slower than vanilla lstm on a titan x. \n"]}, {"number": 2976, "title": "Branch 125447159", "body": "", "comments": ["Jenkins, test this please.\n"]}, {"number": 2975, "title": "ValueError : Variable proj_w already exists, disallowed. did you mean to set reuse=true in VarScope?", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: 7.5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`. 0.8\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\nI have created a REST webservice to execute machine translation with some modifications in translate.py.  If I run decode function in translate.py alone, on multiple runs I get the right output. But when I try to run the decode function through the webservice that I have created, the first time, I get the translation result. But on the second iteration, I get an error mentioned in the title.\n### What have you tried?\n1. I tried to close the session at the end of decode function in translate.py.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n[logs-1.txt](https://github.com/tensorflow/tensorflow/files/325737/logs-1.txt)\n", "comments": ["This sounds like more like a question for Stack Overflow than a bug in TensorFlow, and we'll need more details to answer definitively.\n\nHowever, I suspect the problem is that you're rebuilding the graph in each request, and there is some collision between the shared variables. The best approach would be to create the graph once and reuse it for all requests. A quick workaround would be to wrap your call to `demo1.demo(input)` in a `with tf.Graph().as_default():` block, to ensure that the model is recreated in an empty graph.\n"]}, {"number": 2974, "title": "Could not parse default value '1.0' from Attr(\"distortion: float = 1.0\") [locale dependent float parsing]", "body": "Hi! I link my library against tensorflow library. When loading I get the following error:\n\n> F tensorflow/core/framework/op.cc:160] Check failed: ::tensorflow::Status::OK() == (RegisterAlreadyLocked(deferred_[i])) (OK vs. Invalid argument: Could not parse default value '1.0' from Attr(\"distortion: float = 1.0\") for Op FixedUnigramCandidateSampler)\n\nI tried to figure out what causes this problem and found that tensorflow can't parse OP with float attrs. For example, have a look at [core/ops/candidate_sampling_ops.cc:219](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/candidate_sampling_ops.cc#L219)\n\n> .Attr(\"distortion: float = 1.0\")\n\nIf I rewrite it as follows:\n\n> .Attr(\"distortion: float = 1\")\n\nit works fine until it finds another float attribute. I tried to trace down this problem and stuck here [tensorflow/core/lib/strings/numbers.cc:230](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/strings/numbers.cc#L228):\n\n```\nbool safe_strtof(const char* str, float* value) {\n  char* endptr;\n  *value = strtof(str, &endptr);\n  while (isspace(*endptr)) ++endptr;\n  // Ignore range errors from strtod/strtof.\n  // The values it returns on underflow and\n  // overflow are the right fallback in a\n  // robust setting.\n  return *str != '\\0' && *endptr == '\\0';\n}\n```\n\nthis function returns false for `str` = \"1.0\" because `endptr` points to \".0\".\nPlease, fix this problem or suggest me the to fix it and I'll contribute or... tell me how i can work around this... Thank you.\n\nOperating System:\n$ uname -a\nLinux user-desktop 4.4.0-24-generic #43-Ubuntu SMP Wed Jun 8 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\n$ cat /etc/issue\nUbuntu 16.04 LTS \\n \\l\n\n$ g++ --version\ng++ (Ubuntu 4.9.3-13ubuntu2) 4.9.3\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\ncommit hash: b77f607ec03ef251c1d28cc00bc743aad26606a4\n### UPD. Solution\n\nI found out the reason of this strange behavior. If I run my program as follows:\n`$ LC_NUMERIC=C ./my_program_with_tensorflow` \nthan it works properly (my previous `LC_NUMERIC` was `ru_RU.UTF-8` where decimal separator is '`,`'). I find it weird when correctness of program depends on current user locale (you always use the same decimal separator in OP descriptions). Fix it please.\n", "comments": []}, {"number": 2973, "title": "restore() model with absolute and relative path", "body": "### Comment\n\nrestore succeeded with relative path(script directory), but failed with absolute path.\n### Environment info\n\nOperating System: \n\nUbunto 14.04.1\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n22:16 $ ls -al /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936  8\uc6d4 16  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16  8\uc6d4 16  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19  8\uc6d4 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336  8\uc6d4 16  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192  8\uc6d4 16  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024  5\uc6d4 26 09:37 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 61453024  5\uc6d4 26 09:37 /usr/local/cuda/lib64/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024  5\uc6d4 26 09:37 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862  5\uc6d4 26 09:37 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   tensorflow-0.8.0    \n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0\n```\n### Steps to reproduce\n1. train\n\n```\nhttps://github.com/tensorflow/tensorflow/blob/de1334da2c6e074c427d283898450de6e50a605d/tensorflow/models/embedding/word2vec_optimized.py\n\n$ python word2vec_optimized.py --train_data=text8 --eval_data=questions-words.txt --save_path=tmp\n```\n1. test\n\n```\nhttps://github.com/dsindex/segm-lstm/blob/master/test_word2vec.py\n\n$ python test_word2vec.py --model_path=tmp\n...\ncheckpoint_dir =  tmp\ncheckpoint_path =  tmp/model.ckpt-2264698\nmodel restored from tmp/model.ckpt-2264698\nanalogy = moscow\n...\n\n$ python test_word2vec.py --model_path=/path/to/segm-lstm/tensorflow/tensorflow/models/embedding/tmp\n...\ncheckpoint_dir =  /path/to/segm-lstm/tensorflow/tensorflow/models/embedding/tmp\ncheckpoint_path =  model.ckpt-2264698\nTraceback (most recent call last):\n  File \"test_word2vec.py\", line 224, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"test_word2vec.py\", line 204, in main\n    model.saver.restore(session, ckpt.model_checkpoint_path)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1088, in restore\n    raise ValueError(\"Restore called with invalid save path %s\" % save_path)\nValueError: Restore called with invalid save path model.ckpt-2264698\n```\n", "comments": ["Looking through the documentation for saver.restore it looks like the 'save_path' argument requires a path that was previously returned by saver.save() or save.latest_checkpoint(). Did you try passing in the path returned by save.latest_checkpoint()\n\n// In addition to checkpoint files, savers keep a protocol buffer on disk with\n//  the list of recent checkpoints. This is used to manage numbered checkpoint\n//  files and by `latest_checkpoint()`, which makes it easy to discover the path\n//  to the most recent checkpoint. That protocol buffer is stored in a file named\n//   'checkpoint' next to the checkpoint files.\n", "@andydavis1 \n\ni tried tf.train.latest_checkpoint(). but it returned `None`\n\n``` shell\n\nfull_path = tf.train.latest_checkpoint(checkpoint_dir)\nprint(\"full_path = %s\" % full_path)\n\n$ python test_word2vec.py --model_path=/path/to/segm-lstm/tensorflow/tensorflow/models/embedding/tmp\n...\nfull_path = None\ncheckpoint_dir = /path/to/segm-lstm/tensorflow/tensorflow/models/embedding/aaa\ncheckpoint_path = model.ckpt-20879\n...\n```\n\ninterestingly, the reverse case works. \n\n``` shell\n\n$ python word2vec_optimized.py --train_data=text8 --eval_data=questions-words.txt --save_path=/path/to/segm-lstm/tensorflow/tensorflow/models/embedding/tmp\n\n$ python test_word2vec.py --model_path=tmp\n$ python test_word2vec.py --model_path=/path/to/segm-lstm/tensorflow/tensorflow/models/embedding/tmp\n```\n\ni.e, training with full-path and testing with full-path or relative-path are no problem.\n", "Since tf.train is in heavy development, moving to the new version 0.9.0 might help. Also 0.10.0 might also help which is coming soon. We are unlikely to dig into this deeply with an older version. Please try with the newest release. @martinwicke can correct me if I am wrong.\n", "That is somewhat mysterious. Maybe there is a bug in how checkpoint paths are handled. The checkpoint logic is fairly old, but not deprecated, and ought to work.\n", "For now, please use an absolute path for training as a workaround.\n", "Closing due to inactivity. Please comment with new information and I will reopen.\n", "That was a little harsh -- however, it would be good to know if this is still an issue in 0.9. I think it probably is.\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n", "I can confirm this is still an issue with `'0.10.0rc0'`\nI was using `os.path.join(os.getcwd(), \"checkpoints\")` in my code to save and restore, and we ran into problems. Specifically restoring the checkpoint on another machine fails because it's looking for an absolute path on my machine.\n\n```\n    checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/saver.py\", line 1157, in latest_checkpoint\n    if file_io.get_matching_files(ckpt.model_checkpoint_path):\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/lib/io/file_io.py\", line 52, in get_matching_files\n    return pywrap_tensorflow.GetMatchingFiles(compat.as_bytes(filename), status)\n  File \"/usr/lib/python3.4/contextlib.py\", line 66, in __exit__\n    next(self.gen)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/errors.py\", line 450, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors.NotFoundError: /Users/Jared/Documents/MyProject/checkpoints\n```\n\nWe tried loading a relative path, but that only worked AFTER I re-trained it with relative path.\n@martinwicke's workaround works (train and restore with the same path)\n", "I would like to solve this for 0.10. Reopening the issue and assigning to myself.\n", "Thanks for letting us know this is still an issue @jared-mess \n", "I was not able to reproduce this issue with a small unit test, I will try to manually play around with things and report what I can find\n", "I was going to create a separate issue for this but seems like my issue could be addressed by a fix for this one. I'm having the same issue as @jared-mess. I'm currently training a network on one machine, and want to run validation on another with the train folder mapped over network. Unfortunately the mappings on the machines result in different absolute paths so checkpoint restore fails as in jared's example. \n\nThe latest_checkpoint ('checkpoint') file stores absolute paths when the save path is absolute and relative when the save path is relative as per https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L675\n\nWhen the path is relative in the above code, it re-writes as relative to the save dir. Why isn't the path always be re-written as relative to the save dir, even for the absolute case? Is there a use case I'm missing where it makes any sense to have the paths saved in 'checkpoint' as absolute? \n\nIf there is a use case for absolute checkpoint paths in the latest_checkpoint file, would it make sense to have a 'force_relative_paths' flag on the saver to force absolute paths to be re-written as relative? \n\nMy current ugly hack for the issue is to read and re-write the 'checkpoint' file with a different script on the training machine and strip out the absolute portion of the paths. This works.\n", "I'm hitting this issue as well.  My use case is the same as @jared-mess, where I train the model on machine A, download the model directory to a machine B, and try to restore the model from the model directory on B.  I'm on `0.10.0`.\n", "@concretevitamin could you take a look.\n", "Just to confirm my understanding -- the workflow of \"saving on one machine using relative path, moving the ckpt to another machine, restore there\" **is** supported, correct?\n\nI tried with this snippet:\n\n``` python\n  def testSaveRelative(self):\n    rel_path = \"ckpts/step0\"\n    os.mkdir(os.path.join(os.getcwd(), \"ckpts\"))\n\n    with self.test_session() as sess:\n      v = tf.Variable(1.0, name=\"v\")\n      tf.initialize_all_variables().run()\n      saver = tf.train.Saver()\n      path = saver.save(sess, rel_path)\n    self.assertEqual(rel_path, path)\n```\n\nand it passed, generating this `checkpoint` file:\n\n```\nmodel_checkpoint_path: \"step0\"\nall_model_checkpoint_paths: \"step0\"\n```\n\nSo this should not be marked as a \"bug\", and more like a feature request (use relative paths by default).  For this request, I am not the best person to evaluate the consequences (if any) though.\n", "> (use relative paths by default)\n\n+1\n\nI also hit this when I'm using Tensorflow from within a Docker container.  If my container that trains the model mounts the directory to `/models`, and I have a different container that tries to restore the model, but mounts the directory to `/tf/models`, the second container can't restore it.\n", "@concretevitamin are you working on this? It seems that the only change needed is in saver to make sure the checkpoint index always contains only relative paths, right?\n", "Unfortunately I am currently occupied with some other work, and I don't know the implications (esp. if there are any effects on internal workloads) of just changing the default behavior.  The status quo looks reasonable now, namely users can opt for using relative paths.\n", "I worked around this by following @concretevitamin's example of using a relative path for the model directory.  I actually change the directory to the model directory and set the model directory to the current directory (relative, of course).\n\n```\nos.chdir(model_dir)\nmodel_dir = './'\nprint(\"current working directory: %s\" % os.getcwd())\nprint(\"model directory: %s\" % model_dir)\n...\nm = tf.contrib.learn.LinearClassifier(model_dir=model_dir, feature_columns=wide_columns)\n```\n\nIt's a little weird to me, but the model is saved without any path, and I'm able to load it when mounting the model directory into containers at different mount points.\n", "I certainly understand the concern of not knowing the implications of changing default behaviour. Perhaps adding an argument to force saving as relative to checkpoint dir would be a reasonable compromise that won't risk breaking existing code?\n\nUsing relative paths or changing working directories within the script aren't viable long term workarounds. Anyone who maintains their code and data or output in separate trees (ie /home/blah/code/project_xyz/...  and  /data/models/output/run-123) ... has to use rather ridiculous paths  '../../../../data/models/output/run-123' to get the desired behaviour.\n", "I see. That sounds to me like a good feature to have. Feel free to open a PR.", "I was also facing `Restore called with invalid save path` error. Even after fixing the paths, the error persisted. The solution for me was to upgrade to `v0.12.1` from `v0.11.0`", "Might be helpful:\r\nSupports:\r\n[GPU support works with Tensorflow r0.12 with Checkpoints](https://github.com/ravyg/tensorflow/commit/2a83490455f0f02ea05fc447551cacea1c1b8cb6)", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 2972, "title": "update docs", "body": "", "comments": []}, {"number": 2971, "title": "Enable tf.sqrt() for SparseTensor", "body": "Enabled `tf.sqrt()` for `SparseTensor`. Added tests and verified locally. This partially addresses #1828.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks!\n\nCan you please change this a little and make sure that we use the higher tolerance threshold (1e-3) only for testing sqrt? If the other tests pass with the default value of: 1e-6 let us leave it this way.\n", "@maciekcc Updated the PR. Let me know if this is okay.\n", "Thanks for the update!\n\nSorry for nit-picking :). Can you please pass the tolerance explicitly in the test body, say:\n\n`self._compareBothSparse(z, np.sqrt, tf.sqrt, tolerance=1e-3)`\n\npass it down to _check and use `None` as default value. This way you can rely on the defaults defined in the super class. You do not need to keep them synced in two places, and overload only when you want to. Maybe something like this will do:\n\n```\ndef _check(self, result_tensor, result_np, input_sp_t, tolerance=None):\n  ..\n  if tolerance is None:\n    self.assertAllClose(result_np, result_tensor.values.eval())\n  else:\n    self.assertAllClose(result_np, result_tensor.values.eval(), rtol=tolerance, atol=tolerance)\n\n```\n", "@maciekcc Updated.\n", "Jenkins, test this please.\n"]}, {"number": 2970, "title": "Correct typo in pydoc  for MinOrMaxGrad", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n"]}, {"number": 2969, "title": "R0.9 time fix", "body": "", "comments": ["Rebuilt py3 tests to remove flake: http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/804/\n", "Rebuilt linux tests to remove flake: http://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/1145/\n\nAll tests pass (%flake).\n"]}, {"number": 2968, "title": "Docstring example and formatting updates", "body": "Change: 125205938\n", "comments": ["Contained in 2969.\n"]}, {"number": 2967, "title": "R0.9 comments", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n"]}, {"number": 2966, "title": "Version bump: 0.9.0rc0 --> 0.9.0", "body": "", "comments": ["The test that failed in the Mac build is a known flaky test. It should be okay to merge.\n", "I ran MacOS tests separately again, see http://ci.tensorflow.org/job/tensorflow-pull-requests-mac/975/console, which worked.\n"]}, {"number": 2965, "title": "Fail to activate tensorflow", "body": "I installed tensorflow on raspberry pi but fail to activate it:\n\npi@raspberrypi:~ $ sudo pip install ./tensorflow-0.8.0-cp27-none-linux_armv7l.whl \nUnpacking ./tensorflow-0.8.0-cp27-none-linux_armv7l.whl\nRequirement already satisfied (use --upgrade to upgrade): numpy>=1.8.2 in /usr/lib/python2.7/dist-packages (from tensorflow==0.8.0)\nRequirement already satisfied (use --upgrade to upgrade): protobuf==3.0.0b2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.8.0)\nRequirement already satisfied (use --upgrade to upgrade): wheel in /usr/lib/python2.7/dist-packages (from tensorflow==0.8.0)\nRequirement already satisfied (use --upgrade to upgrade): six>=1.10.0 in /usr/local/lib/python2.7/dist-packages/six-1.10.0-py2.7.egg (from tensorflow==0.8.0)\nInstalling collected packages: tensorflow\nSuccessfully installed tensorflow\nCleaning up...\npi@raspberrypi:~ $ source activate tensorflow\n-bash: activate: No such file or directory\npi@raspberrypi:~ $ \n", "comments": ["`activate` is typically only used if you install TensorFlow in a virtualenv. Since you don't appear to have done this, you should be able to use TensorFlow straight away, e.g. by running `python` and typing `import tensorflow as tf` at the prompt.\n", "Thank you very much. It works.\n\nOn Mon, Jun 20, 2016 at 10:04 PM, Derek Murray notifications@github.com\nwrote:\n\n> activate is typically only used if you install TensorFlow in a\n> virtualenv. Since you don't appear to have done this, you should be able to\n> use TensorFlow straight away, e.g. by running python and typing import\n> tensorflow as tf at the prompt.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_tensorflow_tensorflow_issues_2965-23issuecomment-2D227321041&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=f7QKxKdVHfwQaFGmNlXkW0vMsWtZtv-4Z1qz5QceMBg&s=aEp8jhSrc1NCnXN9gOKYuB9LDTr1Y6ToUpTP6oJuFUE&e=,\n> or mute the thread\n> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe_ALPsQFvWos7bD5kpTc9V2NpkeOhFUGnxks5qN0aQgaJpZM4I6Spc&d=CwMCaQ&c=imBPVzF25OnBgGmVOlcsiEgHoG1i6YHLR0Sj_gZ4adc&r=X3YsXL1AZsKHgsh28HFY8A&m=f7QKxKdVHfwQaFGmNlXkW0vMsWtZtv-4Z1qz5QceMBg&s=5JlhzYACMdz4Op6il8N9KdY3PEcXB7gr6aAolAquXfo&e=\n> .\n"]}, {"number": 2964, "title": "R0.8", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 2963, "title": "Use string_to_hash_bucket_fast in feature_column", "body": "Updated feature_column to use string_to_hash_bucket_fast instead of deprecated string_to_hash_bucket.\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "Jenkins, test this please.\n", "Failing test is a known flake with fix underway.\n"]}, {"number": 2962, "title": "support for cudnn 5.1", "body": "2.7x faster using 3x3 kernel convolution \n", "comments": ["what additional support is needed? It's API compatible with 5.0.  Closing unless there's something new.\n"]}, {"number": 2961, "title": "Fixing minor typo in comments.", "body": "Typo in command-line build example comment.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2960, "title": "Added missing pandas handling, fixed tf dataframe tests", "body": "", "comments": []}, {"number": 2959, "title": "Unhandled API callback messages when using GPU tracing", "body": "Using the GPU tracer (e.g. via session.Run with RunOptions FULL_TRACE) results in warnings like the following:\n\n```\nW tensorflow/core/common_runtime/gpu/gpu_tracer.cc:513] Unhandled API Callback for 2 41\nW tensorflow/core/common_runtime/gpu/gpu_tracer.cc:513] Unhandled API Callback for 2 41\n```\n\nThese are believed to be harmless, but are obviously annoying.  A fix is in progress.\n", "comments": ["Hi, has a fix arrived? Is there a workaround?\n", "I just submitted a fix internally... it should go out to github in the next sync.  \n\nIf this is a blocker for you then a simple workaround is to add the code below [here](https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L488):\n\n```\n  } else if ((domain == CUPTI_CB_DOMAIN_RUNTIME_API) &&\n             (cbid == CUPTI_RUNTIME_TRACE_CBID_cudaMemcpy_v3020 ||\n              cbid == CUPTI_RUNTIME_TRACE_CBID_cudaMemcpyAsync_v3020)) {\n```\n", "Hi, I have just tested with TensorFlow master, and it seems that there are no more \"API  Callback\" warnings anymore.\n\nCommit f2a69f30fb159f4dd7d6550562e6e0c2e1f05c3a seems to have fixed the issue, and I suggest closing it.\n", "Thanks!\n"]}, {"number": 2958, "title": "Update build file to fix #2703", "body": "fixes #2703 \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Jenkins, test this please\n", "CLABot, you crazy.\n"]}, {"number": 2957, "title": "Dequeueing immediately after starting threads fails", "body": "From [a question on Stack Overflow](http://stackoverflow.com/questions/37878696/dequeue-immediately-after-starting-threads-fails), the following code fails:\n\n``` python\nimport tensorflow as tf\nimport time\nwith tf.Graph().as_default():\n    filename_list = ['data_batch_{}.mat'.format(i+1) for i in range(5)]\n    filename_queue = tf.train.string_input_producer(filename_list)\n\n    with tf.Session() as sess:\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        #time.sleep(1) # If I uncomment this it works\n        for i in range(5):\n            print(sess.run(filename_queue.dequeue()))\n\n        coord.request_stop()\n        coord.join(threads)\n```\n\n...with the following error:\n\n`NotFoundError: FetchOutputs node input_producer_Dequeue:0: not found`\n\nIt turns out that my fix for #2425 was incomplete, and there is still a race between concurrent graph modification and `Session.run()` calls. I'm preparing a fix.\n", "comments": ["Fixed by 91d65f6\n"]}, {"number": 2956, "title": "Branch 125246026", "body": "", "comments": []}]