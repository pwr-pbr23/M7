[{"number": 14822, "title": "third_party\\eigen3\\unsupported\\Eigen\\CXX11", "body": "when testing tensorflow.dll in vs2015. but  vs2015 can not recongnize \"third_party\\eigen3\\unsupported\\Eigen\\CXX11\\Tensor\".\r\n\r\nI succeeded in compiling tf1.4 on windows 10+vs2015+cuda8.0+cudnn5.1.  tensorflow.lib and tensorflow.dll  are there.\r\n\r\nthe error log:\r\nE:\\tensorflow2\\third_party\\eigen3\\unsupported/Eigen/CXX11/Tensor(1): fatal error C1014: \u5305\u542b\u6587\u4ef6\u592a\u591a: \u6df1\u5ea6 = 1024\r\n\r\nMy win32 console program does nothing ,only include some files .src code as follow:\r\n#include <memory>\r\n#include <string>\r\n#include <vector>\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n//#include \"tensorflow/core/framework/graph.pb.h\"\r\n//#include \"tensorflow/core/public/session.h\"\r\n", "comments": ["fatal error C1014: \u5305\u542b\u6587\u4ef6\u592a\u591a: \u6df1\u5ea6 = 1024\r\nfatal error C1014:too many files:depth =1024", "I found the error is caused by wrong path.  now  everything is ok.", "How did you fix that?", "@luyuesheng ", "@cocoakang \r\nE:\\tensorflow\r\nE:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\nsync\\src\\nsync\\public\r\nE:\\tensorflow\\tensorflow\\contrib\\cmake\\build\r\nE:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\eigen\\src\\eigen\r\nE:\\tensorflow\\tensorflow\\contrib\\cmake\\build\\protobuf\\src\\protobuf\\src", "Sure, don't use this path: E:\\tensorflow\\third_party\\eigen3"]}, {"number": 14821, "title": "[DONOTMERGE] Debug windows GPU build.", "body": "", "comments": ["OK to close?"]}, {"number": 14820, "title": "make the link be more perfect", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 14819, "title": "Keras Dropout support_masking gets reset to False", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: see below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nThe Keras Dropout layer constructor (tensorflow/python/keras/_impl/keras/layers/core.py) sets support_masking=True and then calls its super constructor, which sets it back to False. Other layers defined in that module appear to set support_masking=True after the super constructor call.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nfrom tensorflow.contrib.keras.api.keras.models import Sequential\r\nfrom tensorflow.contrib.keras.api.keras.layers import Dropout, InputLayer, LSTM, Masking \r\n\r\nif __name__ == '__main__':\r\n\r\n    test1 = True\r\n\r\n    def model1():\r\n        model = Sequential()\r\n        model.add(InputLayer([8, 64]))\r\n        model.add(Masking())\r\n        model.add(Dropout(0.5))\r\n\r\n    def model2():\r\n        model = Sequential()\r\n        model.add(InputLayer([8, 64]))\r\n        model.add(Masking())\r\n        model.add(LSTM(128, return_sequences=True))\r\n        model.add(Dropout(0.5))\r\n\r\n    if test1:\r\n        model1()\r\n    else:\r\n        model2()\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"expose_dropout_bug.py\", line 16, in <module>\r\n    model.add(Dropout(0.5))\r\n  File \"/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py\", line 501, in add\r\n    output_tensor = layer(self.outputs[0])\r\n  File \"/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 252, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"/.venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 594, in __call__\r\n    output_mask = self.compute_mask(inputs, previous_mask)\r\n  File \"/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py\", line 308, in compute_mask\r\n    'but was passed an input_mask: ' + str(mask))\r\nTypeError: Layer dropout_1 does not support masking, but was passed an input_mask: Tensor(\"masking/Any_1:0\", shape=(?, 8), dtype=bool)\r\n```", "comments": ["Sounds like a mistake, cc @fchollet \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ab0fcaceda001825654424bf18e8a8e0f8d39df2/tensorflow/python/keras/_impl/keras/layers/core.py#L107-L113", "Correct, that's a bug, `self.supports_masking = True` should be after the call to the parent's constructor.", "I can work on it. I'll fix it later.", "Fine. It would be better if you can add a corresponding test case here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ab0fcaceda001825654424bf18e8a8e0f8d39df2/tensorflow/python/keras/_impl/keras/layers/core_test.py#L38", "okay", "@facaiy please check if unit test added is valid. Thanks!"]}, {"number": 14818, "title": "Error when setting model_dir for tf.keras.estimator.estimator_from_model()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: tf.VERSION = 1.4.0 tf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0.61/6.0.21\r\n- **GPU model and memory**: NVIDIA Tesla M60 8 GB\r\n- **Exact command to reproduce**: See Below\r\n\r\n### Describe the problem\r\nWhen trying to use an estimator that is derived from ```tf.keras.estimator.estimator_from_model()``` and training with ```tf.estimator.train_and_evaluate()```, setting ```model_dir``` either in the ```RunConfig``` or in ```tf.keras.estimator.model_to_estimator``` causes a ```NotFoundError``` to be thrown. If the model_dir is not set, then a tmp directory is used as expected and the training is completed successfully.\r\n\r\nI also tested this with the canned estimator ```tf.estimator.DNNRegressor```, and the settings were applied as expected when the RunConfig was passed to the estimator or the model_dir passed to the estimator directly.\r\n\r\nBelow is code to demonstrate this issue. \r\n\r\n### Source code / logs\r\nMinimal example, NotFoundError is thrown:\r\n```python\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\ninputs = tf.keras.layers.Input(shape=(10,))\r\noutputs = tf.keras.layers.Dense(10)(inputs)\r\nmodel = tf.keras.models.Model(inputs, outputs)\r\nmodel.compile(optimizer='sgd', loss='mse')\r\n\r\n# Both of these result in a NotFoundError\r\nrun_config = tf.estimator.RunConfig(model_dir='min_out')\r\nest_keras = tf.keras.estimator.model_to_estimator(keras_model=model, config=run_config)\r\n#est_keras = tf.keras.estimator.model_to_estimator(keras_model=model, model_dir='min_out')\r\n\r\ninput_name = model.input_names[0]\r\ndata = np.random.rand(1000,10).astype(np.float32)\r\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn({input_name:data}, data, batch_size=10, num_epochs=None, shuffle=False)\r\n\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)\r\neval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn, steps=10)\r\ntf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)\r\n```\r\n\r\nError generated:\r\n```python\r\n$ python minimal_modeldir.py\r\nINFO:tensorflow:Using the Keras model from memory.\r\n2017-11-22 21:40:44.540162: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-22 21:40:46.525335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-22 21:40:46.525573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 7.43GiB freeMemory: 7.35GiB\r\n2017-11-22 21:40:46.525595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2)\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': None, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa7fe724890>, '_model_dir': 'min_out', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n2017-11-22 21:40:47.048052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2)\r\nINFO:tensorflow:Restoring parameters from min_out/model.ckpt-1\r\n2017-11-22 21:40:47.076405: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key dense_1/kernel not found in checkpoint\r\n2017-11-22 21:40:47.079173: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key SGD/iterations not found in checkpoint\r\n2017-11-22 21:40:47.079637: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key dense_1/kernel not found in checkpoint\r\n         [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]\r\n2017-11-22 21:40:47.080045: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key SGD/lr not found in checkpoint\r\n2017-11-22 21:40:47.080601: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key SGD/decay not found in checkpoint\r\n2017-11-22 21:40:47.081693: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key dense_1/bias not found in checkpoint\r\n2017-11-22 21:40:47.081871: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key training/SGD/Variable not found in checkpoint\r\n2017-11-22 21:40:47.082097: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key SGD/momentum not found in checkpoint\r\n2017-11-22 21:40:47.082403: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key training/SGD/Variable_1 not found in checkpoint\r\nTraceback (most recent call last):\r\n  File \"minimal_modeldir.py\", line 23, in <module>\r\n    tf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 430, in train_and_evaluate\r\n    executor.run_local()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 609, in run_local\r\n    hooks=train_hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 780, in _train_model\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 368, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 673, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 493, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 851, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 856, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 554, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 428, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 273, in prepare_session\r\n    config=config)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 205, in _restore_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1666, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Key dense_1/kernel not found in checkpoint\r\n         [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]\r\n         [[Node: save/RestoreV2_2/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_30_save/RestoreV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op u'save/RestoreV2_5', defined at:\r\n  File \"minimal_modeldir.py\", line 23, in <module>\r\n    tf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 430, in train_and_evaluate\r\n    executor.run_local()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/training.py\", line 609, in run_local\r\n    hooks=train_hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 302, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py\", line 780, in _train_model\r\n    log_step_count_steps=self._config.log_step_count_steps) as mon_sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 368, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 673, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 493, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 851, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 856, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 554, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 419, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 212, in finalize\r\n    self._saver.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1227, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1263, in _build\r\n    build_save=build_save, build_restore=build_restore)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 745, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 470, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 427, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 267, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1021, in restore_v2\r\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Key dense_1/kernel not found in checkpoint\r\n         [[Node: save/RestoreV2_5 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2_5/tensor_names, save/RestoreV2_5/shape_and_slices)]]\r\n         [[Node: save/RestoreV2_2/_9 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_30_save/RestoreV2_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n```\r\n", "comments": ["@fchollet @yifeif May be related to #14504 and/or #14776", "@yifeif could you please take a look at this.", "Thanks for reporting the issue @droidicus. This should be fixed with #14354."]}, {"number": 14817, "title": "1", "body": "ERROR PR", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 14816, "title": "Fix several potential memory leaks", "body": "This fix fixes several potential memory leaks, mostly caused by error return without proper deleting.\r\n\r\nNote: The original issue was raised by @orpillar \ud83d\udc4d , thanks!\r\n\r\nThis fix fixes #14800.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@tensorflow-jenkins test this please"]}, {"number": 14815, "title": "cant downlod tensorflow it shows could not find a versone", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["what os you are using??\r\nYou can use the following guide.\r\nhttps://www.tensorflow.org/install/", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  \r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 14814, "title": "Branch 176709725", "body": "", "comments": []}, {"number": 14813, "title": "Introduce tf_http_archive", "body": "This new repository rule consolidates patched_http_archive,\r\ntemp_workaround_http_archive, http_archive, and new_http_archive.\r\n\r\nThe following behaviors have been introduced:\r\n\r\n- A delete attribute that can rm -rf certain repo content after extraction\r\n- Helpful error messages when mirroring requirements aren't followed\r\n\r\ncc: @gunan", "comments": ["The change LGTM,\r\nHowever @mikecase had a cl to remove tf_workaround_http_archive internally. I expect once this change and his get submitted there will be conflicts.\r\nSo maybe you can sync with him, and we can submit both either internally, or both externally?", "Yeah, I removed temp_http_archive since it no longer seemed necessary and the CL landed internally. So, I'm guessing it will cause rebase conflicts with this CL once it syncs to GitHub.\r\n\r\n@damienmg, what is the risk of double compilation you mentioned? How would that occur?"]}, {"number": 14812, "title": "segmentation fault due to pytorch and tensorflow conflictions", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian GNU/Linux 8 (jessie)\r\n- **TensorFlow installed from (source or binary)**: from Anaconda, with command:\r\n`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp35-cp35m-linux_x86_64.whl`\r\n\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.5.4\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: cuda 8.0\r\n- **GPU model and memory**: TITAN Xp, 12G\r\n- **Exact command to reproduce**:\r\nThis fails:\r\n```python\r\n>>> import torch\r\n>>> import tensorflow as tf\r\nSegmentation fault (core dumped)\r\n```\r\n\r\n\r\n### Describe the problem\r\nI am using pytorch version 0.2.0_4, for python 3.5, with cuda support. I installed it from the following command:\r\n\r\n`conda install pytorch torchvision cuda80 -c soumith`\r\n\r\nWhen I use tensorflow alone, it works fine; i.e., doing an import like \r\n```python\r\n>>> import tensorflow as tf\r\n```\r\nhas no problem. Also, Importing tensorflow before torch seems fine as well.\r\n\r\n\r\nHowever if I import pytorch before tensorflow, it fails and reported a segmentation error (as shown above).\r\n\r\n ", "comments": ["Sounds like this is another case of https://github.com/pytorch/pytorch/issues/3059. So unless this is a different stack trace than https://github.com/tensorflow/tensorflow/issues/13615, we're just waiting on the PyTorch folks.", "Closing this, feel free to reopen if this is a different stack trace."]}, {"number": 14811, "title": "VERBS and OpenMPI not building anymore without CUDA ?", "body": "Hello,\r\n\r\nWhen activating MPI or VERBS without CUDA, build fails with the following error:\r\n\r\n```\r\nERROR: /build/python-tensorflow-cuda-1.4.0/tensorflow/contrib/verbs/BUILD:133:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma' failed (Exit 1): gcc failed: error executing command \r\n  (cd /tmp/tmp.plGX4Xhgql/.cache/bazel/_bazel_pbuilder/436710022b7d9d872ccd97b57710586f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/libfakeroot:/usr/lib64/libfakeroot:/usr/lib32/libfakeroot \\\r\n    PATH=/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3.4 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages/ \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -mavx -msse4.1 -msse4.2 '-std=c++0x' -mavx -msse4.1 -msse4.2 -MD -MF bazel-out/local-py3-opt/bin/tensorflow/contrib/verbs/_objs/rdma/tensorflow/contrib/verbs/rdma.pic.d '-frandom-seed=bazel-out/local-py3-opt/bin/tensorflow/contrib/verbs/_objs/rdma/tensorflow/contrib/verbs/rdma.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -DSNAPPY -DTENSORFLOW_USE_VERBS -iquote . -iquote bazel-out/local-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-py3-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-py3-opt/genfiles/external/local_config_sycl -iquote external/nsync -iquote bazel-out/local-py3-opt/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/local-py3-opt/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/local-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local-py3-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/local-py3-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/local-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local-py3-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/local-py3-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/local-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local-py3-opt/genfiles/external/zlib_archive -iquote external/curl -iquote bazel-out/local-py3-opt/genfiles/external/curl -iquote external/boringssl -iquote bazel-out/local-py3-opt/genfiles/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/local-py3-opt/genfiles/external/jsoncpp_git -iquote external/grpc -iquote bazel-out/local-py3-opt/genfiles/external/grpc -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local-py3-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/local-py3-opt/genfiles/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/local-py3-opt/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/local-py3-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/local-py3-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/local-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local-py3-opt/genfiles/external/zlib_archive -isystem external/curl/include -isystem bazel-out/local-py3-opt/genfiles/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/local-py3-opt/genfiles/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/local-py3-opt/genfiles/external/jsoncpp_git/include -isystem external/grpc/include -isystem bazel-out/local-py3-opt/genfiles/external/grpc/include -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/contrib/verbs/rdma.cc -o bazel-out/local-py3-opt/bin/tensorflow/contrib/verbs/_objs/rdma/tensorflow/contrib/verbs/rdma.pic.o).\r\nIn file included from ./tensorflow/core/platform/stream_executor.h:26:0,\r\n                 from ./tensorflow/core/common_runtime/gpu/gpu_util.h:23,\r\n                 from tensorflow/contrib/verbs/rdma.cc:23:\r\n./tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory\r\n #include \"cuda/cuda_config.h\"\r\n```\r\n\r\nIs that expected ?\r\n\r\nThanks", "comments": ["@junshi15 can you comment on this?  Thanks!", "It was broken in an early commit that was not related to VERBS or OpenMPI.\r\nThis [PR](https://github.com/tensorflow/tensorflow/pull/14290) is supposed to be a fix.", "Thanks @junshi15!  Marking this as \"contributions welcome\", since the in-flight PR will hopefully fix this.", "Or you could use GDR without GPU to enjoy RDMA :)", "@eLvErDe FYI #14290 was merged as 49059695babc08df22f31124480142e7f6aec5eb, which hopefully fixes this problem.\r\n\r\nCan you try again at that commit?", "I will. Is it also fixing openmpi ?", "I do not think the PR touches openmpi, which may need a separate fix.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing this out.  If this is still a problem, just ping and I'll re-open it.", "Hello,\r\n\r\nYes this is still an issue.\r\n\r\nEnabling OpenMPI without CUDA crash build:\r\n`undeclared inclusion(s) in rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so':`", "@tatatodd "]}, {"number": 14810, "title": "Also ignore no_oss tags in windows builds.", "body": "", "comments": []}, {"number": 14809, "title": "Batch Normalization layer is unusable", "body": "Despite the numerous submitted issues, `tf.layers.batch_normalization` still feels completely unusable. The major problems are:\r\n\r\n1. It does not allow for input tensors with varying shapes. It is complete nonsense to have a fixed batch size. It should be allowed for the batch dimension to be vary.\r\n\r\n2. One needs to manually update the running mean and variance. This is very uncomfortable and a very common pitfall for many beginners, while it would take just a couple of lines to do the update internally based on the value of the `training` parameter.\r\n\r\nI have recently seen too many custom implementations of a batch normalization layer because of the above problems and it will definitely be very useful if these problems are fixed ASAP.\r\n\r\nI am using `tensorflow-gpu`, version `1.4`", "comments": ["1. It does allow varying shapes. `tf.layers.batch_normalization(x)` only require x to have known channel.\r\n\r\n2. Doing the update internally can be significantly slower in certain cases, due to the dependency it introduced. So I think the current API is a reasonable choice. Though I don't mind seeing a new API which does the update internally so it's more friendly to beginners.", "Thanks for the quick reply!\r\n\r\n1. It does not seem to work for me. \r\n```\r\n>>> data = tf.placeholder(tf.float32, shape=[None, 2, 3])\r\n>>> norm = tf.layers.batch_normalization(data, axis=0)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 586, in batch_normalization\r\n    return layer.apply(inputs, training=training)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/base.py\", line 671, in apply\r\n    return self.__call__(inputs, *args, **kwargs)\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/base.py\", line 559, in __call__\r\n    self.build(input_shapes[0])\r\n  File \"/usr/lib64/python3.6/site-packages/tensorflow/python/layers/normalization.py\", line 179, in build\r\n    input_shape)\r\nValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([Dimension(None), Dimension(2), Dimension(3)]))\r\n```\r\nIn the source code, it seems that the batch dimension must be fixed: https://github.com/tensorflow/tensorflow/blob/b5df90f91cde6eb12af9cbe818bd2cf4a9bcc687/tensorflow/python/layers/normalization.py#L176-L179 Am I doing something wrong?\r\n\r\n\r\n2. Out of curiosity, is it slower because computing the input for the next layer takes more time when you do the update internally? In any case, there is often a trade-off between code readability and performance, so in my opinion it would definitely be much better if there is some parameter that controls if the update is done internally or not, rather than leaving no choice for the programmer. It would also be a good, if possible, to provide some approximate metric of the speed up, when the update is not done internally, so the programmer can choose for himself. ", "Axis is the last dimension by default, so it indeed is the channel dimension as mentioned by @ppwwyyxx.", "1. `axis` does not mean batch dimension\r\n2. Theoretically, doing updates in the layer means the layer might get slower, because it waits for the updates to finish.\r\nHalf year ago I switched from doing the updates internally to externally in tensorpack, to match the performance of official benchmark ([see the note](https://github.com/ppwwyyxx/tensorpack/issues/254#issuecomment-299681482)). I remembered there was a significant difference at that time. But just now I ran a ResNet50 test, it seems like the performance difference becomes negligible today. Maybe now it makes sense to add the option to improve usability.", "1. Sorry, I got confused and it took me some time to clear it. I feel that the documentation is not clear  enough on the meaning of axis. A lot of the confusion comes from the way that `dense` and `conv` layers are handled. Specifically, in `conv` layers, the moments are taken over all dimensions except for the channel dimension, while in `dense` layers the moments are taken over the batch dimension only (since there are only 2 dimensions). When one reads the code in detail, this becomes clear, but in my opinion it is very unclear the way it is currently written in the documentation.\r\n\r\nI saw that on the master branch it is now allowed to give a list for the `axis` argument and the documentation there is much more clear on the issue. For complete clarity, I think that examples for both `conv` and `dense` layers would save a lot of time for beginners. Meanwhile it would be a very good idea to update the documentation for the 1.4 API so that it is at least as clear as the one on the master branch.\r\n\r\n2. Since the performance difference is negligible, it seems absolutely pointless to have to run the update ops manually. The default behavior must definitely be that the ops are run internally, and possibly a parameter to control this can be included (if there are still ppl interested in running the ops manually).\r\n", "Hi, @nikonikolov. I agree that readable documents can save user a lot of time. And if you'd like, welcome to help revise documents. Thanks.\r\n\r\nCould we update the op internally? I believe @fchollet can give us more useful information.", "I just got bitten by this.  The way that it's mentioned in the doc is very easy to overlook, considering that updating these manually is critical for use, and the note font is smaller than any other on the page...\r\n\r\nIt would be great to have these updated internally by default, providing an option to update manually if desired.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "I'm not sure why this was closed without answers. An option to update the moving averages internally is a totally valid feature request. \r\n\r\nBesides the ease of use, #14699 has mentioned another valid reason for this feature, i.e. to use within `tf.cond`, the update ops should be evaluated depending on the condition.\r\n\r\n`tf.contrib.layers.batch_norm` already supports this feature, btw.", "I also support the idea of updating mean and variance internally. At least it should be optional and these params should be updated internally by default.\r\n\r\nIt should also support batch normalization even when the dimension is not fixed. The training is not always done on fixed size pictures, you do it even for structured data and you may need to change the size of mini batch when training. It is exactly the case I have and because of this I am not able to use the tf.layers.batch_normalization. Now I guess I need to use tf.nn.batch_normalization and update many other things manually.\r\n\r\nI also see that many people address the \"beginners\" issue. Updates need to be done for everyone, not necessarily for the sake of beginners. Tensorflow is a great API! Very advanced! And this work is really admirable! But note that it is still an API. With other words, it is a tool that is used to create other things. The tool/API itself is not the primary target. These functions need to be documented in a way so that the user understands what it is used for just by reading the documentation. There are 1000s of functions in tensorflow and it takes a life if the user will need to get into the source code to understand what it is really doing and how it is used. I have spent hours and even days for simple things. With proper documentation, I would probably solve the issues within minutes.\r\n\r\nDon't get it wrong. This is really great work! But documentation really needs more focus.", "@fchollet Could you take a look?", "axis: An int, the axis that should be normalized (typically the features axis). For instance, after a Convolution2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\r\n\r\nso, you should choose the exact axis that you want to normalize.\r\n\r\nSuch as:\r\nZ = [100, None]\r\nB = tf.layers.batch_normalization(Z, **axis = 0**, training = is_training)\r\n\r\nif you write \"axis = -1\", it will get wrong bug report."]}, {"number": 14808, "title": "Remove useless statements in Dockerfiles", "body": "'CMD [\"/bin/bash\"]' is not useful since it's already provided by the base ubuntu image.\r\n'RUN [\"/bin/bash\"]' looks like a typo and just creates an extra empty layer.\r\n\r\nSigned-off-by: Felix Abecassis <fabecassis@nvidia.com>", "comments": ["Can one of the admins verify this patch?", "@gunan "]}, {"number": 14807, "title": "seg fault training tf.nn.conv3d with minibatch size >2", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code! You can find it here: https://github.com/NERSC/CosmoFlow/tree/master/SegFault\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: SUSE Linux 12.2\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc1-3112-g65b6a75', '1.4.0-rc0') Note this is NOT compiled with the Intel MKL options. \r\n- **Python version**: 2.7.13 \r\n- **Bazel version (if compiling from source)**: 0.6.0\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A, Running on x86_64 Intel Haswell node\r\n- **Exact command to reproduce**: See README in https://github.com/NERSC/CosmoFlow/tree/master/SegFault\r\n\r\n\r\n### Describe the problem\r\n\r\nA seg fault when training a tf.nn.conv3d with minibatch size more than 2 on a single Intel Haswell. The seg fault occurs at [line 187](https://github.com/NERSC/CosmoFlow/blob/master/SegFault/CosmoNet.py#L187).  \r\n\r\n### Source code / logs\r\n\r\nGDB log: https://github.com/NERSC/CosmoFlow/blob/master/SegFault/gdbTrace.log\r\nIt looks like some kind of cyclic dependency in Eigen::TensorEvaluator. ", "comments": ["It could be more helpful if you could create a mini reproducable example. Thanks. ", " I agree - but I haven't had time to make a mini example (this is someone else's code). I will try to find time to do so, but in the meantime the github repo I linked to will reproduce the error shown in the gdb log. ", "@djbard Thanks for filing the issue!  As @facaiy mentioned, a minimum repro would be appreciated.  But I agree that this looks like a cycle in Eigen that ends up exhausting the stack.\r\n\r\n@benoitsteiner @rmlarsen Can you take a look, or suggest someone who can?\r\n\r\nHere's a simplified form of the bottom of the stack trace from the `gdbTrace.log` link above.  I dunno if this is simply a bug that causes an infinite loop, or if there's some static unrolling that's managing to blow our stack, but having the same repeated pattern of calls for >7000 frames definitely looks suspicious:\r\n```\r\n... pattern repeats until frame #0 which segfaults ...\r\n#7671 Eigen::TensorEvaluator::Context::kernel(long, long, long) ()\r\n  \r\n#7672 Eigen::TensorEvaluator::Context::signal_kernel(long, long, long, bool) ()\r\n#7673 Eigen::TensorEvaluator::Context::pack_rhs(long, long) ()\r\n#7674 Eigen::TensorEvaluator::Context::enqueue_packing_helper(long, long, long, bool) ()\r\n#7675 Eigen::TensorEvaluator::Context::pack_lhs(long, long) ()\r\n#7676 Eigen::TensorEvaluator::Context::enqueue_packing_helper(long, long, long, bool) ()\r\n#7677 Eigen::TensorEvaluator::Context::kernel(long, long, long) ()\r\n  \r\n#7678 Eigen::TensorEvaluator::Context::signal_kernel(long, long, long, bool) ()\r\n#7679 Eigen::TensorEvaluator::Context::pack_rhs(long, long) ()\r\n#7680 Eigen::TensorEvaluator::Context::enqueue_packing_helper(long, long, long, bool) ()\r\n#7681 Eigen::TensorEvaluator::Context::pack_lhs(long, long) ()\r\n#7682 Eigen::TensorEvaluator::Context::enqueue_packing_helper(long, long, long, bool) ()\r\n#7683 Eigen::TensorEvaluator::Context::kernel(long, long, long) ()\r\n```\r\nFor readability, I simplified `Eigen::TensorEvaluator::Context` above from the full form, in all its templatized glory:\r\n```\r\nEigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0, Eigen::MakePointer>, 8, 4, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 8, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorVolumePatchOp<-1l, -1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 5, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 4, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >\r\n```", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Has anyone had a chance to look at this issue @tatatodd? ", "I dunno - @benoitsteiner @rmlarsen can you take a look, or suggest someone who can?", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "This looks like a duplicated of  #16139 which has an example to reproduce, so I'm closing this issue."]}, {"number": 14806, "title": "Go TensorFlow 1.4.0: DataType 21 is not supported", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: NA (using go)\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0.176-4 / 7.0.3-1\r\n- **GPU model and memory**: GTX 1060 6GB\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nCalling the Value() method on the evaluated output tensor of dataset related operations from the go package fails with the error `DataType 21 is not supported`. It looks like `op.TextLineDataset()` produces a tensor of type `tf.Half` can't be converted to a go type?\r\nI may be using the datasets wrong. If so, the error and/or documentation should be improved.\r\n\r\n### Source code / logs\r\n\r\n```\r\npackage main\r\n\r\nimport (\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n)\r\n\r\nfunc main() {\r\n\ts := op.NewScope()\r\n\ttextLineHandle := op.TextLineDataset(s,\r\n\t\top.Const(s.SubScope(\"filename\"), \"dataset.txt\"),\r\n\t\top.Const(s.SubScope(\"compression_type\"), \"\"),\r\n\t\top.Const(s.SubScope(\"buffer_size\"), int64(1)),\r\n\t)\r\n\tgraph, err := s.Finalize()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tsess, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tresults, err := sess.Run(nil, []tf.Output{textLineHandle}, []*tf.Operation{})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_ = results[0].Value()\r\n}\r\n```\r\nProduces:\r\n```\r\n[isaac@d6-arch tfes]$ go run dataset_demo.go \r\n2017-11-22 11:06:36.945842: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-22 11:06:37.042484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-22 11:06:37.042786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 5.93GiB freeMemory: 4.58GiB\r\n2017-11-22 11:06:37.042801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\npanic: BUG: Please report at https://github.com/tensorflow/tensorflow/issues with the note: Go TensorFlow 1.4.0: DataType 21 is not supported\r\n\r\ngoroutine 1 [running]:\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.typeOf(0xc400000015, 0x0, 0x0, 0x0, 0x49d72d, 0x4c6e40)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:273 +0x14d\r\ngithub.com/tensorflow/tensorflow/tensorflow/go.(*Tensor).Value(0xc42000c0e0, 0x0, 0xc420057f60)\r\n\t/home/isaac/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:175 +0x8d\r\nmain.main()\r\n\t/home/isaac/go/src/github.com/is8ac/tfes/dataset_demo.go:27 +0x2b0\r\nexit status 2\r\n```", "comments": ["@mrry could you please take a look.", "I think `DataType 21` is `DT_VARIANT`, which is indeed the expected dtype for a `TextLineDataset`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ab0fcaceda001825654424bf18e8a8e0f8d39df2/tensorflow/core/framework/types.proto#L37\r\n\r\nI'm not sure why it's not supported in the Go API, but there is a TODO here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/ab0fcaceda001825654424bf18e8a8e0f8d39df2/tensorflow/go/tensor.go#L237\r\n\r\nSince I believe @keveman is busy with other things ;), I'm reassigning this to @asimshankar who owns the Go API.", "As @mrry mentioned, `op.TextLineDataset` produces a `DT_VARIANT` tensor.\r\nI'm not quite sure what we'd want `.Value()` on it to return. Could you clarify what you were expecting it to?\r\n\r\nNote that the `dataset` tensor is meant for consumption by iterators, and you can construct dataset tensors and create iterators over them in Go (though, you have to do it from the primitive ops as we don't have the convenience APIs as we do in the `tf.data` module in Python). For example:\r\n\r\n```go\r\npackage main\r\n\r\nimport (\r\n        \"fmt\"\r\n        tf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n        \"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n)\r\n\r\nfunc main() {\r\n        s := op.NewScope()\r\n        dataset := op.TextLineDataset(s,\r\n                op.Const(s.SubScope(\"filename\"), \"dataset.txt\"),\r\n                op.Const(s.SubScope(\"compression_type\"), \"\"),\r\n                op.Const(s.SubScope(\"buffer_size\"), int64(1)),\r\n        )\r\n\r\n       \r\n        outputTypes := []tf.DataType{tf.String}\r\n        outputShapes := []tf.Shape{tf.ScalarShape()}\r\n        iterator := op.Iterator(s, \"\", \"\", outputTypes, outputShapes)\r\n        next := op.IteratorGetNext(s, iterator, outputTypes, outputShapes)\r\n        initIterator := op.MakeIterator(s, dataset, iterator)\r\n       \r\n        graph, err := s.Finalize()\r\n        if err != nil {\r\n                panic(err)\r\n        }\r\n        sess, err := tf.NewSession(graph, nil)\r\n        if err != nil {\r\n                panic(err)\r\n        }\r\n        if _, err = sess.Run(nil, nil, []*tf.Operation{initIterator}); err != nil {\r\n                panic(err)\r\n        }\r\n        results, err := sess.Run(nil, next, []*tf.Operation{})\r\n        if err != nil {\r\n                panic(err)\r\n        }\r\n        v := results[0].Value()\r\n        fmt.Printf(\"%T: %v\\n\", v, v)\r\n}\r\n```\r\nI agree the error message can be improved, will try to write up a change for that.\r\nIn the mean time, could you clarify what you may want `dataset.Value()` to return?", "I didn't have any expectations regarding what dataset.Value() returns, I was just playing with the OP to see what it did and was surprised by the error.\r\n\r\nNow that you have shown me the usage of op.Iterator(), I know how to use datasets correctly, and don't need dataset.Value() to return anything in particular. Just a clearer error message would be nice.\r\n\r\nDoes there currently exist documentation / tutorials for the Go bindings beyond the GoDocs? I'm writing some Go TensorFlow tutorials and would be happy to contribute.", "Unfortunately, no, there isn't any additional documentation beyond the godoc. We look forward to your tutorials and more community support/contributions. Thanks!"]}, {"number": 14805, "title": "Installation says to use cuDNN v6.1, but NVIDIA only offers 6.0 and 7.0.4", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nNVIDIA only offers the following from their website\r\n\r\nhttps://developer.nvidia.com/rdp/cudnn-download\r\n\r\nDownload cuDNN v7.0.4 (Nov 13, 2017), for CUDA 9.0\r\nDownload cuDNN v7.0.4 (Nov 13, 2017), for CUDA 8.0\r\nDownload cuDNN v6.0 (April 27, 2017), for CUDA 8.0\r\nDownload cuDNN v6.0 (April 27, 2017), for CUDA 7.5\r\nDownload cuDNN v5.1 (Jan 20, 2017), for CUDA 8.0\r\nDownload cuDNN v5.1 (Jan 20, 2017), for CUDA 7.5\r\n\r\ninstallation instructions say to use \r\nhttps://www.tensorflow.org/install/install_windows\r\n\r\ncuDNN v6.1. For details, ....\r\n\r\nIf you have a different version of one of the preceding packages, please change to the specified versions. In particular, the cuDNN version must match exactly: TensorFlow will not load if it cannot find cuDNN64_6.dll.\r\n\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Thanks for filing the issue @johngrabner \r\n\r\nI suspect the `cuDNN v6.1` in the TensorFlow Windows installation instructions is a typo, and the correct version should be `cuDNN v6.0`.  Notice that the linux instructions mention `v6.0`.\r\nhttps://www.tensorflow.org/install/install_linux\r\n\r\n@mrry can you confirm that this is a typo?  If so I'll fix up the instructions; if not we'll need to find `cuDNN v6.1`  :)", "BTW I should also point out that people are confused by this on the nvidia forums as well, so if this is indeed a typo, we should also leave a comment here:\r\nhttps://devtalk.nvidia.com/default/topic/1023497/no-link-to-download-cudnn-v6-or-v6-1/", "Yep, looks like 5.1 was changed to 6.1 without making sure that that exists :).", "Release note says 6.0\nSo 2 inconsistencies to fix\n\n\n\nSent via the Samsung Galaxy S7 edge, an AT&T 4G LTE smartphone\n\n\n-------- Original message --------\nFrom: Derek Murray <notifications@github.com>\nDate: 11/27/17 6:20 PM (GMT-06:00)\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: johngrabner <john_grabner@hotmail.com>, Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Installation says to use cuDNN v6.1, but NVIDIA only offers 6.0 and 7.0.4 (#14805)\n\n\nYep, looks like 5.1 was changed to 6.1 without making sure that that exists :).\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F14805%23issuecomment-347372142&data=02%7C01%7Cjohn_grabner%40hotmail.com%7C5bcfe3d5f38a4248968708d535f5dace%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636474252401630907&sdata=9OuOU9VMSxomNh3Otbf9CHusrRkuHo%2F5%2Boubs%2Boo8KU%3D&reserved=0>, or mute the thread<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAH1DhVPX0QxOaEmx5XDmY2NtBo0g5YpLks5s61HWgaJpZM4Qn6OM&data=02%7C01%7Cjohn_grabner%40hotmail.com%7C5bcfe3d5f38a4248968708d535f5dace%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636474252401630907&sdata=bmI1G%2Btw49ZjAs54mKnL8a5YtzMg3NYaiedgQUfMMpU%3D&reserved=0>.\n", "@johngrabner To be clear, there was simply a typo in the Windows installation instructions.\r\n\r\nTensorFlow 1.3 onwards requires `cuDNN v6.0` for both Linux and Windows.  The release notes already say`cuDNN v6.0`, which is correct.\r\n\r\nI'm fixing up the docs, which should eventually make it onto the live tensorflow.org site.  I'll also leave a comment on the nvidia forum I mentioned above.\r\n\r\nClosing this out.", "Thank you\n\n\n\nSent via the Samsung Galaxy S7 edge, an AT&T 4G LTE smartphone\n\n\n-------- Original message --------\nFrom: Todd Wang <notifications@github.com>\nDate: 11/27/17 6:49 PM (GMT-06:00)\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: johngrabner <john_grabner@hotmail.com>, Mention <mention@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Installation says to use cuDNN v6.1, but NVIDIA only offers 6.0 and 7.0.4 (#14805)\n\n\n@johngrabner<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fjohngrabner&data=02%7C01%7Cjohn_grabner%40hotmail.com%7C983bbe58e70345d52a1608d535f9d8ed%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636474269550966930&sdata=q2Mb8tWc3ua7TL1XPldi3%2FZxG4w79k5MZ774vl9iKdE%3D&reserved=0> To be clear, there was simply a typo in the Windows installation instructions.\n\nTensorFlow 1.3 onwards requires cuDNN v6.0 for both Linux and Windows. The release notes already saycuDNN v6.0, which is correct.\n\nI'm fixing up the docs, which should eventually make it onto the live tensorflow.org site. I'll also leave a comment on the nvidia forum I mentioned above.\n\nClosing this out.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fissues%2F14805%23issuecomment-347376904&data=02%7C01%7Cjohn_grabner%40hotmail.com%7C983bbe58e70345d52a1608d535f9d8ed%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636474269550966930&sdata=9WAazlAypOuXL6ZkgkEDSi1j1SRlAtZuatDxN1IvikU%3D&reserved=0>, or mute the thread<https://eur01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAH1DhRNaILmZx491zB0WqspkuaNU9vI-ks5s61iIgaJpZM4Qn6OM&data=02%7C01%7Cjohn_grabner%40hotmail.com%7C983bbe58e70345d52a1608d535f9d8ed%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636474269550966930&sdata=0yOH%2BdDth1QSsAFnknjpGqczeHzN%2FkMylN930xoD%2FiQ%3D&reserved=0>.\n", "This link still has the v6.1 typo: https://www.tensorflow.org/install/install_windows#requirements_to_run_tensorflow_with_gpu_support", "@michaelosthege Thanks for noticing this.\r\n\r\nThis is odd, since the underlying doc sources were updated on 11/28/2017:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_windows.md\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/install/install_sources.md\r\n\r\n@dr4b Do we need to do anything special to ensure the new docs make it out to the tensorflow.org site?", "No, it will propagate to root on the next version cut.  You can already see that it has made it out to master:\r\nhttps://www.tensorflow.org/versions/master/install/install_windows#requirements_to_run_tensorflow_with_gpu_support", "I had an offline discussion with @dr4b and @av8ramit - thanks for your help!\r\n\r\nHere's what happened.  By default (without choosing an explicit version), the tensorflow.org site shows the docs for the most recent released version.  Right now that's v1.4, which was released in early November.  The doc fixes were merged on 11/28, so they didn't make it into v1.4, and thus the default docs are still broken.\r\n\r\ncddf8d82dec9fff526f5c064add725b7f35f95fa was just merged, which patched the doc fixes into v1.4.  Now we just need to regenerate the docs and the site will be fixed.\r\n\r\nI'll leave this open until the site is really fixed."]}, {"number": 14804, "title": "Branch 176676125", "body": "", "comments": ["cc @kirilg "]}, {"number": 14803, "title": "Fixes to windows builds.", "body": "-Disable failing data_utils_test in cmake and bazel builds.\r\n-Disable session_partial_run_test in bazel build. It is already not\r\nrunning under cmake build.\r\n-Increase cmake build log verbosity, as we still canot see the root\r\ncause of failures.", "comments": ["Jenkins, test this please."]}, {"number": 14802, "title": "Setup.py on CentOS7 pywrap_tensorflow_internal error", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux CentOS7\r\n- **TensorFlow installed from (source or binary)**:\r\nSetup.py\r\n- **TensorFlow version (use command below)**:\r\n1.3\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n4.8.5\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\nimport tensorflow as tf\r\n\r\nYou can collect some of this information using our environment capture script:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\ntf_env_collect.sh output:\r\n\r\n== cat /etc/issue ===============================================\r\nLinux rs-control1 3.10.0-327.36.1.el7.x86_64 #1 SMP Sun Sep 18 13:04:29 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7 (Core)\"\r\nVERSION_ID=\"7\"\r\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux rs-control1 3.10.0-327.36.1.el7.x86_64 #1 SMP Sun Sep 18 13:04:29 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n=========================================================\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nInstalling using setup.py works until trying to import tensorflow, causing an Import Error for pywrap_tensorflow_internal\r\n\r\nDependencies I believe are met and using setup.py because I'm building this into an rpm to deploy to different nodes and clusters.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nSimply running setup.py and then import tensorflow as tf produces Import Error:\r\nTraceback (most recent call last):\r\n  File \"/tmp/check_tf.py\", line 1, in <module>\r\n    import tensorflow as tf;\r\n  File \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/python-tgt-201701/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\nImportError: No module named pywrap_tensorflow_internal", "comments": []}, {"number": 14801, "title": "On the way to latest CMake, VS2017, CUDA 9, cudNN 7, Win10", "body": "As many of us (#14126,#14691,#12052), I am trying to get TF1.4 build successfully on windows using the latest version of everything. As far as I can judge **I could do it** but with some hacks. As it is too long for me to complete, I would like to share what I did for help finalizing. It is too early for a PR.\r\n\r\nI am using CMake 3.9.6 (though 3.10 came out). I have low cmake skill level.\r\nI am not trying the python bindings.\r\nVS2017 is the community edition.\r\n\r\nWithout GPU it is easy. The only issue is the heap overflow (C1002 or C1006 #11096). The trick is to reduce parallel build by `msbuild /m:4 /p:CL_MPCount=2 ...` such that 4*2 is approximately the number of core you really have (at least it worked for me). Using `/Zm2000` did not work for me, despite a lot of available memory (32G).\r\n\r\nWith GPU it is more tricky: the `tf_core_gpu_kernels.vcxproj` does not compile at all. AFAIU, the CMake strategy changed from v3.6, to allow parallel computing. CUDA is now treated as another language. Without modifications nvcc simply returns with code error 1 (or nothing happen I am not sure). Here are my modifications (from v1.4).\r\n\r\nFrom `tensorflow/tensorflow/contrib/cmake/`\r\n1/ adapt `cmakelists.txt` a little: \r\n- Change `CUDA 8.0` to `CUDA 9.0` l.223.\r\n- Add `enable_language(\"CUDA\")` l.224.\r\n- **The `set(CUDA_NVCC_FLAGS ...)` directives do not work anymore**. See below.\r\n- Add capabilities 6.0 and 6.1 in l.232, as well l.246. Might not be needed (it is only for performance).\r\n- Change `64_80` to `64_90` and `64_6` to `64_7` l.247 and 248, similarly in l.272-276.\r\n\r\n2/ in `tf_core_kernels.cmake`:\r\n- Add `set_source_files_properties(${tf_core_gpu_kernels_srcs} PROPERTIES LANGUAGE CUDA)` to recognize '.cu.cc' extensions as cuda files in l.209.\r\n- Rename `cuda_add_library(...)` as `add_library(...)` l.210.\r\n\r\n3/ edit **(this is the trick)** `tf_core_gpu_kernels.vcxproj`, in the release section:\r\n- Encompass cl.exe flags, ie `/bigobj /nologo ... -Ob2`  with the `-Xcompiler=\"/bigobj ... -Ob2\"` directive l.147. These former flags are for the c++ compiler not for nvcc and result in the crash.\r\n- Add just before `--expt-relaxed-constexpr`, still in the `AdditionalOptions`.\r\n- Switch `PerformDeviceLink`from `false` to `true` l.164.\r\n\r\nThen everything compile (msbuild on  tf_tutorials_example_trainer.vcxproj) (and this tuto works). The remaining point before PR is to avoid third step, i.e. give the right directives to nvcc, by understanding how the CUDA_NVCC_FLAGS works, and add the linking. Hope this solution will work without missing symbols (#6396).\r\n\r\nOtherwise it is a nightmare: both CUDA 8 and CMake 3.6 are not aware of VS2017. CMake compilation is not incremental (#14194) and takes about 4-5H (could use precompiled headers especially in tf_core_kernels)...\r\n", "comments": ["Very nice. Thanks.  The tensorflow team should just release a official TF1.4 CUDA9 win10 build whl file for its users.  I don't get why they don't do so immediately.", "Since I've discovered that despite everything ran fine, the VS2017 (until 15.4) distribution introduced a bug in the /WHOLEARCHIVE trick resulting in unfilled factories (session, device...) [as mentioned in](https://developercommunity.visualstudio.com/content/problem/132766/wholearchive-linker-flag-is-dysfunctional-since-15.html). Therefore I am stuck using vs2017 for linking my application with TF.", "@sylvain-bougnoux I don't believe the WHOLEARCHIVE flag is used when building a Python whl, am I mistaken? ", "I don\u2019t think so either, it is a matter of linking your app with the TF libraries. Making a whl should work, while making an app is prone to fail.\nI have summited this bug to MS but no answer yet.", "Thanks for the notes @sylvain-bougnoux!\r\n\r\nAdding @mrry @gunan @tfboyd since they might be interested in your notes on getting things working.\r\n\r\nAs the referenced bugs mention, support for CUDA 9 / cuDNN 7 is anticipated in TensorFlow 1.5.\r\n\r\nMarking this as \"community support\" since the purpose of this issue seems to be to collect useful tips in making this all work.", "On windows, it looks like we have a bug with NVCC. Building TF with CUDA9 seems to be failing with a compiler crash. NVIDIA is helping investigate this, and once we have an update we will proceed.\r\n\r\nI will mark this as a duplicate issue of #12052 and #14691, if you dont mind.\r\n", "@gunan \r\nWhere is the crash you mention? \r\nActually I could compile everything just by changing the parameters given to nvcc. As I could not run it properly (despite the example trainer) due to the /WHOLEARCHIVE bug, is my success an illusion?\r\nAFAICJ it is just a matter of fixing the cmake file (or bazel).", "http://ci.tensorflow.org/job/tf-pr-win-cmake-gpu/19/console\r\n\r\n```\r\n21:31:54        \"c:\\tf_jenkins\\home\\workspace\\tf-pr-win-cmake-gpu\\cmake_build\\tf_python_build_pip_package.vcxproj\" (default target) (1) ->\r\n21:31:54        \"C:\\tf_jenkins\\home\\workspace\\tf-pr-win-cmake-gpu\\cmake_build\\pywrap_tensorflow_internal.vcxproj\" (default target) (4) ->\r\n21:31:54        \"C:\\tf_jenkins\\home\\workspace\\tf-pr-win-cmake-gpu\\cmake_build\\tf_core_gpu_kernels.vcxproj\" (default target) (38) ->\r\n21:31:54        (CustomBuild target) -> \r\n21:31:54          CUSTOMBUILD : Internal error : assertion failed at: \"C:/dvs/p4/build/sw/rel/gpu_drv/r384/r384_00/drivers/compiler/edg/EDG_4.12/src/lookup.c\", line 2652 [C:\\tf_jenkins\\home\\workspace\\tf-pr-win-cmake-gpu\\cmake_build\\tf_core_gpu_kernels.vcxproj]\r\n```\r\n\r\nThe location of the assertion failure seems to point to a line in nvidia proprietary code."]}, {"number": 14800, "title": "Potential memory leak from deleting array and closing file handler", "body": "Here are couple of minor memory leak for review.\r\n1. https://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/c/c_api.cc#L569-L593 \"delete []base;\" looks missing.\r\n   \r\n\r\n2. https://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/core/lib/io/snappy/snappy_outputbuffer.cc#L164-L173  \"delete []compressed_length_array;\" looks missing when macro TF_RETURN_IF_ERROR() fails.\r\n\r\n3. https://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/core/platform/profile_utils/android_armv7a_cpu_utils_helper.cc#L113-L123 Two potential problems:\r\n    a. There is no \"fclose()\" being called after fscanf() fails\r\n    b. \"fclose()\" could be called instead of \"pclose()\"\r\n\r\n4. https://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/tools/proto_text/gen_proto_text_functions.cc#L132-L137 When \"fwrite() fails\", \"fclose()\" could be called before \"return -1\".\r\n\r\nPS: I don't have handy working environment setup yet, currently browsing code may be better fit for me.", "comments": ["Could you edit your post and wrap code in three backticks (Markdown code highlighting), please?\r\n\r\n```cpp\r\nint main() {\r\n  // This is easier to read.\r\n}\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/1595907/33139045-6ca77b38-cfac-11e7-8a29-233e0918a72d.png)", "Or better yet, paste links to the relevant lines and GitHub will insert code snippets for you:\r\nhttps://github.com/tensorflow/tensorflow/blob/6c95675492aa8d25619f5e4ce1674582c051a7fe/tensorflow/c/c_api.cc#L580-L583", "@carlthome, thanks for the tip. Now I updated the description.", "@orpillar I think those issues are true.  In `snappy_outputbuffer.cc`, ~`unique_ptr` probably could be used.~ Update: Actually there are only 4 bytes so it could be placed into the stack instead.\r\n\r\nWould you like to create a PR for that? Otherwise I could help create a PR for you.", "@yongtang, thanks for looking into the issues. You are right about snappy_outputbuffer.cc.\r\nI am new to open source community, just want to see there is any easy things I could contribute.\r\nPlease feel free to help create a PR. Thanks,", "@orpillar Created PR #14816 for the fix. Thanks for your contribution to TensorFlow community! \ud83d\udc4d  ", "@yongtang. Thanks for the PR. It looks the sanity build had time out.", "@orpillar bumped the build. Thanks!"]}, {"number": 14799, "title": "TFLite: Add unsupported op Equal and ExpandDims", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!"]}, {"number": 14797, "title": "XLA AOT tfcompile failure due to undeclared inclusions in cc_binary rule", "body": "This happens on a freshly cloned TensorFlow master with Bazel 0.7 on Ubuntu 17.04:\r\n\r\n```sh\r\nERROR: tensorflow/BUILD:13:1: undeclared inclusion(s) in rule '//:model':\r\nthis rule is missing dependency declarations for the following files included by 'graph.cc':\r\n  'tensorflow/compiler/tf2xla/xla_compiled_cpu_function.h'\r\n  'tensorflow/compiler/tf2xla/xla_local_runtime_context.h'\r\n  'tensorflow/core/platform/macros.h'\r\n  '/tensorflow/core/platform/types.h'\r\n  '/tensorflow/core/platform/platform.h'\r\n  '/tensorflow/core/platform/default/integral_types.h'\r\n  '/tensorflow/compiler/xla/executable_run_options.h'\r\n```\r\ngraph.cc pretty much just does `#include \"graph.h\"` as per the tfcompile tutorial and it's weird because these headers [seem to be included](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/tfcompile.bzl#L209-L230) in the tf_library rule but Bazel still complains that the subsequent cc_binary rule doesn't list them as dependencies.\r\n\r\nThis is my BUILD file, placed in the repo root (so I use TensorFlow's Bazel workspace after going through ./configure):\r\n```sh\r\nload(\"@org_tensorflow//tensorflow/compiler/aot:tfcompile.bzl\", \"tf_library\")\r\n\r\ntf_library(\r\n  name = \"graph\",\r\n  cpp_class = \"Graph\",\r\n  graph = \"graph.pb\",\r\n  config = \"graph.config.pb\",\r\n)\r\n\r\ncc_binary(\r\n  name = \"model\",\r\n  srcs = [\"graph.cc\"],\r\n  deps = [\":graph\", \"//third_party/eigen3\"],\r\n  linkopts = [\"-lpthread\"]\r\n)\r\n```\r\nI'm not comfortable with Bazel yet but building worked fine with earlier TensorFlow versions. Stuff started to become wonky somewhere around when @org_tensorflow was introduced throughout tfcompile.bzl, I think.", "comments": ["Solved by not running `bazel build :model` but rather `bazel build @org_tensorflow//:model`, though I don't get why. :confounded: ", "@carlthome is it because the name of the workspace is defined as org_tensorflow in the workspace file located in the tensorflow root folder.\r\n\r\nThis may be helpful: \r\nhttps://docs.bazel.build/versions/master/external.html", "@carlthome were you able to configure any projects outside of tensorflow repo to use the tfcompile.bzl? I was not successful due to other dependencies such as tensorrt"]}, {"number": 14796, "title": "Error Too many value to unpack during export_savedmodel in tensorflow", "body": "TensorFlow 1.4.0\r\n\r\n sendingcurrency = tf.feature_column.categorical_column_with_vocabulary_list('sendincurrency', vocabulary_list=['AUD', 'EUR','GBP','USD'])\r\n    recievercurrency = tf.feature_column.categorical_column_with_vocabulary_list('recievercurrency', vocabulary_list=['AUD', 'EUR','GBP','INR','NZD','USD','XCD','XOF'])\r\n    CBRate = tf.feature_column.numeric_column(\"CBRate\",dtype=tf.float32)\r\nlinear_features = [sendingcurrency,recievercurrency,CBRate]\r\nregressor = tf.contrib.learn.LinearRegressor(feature_columns=linear_features,config=tf.contrib.learn.RunConfig(model_dir=\"/tmp/akhil\"))\r\n       \r\nfeature_spec = tf.feature_column.make_parse_example_spec(linear_features)\r\nexport_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)\r\nmodel.export_savedmodel(\"/tmp/akhil/\",serving_input_fn=export_input_fn)", "comments": []}, {"number": 14795, "title": "TypeError: __call__() got an unexpected keyword argument 'input_c'", "body": "I am using `tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops.CudnnGRU` as cudnn_cell.\r\nBut when I call cudnn_cell as follows\r\n`  \t    hiddens, output_h, output_c = cudnn_cell(\r\n   \t        inputs,\r\n   \t        input_h=init_state,\r\n   \t        input_c=init_state,\r\n   \t        params=cudnn_params,\r\n \t        is_training=True)\r\n`, an error occurs saying that input_c was an unexpected keyword.\r\nBut I have checked the source code and I'am certain that there is a keyword argument 'input_c'.", "comments": ["Could you paste your environment information? ", "https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnGRU, it seems that you have given wrong set of arguments."]}, {"number": 14794, "title": "Remove `non-fused` version of `adjust_saturation` as GPU kernel already exists", "body": "In the existing implementation for `adjust_saturation` the non-fused version was still in place. As the non-fused is for non-GPU support of `adjust_saturation` and GPU kernel already exists now (See commit 25c4f27#diff-b53c223158b7c4fd248ef581da6566c2), it makes sense to remove the non-fused version.\r\n\r\nIn addition, with the removal of non-fused implementation of `adjust_saturation`, now it is possible to provide batch support (in 4-D instead of previous 3-D). This resolves issue raised in #8926.\r\n\r\nThis fix removed non-fused version of `adjust_saturation` and added additional test cases for batch support.\r\n\r\nNote: In PR #14187, non-fused version of `adjust_hue` has been removed so batch support for `adjust_hue` has been enabled as well. This PR also adds additional test cases for batch support of `adjust_hue`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Jenkins, test this please.", "The Jenkins build `Linux XLA` failed. Though this same tests seem to have been failing for other PRs as well, I tend to think this is unrelated?", "@tensorflow-jenkins test this please"]}, {"number": 14793, "title": "[feature request] custom GraphKeys QUEUE_RUNNERS for input pipeline", "body": "i find no perfect answer about using input pipeline to train and eval in same Session\r\n===========\r\n[switch input pipeline at stackoverflow](eg: https://stackoverflow.com/questions/41162955/tensorflow-queues-switching-between-train-and-validation-data)\r\n===========\r\nif we define different input pipeline for train and eval,  the following code will start both train and eval input pipeline, that is not we want\r\n```\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n```\r\nif we can custom GraphKeys.QUEUE_RUNNERS  collection for different input pipeline , i think we can start input pipeline through parameter of collection.\r\n`    tf.train.add_queue_runner(qr, collection=tf.GraphKeys.QUEUE_RUNNERS)`\r\n`   eg: tf.train.add_queue_runner(qr, collection=tf.GraphKeys.TRAIN_QUEUE_RUNNERS)`\r\n`   eg: tf.train.add_queue_runner(qr, collection=tf.GraphKeys.EVAL_QUEUE_RUNNERS)`\r\nis it right ?  \r\n@mrry  @all \r\nthanks ", "comments": ["@automateljw The Dataset API is the recommended way to combine train and eval input pipelines in the same Session.  Read about it here:\r\nhttps://www.tensorflow.org/programmers_guide/datasets\r\n\r\nIn particular, there is an example of exactly this usage here (search for `reinitializable iterator`):\r\nhttps://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator\r\n\r\nDoes this resolve your issue?", "@tatatodd  thank your reply firstly.  As i know, Dataset API will replace queue input pipeline,  but it does not support sparse tensor now, is it right ?  or can you show demo about SparseTensor using Dataset API, big thanks! ", "If you upgrade to a nightly build, you will find the `tf.data` now supports sparse tensors natively, thanks to @jsimsa's hard work!", "nice work\uff0c i will try it\uff0c "]}, {"number": 14792, "title": "modified convolution document", "body": "fix #14027.\r\n\r\nDocument of _MaskedConv and MaskedConv2D could be revised too. Reasons are as follows.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5fbda9d8da7b98f62e83a392f047adf307b48b02/tensorflow/contrib/model_pruning/python/layers/core_layers.py#L164-L171\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/5fbda9d8da7b98f62e83a392f047adf307b48b02/tensorflow/contrib/model_pruning/python/layers/core_layers.py#L438-L447", "comments": ["Can one of the admins verify this patch?", "I just changed docstring of functions. But Unit tests couldn't be passed. Some error logs are as follows. Additionally, I merged lastest 'upstream/master' into my branch. What should I do next? Thank anyone who helps me. \r\n\r\n```\r\n-----------------------------------------error logs-------------------------------------------------\r\n/tmpfs/tmp/bazel/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/tensorflow/examples/adding_an_op/cuda_op_test.runfiles/org_tensorflow/tensorflow/tools/ci_build/gpu_build/parallel_gpu_execute: line 40: 30163 Segmentation fault      (core dumped) $@\r\n```"]}]