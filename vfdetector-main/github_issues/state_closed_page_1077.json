[{"number": 20964, "title": "Using tf.contrib.training.batch_sequences_with_states with tf.data.Dataset", "body": "As far as I know, tf.contrib.training.SequenceQueuingStateSaver relies on queues and queue-runners, so we'll still need to call tf.train.start_queue_runners() when used with the new tf.data.Dataset API.\r\n\r\nHowever, when I use tf.data.Iterator.from_structure to construct the input that is reusable with many different datasets, error occurs unless I initialize the Iterator before calling tf.train.start_queue_runners(). So I need to switch to a certain dataset first, and it's a quite strange usage.\r\n\r\n```python\r\niterator = Iterator.from_structure(tf.int64, tf.TensorShape([]))\r\n\r\ndataset_range = Dataset.range(10)\r\nrange_initializer = iterator.make_initializer(dataset_range)\r\n\r\ndataset_evens = dataset_range.filter(lambda x: x % 2 == 0)\r\nevens_initializer = iterator.make_initializer(dataset_evens)\r\n\r\n# Define a model based on the iterator; in this example, the model_fn\r\n# is expected to take scalar tf.int64 Tensors as input (see\r\n# the definition of 'iterator' above).\r\nprediction, loss = model_fn(iterator.get_next())\r\n\r\n# I need to pick a certain dataset, and run the initializer first.\r\nsess.run(range_initializer)\r\ntf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n# Train for `num_epochs`, where for each epoch, we first iterate over\r\n# dataset_range, and then iterate over dataset_evens.\r\nfor _ in range(num_epochs):\r\n  # Initialize the iterator to `dataset_range`\r\n  sess.run(range_initializer)\r\n  while True:\r\n    try:\r\n      pred, loss_val = sess.run([prediction, loss])\r\n    except tf.errors.OutOfRangeError:\r\n      break\r\n\r\n  # Initialize the iterator to `dataset_evens`\r\n  sess.run(evens_initializer)\r\n  while True:\r\n    try:\r\n      pred, loss_val = sess.run([prediction, loss])\r\n    except tf.errors.OutOfRangeError:\r\n      break\r\n```\r\n\r\nBy doing so, it seems that the queue will be closed when I switch to another dataset next time.\r\n\r\nSo will tensorflow add batch_sequences_with_states like support to tf.data.Dataset without relying on the old queue mechanisms and calling tf.train.start_queue_runners explicitly.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "My understanding is that datasets are supposed to replace queue runners, not work in conjunction with them. Why are you not using `dataset.make_one_shot_iterator().get_next()` to get an iterator tensor?", "Because I want to switch between training and validation set during training without having to build two iterators for both, so I don't have to reuse my model for different inputs.\r\n\r\nI found that the queue runner is stopped once the iterator encounters an OutOfRangeError exception.", "@robieta I have the same question. Since the tf.data.Dataset API is expected to replace the queue runners, is there a functionality similar to batch_sequences_with_states in the Dataset API to support truncated back propagation through time? Because, clearly the batch_sequences_with_states method needs queue runners to work. ", "We see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!"]}, {"number": 20963, "title": "find a bug for tensorflow\\cc\\framework\\gradients.cc", "body": "size_t dx_index = 0;\r\n    for (const Edge* e : n->in_edges()) {\r\n      if (e->IsControlEdge()) continue;\r\n      if (dx_index == dx.size()) {\r\n        return errors::Internal(\r\n            \"Invalid gradient output index: \", dx_index, \" size: \", dx.size());\r\n      }\r\n      TF_RETURN_IF_ERROR(\r\n          BackpropAlongEdge(dx[dx_index++], {e->src(), e->src_output()}));\r\n    }\r\n\r\nabove code,  when   dx have different  in_edges() order. Edge is unsortd set, dx is vector", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 20962, "title": "Miss-leading explanation of 'export_tflite_ssd_graph.py' ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: PC\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: newest from master branch\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.14\r\n- **GCC/Compiler version (if compiling from source)**: gcc-7.1\r\n- **CUDA/cuDNN version**: cuda-9.2,cudnn-7.1\r\n- **GPU model and memory**: 16G\r\n- **Exact command to reproduce**:\r\n\r\nThe function explanation of **export_tflite_ssd_graph.py** said \r\n`\r\nOutputs:\r\nIf add_postprocessing_op is true: frozen graph adds a\r\n  TFLite_Detection_PostProcess custom op node has four outputs:\r\n  detection_boxes: a float32 tensor of shape [1, num_boxes, 4] with box\r\n  locations\r\n  detection_scores: a float32 tensor of shape [1, num_boxes]\r\n  with class scores\r\n  detection_classes: a float32 tensor of shape [1, num_boxes]\r\n  with class indices\r\n  num_boxes: a float32 tensor of size 1 containing the number of detected boxes\r\n`\r\n\r\nBut according to my test the output sequence is  **boxes, indice, scores ,num** not **boxes,scores,indice,num**. (in `detection_postprocess.cc` seq is also **boxes, indice, scores ,num** )\r\nAnd in **export_tflite_ssd_graph_lib.py** \r\nthe `def get_const_center_size_encoded_anchors(anchors)` 's return shape is obviously \r\n**[num_anchors,4]** not **[4,num_anchors]**.", "comments": ["Thanks for correction on the comment", "https://github.com/tensorflow/models/pull/4851\r\nThanks. This should merge soon."]}, {"number": 20961, "title": "can't use mnist data after download using tensoerflow.It's give below error", "body": "**I'll use os as fedora\r\nInstall python via Conda-navigator\r\nEditor use pycharm**\r\n\r\nOs Platform:fedora\r\nTensorflow installed from anaconda\r\nTensorflow version:1.8.0\r\npython version : 3.6.5\r\nBazel version:Not installed\r\n\r\nWARNING:tensorflow:From /home/sunil/PycharmProjects/test/testFile.py:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use tf.one_hot on tensors.\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "**I'll use os as fedora\r\nInstall python via Conda-navigator\r\nEditor use pycharm**\r\n\r\n**Os Platform:fedora\r\nTensorflow installed from anaconda\r\nTensorflow version:1.8.0\r\npython version : 3.6.5\r\nBazel version:Not installed**\r\n\r\n**command i have used:\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)**\r\n\r\nWARNING:tensorflow:From /home/sunil/PycharmProjects/test/testFile.py:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use tf.one_hot on tensors.\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /home/sunil/anaconda3/envs/condaEnvTest/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n\r\n\r\n**I'll Updated Please check**", "Help me above problem", "No, can't found solution\n\nOn Fri, Jul 20, 2018, 20:47 JeffersonZeng <notifications@github.com> wrote:\n\n> I just occurred a same question like yours, and my version of tensorflow\n> is 1.8 as well.Have you found the resolution?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20961#issuecomment-406631504>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AZAqAdos-V3PrJSQEoKYyz3LGYONKTBlks5uIfSegaJpZM4VV-H2>\n> .\n>\n", "Which status @tensorflowbutler ", "Nagging Assignee @jart: It has been 104 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tensorflowbutler  \r\nSuppress warnings as below\r\n\r\n```\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nold_v = tf.logging.get_verbosity()\r\ntf.logging.set_verbosity(tf.logging.ERROR)\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\ntrain_data = mnist.train.images  # Returns np.array\r\ntrain_labels = np.asarray(mnist.train.labels, dtype=np.int32)\r\neval_data = mnist.test.images  # Returns np.array\r\neval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\r\n\r\ntf.logging.set_verbosity(old_v)\r\n```", "@zziz Thanks for sharing the workaround for tensorflowbutler. I think tensorflowbutler was updated after initial comment by @Sunil1997. I am closing this due to lack of recent activity regarding the issue. Please open a new issue if the error repeats.\r\n\r\nPlease use more stable approach for mnist data\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()"]}, {"number": 20960, "title": "Placeholder will cause incompelet shape bug in tf.profiler.profile", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.6.5 (default, Apr  1 2018, 05:46:30)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Please run the code in Source code / logs section.\r\n\r\n\r\n### Describe the problem\r\nThe [documentation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md#profile-model-float-operations) tells us that\r\n> It is suggested to pass in -run_meta_path if shape is only known during runtime. tfprof can fill in the missing shape with the runtime shape information from RunMetadata. \r\n\r\nIt cannot work when the graphs contain `placeholder`.  \r\nThe reason is that the node name of placeholder in RunMetadata is different from its name in graph_def.  \r\nThe bug will be reproduced in the next section.\r\n\r\n### Source code / logs\r\nThe code reproduce the bug  \r\n```python   \r\n# The code reproduce the bug\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nX = tf.placeholder(tf.float32, shape=(None, 3), name='X')\r\ny = tf.constant([[1, 2, 3, 4],\r\n                 [1, 2, 3, 4],\r\n                 [1, 2, 3, 4]], dtype=tf.float32)\r\nmul_op = tf.matmul(X, y)\r\n\r\nif __name__ == \"__main__\":\r\n    x = np.random.random([2, 3])\r\n\r\n    tf.profiler.profile(\r\n        tf.get_default_graph(),\r\n        cmd='op',\r\n        options=tf.profiler.ProfileOptionBuilder.float_operation())\r\n\r\n    run_metadata = tf.RunMetadata()\r\n    with tf.Session() as sess:\r\n        print(sess.run(mul_op, feed_dict={X: x},\r\n                       options=tf.RunOptions(\r\n                           trace_level=tf.RunOptions.FULL_TRACE),\r\n                       run_metadata=run_metadata))\r\n\r\n    tf.profiler.profile(\r\n        tf.get_default_graph(),\r\n        cmd='op', run_meta=run_metadata,\r\n        options=tf.profiler.ProfileOptionBuilder.float_operation())\r\n\r\n```\r\nThe output is  \r\n```\r\n1 ops no flops stats due to incomplete shapes.\r\nParsing Inputs...\r\nIncomplete shape.\r\nIncomplete shape.\r\nIncomplete shape.\r\n\r\n=========================Options=============================\r\n-max_depth                  10000\r\n-min_bytes                  0\r\n-min_peak_bytes             0\r\n-min_residual_bytes         0\r\n-min_output_bytes           0\r\n-min_micros                 0\r\n-min_accelerator_micros     0\r\n-min_cpu_micros             0\r\n-min_params                 0\r\n-min_float_ops              1\r\n-min_occurrence             0\r\n-step                       -1\r\n-order_by                   float_ops\r\n-account_type_regexes       .*\r\n-start_name_regexes         .*\r\n-trim_name_regexes          \r\n-show_name_regexes          .*\r\n-hide_name_regexes          \r\n-account_displayed_op_only  true\r\n-select                     float_ops\r\n-output                     stdout:\r\n\r\n==================Model Analysis Report======================\r\n\r\nDoc:\r\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\r\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\r\n\r\nProfile:\r\nnode name | # float_ops\r\n\r\n======================End of Report==========================\r\nIncomplete shape.\r\nIncomplete shape.\r\nIncomplete shape.\r\n2018-07-19 03:34:30.726274: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n[[1.4957699 2.9915397 4.48731   5.9830794]\r\n [2.083395  4.16679   6.2501845 8.33358  ]]\r\nParsing Inputs...\r\nIncomplete shape.\r\nIncomplete shape.\r\nIncomplete shape.\r\n\r\n=========================Options=============================\r\n-max_depth                  10000\r\n-min_bytes                  0\r\n-min_peak_bytes             0\r\n-min_residual_bytes         0\r\n-min_output_bytes           0\r\n-min_micros                 0\r\n-min_accelerator_micros     0\r\n-min_cpu_micros             0\r\n-min_params                 0\r\n-min_float_ops              1\r\n-min_occurrence             0\r\n-step                       -1\r\n-order_by                   float_ops\r\n-account_type_regexes       .*\r\n-start_name_regexes         .*\r\n-trim_name_regexes          \r\n-show_name_regexes          .*\r\n-hide_name_regexes          \r\n-account_displayed_op_only  true\r\n-select                     float_ops\r\n-output                     stdout:\r\n\r\n==================Model Analysis Report======================\r\nIncomplete shape.\r\nIncomplete shape.\r\nIncomplete shape.\r\n\r\nDoc:\r\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\r\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\r\n\r\nProfile:\r\nnode name | # float_ops\r\n\r\n======================End of Report==========================\r\n```\r\nWhen I first call the tf.profiler.profile, I do not pass run_metadata to it. Therefore, here should be the `incomplete shape`. However, it does not make sense that it still reports the `Incomplete shape` even if I pass the run_matadata to call the tf.profiler.profile again. \r\n\r\nI think the problem is that the node name of placeholder in RunMetadata is different with its name in graph_def.  I traced the code until [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/tfprof_logger.py#L46), and printed the 'run_meta' and 'graph_def'.  \r\nThe output is  \r\n```pdb  \r\n(Pdb) p run_meta\r\nstep_stats {\r\n  dev_stats {\r\n    device: \"/job:localhost/replica:0/task:0/device:CPU:0\"\r\n    node_stats {\r\n      node_name: \"_SOURCE\"\r\n      all_start_micros: 1531987298848369\r\n      op_start_rel_micros: 2\r\n      op_end_rel_micros: 3\r\n      all_end_rel_micros: 9\r\n      memory {\r\n        allocator_name: \"cpu\"\r\n      }\r\n      timeline_label: \"_SOURCE = NoOp()\"\r\n      scheduled_micros: 1531987298848358\r\n      memory_stats {\r\n      }\r\n    }\r\n    node_stats {\r\n      node_name: \"_arg_X_0_0\"\r\n      all_start_micros: 1531987298848383\r\n      op_end_rel_micros: 2\r\n      all_end_rel_micros: 7\r\n      memory {\r\n        allocator_name: \"cpu\"\r\n      }\r\n      output {\r\n        tensor_description {\r\n          dtype: DT_FLOAT\r\n          shape {\r\n            dim {\r\n              size: 2\r\n            }\r\n            dim {\r\n              size: 3\r\n            }\r\n          }\r\n          allocation_description {\r\n            requested_bytes: 24\r\n            allocator_name: \"cpu\"\r\n          }\r\n        }\r\n      }\r\n      timeline_label: \"_arg_X_0_0 = _Arg()\"\r\n      scheduled_micros: 1531987298848378\r\n      memory_stats {\r\n      }\r\n    }\r\n    node_stats {\r\n      node_name: \"Const\"\r\n      all_start_micros: 1531987298848391\r\n      op_end_rel_micros: 3\r\n      all_end_rel_micros: 4\r\n      memory {\r\n        allocator_name: \"cpu\"\r\n      }\r\n      output {\r\n        tensor_description {\r\n          dtype: DT_FLOAT\r\n          shape {\r\n            dim {\r\n              size: 3\r\n            }\r\n            dim {\r\n              size: 4\r\n            }\r\n          }\r\n          allocation_description {\r\n            requested_bytes: 48\r\n            allocator_name: \"cpu\"\r\n            ptr: 140275567738880\r\n          }\r\n        }\r\n      }\r\n      timeline_label: \"Const = Const()\"\r\n      scheduled_micros: 1531987298848390\r\n      memory_stats {\r\n        persistent_memory_size: 48\r\n      }\r\n    }\r\n    node_stats {\r\n      node_name: \"MatMul\"\r\n      all_start_micros: 1531987298848396\r\n      op_end_rel_micros: 69\r\n      all_end_rel_micros: 72\r\n      memory {\r\n        allocator_name: \"cpu\"\r\n        total_bytes: 32\r\n        peak_bytes: 32\r\n        live_bytes: 32\r\n        allocation_records {\r\n          alloc_micros: 1531987298848437\r\n          alloc_bytes: 32\r\n        }\r\n      }\r\n      output {\r\n        tensor_description {\r\n          dtype: DT_FLOAT\r\n          shape {\r\n            dim {\r\n              size: 2\r\n            }\r\n            dim {\r\n              size: 4\r\n            }\r\n          }\r\n          allocation_description {\r\n            requested_bytes: 32\r\n            allocated_bytes: 32\r\n            allocator_name: \"cpu\"\r\n            allocation_id: 1\r\n            has_single_reference: true\r\n            ptr: 140275513085952\r\n          }\r\n        }\r\n      }\r\n      timeline_label: \"MatMul = MatMul(_arg_X_0_0, Const)\"\r\n      scheduled_micros: 1531987298848395\r\n      memory_stats {\r\n      }\r\n    }\r\n    node_stats {\r\n      node_name: \"_retval_MatMul_0_0\"\r\n      all_start_micros: 1531987298848470\r\n      op_start_rel_micros: 1\r\n      op_end_rel_micros: 1\r\n      all_end_rel_micros: 3\r\n      memory {\r\n        allocator_name: \"cpu\"\r\n      }\r\n      timeline_label: \"_retval_MatMul_0_0 = _Retval(MatMul)\"\r\n      scheduled_micros: 1531987298848468\r\n      memory_stats {\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n```pdb    \r\n(Pdb) p graph.as_graph_def()\r\nnode {\r\n  name: \"X\"\r\n  op: \"Placeholder\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shape\"\r\n    value {\r\n      shape {\r\n        dim {\r\n          size: -1\r\n        }\r\n        dim {\r\n          size: 3\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"Const\"\r\n  op: \"Const\"\r\n  attr {\r\n    key: \"dtype\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"value\"\r\n    value {\r\n      tensor {\r\n        dtype: DT_FLOAT\r\n        tensor_shape {\r\n          dim {\r\n            size: 3\r\n          }\r\n          dim {\r\n            size: 4\r\n          }\r\n        }\r\n        tensor_content: \"\\000\\000\\200?\\000\\000\\000@\\000\\000@@\\000\\000\\200@\\000\\000\\200?\\000\\000\\000@\\000\\000@@\\000\\000\\200@\\000\\000\\200?\\000\\000\\000@\\000\\000@@\\000\\000\\200@\"\r\n      }\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"MatMul\"\r\n  op: \"MatMul\"\r\n  input: \"X\"\r\n  input: \"Const\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"transpose_a\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"transpose_b\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n}\r\nversions {\r\n  producer: 26\r\n}\r\n\r\n```\r\n\r\nWe can find the node name of placeholder is different in the two output, in the run_meatdata, the node name is `_arg_X_0_0`, however, it is `X` in the graph_def. \r\nAs a result, code [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/profiler/tfprof_logger.py#L46) will not work properly.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "I have updated it. Thanks.", "Unfortunately I don't believe anyone is maintaining tfprof currently. Please submit a PR if you're able. You may wanna look at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/timeline.py#L346, but it basically isn't documented. I found this [tutorial](http://www.riptutorial.com/tensorflow/example/13324/basic-example-with-tensorflow-s-timeline-object) though.", "@zhangyaobit are you maintaining the profiler? \r\n\r\nIf you are not, I don't know who is, and I think we should delete it.\r\n\r\n@pbar @zheng-xq @ewilderj @petermattson FYI", "Please discuss with @zheng-xq regarding who will be maintaining and if this should be deleted.", "Any progress? I met the same issue when calculating flops. Do you solve the problem now? @caffett \r\nThanks.", "> Any progress? I met the same issue when calculating flops. Do you solve the problem now? @caffett\r\n> Thanks.\r\n\r\nTry tf.data API.", "@caffett Thank you for your suggestion. But I have no idea how to use it to calculate flops. Could you be more specific? Thank you very much~", "> @caffett Thank you for your suggestion. But I have no idea how to use it to calculate flops. Could you be more specific? Thank you very much~\r\n\r\nTutorial for tf.data: https://docs.google.com/presentation/d/16kHNtQslt-yuJ3w8GIx-eEH6t_AvFeQOchqGRFpAD7U/edit#slide=id.p\r\n\r\nUse this to replace the placeholder.\r\n", "> > @caffett Thank you for your suggestion. But I have no idea how to use it to calculate flops. Could you be more specific? Thank you very much~\r\n> \r\n> Tutorial for tf.data: https://docs.google.com/presentation/d/16kHNtQslt-yuJ3w8GIx-eEH6t_AvFeQOchqGRFpAD7U/edit#slide=id.p\r\n> \r\n> Use this to replace the placeholder.\r\n\r\nGreat! I'll have a try. Thanks", "[incompleteshapeerror.txt](https://github.com/tensorflow/tensorflow/files/2828311/incompleteshapeerror.txt)\r\ni am getting an error \"144 ops no flops stats due to incomplete shapes.\" while export the inference graph i used the sentex tutorial https://www.youtube.com/watch?v=srPndLNMMpk&list=PLQVvvaa0QuDcNK5GeCQnxYnSSaar2tpku&t=1s&index=7\r\nbut i did in different way instead of using RGB images while annotating the features i used a binary images[0-255] the features a looks like (train.csv) please refer attached screenshot of the sample image\r\nImage size=>(640X300)\r\nfilename | width | height | class        | xmin | ymin | xmax | ymax\r\ntest1.jpg | 640 |      290 |  minutiae | 274 | 46 |      277 |     49\r\n\r\nNote: features are marked manually zoomed in and marked \r\n\r\ni trained almost 14k steps loss is about ~1.5% \r\ni used the model=> ssd_mobilenet_v1_pets.config [please find attachment]\r\n[ssdconfig.txt](https://github.com/tensorflow/tensorflow/files/2828455/ssdconfig.txt)\r\n\r\n`i modified the code in object_detection_tutorial.ipynb in models/research/object-detection\r\nwhile running the code marked objects are not detected but simply showing the input image as a output\r\ncould anyone please provide the solution for this problem ? \r\n\r\n\r\n", "I couldn't find an easy way to get the number of operations using Timeline. It was reporting each operation in milliseconds.  \r\n\r\nThis, however invasive, worked: [link](https://medium.com/@fanzongshaoxing/model-flops-measurement-in-tensorflow-a84084bbb3b5)", "> I couldn't find an easy way to get the number of operations using Timeline. It was reporting each operation in milliseconds.\r\n> \r\n> This, however invasive, worked: [link](https://medium.com/@fanzongshaoxing/model-flops-measurement-in-tensorflow-a84084bbb3b5)\r\n\r\nDoes this link work? I was trying, but failed.", "Since we no longer support Tensorflow 1.x issues and placeholder has been depreciated in Tensorflow 2, could you please upgrade your code to the latest Tensorflow version.Thank You.!"]}, {"number": 20959, "title": "Build tensorflow-model-server for gpu - Cannot find cuda library libcudnn.so.6", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ip-172-30-1-83 4.4.0-1062-aws #71-Ubuntu SMP Fri Jun 15 10:07:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**:\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-9.0/doc/man/man7/libcudart.7\r\n\r\nBut I also have these files:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so -> /etc/alternatives/libcudnn_so\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6 -> libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.6.0.21\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7 -> libcudnn.so.7.0.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.7.0.5\r\n\r\n- **GPU model and memory**: Tesla K80\r\n\r\n### Describe the problem\r\n\r\nI try to build tensorflow-model-server with gpu support, as I saw that the apt-get version is only for CPU.\r\n\r\nI did:\r\n1. git clone --recurse-submodules https://github.com/tensorflow/serving\r\n2. bazel clean --expunge && export TF_NEED_CUDA=1\r\n3. bazel query 'kind(rule, @local_config_cuda//...)'\r\n\r\nAnd got:\r\n\r\nCuda Configuration Error: Cannot find cuda library libcudnn.so.6\r\n\r\nWhen I do: bazel build -c opt --config=cuda tensorflow_serving/model_servers:tensorflow_model_server\r\nI get the same error.\r\n", "comments": ["Hi. Try to set the env variable to `LD_LIBRARY_PATH`:\r\n\r\n```\r\nexport D_LIBRARY_PATH=\"/usr/local/cuda-9.0/\"\r\n\r\n```\r\nHope it helps :) ", "You meant `export D_LIBRARY_PATH=\"/usr/local/cuda-9.0/\"`\r\n\r\nNope... Still same error message:\r\n\r\n`Cuda Configuration Error: Cannot find cuda library libcudnn.so.6`\r\n\r\nThe file libcudnn.so.6 is found on my server at `/usr/lib/x86_64-linux-gnu/libcudnn.so.6`\r\nBut it can't find it... Is it found on the right place on my computer?", "Sorry  I meant,\r\n```\r\nexport LD_LIBRARY_PATH=\"/usr/local/cuda-9.0/\"\r\n```\r\nI am not an expert I was just trying to help since I had a similar issue on ubuntu 18.04 with `tensorflow-gpu`. So, did you have the `/usr/local/cuda-9.0/` folder? Just in case, try to echo your environment variables and see if there is LD_LIBRARY_PATH. \r\n\r\nI hope maybe it will help", "This should be filed with the [tensorflow/serving repo](https://github.com/tensorflow/serving/issues). So I'll close this issue.\r\n\r\nNot sure why Tensorflow Serving needs cudnn 6. In TensorFlow, currently by default we use cudnn 7.", "Thanks. I will file it in the other repo."]}, {"number": 20958, "title": "[XLA] Disable tests which do a windowed reduction on a scalar sized window", "body": "This change allows some tests in the reduction test suite to be disabled using a manifest file.\r\n\r\n", "comments": ["@martinwicke can you please merge this, I'm unfamiliar with the GH workflow (used by Tensorflow) and want to make sure this PR doesn't get dropped, thanks!", "Nagging Assignee @rmlarsen: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20957, "title": "Update keras.md", "body": "tf.keras.models should load model configuration with `model_from_json` or `model_from_yaml`, not `from_json` or `from_yaml`", "comments": ["@cheerss Thanks!"]}, {"number": 20956, "title": "tensorflow.contrib.autograph generated the code to update variables with conditions but throws UnboundLocalError", "body": "### Environment\r\n\r\npython3 -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\nv1.9.0-rc2-724-gc0dbd7e456 1.10.0-dev20180718\r\n\r\n== cat /etc/issue ===============================================\r\nLinux localhost 4.15.13-x86_64-linode106 #1 SMP Tue Mar 27 14:42:14 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.4 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux localhost 4.15.13-x86_64-linode106 #1 SMP Tue Mar 27 14:42:14 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy             1.14.5\r\nprotobuf          3.6.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.6.0\r\ntf.GIT_VERSION = v1.6.0-0-gd2e24b6039\r\ntf.COMPILER_VERSION = v1.6.0-0-gd2e24b6039\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Describe the problem\r\n\r\nWe use `tensorflow.contrib.autograph` and try to update the variable with conditional logic. The code is simple and should work with Python.\r\n\r\n``` \r\na = \"test\"\r\nif x == 0:\r\n  a = \"test1\"\r\nelse:\r\n  a = \"test2\"\r\n```\r\n\r\nBut it throws `UnboundLocalError` and unable to access the existing variables.\r\n\r\n### Source code / logs\r\n\r\nHere is the example code to reproduce the issue.\r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import autograph as ag\r\n\r\ndef f(x):\r\n  a = \"test\"\r\n  if x == 0:\r\n    a = \"test1\"\r\n  else:\r\n    a = \"test2\"\r\n\r\nconverted_f = ag.to_graph(f)\r\n\r\nprint(ag.to_code(f))\r\nprint(converted_f(tf.constant(-1)))\r\n```\r\n\r\nAnd it is the generated code of the above code.\r\n\r\n```\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\ndef tf__f(x):\r\n  try:\r\n    with tf.name_scope('f'):\r\n      a = 'test'\r\n\r\n      def if_true():\r\n        with tf.name_scope('if_true'):\r\n          a, = a,\r\n          a = 'test1'\r\n          return 1,\r\n\r\n      def if_false():\r\n        with tf.name_scope('if_false'):\r\n          a_1, = a,\r\n          a_1 = 'test2'\r\n          return 1,\r\n      with ag__.utils.control_dependency_on_returns(ag__.utils.run_cond(tf.\r\n          equal(x, 0), if_true, if_false)):\r\n        x_1, if_true_1, if_false_1 = ag__.utils.alias_tensors(x, if_true,\r\n            if_false)\r\n  except:\r\n    ag__.rewrite_graph_construction_error(ag_source_map__)\r\n```\r\n\r\nIt is the full error log when try to run generated graph with test data.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/tmp/tmpn2n1i2l2.py\", line 20, in tf__f\r\n    equal(x, 0), if_true, if_false)):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/utils/multiple_dispatch.py\", line 50, in run_cond\r\n    return control_flow_ops.cond(condition, true_fn, false_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2048, in cond\r\n    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1895, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/tmp/tmpn2n1i2l2.py\", line 10, in if_true\r\n    a, = a,\r\nUnboundLocalError: local variable 'a' referenced before assignment\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./demo2.py\", line 16, in <module>\r\n    print(converted_f(tf.constant(-1)))\r\n  File \"/tmp/tmpn2n1i2l2.py\", line 24, in tf__f\r\n    ag__.rewrite_graph_construction_error(ag_source_map__)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/core/errors.py\", line 158, in rewrite_graph_construction_error\r\n    raise new_error\r\ntensorflow.contrib.autograph.core.errors.GraphConstructionError: Traceback (most recent call last):\r\n  File \"./demo2.py\", line 8, in f\r\n    if x == 0:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/autograph/utils/multiple_dispatch.py\", line 50, in run_cond\r\n    return control_flow_ops.cond(condition, true_fn, false_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2048, in cond\r\n    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1895, in BuildCondBranch\r\n    original_result = fn()\r\n  File \"/tmp/tmpn2n1i2l2.py\", line 10, in if_true\r\n    a, = a,\r\n\r\nlocal variable 'a' referenced before assignment\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Hi, looks like a bug in the static analysis - it gets confused because `a` is not used after the conditional.\r\n\r\nWe'll look at the bug, but in the mean time, returning `a` from the function should clear the error:\r\n\r\n```\r\ndef f(x):\r\n  a = \"test\"\r\n  if x == 0:\r\n    a = \"test1\"\r\n  else:\r\n    a = \"test2\"\r\n  return a\r\n```\r\n\r\n", "I am new to this kind of environment of open source project / product\ndevelopment can suggest me few guide lines regarding how to learn and\nunderstand about the flow of project.\nAny help is appreciated...\n\nThanks regards\nSidhartha\n\nOn 05-Aug-2018 12:16 AM, \"Alfred Sorten Wolf\" <notifications@github.com>\nwrote:\n\n> Nagging Assignee @mdanatg <https://github.com/mdanatg>: It has been 14\n> days with no activity and this issue has an assignee. Please update the\n> label and/or status accordingly.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/20956#issuecomment-410469504>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYd6CcXiA5BLLVAoUFreKkJbGKRtCEa-ks5uNevzgaJpZM4VV17n>\n> .\n>\n", "Hi Sid, please have a look at the [general TF contrib guidelines](https://www.tensorflow.org/community/contributing for contribution) and the [AutoGraph supplement](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/autograph/CONTRIBUTING.md) to get you started. We're planning to include additional developer documentation, but that's not yet ready.", "Sorry for the delayed response. We were combing through our old bugs and noticed that this one should already be fixed. Please reopen this bug if you are still seeing issues."]}, {"number": 20955, "title": " the quantized form of Shape operation is not yet implemented", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:N/A\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.9.0\r\n- **Python version**:2.7.3\r\n- **Bazel version (if compiling from source)**:0.12.0\r\n- **GCC/Compiler version (if compiling from source)**:c++11\r\n- **CUDA/cuDNN version**:7.5.18\r\n- **GPU model and memory**:TITAN,12GB\r\n- **Exact command to reproduce**:\r\n ./bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=~/deeplabv3_mobinetv2/frozen_inference_graph.pb   --output_file=~/deeplabv3_mobinetv2/foo.cc   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=QUANTIZED_UINT8   --input_shape=1,513,513,3   --input_array=ImageTensor   --output_array=logits/semantic/BiasAdd   --default_ranges_min=0   --default_ranges_max=6   --mean_value=127.5   --std_value=127.5\r\n\r\n### Describe the problem\r\nI want to use dummy quantization to quantize deeplabv3_mobilenetv2 model \"mobilenetv2_coco_voc_trainaug\" from https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md.\r\nBut I got the shape operation is not yet implemented.\r\nDo you have plan to implement it?\r\n\r\n### Source code / logs\r\n2018-07-19 13:49:26.114180: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:459] Unimplemented: this graph contains an operator of type Shape for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nAborted (core dumped)\r\n", "comments": ["Adding @suharshs to comment on this.", "@raninbowlalala We have noted your request and will look in to the quantized implementation of shape op. We will update you on this.", "@achowdhery Thanks for your great work!", "Shape should now support quantization, but I believe there may be other ops in this model that require additional work (Cast, in particular).\r\n\r\n@raninbowlalala would you mind trying again?", "@jdduke Shape has supportted quantization, thank you! And I got error log as below:\r\n`2018-08-10 09:08:16.577780: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:473] Unimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).`\r\n\r\nDo you have plan to implement Cast op for the quantized form? \r\n", "@jdduke Hi, I download the new code to convert deeplabv3_mnv2 model, and I got error message as blow:\r\n`Array MobilenetV2/Conv/Relu6, which is an input to the DepthwiseConv operator producing the output array MobilenetV2/expanded_conv/depthwise/Relu6, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.`\r\nDose this mean Relu6 is not suport quantization?", "I'll let @suharshs comment further about how to proceed with quantization (both for the Relu6 issue and the Cast issue).", "Regarding cast, why does the model have a Cast operation?\r\nMake sure you are converting a eval graph and not a train graph which can sometimes have spurious unsupported operations.\r\n\r\nAre you trying to quantize the model using the tool here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantize\r\n\r\nyou should train with that which should place quantization ops in the graph to collect info. That second error you get means that the graph is not getting the correct quantization operations in the graph.\r\n\r\nHow are you making your fake quantized frozen graph to provide to tflite conversion?", "@suharshs \r\nFor cast op, I want to use dummy quantization for deeplabv3+ (mobilenetv2) model named \"mobilenetv2_coco_voc_trainaug\" which download  from https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md. I use tensorboard to see the graph and there are \"cast\" op.\r\n\r\nFor training a quantization model. I am sorry I forgot to add \"tf.contrib.quantize.create_eval_graph()\", convert model successfully after I add this command. Thank you very much!\r\n\r\n", "Yes, after inspecting the graph, it seems there is a Cast a the start of the model. It takes in uint8 and casts to float before ResizeBilinear. This doesn't make sense for a quantized model since all edges in a fully quantized model are uint8 and this cast should really be ignored. So I guess the correct solution is to either somehow remove the Cast from the graph, or the tflite converter should remove it in the case of a quantized model.\r\n\r\n<img width=\"653\" alt=\"screen shot 2018-08-19 at 7 38 57 pm\" src=\"https://user-images.githubusercontent.com/1450614/44317917-6fe7c300-a3e8-11e8-9a63-ec8cca0b9635.png\">\r\n\r\nWill think more about the right way to address this, thanks!", "@suharshs Hi, could you add \"sub\" and \"mul\" op with quantization supported? I found after I add command \"tf.contrib.quantize.create_eval_graph()\" there are not fake node with \"sub\" and \"mul\".", "@raninbowlalala Agree with you ! I also found some basic ops are not supported with quantization, such as \"Add\" , \"Mul\", \"Sub\", \"Mean\" etc.", "The issue is that the contrib/quantize rewriter is not very robust to any arbitrary model yet. \r\n\r\nIt can be complicated and sometimes not possible to fully quantize certain models due to the available fused operations at any given time , if you goal is to just get a smaller and faster model, I recommend trying the --post_training_quantize flag to tflite_convert. With that you keep the inference_type=FLOAT and pass a floating point version of your model (no need to call the contrib/quantize tool). That may provide sufficient speedup for your use case. Check it out here: https://www.tensorflow.org/performance/post_training_quantization\r\n\r\nWe do plan on making better tooling for fully quantized models as well, but as a first pass --post_training_quantize will get you the furthest, and if speed/accuracy aren't sufficient, we should add FakeQuant nodes into the graph with the contrib/quantize tool and sometimes manually for patterns that aren't recognized.", "> @raninbowlalala Agree with you ! I also found some basic ops are not supported with quantization, such as \"Add\" , \"Mul\", \"Sub\", \"Mean\" etc.\r\n\r\nsquare, sqrt and squared_difference are also not supported.", "@raninbowlalala,\r\nSorry for the delayed response. As per the documentation of [Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization), \r\n\r\n> You can quantize an already-trained float TensorFlow model when you convert it to TensorFlow Lite format using the [TensorFlow Lite Converter](https://www.tensorflow.org/lite/convert/).\r\n\r\nMany operations like **`Addition`**, **`Multiply`**, **`Divide`**, **`Square Root`**, etc.. are supported now, as part of **`Quantization`**. Please refer [this documentation](https://www.tensorflow.org/lite/guide/ops_compatibility#straight-forward_conversions_constant-folding_and_fusing) for the list of supported **`OPs`**.\r\n\r\n ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 20954, "title": "Entry point not found error reported when calling tf.contrib.rnn.BasicRNNCell", "body": "Python version = 3.5.0\r\n\r\n- Have I written custom code --> No\r\n- OS Platform and Distribution  --> Windows 10\r\n- TensorFlow installed from --> pip install --ignore-installed --upgrade tensorflow-gpu \r\n- TensorFlow version --> 1.8.0-gpu\r\n- Bazel version --> NA\r\n- CUDA/cuDNN version --> 9.0\r\n- GPU model and memory --> GeForce GTX 960M, 3GB\r\n- Exact command to reproduce --> See below\r\n\r\nX0 = tf.placeholder(tf.float32, [None, n_inputs])\r\nX1 = tf.placeholder(tf.float32, [None, n_inputs])\r\nbasic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)  # Errs out here\r\n\r\nError message:\r\n\r\n\"The procedure entry point\r\n?AddCleanup@Arenalmpl@internal@protobuf@google@@QEAAXXEPEAXP6AX0@Z@Z could not be located in the dynamic link library C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\data\\_dataset_ops.so\"\r\n\r\n![image](https://user-images.githubusercontent.com/8305769/42923931-d0e91e26-8adc-11e8-802c-29ff0a065301.png)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Requested info added to original post.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "OK. But did you observe the same phenomenon? How did you come to the conclusion that it is not an issue with the file _dataset_ops.so?", "I came across this same issue today, can anyone suggest what's the workaround @solarbear123 @bignamehyp. Thanks."]}, {"number": 20953, "title": "Reset the unique_ptr instead of releasing it which cause memory leak", "body": "", "comments": []}, {"number": 20952, "title": "How can tensorflow compiled with MKL rathan MKL-DNN", "body": "\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.9.0\r\n- **Python version**: python 3.4\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nIs it possible to compile tensorflow with Eigen using MKL as backend, but not using MKL-DNN's optimized kernels? And how? As mentioned here https://github.com/intel/mkl-dnn/issues/282 , I think in my case (conv2d on sandybridge), the MKL-DNN integration seems harm to performance.\r\n\r\nThanks very much.\r\n\r\n### Source code / logs\r\n\r\n", "comments": ["It turns out that for my machine (sandybridge), tensorflow v1.9.0 is not built with MKL BLAS support. This seems fixed by https://github.com/tensorflow/tensorflow/commit/64117da0c36f0697467ce6d56a7be6837da24d2f .\r\nI tested the latest version, there is no more this problem.\r\nThank you!"]}, {"number": 20951, "title": "[CMAKE] Fix cmake build error related to #20669", "body": "In this PR, we fixed cmake build error when generating python pip package.\r\n- Add a patch python script for protobuf to generate necessary pb files, prevent importing error in `create_python_api.py`\r\n\r\n- For MKL, we copy MKL shared binaries into python package, to make things easy to use. Also this could remove the MKL `IF-ELSE` in `tf_python.cmake` because we don't need to assign an external path for mkl any more.", "comments": ["I have one comment, but adding @mrry and @annarev for a more thorough review.", "Nagging Reviewer @gunan, @mrry, @gunan, @annarev: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "After the merge of PR #21201, there are some conflicts in generating `tf_python_api` and `estimator_python_api`, I'm resolving this.", "Since r1.10 and later version will remove cmake support, close this PR"]}, {"number": 20950, "title": "C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source (r1.9 and r1.10)\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**: 3.7.0\r\n- **Bazel version (if compiling from source)**: 0.15.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc-7\r\n- **CUDA/cuDNN version**: 9.2/7.1.4\r\n- **GPU model and memory**: GTX 1060 3GB\r\n- **Exact command to reproduce**: `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\nUnable to build TensorFlow from source.\r\n\r\n##### Output of `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n```\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nLoading: \r\nLoading: 0 packages loaded\r\nWARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/mukundan/machine_learning/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).\r\nINFO: Found 1 target...\r\n[0 / 1] [-----] BazelWorkspaceStatusAction stable-status.txt\r\nERROR: /home/mukundan/.cache/bazel/_bazel_mukundan/f5df5e8cbf5aa4e0efe27dc48f25989d/external/protobuf_archive/BUILD:665:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::New(PyTypeObject*, PyObject*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:139:46: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n   static char* kwlist[] = {\"descriptor_db\", 0};\r\n                                              ^\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindMessageByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:169:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindFileByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:190:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindFieldByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:206:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindExtensionByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:224:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindEnumTypeByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:241:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindOneofByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:258:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindServiceByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:275:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindMethodByName(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:292:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindFileContainingSymbol(google::protobuf::python::PyDescriptorPool*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:45: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n        ((*(charpp) = PyUnicode_AsUTF8AndSize(ob, (sizep))) == NULL? -1: 0): \\\r\n                      ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:309:7: note: in expansion of macro 'PyString_AsStringAndSize'\r\n   if (PyString_AsStringAndSize(arg, &name, &name_size) < 0) {\r\n       ^~~~~~~~~~~~~~~~~~~~~~~~\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-writable-strings'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1.309s, Critical Path: 1.08s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "comments": ["This breakage is caused by change in Python-3.7 C API used by protobuf, and thus protobuf build is broken. Protobuf addresses this issue in https://github.com/google/protobuf/issues/4086.", "so should i close this issue", "I don't think that this issue should be closed until new protobuf release and until tensorflow will start using that release.", "Not the same but similar here:\r\n```\r\nERROR: /home/francesco/.cache/bazel/_bazel_francesco/e14db5a97cb9042cd7bbcf71326995ef/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/message.cc: In function 'PyObject* google::protobuf::python::message_meta::New(PyTypeObject*, PyObject*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/message.cc:208:54: warning: ISO C++ forbids converting a string constant to 'char*' [-Wwrite-strings]\r\n   static char *kwlist[] = {\"name\", \"bases\", \"dict\", 0};\r\n```\r\n\r\nreverting to `python=3.6`", "Is there a work-around for this issue, short of going back to Python version 3.6?", "@seehuhn patch file and discussion in https://github.com/tensorflow/tensorflow/pull/21202"]}, {"number": 20949, "title": "How to add/print logs from TFLite JNI.cc (native code) files?", "body": "Hello, I'm trying to put prints in the [native code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc) files of TFLite files in the repository to get good understanding of TFlite and Android ecosystem. I have tried including <stdio.h> and printing with printf/fprintf/cout. Noting seems to be coming in the adb logcat.\r\n\r\nCan anyone help me how to get the prints from the native code?\r\n\r\nThanks", "comments": ["@madhu-sudhan : You need to use Android native logging macros: https://developer.android.com/ndk/reference/group/logging#__android_log_print", "Hi @shashishekhar , good to see quick response on the issue. \r\n\r\nYes, as you have mentioned I have just tried including the file <android/log.h> and I have put a sample print at [Java_org_tensorflow_lite_NativeInterpreterWrapper_run](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L349):\r\n\r\n__android_log_print(ANDROID_LOG_ERROR, \"MyLog\",\"=========================\");\r\n\r\nBut it seems it is not turning up in the logcat. I'm pretty sure that the control in going through this function (I have verified that by putting a dummy return instead of print statement). Can you please tell if I could have missed something? Or is it that no debug prints comes from this particular file?\r\n\r\nThanks !!", "@madhu-sudhan you probably want add `-llog` when building your jni .so (if you are building .so or say .aar with .so inside)", "Hi @freedomtan , I'm trying to build with bazel command like mentioned below.\r\n\r\nbazel build --cxxopt=--std=c++11 -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a tensorflow/contrib/lite/java:libtensorflowlite_jni.so\r\n\r\nI have specified -llog with an option (--linkopt=-llog) to bazel command like shown below, but still am not able to see any prints in logcat.\r\n\r\nbazel build --linkopt=-llog --cxxopt=--std=c++11 -c opt --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a tensorflow/contrib/lite/java:libtensorflowlite_jni.so\r\n\r\nCan you please suggest some workarounds?\r\n\r\nThanks", "@madhu-sudhan :  this should work and is well tested. You probably are missing a step or apk is not updated.", "Hi @shashishekhar , yeah I dont know what i have missed. But the above mentioned steps are not helping to get prints in android logcat. Can you please help me identify if something I have missed?\r\n\r\nThanks\r\nMadhu", "@madhu-sudhan : You can try __android_log_assert and see if you get a fatal crash, then at least your log is getting triggered.\r\n\r\nIf that works then try \"adb logcat -P ''\" and relaunch the app, if you still don't see the log, I will suggest first writing a simple Android app with JNI and using that to do Android logging, there are plenty of examples on the web. This will make you familiar with how JNI works on Android. Unfortunately can't help you anymore, without knowing how you are using the code."]}, {"number": 20948, "title": "How can I do type cast:  Tensor<Float>  to  Tensor<String> ? ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n\r\n(Also, if posting on StackOverflow, I'd advise adding much more detail to the question)"]}, {"number": 20947, "title": "tensorflow 1.8.0 may not be compatible with numpy 1.14.5", "body": "", "comments": []}, {"number": 20946, "title": "Update RELNOTE formatting.", "body": "", "comments": ["These are relnote organization changes suggested by martin", "Good eye! Thanks, should be fixed."]}, {"number": 20945, "title": "Fix examples_test.sh script which was broken by added debug_keras test.", "body": "", "comments": []}, {"number": 20944, "title": "Add SQLITE_OPEN_URI flag to sqlite open to fix create_db_writer failure to open the db", "body": "**Repro:** \r\n- Create a summary_ops.create_db_writer with the db_uri as a uri fails. \r\n- [API](https://www.tensorflow.org/api_docs/python/tf/contrib/summary/create_db_writer) Example: db_uri: For example \"file:/tmp/foo.sqlite\" \r\n\r\n`2018-07-17 11:53:26.760581: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at summary_kernels.cc:91 : Invalid argument: Sqlite::Open(file:/tmp/foo.sqlite) **failed: unable to open database file**`\r\n\r\n**Workaround:**\r\nOne workaround is to remove the uri, and just give the path for the db_uri parameter \r\n\r\n**Fix:**  \r\n- This PR adds the SQLITE_OPEN_URI flag to enable its URI filename capability.  \r\n- Reference: https://sqlite.org/uri.html\r\n\r\n**Testing:**\r\n - A new unit test has been added.", "comments": ["One of the macOS CI test failed because it could not import pathlib.  I have removed the dependency on it and instead used the six.moves.urllib_parse.urljoin to workaround that.  Please take a look.  Thanks. ", "@skambha Thanks!"]}, {"number": 20943, "title": "Update Eigen commit pointer to the commit that adds AMD GPU support in Eigen", "body": "Updating the eigen commit pointer to the commit / PR that add the AMD GPU support in the Eigen codebase.\r\n\r\nThe change in eigen renames some of the APIs from Cuda* to Gpu*.  This renaming requires corresponding updates in the Tensorflow code that uses those APIs. That is the reason for the changes in the Tensorflow code in this PR.\r\n\r\n\r\nThere is commit (in the eigen codebase) which is chronologically between the commit currently pointed to by TF and the commit that this PR updates to, which will cause a compiler failure for the TF build.  That commit is \r\nhttps://bitbucket.org/eigen/eigen/commits/4af74f577a4fc09dcfa202064e1291038d2046da?at=default\r\n\r\nThe eigen patch file is required to workaround the compile failure introduced by that commit (it does so by essentially turning OFF the AVX vectorization functionality introduced by that commit). It is assumed that this compile failure will be fixed by code updates either on the TF side or eigen side, after which the eigen patch file will again not be needed\r\n\r\n\r\nThere is already a bug  filled on Eigen for this compile failure \r\n\r\nhttp://eigen.tuxfamily.org/bz/show_bug.cgi?id=1569\r\n\r\nThe compile failure in question looks like\r\n\r\n```\r\n  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \\\r\n    PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -g0 '-march=haswell' -g0 -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/segment_reduction_ops/tensorflow/core/kernels/segment_reduction_ops.pic.d '-frandom-seed=bazel-out/\\\r\nhost/bin/tensorflow/core/kernels/_objs/segment_reduction_ops/tensorflow/core/kernels/segment_reduction_ops.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquot\\\r\ne . -iquote bazel-out/host/genfiles -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_\\\r\narchive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote external/jemalloc -iquote bazel-out/host/genfiles/external/jemallo\\\r\nc -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_g\\\r\nooglesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external\\\r\n/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_cuda\\\r\n -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem external/bazel_to\\\r\nols/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem external/jemalloc/include -isystem bazel-out/host/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/host/genfile\\\r\ns/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/png_archive -isys\\\r\ntem bazel-out/host/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem external/local_c\\\r\nonfig_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem external/d\\\r\nouble_conversion -isystem bazel-out/host/genfiles/external/double_conversion '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall \\\r\n-fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -c tensorflow\\\r\n/core/kernels/segment_reduction_ops.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/segment_reduction_ops/tensorflow/core/kernels/segment_reduction_ops.pic.o)^M\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:94:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from tensorflow/core/kernels/segment_reduction_ops.cc:24:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFunctors.h: In instantiation of 'Packet Eigen::internal::MeanReducer<T>::finalizePacket(const Packet&) const [with Packet = Eigen::internal::Packet8h; T = Eigen::half]':\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:654:44:   required from 'Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::PacketReturnType Eigen::TensorEvaluator<const Eigen::TensorReduction\\\r\nOp<Op, Dims, XprType, MakePointer_>, Device>::packet(Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::Index) const [with int LoadMode = 0; Op = Eigen::internal::MeanReducer<Eigen::half>; Dims = const Eigen::IndexList<E\\\r\nigen::type2index<0l> >; ArgType = const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 2, 1, long int>, 0, Eigen::MakePointer>; MakePointer_ = Eigen::MakePointer; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Op, Dims, XprTy\\\r\npe, MakePointer_>, Device>::PacketReturnType = Eigen::internal::Packet8h; Eigen::TensorEvaluator<const Eigen::TensorReductionOp<Op, Dims, XprType, MakePointer_>, Device>::Index = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h:142:5:   required from 'void Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::evalPacket(Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXp\\\r\nrType>, Device>::Index) [with LeftArgType = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 0, Eigen::MakePointer>; RightArgType = const Eigen::TensorReductionOp<Eigen::internal::MeanReducer<Eigen::half>, const Eigen::IndexList<Eigen::type2index<0l> >, \\\r\nconst Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 2, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer>; Device = Eigen::DefaultDevice; Eigen::TensorEvaluator<const Eigen::TensorAssignOp<LhsXprType, RhsXprType>, Device>::Index = long int]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:68:11:   required from 'static void Eigen::internal::TensorExecutor<Expression, Eigen::DefaultDevice, true>::run(const Expression&, const Eigen::DefaultDevice&) [with Expression = const Eigen\\\r\n::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 0, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::MeanReducer<Eigen::half>, const Eigen::IndexList<Eigen::type2index<0l> >, const Eigen::TensorMap<Eigen::Tensor<const\\\r\n Eigen::half, 2, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >]'\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorMap.h:310:65:   required from 'Eigen::TensorMap<PlainObjectType, Options_, MakePointer_>::Self& Eigen::TensorMap<PlainObjectType, Options_, MakePointer_>::operator=(const OtherDerived&) [with OtherDeriv\\\r\ned = Eigen::TensorReductionOp<Eigen::internal::MeanReducer<Eigen::half>, const Eigen::IndexList<Eigen::type2index<0l> >, const Eigen::TensorMap<Eigen::Tensor<const Eigen::half, 2, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer>; PlainObjectType = Eigen::Te\\\r\nnsor<Eigen::half, 1, 1, long int>; int Options_ = 0; MakePointer_ = Eigen::MakePointer; Eigen::TensorMap<PlainObjectType, Options_, MakePointer_>::Self = Eigen::TensorMap<Eigen::Tensor<Eigen::half, 1, 1, long int>, 0, Eigen::MakePointer>]'\r\ntensorflow/core/kernels/segment_reduction_ops.cc:186:19:   required from 'void tensorflow::SegmentReductionOp<Device, T, Index, Reducer, default_value>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice; T = Eigen::half; Index = long long \\\r\nint; Reducer = Eigen::internal::MeanReducer<Eigen::half>; int default_value = 0]'\r\ntensorflow/core/kernels/segment_reduction_ops.cc:1142:1:   required from here\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFunctors.h:174:38: error: no matching function for call to 'pset1(const DenseIndex&)'\r\n     return pdiv(vaccum, pset1<Packet>(packetCount_));\r\n                                      ^\r\nIn file included from external/eigen_archive/Eigen/Core:409:0,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from tensorflow/core/kernels/segment_reduction_ops.cc:23:\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:227:1: note: candidate: template<class Packet> Packet Eigen::internal::pset1(const typename Eigen::internal::unpacket_traits<Packet>::type&)\r\n pset1(const typename unpacket_traits<Packet>::type& a) { return a; }\r\n ^\r\nexternal/eigen_archive/Eigen/src/Core/GenericPacketMath.h:227:1: note:   template argument deduction/substitution failed:\r\nIn file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:94:0,\r\n                 from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from tensorflow/core/kernels/segment_reduction_ops.cc:24:\r\nexternal/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorFunctors.h:174:38: note:   cannot convert '((const Eigen::internal::MeanReducer<Eigen::half>*)this)->Eigen::internal::MeanReducer<Eigen::half>::packetCount_' (type 'const DenseIndex {aka const long int}\\\r\n') to type 'const type& {aka const Eigen::half&}'\r\n     return pdiv(vaccum, pset1<Packet>(packetCount_));\r\n\r\n```\r\n", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "closing this one ... and re-opening it as a new one to see if cla issues are resolved"]}, {"number": 20942, "title": "tf.contrib.factorization.KMeansClustering training on CPU and not GPU", "body": "I am currently running tensorflow-gpu 1.9 avx2 with cuda 9.2. When I train the model below, I notice that my CPU usage is 99% but my Tesla M60 usage is at 0%. \r\n\r\n\r\n\r\n    def input_fn():\r\n        return tf.train.limit_epochs(tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)\r\n\r\n    kmeans = tf.contrib.factorization.KMeansClustering(num_clusters=num_clusters, use_mini_batch=False)\r\n    num_iterations = 20\r\n\r\n    previous_centers = None\r\n    for _ in range(num_iterations):\r\n        kmeans.train(input_fn)\r\n        cluster_centers = kmeans.cluster_centers()\r\n        print('score:', kmeans.score(input_fn))\r\n\r\n    # map the input points to their clusters\r\n    cluster_indices = list(kmeans.predict_cluster_index(input_fn))\r\n\r\n`", "comments": ["From what I understand, there is nothing in your post to indicate that this is a tensorflow issue. Can you please move your question to stack overflow? \r\nMeanwhile, did you confirm that tf.test.gpu_device_name() actually prints the name of a GPU in use?", "Okay I moved it to [stack overflow](https://stackoverflow.com/questions/51411442/tensorflow-kmeansclustering-training-on-cpu-and-not-gpu)\r\n\r\nWhen I run tf.test.gpu_device_name() I get this\r\n\r\n`2018-07-18 15:28:47.105452: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1392] Found device 0 with properties: \r\nname: Tesla M60 major: 5 minor: 2 memoryClockRate(GHz): 1.1775\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 7.44GiB freeMemory: 7.12GiB\r\n2018-07-18 15:28:47.105846: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1471] Adding visible gpu devices: 0\r\n2018-07-18 15:28:47.852029: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-18 15:28:47.852268: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:958]      0 \r\n2018-07-18 15:28:47.852440: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971] 0:   N \r\n2018-07-18 15:28:47.852859: I c:\\users\\user\\source\\repos\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1084] Created TensorFlow device (/device:GPU:0 with 6874 MB memory) -> physical GPU (device: 0, name: Tesla M60, pci bus id: 0000:00:1e.0, compute capability: 5.2`\r\n\r\nI am posting this here because it doesn't seem like KMeansClustering supports GPU acceleration ", "Correct, it seems that the kernels are only implemented in CPU. They are using eigen, so it's possible that they also work in GPU, if you register them. Could you try that?", "@drpngx can you elaborate how to go about this? I am using Python 3.6", "So, the way it works is that the low-level operations are called kernels. The kernels are wrapped into ops. The ops define the interface, and the kernels define the implementation.\r\n\r\nWhat we have in [`clustering_ops`](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/kernels/clustering_ops.cc#L573) is that the kernels are registered for CPU only. You could try to register them for GPU, and then rebuild. Typically you also need to make other changes so that might some effort, particularly if you haven't worked with tensorflow internals before.\r\n\r\nIf you just want KMeans, it's really just a bunch of vector distance computations. You could just do that by hand using off-the-shelf tensorflow operators, all of which should already be implemented on GPU.", "@drpngx I was able to implement the KMeans training using tensorflow operators\r\n\r\n`class KmeansTensorflow:\r\n    def __init__(self, input_matrix, num_clusters):\r\n        self._input = input_matrix.todense()\r\n        self._num_clusters = num_clusters\r\n\r\n    def train(self):\r\n        k = self._num_clusters\r\n        # centroid initialization\r\n        start_pos = tf.Variable(self._input[np.random.randint(self._input.shape[0], size=k), :],\r\n                                dtype=tf.float32)\r\n        centroids = tf.Variable(start_pos.initialized_value(), 'S', dtype=tf.float32)\r\n\r\n        # populate points\r\n        points = tf.Variable(self._input, 'X', dtype=tf.float32)\r\n        ones_like = tf.ones((points.get_shape()[0], 1))\r\n        prev_assignments = tf.Variable(tf.zeros((points.get_shape()[0],), dtype=tf.int64))\r\n\r\n        # distance function\r\n        p1 = tf.matmul(\r\n            tf.expand_dims(tf.reduce_sum(tf.square(points), 1), 1),\r\n            tf.ones(shape=(1, k))\r\n        )\r\n        p2 = tf.transpose(tf.matmul(\r\n            tf.reshape(tf.reduce_sum(tf.square(centroids), 1), shape=[-1, 1]),\r\n            ones_like,\r\n            transpose_b=True\r\n        ))\r\n        distance = tf.sqrt(tf.add(p1, p2) - 2 * tf.matmul(points, centroids, transpose_b=True))\r\n\r\n        # assign each point to a closest centroid\r\n        point_to_centroid_assignment = tf.argmin(distance, axis=1)\r\n\r\n        # recalculate the centroid (mean)\r\n        total = tf.unsorted_segment_sum(points, point_to_centroid_assignment, k)\r\n        count = tf.unsorted_segment_sum(ones_like, point_to_centroid_assignment, k)\r\n        means = total / count\r\n\r\n        # continue if there is any delta\r\n        is_continue = tf.reduce_any(tf.not_equal(point_to_centroid_assignment, prev_assignments))\r\n\r\n        with tf.control_dependencies([is_continue]):\r\n            loop = tf.group(centroids.assign(means), prev_assignments.assign(point_to_centroid_assignment))\r\n\r\n        sess = tf.Session()\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        # 1000 iterations or no delta\r\n        has_changed, cnt = True, 0\r\n        while has_changed and cnt < 1000:\r\n            cnt += 1\r\n            has_changed, _ = sess.run([is_continue, loop])\r\n        # see how the data is assigned\r\n        res = sess.run(point_to_centroid_assignment)\r\n        print(list(res))\r\n`\r\n\r\nBut I have been having trouble with making a prediction method. That's why I thought about using tf.contrib.factorization.KmeansClustering ", "Nice! For the runtime, you just need the nearest centroid?", "@drpngx Yes given an input, I want to find which cluster it belongs in. I'm fairly new to tensorflow", "so if you compute the distance from your point to every centroid, then it should just be `tf.argmin`.", "Chiming in here:\r\n\r\nI've been using `KMeansClustering` with a single GPU, and I **do** see activity on the GPU\r\nwhen I look via `nvidia-smi`, so now after reading this thread I'm puzzled because I'm using\r\nthe standard pre-canned `KMeansClustering`.\r\n\r\nBut, that's not why I found this issue. I'm here because I'd like to distribute training of `KmeansClustering` via `tf.contrib.distribute.MirroredStrategy`.\r\n\r\nI've tried to set up distributed training with the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport multiprocessing\r\n\r\ndef parse_fn(record):\r\n    '''\r\n    features={\r\n        'feats': tf.FixedLenFeature([], tf.string)\r\n    }\r\n    parsed = tf.parse_single_example(record, features)\r\n    feats= tf.convert_to_tensor(tf.decode_raw(parsed['feats'], tf.float64))\r\n\r\n    return {'feats': feats}                                                                                                                                 \r\n\r\n\r\n\r\ndef my_input_fn(tfrecords_path, model):\r\n    dataset = (\r\n        tf.data.TFRecordDataset(tfrecords_path)\r\n        .apply(\r\n            tf.contrib.data.map_and_batch(\r\n                map_func=parse_fn,\r\n                batch_size=4096,\r\n                num_parallel_batches=multiprocessing.cpu_count()\r\n            )\r\n        )\r\n        .prefetch(4096)\r\n    )\r\n\r\n    return dataset\r\n\r\n\r\n### Multi-GPU training config ###                                                                                                                                                                           \r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nrun_config = tf.estimator.RunConfig(train_distribute=distribution)\r\n\r\ntrain_spec_kmeans = tf.estimator.TrainSpec(input_fn = lambda: my_input_fn('train.tfrecords'))\r\neval_spec_kmeans = tf.estimator.EvalSpec(input_fn = lambda: my_input_fn('eval.tfrecords') )\r\n\r\nKMeansEstimator = tf.contrib.factorization.KMeansClustering(\r\n    num_clusters=1024,\r\n    feature_columns = [tf.feature_column.numeric_column(\r\n        key='feats',\r\n        dtype=tf.float64,\r\n        shape=(806,),\r\n    )],          \r\n    model_dir = '/tmp/tf',\r\n    use_mini_batch = True,\r\n    config = run_config)\r\n\r\n\r\ntf.estimator.train_and_evaluate(KMeansEstimator, train_spec_kmeans, eval_spec_kmeans)\r\n```\r\n\r\n\r\nHowever, when I set up the training to use ` tf.contrib.distribute.MirroredStrategy`, it crashes\r\nwith the following Error:\r\n\r\n\r\n```\r\n2018-07-30 11:42:05.948549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-07-30 11:42:05.948629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-30 11:42:05.948646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 \r\n2018-07-30 11:42:05.948660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y \r\n2018-07-30 11:42:05.948669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y \r\n2018-07-30 11:42:05.948676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y \r\n2018-07-30 11:42:05.948687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N \r\n2018-07-30 11:42:05.949163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 14867 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\r\n2018-07-30 11:42:05.949304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 14867 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\r\n2018-07-30 11:42:05.949936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 14867 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\r\n2018-07-30 11:42:05.950061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:3 with 14867 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1567, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 2 for 'cond/cond_1/concat' (op: 'ConcatV2') with input shapes: [0], [?,806], [].\r\n```\r\n\r\nIn this case, TF finds all four of my GPUs, but the shape of the data is off. \r\n\r\nI run this exact same code without `MirroredStrategy` everything works fine (but I do have to add an `Iterator` object back in).\r\n\r\nThoughts?\r\n\r\n", "@yuefengz is that a bug in the `MirroredStrategy`?", "@yuefengz and @drpngx - here's what I'm seeing from `nvidia-smi`\r\nwhen I run `KMeansClustering` without the `MirroredStrategy`\r\n\r\nAll four GPUs have a Python process on them, but only one seems\r\nto perform any computation.\r\n\r\n![kmeans-gpu](https://user-images.githubusercontent.com/8389864/43524458-fc90b54c-9564-11e8-98af-d7fd83197933.png)\r\n\r\nIf I do run training with `MirroredStrategy`, then training crashes and\r\nI get the above mentioned Error message, namely:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1567, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 2 for 'cond/cond_1/concat' (op: 'ConcatV2') with input shapes: [0], [?,806], [].\r\n```", "This is a friendly ping for @yuefengz and @drpngx :)", "One more friendly ping for @yuefengz :smiley_cat: ", "One more friendly ping for @yuefengz :wave: \r\n", "Do you know where does this concat op come from? cc @agarwal-ashish ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 20941, "title": "R1.10 cherry-pick request: Fix TensorRT build/test problems.", "body": "", "comments": []}, {"number": 20940, "title": "Remove remove_undocumented from parallel_for.", "body": "This isn't needed in core TF code as we have the public API\r\ngeneration tool now. Also, this causes problem with the release\r\npip tests.", "comments": ["This is taking a while to make it into master branch. So submitting this change separately to 1.10 branch."]}, {"number": 20939, "title": "Improve error messages for gather_nd and scatter_nd", "body": "Use SliceDebugString to produce nice error messages using\r\nmultidimensional indexes.", "comments": ["@ebrevdo Want to review?", "@girving thanks!"]}, {"number": 20938, "title": "1.9 RC2 build from source fails cc1plus: warning: unrecognized command line option \"-Wno-writable-strings\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDebian 9\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nn/a\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.9 rc2\r\n- **Python version**:\r\n3.6\r\n\r\n- **Bazel version (if compiling from source)**:\r\n0.15.2\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc version 4.9.2 (Debian 4.9.2-10+deb8u1)\r\n\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n\r\n- **GPU model and memory**:\r\nn/a\r\n\r\n- **Exact command to reproduce**:\r\n```\r\ncd /usr/local/src \r\ngit clone https://github.com/tensorflow/tensorflow\r\ncd tensorflow\r\ncat /dev/null | ./configure\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\npip install /tmp/tensorflow_pkg/tensorflow*.whl\r\n\r\n```\r\n\r\n### Describe the problem\r\n\r\nBuilding Tensorflow from source fails with the error below. We're trying to update the docker images for over at Kaggle (https://github.com/kaggle/docker-python )\r\n\r\n### Source code / logs\r\n[tf.log](https://github.com/tensorflow/tensorflow/files/2206995/tf.log)\r\n\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option \"-Wno-writable-strings\"\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 295.663s, Critical Path: 79.71s\r\nINFO: 2072 processes: 2072 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["I'm having the same failure message as @crawforc3 \r\n\r\n\r\n## System information\r\n\r\n\r\n#### Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\n\r\n#### OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 16.04, 4.15.0-29-generic\r\n\r\n#### Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nn/a\r\n\r\n#### TensorFlow installed from (source or binary):\r\nSource\r\n\r\n#### TensorFlow version (use command below):\r\n`1.9.0`\r\n\r\n#### Python version:\r\n3.7\r\n\r\n#### Bazel version (if compiling from source):\r\n0.15.2\r\n\r\n#### GCC/Compiler version (if compiling from source):\r\n`gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.10)`\r\n\r\n#### CUDA/cuDNN version:\r\n9.1/7.1\r\n\r\n#### GPU model and memory:\r\nGeForce GTX 1070, 8g\r\n\r\n#### Exact command to reproduce:\r\n```bash\r\n`git clone https://github.com/tensorflow/tensorflow && cd tensorflow`\r\n`./configure`\r\n`bazel build  //tensorflow/tools/pip_package:build_pip_package`\r\n```\r\nAlso tried:\r\n`bazel build  --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n`bazel build  --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n`bazel build  --config=opt --config=cuda --config=mkl //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n\r\n## Describe the problem\r\nBuilding TF from source fails with the error below. Trying to update current TF build from 3.6.6/1.8 (GPU), to 3.7.0/1.9 (GPU). Never had issues with building TF before. Built many times.\r\n\r\n## Source code/logs\r\n\r\n```bash\r\nAt global scope:\r\ncc1plus: warning: unrecognized command line option '-Wno-writable-strings'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 157.161s, Critical Path: 31.02s\r\nINFO: 1198 processes: 1198 local.\r\n```\r\n", "Also getting this. An update?", "@SilverMight  are you trying to build from python 3.7? That ended up being the issue in my situation. Apparently TF doesn't support 3.7 yet, which is one reason this error occurs. I was able to build 1.10.0 from 3.6 without issue.", "@evdcush Yeah, just realized that. Hopefully it'll support 3.6 soon, #21202 may fix the issue but I haven't tried it yet", "What is pasted is just a warning. It did not cause your build to fail.\r\nCould you use pastebin to share the full log?", "@gunan I am experiencing the same issue. Here is my full log: https://pastebin.com/5T4C9841", "The actual compilation failure is right here:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/external/protobuf_archive/BUILD:659:1: C++ compilation of rule '@protobuf_archive//:python/google/protobuf/pyext/_message.so' failed (Exit 1)\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::New(PyTypeObject*, PyObject*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:139:46: warning: deprecated conversion from string constant to 'char*' [-Wwrite-strings]\r\n   static char* kwlist[] = {\"descriptor_db\", 0};\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc: In function 'PyObject* google::protobuf::python::cdescriptor_pool::FindMessageByName(PyObject*, PyObject*)':\r\nexternal/protobuf_archive/python/google/protobuf/pyext/descriptor_pool.cc:51:57: error: invalid conversion from 'const char*' to 'char*' [-fpermissive]\r\n```\r\n\r\nLooks to be a protobuf issue.", "`In file included from bazel-out/armeabi-opt/genfiles/external/local_config_python/python_include/Python.h:53:0,\r\n                 from external/protobuf_archive/python/google/protobuf/internal/api_implementation.cc:31:\r\nbazel-out/armeabi-opt/genfiles/external/local_config_python/python_include/pyport.h:686:2: error: #error \"LONG_BIT definition appears wrong for platform (bad gcc/glibc config?).\"\r\n #error \"LONG_BIT definition appears wrong for platform (bad gcc/glibc config?).\"\r\n  ^~~~~\r\ncc1plus: warning: unrecognized command line option '-Wno-writable-strings'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1056.799s, Critical Path: 77.71s\r\nINFO: 857 processes: 857 local.\r\nFAILED: Build did NOT complete successfully\r\n`", "Warnings *by definition* don't make anything fail (and this can just be solved with https://github.com/protocolbuffers/protobuf/pull/5516)\r\nBoth OP and the last post had probably something to do with some quirk in python instead. ", "Looks like from TF side only issue was compiler warnings, which are OK.\r\nAll other compilation issues seem to be protobuf compilation issues.\r\nTherefore, I will close this issue."]}, {"number": 20937, "title": "__main__.UserInputError: Invalid TF_NCCL setting was provided 10 times in a row. Assuming to be a scripting mistake.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Modified Dockerfile provided for TensorFlow devel GPU\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source (attempt)\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**: 0.15.2\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 7\r\n- **GPU model and memory**: \r\n- **Exact command to reproduce**: docker build -t tensorflow_source .\r\n\r\n### Describe the problem\r\nI am trying to build a docker image with TensorFlow installed from source. The docker image I am using is the Dockerfile.devel-gpu (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu) file, with an updated version of Bazel. There is an issue with the NCCL link in the script, resulting in the following error. \r\n\r\n### Source code / logs\r\n\r\n``` \r\nStep 19 : RUN ./configure\r\n ---> Running in 350827ccf62f\r\nExtracting Bazel installation...\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.2 installed.\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: jemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: Amazon AWS Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: No XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: No GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: No VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nPlease specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: No TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nPlease specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: \r\n\r\nPlease specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\nInvalid path to NCCL 2 toolkit, /usr/local/cuda-9.0/lib/libnccl.so.2 or /usr/local/cuda-9.0/include/nccl.h not found. Please use the O/S agnostic package of NCCL 2\r\nTraceback (most recent call last):\r\n  File \"./configure.py\", line 1559, in <module>\r\n    main()\r\n  File \"./configure.py\", line 1503, in main\r\n    set_tf_nccl_install_path(environ_cp)\r\n  File \"./configure.py\", line 1156, in set_tf_nccl_install_path\r\n    _DEFAULT_PROMPT_ASK_ATTEMPTS)\r\n__main__.UserInputError: Invalid TF_NCCL setting was provided 10 times in a row. Assuming to be a scripting mistake.\r\n```\r\n\r\nNot sure what the problem is, despite the soft links provided. ", "comments": ["/CC @tfboyd can you look into this?", "I have a few options for you and I am cover some other scenarios so I can point any other issues back to this one.  Please let me know how it goes so I can modify anything to be more helpful.  \r\n\r\nBelow are instructions on how to build with NCCL 2.x for a variety of situations.  If you are not using NCCL you can take a short-cut and specify NCCL 1.3 to ./configure and it will compile with open source NCCL 1.3 and put it in the binary just as before.  If you are using single-GPU there is no reason to get NCCL 2.x just say NCCL 1.3 and be done.  \r\n\r\n**Building with NCCL 2.x**\r\n\r\n1. Use the [nightly-devel-gpu](https://hub.docker.com/r/tensorflow/tensorflow/tags/) docker and then go back to using the latest nightly-devel-gpu when TF 1.10 is released.\r\n\r\n2.  Upgrade your current devel-gpu docker with NCCL 2.x. \r\n```bash\r\napt-get update && \\\r\n        apt-get install \\\r\n        libnccl2=2.2.13-1+cuda9.0 \\\r\n        libnccl-dev=2.2.13-1+cuda9.0 \\\r\n```\r\n\r\n3. If you are not using Docker you can install NCCL ([NVIDIA NCCL landing page](https://developer.nvidia.com/ncc)) by following one of NVIDIA's suggested methods.  The easiest for builds is to get `NCCL 2.x.x O/S agnoatic` and most likely the CUDA 9.0 edition.  Unzip it into a folder and provide that path to ./configure when asked for the path.  Note the path is to the parent directory which has lib and include inside.  \r\n\r\n**Installing just the NCCL 2.x binary:**\r\n\r\nIf you just need to upgrade a system running (not building) TensorFlow to have NCCL 2.x I suggest using the \"NVIDIA Network Installer approach\" as that installed the .deb package that you get to from the [NCCL landing page](https://developer.nvidia.com/nccl) and puts everything into the right path.  Or you can use the NVIDIA `apt-get repo` as follows\r\n\r\n```bash\r\nsudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\r\nwget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.deb\r\nwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb\r\nsudo dpkg -i nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb\r\nsudo apt-get update\r\n# Includes optional NCCL 2.x.\r\nsudo apt-get install libnccl2=2.2.13-1+cuda9.0 \r\n\r\n# or just install everything\r\nsudo apt-get install cuda9.0 cuda-cublas-9-0 cuda-cufft-9-0 cuda-curand-9-0 \\\r\n  cuda-cusolver-9-0 cuda-cusparse-9-0 libcudnn7=7.1.4.18-1+cuda9.0 \\\r\n   libnccl2=2.2.13-1+cuda9.0 cuda-command-line-tools-9-0\r\n```\r\n\r\np.s. I will attempt to edit this over time to be more clear and based on feedback.  The install guides will be updated with TF 1.10 RC0 release.  Leaving ticket open until original poster's issue is resolved.\r\n", "Nagging Assignee @tfboyd: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 20936, "title": "Adding MKL DNN license from Intel's repo.", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n<!-- cla_yes -->"]}, {"number": 20935, "title": "Cherry pick MKL license change into 1.10", "body": "", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}]