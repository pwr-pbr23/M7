[{"number": 17789, "title": "Could not find a version that satisfies the requirement numpy>=1.13.3 (from tensorflow) ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Automatically closing due to no acitivity."]}, {"number": 17788, "title": "contrib: minor spelling tweaks", "body": "packages:\r\n  model_pruning\r\n  rnn\r\n  solvers\r\n  tensorrt", "comments": []}, {"number": 17787, "title": "Manual Build libhexagon_controller.so Failed", "body": "Hi , I follow steps in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/hvx README.md to build manually tensorflow with Hexagon support .\r\n\r\nstep Calling \"make tree VERBOSE=1 V=android_Release\" in section \"_Build libhexagon_controller.so_\"\r\ncausing :\r\n\r\n```\r\n/home/Qualcomm/Hexagon_SDK/3.3.3/build/make.d/rules.min:362: START\r\n/home/Qualcomm/Hexagon_SDK/3.3.3/build/make.d/rules.min:363: DEPS=/home/dnozik/Qualcomm/Hexagon_SDK/3.3.3/libs/common/rpcmem/android_Release/ship/rpcmem.a adspmsgd.a /home/dnozik/Qualcomm/Hexagon_SDK/3.3.3/libs/common/remote/ship/android_Release/libadsprpc.so\r\n/home/Qualcomm/Hexagon_SDK/3.3.3/build/make.d/rules.min:364: END\r\n/home/Qualcomm/Hexagon_SDK/3.3.3/build/make.d/rules.min:581: QEXE_EXEC: \r\n/home/Qualcomm/Hexagon_SDK/3.3.3/build/make.d/rules.min:698: LD_INPUTS: -Wl,--start-group  -Wl,--end-group -L/home/Qualcomm/Hexagon_SDK/3.3.3/tools/android-ndk-r14b/platforms/android-21/arch-arm/usr/lib -lm -lstdc++ -lc  -lgcc\r\n/home/Qualcomm/Hexagon_SDK/3.3.3/build/make.d/rules.min:704: LD_INPUTS: -Wl,--start-group  -Wl,--end-group -L/home/dnozik/Qualcomm/Hexagon_SDK/3.3.3/tools/android-ndk-r14b/platforms/android-21/arch-arm/usr/lib -lm -lstdc++ -lc  -lgcc\r\n/home/Qualcomm/Hexagon_SDK/3.3.3/tools/android-ndk-r14b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -mthumb  -c -fpie -fpic -fPIE -fPIC -pie -Wall -Wno-missing-braces -mword-relocations -mthumb-interwork -march=armv7-a -Werror -O2 -fno-strict-aliasing -isystem /home/dnozik/Qualcomm/Hexagon_SDK/3.3.3/tools/android-ndk-r14b/platforms/android-21/arch-arm/usr/include -D_ANDROID_ -DANDROID -std=gnu99           -D__FILENAME__=\\\"hexagon_controller.c\\\"  -I../../../libs/common/adspmsgd/ship/android_Release -Isrc_impl/include -Isrc_log/include -Isrc_soc_interface/include -I/home/Qualcomm/Hexagon_SDK/3.3.3/libs/common/adspmsgd/ship/android_Release -I/home/dnozik/Qualcomm/Hexagon_SDK/3.3.3/libs/fastcv/dspCV/android_Release/ship -I/home/dnozik/Qualcomm/Hexagon_SDK/3.3.3/incs -I/home/Qualcomm/Hexagon_SDK/3.3.3/libs/common/remote/ship/android_Release -I/home/Qualcomm/Hexagon_SDK/3.3.3/incs/stddef -I/home/Qualcomm/Hexagon_SDK/3.3.3/libs/common/rpcmem/android_Release/ship -Iandroid_Release  -oandroid_Release/hexagon_controller.o src_impl/hexagon_controller.c\r\nsrc_impl/hexagon_controller.c:25:22: fatal error: adspmsgd.h: No such file or directory\r\n #include \"adspmsgd.h\"\r\n                      ^\r\ncompilation terminated\r\n```.\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16\r\n- **TensorFlow installed from (source or binary)**:\r\nMaster from GitHub\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n1.10\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nNo\r\n- **GPU model and memory**:\r\nNo\r\n- **Exact command to reproduce**:\r\n\r\n\r\nThanks for help.\r\n", "comments": ["Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17786, "title": "Update version strings for 1.7.0-rc1", "body": "", "comments": []}, {"number": 17785, "title": "Apply 1.4.2 patch update", "body": "This PR applies the patches requested for the 1.4.2 patch release, and updates the version strings accordingly.", "comments": ["The internal CIs are failing with messages like this:\r\n\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):\r\n\tFile \"/tmpfs/src/github/tensorflow/WORKSPACE\", line 15\r\n\t\tclosure_repositories()\r\n\tFile \"/tmpfs/tmp/bazel/external/io_bazel_rules_closure/closure/repositories.bzl\", line 69, in closure_repositories\r\n\t\t_check_bazel_version(\"Closure Rules\", \"0.4.5\")\r\n\tFile \"/tmpfs/tmp/bazel/external/io_bazel_rules_closure/closure/repositories.bzl\", line 172, in _check_bazel_version\r\n\t\tfail((\"%s requires Bazel >=%s but was...)))\r\nClosure Rules requires Bazel >=0.4.5 but was 0.10.0\r\nERROR: Error evaluating WORKSPACE file\r\n```\r\n\r\n10 is larger than 4 (usually). Is there a cherry-pick we're missing?", "I think the root cause is most of the Internal CI machines are upgraded, this branch was using older configs and jenkins. I think we will need to manually trigger tests on this PR using jenkins, I will look into that.", "@gunan Ping; what can I do to help push this through?", "1.5.1 is our. we can proceed with this as soon as 1.7 is complete."]}, {"number": 17784, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError", "body": "### System information\r\n\r\n-    Have I written custom code: Yes\r\n-    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n-   TensorFlow installed from (source or binary): pip\r\n-    TensorFlow version (use command below): 1.6.0\r\n- Bazel version: Build label: 0.11.1\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory:\r\n\r\n```\r\n== nvidia-smi ===================================================\r\nSun Mar 18 02:30:17 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 390.30                 Driver Version: 390.30                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro P400         Off  | 00000000:01:00.0 Off |                  N/A |\r\n| 34%   35C    P0    N/A /  N/A |     35MiB /  1999MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro GP100        Off  | 00000000:02:00.0 Off |                  Off |\r\n| 26%   42C    P0    35W / 235W |  15483MiB / 16278MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n\r\n### Describe the problem\r\n\r\nHow can I use `tf.scatter_update` with `tf.bool`  in the eager mode?\r\n\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'dtype' of bool is not in the list of allowed values: float, double, int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16, complex128, half, uint32, uint64\r\n\t; NodeDef: ResourceScatterUpdate = ResourceScatterUpdate[Tindices=DT_INT32, dtype=DT_BOOL](dummy_input, dummy_input, dummy_input); Op<name=ResourceScatterUpdate; signature=resource:resource, indices:Tindices, updates:dtype -> ; attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=Tindices:type,allowed=[DT_INT32, DT_INT64]; is_stateful=true> [Op:ResourceScatterUpdate]\r\n```\r\n\r\n### Source code / logs\r\n\r\n```python\r\nfrom tensorflow.contrib.eager.python import tfe\r\nimport tensorflow as tf\r\n\r\n\r\ndef main(_):\r\n\tref = tfe.Variable([False, True, False], trainable=False)\r\n\tindices = tf.range(3)\r\n\tupdates = tf.constant([True, True, True])\r\n\t_update = tf.scatter_update(ref, indices, updates)\r\n\r\n\r\nif __name__ == '__main__':\r\n\ttfe.enable_eager_execution()\r\n\ttf.app.run()\r\n```\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This happens because the eager placer is a little too aggressive about putting your variable on the GPU if one is available. Place it on the cpu by putting a `with tf.device('cpu:0'):` around the variable declaration.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17782, "title": "Compatible to 9.1 now?", "body": "Hi there,\r\n\r\nas in the release notes said, it is compatible to 9.0, and you said you are going to fix the bug till Februray 2018. \r\n\r\nSo is tf now compatible to CUDA9.1?", "comments": ["There's no update in 1.7 release notes, so I believe you need to build it yourself in order to get CUDA 9.1 -- https://github.com/tensorflow/tensorflow/releases/tag/v1.7.0-rc0\r\n\r\nSwitching to CUDA 9.1 in binary release will be kind of a pain for those of us using cloud, because it requires driver update", "When loading 9.* library, TF is searching 9.0 string in library name. So how to fix this?", "version is baked in during compilation. You could run ./configure and specify 9.1 during one of the questions, and then bazel build", "I would appreciate if anyone could provide **prebuilt-binaries for 9.1** if he made it. Thanks", "Someone posted CUDA 9.1 binary for Windows on https://github.com/yaroslavvb/tensorflow-community-wheels/issues", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17781, "title": "1.7.0rc2 cherry-pick request: Fixes a bug in `tf.contrib.data.map_and_batch()` shape inference", "body": "This backports a fix for issue #17720 to the release branch. (It would not be the end of the world if this change wasn't cherry picked, but I'm sending it opportunistically in case it's be possible to fix the bug in the new release.)", "comments": []}, {"number": 17780, "title": "[update TensorRT converter]", "body": "  fixed FusedBatchNorm to support broadcast;\r\n  remove fp16 conversion for type int const\r\n  add Snapshot in conversion (treated as identity)", "comments": ["@gunan @aaroey @yifeif \r\nr1.7 patch applied to master", "I guess @gunan will help to merge all cherrypicked PRs in 1.7 branch to master?", "@gunan told me to send this to master in the r1.7 PR  #17772 "]}, {"number": 17779, "title": "Fix inconsistency in parameter naming for tf.nn.conv1d", "body": "This fix tries to address the issue raised in #6379 where parameter namings for tf.nn.conv1d/conv2d/conv3d are inconsistent. While tf.nn.conv2d/conv3d (and tf.nn.convolution) uses `input` and `filter`, conv1d uses `value` and `filters`.\r\n\r\nThis fix addresses the issue by deprecating `value` and `filters`, using `input` and `filter`, and, at the same time, maintains backward-compatibility so that current user does not need to change any existing code base.\r\n\r\nThis fix fixes #6379.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks for the PR! API owners wonders if the benefit outweighs the churn here. We'd like to open this up to the wider community to see if there is support for this change (looks like there is a single thumbs up on the original issue at the moment). ", "Nagging Reviewer @jhseu: It has been 14 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @jhseu: It has been 29 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "Nagging Reviewer @jhseu: It has been 46 days with no activity and the `awaiting review` label was assigned. Can you please take a look?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17778, "title": "Bad_alloc when building standalone project in Debug", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: Most recent, pulled from github\r\n- **GCC/Compiler version (if compiling from source)**: VS 2015\r\n- **CUDA/cuDNN version**: CPU Only\r\n- **Bazel version**:  NA\r\n- **GPU model and memory**:  NA\r\n- **Exact command to reproduce**: Practically any  tensorflow command in debug\r\n\r\nI have a large project (C++) that uses a CNN for a small part of the code. I have built tensorflow from sources as a shared library using the cmake instructions. I have linked against this built library and integrated it in our code. The project uses QTCreator and Qmake.\r\n\r\nIn currently works perfect in Release mode, however as it is a larger project there is needs to occasionally build in debug. When doing a debug build I get an exception thrown on basically the first instance of tensorflow (currently a call to ReadBinaryProto). If you comment that line out it will break on the next tensorflow call.\r\n\r\nI have built a standalone project in visual studio 2015 that isolates the tensorflow part of the project and it behaves exactly the same way.\r\n\r\nThe exception is Microsoft C++ exception: std::bad_alloc at memory location 0x00000071F8AFC730.\r\n\r\nI know there is no supported way to build a debug version of the library, but I need to use this library like this. I have no pressing need to debug tensorflow related code just the rest of it.\r\n\r\nThank you\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nGPU model and memory\nExact command to reproduce", "original post updated with information", "Nagging Assignee @aselle: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @aselle: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@mrry, do you have any suggestions?", "I think this is a known limitation of MSVC, e.g. see [this forum post](http://forums.codeguru.com/showthread.php?511025-bad_alloc-debug-mode-application-calls-release-dll&s=fa99018b96d1f3bd94c6a0061eafad44&p=2008439#post2008439). Presumably the issue is a different memory layout for `std::string` in the two libraries, and this will affect many of our C++ APIs.\r\n\r\nIt might work if you replace the uses of the C++ API with equivalent calls to TensorFlow's C API.", "@jtavrisov ,\r\nIs this still an issue?\r\nCould you please update TensorFlow to the latest stable version v.2.6 and let us know if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17778\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/17778\">No</a>\n"]}, {"number": 17777, "title": "Should this error message be a bracket instead of parentheses ?", "body": "> Received a label value of 1 which is outside the valid range of [0, 1).\r\n\r\nAm I reading it wrong? or it should be `[0, 1]` insteado fo `[0, 1)` ?\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/982549ea3423df4270ff154e5c764beb43d472da/tensorflow/core/kernels/sparse_xent_op.cc#L45", "comments": ["`[0, 1)` is the interval `0 <= x < 1`, which 1 is outside. `[0, 1]` is the interval `0 <= x <= 1`, which 1 is inside, and so changing it would not be correct."]}, {"number": 17776, "title": "Fix incorrect link of checkpoint files in CNN tutorials", "body": "This PR is to fix the incorrect link of checkpoint files in CNN tutorials.\r\n\r\nAs we can see in [CNN tutorials](https://www.tensorflow.org/tutorials/deep_cnn#launching_and_training_the_model), below checkpoint files links to https://www.tensorflow.org/programmers_guide/variables#saving-and-restoring which is not correct. \r\n> cifar10_train.py periodically saves all model parameters in **checkpoint files** but it does not evaluate the model. \r\n\r\nThe correct intended link should be https://www.tensorflow.org/programmers_guide/saved_model instead.\r\n", "comments": []}, {"number": 17775, "title": "MKL Fixed timeline_test unit test failure, changed the test so that", "body": "Fixed timeline_test unit test fails, changed the test so that it can take cpu name changed with MKLDNN naming conversion", "comments": []}, {"number": 17774, "title": "Revert \"Windows: Enable tensorflow/contrib in Bazel build (#16659)\"", "body": "This reverts commit c6a12c77a50778e28de3590f4618bc2b62f3ecab.", "comments": []}, {"number": 17773, "title": "Updated tf.Session.run(...) documentation", "body": "Added information that order in which `fetches` are evaluated inside the call is undefined.\r\nDiscussion: https://github.com/tensorflow/tensorflow/issues/13133", "comments": []}, {"number": 17772, "title": "[update TensorRT converter]", "body": "  fixed FusedBatchNorm to support broadcast;\r\n  remove fp16 conversion for type int const\r\n  add Snapshot in conversion (treated as identity)", "comments": ["Patching TensorRT converter\r\n@aaroey @gunan ", "Could you send this to the master branch, too?\r\n@yifeif CC, current release owner", "Sure.\r\nWorking on it right now", "@aaroey let me know if this is ready to be merged. Thanks.", "@yifeif sorry I haven't had a chance to look into this, will do probably later today.\r\n@jjsjann123 I'm just wondering, does this PR need to be merged in 1.7? If so do you need to test everything you have done before? As @cliffwoolley mentioned that we're already good to go as-is.\r\nAlso @samikama.", "This patch is to support more slim networks. We want this to go into 1.7.\r\nSince changes are only made to the graph converter, it will not affect the build nor Tensorflow functionalities outside the conversion.\r\n", "Thanks @jjsjann123 for the fix. PR looks good to me, will wait for @gunan for the merge."]}, {"number": 17771, "title": "TypeError: Expected binary or unicode string, got None.  who can help me,thanks..", "body": "this is code:\r\n# coding=utf-8\r\nimport tensorflow as tf\r\nfrom PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport input_data\r\nimport numpy as np\r\nimport model\r\nimport os\r\nfrom skimage import io\r\n\r\n\r\ndef get_one_image(train):\r\n    files = os.listdir (train)\r\n    n = len (files)\r\n    ind = np.random.randint (0, n)\r\n    img_dir = os.path.join (train, files[ind])\r\n    image = io.imread (img_dir,as_grey=True)\r\n    plt.imshow (image,cmap ='gray')\r\n    plt.show ()\r\n    image = image.resize ([50, 50,1])\r\n    image = np.asarray (image)\r\n    return image\r\n\r\n\r\ndef evaluate_one_image():\r\n    train = './tst/'\r\n\r\n    image_array = get_one_image (train)\r\n\r\n    with tf.Graph ().as_default ():\r\n        BATCH_SIZE = 1  \r\n        N_CLASSES = 2  \r\n\r\n        image = tf.cast (image_array, tf.float32)\r\n\r\n        image = tf.image.per_image_standardization (image)\r\n\r\n        image = tf.reshape (image, [1,50, 50,1])\r\n        logit = model.inference (image, BATCH_SIZE, N_CLASSES)\r\n\r\n        logit = tf.nn.softmax (logit)\r\n\r\n        x = tf.placeholder (tf.float32, shape=[50, 50,1])\r\n\r\n\r\n        logs_train_dir = './save/'\r\n\r\n        saver = tf.train.Saver ()\r\n\r\n        with tf.Session () as sess:\r\n\r\n            print (\"\u4ece\u6307\u5b9a\u7684\u8def\u5f84\u4e2d\u52a0\u8f7d\u6a21\u578b\u3002\u3002\u3002\u3002\")\r\n\r\n            ckpt = tf.train.get_checkpoint_state (logs_train_dir)\r\n            if ckpt and ckpt.model_checkpoint_path:\r\n                global_step = ckpt.model_checkpoint_path.split ('/')[-1].split ('-')[-1]\r\n                saver.restore (sess, ckpt.model_checkpoint_path)\r\n                print ('\u6a21\u578b\u52a0\u8f7d\u6210\u529f, \u8bad\u7ec3\u7684\u6b65\u6570\u4e3a %s' % global_step)\r\n            else:\r\n                print ('\u6a21\u578b\u52a0\u8f7d\u5931\u8d25\uff0c\uff0c\uff0c\u6587\u4ef6\u6ca1\u6709\u627e\u5230')\r\n                # \u5c06\u56fe\u7247\u8f93\u5165\u5230\u6a21\u578b\u8ba1\u7b97\r\n            prediction = sess.run (logit, feed_dict={x: image_array})\r\n            # \u83b7\u53d6\u8f93\u51fa\u7ed3\u679c\u4e2d\u6700\u5927\u6982\u7387\u7684\u7d22\u5f15\r\n            max_index = np.argmax (prediction)\r\n            if max_index == 0:\r\n                print ('\u732b\u7684\u6982\u7387 %.6f' % prediction[:, 0])\r\n            else:\r\n                print ('\u72d7\u7684\u6982\u7387 %.6f' % prediction[:, 1])\r\n\r\nevaluate_one_image ()\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17770, "title": "TypeError: Expected binary or unicode string, got None", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I believe this is https://github.com/tensorflow/tensorflow/issues/17771. Please reopen if not the case."]}, {"number": 17769, "title": "SSD Resnet50 FPN ValueError: Dimensions must be equal", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom .config file, see below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7-rc0 and also tried on 1.6\r\n- **Python version**: Tried 2.7 and 3.5\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: 8.0 / 6.0\r\n- **GPU model and memory**: GeForce GTX 1060 6GB\r\n- **Exact command to reproduce**: \r\n```\r\npython3 object_detection/train.py \\\r\n    --pipeline_config_path=object_detection/samples/ssd_resnet_50_fpn_drone.config \\\r\n    --train_dir=object_detection/drone \\\r\n    --num_clones=1\r\n```\r\n\r\nWhen I'm trying to run test FPN config from model_builder_test.py (see attach), I'm getting error:\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\nWARNING:tensorflow:From /home/undead/reps/tf_models/object_detection/trainer.py:228: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.create_global_step\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 686, in _call_cpp_shape_fn_impl\r\n    input_tensors_as_shapes, status)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 6 and 5 for 'FeatureExtractor/fpn/top_down_features/add' (op: 'Add') with input shapes: [4,6,6,256], [4,5,5,256].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"object_detection/train.py\", line 167, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"object_detection/train.py\", line 163, in main\r\n    worker_job_name, is_chief, FLAGS.train_dir)\r\n  File \"/home/undead/reps/tf_models/object_detection/trainer.py\", line 246, in train\r\n    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\r\n  File \"/home/undead/reps/tf_models/slim/deployment/model_deploy.py\", line 193, in create_clones\r\n    outputs = model_fn(*args, **kwargs)\r\n  File \"/home/undead/reps/tf_models/object_detection/trainer.py\", line 179, in _create_losses\r\n    prediction_dict = detection_model.predict(images, true_image_shapes)\r\n  File \"/home/undead/reps/tf_models/object_detection/meta_architectures/ssd_meta_arch.py\", line 350, in predict\r\n    preprocessed_inputs)\r\n  File \"/home/undead/reps/tf_models/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py\", line 164, in extract_features\r\n    scope='top_down_features')\r\n  File \"/home/undead/reps/tf_models/object_detection/models/feature_map_generators.py\", line 217, in fpn_top_down_feature_maps\r\n    top_down = 0.5 * top_down + 0.5 * residual\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 971, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 296, in add\r\n    \"Add\", x=x, y=y, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3292, in create_op\r\n    compute_device=compute_device)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3332, in _create_op_helper\r\n    set_shapes_for_outputs(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2496, in set_shapes_for_outputs\r\n    return _set_shapes_for_outputs(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2469, in _set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2399, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 627, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py\", line 691, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Dimensions must be equal, but are 6 and 5 for 'FeatureExtractor/fpn/top_down_features/add' (op: 'Add') with input shapes: [4,6,6,256], [4,5,5,256].\r\n```\r\nmodel_builder_test.py and ssd_resnet_v1_fpn_feature_extractor_test.py passes on Python2.7.\r\nTried TF 1.6 and 1.7, Python 2.7 and 3.5, tried different input resolutions. All the same. Seems like a bug.\r\n\r\n[ssd_resnet_50_fpn_drone.config.zip](https://github.com/tensorflow/tensorflow/files/1819317/ssd_resnet_50_fpn_drone.config.zip)\r\n\r\n", "comments": ["Very sorry, missed with repository. Remove, please."]}, {"number": 17768, "title": "How to update columns in tf.Values", "body": "OS Platform and Distribution: windows 10\r\nTensorFlow installed from: anconda\r\nTensorFlow version: tensorflow-gpu 1.1.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: 8\r\nGPU model and memory: GTX860m 2GB\r\nExact command to reproduce: N/A\r\n\r\nI use tf.scatter_nd_update() method to update lines, but I not find any method to update columns. So I was wondering whether has a convenient way to update columns? \r\n\r\nFor example, data=tf.Values(tf.zeros([10,100])), and I want to update data with \"data[:,0]=tf.ones([10])\" or other methods like tf.scatter_nd_update.\r\n\r\nI will appreciate it if anyone can help me!", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "@angersson \r\n\r\nThank you for your response. I think it may become a feature, because the numpy lib support columns opertaions and there is lines opertaions in tensorflow. It might be confused that tf is not support columns operations and we have to use while_loop to update data. I think it might be not convenient.\r\n\r\nSo, if you ( or your team) have a plan to update operations, it might be a good attempt to add columns operations.\r\n\r\n"]}, {"number": 17767, "title": "fix the bug that all string fields in protobuf share same memory location in the monolithic build", "body": "All string fields in protobuf share same memory location in the monolithic build. This problem is described in #16291. Export protobuf related symbols in the library to fix it.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "I do not think we would like to export protobuf symbols in our libraries. @allenlavoie @aselle @martinwicke  for further suggestions, but I think this cl is the exact opposite of the direction we would like to go.", "This would force everyone to use our protobuf symbols when linking against the C++ API, which I believe has already caused problems for people when linking against libtensorflow_framework.so.\r\n\r\nWhich raises the question: why not use libtensorflow_framework.so (build the C++ API without --config=monolithic) for this behavior? Currently that will include all of the protocol buffer symbols since we don't run a linker script on it.\r\n\r\nI'd note though that the \"string sharing\" issue in https://github.com/tensorflow/tensorflow/issues/16291 came up for me when sharing un-serialized protocol buffers across two shared objects which both have their own protocol buffer arena allocators. So the reason this PR fixes the issue may be just that it allows TensorFlow to clobber your protocol buffer symbols. If you're passing un-serialized protocol buffers to TensorFlow, you should avoid including your own copy of the arena allocator (i.e. you should link against only protocol buffer headers).", "@gunan @allenlavoie \r\nThe reason why I do not use libtensorflow_framework.so is that it also exports some symbols which will conflict with OpenCV which has been described in #1924. So I use monolithic build a single `libtensorflow_cc.so` linked to my project. But there will be some undefined reference to protobuf error if I only link `libtensorflow_cc.so` and therefore I add a `-lprotobuf` to manage to compile my project. And this solution will cause that wired bugs in #16291. So I export the protobuf related symbol and everything works fine.\r\n\r\nI am sorry I am not familiar with the protobuf implementation details and what you said about linking against only protobuf header and avoiding including my own copy of the arena allocator. The solution I adopted is wrong? What's the right way for this situation?  Many thanks!", "Ah, yes, you're in a bit of a pickle then with OpenCV.\r\n\r\nOn the TensorFlow side the preferred solution would be to split out a shared object with just our protocol buffer symbols (giving us three in a C++ API package: libtensorflow_cc, libtensorflow_framework, libtensorflow_protobuf). That way people using the C++ API who don't need them don't run into conflicts with them, and people who want them but don't want the rest of TensorFlow's C++ symbols (OpenCV conflict) can link to that but not libtensorflow_framework. This is a somewhat tricky change. I don't think anyone's actively working on it, but it is planned.\r\n\r\nThis could be worked around on your end too. Basically you'd have your own shared object which wraps either your interaction with OpenCV or with TensorFlow (libtensorflow_cc.so + libtensorflow_framework.so). This would avoid code calling into OpenCV seeing TensorFlow symbols, and so avoid the symbol conflict.", "Hi @allenlavoie, So if I have a monolithic build, what would be the fix for \"string sharing\"[ #16291 ]? ", "@achalshah20 Did you figure out the resolution to the string sharing?", "@Kejie-Wang could you describe how you fixed this?", "@ttdd11 Actually, this pull request has given the solution. And it is proved to be work properly by a year of the test using this solution.", "@Kejie-Wang Okay I will try this. Are you using tensorflow::graph::SetDefaultDevice(\"/gpu:0\", &graph_def); ?\r\n\r\nI'm trying a bazel build of v1.12 and I don't this this commit is in that branch.", "@Kejie-Wang what are you bazel build parameters. Looking to build this with cuda 10.", "@Kejie-Wang How did you create your .lib from the generated dll?", "Sorry, I use tensorflow only on Linux platform and I haven't tried to compile it on Windows. Also, I use it tensorflow with dynamic library (.so on Linux same as the .dll on Windows). The bazel build parameters is `bazel build  -c opt --config=cuda --config=monolithic --copt=-march=native --copt=-mno-avx --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0 tensorflow:libtensorflow_cc.so\"`."]}, {"number": 17766, "title": "Branch 189319553", "body": "", "comments": []}, {"number": 17765, "title": "how to set sample weight  for binary classfication", "body": "I Know `tf.losses.sparse_softmax_cross_entropy` could set weights for different samples, But I don not know how to use it in my_custom_model\r\n\r\nfor example, in the ctr predicition, I want set 10 weights for the order samples, and the weight of click samples and the unclick sample is still 1.\r\n\r\nHere is my unweighted code\r\n\r\n`\r\n\r\ndef my_custom_model(features, labels, mode, params):\r\n\r\n    net = tf.feature_column.input_layer(features, params['feature_columns'])\r\n\r\n    for units in params['hidden_units']:\r\n        net = tf.layers.dense(net, units=units, activation=params[\"activation\"])  \r\n\r\n    logits = tf.layers.dense(net, params['n_classes'], activation=None)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\r\n            'probabilities': tf.nn.softmax(logits),\r\n            'logits': logits,\r\n       }\r\n       return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n\r\n    metrics = {'auc': tf.metrics.auc(labels=labels, predictions=tf.nn.softmax(logits)[:,1])}\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    assert mode == tf.estimator.ModeKeys.TRAIN\r\n    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step()) \r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n`\r\n`\r\ntrain_input_fn = tf.estimator.inputs.pandas_input_fn(x=data_train, y=data_train_click, batch_size = 1024, num_epochs=1, shuffle=False)\r\nclassifier.train(input_fn=train_input_fn)`\r\n\r\nHere `data_train_click` is a Series, which the click samples are 1 and the unclicked samples are 0. \r\n\r\nAnd in the weight Series `data_train_weight` , the order samples are 10 and the others are 1.\r\n\r\nHowever, I don't known how to use `data_train_weight` in my model.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17764, "title": "Add missing linking libs for iOS", "body": "When creating a new iOS app in Xcode with Tensorflow, we need to add these references to fix linking errors.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "signed it!\u00a0\u00a016.03.2018, 11:58, \"googlebot\" <notifications@github.com>:Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).<g-emoji class=\"g-emoji\" alias=\"memo\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f4dd.png\">\ud83d\udcdd Please visit https://cla.developers.google.com/ to sign.Once you've signed (or fixed any issues), please reply here (e.g. I signed it!) and we'll verify it.What to do if you already signed the CLAIndividual signersIt's possible we don't have your GitHub username or you're using a different email address on your commit. Check your existing CLA data and verify that your email is set on your git commits.Corporate signersYour company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.The email used to register you as an authorized contributor must be the email used for the Git commit. Check your existing CLA data and verify that your email is set on your git commits.The email used to register you as an authorized contributor must also be attached to your GitHub account.\u2014You are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or mute the thread.\u00a0", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17763, "title": "TensorFlowInferenceInterface: OutOfMemoryError on Android SDK 16", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android SDK 16 (4.1.2)\r\n- **TensorFlow installed from (source or binary)**: compile 'org.tensorflow:tensorflow-android:+\r\n- **TensorFlow version (use command below)**: latest\r\n- **Python version**: NA\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nI found a strange bug using the _TensorFlowInferenceInterface_ on Android SDK 16. When closing the interface once (using `inferenceInterface.close()`), the next start (using `new TensorFlowInferenceInterface()`) will cause an OutOfMemoryError, even though there is enough free RAM available. In contrast, there isn't any problem when keeping the interface alive to reuse it. The latter implies wasting resources though. The only way to prevent the OOM seems to be the manual kill of the surrounding process (`android.os.Process.killProcess(android.os.Process.myPid());`).\r\n\r\nJust to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.\r\n\r\nI found this behavior using an old Samsung Galaxy S2. Couldn't reproduce it on newer Android versions. As SDK 16 is already quite old, I don't know whether anybody is willing to dig deeper into this bug. However, it costs me a lot of time to isolate the problem and I hope the workaround (using a separate process and killing it manually after closing the interface) may at least save somebody else some time.\r\n", "comments": ["What is the size of your model?\r\n\r\nClosing the `TensorFlowInferenceInterface` isn't guaranteed to immediately free all the used memory from the JVM. Moreover, while your device might have free memory available, the JVM heap size is typically much smaller than the device's memory limit.\r\n\r\nCan you try adding `android:largeHeap=\"true\"` to your manifest? In the meantime, we'll see if there's any way for `TensorFlowInferenceInterface.close()` to be more aggressive about releasing any held resources, though again it's still not guaranteed to solve the problem due to the unpredictability of garbage collection.", "Thanks for your reply.\r\n\r\n> What is the size of your model?\r\n\r\nI tried e.g. the Inception v3 model, which is about 90MB.\r\n\r\n> while your device might have free memory available, the JVM heap size is typically much smaller than the device's memory limit.\r\n\r\nAs already stated above: Just to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.\r\n\r\n> Can you try adding android:largeHeap=\"true\" to your manifest?\r\n\r\nI'm already using this option, but the error still occurs from time to time.\r\n\r\n> In the meantime, we'll see if there's any way for TensorFlowInferenceInterface.close() to be more aggressive about releasing any held resources,\r\n\r\nCool, thanks!\r\n\r\n> Closing the TensorFlowInferenceInterface isn't guaranteed to immediately free all the used memory from the JVM.  [..] though again it's still not guaranteed to solve the problem due to the unpredictability of garbage collection.\r\n\r\nGarbage collection shouldn't be a problem here, should it? I guess it doesn't matter whether the garbage collection has actually freed the resources before I do anything, as long as they have been \"marked to be removed the next time the garbage collection runs\". Just before the OOM is raised, garbage collection will be triggered (automatically) at least once. Just to be sure, I even tried to trigger it manually. However, it fails to actually free the required resources. Maybe it's about segmentation / fragmentation of the free RAM? Doesn't occur on newer Android versions though.", "I haven't been able to repro locally, but I'd be curious if a heap dump might help isolate the problem or identify a leak.\r\n\r\nIf possible, you might try 1) Call `inferenceInterface.close()`, 2) Force a GC, 3) capture a heap dump. From there I'd be happy to look at the dump to see if anything stands out.", "All that said, have you looked into using TensorFlow Lite? It can substantially reduce the required memory footprint for certain models, and allows zero-copy loads of the input model as well as tensor inputs. There are pre-trained models for Inception v3 (as well as various other common models @ https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md).  See https://www.tensorflow.org/mobile/tflite/ for more documentation.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17762, "title": "UnimplementedError: Broadcast between <Tensor> and <Tensor> is not supported yet.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10 Pro 64-bit (10.0, Build 16299)\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nb'unknown' 1.5.0\r\n- **Python version**: \r\n3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI get 'UnimplementedError (see above for traceback): Broadcast between <Tensor> and <Tensor> is not supported yet' when trying to broadcast between different kinds of tensors.\r\n\r\nWhat I take from this is that the broadcasting system in tensorflow has limits on how many dimensions that can be broadcasted. After having tried multiple tensor permutations, I have further concluded that  the constraints on broadcasting for tensors is a bit unintuitive. For example:\r\n\r\nThis works:\r\n[1,2,1,1,2,2,2,2]*\r\n[2,2,2,2,1,2,2,2]\r\n\r\nThis doesn't work:\r\n[1,2,1,1,2,2,2,2]*\r\n[2,2,2,2,1,2,2,1]\r\n\r\nThis works:\r\n[2,2,2,2,1]*\r\n[1,2,1,2,2]\r\n\r\nThis doesn't work:\r\n[2,2,2,2,1,2]*\r\n[1,2,1,2,2,2]\r\n\r\nWhat I take from this is that broadcasting works for any permutation when the tensor rank is less than 6. When it is above 6, broadcasting only works when there is a maximum of 2 regions of consecutive 1's (both tensors' 1's taken into consideration). For example:\r\n\r\nThis has two regions of consecutive 1's:\r\n[1,2,1,1,2,2,2,2]*\r\n[2,2,2,2,1,2,2,2]\r\n\r\nThis has three regions of consecutive 1's:\r\n[1,2,1,1,2,2,2,2]*\r\n[2,2,2,2,2,1,2,2]\r\n\r\nAlthough, this rule seems to be wrong when calculating the product below. This doesn't work:\r\n[1,2,1,1,2,1,2,2]*\r\n[2,2,2,2,1,2,2,2]\r\n\r\nMaybe there is also a constraint on each individual tensor that it cannot contain more than two regions of consecutive 1's. An example of this:\r\n\r\nThis works:\r\n[1,2,1,1,2,2,2,2]*\r\n[2,1,2,2,1,2,2,2]\r\n\r\nNevertheless, I hope that I have demonstrated that the underlying constraints for broadcasting are confusing and unintuitive, so could you please show me the current constraints on broadcasting or confirm that above constraints are the only ones? \r\n\r\nConfigurations aside, is it possible to fix the broadcasting system such that it can support any broadcasting configuration? If not, is there at least a way to increase the number of regions of consecutive 1's in the constraints for broadcastability between two tensors? I.e. is there a way to increase the maximum of 2 regions of consecutive 1's to a maximum of e.g. 4 regions?\r\n\r\nI understand that a workaround is to tile the tensors, but I assume that this increases memory usage (by quite a lot when the tensors' ranks are high); something that is very limited in my current program.\r\n\r\n### Source code / logs\r\nAn example of case 1 and 2 above:\r\n\r\n```\r\nimport tensorflow as tf\r\nt1 = tf.random_normal([1,2,1,1,2,2,2,2])\r\nt2 = tf.random_normal([2,2,2,2,1,2,2,2])\r\nt3 = d*e\r\n\r\nt4 = tf.random_normal([1,2,1,1,2,2,2,2])\r\nt5 = tf.random_normal([2,2,2,2,1,2,2,1])\r\nt6 = d*e\r\n\r\nsess = tf.InteractiveSession()\r\nsess.run(t3)\r\nsess.run(t6)\r\n```\r\n", "comments": ["PING! Any progress? My tensor shapes are: [32,150,160] and [32,1,160].", "@rmlarsen Here's some feedback on `BinaryOp` merely supporting five dimensions. I agree the error message could be more helpful.", "@aselle Can you please take a look? Thanks!", "Closing this issue due to staleness. Please test against the latest version of TF and check if the issue still persists. Thanks!"]}, {"number": 17761, "title": "Can't import TensorFlow", "body": "### System information\r\n- OS Platform:\r\n    Windows 10 Pro 64bit\r\n- Python version: \r\n   Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32\r\n- TensorFlow version\r\n   1.6.0 installed using:\r\n`pip3 install --upgrade tensorflow`\r\n\r\n```\r\nCollecting tensorflow\r\n  Using cached tensorflow-1.6.0-cp36-cp36m-win_amd64.whl\r\nRequirement already up-to-date: astor>=0.6.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: protobuf>=3.4.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: grpcio>=1.8.6 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: wheel>=0.26 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: numpy>=1.13.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: tensorboard<1.7.0,>=1.6.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: termcolor>=1.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: absl-py>=0.1.6 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: six>=1.10.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: gast>=0.2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorflow)\r\nRequirement already up-to-date: setuptools in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from protobuf>=3.4.0->tensorflow)\r\nRequirement already up-to-date: bleach==1.5.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\r\nRequirement already up-to-date: werkzeug>=0.11.10 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\r\nRequirement already up-to-date: html5lib==0.9999999 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\r\nRequirement already up-to-date: markdown>=2.6.8 in c:\\users\\pc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\r\nInstalling collected packages: tensorflow\r\nSuccessfully installed tensorflow-1.6.0\r\n```\r\n- Have I written custom code: No\r\n- Bazel version : I don't have\r\n- CUDA/cuDNN version : I don't have\r\n- GPU model and memory :N\\A\r\n- Exact command to reproduce : `import tensorflow as tf`\r\n### The problem\r\nCan't import TensorFlow\r\n`import tensorflow as tf`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\nI have no idea what to do.\r\nTY in advance", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Installing tensorflow using ```conda install tensorflow``` can fix this error", "I'm having the same issue above on Windows Server 2012 r2 datacenter, version 6.3.9600  I tried multiple uninstalls but can't get it to work.  \r\n\r\n```\r\nC:\\Users\\Administrator>python\r\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)]\r\n on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_impor\r\nt_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\import\r\nlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_impor\r\nt_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\import\r\nlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_impor\r\nt_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\import\r\nlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-p\r\nackages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_impor\r\nt_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python36\\lib\\import\r\nlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n```", "@gunan do you know if someone can triage installation issues?", "We have seen this on windows where users were running on old CPUs that did not have AVX instruction set available.\r\nWhat is your CPU model?", "Sorry to hijack your thread @Gad1001\r\n\r\nI compiled and ran [this tool](https://gist.github.com/hi2p-perim/7855506#file-ssecheck-cpp) to check for AVX support and it shows my CPU does not, also confirmed at [cpu boss](http://cpuboss.com/cpu/Intel-Xeon-W3670)   I'm running on a VM in hyper-V on a xeon W3670. \r\n\r\nI just read the releases page mentioning AVX starting with 1.6.   I was running tensorflow 1.5 before so this makes sense!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I am facing the same issue on windows 8. Plus I have set installed VC++ 2015 Update 6 and set path to the required dll. Still facing the above error. I used the above tool and I donot have AVX support.", "If you do not have a CPU that supports AVX instruction sets, prebuilt binaries wont work on your system.\r\nWhile we work on a more portable and fast solution, you can build TF from sources to work around this problem. If you search existing issues, you can also find community built packages that can work on older systems.", "just downgrade tensorflow version to 1.5...............wasted 4 days to get this right...................rest all worked fine automatically (all errors gone by this method)\r\nuse command \"pip install tensorflow==1.5\""]}, {"number": 17760, "title": "Feature: tf.contrib.data.unordered_merge() #17471", "body": "Here is the feature for issue #17471.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "@mrry How is my submitted code?", "@deasuke  That is good and your intent is clear.  I removed the label.  This is a good and substantial add as far as I can tell let's try to resolve these conflicts and get this in.  ", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 44 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17759, "title": "Fix typo", "body": "fix typo", "comments": []}]