[{"number": 4882, "title": "S_IREAD and S_IWRITE errors from gif_archive in CentOS 6.8 Build", "body": "Hi there,\n\nI've been attempting off and on for the last 1-2 weeks to get Tensorflow to build on CentOS 6.8. I've spent a lot of time reading through old issues and piecing/hacking together suggestions from developers and other users on how to configure the build. I'm using a hand-built version of GCC (v. 4.9.3) in a non-default location with the system version of binutils (more info below). \n\nI've managed to get the tutorial_trainers_example to build and run successfully, but I'm now running into trouble when attempting to build the pip wheel (see below). It looks like there may be some missing header files.\n### Relevant Threads\n\nThe most helpful threads I've found during this process have been the following:\n\nhttps://github.com/tensorflow/tensorflow/issues/110 (especially comments from @rdipietro)\nhttps://github.com/bazelbuild/bazel/issues/760 (thanks @damienmg)\n\nThe only mention I can find of the specific error (granted, for a completely different platform) is in this recent Stackoverflow thread:\n\nhttp://stackoverflow.com/questions/39855672/tensorflow-how-to-compile-libtensorflow-cc-so-for-android\n\nThe fact that this is a recent thread makes me think that perhaps there was some sort of error that was introduced into the build config in the last few weeks/months.\n### Environment info\n\nOperating System:\n\n```\n$ lsb_release -a\nLSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch\nDistributor ID: CentOS\nDescription:    CentOS release 6.8 (Final)\nRelease:    6.8\nCodename:   Final\n$ uname -a\nLinux vmp1250 2.6.32-642.1.1.el6.x86_64 #1 SMP Tue May 31 21:57:07 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -lh /gpfs22/local/centos6/cuda-7.5/lib/libcud*\n-rw-r--r--. 1 root root 185K Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudadevrt.a\nlrwxrwxrwx. 1 root root   16 Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx. 1 root root   19 Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x. 1 root root 305K Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudart.so.7.5.18\n-rw-r--r--. 1 root root 545K Sep 17  2015 /gpfs22/local/centos6/cuda-7.5/lib/libcudart_static.a\n```\n\n```\n$ ls -lh /gpfs22/local/centos6/cudnn-7.5-v5/lib64/\ntotal 227M\nlrwxrwxrwx. 1 1000 1000  13 May 25 10:38 libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx. 1 1000 1000  17 May 25 10:38 libcudnn.so.5 -> libcudnn.so.5.0.5\n-rwxrwxr-x. 1 1000 1000 58M Apr 22 19:15 libcudnn.so.5.0.5\n-rw-rw-r--. 1 1000 1000 57M Apr 22 19:15 libcudnn_static.a\n```\n\nIf installed from source, provide \n\nThe commit hash (`git rev-parse HEAD`):\n- Most recent (df871edcff2faf643975b9863100ed41b6da9c3f)\n\nThe output of `bazel version`:\n\n```\n$ bazel version\nBuild label: 0.3.2- (@non-git)\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 20:15:17 2016 (1475871317)\nBuild timestamp: 1475871317\nBuild timestamp as int: 1475871317\n```\n### Custom Build Config for Bazel & Tensorflow\n\nFor the bazel build:\n- Modified `tools/cpp/CROSSTOOL` to point to C and C++ compilers in non-default locations \n- Added an extra `linker_flag` line in `tools/cpp/CROSSTOOL`\n- Added a bunch of `cxx_builtin_include_directory` lines to point to all the appropriate header files within GCC 4.9.3 in `tools/cpp/CROSSTOOL` \n\nFor the Tensorflow build:\n- Modified `third_party/gpus/crosstool/CROSSTOOL.tpl` to point to C++ compiler in custom location \n- Added an extra `linker_flag` line to `third_party/gpus/crosstool/CROSSTOOL.tpl` \n- Updated `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl` to point to the versions of NVCC and GCC that are in non-default locations \n- Commented out the line `cmd = 'PATH=' + PREFIX_DIR + ' ' + cmd` in `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl` (otherwise `as` cannot be found). \n\nI can provide the exact lines I used if you'd like. Just let me know. \n### Description of Problem\n\nEnvironment setup (I realize some of this may be overkill):\n\n```\n$ export PATH=/gpfs22/local/centos6/swig/3.0.5/x86_64/gcc49/nonet/bin:/gpfs22/local/centos6/cuda-7.5/bin:/gpfs22/local/centos6/python2/anaconda2/bin:/gpfs21/scratch/frenchwr/tensorflow/bazel-0.3.2/output:/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin:/gpfs22/local/centos6/java/1.8.0/bin:/usr/local/git/latest/x86_64/gcc46/nonet/bin:/usr/local/git/latest/x86_64/gcc46/nonet/libexec/git-core:/usr/scheduler/slurm/sbin:/usr/scheduler/slurm/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/local/common/bin:/usr/bin:/bin:/usr/scheduler/slurm/sbin:/usr/scheduler/slurm/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/local/common/bin:/usr/bin:/bin:/usr/local/git/latest/x86_64/gcc46/nonet/bin:/usr/local/git/latest/x86_64/gcc46/nonet/libexec/git-core:/usr/scheduler/slurm/sbin:/usr/scheduler/slurm/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/local/common/bin:/usr/bin:/bin:/usr/scheduler/slurm/sbin:/usr/scheduler/slurm/bin:/usr/lpp/mmfs/bin:/usr/local/bin:/usr/local/common/bin:/usr/bin:/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/var/cfengine/bin:/var/cfengine/bin\n$ export LD_LIBRARY_PATH=/gpfs22/local/centos6/cudnn-7.5-v5/lib64:/gpfs22/local/centos6/swig/3.0.5/x86_64/gcc49/nonet/share/swig/3.0.5:/gpfs22/local/centos6/cuda-7.5/lib:/gpfs22/local/centos6/cuda-7.5/lib64:/gpfs22/local/centos6/java/1.8.0/lib:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64:/gpfs22/local/centos6/java/1.8.0/lib/:/gpfs22/local/centos6/python2/anaconda2/lib\n$ export JAVA_HOME=/gpfs22/local/centos6/java/1.8.0\n$ export JAVA_ROOT=/gpfs22/local/centos6/java/1.8.0\n$ export CXX=/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/g++\n$ export CC=/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/gcc\n$ export GCC_ROOT=/gpfs22/local/centos6/gcc/4.9.3/x86_64\n$ export LDFLAGS=\"-L/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib -L/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64 -L/gpfs22/local/centos6/cuda-7.5/lib64 -L/gpfs22/local/centos6/cuda-7.5/lib -L/gpfs22/local/centos6/cudnn-7.5-v5/lib64 -L/gpfs22/local/centos6/python2/anaconda2/lib\"\n$ export CXXFLAGS=\"-L/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib -L/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64 -L/gpfs22/local/centos6/cuda-7.5/lib64 -L/gpfs22/local/centos6/cuda-7.5/lib -L/gpfs22/local/centos6/cudnn-7.5-v5/lib64 -L/gpfs22/local/centos6/python2/anaconda2/lib\"\n$ export CPLUS_INCLUDE_PATH=\"/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include-fixed/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/include/c++/4.9.3/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/include/c++/4.9.3/x86_64-unknown-linux-gnu/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/:/gpfs22/local/centos6/cuda-7.5/include/:/gpfs22/local/centos6/cudnn-7.5-v5/include/\"\n$ export PYTHONPATH=/gpfs22/local/centos6/python2/anaconda2/lib/python2.7:$PYTHONPATH\n$ export PYTHON_ROOT=/gpfs22/local/centos6/python2/anaconda2\n$ export PYTHON27_INC=$PYTHON_ROOT/include/python2.7\n$ export CUDA_HOME=/gpfs22/local/centos6/cuda-7.5\n$ export SWIG_ROOT=/gpfs22/local/centos6/swig/3.0.5/x86_64/gcc49/nonet\n$ export NVVM_ROOT=/gpfs22/local/centos6/cuda-7.5/nvvm\n$ export LIBRARY_PATH=/gpfs22/local/centos6/cudnn-7.5-v5/lib64:$LIBRARY_PATH\n$ export CPATH=/gpfs22/local/centos6/cudnn-7.5-v5/include:$CPATH\n$ export CUDNN_ROOT=/gpfs22/local/centos6/cudnn-7.5-v5\n$ export SWIG_PATH=$(which swig)\n```\n\nTensorflow Configuration:\n\n```\n$ $ ./configure\n/scratch/frenchwr/tensorflow/tensorflow-head /scratch/frenchwr/tensorflow/tensorflow-head\nPlease specify the location of python. [Default is /gpfs22/local/centos6/python2/anaconda2/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nDo you wish to build TensorFlow with Hadoop File System support? [y/N] N\nNo Hadoop File System support will be enabled for TensorFlow\nFound possible Python library paths:\n  /gpfs22/local/centos6/python2/anaconda2/lib/python2.7/site-packages\n  /gpfs22/local/centos6/python2/anaconda2/lib/python2.7\nPlease input the desired Python library path to use.  Default is [/gpfs22/local/centos6/python2/anaconda2/lib/python2.7/site-packages]\n\n/gpfs22/local/centos6/python2/anaconda2/lib/python2.7/site-packages\nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /gpfs22/local/centos6/cuda-7.5\nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 5.0.5\nPlease specify the location where cuDNN 5.0.5 library is installed. Refer to README.md for more details. [Default is /gpfs22/local/centos6/cuda-7.5]: /gpfs22/local/centos6/cudnn-7.5-v5\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: 5.2\nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n.................\nINFO: All external dependencies fetched successfully.\nConfiguration finished\n```\n\nTutorials Example Trainer Build:\n\n```\nbazel build -c opt --config=cuda --verbose_failures --subcommands //tensorflow/cc:tutorials_example_trainer\n```\n\nTutorials Example Trainer Run:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so.7.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5.0.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so.7.5 locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\n000000/000001 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]\n000000/000009 lambda = 1.841570 x = [0.669396 0.742906] y = [3.493999 -0.669396]\n...\n...\n```\n\nPip Wheel Build (extra command line options suggested by @rdipietro in https://github.com/tensorflow/tensorflow/issues/110):\n\n```\n$ bazel build -c opt --config=cuda --linkopt '-lrt' --copt=\"-DGPR_BACKWARDS_COMPATIBILITY_MODE\" --conlyopt=\"-std=c99\" //tensorflow/tools/pip_package:build_pip_package\n...\n...\nINFO: From Compiling external/protobuf/src/google/protobuf/compiler/cpp/cpp_message.cc:\nexternal/protobuf/src/google/protobuf/compiler/cpp/cpp_message.cc:376:8: warning: 'std::string google::protobuf::compiler::cpp::{anonymous}::MessageTypeProtoName(const google::protobuf::FieldDescriptor*)' defined but not used [-Wunused-function]\n string MessageTypeProtoName(const FieldDescriptor* field) {\n        ^\nERROR: /gpfs22/home/frenchwr/.cache/bazel/_bazel_frenchwr/4ceedd0aac0f37a9bc9063198121367a/external/gif_archive/BUILD:6:1: C++ compilation of rule '@gif_archive//:gif' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 37 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nexternal/gif_archive/egif_lib.c: In function 'EGifOpenFileName':\nexternal/gif_archive/egif_lib.c:62:6: error: 'S_IREAD' undeclared (first use in this function)\n      S_IREAD | S_IWRITE);\n      ^\nexternal/gif_archive/egif_lib.c:62:6: note: each undeclared identifier is reported only once for each function it appears in\nexternal/gif_archive/egif_lib.c:62:16: error: 'S_IWRITE' undeclared (first use in this function)\n      S_IREAD | S_IWRITE);\n                ^\nexternal/gif_archive/egif_lib.c: In function 'EGifOpenFileHandle':\nexternal/gif_archive/egif_lib.c:119:5: warning: implicit declaration of function 'fdopen' [-Wimplicit-function-declaration]\n     f = fdopen(FileHandle, \"wb\");    /* Make it into a stream: */\n     ^\nexternal/gif_archive/egif_lib.c:119:7: warning: assignment makes pointer from integer without a cast\n     f = fdopen(FileHandle, \"wb\");    /* Make it into a stream: */\n       ^\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 403.336s, Critical Path: 394.04s\n```\n", "comments": ["Try editing `external/gif_archive/egif_lib.c` and adding `#include <fcntl.h>`\n", "Looks like it's already there:\n\n``` C\n/******************************************************************************\n\negif_lib.c - GIF encoding\n\nThe functions here and in dgif_lib.c are partitioned carefully so that\nif you only require one of read and write capability, only one of these\ntwo modules will be linked.  Preserve this property!\n\n*****************************************************************************/\n\n#include <unistd.h>\n#include <stdint.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n#include <fcntl.h>\n\n#ifdef _WIN32\n#include <io.h>\n#else\n#include <sys/types.h>\n#endif /* _WIN32 */\n#include <sys/stat.h>\n\n#include \"gif_lib.h\"\n#include \"gif_lib_private.h\"\n...\n...\n```\n\n`/usr/include/fcntl.h` exists on my system; however, `S_IREAD` and `S_IWRITE` are both prepended with two underscores:\n\n```\n$ grep S_IREAD /usr/include/fcntl.h \n# define S_IRUSR    __S_IREAD       /* Read by owner.  */\n# define S_IRWXU    (__S_IREAD|__S_IWRITE|__S_IEXEC)\n$ grep S_IWRITE /usr/include/fcntl.h \n# define S_IWUSR    __S_IWRITE      /* Write by owner.  */\n# define S_IRWXU    (__S_IREAD|__S_IWRITE|__S_IEXEC)\n```\n", "Some additional data in case it's helpful:\n\n```\n$ echo | gcc -E -xc++ - -v\nUsing built-in specs.\nCOLLECT_GCC=gcc\nTarget: x86_64-unknown-linux-gnu\nConfigured with: /scratch/vanzod/GCC/source/configure --disable-multilib --prefix=/usr/local/gcc/4.9.3/x86_64\nThread model: posix\ngcc version 4.9.3 (GCC) \nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/libexec/gcc/x86_64-unknown-linux-gnu/4.9.3/cc1plus -E -quiet -v -iprefix /gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/ -D_GNU_SOURCE - -mtune=generic -march=x86-64\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../include/c++/4.9.3\"\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../include/c++/4.9.3/x86_64-unknown-linux-gnu\"\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include\"\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include-fixed\"\nignoring nonexistent directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../x86_64-unknown-linux-gnu/include\"\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/../../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../include/c++/4.9.3\"\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/../../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../include/c++/4.9.3/x86_64-unknown-linux-gnu\"\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/../../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../include/c++/4.9.3/backward\"\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/../../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include\"\nignoring duplicate directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/../../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include-fixed\"\nignoring nonexistent directory \"/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/../../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../x86_64-unknown-linux-gnu/include\"\nignoring duplicate directory \"/gpfs22/local/centos6/cudnn-7.5-v5/include\"\n  as it is a non-system directory that duplicates a system directory\n#include \"...\" search starts here:\n#include <...> search starts here:\n .\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include/\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/include-fixed/\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/include/c++/4.9.3/\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/include/c++/4.9.3/x86_64-unknown-linux-gnu/\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/lib64/\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/\n /gpfs22/local/centos6/cuda-7.5/include/\n /gpfs22/local/centos6/cudnn-7.5-v5/include/\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../include/c++/4.9.3/backward\n /usr/local/include\n /gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/../../include\n /usr/include\nEnd of search list.\n# 1 \"<stdin>\"\n# 1 \"<built-in>\"\n# 1 \"<command-line>\"\n# 1 \"<stdin>\"\nCOMPILER_PATH=/gpfs22/local/centos6/gcc/4.9.3/x86_64/libexec/gcc/x86_64-unknown-linux-gnu/4.9.3/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/libexec/gcc/\nLIBRARY_PATH=/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/x86_64-unknown-linux-gnu/4.9.3/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/lib/gcc/:/gpfs22/local/centos6/cudnn-7.5-v5/lib64/../lib64/:/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../../lib64/:/lib/../lib64/:/usr/lib/../lib64/:/gpfs22/local/centos6/cudnn-7.5-v5/lib64/:./:/gpfs22/local/centos6/gcc/4.9.3/x86_64/bin/../lib/gcc/x86_64-unknown-linux-gnu/4.9.3/../../../:/lib/:/usr/lib/\nCOLLECT_GCC_OPTIONS='-E' '-v' '-mtune=generic' '-march=x86-64'\n```\n", "You could try\n\n```\ngrep -r S_IREAD /usr/include/\n```\n\nAlso take a look at this StackOverflow related answer http://stackoverflow.com/questions/39855672/tensorflow-how-to-compile-libtensorflow-cc-so-for-android\n", "Okay, getting closer. It looks like these are defined in `/usr/include/sys/stat.h` (which is included in `external/gif_archive/egif_lib.c`) but inside a block that _should_ evaluate to true:\n\n``` C\n...\n...\n#if defined __USE_MISC && defined __USE_BSD\n# define S_IREAD    S_IRUSR\n# define S_IWRITE   S_IWUSR\n# define S_IEXEC    S_IXUSR\n#endif\n...\n...\n```\n\nTo test I wrote a short program and built and ran it in my same environment that I'm trying to build Tensorflow in:\n\n``` C\n#include <stdio.h>\n\nint main()\n{\n#if defined __USE_MISC\n   printf(\"user misc evaluates to true\\n\");\n#endif\n\n#if defined __USE_BSD\n   printf(\"user bsd evaluates to true\\n\");\n#endif\n\n#if defined __USE_MISC && defined __USE_BSD\n   printf(\"user misc and user bsd evaluate to true\\n\");\n#endif\n\nreturn 0;\n}\n```\n\n```\n$ gcc test.c \n$ ./a.out \nuser misc evaluates to true\nuser bsd evaluates to true\nuser misc and user bsd evaluate to true\n```\n\nI mentioned that SO thread in my original post. I tried the proposed solution and unfortunately it didn't help.\n", "In addition, I'm able to pull in S_IREAD and S_IWRITE from a simple toy example:\n\n``` C\n#include <stdio.h>\n#include <sys/stat.h>\n\nint main()\n{\n\nint FileHandle;\n\nFileHandle = S_IREAD;\nprintf(\"FileHandle: %d\\n\",FileHandle);\nFileHandle = S_IWRITE;\nprintf(\"FileHandle: %d\\n\",FileHandle);\n\nreturn 0;\n\n}\n```\n\n```\n$ gcc test.c \n$ ./a.out \nFileHandle: 256\nFileHandle: 128\n```\n\nAny idea why Tensorflow might behave differently? \n", "Make sure you  are matching build options that bazel uses (use the debug option that prints the exact gcc command). For example, you aren't setting the c compiler to use c99.\n", "Good call!\n\n```\n$ gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGPR_BACKWARDS_COMPATIBILITY_MODE '-std=c99' -MD -MF -c test.c \ntest.c: In function \u2018main\u2019:\ntest.c:9:14: error: \u2018S_IREAD\u2019 undeclared (first use in this function)\n FileHandle = S_IREAD;\n              ^\ntest.c:9:14: note: each undeclared identifier is reported only once for each function it appears in\ntest.c:11:14: error: \u2018S_IWRITE\u2019 undeclared (first use in this function)\n FileHandle = S_IWRITE;\n              ^\n```\n\nHowever, the toy example compiles fine if I remove the c99 option. \n\nI'm now attempting to build the pip wheel using the following command (stripped off the extra arguments that previous users indicated would prevent certain error messages):\n\n```\n$ bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package\n```\n\nI'll post an update tomorrow.\n", "Just following up, the build completed and so far all the tests I have run completed normally. Many thanks for the tips and timely response!\n", "The TensorFlow team is happy to be of service.\n\nCould we summarize what happened here? It seems like you were trying to build the pip package with custom gcc flags, which failed, and the solution was to not pass the custom flags.\n\nCan you think of any actionable improvements we might be able to make in the TensorFlow build to help gif always compile?\n", "> Could we summarize what happened here? It seems like you were trying to build the pip package with custom gcc flags, which failed, and the solution was to not pass the custom flags.\n\nExactly. I probably should have tested without the flags before opening an issue since they are never mentioned in the standard TF build.\n\nSpecifically, it looks like neither the `__USE_MISC` or `__USE_BSD` macros are defined when building with the `-std=c99` flag, which results in a few other macros (`S_IREAD` and `S_IWRITE`) never getting defined that are needed by `external/gif_archive/egif_lib.c`. I have not tested with other versions of GCC or on other systems, so I have no idea if this is generally applicable or just a quirk of my environment.  \n\n> Can you think of any actionable improvements we might be able to make in the TensorFlow build to help gif always compile?\n\nNot really, at least not without knowing first knowing much more about the `__USE_MISC` and `__USE_BSD` macros and why they should first be defined in order for `S_IREAD` and `S_IWRITE` to then be defined. I'm sure there is a hacky way to force `__USE_MISC` and `__USE_BSD` to be defined regardless of the options you pass to `gcc`, but who knows what sort of unintended side effects that might have.\n", "Thanks for the summary @frenchwr. Given that there is no clear bug here, I'm closing this for now. \n", "Thanks @aselle and @frenchwr . I had the same issue and removing the extra flags helped me solve the problem."]}, {"number": 4881, "title": "Multi gpu cifa10 example, putting ops outside of towerloss to cpu actually hurt performance", "body": "From cifa 10 example tutorial  in train.py  \"with tf.Graph().as_default(), tf.device('/cpu:0'):\" putting all the ops but in tower loss to cpu. \nas in the tutorial [cifa 10 tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#cifar-10-model)\n\"This setup requires that all GPUs share the model parameters. A well-known fact is that transferring data to and from GPUs is quite slow. For this reason, we decide to store and update all model parameters on the CPU (see green box). A fresh set of model parameters is transferred to the GPU when a new batch of data is processed by all GPUs.\"\n\nHowever I find in my rnn application(tower loss 4GPU), without setting  tf.device('/cpu:0'): actually is faster.\nSetting   tf.device('/cpu:0'):\nabout 44s per 100 step, take 9min43s to finish the first 1000 steps\n2016-10-10 17:37:53 epoch:0.1664 train_step:100 duration:0.474 elapsed:56.103 train_avg_metrics:['loss:0.496']  ['loss:0.486']\n2016-10-10 17:38:38 epoch:0.3328 train_step:200 duration:0.520 elapsed:45.064 train_avg_metrics:['loss:0.464']  ['loss:0.448']\n2016-10-10 17:39:25 epoch:0.4992 train_step:300 duration:0.408 elapsed:46.612 train_avg_metrics:['loss:0.383']  ['loss:0.238']\n2016-10-10 17:40:10 epoch:0.6656 train_step:400 duration:0.471 elapsed:44.819 train_avg_metrics:['loss:0.308']  ['loss:0.317']\n2016-10-10 17:40:54 epoch:0.8319 train_step:500 duration:0.431 elapsed:44.716 train_avg_metrics:['loss:0.269']  ['loss:0.245']\n2016-10-10 17:41:39 epoch:0.9983 train_step:600 duration:0.418 elapsed:44.256 train_avg_metrics:['loss:0.242']  ['loss:0.234']\n2016-10-10 17:42:35 epoch:1.1647 train_step:700 duration:0.424 elapsed:56.733 train_avg_metrics:['loss:0.214']  ['loss:0.168']\n2016-10-10 17:43:19 epoch:1.3311 train_step:800 duration:0.402 elapsed:43.769 train_avg_metrics:['loss:0.209']  ['loss:0.213']\n2016-10-10 17:44:01 epoch:1.4975 train_step:900 duration:0.475 elapsed:41.747 train_avg_metrics:['loss:0.201']  ['loss:0.193']\n2016-10-10 17:44:43 epoch:1.6639 train_step:1000 duration:0.423 elapsed:42.351 train_avg_metrics:['loss:0.190']  ['loss:0.238']\n2016-10-10 17:44:43 0:08:14 epoch:1.6639 eval_step: 1000 train_avg_loss:['loss:0.298'] \nNot setting tf.device('/cpu:0'):\nabout 35s per 100 step, take 6min48s to finsish the first 1000 steps\n2016-10-11 04:24:43 epoch:0.1664 train_step:100 duration:0.371 elapsed:47.976 train_avg_metrics:['loss:0.496']  ['loss:0.485']\n2016-10-11 04:25:19 epoch:0.3328 train_step:200 duration:0.385 elapsed:36.630 train_avg_metrics:['loss:0.470']  ['loss:0.408']\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 1680058 get requests, put_count=1680141 evicted_count=3000 eviction_rate=0.00178556 and unsatisfied allocation rate=0.00195172\n2016-10-11 04:25:56 epoch:0.4992 train_step:300 duration:0.319 elapsed:37.185 train_avg_metrics:['loss:0.386']  ['loss:0.336']\n2016-10-11 04:26:32 epoch:0.6656 train_step:400 duration:0.411 elapsed:35.670 train_avg_metrics:['loss:0.320']  ['loss:0.303']\n2016-10-11 04:27:07 epoch:0.8319 train_step:500 duration:0.360 elapsed:35.204 train_avg_metrics:['loss:0.268']  ['loss:0.272']\n2016-10-11 04:27:43 epoch:0.9983 train_step:600 duration:0.333 elapsed:35.526 train_avg_metrics:['loss:0.248']  ['loss:0.167']\n2016-10-11 04:28:31 epoch:1.1647 train_step:700 duration:0.378 elapsed:48.393 train_avg_metrics:['loss:0.218']  ['loss:0.170']\n2016-10-11 04:29:06 epoch:1.3311 train_step:800 duration:0.334 elapsed:34.472 train_avg_metrics:['loss:0.208']  ['loss:0.264']\n2016-10-11 04:29:38 epoch:1.4975 train_step:900 duration:0.322 elapsed:32.613 train_avg_metrics:['loss:0.199']  ['loss:0.198']\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:245] PoolAllocator: After 7824854 get requests, put_count=7824942 evicted_count=13000 eviction_rate=0.00166135 and unsatisfied allocation rate=0.00169639\n2016-10-11 04:30:11 epoch:1.6639 train_step:1000 duration:0.320 elapsed:32.889 train_avg_metrics:['loss:0.199']  ['loss:0.219']\n2016-10-11 04:30:11 0:06:48 epoch:1.6639 eval_step: 1000 train_avg_loss:['loss:0.301'] \n", "comments": ["I've noticed same thing on cirrascale GPU machines - putting parameters on gpu:0 and using gpu->gpu transfer was a bit faster. I suppose this depends on particular details of hardware -- if you don't have p2p connectivity between your video cards then keeping parameters on CPU:0 gives faster training. \n", "@yaroslavvb  I see, thanks for your quick reply. I'm not sure p2p connetctivity, testing on 4 K40 gpu cards.\nAnother thing can you help me look at this ?\nI find average_gradients(tower_grads) costing lots of time. \n[average gradients problem](http://stackoverflow.com/questions/39952878/tensorflow-multi-gpu-much-slower-than-single-gpu-if-embeddingvocabulary-is-la)\n", "It's printed when you start TensorFlow. So if you have 8 GPU's that can all talk to each other, you'll see something like this in your console.\n\n```\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 4 5 6 7 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y Y Y Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y Y Y Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   Y Y Y Y Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   Y Y Y Y Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 4:   Y Y Y Y Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 5:   Y Y Y Y Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 6:   Y Y Y Y Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 7:   Y Y Y Y Y Y Y Y \n```\n\nI'm not sure this would qualify as a bug, a person who doesn't have fancy motherboard where GPU->GPU transfers have to go through QPI link might still benefit from pinning parameters on CPU:0, such optimizations are hardware dependent\n", "@yaroslavvb \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y N N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y N N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 2:   N N Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 3:   N N Y Y \n\nhere is mine, so this tells why in my exp, 3 gpu performance is similar to 2 gpu, not improve much?\nMay be not as a bug, but we might put this info to the tutorial.\n", "@chenghuige : If you'd like to send a pull request to update the tutorial with this observation, we'd be glad to take it\n", "This issue is quite old and hasn't had recent activity. Please check the recently published performance guide and benchmarked example models."]}, {"number": 4880, "title": "Needed to install `pbr` and `funcsigs`", "body": "I was following the [Pip Installation](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation) instructions.  When I ran the [Test the TensorFlow installation](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#test-the-tensorflow-installation) steps, I hit two errors, listed below.\n### The Fix!\n\nTo fix, I ran `pip install pbr funcsigs`, then the test Python program worked.\n### Environment info\n\nOperating System: Mac OSX 10.10.5\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n   I installed this one:\n\n```\n# Mac OS X, CPU only, Python 2.7:\n$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc0-py2-none-any.whl\n```\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n0.11.0rc0\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI was following the existing instructions, I don't _think_ I missed any steps...\n### Logs or other output that would be helpful\n\nHere are the two errors that I saw:\n\n1) \n\n```\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 98, in <module>\n    from tensorflow.python.platform import test\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/platform/test.py\", line 77, in <module>\n    import mock                # pylint: disable=g-import-not-at-top,unused-import\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/mock/__init__.py\", line 2, in <module>\n    import mock.mock as _mock\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/mock/mock.py\", line 69, in <module>\n    from pbr.version import VersionInfo\nImportError: No module named pbr.version\n```\n\n2)\n\n```\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 98, in <module>\n    from tensorflow.python.platform import test\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/platform/test.py\", line 77, in <module>\n    import mock                # pylint: disable=g-import-not-at-top,unused-import\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/mock/__init__.py\", line 2, in <module>\n    import mock.mock as _mock\n  File \"/usr/local/Cellar/python/2.7.6/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/mock/mock.py\", line 80, in <module>\n    import funcsigs\nImportError: No module named funcsigs\n```\n", "comments": ["According to #3714 \"El Capitan has a protection feature built-in that prevents overwriting some python dirs even with sudo\" and \"Do not use sudo pip on Mac. use `pip install --user <package_name>`.\" Maybe those security features are why pip wasn't able to install pbr and funcsigs, even though they're dependencies of mock, which was installed.\n\nBut since mock declares its dependencies in an unusual way, maybe testing-cabal/mock#261 is relevant. Do you need to upgrade setuptools or pip?\n", "I was installing TensorFlow as part of setting up [this neural style project](https://github.com/anishathalye/neural-style).  At some point while installing all the dependencies I _did_ find that I needed to upgrade pip, as I was running pip 1.5.2.  I upgraded it to pip 8.1.2.  If a minimum pip version is required maybe add that to the [Pip Installation](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#pip-installation) documentation?\n", "We know pip 1.5.4 works because that's what comes with Ubuntu 14.04. Is there anything in the [pip release notes](https://pip.pypa.io/en/stable/news/) that you think might have caused this issue?\n\n![image](https://cloud.githubusercontent.com/assets/49262/19413056/375c65f4-92d8-11e6-8c27-5dbd457ec3ef.png)\n\nAlso I'm not even sure if it's possible for us to tune our setup.py to throw an error if a particular version of pip is being used. pip seems independent of the setuptools process.\n", "Automatically closing due to lack of recent activity. Please provide further information when it becomes available, and we will reopen the issue.\n"]}, {"number": 4879, "title": "Add tip to fix \"RuntimeError: Broken toolchain\"", "body": "This issue hit me when I tried to install TensorFlow.  I checked the \"Common Problems\" page and when I didn't see this issue there I searched the interwebz to find [the fix](http://stackoverflow.com/a/26764577/112705).  Only after I found the fix did I notice the helpful `You have not agreed to the Xcode license agreements` message further up in the Terminal output...\n\nTESTING\n\nHere's a screenshot of how the updated section of the README looks like in [my forked repository](https://github.com/daj/tensorflow/blob/0ea28c550a8d432b09055d5e2288dc3d18dd7c7e/tensorflow/g3doc/get_started/os_setup.md):\n\n![screen shot 2016-10-10 at 2 29 30 pm](https://cloud.githubusercontent.com/assets/739125/19246784/0f70ae50-8ef6-11e6-9737-0fc68144428f.png)\n", "comments": ["Can one of the admins verify this patch?\n", "@daj, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @tensorflower-gardener to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4878, "title": "Branch 135689005", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4877, "title": "fully_connected() doesn't reshape input", "body": "[According to the docs](https://www.tensorflow.org/versions/r0.11/api_docs/python/contrib.layers.html#fully_connected),  if `inputs`' rank is >2 fully_connected() flattens it. However, running with `inputs.shape = (None, m, n)` I get:\n\n```\nValueError: logits and targets must have the same shape ((?, m, 1) vs (?, 1))\n```\n\nAnd when I run with `inputs.shape = (None, m*n)` everything is fine.\n\nOn `0.11rc0`.\n", "comments": ["Could you share the backtrace and a small code example to reproduce?\n", "```\nimport tensorflow as tf\nfrom tensorflow.contrib import layers\nimport numpy as np\n\n\nx = tf.placeholder(tf.float32, shape=(None,3,4))\ny = tf.placeholder(tf.float32, shape=(None, 1))\n\nout = layers.fully_connected(inputs=x, num_outputs=1)\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(out, y))\noptimizer = tf.train.AdamOptimizer().minimize(loss)\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    x_iter = np.random.rand(2, 3, 4)\n    y_iter = np.random.rand(2, 1)\n    _ = sess.run(optimizer, feed_dict={x: x_iter, y: y_iter})\n\n## -- End pasted text --\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-1-8929993d27e5> in <module>()\n      8\n      9 out = layers.fully_connected(inputs=x, num_outputs=1)\n---> 10 loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(out, y))\n     11 optimizer = tf.train.AdamOptimizer().minimize(loss)\n     12\n\n/Users/olevinkr/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn.pyc in sigmoid_cross_entropy_with_logits(logits, targets, name)\n    432     except ValueError:\n    433       raise ValueError(\"logits and targets must have the same shape (%s vs %s)\"\n--> 434                        % (logits.get_shape(), targets.get_shape()))\n    435\n    436     # The logistic loss formula from above is\n\nValueError: logits and targets must have the same shape ((?, 3, 1) vs (?, 1))\n```\n\nFlattening `x` works:\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib import layers\nimport numpy as np\n\n\nx = tf.placeholder(tf.float32, shape=(None,3*4))\n# x = tf.placeholder(tf.float32, shape=(None,3,4))\ny = tf.placeholder(tf.float32, shape=(None, 1))\n\nout = layers.fully_connected(inputs=x, num_outputs=1)\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(out, y))\noptimizer = tf.train.AdamOptimizer().minimize(loss)\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # x_iter = np.random.rand(2, 3, 4)\n    x_iter = np.random.rand(2, 3 * 4)\n    y_iter = np.random.rand(2, 1)\n    _ = sess.run(optimizer, feed_dict={x: x_iter, y: y_iter})\n```\n\nAm I misunderstanding the docs?\n", "It's possible you might have read the documentation for `fully_connected` assuming it would reshape the output. I think it only reshapes the input for matmul, then puts it back to the original shape. We recommend using [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) for community support in the future. We try to keep this issue tracker focused on bugs and feature requests.\n"]}, {"number": 4876, "title": "Eigen::TensorEvaluator `m_impl` is private, compiling cxx11_tensor_cuda.cu", "body": "i know this is a bug for Eigen repo, but the owner of the file seems to be Benoit Steiner, so tentatively thinking here might be a good place to file it?\n\nIf I build cxx11_tensor_cuda.cu using standard eigen build process, it does in fact compile.\n\nHowever, if I build using clang, I get the following error:\n\n```\nIn file included from test/eigen/cxx11_tensor_cuda.cu:19:\nIn file included from /usr/local/eigen/unsupported/Eigen/CXX11/Tensor:95:\n/usr/local/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReductionCuda.h:687:50: error: \n      'm_impl' is a private member of 'Eigen::TensorEvaluator<const\n      Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>, const\n      Eigen::array<long, 2>, const Eigen::TensorMap<Eigen::Tensor<float, 4, 0,\n      long>, 0> >, Eigen::GpuDevice>'\n      typename Self::CoeffReturnType val = input.m_impl.coeff(j * num_pr...\n...\ntest/eigen/cxx11_tensor_cuda.cu:234:30: note: in instantiation of function\n      template specialization\n      'Eigen::TensorDevice<Eigen::TensorMap<Eigen::Tensor<float, 2, 0, long>,\n      0>,\n      Eigen::GpuDevice>::operator=<Eigen::TensorReductionOp<Eigen::internal::MaxReducer<float>,\n      const Eigen::array<long, 2>, const Eigen::TensorMap<Eigen::Tensor<float,\n      4, 0, long>, 0> > >' requested here\n  gpu_out.device(gpu_device) = gpu_in1.maximum(reduction_axis);\n                             ^\n/usr/local/eigen/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:732:36: note: \n      declared private here\n  TensorEvaluator<ArgType, Device> m_impl;\n```\n\nI'm building as follows:\n- (optional) comment out lines 15-17, to disable fp16\n- run, from cloned eigen repo:\n\n```\n clang++-3.8 -std=c++11 -I. -Itest -Ibuild/test -I/usr/local/cuda-7.5/include unsupported/test/cxx11_tensor_cuda.cu --cuda-host-only -emit-llvm  -O3 -S -o cxx11_tensor_cuda-hostraw.ll\n```\n\nThoughts?  @benoitsteiner \n\n(edited to have a correct commandline, tested from root of cloned eigen repo)\n", "comments": ["@benoitsteiner could you take a look\n", "The OuterReduction kernel is declared as a friend on the evaluator, and as such has access to the private members of the evaluator. nvcc has no problem handling this, but somehow clang doesn't seem to be able to honor with friendship with cuda kernel code\n", "Ah, fair enough.  Will raise an issue with `clang`.  Thanks!\n", "(note: seems like this is fixed in clang 3.9)\n", "(though I might just hack the eigen sourcecode for now, since I dont feel like retesting everything after migrating to clang 3.9)\n"]}, {"number": 4875, "title": "Error for Mac El-capitan while installing tensorflow GPU version on CUDA 8.0", "body": "Below is the error I am getting when I import tensorflow:\n\nimport tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 49, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\nImportError: dlopen(/Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.7.5.dylib\n  Referenced from: /Users/d/anaconda/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n  Reason: image not found\n\nI don't understand why it is asking for libcudart.7.5.dylib when I have installed CUDA 8.0. Theano is working perfectly fine for me on GPU but tensorflow is not. Any help would be appreciated. I have exactly followed the steps mentioned on tensorflow website. I also tried disabling SIP protection in El-capitan as mentioned in other similar github issues but no luck.\n", "comments": ["I'm a total TF newbie (literally running first net now), but AFAICT:\nThe stable version 0.11rc0 doesn't support CUDA 8. Next version (0.11rc1, <= 1 week) should solve this.\n\nRefs:\nhttps://github.com/tensorflow/tensorflow/issues/3069#issuecomment-252493740\nhttps://github.com/tensorflow/tensorflow/issues/3052#issuecomment-252470239\nhttps://github.com/tensorflow/tensorflow/issues/2559\n", "Correct. We don't support CUDA 8 yet. But that should hopefully change in a matter of weeks when rc2 comes out. Thanks for taking the time to report the issue.\n", "For people googling this issue in the future: the solution may have been to disable SIP (System Integrity Protection) by running `csrutil disable`."]}, {"number": 4874, "title": "Fixed and improved ./configure on Windows", "body": "1. Make the script work with path containing spaces (If users install python by Anaconda, the python path will probably contain space)\n2. On Windows, disable bazel clean --expunge(it's broken)\n3. Create junctions instead of copying python lib, include, numpy folders\n4. fixed symlink creation in gen_git_source.py with python 3.5\n", "comments": ["Can one of the admins verify this patch?\n", "@mrry Yes, I think it's a good idea to use `is_windows` to opt out GCP, HDFS, CUDA support on Windows for now.\n", "@rohan100jain For example, if users install python using Anaconda(which is recommended in CMake build), the python path will probably contain spaces like this `C:\\Program Files\\Anaconda3\\lib\\site-packages`, so we have to add quotes to the variable.\n", "@mrry Thanks for catching so many typos.. Please review again. :)\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please\n", "The configure script does cause a bug on Mac, I'll look into it.\n", "`read -r -a python_lib_path <<< $(python_path)` doesn't really split the string into a bash array on OSX, so switch to another way.\nPlease test again! :)\n", "I install the tensorflow on windows via the cmake/README. It seems everything goes well. And after I \"activate tensorflow\" and \"import tensorflow as tf\". It says \"No module named 'tensorflow'\" . I do not know how to fix it.\n", "@tensorflow-jenkins test this please.\n", "Great, looks like I fixed it!\n"]}, {"number": 4873, "title": "issue 1702", "body": "https://github.com/tensorflow/tensorflow/issues/1702\n\n```\na1=tf.constant([[1],[2]])\na2=tf.constant([1,2])\na3=tf.constant([1,2,3])\na4=tf.constant([[1],[2],[3]])\na5=tf.constant([[1,2], [1,2]])\na6=tf.constant([[1,2,3], [1,2,3]])\na7=tf.constant([[1,2,3], [1,2,3], [1,2,3]])\na8=tf.constant([[[1,2],[3,4]], [[5,6],[7,8]]])\nx1 = tf.transpose(a8, [1, 0, 2])\nx2 = tf.transpose(a8 , [2,1,0])\nb1=tf.Print(a1,[a1])\nb2=tf.Print(a2,[a2])\nb3=tf.Print(a3,[a3])\nb4=tf.Print(a4,[a4],summarize=8)\nb5=tf.Print(a5,[a5],summarize=8)\nb6=tf.Print(a6,[a6])\nx1p = tf.Print(x1,[x1],summarize=8)\nx2p = tf.Print(x2,[x2],summarize=8)\nwith tf.Session() as sess:\n    sess.run(b1)\n    sess.run(b2)\n    sess.run(b3)\n    sess.run(b4)\n    sess.run(b5)\n    sess.run(b6)\n    sess.run(x1p)\n    sess.run(x2p)\n\nsess.close()\n```\n\nresult:\n\n```\n\nI tensorflow/core/kernels/logging_ops.cc:79] [[1][2]]\nI tensorflow/core/kernels/logging_ops.cc:79] [1 2]\nI tensorflow/core/kernels/logging_ops.cc:79] [1 2 3]\nI tensorflow/core/kernels/logging_ops.cc:79] [[1][2][3]]\nI tensorflow/core/kernels/logging_ops.cc:79] [[1 2][1 2]]\nI tensorflow/core/kernels/logging_ops.cc:79] [[1 2 3]...\nI tensorflow/core/kernels/logging_ops.cc:79] [[[1 2][5 6]][[3 4][7 8]]]\nI tensorflow/core/kernels/logging_ops.cc:79] [[[1 5][3 7]][[2 6][4 8]]]\n```\n", "comments": ["Can one of the admins verify this patch?\n", "@guotong1988, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @tensorflower-gardener to be potential reviewers.\n", "Sorry , I miss some code .I will submit them soon.\n", "It's 2:50 am now I 'm on the bed.\nI will open my computer morning.\n", "Closing in lieu of the other PR\n"]}, {"number": 4872, "title": "Removed orphaned declaration of extern \"C\" void RegisterOps(void *)", "body": "In op.h the declaration of `extern \"C\" void RegisterOps(void *)` was missed when removing it's implementation in e774fa829a1d74ab2d410cefc62cbd47e20b0317. This PR removes the orphaned declaration.\n", "comments": ["Can one of the admins verify this patch?\n", "@andreas-eberle, thanks for your PR! By analyzing the history of the files in this pull request, we identified @josh11b, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4871, "title": "Can I back-propagate the already calculated gradients w.r.t the outputs to all the parameters, and then apply the gradients to the whole network? ", "body": "I can get the gradients w.r.t the outputs of the network(calculated by python or other ways), Can I just back-propagate the gradients w.r.t the outputs to all the parameters, and then apply them to the whole network? \n", "comments": ["We recommend posting on [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) for community-driven support. We try to keep this issue tracker focused on bugs and feature requests.\n"]}, {"number": 4870, "title": "Fixed the wrong comments of `tf.scan`.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@haosdent, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @ebrevdo and @yuanbyu to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 4869, "title": "tensorflow R0.9", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@badman1111, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @yuanbyu to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 4868, "title": "Updates to documentation of two tf.contrib.metrics functions.", "body": "- Added code wrapping markdown for function `tf.contrib.metrics.aggregate_metrics`.\n- General editing of `tf.contrib.metrics.confusion_matrix` for clarity.\n", "comments": ["Can one of the admins verify this patch?\n", "@kbrose, thanks for your PR! By analyzing the history of the files in this pull request, we identified @mrry, @drpngx and @tensorflower-gardener to be potential reviewers.\n", "@rohan100jain Added new commit, updated description to clarify how `num_classes` is computed, and updated example code to hopefully clarify further what labels are assumed to exist.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 4867, "title": "[0.11]Can't bind GPU with tf.device()", "body": "I installed the latest TF v0.11  and found that it can't bind specific GPU with `tf.device(\"gpu:1\")`.  It always applies all GPU resource when I run `tf.Session()`, which is unreasonable.\nFor example:\n\n``` python\nimport tensorflow as tf\nwith tf.device(\"gpu:1\"):\n  sess = tf.Session()\n```\n\nGPU resource:\n\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      7895    C   python                                       10963MiB |\n|    1      7895    C   python                                       10922MiB |\n|    2      7895    C   python                                       10923MiB |\n|    3      7895    C   python                                       10922MiB |\n+-----------------------------------------------------------------------------+\n```\n\nBut when I switch to TF v0.9, everything is fine.\nGPU resource:\n\n```\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      5161    C   python                                          65MiB |\n|    1      5161    C   python                                          65MiB |\n|    2      5161    C   python                                          65MiB |\n|    3      5161    C   python                                          65MiB |\n+-----------------------------------------------------------------------------+\n```\n\nI think it's a bug and caused by the update of `tf.Session()`.\nI hope I can fix it ASAP\n", "comments": ["There was a change a while back to new memory allocator that grabs all GPU memory of all GPUs on initialization for  performance reasons. There are some flags to limit how much memory it grabs -- http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n", "@yaroslavvb \n\n```\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n```\n\nBut I think this way can't to set on a per-GPU basis.\nIf we need train two different models  on two different GPUs at same time,how can we \nallocate GPU memory.\n", "If they run in difference processes use `CUDA_VISIBLE_DEVICES` to limit GPUs visible to each process\n", "Or try the allow_growth option instead.\n", "Seems like the question of how to limit a process to a subset of GPUs has been resolved (use `CUDA_VISIBLE_DEVICES`). I'm going to go ahead and close this issue, feel free to reopen if the concerns were not addressed. Thanks!\n"]}, {"number": 4866, "title": "Add zlib to the CMake build.", "body": "Previously, we relied on a system-installed copy of zlib; now we build it from source and link it into the generated executables. This removes the previous implicit requirement that zlib must already be installed.\n\nFixes #4853.\n", "comments": ["@mrry, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ageron, @lilac and @clsung to be potential reviewers.\n"]}, {"number": 4865, "title": "GraphDef duplication in event log", "body": "This is a little difficult to explain, and I'm guessing someone has to have run into it before, but it would be nice to at least get an explanation for what's going on here.\n\nThe issue I'm finding is that when you specify a `model_dir` argument for any tflearn model, the first time you fit that model, the tensorboard warning like the following will always appear:\n\n`WARNING:tensorflow:Found more than one graph event per run. Overwriting the graph with the newest event.`\n\nIf you clear the contents of that directory and rerun the same model, the warning no longer appears.\n\nFor example, here is a piece of code to reproduce:\n\n```\nimport tensorflow as tf\nimport random\n\nfrom tensorflow.contrib.learn.python import learn\nfrom tensorflow.contrib.learn.python.learn import datasets\nfrom tensorflow.contrib.learn.python.learn.estimators._sklearn import accuracy_score\nfrom tensorflow.contrib.learn.python.learn.estimators._sklearn import train_test_split\nrandom.seed(42)\n\niris = datasets.load_iris()\nx_train, x_test, y_train, y_test = train_test_split(iris.data,\n                                                    iris.target,\n                                                    test_size=0.2,\n                                                    random_state=42)\n\ndef custom_optimizer(learning_rate):\n    return tf.train.MomentumOptimizer(learning_rate, 0.9)\n\nclassifier = learn.TensorFlowDNNClassifier(\n    hidden_units=[10, 20, 10],\n    n_classes=3,\n    steps=400,\n    learning_rate=0.01,\n    optimizer=custom_optimizer,\n    model_dir='/tmp/tf/bug_test_1')\nclassifier.fit(x_train, y_train)\n```\n\nIf you run this in a jupyter notebook, wait until it is done, cd into `/tmp/tf/bug_test_1`, and run `tensorboard --logdir=.`, the warning will appear.  If you clear the contents and run again, it will no longer appear.  If you, in the same notebook within the same kernel, change the directory to something else (e.g. `/tmp/tf/bug_test_2`) and run again, the warning will again appear.  And the warning will again go away if you clear the contents and run again with the same directory.\n\nI also tried pre-creating the directory but that appears to make no difference.\n\nI wouldn't imagine this indicates a major problem but I try to take those warnings seriously since they have indicated more problematic issues in the past with running multiple models in the same kernel, so it would be great to know what this means.\n### Environment info\n\nOperating System: OS X Yosemite (10.10.5)\nTensorflow Version: 0.10.0\n", "comments": ["Thank you for reporting this issue. You're correct that warnings like these should be taken seriously. To help me prioritize things, I just want to confirm whether or not this is causing TensorBoard to display incorrect data.\n", "I don't believe the data shown is incorrect because of this, I think it's just a false warning.  Although I can't say that with 100% confidence since I'm really sure if there may be subtle differences when events already exist from previous runs (but there are no obvious differences as far as I can tell).\n", "Thanks for clarifying. I'm going to mark this 'contributions welcome' because we've got a bit of a backlog at the moment.\n", "I test with RNN, the case looks different. I think we should compare the content of the graph_def and only warn if their are different. Make sense? @jart \n", "Conditionally warning based equality sounds like a band-aid rather than a fix. Ideally I would like to address the root cause.\n", "@jart \r\nI get these warnings as well, and additionally, on the second run, if I go to TensorBoard's graph section, my graph get's duplicated. Like it shows 2 separate identical copies of the graph. If I delete all the logs and just run once then my graph is displayed correctly.", "@itsmeolivia Why did you close this issue? At least, you should have explained why this issue was closed, given that it doesn't look like it's solved.", "It seems like this issue still persists at least I have the same warning with version 1.11"]}, {"number": 4864, "title": "Replace all calls to contrib.learn.datasets.base.load_csv with calls \u2026", "body": "\u2026to contrib.learn.datasets.base.load_csv_with_header and adjust tutorials accordingly, as contrib.learn.datasets.base.load_csv is not defined anymore. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/datasets/base.py.\n\nIn https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/contrib/learn/python/learn/datasets/base.py one can see how contrib.learn.datasets.base.load_csv was defined in the past:\n\n``` py\n@deprecated('2016-09-15', 'Please use load_csv_{with|without}_header instead.')\ndef load_csv(filename, target_dtype, target_column=-1, has_header=True):\n  \"\"\"Load dataset from CSV file.\"\"\"\n  if has_header:\n    return load_csv_with_header(filename=filename,\n                                target_dtype=target_dtype,\n                                features_dtype=np.float64,\n                                target_column=target_column)\n  else:\n    return load_csv_without_header(filename=filename,\n                                   target_dtype=target_dtype,\n                                   features_dtype=np.float64,\n                                   target_column=target_column)\n```\n", "comments": ["@esBeee, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @haosdent and @philstahlfeld to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Thank you for working on this edit, I was very confused when the load_csv call did not work. After using your changes, I get a red wall of warnings: (which is certainly better than nothing working at all). Additionally, the classification seems to change for the second example we run predict on. I have listed the the key warnings and bolded them in the pasted output:\n- default value of enable_centered_bias\n- Using default config\n- float64 is not supported by many models, consider casting to float32\n- Given features\n- Given targets\n- The default behavior of predict() is changing\n- The prediction changed from [1, 2] to [1,1]\n\nWARNING:tensorflow:Change warning: **default value of enable_centered_bias** will change after 2016-10-09. It will be disabled by default.Instructions for keeping existing behaviour:\nExplicitly set `enable_centered_bias` to 'True' if you want to keep existing behaviour.\nWARNING:tensorflow:**Using default config**.\nWARNING:tensorflow:**float64 is not supported by many models, consider casting to float32.**\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\nWARNING:tensorflow:  **Given features**: Tensor(\"input:0\", shape=(?, 4), dtype=float64), required signatures: TensorSignature(dtype=tf.float64, shape=TensorShape([Dimension(None), Dimension(4)]), is_sparse=False).\nWARNING:tensorflow: **Given targets**: Tensor(\"output:0\", shape=(?,), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False).\nAccuracy: 0.966667\nWARNING:tensorflow:Calling predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with as_iterable=False is deprecated and will be removed after 2016-09-15.\nInstructions for updating:\nThe default behavior of predict() is changing. The default value for\nas_iterable will change to True, and then the flag will be removed\naltogether. The behavior of this flag is described below.\nWARNING:tensorflow:Calling predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with as_iterable=False is deprecated and will be removed after 2016-09-15.\nInstructions for updating:\n**The default behavior of predict() is changing**. The default value for\nas_iterable will change to True, and then the flag will be removed\naltogether. The behavior of this flag is described below.\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n**Predictions: [1 1]**\n\n", "Closing due to lack of CLA signing after a few weeks -- please feel free to send in a new PR / re-open when it is signed.\n"]}, {"number": 4863, "title": "iOS error: No OpKernel was registered to support Op 'Mul' with these attrs [T=DT_INT32]", "body": "I have some issues performing a multiplication of int32 data on iOS. Session::Run fails with the error shown below, indicating that this particular multiplication op is not supported. I've already checked tf_op_files.txt, and 'tensorflow/core/kernels/cwise_op_mul.cc' is there, obviously, and it looks to like the int32 version should also get registered there.\n\nDo I need to take any extra steps to enable int32 multiplication on iOS?\n\nThis is the exact error message I'm getting:\n\n```\nNo OpKernel was registered to support Op 'Mul' with these attrs\n     [[Node: mul = Mul[T=DT_INT32](Cast, Cast)]]\n```\n\nI'm using TensorFlow 0.10. Here is how I create the graph def file in Python:\n\n```\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\n\ninput = tf.placeholder(tf.float32, shape=(1,4), name='input')\n\nv = tf.cast(input, tf.int32)\nv = v * v\noutput = tf.cast(v, tf.float32, name='output')\n\nwith tf.Session() as sess:\n    output_graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def, ['output'])\n\nwith tf.gfile.GFile('/tmp/test_graph.pb', 'wb') as f:\n    f.write(output_graph_def.SerializeToString())\n```\n\nThanks a lot,\nPeter\n", "comments": ["Can anyone shed some light on this? Are int32 ops not supported on iOS by default? If so, is there a way to enable them? I'm kind of stuck with this at the moment, so any input is highly appreciated.\n\nThanks,\nPeter\n", "Thanks for reaching out @peterkfm, and for your patience. This is a question that's probably more appropriate for [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow). We rely on the community for support and try to keep this issue tracker focused on bugs and feature requests.\n", "I actually just had a chance to look into this, and I believe it's a bug with our op registration code. It's expected that we only register float and int32 variants of kernels on mobile platforms:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/register_types.h\n\nBut the code for cwise ops makes some assumptions about the order of types, assuming that only the first two arguments to REGISTER<N> macros should be used, in the hope that they're DT_FLOAT32 and DT_INT32:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops_common.h#L413\n\nUnfortunately the code for registering the \"Mul\" kernels passes in a float, then a half as the first two arguments to REGISTER5:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_op_mul_1.cc\n\nThis means that only the float kernel gets registered. A simple workaround for now would be to change the line to be:\n\n`REGISTER5(BinaryOp, CPU, \"Mul\", functor::mul, float, int32, double, uint8, Eigen::half);`\n\nI haven't tested this myself, and it doesn't fix other occurrences of this same problem, but could you give it a try @peterkfm ?\n", "@petewarden Are you still planning to fix this?  Should I take off awaiting response?\n", "Thanks a lot Pete, that was indeed what was causing the problem. I ended up working around it by defining `__ANDROID_TYPES_FULL__` (instead of `__ANDROID_TYPES_SLIM__`) in the Makefile. After that, to fix other missing ops, I extended tf_op_files.txt to contain practically everything that compiles on iOS, and changed register_types.h to make sure bool ops get defined as well.\n\nThanks again for your help, much appreciated!\n", "Any updates on a permanent fix to this issue? ", "> But the code for cwise ops makes some assumptions about the order of types, assuming that only the first two arguments to REGISTER macros should be used\r\n\r\nIt looks like only the *first* argument is used if `__ANDROID_TYPES_SLIM__` is defined.\r\nIn addition to \r\n`REGISTER5(BinaryOp, CPU, \"Mul\", functor::mul, float, int32, double, uint8, Eigen::half);`\r\n\r\nI changed REGISTER5 to\r\n\r\n```\r\n#define REGISTER5(OP, D, N, F, T0, T1, T2, T3, T4) \\\r\n    REGISTER(OP, D, N, F, T0) \\\r\n    REGISTER(OP, D, N, F, T1)\r\n```\r\n\r\nNote that by setting `__ANDROID_TYPES_FULL__` library size increases significantly and should probably be avoided.", "@sschaetz Do you mean just changing the macro of `REGISTER5`, and without defining `__ANDROID_TYPES_FULL__`, then this issue can be fixed (at least temporarily)?  Thanks in advance.", "I have figured out what you mean.  So both `cwise_op_mul_1.cc` and `REGISTER5` macro need change, and leave `__ANDROID_TYPES_FULL__` not modified.  Thanks.", "I ran into this very same issue, but for Op 'Add'. \r\nI applied this solution and it worked as expected.", "@peterkfm How did you find which ops compile on iOS? Can you also share the changes you made on register_types.h?", "I ran into this issue with MULTIPLY. I fixed up the MULTIPLY REGISTER statement and then ran into the same issue with ADD. I was wondering if anyone had a solution other than manually changing each cwise ops file.", "@scottwilson312 it looks like a fix 1410510 might have been  committed. See above in the thread. ", "@scottwilson312 That commit is just for my PERSONAL use only in a fork of tensorflow, following the comments above.  It is not a clean fix.  Be sure to understand it before using it.", "@BrianOn99 what exactly are the issues with this fix? ", "@cancan101 \r\nAs @petewarden mentioned\r\n> This means that only the float kernel gets registered. A simple workaround for now would be to change the line to be:\r\n>\r\n> REGISTER5(BinaryOp, CPU, \"Mul\", functor::mul, float, int32, double, uint8, Eigen::half);\r\n\r\nIt is just naively switching position of parameters, in order to switch `int32` to the 5th parameter.  That means which op get registered depends on its position, which is totally nonsense I think.  Just a temporary workaround.  Anyway I am not a dev of tensorflow so it is better to let tensorflowers to make a better fix.", "Right, but that is _already_ the status quo with how this builds? This change seems like an improvement over what is in there", "@cancan101 To be more concrete, I also met this issue with Op 'Mul', replaced by Op 'Add' recently.  Then I \"fixed\" it in the same way in tensorflow/core/kernels/cwise_op_add_1.cc by switching parameter.  Then should we apply the same dirty trick in all of these files?\r\n\r\nI think a more appropriate way is to put some notes in the README or error message, and even better, an easier way for users to compile optional operator perhaps in a txt file.", "Well assuming this kernel is relatively frequently used, it would be nice if tf worked out of the box so to speak ", "It seems commit 26be523ed4ab3a573af7771aec770832d9c30f90 is intended to solve this issue.  Any one verify it?\r\nIt is merged in the both master and v1.1.0rc2, but not in  v1.1.0.", "I have had the same issue with \"Less\", then with \"Add\" and then with some other operation when I stopped make changes to \"cwise_op_*\" files. I think there should be some simple way to configure which operation and type to include or not for mobile devices. I have also tried to use `print_selective_registration_header` functionality but it does not help. Even though I added `-D__ANDROID_TYPES_FULL__` flag I still get `No OpKernel was registered to support Op 'GatherNd' with these attrs` message.", "I am getting this issue also, with 'Add' on iOS.", "The proposed change in https://github.com/tensorflow/tensorflow/commit/1410510fd1667f71338b43d6641c3ee5738c4b87 Worked when I applied it to the add_1 function. Now I'm getting the same error with 'Dilation2D'", "I take a look at the `tensorflow/core/kernels/cwise_ops_common.h` and `tensorflow/core/framework/register_types.h`.\r\n\r\nI don't have a android or iOS build but correct me if I am wrong: it seems that we could just use `TF_CALL_float`, `TF_CALL_int32` explicitly for all types without worrying about the platform, and moves `__ANDROID_TYPES_SLIM__` to `register_types.h` so that different platform only register a subset of the ops.\r\n\r\nFor example, in `register_types.h`:\r\n- only half, float, int32, int64, bool, and quantized types are supported for `defined(__ANDROID_TYPES_FULL__)`\r\n- Only float, int32, and bool are supported are supported for `defined(IS_MOBILE_PLATFORM) && !defined(__ANDROID_TYPES_FULL__)`\r\n\r\nI don't see an explicit `__ANDROID_TYPES_SLIM__` but if `__ANDROID_TYPES_SLIM__ == !__ANDROID_TYPES_FULL__` then we could just use TF_CALL_xxx for all types and it should have already been handled.\r\n\r\nIf `__ANDROID_TYPES_SLIM__` means we only want to register `float` (not even int32) then we could add additional defines in `register_types.h`.\r\n\r\nI think that should be better than order-dependent `REGISTER2`, `REGISTER3`, `REGISTER4`?", "Given that this is substantially the same as https://github.com/tensorflow/tensorflow/issues/9476, if the official recommendation is to use `__ANDROID_TYPES_FULL__`, then we should consider closing this out.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Closing this, since the preferred approach is to use `__ANDROID_TYPES_FULL__`."]}, {"number": 4862, "title": "Branch 135614756", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jhseu, @tensorflower-gardener and @jart to be potential reviewers.\n"]}, {"number": 4861, "title": "Example mnist_rnn Not Working with Docker Image", "body": "# Issue: Example mnist_rnn does run on docker image.\n\n```\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n<ipython-input-6-3bb5b939d552> in <module>()\n      3 from __future__ import print_function\n      4 \n----> 5 from sklearn import metrics, preprocessing\n      6 \n      7 import tensorflow as tf\n\nImportError: No module named sklearn\n\n```\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nAn example in the code base does not work with the docker image. It is the opinion of the filer that all examples should run without any need for configuration on the docker image because the project has control over what is installed on the docker image.\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/mnist_rnn.py\n### Environment info\n\nOperating System:\n\ndocker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\n\n> Installed version of CUDA and cuDNN: \n\n**NONE, CPU based container**\n\nIf installed from binary pip package, provide:\n\n> The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n# python -c \"import tensorflow; print(tensorflow.__version__)\"\n0.11.0rc0\n```\n\n> If installed from source, provide \n\nNot installed from source\n\n> If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nExample given at beginning of ticket.\n\n> What other attempted solutions have you tried?\n\nRemoved all references to sklearn. Application works.\n\n```\n# It's useful to scale to ensure Stochastic Gradient Descent will do the right thing\n#scaler = preprocessing.StandardScaler()\n#X_train = scaler.fit_transform(X_train)\n#X_test = scaler.fit_transform(X_test)\n```\n\n> Logs or other output that would be helpful\n\nNo logs produced.\n", "comments": ["Try running `sudo apt-get install python-sklearn` inside the Docker container.\n\nThank you for reporting this issue. I'm writing up a change to the Dockerfile now so others don't run into this same problem. It'll be exported to GitHub in a matter of days. This bug will be closed when that happens.\n"]}, {"number": 4860, "title": "Fixed leak `_EventLoggerThread` threads", "body": "This fixes #4820 .\n", "comments": ["@haosdent, thanks for your PR! By analyzing the history of the files in this pull request, we identified @danmane to be a potential reviewer.\n", "Can one of the admins verify this patch?\n", "Hi, @tatatodd Would you available to review this? Thank you in advance.\n", "Sorry, I'm not a good reviewer for this, since I don't actually know Python that well.\n\nRe-assigning to @mrry since it seems related to stuff that he's worked on.\n", "@mrry Sorry for the delay, I have addressed your comments and add new test cases, would you help to review this again? Thank you in advance.\n", "Any updates @haosdent ?\n", "Hi @vrv Sorry for the delay, I am in vacation recently and would back to this after 3 days.\n", "@haosdent When would you be able to update this?\n", "@haosdent Any update? If there isn't any activity on this soon I will likely close the PR. ", "I'm going to close this PR due to inactivity, and because it still needs quite a lot of work before it'd be ready to merge. Feel free to open a new PR if you recommence working on this, and thanks for contributing!"]}, {"number": 4859, "title": "incorrect gradient for complex matmul", "body": "The `MatMul` kernel is registered for `complex64` and `complex128` on CPU and GPU. However the gradient is incorrect. This issue has not been caught because there is no test for a complex-valued `MatMul` gradient in `tensorflow\\python\\kernel_tests\\matmul_op_test.py`.\n\nWhen computing the complex gradient we need to apply the conjugate of the input. Several cwise gradient ops in TF were ignoring this step until it was addressed by @girving and @tensorflower-gardener in https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f. However it seems that `MatMul` snuck through the cracks.\n\n**To Reproduce**\n\n``` python\nops.reset_default_graph()\nsess = tf.Session()\nx = (np.linspace(-3, 3, 6) + 1j*np.linspace(3, -3, 6)).reshape(3, 2)\ny = (np.linspace(-3, 3, 8) + 1j*np.linspace(3, -3, 8)).reshape(2, 4)\nwith sess.as_default():\n    with tf.device('/cpu'):\n        x_tf = tf.constant(x, dtype=tf.complex64, name=\"x\")\n        y_tf = tf.constant(y, dtype=tf.complex64, name=\"y\")\n        m = tf.matmul(x_tf, y_tf, name=\"matmul\")\n        err = tf.test.compute_gradient_error(x_tf, [3, 2], m, [3, 4])\n        print(err)\n\nwith sess.as_default():\n    with tf.device('/gpu'):\n        x_tf = tf.constant(x, dtype=tf.complex64, name=\"x\")\n        y_tf = tf.constant(y, dtype=tf.complex64, name=\"y\")\n        m = tf.matmul(x_tf, y_tf, name=\"matmul\")\n        err = tf.test.compute_gradient_error(x_tf, [3, 2], m, [3, 4])\n        print(err)\n```\n\n``` python\n>>> 6.00002098083\n>>> 6.00002098083\n```\n\n**Proposed Fix**\nReplace [these lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L697-L714) with:\n\n``` python\n@ops.RegisterGradient(\"MatMul\")\ndef _MatMulGrad(op, grad):\n  with ops.control_dependencies([grad.op]):\n      inp0 = math_ops.conj(op.inputs[0])\n      inp1 = math_ops.conj(op.inputs[1])\n      t_a = op.get_attr(\"transpose_a\")\n      t_b = op.get_attr(\"transpose_b\")\n      if not t_a and not t_b:\n        return (math_ops.matmul(grad, inp1, transpose_b=True),\n                math_ops.matmul(inp0, grad, transpose_a=True))\n      elif not t_a and t_b:\n        return (math_ops.matmul(grad, inp1),\n                math_ops.matmul(grad, inp0, transpose_a=True))\n      elif t_a and not t_b:\n        return (math_ops.matmul(op.inp1, grad, transpose_b=True),\n                math_ops.matmul(op.inp0, grad))\n      elif t_a and t_b:\n        return (math_ops.matmul(op.inp1, grad, transpose_a=True,\n                                transpose_b=True),\n                math_ops.matmul(grad, op.inp0, transpose_a=True,\n                                transpose_b=True))\n```\n\nand add a complex gradient test to `tensorflow\\python\\kernel_tests\\matmul_op_test.py`.\n", "comments": ["I'm not sure I am thinking about this correctly. I've noticed from a previous issue that I posted (https://github.com/tensorflow/tensorflow/issues/2516) that the numerical and theoretical gradients provided by `tf.test.compute_gradient_error` should not be the same in the case of complex ops. I'll close this issue and reopen later if necessary.\n", "I've considered my question a bit more and I believe this issue should remain open. I would love to hear someone weigh in (perhaps @ibab, who has been involved in resolving several TF issues for complex computations).\n\nAs noted above, the recent commit  https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f by @girving and @tensorflower-gardener changed the python-computed gradients for several complex analytic functions. I believe that `tf.matmul` needs a similar upgrade. As evidence, TF returns different gradients for scalar `matmul` and scalar `mul`, so one of them must be incorrect, right?\n\n``` python\nsess = tf.Session()\nx = (np.random.randn(1,1) + 1j*np.random.randn(1,1)).astype(np.complex64)\ny = (np.random.randn(1,1) + 1j* np.random.randn(1,1)).astype(np.complex64)\nwith sess.as_default():\n    x_tf = tf.constant(x)\n    y_tf = tf.constant(y) \n    f_mul = x_tf * y_tf\n    f_matmul = tf.matmul(x_tf, y_tf)\n    assert f_mul.eval() == f_matmul.eval()\n    print('df/dx computed using tf.mul:\\n\\t{}'.format(sess.run(tf.gradients(f_mul, x_tf))))\n    print('df/dx computed using tf.matmul:\\n\\t{}'.format(sess.run(tf.gradients(f_matmul, x_tf))))\n```\n\n``` python\n>>> df/dx computed using tf.mul:\n        [array([[ 2.16470551+0.75122446j]], dtype=complex64)]\n>>> df/dx computed using tf.matmul:\n    [array([[ 2.16470551-0.75122446j]], dtype=complex64)]\n```\n", "@woodshop Thank you for putting the effort into creating a very readable issue.\n\n@girving Could you please take a look into this one?\n", "@jart On leave at the moment.\n", "@ebrevdo can you take a look on this since @girving is not available right now?\n", "As a follow-up: I confirmed empirically that the complex gradient is incorrect and I have been using `graph.gradient_override_map` to successfully train my models. I'll be happy to create a pull request with the solution proposed above if a googler agrees. \n", "It appears that this issue was recently fixed by 3fe917bfb116ecebdd73de632c18cef6a1a04d9c. I'll leave it open until a googler confirms.\r\n", "I believe the referred commit does fix the issue."]}, {"number": 4858, "title": "macbook pro GPU version, successfully opened CUDA library, but not found the GPU", "body": "Hi ALL:\nI am a chinese student, so may be English is bas. Sorry.\nnow I use the tensorflow GPU version. I user the pip to down the python3 GPU version, and down the all about the GPU file.\nBut i can successfully opened CUDA library, but not found the GPU, like this:\n`In [1]: import tensorflow as tf\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.1.dylib locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.dylib locally\n`\n\n`\nIn [2]: tf.Session()\nE tensorflow/stream_executor/cuda/cuda_driver.cc:491] failed call to cuInit: CUDA_ERROR_NO_DEVICE\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:153] retrieving CUDA diagnostic information for host: wangxiaoweideWindows.local\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:160] hostname: wangxiaoweideWindows.local\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:185] libcuda reported version is: 346.3.6\nI tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] kernel reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"\"\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:81] No GPU devices available on machine.\n`\n\nand I find the I can't deviceQuery the GPU:\n\n`\u279c  ~ nvcc -V\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2015 NVIDIA Corporation\nBuilt on Mon_Apr_11_13:23:40_CDT_2016\nCuda compilation tools, release 7.5, V7.5.26\n`\n\n`\n\u279c  ~ ~/cuda-samples/bin/x86_64/darwin/release/deviceQuery\n/Users/codeMan/cuda-samples/bin/x86_64/darwin/release/deviceQuery Starting...\n CUDA Device Query (Runtime API) version (CUDART static linking)\ncudaGetDeviceCount returned 38\n-> no CUDA-capable device is detected\nResult = FAIL`\n\nnow I use the mac os, and Xcode8, how to solve it ?\n", "comments": ["If you can't run deviceQuery, then the problem is not with TensorFlow. Perhaps your computer does not have a CUDA-capable GPU\n", "@yaroslavvb \nMy macbook pro 2014 have NVIDIA 750M, and I find the mac GPU course to download the library.\nso how to solve? I don't know. THX\n", "OK. I solve this problem, is easy:NVIDIA CUDA 7.5 FOR MAC OS X RELEASE\ndownload this, then OK!!!!!!\nhttp://www.nvidia.com/object/macosx-cuda-7.5.30-driver.html\n\n@yaroslavvb THX!\n", "Glad you solved the problem @wwxFromTju. We hope TensorFlow serves you well in your studies. If you have any other questions in the future, the [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow) community is a better forum for support. We try to keep this issue tracker focused on bugs and feature requests.\n", "Thanks, That helps me.\n", "Hello @jart @wwxFromTju \r\n\r\nI have similar problem as yours.:\r\n\r\n**_Any help appreciated!! thanks!_**\r\n\r\n**\"libcuda reported version is: 310.42.25; \r\nkernel reported version is: Invalid argument: expected %d.%d or %d.%d.%d form for driver version; got \"\"**\r\nMy detailed problem is here: https://github.com/tensorflow/tensorflow/issues/2882\r\n\r\nIn brief: CUDA Driver (from Apple > System Preferences > CUDA)\r\n```\r\nCUDA Driver Version: 8.0.57\r\nGPU Driver Version: 10.10.14 310.42.25f02\r\n```\r\n\r\nMy setup:\r\nI'm on Mac OS X 10.11.6. Installed CUDA 8, and CUDNN 5.1. I've followed the install instructions from official TF site. (as of Jan 23, 2017)\r\n\r\n```\r\n./deviceQuery \r\n./deviceQuery Starting...\r\n\r\n CUDA Device Query (Runtime API) version (CUDART static linking)\r\n\r\ncudaGetDeviceCount returned 38\r\n-> no CUDA-capable device is detected\r\nResult = FAIL\r\n```", "@laventura sounds like you don't have CUDA-capable device (if your video card is AMD, it is not CUDA-capable)", "@yaroslavvb \r\n\r\nHere are the reports from System Report:  _(About This Mac > System Report > Graphics )_\r\n\r\nSo - _does this support CUDA or does it not? I 'm at a loss to figure this out now._ \r\n\r\n```\r\nNVIDIA GeForce GT 750M:\r\n  Chipset Model:\tNVIDIA GeForce GT 750M\r\n  Type:\tGPU\r\n  Bus:\tPCIe\r\n  PCIe Lane Width:\tx8\r\n  VRAM (Total):\t2048 MB\r\n  Vendor:\tNVIDIA (0x10de)\r\n  Device ID:\t0x0fe9\r\n  Revision ID:\t0x00a2\r\n  ROM Revision:\t3776\r\n  gMux Version:\t4.0.8 [3.2.8]\r\n  Displays:\r\nColor LCD:\r\n  Display Type:\tRetina LCD\r\n  Resolution:\t2880 x 1800 Retina\r\n  Retina:\tYes\r\n  Pixel Depth:\t32-Bit Color (ARGB8888)\r\n  Main Display:\tYes\r\n  Mirror:\tOff\r\n  Online:\tYes\r\n  Built-In:\tYes\r\n\r\n---- \r\nIntel Iris Pro:\r\n  Chipset Model:\tIntel Iris Pro\r\n  Type:\tGPU\r\n  Bus:\tBuilt-In\r\n  VRAM (Dynamic, Max):\t1536 MB\r\n  Vendor:\tIntel (0x8086)\r\n  Device ID:\t0x0d26\r\n  Revision ID:\t0x0008\r\n  gMux Version:\t4.0.8 [3.2.8]\r\n```", "Yes it should. It seems your problem is outside of tensorflow since devicequery fails, maybe Nvidia support forums have tips", "I've been running between CUDA/GPU and Tensorflow issues... \ud83d\ude29\ud83d\ude29\r\n\r\n**_TensorFlow on GPU worked for me earlier,_** with an older version of TensorFlow (0.10? cant recall) and   perhaps older CUDA too (also can't recall) \r\n\r\nI've seen the networks train faster with this same GPU... \r\n\r\nUnfortunately, upgrading TF led to many of these errors.. and now I can't figure out if it is GPU/CUDA problem or TF.\r\n\r\nOf course, nobody in the Nvidia DevTalk forums have had similar issues... \ud83d\ude1f\r\n\r\n[The older problem was that TF couldn't use all the GPU memory....]\r\n\r\n(See here: https://github.com/aymericdamien/TensorFlow-Examples/issues/38#issuecomment-265595019) ", "`./deviceQuery` failing indicates this is is a general GPU/CUDA problem, not TF problem", "This message was created automatically by mail delivery software.\n\nA message that you sent could not be delivered to one or more of its\nrecipients. This is a temporary error. The following address(es) deferred:\n\n  acmeideal@gmail.com\n    Domain biomassiv.es has exceeded the max emails per hour (122/100 (122%)) allowed.  Message will be reattempted later\n\n------- This is a copy of the message, including all the headers. ------\n------ The body of the message is 5256 characters long; only the first\n------ 5000 or so are included here.\nReceived: from github-smtp2-ext4.iad.github.net ([192.30.252.195]:48764 helo=github-smtp2a-ext-cp1-prd.iad.github.net)\n\tby chi-server32.websitehostserver.net with esmtps (TLSv1.2:ECDHE-RSA-AES256-GCM-SHA384:256)\n\t(Exim 4.87)\n\t(envelope-from <noreply@github.com>)\n\tid 1cWmia-0032g0-Op\n\tfor greg@biomassiv.es; Thu, 26 Jan 2017 10:16:12 -0600\nDate: Wed, 25 Jan 2017 13:25:14 -0800\nDKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=github.com;\n\ts=pf2014; t=1485379514;\n\tbh=6z/AkFDojFc8QDhBXTHoeUbzRf+AWSxwJCLdajE99mk=;\n\th=From:Reply-To:To:Cc:In-Reply-To:References:Subject:List-ID:\n\t List-Archive:List-Post:List-Unsubscribe:From;\n\tb=iYzUvQiiIlFeLCLDQslc/bdByhGKReWQ0ncUmNl/48FWVUBdFdEHDGYU83YoU3dzU\n\t PLH5UC7CG6J9Kbr99g7NxOZsdeH/BDN5DxKCL+3txE83r0vj5shuDzcVgOwwn2Du6R\n\t pp9I1AUWA3yEjsTO2CxyxCXetJ8lH3QAb0sbrHrw=\nFrom: Atul Acharya <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: Subscribed <subscribed@noreply.github.com>\nMessage-ID: <tensorflow/tensorflow/issues/4858/275237366@github.com>\nIn-Reply-To: <tensorflow/tensorflow/issues/4858@github.com>\nReferences: <tensorflow/tensorflow/issues/4858@github.com>\nSubject: Re: [tensorflow/tensorflow] macbook pro GPU version, successfully\n opened CUDA library, but not found the GPU (#4858)\nMime-Version: 1.0\nContent-Type: multipart/alternative;\n boundary=\"--==_mimepart_588917ba96a54_1d6c3fa0f1c2f138236643\";\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\nPrecedence: list\nX-GitHub-Sender: laventura\nX-GitHub-Recipient: biomassives\nX-GitHub-Reason: subscribed\nList-ID: tensorflow/tensorflow <tensorflow.tensorflow.github.com>\nList-Archive: https://github.com/tensorflow/tensorflow\nList-Post: <mailto:reply@reply.github.com>\nList-Unsubscribe: <mailto:unsub+0042d4e279cd4dd9047c8c056d31277adda34ea1accfdc7f92cf0000000114a0d9ba92a169ce0ad7918e@reply.github.com>,\n <https://github.com/notifications/unsubscribe/AELU4gthhf9cfFjfk3zWNvG6DixiDos0ks5rV726gaJpZM4KSDvL>\nX-Auto-Response-Suppress: All\nX-GitHub-Recipient-Address: greg@biomassiv.es\n\n\n----==_mimepart_588917ba96a54_1d6c3fa0f1c2f138236643\nContent-Type: text/plain;\n charset=UTF-8\nContent-Transfer-Encoding: 7bit\n\n@yaroslavvb \n\nHere are the reports from System Report:  _(About This Mac > System Report > Graphics )_\n\nSo - _does this support CUDA or does it not? I 'm at a loss to figure this out now._ \n\n```\nNVIDIA GeForce GT 750M:\n  Chipset Model:\tNVIDIA GeForce GT 750M\n  Type:\tGPU\n  Bus:\tPCIe\n  PCIe Lane Width:\tx8\n  VRAM (Total):\t2048 MB\n  Vendor:\tNVIDIA (0x10de)\n  Device ID:\t0x0fe9\n  Revision ID:\t0x00a2\n  ROM Revision:\t3776\n  gMux Version:\t4.0.8 [3.2.8]\n  Displays:\nColor LCD:\n  Display Type:\tRetina LCD\n  Resolution:\t2880 x 1800 Retina\n  Retina:\tYes\n  Pixel Depth:\t32-Bit Color (ARGB8888)\n  Main Display:\tYes\n  Mirror:\tOff\n  Online:\tYes\n  Built-In:\tYes\n\n---- \nIntel Iris Pro:\n  Chipset Model:\tIntel Iris Pro\n  Type:\tGPU\n  Bus:\tBuilt-In\n  VRAM (Dynamic, Max):\t1536 MB\n  Vendor:\tIntel (0x8086)\n  Device ID:\t0x0d26\n  Revision ID:\t0x0008\n  gMux Version:\t4.0.8 [3.2.8]\n```\n\n-- \nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/tensorflow/tensorflow/issues/4858#issuecomment-275237366\n----==_mimepart_588917ba96a54_1d6c3fa0f1c2f138236643\nContent-Type: text/html;\n charset=UTF-8\nContent-Transfer-Encoding: quoted-printable\n\n<p><a href=3D\"https://github.com/yaroslavvb\" class=3D\"user-mention\">@yaro=\nslavvb</a></p>\n<p>Here are the reports from System Report:  <em>(About This Mac &gt; Sys=\ntem Report &gt; Graphics )</em></p>\n<p>So - <em>does this support CUDA or does it not? I 'm at a loss to figu=\nre this out now.</em></p>\n<pre><code>NVIDIA GeForce GT 750M:\n  Chipset Model:\tNVIDIA GeForce GT 750M\n  Type:\tGPU\n  Bus:\tPCIe\n  PCIe Lane Width:\tx8\n  VRAM (Total):\t2048 MB\n  Vendor:\tNVIDIA (0x10de)\n  Device ID:\t0x0fe9\n  Revision ID:\t0x00a2\n  ROM Revision:\t3776\n  gMux Version:\t4.0.8 [3.2.8]\n  Displays:\nColor LCD:\n  Display Type:\tRetina LCD\n  Resolution:\t2880 x 1800 Retina\n  Retina:\tYes\n  Pixel Depth:\t32-Bit Color (ARGB8888)\n  Main Display:\tYes\n  Mirror:\tOff\n  Online:\tYes\n  Built-In:\tYes\n\n---- =\n\nIntel Iris Pro:\n  Chipset Model:\tIntel Iris Pro\n  Type:\tGPU\n  Bus:\tBuilt-In\n  VRAM (Dynamic, Max):\t1536 MB\n  Vendor:\tIntel (0x8086)\n  Device ID:\t0x0d26\n  Revision ID:\t0x0008\n  gMux Version:\t4.0.8 [3.2.8]\n</code></pre>\n\n<p style=3D\"font-size:small;-webkit-text-size-adjust:none;color:#666;\">&m=\ndash;<br />You are receiving this because you are subscribed to this thre=\nad.<br />Reply to this email directly, <a href=3D\"https://github.com/tens=\norflow/tensorflow/issues/4858#issuecomment-275237366\">view it on GitHub</=\na>, or <a href=3D\"https://github.com/notifications/unsubscribe-auth/AELU4=\nuDLkb61KX4qOI8UzJO2Lek2Cddrks5rV726gaJpZM4KSDvL\">mute the thread</a>.<img=\n alt=3D\"\" height=3D\"1\" src=3D\"https://github.com/notifications/beacon/AEL=\nU4nyDls2guNQ53qI1MpYHuHWcDVQ3ks5rV726gaJpZM4KSDvL.gif\" width=3D\"1\" /></p>=\n\n<div itemscope itemtype=3D\"http://schema.org/EmailMessage\">\n<div itemprop=3D\"action\" itemscope itemtype=3D\"http://schema.org/ViewActi=\non\">\n  <link itemprop=3D\"url\" href=3D\"https://github.com/tensorflow/tensorflow=\n/issues/4858#issuecomment-275237366\"></link>\n  <meta itemprop=3D\"name\" content=3D\"View Issue\"></meta>\n</div>\n<meta itemprop=3D\"description\" content=3D\"View this Issue on GitHub\"></me=\nta>\n</div>\n\n<script type=3D\"application/json\" data-scope=3D\"inboxmarkup\">{\"api_versio=\nn\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\"=\n:\"GitHub\"},\"entity\":{\"external_key\":\"github/tensorflow/tensorflow\",\"title=\n\":\"tensorflow/tensorflow\",\"subtitle\":\"GitHub repository\",\"main_image_url\"=\n:\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d8=\n8-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubuse=\nrcontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.=\npng\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/tensorfl=\now/tensorflow\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@lave=\nntura in #4858: @yaroslavvb \\r\\n\\r\\nHere are the reports from System Repo=\nrt:  _(About This Mac \\u003e System Report \\u003e Graphics )_\\r\\n\\r\\nSo -=\n _does this support CUDA or does it not? I 'm at a loss to figure this ou=\nt now._ \\r\\n\\r\\n```\\r\\nNVIDIA GeForce GT 750M:\\r\\n  Chipset Model:\\tNVIDI=\nA GeForce GT 750M\\r\\n  Type:\\tGPU\\r\\n  Bus:\\tPCIe\\r\\n  PCIe Lane Width:\\t=\nx8\\r\\n  VRAM (Total):\\t2048 MB\\r\\n  Vendor:\\tNVIDIA (0x10de)\\r\\n  Device =\nID:\\t0x0fe9\\r\\n  Revision ID:\\t0x00a2\\r\\n  ROM Revision:\\t3776\\r\\n  gMux =\nVersion:\\t4.0.8 [3.2.8]\\r\\n  Displays:\\r\\nColor LCD:\\r\\n  Display Type:\\t=\nRetina LCD\\r\\n  Resolution:\\t2880 x 1800 Retina\\r\\n  Retina:\\tYes\\r\\n  Pi=\nxel Depth:\\t32-Bit Color (ARGB8888)\\r\\n  Main Display:\\tYes\\r\\n  Mirror:\\=\ntOff\\r\\n  Online:\\tYes\\r\\n  Built-In:\\tYes\\r\\n\\r\\n---- \\r\\nIntel Iris Pro=\n:\\r\\n  Chipset Model:\\tIntel Iris Pro\\r\\n  Type:\\tGPU\\r\\n  Bus:\\tBuilt-In=\n\\r\\n  VRAM (Dynamic, Max):\\t1536 MB\\r\\n  Vendor:\\tIntel (0x8086)\\r\\n  Dev=\nice ID:\\t0x0d26\\r\n", "@biomassives you seem to have started spamming github threads, so I have blocked you.  Please contact us offline once you've fixed whatever you're doing to stop spamming threads.", "@laventura hello, I'm running exactly the same problem as you, with `./deviceQuery` failing. Did you solve that problem? Thanks!", "@laventura Hello, have you solved your problem? I have run into exactly the same problem, with `./deviceQuery` failing indicates the CUDA-capable device, but I follow the instruction on [here](https://gist.github.com/jganzabal/8e59e3b0f59642dd0b5f2e4de03c7687). Thanks!"]}, {"number": 4857, "title": "tf.extract_image_patches Trying stride only on row", "body": "Hi, Tried to use tf.extract_image_patche() of a tensor [N, sequence_length, embeding_size, 1] to do n-gram with patch size [1,sequence_length-#gram+1, embedding_size, 1]. Thus I set the stride to be [1,1,0,1]. However this leds to the error: `ZeroDivisionError: integer division or modulo by zero`\n\nIs there a way to only stride on one dim and avoid this error?\n\nbelow is the skeleton of my code and error message:\n\n``` (python)\nself.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\nself.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\nslic = tf.(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')\n```\n\n```\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n/home/zijia/nlp/A2/train.py in <module>()\n     85             embedding_size=FLAGS.embedding_dim,\n     86             n_gram=FLAGS.n_gram,\n---> 87             l2_reg_lambda=FLAGS.l2_reg_lambda)\n     88\n     89         # Define Training procedure\n\n/home/zijia/nlp/A2/text_cnn.py in __init__(self, sequence_length, num_classes, vocab_size, embedding_size, n_gram, l2_reg_lambda)\n     32                 grams = [ self.embedded_chars_expanded ]\n     33                 for i in range(2,n_gram+1):\n---> 34                     slic = tf.extract_image_patches(self.embedded_chars_expanded, [1,i,embedding_size,1], [1,1,0,1], [1,1,1,1], 'VALID')\n     35                     print slic.get_shape()\n     36                     slic = tf.reduce_sum( slic, 3 )\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc in extract_image_patches(images, ksizes, strides, rates, padding, name)\n    918   result = _op_def_lib.apply_op(\"ExtractImagePatches\", images=images,\n    919                                 ksizes=ksizes, strides=strides, rates=rates,\n--> 920                                 padding=padding, name=name)\n    921   return result\n    922\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    701           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    702                            input_types=input_types, attrs=attr_protos,\n--> 703                            op_def=op_def)\n    704           outputs = op.outputs\n    705           return _Restructure(ops.convert_n_to_tensor(outputs),\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   2317                     original_op=self._default_original_op, op_def=op_def)\n   2318     if compute_shapes:\n-> 2319       set_shapes_for_outputs(ret)\n   2320     self._add_op(ret)\n   2321     self._record_op_seen_by_control_dependencies(ret)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1709       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1710                          % op.type)\n-> 1711   shapes = shape_func(op)\n   1712   if shapes is None:\n   1713     raise RuntimeError(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc in _ExtractImagePatchesShape(op)\n   2290                                                             ksize_c_eff,\n   2291                                                             stride_r, stride_c,\n-> 2292                                                             padding)\n   2293\n   2294   out_depth = None if in_depth is None else ksize_r * ksize_c * int(in_depth)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get2d_conv_output_size(input_height, input_width, filter_height, filter_width, row_stride, col_stride, padding_type)\n    182   return get_conv_output_size((input_height, input_width),\n    183                               (filter_height, filter_width),\n--> 184                               (row_stride, col_stride), padding_type)\n    185\n    186\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in get_conv_output_size(input_size, filter_size, strides, padding_type)\n    159     output_size = [\n    160         _valid(in_dim, k_dim, s_dim)\n--> 161         for in_dim, k_dim, s_dim in zip(input_size, filter_size, strides)\n    162     ]\n    163   elif padding_type == b\"SAME\":\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc in _valid(in_dim, k_dim, s_dim)\n    153     def _valid(in_dim, k_dim, s_dim):\n    154       if in_dim is not None and k_dim is not None:\n--> 155         return (in_dim - k_dim + s_dim) // s_dim\n    156       else:\n    157         return None\n\nZeroDivisionError: integer division or modulo by zero\n```\n", "comments": ["strides: A list of ints that has length >= 4. 1-D of length 4. How far the centers of two consecutive patches are in the images. Must be: [1, stride_rows, stride_cols, 1].\n\nWhy would you set stride_cols to 0?\n", "Because I want to patch only move on rows. What's the right way to do so?\n", "Does setting stride_cols to 1 work?\n\nThis question might actually be more appropriate for the [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) community. There's a lot of amazing people on there who offer community-driven support we don't have the cycles to give. We try to keep this issue tracker focused on bugs and feature requests.\n", "I have a similar issue @ZijiaLewisLu: were you able to find a way to do this?"]}, {"number": 4856, "title": "TensorBoard charts blank on Firefox", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\n\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI have not found any related issues\n### Environment info\n\nOperating System:\n\nUbuntu 16.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n\nrik@rik-MS-7971:~$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Sep 23 09:09 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 23 09:09 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 23 09:09 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Sep 23 09:09 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Sep 23 09:09 /usr/local/cuda/lib64/libcudart_static.a\nlrwxrwxrwx 1 rik  rik        13 Jul 27 01:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 rik  rik        17 Jul 27 01:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxrwxr-x 1 rik  rik  79337624 Sep 26 21:55 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-rw-r-- 1 rik  rik  69756172 Sep 26 21:55 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\nrik@rik-MS-7971:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n0.11.0rc0\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n### What other attempted solutions have you tried?\n\nTried erasing files from logdir and re-running. Tried running mnist with summaries tutorial, this is also blank charts\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n---\n\nAll, when running [MNIST with Summaries example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py) charts exist on events and distribution tabs, they are blank. Histograms and appear correct. Also, code is running slow compated to non-tensorboard version. Please help, Thanks. Below is output.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.797\npciBusID 0000:01:00.0\nTotal memory: 7.92GiB\nFree memory: 7.36GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)\nAccuracy at step 0: 0.0802\nAccuracy at step 10: 0.6788\nAccuracy at step 20: 0.8217\nAccuracy at step 30: 0.8495\nAccuracy at step 40: 0.8734\nAccuracy at step 50: 0.8794\nAccuracy at step 60: 0.8847\nAccuracy at step 70: 0.8863\nAccuracy at step 80: 0.8898\nAccuracy at step 90: 0.8939\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcupti.so locally\nAdding run metadata for 99\nAccuracy at step 100: 0.9015\nAccuracy at step 110: 0.9132\nAccuracy at step 120: 0.9163\nAccuracy at step 130: 0.9197\nAccuracy at step 140: 0.9252\nAccuracy at step 150: 0.9182\nAccuracy at step 160: 0.9283\nAccuracy at step 170: 0.9243\nAccuracy at step 180: 0.9231\nAccuracy at step 190: 0.9266\nAdding run metadata for 199\nAccuracy at step 200: 0.9327\nAccuracy at step 210: 0.9336\nAccuracy at step 220: 0.9329\nAccuracy at step 230: 0.9311\nAccuracy at step 240: 0.9308\nAccuracy at step 250: 0.9214\nAccuracy at step 260: 0.9324\nAccuracy at step 270: 0.9393\nAccuracy at step 280: 0.9327\nAccuracy at step 290: 0.9401\nAdding run metadata for 299\nAccuracy at step 300: 0.9442\nAccuracy at step 310: 0.9405\nAccuracy at step 320: 0.9424\nAccuracy at step 330: 0.9425\nAccuracy at step 340: 0.9468\nAccuracy at step 350: 0.9461\nAccuracy at step 360: 0.9427\nAccuracy at step 370: 0.9434\nAccuracy at step 380: 0.9474\nAccuracy at step 390: 0.9449\nAdding run metadata for 399\nAccuracy at step 400: 0.9443\nAccuracy at step 410: 0.9449\nAccuracy at step 420: 0.9474\nAccuracy at step 430: 0.9455\nAccuracy at step 440: 0.952\nAccuracy at step 450: 0.9505\nAccuracy at step 460: 0.9525\nAccuracy at step 470: 0.9493\nAccuracy at step 480: 0.9489\nAccuracy at step 490: 0.9515\nAdding run metadata for 499\nAccuracy at step 500: 0.9517\nAccuracy at step 510: 0.9515\nAccuracy at step 520: 0.9499\nAccuracy at step 530: 0.9539\nAccuracy at step 540: 0.9548\nAccuracy at step 550: 0.9573\nAccuracy at step 560: 0.9548\nAccuracy at step 570: 0.9569\nAccuracy at step 580: 0.9552\nAccuracy at step 590: 0.9548\nAdding run metadata for 599\nAccuracy at step 600: 0.9551\nAccuracy at step 610: 0.9535\nAccuracy at step 620: 0.96\nAccuracy at step 630: 0.9599\nAccuracy at step 640: 0.9603\nAccuracy at step 650: 0.9616\nAccuracy at step 660: 0.9617\nAccuracy at step 670: 0.9623\nAccuracy at step 680: 0.9613\nAccuracy at step 690: 0.9614\nAdding run metadata for 699\nAccuracy at step 700: 0.9592\nAccuracy at step 710: 0.9614\nAccuracy at step 720: 0.9595\nAccuracy at step 730: 0.9646\nAccuracy at step 740: 0.9636\nAccuracy at step 750: 0.964\nAccuracy at step 760: 0.9656\nAccuracy at step 770: 0.9616\nAccuracy at step 780: 0.9631\nAccuracy at step 790: 0.964\nAdding run metadata for 799\nAccuracy at step 800: 0.9637\nAccuracy at step 810: 0.9637\nAccuracy at step 820: 0.9653\nAccuracy at step 830: 0.9636\nAccuracy at step 840: 0.9615\nAccuracy at step 850: 0.9654\nAccuracy at step 860: 0.9668\nAccuracy at step 870: 0.9653\nAccuracy at step 880: 0.965\nAccuracy at step 890: 0.9645\nAdding run metadata for 899\nAccuracy at step 900: 0.9664\nAccuracy at step 910: 0.967\nAccuracy at step 920: 0.9658\nAccuracy at step 930: 0.9664\nAccuracy at step 940: 0.9671\nAccuracy at step 950: 0.967\nAccuracy at step 960: 0.9663\nAccuracy at step 970: 0.968\nAccuracy at step 980: 0.9679\nAccuracy at step 990: 0.9666\nAdding run metadata for 999\n```\n\n---\n", "comments": ["I'm not sure if this is the same issue, but I had blank (black images) in place of the events and distribution plots. This occurred with Firefox version 49.0, when I tried in chrome version 54.0.2840.59 (64-bit), the events and distributions plots would display correctly. \n\nThe 3d histogram plots displayed correctly in both firefox and chrome. Did not test in other browsers.\n\nImages from firefox 49.0 below:\n![screenshot from 2016-10-12 13-56-25](https://cloud.githubusercontent.com/assets/9807648/19327449/f3e7915a-9083-11e6-83d5-47a4caafd66b.png)\n![screenshot from 2016-10-12 13-56-53](https://cloud.githubusercontent.com/assets/9807648/19327450/f4095542-9083-11e6-99bc-0898bbc36398.png)\n", "Thank you both for reporting. @stepseazy Are you using Firefox? Where did you get TensorBoard? Bazel at HEAD? Or pip install? I'd also be interested in knowing the same from @alexbeloi. \n\nCC: @danmane \n", "@jart : I have installed with pip install\n\ntensorflow version 0.11.0rc0\nnot sure how to get tensorboard version if it is independent from tensorflow\n", "Yes Jart, I was indeed using Firefox. It's the only browser I have installed in ubuntu right now so while I saw alex's response (thanks alex) I haven't tried another browser yet but I'll report back shortly. I believe I used pip install to install tensorboard...\n", "I ssem to be having another issue now and I can't use tensorboard on either browser. Tensorboard can't seem to find these files that exist but in a different location. For example the first file exists in /home/rik/anaconda3/lib/python3.5/site-packages/external/webcomponentsjs but not in /home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/tensorboard/webcomponentsjs/\n\n```\nStarting TensorBoard b'29' on port 6006\n(You can navigate to http://127.0.1.1:6006)\n127.0.0.1 - - [14/Oct/2016 09:03:41] \"GET / HTTP/1.1\" 200 -\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js' on path /home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/tensorboard/webcomponentsjs/webcomponents-lite.min.js\n127.0.0.1 - - [14/Oct/2016 09:03:41] \"GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [14/Oct/2016 09:03:41] \"GET /lib/css/global.css HTTP/1.1\" 200 -\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/tensorboard/dist/bazel-html-imports.html' on path /home/rik/anaconda3/lib/python3.5/site-packages/tensorflow/tensorboard/dist/bazel-html-imports.html\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/home/rik/anaconda3/lib/python3.5/site-packages/external/dist/bazel-html-imports.html' on path /home/rik/anaconda3/lib/python3.5/site-packages/external/dist/bazel-html-imports.html\n\n```\n", "Good to know. We're definitely going to make it a priority to fix the Firefox support. As for the IOErrors, you probably ran into the same issue as #4830. This was just fixed internally and will be pushed to GitHub soon, if it hasn't been already.\n", "I used this to install:\n\n```\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp35-cp35m-linux_x86_64.whl\nsudo pip3 install --upgrade $TF_BINARY_URL\n```\n\nfrom [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md). I'm assuming this is what will be updated?\n", "Yes it will be updated in time. But it would help us if you could test out the repository and HEAD and let us know if it works.\n", "I'm having the same issue with tensorflow r0.11 bazel install. I get the black charts only when connecting from a different computer than the one running tensorboard (both local and remote are running firefox 49).\n", "Looks like Plottable CSS rules aren't getting applied for some reason. \nI'm surprised that it happens only when connected remotely. I would expect it to either always be missing or always be present. This is quite weird :< \n", "I met the same issue with r0.11rc1 (and  previously with r0.11rc0) \n\nDetails : \n- using  Ubuntu 14.04 with Nvidia GTX1080 (latest drivers, latest version of Cuda 8 and cuDnn5)\n- Install and build  from sources\n- testing with mnist_with_summaries.py\n\n`~/develop/tensorflow/tutorials/mnist$ python mnist_with_summaries.py \nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\nExtracting /tmp/data/train-images-idx3-ubyte.gz\nExtracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:04:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)\nAccuracy at step 0: 0.118\nAccuracy at step 10: 0.6865\nAccuracy at step 20: 0.81\n[...]\nAccuracy at step 980: 0.9673\nAccuracy at step 990: 0.967\nAdding run metadata for 999\n`  \n- then launch tensorboard : \n  `\n  ~/develop/tensorflow/tutorials/mnist$ tensorboard --logdir=/tmp/mnist_logs/\n  I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\n  I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so.5 locally\n  I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\n  I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\n  I tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\n  Starting TensorBoard 29 on port 6006\n  `\n- With Mac OS X 10.11.6 Le Capitan and Safari Version 10.0 (11602.1.50.0.10), I can display tensorborad. HIstograms, images are well displayed. However if I try do display Events, like accuracy, cross entropy, ... just a blanck/black image is displayed instead of the graphs\n- also an error in the console used to launch tensorboard\n  `127.0.0.1 - - [23/Oct/2016 12:07:27] \"GET / HTTP/1.1\" 200 -\n  127.0.0.1 - - [23/Oct/2016 12:07:28] \"GET /webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 200 -\n  127.0.0.1 - - [23/Oct/2016 12:07:28] code 404, message Not Found\n  127.0.0.1 - - [23/Oct/2016 12:07:28] \"GET /dist/bazel-html-imports.html HTTP/1.1\" 404 -\n  127.0.0.1 - - [23/Oct/2016 12:07:28] \"GET /lib/css/global.css HTTP/1.1\" 200 -\n  127.0.0.1 - - [23/Oct/2016 12:07:28] \"GET /dist/tf-tensorboard.html HTTP/1.1\" 200 -\n  127.0.0.1 - - [23/Oct/2016 12:07:28] \"GET /polymer/polymer.html HTTP/1.1\" 200 -\n  `\n\nand if I try to display Events/accuracy  and cross entropy there does not seem to have some errors \n`\n127.0.0.1 - - [23/Oct/2016 12:08:45] \"GET /data/scalars?run=test&tag=accuracy HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Oct/2016 12:08:45] \"GET /data/scalars?run=train&tag=accuracy HTTP/1.1\" 200 -\n\n127.0.0.1 - - [23/Oct/2016 12:09:11] \"GET /data/scalars?run=test&tag=cross%20entropy HTTP/1.1\" 200 -\n127.0.0.1 - - [23/Oct/2016 12:09:11] \"GET /data/scalars?run=train&tag=cross%20entropy HTTP/1.1\" 200 -\n`\n\nNote when testing **with previous release candidate r0.11rc0 , I met lots of IOError** (a few examples below). **I do NOT have this errors with r0.11rc1** : this must have been corrected.\n\n`WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/polymer/polymer.html' on path /usr/local/lib/python2.7/dis\nt-packages/tensorflow/tensorboard/polymer/polymer.html\n127.0.0.1 - - [20/Oct/2016 16:49:01] \"GET /polymer/polymer.html HTTP/1.1\" 200 -\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/iron-icons/iron-icons.html' on path /usr/local/lib/python2\n.7/dist-packages/tensorflow/tensorboard/iron-icons/iron-icons.html\n127.0.0.1 - - [20/Oct/2016 16:49:01] \"GET /iron-icons/iron-icons.html HTTP/1.1\" 200 -\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/paper-tabs/paper-tabs.html' on path /usr/local/lib/python2\n.7/dist-packages/tensorflow/tensorboard/paper-tabs/paper-tabs.html\n`\n", "More info : \n- problem occurs in Firefox 49.0.2 (Mac OS X El Capitan), Firefox 47.0.1 (Windows 7)\n- problem occurs in Safari Version 10.0 (11602.1.50.0.10) (Mac OS X El Capitan)\n- Events graphs are well displayed with Google Chrome 54.0.2840.71 (Window 7) \n\nNow looking in the Safari console, a javascript error occurs when trying to display the accuracy graphs : \n[Warning] Warning: Problem parsing viewBox=\"\" (d3.js, line 662)\n\n```\n    function attrConstant() {\n      this.setAttribute(name, value);          # <---- Problem parsing viewBox=\"\"\n    }\n```\n\nIn fact the same error occurs in Google Chrome but it seems more fault tolerant as the graph is well displayed in the end\n\n```\nd3.js:662 Error: <svg> attribute viewBox: Unexpected end of attribute. Expected number, \"\".\nattrConstant                   @ d3.js:662\n(anonymous function)   @ d3.js:961\nd3_selection_each         @ d3.js:967\nd3_selectionPrototype.each     @ d3.js:960\nd3_selectionPrototype.attr       @ d3.js:651\nLineChart.setViewBox  @ tf-tensorboard.html:3852\nLineChart.renderTo       @ tf-tensorboard.html:3841\n(anonymous function)   @ tf-tensorboard.html:4195\n(anonymous function)   @ polymer.html:1307\n```\n", "Same problem. \n\nTensorflow 0.11.0rc2 installed via pip\nFirefox 49.0.2\nOSX 10.12.1\n\nWorks fine on Chrome 54.0.2840.71\n", "Same problem.\r\n\r\nTensorflow 0.11.0rc1 installed via pip\r\nFirefox 49.0.2\r\nUbuntu 16.04\r\n\r\nwork fine on Chrome 54.0.x", "@Samurais Either upgrade to new version of tensorflow, or link the css file to the html i.e. apply the fix (https://github.com/tensorflow/tensorflow/pull/5755/files) yourself."]}, {"number": 4855, "title": "DeepDream tutorial Unsupervised", "body": "Is it possibile to have a unsupervised version of the DeepDream tutorial?\nAnd what kind of solution could be adopted? VAE? DCGAN? \nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream\n", "comments": ["What do you think to switch the notebook to MetaGraph? I see that fine tuning it is more oriented on tfslim this days and slim models use the new meta+check point.\n", "The last comment it is another topic so I have opened a new issue https://github.com/tensorflow/tensorflow/issues/4907\n", "Thanks for reaching out. I've passed word along.\n", "Automatically closing due to lack of recent activity. "]}, {"number": 4854, "title": "Can we ignore 'tensorflow.python.framework.errors.DataLossError: corrupted record at 0'", "body": "I will face this problem when reading tf records on hdfs, may be running one hour or two hours, then my program will crash down due to this assertion fail from tensorflow c++ code.\nCan we just ignore this, ie. ignore the batch with DataLossErro, so can continue training next batch without having to stop the program.\n", "comments": ["The problem is due to writting tfrecords without finally call close(). Close this.\n", "Can you paste example code for what you mean here?  That's why the thumbs are down.", "I meet the common problem,do you know how to ignore it", "The Same Problem"]}, {"number": 4853, "title": "Windows version of Tensorflow won't compile", "body": "@mrry \nThank you for your great work for the windows version of tensorflow. I've tried to build tensorflow on windows based on your tensorflow CMake build guide.  The only difference is that I didn't use anaconda. \n\nI've tried to build the tf_tutorials_example_trainer target. However it seems to depend on gif and png target, which are not able to get built. \n\nFor the gif target, it seems to be invoking a configure shell script which is not able to be executed in windows cmd. The error reads like:\n\n```\nPerforming download step (download, verify and extract) for 'gif'\n  -- verifying file...\n         file='C:/Users/v-liaha/Documents/GitHub/tensorflow/tensorflow/contrib/cmake/build/downloads/giflib-5.1.4.tar.gz'\n  -- File already exists and hash match (skip download):\n    file='C:/Users/v-liaha/Documents/GitHub/tensorflow/tensorflow/contrib/cmake/build/downloads/giflib-5.1.4.tar.gz'\n    SHA256='34a7377ba834397db019e8eb122e551a49c98f49df75ec3fcc92b9a794a4f6d1'\n  -- extracting...\n       src='C:/Users/v-liaha/Documents/GitHub/tensorflow/tensorflow/contrib/cmake/build/downloads/giflib-5.1.4.tar.gz'\n       dst='C:/Users/v-liaha/Documents/GitHub/tensorflow/tensorflow/contrib/cmake/build/gif/src/gif'\n  -- extracting... [tar xfz]\n  -- extracting... [analysis]\n  -- extracting... [rename]\n  -- extracting... [clean up]\n  -- extracting... done\n  No update step for 'gif'\n  No patch step for 'gif'\n  Performing configure step for 'gif'\n  'C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gif\\src\\gif\\configure' is not recognized as an internal or external command,\n  operable program or batch file.\nC:\\Program Files (x86)\\MSBuild\\Microsoft.Cpp\\v4.0\\V140\\Microsoft.CppCommon.targets(171,5): error MSB6006: \"cmd.exe\" exited with code 9009. [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\gif.vcxproj]\n```\n\nFor the png target, the error reads like:\n\n```\nC:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\n\n\n  \"C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\ALL_BUILD.vcxproj\" (default target) (1) ->\n  \"C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj\" (default target) (3) ->\n  (Link target) -> \npng.obj : error LNK2019: unresolved external symbol __imp_inflateReset referenced in function png_reset_zstream [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngrutil.obj : error LNK2001: unresolved external symbol __imp_inflateReset [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npng.obj : error LNK2019: unresolved external symbol __imp_crc32 referenced in function png_calculate_crc [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngpread.obj : error LNK2019: unresolved external symbol __imp_inflate referenced in function png_process_IDAT_data [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngread.obj : error LNK2001: unresolved external symbol __imp_inflate [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngrutil.obj : error LNK2001: unresolved external symbol __imp_inflate [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngread.obj : error LNK2019: unresolved external symbol __imp_inflateEnd referenced in function png_read_destroy [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngread.obj : error LNK2019: unresolved external symbol __imp_inflateInit_ referenced in function png_create_read_struct_2 [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngwrite.obj : error LNK2019: unresolved external symbol __imp_deflate referenced in function png_write_flush [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngwutil.obj : error LNK2001: unresolved external symbol __imp_deflate [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngwrite.obj : error LNK2019: unresolved external symbol __imp_deflateEnd referenced in function png_write_destroy [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngwutil.obj : error LNK2019: unresolved external symbol __imp_deflateReset referenced in function png_write_compressed_data_out [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\npngwutil.obj : error LNK2019: unresolved external symbol __imp_deflateInit2_ referenced in function png_write_IHDR [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\nC:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\Release\\libpng12.dll : fatal error LNK1120: 9 unresolved externals [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png\\src\\png-build\\png12.vcxproj] [C:\\Users\\v-liaha\\Documents\\GitHub\\tensorflow\\tensorflow\\contrib\\cmake\\build\\png.vcxproj]\n\n      51 Warning(s)\n      14 Error(s)\n\n  Time Elapsed 00:00:04.48\n  The command exited with code 1.\n```\n\nAnd I checked issue [#4798](https://github.com/tensorflow/tensorflow/issues/4798), @laudney and @elcombato seems not having such a problem. \n", "comments": ["We've only successfully built TensorFlow with Anaconda installed so far. It includes a version of zlib, on which libpng depends. Can you confirm whether the build works if you install zlib on your system? (You could do this, for example, by installing Anaconda.)\n\nWe should make it possible to build without depending on an external version of zlib. I'll investigate a quick fix.\n", "Thank you for the reply @mrry.  Actually I've resolved the ZLIB dependency by downloading a zlib for windows package online. The errors I'm getting previously occurs when there is no dependenciy errors. \n\nI've tried Anaconda3, and the png target is now successfully built. Anaconda3 has zlib 1.2.8, while the zlib for windows package I got is 1.2.3. I think that version mismatch caused unresolved external symbol errors previously.\n\nHowever, the gif target still won't build, and the same error persists that the configure shell script cannot be executed. Did you guys use mingw to build these target?\n", "This is strange. The CMake [external project definition](https://github.com/tensorflow/tensorflow/blob/df871edcff2faf643975b9863100ed41b6da9c3f/tensorflow/contrib/cmake/external/gif.cmake) for GIFLIB should only attempt to run when the CMake variable `WIN32` is false. The only supported build type on Windows (currently) is Visual Studio 14.0 projects, and this includes the GIFLIB dependency.\n\nIs it possible that you are using a version of CMake installed via CygWin? According to [this documentation](https://cmake.org/Wiki/CMake_Useful_Variables), after CMake 2.8.4 `WIN32` is no longer true when using the CygWin version of CMake, which would cause the CMake rules to take the wrong path (when generating a Visual Studio project) and attempt to build the Linux version. \n\nIf so, can you try downloading a native Windows version of CMake and use that to build the project files? (It should still be possible to invoke this version of CMake from the CygWin shell, if you prefer, although I personally use a `cmd.exe` shell as I find it easier to run `vcvarsall.bat` to set up the environment.)\n", "Thank you very much @mrry. I can now successfully build tensorflow on windows. The problem is that I didn't compile with the master branch, instead I tried on r0.11 branch. Now it is resolved. \n", "Ah yes, Windows support is not available on the `r0.11` branch. Thanks for clarifying!\n"]}]