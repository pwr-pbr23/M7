[{"number": 39618, "title": "The model does not detect any objects after converting to tflite using tflite_convert", "body": "**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.15\r\n- TensorFlow version: 1.15\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\ntoco --graph_def_file=./tflite_graph.pb --output_file=detect.tflite \\\r\n --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n --allow_custom_ops --mean_values=128   \\\r\n --std_dev_values=127\r\n\r\n```\r\n\r\nNote: I've also tried using TOCO to convert to Tflite but achieved same results.\r\n\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-05-17 14:41:03.490293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-05-17 14:41:04.310308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\n2020-05-17 14:41:04.311111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:81:00.0\r\n2020-05-17 14:41:04.311381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-05-17 14:41:04.312893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-05-17 14:41:04.314180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-05-17 14:41:04.314535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-05-17 14:41:04.316277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-05-17 14:41:04.317545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-05-17 14:41:04.321652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-17 14:41:04.333378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2020-05-17 14:41:04.333816: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-17 14:41:04.362575: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2151740000 Hz\r\n2020-05-17 14:41:04.364831: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559a2dc7ece0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-17 14:41:04.364871: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-17 14:41:04.945461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559a2dce1120 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-05-17 14:41:04.945510: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-05-17 14:41:04.945521: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2020-05-17 14:41:04.947161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\n2020-05-17 14:41:04.948586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:81:00.0\r\n2020-05-17 14:41:04.948648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-05-17 14:41:04.948676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-05-17 14:41:04.948701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-05-17 14:41:04.948726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-05-17 14:41:04.948753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-05-17 14:41:04.948779: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-05-17 14:41:04.948806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-17 14:41:04.954369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1\r\n2020-05-17 14:41:04.954434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-05-17 14:41:04.962865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-05-17 14:41:04.962892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 \r\n2020-05-17 14:41:04.962902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N \r\n2020-05-17 14:41:04.962909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N \r\n2020-05-17 14:41:04.965698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2020-05-17 14:41:04.967095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10481 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1)\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nI trained this using tensorflow object-detection api from [ssdlite_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) in model zoo. I also used graph_rewriter in the pipeline config file.\r\n\r\n```\r\nhttps://drive.google.com/file/d/1pG3UHIcD3aE8rw8l_8MCqo9f8i9mTl61/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n\r\n- When I load the detect.tflite file into my application it does not detect any objects. But, the frozen_inference_graph.pb file generated using export_inference_graph.py works fine.\r\n\r\n\r\n\r\n", "comments": ["I used this command and the problem was solved:\r\n\r\n`tflite_convert --graph_def_file tflite_inference_graph/tflite_graph.pb --output_file=./detect.tflite --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --change_concat_input_ranges=false --allow_custom_ops`\r\n\r\nplease use tflite_convert instead of toco."]}, {"number": 39617, "title": "Compile Tensorflow with c++17 using gcc", "body": "**System information**\r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n- Platform: X86_64\r\n- TensorFlow installed from (source or binary): from source\r\n- TensorFlow version: 1.15 and 2.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Using Bazel in conda environment to generate the binary\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 7.5.0 and 8.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nI am unable to compile Tensorflow 2.1 with c++17.\r\n\r\nThe configuration was the default values displayed. \r\n\r\n```\r\nDo you wish to build TensorFlow with XLA JIT support? [Y/n]: \r\nXLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: \r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: \r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: \r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n```\r\n\r\nSteps:\r\n\r\n```\r\n% git clone -b r2.1 https://github.com/tensorflow/tensorflow.git\r\n[configure - as above]\r\n```\r\n\r\nFix undefine `ABSL_MUST_USE_RESULT` error.\r\n`tensorflow/compiler/xla/service/slow_operation_alarm.h`\r\n\r\n```diff\r\n-ABSL_MUST_USE_RESULT std::unique_ptr<SlowOperationAlarm> SlowCompilationAlarm();\r\n+//ABSL_MUST_USE_RESULT \r\n+std::unique_ptr<SlowOperationAlarm> SlowCompilationAlarm();\r\n```\r\n\r\nFix memcpy error:\r\n`/tensorflow/core/lib/gif/gif_io.cc`\r\n\r\n```diff\r\n+#include <cstring>\r\n```\r\n\r\n```\r\n% cd tensorflow\r\n% export BAZEL_CXXOPTS=-std=c++17\r\n% bazel build -s --verbose_failures -c dbg --cxxopt=-std=c++17 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 //tensorflow/tools/pip_package:build_pip_package\r\n ```\r\n\r\nError:\r\n```\r\nERROR: /wrk/xsjhdnobkup1/vincentm/Projects/ML/tensorflow-2.1/tensorflow/tensorflow/lite/toco/BUILD:136:1: C++ compilation of rule '//tensorflow/lite/toco:model_cmdline_flags' failed (Exit 1)\r\ntensorflow/lite/toco/args.cc: In member function 'bool toco::Arg<toco::IntList>::Parse(std::__cxx11::string)':\r\ntensorflow/lite/toco/args.cc:121:12: error: 'SimpleAtoi' was not declared in this scope\r\n       if (!SimpleAtoi(part, &element)) return false;\r\n            ^~~~~~~~~~\r\ntensorflow/lite/toco/args.cc:121:12: note: suggested alternative:\r\nIn file included from ./tensorflow/lite/toco/args.h:25:0,\r\n                 from tensorflow/lite/toco/args.cc:16:\r\nexternal/com_google_absl/absl/strings/numbers.h:183:27: note:   'absl::SimpleAtoi'\r\n ABSL_MUST_USE_RESULT bool SimpleAtoi(absl::string_view s, int_type* out) {\r\n                           ^~~~~~~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 986.681s, Critical Path: 199.60s\r\nINFO: 13774 processes: 13774 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nI have a feeling it is the absl library that is suppose to augment the C++ standard library and extend C++11 with features in C++17.\r\n\r\nNote that I experience the same issue with TF version 1.15.\r\n\r\nAny help would be appreciated.\r\n\r\nThank you.", "comments": ["We only build with C++11 and C++14. Perhaps SIG Build would be a better fit for this", "Thank you for your reply Mihai.", "Closing this issue for now. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39617\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39617\">No</a>\n"]}, {"number": 39616, "title": "No module named 'tensorflow.contrib'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: jupyter  notebooks\r\n- TensorFlow installed from (source or binary): Github\r\n- TensorFlow version: 2.1.0\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:Intel HD\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n 11 sys.path.append(\"C:/Users/xx/.conda/envs/tensorflow_env/Lib/site-packages/tensorflow/models\")\r\n     12 import cv2\r\n---> 13 from nets import vgg\r\n     14 from preprocessing import vgg_preprocessing\r\n     15 from mlxtend.preprocessing import shuffle_arrays_unison\r\n\r\n~\\.conda\\envs\\tensorflow_env\\lib\\site-packages\\slim-0.1-py3.7.egg\\nets\\vgg.py in <module>\r\n     43 \r\n     44 import tensorflow as tf\r\n---> 45 # from tensorflow.contrib import slim as contrib_slim\r\n     46 \r\n     47 slim = contrib_slim\r\n\r\nModuleNotFoundError: No module named 'tensorflow.contrib'\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nimport os \r\nimport sys\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nfrom skimage.transform import resize\r\nfrom tqdm import tqdm\r\nfrom sklearn.model_selection import train_test_split\r\n%matplotlib inline\r\nsys.path.append(\"C:/Users/Firoz/.conda/envs/tensorflow_env/Lib/site-packages/tensorflow/models\")\r\nimport cv2\r\nfrom nets import vgg\r\nfrom preprocessing import vgg_preprocessing\r\nfrom mlxtend.preprocessing import shuffle_arrays_unison\r\ncheckpoints_dir = 'C:/Users/Firoz/checkpoints'\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\r\n\r\n\r\n\r\nI have gone through these steps\r\n1. https://github.com/tensorflow/tensorflow/issues/30794\r\n2. https://github.com/tensorflow/tensorflow/issues/31350\r\n\r\n without downgrading my tensorflow,  so Assisstance ??", "comments": ["Contrib does not exist in 2.0 or later. If you need `contrib` you have to use 1.15, although we recommend upgrading code to the contrib alternatives (since they are being actively maintained by an owner, unlike `contrib` which doesn't have an owner to fix issues)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39616\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39616\">No</a>\n", "When I do \"sudo -H pip2 install tensorflow==1.15\" I get\r\nERROR: Package 'mock' requires a different Python: 2.7.18 not in '>=3.6'\r\n\r\nWhen I do \"sudo -H pip3 install tensorflow==1.15\" I get\r\nERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0).\r\n\r\nAfter removing tensorflow.contrib python3, as well as older versions of tensorflow from pyhon3, there is pain. Any help?", "Before TF 2.2 you need python 3.5, 3.6 or 3.7. Not Python 2 or python 3.8", "Thank you. \r\n\r\nMy course:\r\ntrizen -S python37 --noconfirm\r\nsudo -H python3.7 -m pip install tensorflow==1.15.3\r\nsudo -H python3.7 -m pip install tf_slim\r\nsed -i \"s/from tensorflow.contrib import slim/import tf_slim as slim/g\" file.py\r\nsudo -H python3.7 -m pip install opencv-python", "> Contrib does not exist in 2.0 or later. If you need `contrib` you have to use 1.15, although we recommend upgrading code to the contrib alternatives (since they are being actively maintained by an owner, unlike `contrib` which doesn't have an owner to fix issues)\r\n\r\nThank you, it was really helpful @mihaimaruseac"]}, {"number": 39615, "title": "tf.keras.Model docs missing get_weights method and metrics property", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe docs is missing `get_weights`, `set_weights` method and `metrics` property.\r\n\r\n`get_weights` method and `metrics` property are defined in the src. But not in the generated docs.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/training.py#L190-L197\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/training.py#L361-L410\r\n\r\n`set_weights` method is mentioned in [keras docs](https://keras.io/api/models/model_saving_apis/#setweights-method).\r\n", "comments": ["The `get_weights` and `set_weights` property belongs to [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#get_weights) \r\nSo when you apply them on to whole model it will get/set weights for each individual layer of the model.", "But why generated docs doesn't have `set_weights` and `metrics`? Is it excluded somewhere, or a bug in docs generator.", "Closing issue since get_weights and set_weights can be found at tf.keras.layers.Layer as mentioned earlier."]}, {"number": 39614, "title": "Cannot convert model into tflite file format", "body": "System Information :\r\n- Tensorflow Object detection api : using pipeline.config file for ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03. \r\n- OS Platform and Distribution : Linux Ubuntu 16.04\r\n- TensorFlow installed from source\r\n- TensorFlow version : 1.14.0\r\n- Python version: 3\r\n- CUDA/cuDNN version: CUDA Version 10.0.130\r\n- GPU model and memory: Tesla M60 \r\n\r\nProblem :\r\nAfter making use of \r\npython3 model_main.py --alsologtostderr --model_dir=training/ --pipeline_config_path=training/pipeline.config\r\n\r\nI trained the model for 94k steps.\r\nThe model was converted to frozen graph format using export_tflite_ssd_graph.py. \r\nThe model was trained using quantization aware training by the following lines in the pipeline.config:\r\n\r\ngraph_rewriter \r\n{ \r\n quantization \r\n { \r\n  delay: 48000 \r\n  weight_bits: 8 \r\n  activation_bits: 8\r\n  }\r\n}\r\n\r\nIssue : \r\nUsing tflite convert i made use of the frozen graph from the previous step to convert the model into QUANTIZED_UINT8 format. I use the following command :\r\n\r\ntflite_convert   --output_file='../test3.tflite' --graph_def_file='.../tflite_graph.pb' --inference_type=QUANTIZED_UINT8\r\n --input_arrays='normalized_input_image_tensor' \r\n--output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \r\n--mean_values=128 \r\n--std_dev_values=128 \r\n--input_shapes=1,300,300,3\r\n --change_concat_input_ranges=false \r\n--allow_nudging_weights_to_use_fast_gemm_kernel=true \r\n--allow_custom_ops\r\n\r\nThe model asks for default max & min ranges.\r\n\r\nAfter giving the ranges the following error pops up : \r\nF ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\nFatal Python error: Aborted\r\n\r\nAny help would be quite beneficial.\r\n\r\n", "comments": ["Hi @vedrocks15, \r\n\r\nI wonder if you are using a mismatching version of the object detection api: you should use the one matching your tensorflow version (https://github.com/tensorflow/models/releases/tag/v1.13.0)", "Yup this worked.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39614\">No</a>\n"]}, {"number": 39613, "title": "Is it possible to have a keras API for PhasedLSTM similar to CuDNNLSTM?", "body": "Is it possible to have a keras API for PhasedLSTM similar to CuDNNLSTM?", "comments": ["@abinashsinha330 Can you please add more details to know whether you are looking for a feature or a support kind of questions that are good to be on Stackoverflow? thanks", "Correct me if I am wrong, I think it would be counted as a new feature since I am requesting an API similar to CuDNNLSTM that we have in keras.\r\nBasically, I am asking if we can expect an APi called CuDNNPhasedLSTM sometime in future?", "I think PhasedLSTM or its Cudnn implementation are widely used like normal LSTM and GRU. This means we won't contain them in the core Keras API. \r\n\r\nAlternatively you can propose this request to tensorflow/addons repository, which is the place for hosting additional APIs that is not in Keras or TF API."]}, {"number": 39612, "title": "native crash when using GpuDelegate on Float16 nn from java android.", "body": "when configuring:\r\n```kotlin\r\nInterpreter.Options().apply {\r\n    val gpuDelegate = GpuDelegate()\r\n    addDelegate(gpuDelegate)\r\n}\r\n```\r\nthe code crashes when the interpreter is built, even before running it, with a seg-fault from dereferencing a 0 pointer. as usual with seg-faults, no other information is given, like line-number or message.\r\n  this error only happens when the nn is of type Float16, but not with Float32 (which, of coarse, look the exact same from the java side)\r\nbut adding eiter `.setAllowFp16PrecisionForFp32(true)` or `.setAllowFp16PrecisionForFp32(false)`\r\nremoves the crash.\r\n\r\ni think the reason of the crash is that `Interperter.Options.allowFp16PrecisionForFp32` is declared `Boolean`, thus allowing `null` as a value, which crashes in native without a clear message.\r\n\r\nas i see it, there are 3 solutions:\r\n* declaring it `boolean` and initializing with `true` or `false`.\r\n* checking on the java side if the value is null, in case the configuration needs it, and throwing an appropriate error with a clear message. ( i prefer this one)\r\n* checkint / trying in native\r\n", "comments": ["@noamfreeman Could you please have a look at this [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java#L81-L84) ,probably it is a fix for this issue.Please let us know if it helps?Thank you!", "@sushreebarsa thanks. it does.\r\nshame on me not specifying the exact version i used (i see the null check is pretty old).\r\nthanks for checking into it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39612\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39612\">No</a>\n"]}, {"number": 39611, "title": "No gradients provided for any variable", "body": "I'm currently coding a text generation program to compare GRU, LSTM, and Bi-LSTM.\r\n\r\nRight now, I'm facing a problem with the gradients problem. It said no gradients provided for any variable.\r\n\r\nMy best guess is in the model or in the fitting, but no ideas. The other is the dataset.\r\n\r\n[I'm using Google CoLab for this program](https://colab.research.google.com/drive/1G4eilixtLJdW0c5TkgiSzunsqK2VUXTT?usp=sharing). I share it in viewer mode (unless you want me to change it).", "comments": ["Well, that explains why. I forget to add a loss function.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39611\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39611\">No</a>\n"]}, {"number": 39610, "title": "Custom Keras model and layers demonstrate different behavior (nonzero vs zero grads) on CPU vs GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): Tested against 2.1.0 and 2.2.0 GPU vs CPU, as well as TF-nightly 2.3.0.dev20200516 on GPU\r\n- Python version: 3.6.9\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n- GPU model and memory: GeForce GTX 1070 Ti\r\n\r\n\r\n**Current, expected behavior, best repro I can get**\r\nHi TF core,\r\n\r\nWriting some custom layers and models for an at-home research project, primarily using Keras, but also some lower-level TF constructs to programatically construct a few matrices. I am in an interesting position in this project currently, where I can flip between my sanity-check tests passing and failing by *changing the TF version backing my Python interpreter*, specifically by using a TF-GPU version versus a pure TF-CPU version. To be clear, all the gradients in my custom model are 0 (0, not `None`) when using TF-GPU and nonzero when using TF-CPU.\r\n\r\nI can inline the tests whose pass and failure can be flipped in this way, but pulling an entire standalone example might be a nontrivial task due to the layering The failing test is as follows:\r\n\r\n```python\r\n  def test_grads_non_zero(self):\r\n    model = classification_models.TwoLayerLinalgConv(1, 784)\r\n    with tf.GradientTape() as g:\r\n      result = model(MNIST_DATA)\r\n      loss = tf.keras.losses.sparse_categorical_crossentropy(MNIST_CLASSES, result, from_logits=True)\r\n    grads = g.gradient(loss, model.trainable_weights)\r\n    for idx, grad_elem in enumerate(grads):\r\n      self.assertFalse(grad_elem is None)\r\n      self.assertNotEqual(self.evaluate(tf.reduce_sum(tf.math.abs(grad_elem))), 0.)\r\n```\r\n\r\nThe model which is used here is a simple composition (via subclassing of `tf.keras.Model`), as follows:\r\n\r\n```python\r\nclass TwoLayerLinalgConv(tf.keras.Model):\r\n  \"\"\"Simple 2-dimensional convolution-like model via custom layer.\"\"\"\r\n\r\n  def __init__(\r\n      self,\r\n      n_filters,\r\n      data_size,\r\n      # ...other things\r\n      ):\r\n    super(TwoLayerLinalgConv, self).__init__()\r\n    # Some initializations\r\n    # etc\r\n    self.conv1 = layers.LaplacianLayer(n_filters=self._n_filters,\r\n                                       n_input_channels=self._n_input_channels,\r\n                                       n_params_to_fit=self._kernel_size,\r\n                                       n_dimensions=self._n_dimensions)\r\n        # Must have 1 filter here for the dense layer in the end.\r\n    self.conv2 = layers.LaplacianLayer(n_filters=1,\r\n                                       n_input_channels=n_filters,\r\n                                       n_params_to_fit=self._kernel_size,\r\n                                       n_dimensions=self._n_dimensions)\r\n    self.dense = tf.keras.layers.Dense(output_dim, activation=None)\r\n\r\n  def call(self, inputs):\r\n    x = tf.nn.relu(self.conv1(tf.reshape(inputs, [-1, self._n_input_channels, self._data_size])))\r\n    x = tf.nn.relu(self.conv2(x))\r\n    return self.dense(tf.reshape(x, [-1, self._data_size]))\r\n```\r\n\r\nIt is probably worth noting that the layers used in this model, when tested as follows, actually *pass* with CPU and GPU-backed environments:\r\n\r\n```python\r\n  def test_operator_trains(self):\r\n    l = layers.LaplacianLayer(1, 1, 1)\r\n    l.build([10, 1, MNIST_SIZE])\r\n    with tf.GradientTape() as g:\r\n      input_with_channel = tf.reshape(MNIST_DATA, [-1, 1, MNIST_SIZE])\r\n      result_of_layer = l(input_with_channel)\r\n      pred = tf.keras.layers.Dense(10)(result_of_layer)\r\n      result = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\r\n          MNIST_LABELS, tf.reshape(pred, shape=[10, 10]), from_logits=True))\r\n\r\n    grad = g.gradient(result, l.trainable_weights)\r\n    for grad_elem in grad:\r\n      self.assertFalse(grad_elem is None)\r\n      self.assertNotEqual(self.evaluate(tf.reduce_sum(tf.math.abs(grad_elem))), 0.)\r\n```\r\n\r\nI am wondering: is this kind of divergence a known issue? Is there a simple way to mitigate this problem? Is there any more information that would be helpful, or pointers on debugging? I am pretty much at a loss on where to go from here...\r\n\r\nThanks!\r\nKeith", "comments": ["@jkr26 \r\nI ran the code shared above and do not see any output to compare, please find the gist for [nightly](https://colab.sandbox.google.com/gist/Saduf2019/a28645812ad436dc89fc5940f9656baf/untitled183.ipynb) and [tf 2.1](https://colab.sandbox.google.com/gist/Saduf2019/4a55ecb963b9afe6565421b3a10c3fa7/untitled182.ipynb).\r\nCan you please share a colab gist for the reported issue, error logs for us to analyse.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@Saduf2019 \r\n\r\nYes, the code above is not entirely self-contained--it would be rather nontrivial to produce such a complete example, but doable. Is this what you would prefer?", "@jkr26 it would be very helpful if you can provide a reproducible colab gist of the issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39610\">No</a>\n"]}, {"number": 39609, "title": "Implement __reduce__ on tf.keras.Model to copy.deepcopy and pickle", "body": "Adding `__reduce_ex__` in this manner allows Keras Model instances to be compatible with Python's pickle and copy modules. This should resolve a couple of related issues, ex #36897, #34697.\r\n\r\n**Why this would be useful**\r\nI know there are other preferred ways to save tensorflow models, but it's not always feasible to save to a file, or the issue may not even be up to the user (ex: they are using a pip installed module that calls `copy.deepcopy`, modifying the pip installed module would not be straightforward).\r\n\r\n**Potential issues**\r\nI can see a lot of edge cases. I'm not sure what happens with more advanced usage, like nested models. I was hoping to get some feedback regarding potential issues so that I can write some more tests and see if this will work for all cases.\r\n\r\n**Maintenance burden**\r\nThis is really just wrapping existing tensorflow saving tools, so I would not anticipate this breaking unless those tools themselves broke/changed.", "comments": ["Pickling is really unsafe, I'm not sure we should resort to that.", "> Pickling is really unsafe, I'm not sure we should resort to that.\r\n\r\nDo you mean that you don't want to encourage users to resort to pickling (just asking for clarification)? I feel that if a user wants to pickle stuff, they can do it regardless (ex: #34697), this just makes it easier.\r\n\r\nAlthough this enables pickling, it more importantly enables `copy.deepcopy` to work, which is used in plenty of workflows/packages. Let me change the title to reflect that that is the more important goal. I'm guessing we could add `__deepcopy__` or register Model with `copyreg` instead of `__reduce_ex__` to enable just `copy.deepcopy` but not pickle. Let me know if that would be easier. It would still be very useful.", "@mihaimaruseac I could be wrong, but it _seemed_ like the failures were unrelated to these changes (please do correct me if I am wrong). I rebased onto master, hopefully that will fix things. Can you trigger the CI again? Thanks", "Glad to see Ubuntu CPU is now passing. I think I should have put this in `Functional` from the get-go. I looked at the Mac OS failures, it seems like they are present in the master branch (https://source.cloud.google.com/results/invocations/48889b0f-3d18-4cf1-a4d0-30254fef918f/targets).", "Confirming that the failures are from master, they are fixed in b3387c0c19c7ab6e637bcc3d63fbd1854d64f414.\r\n\r\n@mihaimaruseac let me know what next steps I can take here for this PR now that basic tests are passing.", "You don't have to do anything at this time. The PR is getting imported internally, passing through another round of reviews and CI and then if all is good it will get merged.\r\n\r\nIf there are issues, we will come back to the PR with additional steps.\r\n\r\nThank you for the PR", "Awesome, glad to try to help!", "@adriangb Thanks for the PR!\r\n\r\nI really like the overall idea in this PR, and IMO `pickle` and `copy.deepcopy` is something we should definitely support. We actually had some [tests for this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer_test.py#L224) but they don't have enough coverage\r\n\r\nI have a few worries with the implementation hereL\r\n\r\n(1) This will only work with `Functional API` models. IMO we need a solution that works for user-subclassed `Model`s as well\r\n(2) We're locking ourselves into an implicit serialization format for `Model` that turns it into a 3-tuple of `(Model.get_config()`, compiled_state, weights)`\r\n\r\nWhat I'd really like to see is an approach to `pickle` that uses the best of `SavedModel` and the best of `pickle`. That is, an implementation that delegates saving of `Checkpoints`, `tf.function` traces, etc to `SavedModel`, but is able to faithfully recreate the Python state using `pickle`.\r\n\r\nIdeally I think it would be something like this:\r\n\r\nWhen pickling:\r\n\r\n(1) Extract all the things SavedModel can handle well (`tf.Variable`s, `tf.function`s, etc)\r\n(2) Save those things using SavedModel format\r\n(3) Remove those things from the Model\r\n(4) Pickle the stripped Model\r\n\r\nWhen unpickling:\r\n\r\n(1) Unpickle the stripped Model\r\n(2) Load the SavedModel saved file corresponding to the pickle\r\n(3) Restore those objects into the stripped Model\r\n\r\nThis would make sure that you could `pickle` an object in one `DistributionStrategy` context and load it in another. It would also work with user-subclassed Models\r\n\r\nI'm not sure how feasible this approach is though, wdyt?\r\n\r\n@k-w-w for SavedModel and serialization knowledge", "I think that would be great!\r\n\r\nI have very limited knowledge, but as far as I can tell, SavedModel/`tf.saved_model` (I'm going to use them interchangeably) only works saving to an entire directory, which IMO is incompatible with in memory serialization (`deepcopy` or `pickle`). I think this addition is only useful if it is fully compatible with fully in-memory serialization.\r\n\r\nIf SavedModel or `Model.save` was able to dump to an in-memory bytes like object, what you are proposing would be super straightforward. From a brief peak into SavedModel, this seemed pretty complex to implement, but certainly not impossible. I actually wanted to do that as part of GSOC but I was late to submit by a day :/\r\n\r\nMaybe this can be a smaller subset of a larger plan? Like implement this for only Functional API models and then work slowly on an overhaul of in-memory serialization in general, including SavedModel. SavedModel could be expanded to accept it's current format (IIRC a directory with multiple files), HDF5 (replacing `model.save`) as well as arbitrary byte streams (for in-memory serialization). Then `Model.save` could just call `SavedModel` and `__reduce_ex__` could just call `Model.save` and return the byte-stream.\r\n\r\nSomething now is better than nothing!\r\n\r\n", "We could implement an in-memory filesystem plugin (tensorflow/community/pull/101) and then serialize the SavedModel to in memory location. But this will also take time.", "> We could implement an in-memory filesystem plugin ([tensorflow/community/pull/101](https://github.com/tensorflow/community/pull/101)) and then serialize the SavedModel to in memory location. But this will also take time.\r\n\r\nRight, that definitely seems like a more universal solution than this, but as you say, the timeline for that is unknown, but probably no soon. This could happen now.", "The current approach should work for Subclassed models as long as the model is unpickled in a [CustomObjectScope](https://www.tensorflow.org/api_docs/python/tf/keras/utils/CustomObjectScope) or if the class is decorated with [`tf.keras.utils.register_keras_serializable`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/register_keras_serializable). \r\n\r\nThe current approach matches how things are saved in the HDF5 format (except that in HDF5, weights are saved as a dictionary mapping layer names to `layer.get_weights()` rather than in a single `model.get_weights()` list). We should be able to extend the pickled content to include traced tf.functions so that custom objects can be deserialized without being registered.", "> in HDF5, weights are saved as a dictionary mapping layer names to `layer.get_weights()` rather than in a single `model.get_weights()` list\r\n\r\nIs there any advantage to doing things that way (`dict` vs `list`)?\r\n\r\n> We should be able to extend the pickled content to include traced tf.functions so that custom objects can be deserialized without being registered.\r\n\r\nThat would be awesome!\r\n\r\nI'll study this things a bit further on my end, but let me know if there is any specific tasks or testing you'd like to offload to me.", "Thinking out loud:\r\n\r\nPython provides basically two ways to make an object picklable/copyable:\r\n1. Ask the object to serialize itself (`__reduce__`,  `__deepcopy__`, etc.)\r\n2. Register with the global copyreg dispatch via `copyreg.pickle`.\r\n\r\nIt seems to me that the current saving utilities in Keras lean towards the second approach: have a single function that \"knows\" about all of the Keras objects and how to serialize them (specifically, the `keras.layers.serialization` module). I think the simplest way to extend this to be compatible with `deepcopy` and `pickle` would be to add the following to `populate_deserializable_objects`:\r\n```python3\r\nfor typ in LOCAL.ALL_OBJECTS.values():\r\n  copyreg.pickle(typ, lambda obj: (deserialize, (serialize(obj),), ))\r\n```\r\nThat would register `serialize`/`deserialize` as the pickling functions for objects objects of type `typ`. Then `generic_utils.serialize_keras_object` would have to be expanded to know about weights and compiling models so that it can restore those like I am doing in this PR. I think this would enable `deepcopy` for all Keras serializables, including subclassed models that are registered as such.\r\n\r\nThat said, this approach suffers from the drawback that it still requires registering subclassed Model \r\n (and other inherited classes). If approach 1) above was used, only implementing `__reduce_ex__` specifically for the classes that contain thread objects or other unserializables would be required. After the unserializables are removed or packaged, `copy.deepcopy` is recursively called on the rest of the attributes. This way a subclassed Model that does not add new unserializable attributes will require no extra work to be serializable. This would also avoid having to add Model specific information (like `get_weights`) to a generalized function (`serialize_keras_object`).\r\nA lot of classes already have a `serialize` and `deserialize` method, which could probably be adapted easily. But certainly in-depth knowledge of what objects require special treatment would be needed.", "@adriangb Can you please resolve conflicts? Thanks!", "@gbaned, done. tests would need to run again, but no real changes were required so it should be good!", "would it be also possible to work around this issue by overriding `__getstate__` like it's done here: \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8398b862ffc988165c63ed5c76fdeae9635d3640/tensorflow/python/keras/engine/base_layer.py#L3016-L3024\r\n\r\nsee also my comment on the issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/34697#issuecomment-642296650\r\n", "@adriangb Can you please resolve conflicts? Thanks!", "Thanks for this PR and implementation @adriangb! I'm looking forward to having it merged. Other downstream libraries like Dask-ML depend on it (I unsuccessfully tried to monkey-patch it into Dask-ML last night).", "@adriangb Can you please resolve conflicts? Thanks!", "@adriangb,  Any update on this PR? Please. Thanks!", "Hi @gbaned, I think we are waiting for internal review at Google before doing anything with this PR. Happy to resolve conflicts and such if you decide to move forward with this PR as-is.", "@omalleyt12, @k-w-w Can you please assist on above comments from @adriangb. Thanks!", "In https://github.com/tensorflow/tensorflow/commit/0e529eb70debc7224b83d1b654f22324cd1a7d53 a local lambda was introduced to optimizers which now makes those unpicklable as well. Over at https://github.com/adriangb/scikeras/issues/70 we are debating if we should monkey patch optimizers as well in addition to `Model` (which we are already doing). I really would prefer to have this fixed upstream as proposed in this PR instead of having to monkey patch every single class.\r\n\r\nIf there is any debate as to whether this should be done or not, please see this blog post (credit to @stsievert  for sharing this with me): https://matthewrocklin.com/blog/work/2018/07/23/protocols-pickle.\r\n\r\nAs a proof of concept, here is a notebook that implements a working pickle protocol for most Keras classes: https://colab.research.google.com/drive/14ECRN8ZQDa1McKri2dctlV_CaPkE574I?authuser=1\r\n\r\nI'm happy to help implement this, but I am going to need support from the TF team. Please let me know if we can move forward with this.", "i still maintain that SavedModels and checkpoints should be used instead of pickling.\r\n\r\nAs to deep copy, I'm also not certain that that's what we want, given that some of the deep python bits are actually references to stuff in C++ land so deep-copying might result in use after free and other vulnerabilities.", "@mihaimaruseac thank you for the update\r\n\r\nGenerally, I feel that your points are very similar to those in the article linked above. I will do my best to respond with TF specific context.\r\n\r\n> We already have better options than pickling (save model, checkpoints)\r\n\r\nThe nice thing is that you can implement the pickle protocol using these as a backend. You can also implement different backends for in memory copies (deepcopy) vs. serialization to bytes (pickle), but what I am proposing here as a good starting point is to use serialization to bytes for both options, for the reason below.\r\n\r\n> Using deepcopy or pickle may cause bugs\r\n\r\nPickle is a protocol. If you implement pickle using TFs internal `serialize`/`deserialize` methods, there will only be bugs if those methods have bugs. If TFs internal methods work correctly, there will be no bugs. As proof of this, I can safely deepcopy _all_ of the stdlib and numpy, a lot of which is built in C/Cython. Also see my monkey patches implementation above: all I do is call `serialize/deserialize`.\r\n\r\n> Using pickle is unsafe, we don't want users to have to unpickle models they download online\r\n\r\nNo one is suggesting that TF users should start posting (and downloading -> loading) pickled models online. That would be bad. But just because pandas supports pickling doesn't mean every tutorial or paper published using a pandas dataframe posts their data as a pickled dataframe. Pickle is not for that.", "In that case, I think we first need a community RFC to design the entire pickle support, and once that gets accepted we can start working on it.", "I am happy to begin submitting an RFC. Is [this](https://www.tensorflow.org/community/contribute/rfc_process) the right procedure?", "Yes, sorry for not linking it in the previous comment.", "Great are you (@mihaimaruseac) and/or @k-w-w, @omalleyt12 willing to sponsor this RFC (I am just tagging TFers that have chimed in here)?", "Sure, I can be a sponsor.", "> We could implement an in-memory filesystem plugin ([tensorflow/community/pull/101](https://github.com/tensorflow/community/pull/101)) and then serialize the SavedModel to in memory location. But this will also take time.\r\n\r\n@mihaimaruseac I wanted to resurface this comment since it would be relevant for any RFC. Are there any updates on use of SaveModel to write to arbitrary file-like objects (i.e. in memory instead of to a folder)? That would obviously be the better way to implement this (since I think `SaveModel` can handle more than `serialize/deserialize` + weights can), so I would like to mention it in the RFC. ", "@adriangb Sorry for the delay. We now have a ram filesystem that could be used for this, I think", "> @adriangb Sorry for the delay. We now have a ram filesystem that could be used for this, I think\r\n\r\nI'm not sure what you mean. Anything that is currently being written to a directory can be done using something like:\r\n\r\n```python3\r\nimport zipfile\r\nimport io\r\nimport os\r\n\r\n\r\nin_memory_zip = io.BytesIO()\r\n\r\n# Write files to this in-memory \"file system\"\r\nwith zipfile.ZipFile(in_memory_zip, mode=\"w\") as zf:\r\n    zf.writestr(os.path.join('test', 'model.pb2'), \"serialized_pb2_content\".encode())\r\n\r\n# Now write to disk (if needed)\r\nwith zipfile.ZipFile(in_memory_zip, mode=\"r\") as zf:\r\n    zf.extractall(\"destination_dir\")\r\n\r\n# Verify the result\r\nfor root, dirnames, filenames in os.walk(\"destination_dir\"):\r\n    if filenames:\r\n        assert root == os.path.join(\"destination_dir\", \"test\")\r\n        assert len(filenames) == 1 and filenames[0] == \"model.pb2\"\r\n        with open(os.path.join(root, filenames[0])) as f:\r\n            contents = f.read()\r\n            assert contents == \"serialized_pb2_content\"\r\n```\r\n\r\nEdit: sorry, I misread your comment. I read _we do not currently have a RAM filesystem_. If you do have something that is more performant or has other advantages to the above, of course that can be used. As long as `Model.save(...)` can return a file-like object that can be safely copied in memory, it's all the same as far as pickling is concerned.", "@adriangb Can you please resolve conflicts? Thanks!", "We have [this filesystem](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/ram_file_system.h;drc=49b58c7b7f7c4b66d68963cd6212f328d5a3527f) which is [registered to handle all URIs that start with `ram://`](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/windows/env.cc;l=196;drc=77ee5e02721ba797fe01d47019e6017d2bb09ab7)\r\n\r\nSo something like this will use the RAM filesystem:\r\n\r\n```python\r\nwith tf.io.gfile.GFile(\"ram://some/file\", \"w\") as f:\r\n  f.write(\"asdf\")\r\n```", "Reading the docs for ` tf.io.gfile`, I think this would look something like this:\r\n\r\n```python3\r\ndef unpack(byte_stream):\r\n  \"\"\"Use load_model to load from a byte stream.\r\n  \"\"\"\r\n  temp_ram_location = f\"ram://{id(byte_stream)}\"\r\n\twith tf.io.gfile.GFile(temp_ram_location, \"w\") as f:\r\n            f.write(byte_stream)\r\n  return load_model(temp_ram_location)\r\n\r\n\r\nclass Model:\r\n    def __reduce_ex__(self, protocol=0):\r\n      \t\"\"\"Save model for pickling\r\n      \t\"\"\"\r\n      \ttemp_ram_location = f\"ram://{id(self)}\"\r\n      \tself.save(temp_ram_location)\r\n        with tf.io.gfile.GFile(temp_ram_location, \"r\") as f:\r\n              data = f.read()\r\n      \treturn (unpack, (data,))\r\n```\r\n\r\n\r\nIt seems like `SaveModel` _can_ write to this RAM filesystem, but it ends up writing an entire folder structure, and as far as I can tell there is no way to read that folder structure back, i.e.:\r\n\r\n```python3\r\nwith tf.io.gfile.GFile(\"ram://test_folder/test_file.pb\", \"w\") as f:\r\n    f.write(\"TEST\")\r\n\r\nwith tf.io.gfile.GFile(\"ram://test_folder\", \"r\") as f:   # does not work!\r\n    data = f.read()\r\n```", "I tested a bit, I _almost_ got things working:\r\n\r\n```python3\r\ndef unpack(bytes_model):\r\n    \"\"\"Use load_model to load from a bytes like object\r\n    that came from pickling.\r\n    \"\"\"\r\n    save_folder = f\"{id(bytes_model)}\"\r\n    ram_prefix = \"/tmp/\"   # fails if set to \"ram://\"\r\n    temp_ram_location = os.path.join(ram_prefix, save_folder)\r\n    with zipfile.ZipFile(bytes_model, \"r\", zipfile.ZIP_DEFLATED) as zf:\r\n        for path in zf.namelist():\r\n            if not tf.io.gfile.exists(os.path.dirname(os.path.join(temp_ram_location, path))):\r\n                tf.io.gfile.makedirs(os.path.dirname(os.path.join(temp_ram_location, path)))\r\n            with tf.io.gfile.GFile(os.path.join(temp_ram_location, path), \"wb+\") as f:\r\n                f.write(zf.read(path))\r\n    return keras.models.load_model(temp_ram_location)\r\n\r\n\r\ndef pack_keras_model(model, protocol=0):\r\n    \"\"\"Use save_model to write to a bytes like object\r\n    which is then pickled.\r\n    \"\"\"\r\n    save_folder = f\"{id(model)}\"\r\n    ram_prefix = \"ram://\"\r\n    temp_ram_location = os.path.join(\"ram://\", save_folder)\r\n    model.save(temp_ram_location)\r\n    b = io.BytesIO()\r\n    with zipfile.ZipFile(b, \"w\", zipfile.ZIP_DEFLATED) as zf:\r\n        for dirpath, dirnames, filenames in tf.io.gfile.walk(temp_ram_location):\r\n            for dir in dirnames:\r\n                with tf.io.gfile.GFile(dir, \"rb\") as f:\r\n                    zf.writestr(os.path.relpath(dir, temp_ram_location), f.read())\r\n    return (unpack, (b,))\r\n```\r\n\r\nThere seems to be some problem with the directory creation in `unpack`. If I change from ` ram_prefix = \"/tmp/\"` -> ` ram_prefix = \"ram://\"` something breaks.\r\n\r\nI think this may be due to a bug in `tf.io.gfile.exists`:\r\n```python3\r\nassert not tf.io.gfile.exists(\"ram://testdir/\")\r\nassert not tf.io.gfile.exists(\"ram://testdir/SHOULDNOTEXIST.txt\")\r\nwith tf.io.gfile.GFile(\"ram://testdir/testfile.txt\", \"w\") as f:\r\n    f.write(\"test\")\r\nassert tf.io.gfile.exists(\"ram://testdir/\")\r\nassert not tf.io.gfile.exists(\"ram://testdir/SHOULDNOTEXIST.txt\")  #fails, tf.io.gfile.exists returns True\r\n```", "That might be a real bug. Which version are you testing with?", "> That might be a real bug. Which version are you testing with?\r\n\r\nNot sure (I switch them around all the time to test SciKeras things) but I was able to reproduce in Colab on 2.3.0: https://colab.research.google.com/drive/1vmUI4bAXRqnDQ4h7I8VovNI55ILQX_g7?usp=sharing\r\n\r\nCurrently this breaks things in https://github.com/tensorflow/tensorflow/pull/39609#issuecomment-689086677 because at some point `SaveModel` checks for a file that doesn't exist and tries to load it, causing an IO error.", "Can you also attempt to reproduce against TF nightly? I recall a recent change to fix the buggy behavior", "The change was in 49b58c7b7f7c4b66d68963cd6212f328d5a3527f and seems relevant", "Seems to be fixed (tested in the same notebook)! I'll test https://github.com/tensorflow/tensorflow/pull/39609#issuecomment-689086677 in nightly.", "@mihaimaruseac I was able to get this working with SaveModel.\r\n\r\nI pushed the tests to this PR since it is directly relevant. I did not test locally since I'm having trouble getting bazel to build things correctly, but feel free to kick off CI if you want to see the tests. I was also able to test pickling using a numpy array as a container since that should enable [pickle protocol 5]( https://docs.python.org/3/library/pickle.html#out-of-band-buffers ) (thank you @jakirkham).\r\n\r\nThe implementation is pretty messy with all of the zipfile stuff going on. If `tf.io.gfile.GFile` was able to dump an entire RAM based folder to a binary stream that would make things much easier, i.e.:\r\n\r\n```python3\r\nwith tf.io.gfile.GFile(\"ram://testdir/testfile.txt\", \"w\") as f:\r\n    f.write(\"test\")\r\nwith tf.io.gfile.GFile(\"ram://testdir\", \"r\") as f:\r\n    print(f.read())\r\n```", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39609) for more info**.\n\n<!-- need_author_cla -->", "> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.\r\n\r\n@jakirkham I think this is for you", "Sounds great. Let's get this in the RFC and get that ready to send :)", "> Sounds great. Let's get this in the RFC and get that ready to send :)\r\n\r\n> The implementation is pretty messy with all of the zipfile stuff going on. If tf.io.gfile.GFile was able to dump an entire RAM based folder to a binary stream that would make things much easier\r\n\r\n@mihaimaruseac do you know if this would be possible/feasible? I realize there may be some API concerns here since the behavior might be different for ram vs. permanent storage. I'd like to put this simplified version into the RFC instead of the messy version we have here, but I don't think it's fair to do that if that's not a realistic implementation.", "We could add some additional methods to the RAM filesystem or use the [configurable support for filesystems RFC](https://github.com/tensorflow/community/pull/277) but it would be best to have all filesystem implementations offer the same API if possible.\r\n\r\nAlternatively, we can hide the complexity under a cleaner API.", "Any of those sound feasible.\r\n\r\nTo be clear, the requirement would be the ability to do:\r\n`with tf.io.gfile.GFile(\"ram://directory/\", \"r\") as f: f.read()`\r\nor otherwise load the entire saved model into a single binary stream of data.\r\n\r\nI know this cannot be done with Python's regular IO modules, ex the following does NOT work:\r\n`with open(\"/tmp/\", 'r') as f: f.read()`\r\n(nor am I suggesting it should work, I don't think that operation even makes sense for a real filesystem).\r\n\r\nMaybe the cleanest way would be to have a `load_folder` and `write_folder` method for the RAM filesystem? It could essentially do the `walk` I'm doing above and encode each file into a binary stream + a header containing the information on the folder structure and start/stop for each file.", "We would prefer the file IO API to match the Python's one as much as possible and also to be consistent w.r.t URI scheme.\r\n\r\nI think the best path forward would be to create helper methods for `load_directory` and `save_directory` that use the file IO API. In the end, some user might want to load a directory on a different filesystem too, not only from RAM.", "> We would prefer the file IO API to match the Python's one as much as possible and also to be consistent w.r.t URI scheme.\r\n\r\nI absolutely agree.\r\n\r\n> the best path forward would be to create helper methods for load_directory and save_directory\r\n\r\nIf you're open to implementing `load_directory` and `save_directory`, that would work great for this project. I'll go ahead and write the example in the RFC assuming the existence of `tf.io.gfile.GFile(...).load_directory(...) -> bytes`", "@adriangb   Can you please sign CLA? Thanks!", "> @adriangb Can you please sign CLA? Thanks!\r\n\r\nI think it's @jakirkham that needs to sign the CLA. I can always force push and override their commit if needed, but I'd like to give them the option of signing it and keeping the credit as long as this PR is not ready to be merged.", "Sorry for the radio silence here. Feel free to force push, @adriangb. I'm not that worried about credit for that relatively minor change. After all you have been doing the bulk of the work here (thanks for doing that btw), so am not worried about getting credit here :slightly_smiling_face:", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F39609) for more info**.\n\n<!-- ok -->", "So this will be waiting on the RFC, right?", "Yes, we're still working on the RFC. I think it's almost ready, we should have it in by EOW for sure.", "> So this will be waiting on the RFC, right?\r\n\r\nRFC submitted.\r\n\r\nI think the blocker for this (other than RFC approval) is going to be  `load_directory` and `save_directory` as discussed above.", "Can we add these methods to the RFC too? I didn't get a chance to look at it yet, sorry for the delay", "They are mentioned in passing [here](https://github.com/tensorflow/community/blob/960af33814aeafca475518adb2e370261de990fa/rfcs/20200921-pickle-for-keras.md#design-proposal).\r\n\r\nDo you want more detail on them in the RFC? I felt that going into more detail would be muddling the RFC with implementation details that are not strongly tied to the RFC (i.e. the exact name, API and implementation of these functions does not matter for this RFC, just the general feature, and even that can be worked around, as demonstrated by the crude implementation in this PR).", "Let me read the RFC and will comment there. I think I'll get a first review by the end of the weekend.", "Initial review looks good to me. Now we need to send an email with link to this to developers@tensorflow.org and after 2 weeks of people reviewing the proposal we can schedule the final design review meeting. I'll do the meeting schedule, but can you do the emailing. please?\r\n\r\nThank you", "> Initial review looks good to me.\r\n\r\nAwesome! Thank you for taking a first pass.\r\n\r\n> Now we need to send an email with link to this to [developers@tensorflow.org](mailto:developers@tensorflow.org)\r\n\r\nA link to the RFC? Anything else to include in the email?\r\n\r\nThanks!", "Nevermind, I found the section in the RFC guide, it's a link to the RFC PR. Email sent, I cc'ed you @mihaimaruseac .", "@mihaimaruseac Any update on this PR? Please. Thanks!", "@mihaimaruseac now that https://github.com/tensorflow/community/pull/286 is merged, we can continue work here right? I think the next step is figuring out the `load_directory` and `save_directory` utility functions.", "That is correct. We can do smaller helper PRs too if needed. ", "> We can do smaller helper PRs too if needed.\r\n\r\nAgreed, we should probably implement those in another PR. Do you plan on working on those internally, or should I plan on implementing them? I am not too familiar with the inner workings of `tf.io`.\r\n", "> > We can do smaller helper PRs too if needed.\r\n> \r\n> Agreed, we should probably implement those in another PR. Do you plan on working on those internally, or should I plan on implementing them? I am not too familiar with the inner workings of `tf.io`.\r\n\r\nUnfortunately my team lost 50% of its members so at least until mid of next quarter we won't be able to take on new work items :(", "> Unfortunately my team lost 50% of its members so at least until mid of next quarter we won't be able to take on new work items :(\r\n\r\n:( I'll try to take a stab at it at some point; I'll let you know", "> > > We can do smaller helper PRs too if needed.\r\n> > \r\n> > \r\n> > Agreed, we should probably implement those in another PR. Do you plan on working on those internally, or should I plan on implementing them? I am not too familiar with the inner workings of `tf.io`.\r\n> \r\n> Unfortunately my team lost 50% of its members so at least until mid of next quarter we won't be able to take on new work items :(\r\n\r\nWould it make sense to integrate this change without those other changes (assuming no one has time to do them yet)? Or has the situation for your team changed since then?", "I ran some benchmarks, which may help make a more informed decision.\r\n\r\nFirstly, I used slightly more updated serialization code from SciKeras, source [here](https://github.com/adriangb/scikeras/blob/08f82398aff0f0ea8ba99f9ca512c928d2923e95/scikeras/_saving_utils.py). This is far from perfect, and it can probably still be cleaned up/improved, eg. by using a tarfile instead of a zipfile.\r\n\r\nMy benchmark consisted of comparing roundtrips using pure `SaveModel` serialization to RAM to using pickle (backed by `SaveModel`). I benchmarked against `SaveModel` by comparing pickling to just `load_model(model.save(...))`.\r\n\r\nI ran two benchmarks:\r\n1. A simple simple model consisting of varying number of `Dense` layers, to explore the effect of _a_ variable in the Model size.\r\n2. Repeated (30) save/loads with `MobileNetv3Small`.\r\n\r\nHere are the results I got. Error bars in the first plot (yes, there are error bars) are standard deviations from 5 repeats:\r\n\r\n![image](https://user-images.githubusercontent.com/1755071/108636466-39b4af80-743a-11eb-823a-6ebf4ee4bee9.png)\r\n\r\nNote that `Pickle` in this case is not only taking the `SaveModel` data and making it an array of bytes, it is also cleaning up the saved data (i.e. nothing is left over in RAM after every roundtrip). I am not sure if `SaveModel` on its own does this, I assume it does not.\r\n\r\nSource code as a gist [here](https://gist.github.com/adriangb/996999e708b0ec38343268e5c4fef908), Colab notebook [here](https://colab.research.google.com/drive/1UZdim7DekzeuffZsC7S0WxU-ZKd3klcy?usp=sharing) (I can't promise I'll keep this around forever).\r\n\r\nI think that having helper functions in the C++ side would be nice, but as evidenced from these benchmarks, there's no performance penalty for doing it in Python because serializing the model takes a lot more time than creating a tarball of the serialization. Besides, doing it in C++ efficiently probably requires that the data is stored adjacently in memory, etc. and also requires coming up with some sort of API (does it return bytes with no promise of their structure? a valid tarfile? how does one maintain version compatibility if it is just bytes? etc.)", "The 2 bugs seem to be resolved now.", "Yes, thank you for the quick turnaround. I'll update this branch.\n\nOn Fri, Feb 26, 2021, 4:41 PM Mihai Maruseac <notifications@github.com>\nwrote:\n\n> The 2 bugs seem to be resolved now.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/39609#issuecomment-786933802>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANMPP37X6MKLYSJAIBSDJLTBAPRLANCNFSM4NDCBPRQ>\n> .\n>\n", "Hi Googlers, is the oneDNN failure related to the PR? Can we get a full CI run on this? Thanks!", "@mihaimaruseac thanks for the run. I think I've fixed the errors, at least locally (Docker) the sanity checks and the tests that were failing are now passing.", "Hmm, can you look at the sanity and ubuntu cpu builds?", "Just looked. It looks like they were both the same issue: I was missing an argument (`memo`) in `__deepcopy__` as per the [copy docs](https://docs.python.org/3/library/copy.html#copy.deepcopy); sorry about that! I'll try to test locally again, but if you can kick off CI again to confirm that would be great.", "A postmortem on [this conversation](https://github.com/tensorflow/tensorflow/pull/39609#discussion_r588921787) and the changes in c3e615a.\r\n\r\nInitially, I had implemented only `__reduce__` and not `__copy__` or `___deepcopy__`. Since `SaveModel` is AFAIK the only \"sanctioned\" way to serialize a Model, this made sense because calling `copy.deepcopy` would just delegate to `__reduce__` which in turn delegates to `SaveModel`.\r\n\r\nHowever, I then discovered that [this test ](https://github.com/tensorflow/tensorflow/blob/e3b26219124fc5ccdcd5ae7a102ac42a3e14ef34/tensorflow/python/keras/tests/model_subclassing_test.py#L718-L745) is testing for `deepcopy` _on an unbuilt Model_. TF team please correct me if I am wrong, but an \"unbuilt\" model would just be a model where none of `fit`, `predict` or `build` have been called and hence the input shape has not been set. Apparently, `SaveModel` does not support serializing this type of model. So how was this test passing then? Well it fell back to `object.__reduce__`. So it turns out that although TensorFlow does not support pickling, there is actually a test that is literally testing pickling a Model!\r\n\r\nTo solve this, I had to implement `__deepcopy__`. I still delegate to `__reduce__` here (not to `object.__reduce__` directly, just up the MRO, which means `Layer.__reduce__`, but will currently land at `object.__reduce__` as long as nothing in between is implemented). The implementation is an abbreviated version of what happens in `copy.deepcopy`.\r\n\r\nI think this experience shows why its probably better to implement `__deepcopy__`, `__reduce__`, etc., even if it's just to raise a `NotImplementedError` with a user friendly explanation as to what went wrong as opposed to \"not supporting them\" by not implementing anything. They will still be implemented, just not in the way you think.\r\n\r\nIn summary, serialization will now be supported as follows:\r\n\r\n|         | pickle                                      | copy/deepcopy                              |\r\n|---------|---------------------------------------------|-----------------------------------|\r\n| built   | via SaveModel                               | via SaveModel                     |\r\n| unbuilt | via SaveModel, will fail with a SaveModel error | via `object.__reduce__` |", "Can you fix the sanity build please?\r\n\r\n", "I gave it a shot, can you re-run the tests?\r\n\r\nI appreciate your patience in re-running tests, I haven't been able to get the sanity checks to run locally ([issue](https://github.com/tensorflow/tensorflow/issues/47989)).", "Progress!\r\n\r\nNow I'm only seeing a failure in Windows.:\r\n\r\n```\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2090, in __deepcopy__\r\n    deserializer, serialized = pack_model(self)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\pickle_utils.py\", line 75, in pack_model\r\n    info.size = f.size()\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 98, in size\r\n    return stat(self.__name).length\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 871, in stat\r\n    return stat_v2(filename)\r\n  File \"C:\\Python37\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 887, in stat_v2\r\n    return _pywrap_file_io.Stat(compat.path_to_str(path))\r\ntensorflow.python.framework.errors_impl.NotFoundError: \\assets; No such file or directory\r\n```\r\n\r\nIs it possible that `stat` is not implemented in Windows?", "[It is implemented](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/core/platform/windows/windows_file_system.cc;l=616;drc=f9fd71a45e04b3f0cef48b3283c54f69ad1a0991)", "Hmm, upon closer look it seems that what is happening is that gfile's `size`, `exists`, etc. are reporting that files don't exist even though `walk` is reporting them as existing. This is only happening on Windows and only for `ram://`.\r\n\r\nI was able to make a minimal reproducible example, can you take a look?\r\n\r\n[windows logs (FAIL)](https://github.com/adriangb/tensorflow-test/runs/2186564165?check_suite_focus=true)\r\n[ubuntu logs (PASS)](https://github.com/adriangb/tensorflow-test/runs/2186564130?check_suite_focus=true)\r\n[source code](https://github.com/adriangb/tensorflow-test/blob/master/test.py)\r\n\r\nI made a repo and run it in GitHub Actions since I don't have a WIndows machine and Colab doesn't run on Windows.", "I was able to dive deeper and find the root cause. On windows, `walk` is returning a leading `\\`. So if you have `ram://test/test.txt` and do `walk(ram://test)` you get back `root=\"ram://test\"` (correct) but `filename=\"\\test.txt\"`. Calling `os.path.join(\"ram://test\", \"\\test.txt\")` on Windows (or the equivalent `os.path.join(\"ram://test\", \"/test.txt\")` on nix) returns just `\\test.txt`). Since just `\\test.txt` does not exist, things fail.\r\n\r\nThis seems like a but in the Windows implementation of `walk`. Is that correct? I can open an issue for it.", "Is it walk or is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/ram_file_system.h#L347 and we need to use `std::filesystem::path::preferred_separator`?", "I think the `filenames` return value of `walk` should be a list of bare filenames with no separator.", "This is currently blocked by #48125 and/or #48124", "I've closed https://github.com/tensorflow/tensorflow/pull/48124 as it seems that we want to use `/` sep for `://` other than `file`", "Are these remaining CI failures relevant?", "@jakirkham dep on https://github.com/tensorflow/tensorflow/pull/48125", "@adriangb  Can you please resolve conflicts? Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "This is not stalled. It is blocked by #48125. I didn't see any comments from you to respond to.", "@mihaimaruseac  Can you please review this PR ? Thanks!", "FYI I moved this PR to the Keras repo (https://github.com/keras-team/keras/pull/14748) and have been working with Fran\u00e7ois to get it merged. I'm going to close it here to avoid further confusion, sorry for not doing that earlier!"]}, {"number": 39608, "title": "Equation not being rendered in docs page", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/agents/tutorials/0_intro_rl\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe equation after \"The optimal Q-function obeys the following Bellman optimality equation:\" is not rendering correctly. This is what I see on my screen:\r\n\r\n```\r\n$\\begin{equation} Q^(s, a) = \\mathbb{E}\\left[ r + \\gamma \\max_{a'} Q^(s', a')\\right] \\end{equation}$\r\n```\r\n", "comments": ["Reassigning to the TensorFlow Docs team ([`tensorflow/docs`](https://www.github.com/tensorflow/docs)). @lamberta, FYI."]}, {"number": 39607, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 64 bit\r\n- TensorFlow installed from (source or binary): I used conda \r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.5\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source): Spyder (?) (is this meant?)\r\n- CUDA/cuDNN version: 7.6.5  (cuda 10.0)\r\n- GPU model and memory: Intel(R) UHD Graphics 620,  4164MB\r\nIntel(R) Core(TM) I5-8650U CPU @ 1.70GHz 1.90GHz  ,  8.00 GB RAM\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen i try to import tensorflow in spyder:\r\n\r\nimport tensorflow as tf\r\n\r\nI get the following error:\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-64156d691fe5>\", line 1, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\rensj\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n.\r\n---\r\nI found some similar topics, and i updated my visual studios and drivers now I think. Still the same error.  Please help \r\n", "comments": ["Closing as duplicate.\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/36167#issuecomment-577886156\r\n\r\nJust to sample over 100 similar issues: #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\n\r\nPlease make sure you do a search in the future.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39607\">No</a>\n"]}, {"number": 39606, "title": "tensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol Zeros is already exposed as ()", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nI tried to install object_detection model and followed these instructions (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md). \r\nBut the line \"import tensorflow as tf\" raised this:\r\n```\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/__init__.py\", line 39, in <module>\r\n    from tensorflow.python.keras import ops\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/ops.py\", line 30, in <module>\r\n    init_ops.Zeros)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/tf_export.py\", line 310, in __call__\r\n    self.set_attr(undecorated_func, api_names_attr, self._names)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/tf_export.py\", line 322, in set_attr\r\n    (func.__name__, getattr(func, api_names_attr)))  # pylint: disable=protected-access\r\ntensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol Zeros is already exposed as ().\r\n```\r\nI am new to all of this and I have no idea how to deal with it.", "comments": ["@fevrill \r\nIs there any particular version for using such an old version of tensor flow, can you try using a later version and let us know if it helps", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39606\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39606\">No</a>\n", "I also experienced the same whenever I write the `import tensorflow as tf`", "I experienced the same thing with you. I changed the python version to 3.6, while the version of tensorflow is still 1.14, now it can work.", "Hi i am having same issue\r\ni am using:\r\npython:3.9.5\r\ntensorflow: 2.7.0\r\ncan someone help me please?"]}, {"number": 39605, "title": "Memory Usage of a model", "body": "I'd like to have a method which returns the needed memory for a model since the allocated VRAM does not reflect the actual usage.", "comments": ["@Arktius \r\n\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Please, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "- Microsoft Windows [Version 10.0.18362.418]\r\n- Nvidia 2060 Super\r\n- TF version 2.0.0\r\n- Are you willing to contribute it (Yes/No): Don't know yet.\r\n\r\n**Who will benefit with this feature?**\r\n- Everyone who's interested in optimizing lots of models while logging needed memory. \r\n- Everyone who wants to shift an application to another machine with a different hardware setup.\r\n\r\nThe problem is that the following commands do not free the allocated memory.\r\n```\r\ntf.compat.v1.reset_default_graph()\r\ntf.keras.backend.clear_session()\r\ngc.collect()\r\n```\r\nOne can use \r\n```\r\ngpu = tf.config.experimental.list_physical_devices('GPU') \r\ntf.config.experimental.set_memory_growth(gpu[0], False)\r\n```\r\nto give as much memory as the GPU needs. Nevertheless, the allocated memory is correctly displayed only for a single model. After training several models, the allocated memory that is displayed by nvidia-smi is the one of the largest model. \r\n\r\nHow can I submit a PR(pull request)? ", "For submitting a pull request see https://www.tensorflow.org/community/contribute/code", "Hi @Arktius ! Does [get_memory_usage api ](https://www.tensorflow.org/api_docs/python/tf/config/experimental/get_memory_usage)resolve this issue? You can add this before model training and get memory usage of the model training operation. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39604, "title": "Getting error saved model to tflite - Float 64 error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 2.2.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# Copy and paste here the exact command\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_file_pb)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\n```\r\n**The output from the converter invocation**\r\nERROR\r\n``` \r\n# Copy and paste the output here.\r\n\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-05-16 20:04:12.645740: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f98f6906430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-16 20:04:12.645752: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-05-16 20:04:12.885160: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\r\n2020-05-16 20:04:13.539394: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: MODEL/PB/64/tf_i11_v210/\r\n2020-05-16 20:04:13.765676: I tensorflow/cc/saved_model/loader.cc:364] SavedModel load for tags { serve }; Status: success: OK. Took 1183597 microseconds.\r\nerror: type of return operand 2 ('tensor<?x400xf32>') doesn't match function result type ('tensor<?x400xf64>')\r\nTraceback (most recent call last):\r\n  File \"/Users/visood/opt/anaconda3/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/Users/visood/opt/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/Users/visood/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/Users/visood/opt/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/Users/visood/opt/anaconda3/lib/python3.7/site-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/Users/visood/opt/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: type of return operand 2 ('tensor<?x400xf32>') doesn't match function result type ('tensor<?x400xf64>')\r\n<unknown>:0: note: see current operation: \"std.return\"(%12, %11, %13, %14, %15) : (tensor<?x400xf64>, tensor<?x1x400xf64>, tensor<?x400xf32>, tensor<?x400xf32>, tensor<f32>) -> ()\r\n```\r\n**Also, please include a link to the saved model or GraphDef**\r\nhttps://github.com/vissood/resources/tree/master/TFLITE_ISSUE\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing wrong results and/or decrease in accuracy\r\n- Producing correct results, but the model is slower than expected (model generated from old converter)\r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @vissood,\r\nMany TFLite builtin ops don't support float64  as an input type. This is on the map however, @abattery  might be able to give more of an update.", "@vissood,\r\nI am facing an error stating `ValueError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /content/saved_model/1/variables/variables: Not found: /content/saved_model/1/variables; No such file or directory` on running the code. \r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/efc55572411528527be94efaddb5c4c1/39604.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please share all the files in the saved model folder. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39603, "title": "Tensorflow profiler returning the wrong total_float_ops numbers for Keras Model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI'm trying to calculate FLOPs, and MAC's for the Keras models. I'm using a Tensorflow profiler from tensorflow.compat.v1. I API. I've attached the code snippet to calculate the FLOPs by using the saved Keras model. The profiler returning an incremental total_float_ops values for the same model whenever I execute.\r\n**Describe the expected behavior**\r\nSince no changes in the saved model, the developer expects the same total_float_ops numbers. \r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\ndef get_flops(model_h5_path):\r\n    session = tf.compat.v1.Session()\r\n    graph = tf.compat.v1.get_default_graph()\r\n        \r\n\r\n    with graph.as_default():\r\n        with session.as_default():\r\n            model = tf.keras.models.load_model(model_h5_path)\r\n\r\n            run_meta = tf.compat.v1.RunMetadata()\r\n            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\r\n        \r\n            # We use the Keras session graph in the call to the profiler.\r\n            flops = tf.compat.v1.profiler.profile(graph=graph,\r\n                                                  run_meta=run_meta, cmd='op', options=opts)\r\n            print(flops)\r\n            return flops.total_float_ops\r\n\r\nFLOPs = get_flops(\"model.h5\")\r\n```\r\n```bash\r\nFor Example:\r\nFirst Time:\r\nname: \"_TFProfRoot\"\r\ntotal_float_ops: 851781854\r\n\r\nSecond Time:\r\nname: \"_TFProfRoot\"\r\ntotal_float_ops: 929216568\r\n\r\nThird Time:\r\nname: \"_TFProfRoot\"\r\ntotal_float_ops: 1006651282\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.", "comments": ["@nullbyte91 \r\nI ran the code shared and face a different error, please find [gist here](https://colab.sandbox.google.com/gist/Saduf2019/dabcbc4e05fe45988ad390cf2fd86d3f/untitled183.ipynb)", "@Saduf2019 You need a Keras saved model to test the get_flops function. Please let me know if you need dummy models to test.", "@nullbyte91 \r\nYes please share all dependencies or if possible share a colab gist with the error faced for us to analyse", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39603\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39603\">No</a>\n"]}, {"number": 39602, "title": "model.evaluate generates different accuracy and loss values on the same network and test dataset ", "body": "System Info:\r\nWindows 10 \r\nTensorflow 2.1.0\r\nIm using GPU Nvidia v100-sxm2 on a cluster. \r\n\r\nI have trained a CNN model using Tensorflow 2 for gesture classification. The training is done and the results are good. my problem is for my task (Im trying to use Reinforcement Learning to prune the network) I need to evaluate the network many times. I have not changed the testing dataset. and didnt change anything about the network between evaluating. I have tried to freeze the network `model.trainable = False `  I thought it might have an influece but didn't help. \r\n\r\nI know it's not an issue but I already asked in [stackoverflow](https://stackoverflow.com/questions/61822122/running-model-evaluate-many-times-results-different-accuracy-and-loss-value-tens) but didn't get any useful answer and it effects my job alot. \r\n\r\nmy code:\r\n\r\nmaking dataset:\r\n\r\n```\r\ndef split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\r\n\r\n    validation_data_percent = round(validation_data_fraction * 100)\r\n    if not (0 <= validation_data_percent <= 100):\r\n        raise ValueError(\"validation data fraction must be \u2208 [0,1]\")\r\n\r\n    dataset = dataset.enumerate()\r\n    train_dataset = dataset.filter(lambda f, data: f % 100 >= validation_data_percent)\r\n    validation_dataset = dataset.filter(lambda f, data: f % 100 < validation_data_percent)\r\n\r\n    # remove enumeration\r\n    train_dataset = train_dataset.map(lambda f, data: data)\r\n    validation_dataset = validation_dataset.map(lambda f, data: data)\r\n\r\n    return train_dataset, validation_dataset\r\n\r\ndef load_data(path):\r\n    data, label = data_prep(path)\r\n    dataset = tf.data.Dataset.from_tensor_slices((data, label))\r\n    dataset = dataset.shuffle(100000)\r\n    train_dataset, rest = split_dataset(dataset, 0.3)\r\n    test_dataset, valid_dataset = split_dataset(rest, 0.5)\r\n    train_data = train_dataset.shuffle(1000).batch(10)\r\n    valid_data = valid_dataset.batch(10)\r\n    test_data = test_dataset.batch(10)\r\n    return train_data, valid_data, test_data\r\n```\r\n\r\nmy model:\r\n\r\n```\r\ndef make_model():\r\n    model = tf.keras.Sequential([\r\n    Input((1,30,30)),\r\n    Conv2D(filters = 8, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"c1\", data_format=\"channels_first\"),\r\n    Conv2D(filters = 16, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"c2\", data_format=\"channels_first\"),\r\n    MaxPool2D(pool_size=(2,2), strides=(1,1),padding=\"same\", name=\"m1\", data_format=\"channels_first\"),\r\n    \r\n    Conv2D(filters = 16, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"c3\", data_format=\"channels_first\"),\r\n    MaxPool2D(pool_size=(2,2), strides=(1,1),padding=\"same\", name=\"m2\",data_format=\"channels_first\"),\r\n    \r\n    Flatten(),\r\n    Dense(256, activation=\"relu\", use_bias=True),\r\n    Dense(5,  use_bias=True)])\r\n    return model\r\n\r\n\r\nmodel = make_model()\r\nprint(model.summary())\r\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\r\nmodel.fit(train_data, verbose=1, validation_data=valid_data, epochs=20)\r\n```\r\n\r\nand evaluating:\r\n\r\n```\r\nmodel.evaluate(test_data)\r\n\r\n```\r\n", "comments": ["Shuffling the data and splitting it to train and validation set is a good way to make both the sets more uniformly distributed. Though a random shuffle does not always guarantees the uniform distribution always. There may be cases where some data is present in the validation set but not present in the training set, hence changing the accuracy of the model. The changes in accuracy are however very minute.", "@deadshotsb Thats true. But the thing is even if all the weights are random and the data is bad it shouldn't change the output of the network if I evaluate the same data N times in a row. The problem is not the performance of the network, it's actually working very well, but the fact that it changes. I'm not able to understand this small randomnes in the output. here is for example the result of 5 times running `model.evaluate (test_data)` in a row:\r\n\r\n 885/Unknown - 2s 2ms/step - loss: 0.1039 - accuracy: 0.9663\r\n885/Unknown - 2s 2ms/step - loss: 0.0959 - accuracy: 0.9675\r\n885/Unknown - 2s 2ms/step - loss: 0.0999 - accuracy: 0.9661\r\n885/Unknown - 2s 2ms/step - loss: 0.0888 - accuracy: 0.9688\r\n885/Unknown - 2s 2ms/step - loss: 0.0799 - accuracy: 0.9715 ", "@hamidkhb Consider that your data contains samples of A (95%) and B (5%). But when we are randomly arranging the dataset into the train and validation, a case may occur that only few samples of that 5% of B goes to training set and majority of B sample goes to validation. So in this case, I don't expect my model to perform well in predicting of sample B. Whereas if the opposite case occurs that I have maximum of B samples in my training set, then it is expected to perform well in predicting sample B. Generally, it occurs due to huge difference between each kind of samples in the dataset.", "@deadshotsb I understand this and this is true. But it still doesn't explain the random output of the test. the test dataset contains specific number of images that aren't changing every time I use them to evaluate the same network the output is different. Bad distribution of datasets can't explain this. putting aside that the network is performing very well on unseen data (the results are in my previous post). ", "I found the Problem. apparantely running tf.data.Dataset changes the order of the data in the dataset. It's working somehow as a random iterator and does that every time one uses it. thats why it's generating a bit of randomnes", "yup told you", "@deadshotsb It wasn't caused by the distribution of the data. Like mentioned before the performance of the network is very well. It was caused by different order of the **same** data in the iterator of test dataset. This caused different mean value of loss and accuracy. evaluating the same data in test dataset **manually** solved the problem. "]}, {"number": 39601, "title": "REDUCE_MAX operator support for int8 tflite", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.4\r\n- TensorFlow installed from (source or binary): tf-nightly (2.2.0-dev20200405)\r\n\r\n**Any other info / logs**\r\n\r\nI am using tf-nightly that integrates the new converter and trying to export a fully quantized int8 model using post-training quantization with calibration. \r\n\r\nThe output model holds int8 operators including int8 input and output nodes. However, there is a specific node **REDUCE_MAX** that seems not to have support in tflite, therefore, there are quantization and dequantization operations around it, ending-up having a \"hybrid\" model with reference fp32 implementations as is shown below.\r\n\r\n\r\n<img width=\"441\" alt=\"Screenshot 2020-05-16 at 14 34 26\" src=\"https://user-images.githubusercontent.com/3832904/82121078-5c66b900-9782-11ea-83a5-8f3d33dfc66a.png\">\r\n\r\nAre there any plans for this OP to be supported in tflite and in TFLu in the future? Is it possible that this operator is already supported but the properties of the operator won't allow full quantization? \r\n\r\nThanks!", "comments": ["@kmonachopoulos,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Do you want me to provide the whole source code that I used to train the model, the code to export it, and the code for inference? I think that the description of this issue is pretty straight forward above..", "@kmonachopoulos,\r\nCould you please provide the code to generate the model and to convert it, so that we can take a look the nodes in it. Thanks!", "The code is integrated into a larger framework called DeePavlov that is commonly used for NLP tasks. I can link the repo but I don't think that this is sufficient. Actually, I think that the issue raised here is pretty straight forward ->  **REDUCE_MAX** is not supported in int8... ", "I believe we have a kernel that supports this, so we may need to look into the conversion. @suharshs can assist.", "> \r\n> \r\n> I believe we have a kernel that supports this, so we may need to look into the conversion. @suharshs can assist.\r\n\r\nYes, it appears this op is not listed in [GetOperatorProperty](https://github.com/tensorflow/tensorflow/blob/079520f252ce6ba7788b81904a40fb98895b288d/tensorflow/lite/tools/optimize/operator_property.cc#L67).", "I would also like use `reduce_max` in an 8 bit quantized model. Is it planned to be added in the near future?\r\n\r\n@kmonachopoulos were you able to find a workaround?", "> I would also like use `reduce_max` in an 8 bit quantized model. Is it planned to be added in the near future?\r\n> \r\n> @kmonachopoulos were you able to find a workaround?\r\n\r\nNot yet, waiting for support as well!", "Ah bummer. Thanks for letting me know, @kmonachopoulos ! I did see this PR (https://github.com/tensorflow/tensorflow/pull/39946) adding `reduce_max` for TFLite micro. Maybe it can be ported back to TFLite?"]}, {"number": 39600, "title": "overlapping bounding boxes", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution : Linux Ubuntu 18.04\r\n\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.7.4 \r\n\r\n- CUDA/cuDNN version: 10.2 \r\n- GPU model and memory: GeForce GTX 1050\r\n\r\n\r\n\r\n**Describe the current behavior**\r\ni trained an object detection model using resnet101 faster rcnn, i keep getting these overlapping bounding boxes ( i know that i should apply IoU) but couldn't find a way to do it\r\n![Screenshot from 2020-05-16 13-43-54](https://user-images.githubusercontent.com/62834628/82120053-54efe180-977b-11ea-903d-5e210a28d286.png)\r\n\r\nhere's the python code that i'm running : \r\n```\r\n######## Image Object Detection Using Tensorflow-trained Classifier #########\r\n#\r\n# Author: Evan Juras\r\n# Date: 1/15/18\r\n# Description:\r\n# This program uses a TensorFlow-trained neural network to perform object detection.\r\n# It loads the classifier and uses it to perform object detection on an image.\r\n# It draws boxes, scores, and labels around the objects of interest in the image.\r\n\r\n## Some of the code is copied from Google's example at\r\n## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n\r\n## and some is copied from Dat Tran's example at\r\n## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\r\n\r\n## but I changed it to make it more understandable to me.\r\n\r\n# Import packages\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\n\r\n# This is needed since the notebook is stored in the object_detection folder.\r\nsys.path.append(\"..\")\r\n\r\n# Import utilites\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\n\r\n# Name of the directory containing the object detection module we're using\r\nMODEL_NAME = 'trio'\r\nIMAGE_NAME = '2.jpg'\r\n\r\n# Grab path to current working directory\r\nCWD_PATH = os.getcwd()\r\n\r\n# Path to frozen detection graph .pb file, which contains the model that is used\r\n# for object detection.\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\r\n\r\n# Path to label map file\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,'training','tfobject.pbtxt')\r\n\r\n# Path to image\r\nPATH_TO_IMAGE = os.path.join(CWD_PATH,'test_images',IMAGE_NAME)\r\n\r\n# Number of classes the object detector can identify\r\nNUM_CLASSES = 3\r\n\r\n# Load the label map.\r\n# Label maps map indices to category names, so that when our convolution\r\n# network predicts `5`, we know that this corresponds to `king`.\r\n# Here we use internal utility functions, but anything that returns a\r\n# dictionary mapping integers to appropriate string labels would be fine\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n# Load the Tensorflow model into memory.\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.compat.v1.GraphDef()\r\n    with tf.compat.v2.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\n    sess = tf.Session(graph=detection_graph)\r\n\r\n# Define input and output tensors (i.e. data) for the object detection classifier\r\n\r\n# Input tensor is the image\r\n\r\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n\r\n# Output tensors are the detection boxes, scores, and classes\r\n# Each box represents a part of the image where a particular object was detected\r\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n\r\n# Each score represents level of confidence for each of the objects.\r\n# The score is shown on the result image, together with the class label.\r\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n\r\n# Number of objects detected\r\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\n# Load image using OpenCV and\r\n# expand image dimensions to have shape: [1, None, None, 3]\r\n# i.e. a single-column array, where each item in the column has the pixel RGB value\r\nimage = cv2.imread(PATH_TO_IMAGE)\r\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\nimage_expanded = np.expand_dims(image_rgb, axis=0)\r\n\r\n# Perform the actual detection by running the model with the image as input\r\n(boxes, scores, classes, num) = sess.run(\r\n    [detection_boxes, detection_scores, detection_classes, num_detections],\r\n    feed_dict={image_tensor: image_expanded})\r\n\r\n# Draw the results of the detection (aka 'visulaize the results')\r\n\r\nvis_util.visualize_boxes_and_labels_on_image_array(\r\n    image,\r\n    np.squeeze(boxes),\r\n    np.squeeze(classes).astype(np.int32),\r\n    np.squeeze(scores),\r\n    category_index,\r\n    use_normalized_coordinates=True,\r\n    line_thickness=8,\r\n    min_score_thresh=0.8)\r\n\r\n# All the results have been drawn on image. Now display the image.\r\ncv2.imshow('Object detector', image)\r\n\r\n# Press any key to close the image\r\ncv2.waitKey(0)\r\n\r\n# Clean up\r\ncv2.destroyAllWindows()\r\n```\r\n\r\n\r\n", "comments": ["@TekayaNidham \r\nI ran the code shared above and face a different error, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/1b089e338a264a2621236bf0394102af/untitled184.ipynb)", "@Saduf2019 it's taken from models/research/object_detection/utils\r\nhttps://github.com/tensorflow/models/tree/master/research/object_detection/utils\r\ni just placed the folder in my workspace ", "However, i ran \r\n`python model_main.py --alsologtostderr --run_once --checkpoint_dir=training/ --model_dir=eval/ --pipeline_config_path=training/rfcn_resnet101.config`\r\nand :\r\n`tensorboard logdir='eval'`\r\ninto the object detection api\r\nand i got this on tensorboard \r\n\r\n![Screenshot_2020-05-17 TensorBoard(1)](https://user-images.githubusercontent.com/62834628/82154414-a1afe700-9865-11ea-90ef-9f6728faeb43.png)\r\n\r\n\r\n![Screenshot_2020-05-17 TensorBoard](https://user-images.githubusercontent.com/62834628/82154403-89d86300-9865-11ea-94f6-c8dfe4694fd6.png)\r\n\r\ni'm wondering how can i get this ground truth into the code i'm using \r\n", "@TekayaNidham  \r\nWould you want to try in later version of tensorflow as this is a very old version that you are using and let us know if that helps.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39599, "title": "what-if-tool link broken in tensorflow.org Tools page", "body": "#38076  URL(s) with the issue:\r\nhttps://www.tensorflow.org/resources/tools#\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/resources/tools#\r\n\r\n## Description of issue (what needs changing):\r\nLink to what-if-tool is broken, as what-if moved to new repo.\r\n\r\n### Clear description\r\nCurrent broken link, when someone clicks on \"Get Started\" link at \r\nhttps://www.tensorflow.org/resources/tools\r\n\r\nBroken link ->\r\nhttps://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/interactive_inference/What_If_Tool_Notebook_Usage.ipynb\r\n\r\nNew Correct link should be ->\r\nhttps://github.com/PAIR-code/what-if-tool/blob/master/What_If_Tool_Notebook_Usage.ipynb\r\n\r\nFor example, why should someone use this method? How is it useful?\r\n\r\n### Correct links\r\nhttps://github.com/PAIR-code/what-if-tool/blob/master/What_If_Tool_Notebook_Usage.ipynb\r\n\r\nI wanted to submit a pull-request for this\r\n", "comments": ["Hi @bikashkumars, thanks for reporting this. \r\n\r\nI've converted this to an internal bug assigned to the page owner (that's a page I can't fix). \r\n\r\nI'm closing this as we have the internal bug now.\r\n\r\nThanks."]}, {"number": 39598, "title": "Autograph AssertionError on custom layer call", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\nThe errors says, \"Please report this to the TensorFlow team. When filing the bug...\"\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes, custom layer\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nNo\r\n- TensorFlow installed from (source or binary):\r\nAnaconda environment with pip install\r\n- TensorFlow version (use command below):\r\nv2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version:\r\nPython 3.6.10 |Anaconda, Inc.| (default, Mar 23 2020, 17:58:33) [MSC v.1916 64 bit (AMD64)]\r\n- Bazel version (if compiling from source):\r\nN/a\r\n- GCC/Compiler version (if compiling from source):\r\nN/a\r\n- CUDA/cuDNN version:\r\n10.1\r\n- GPU model and memory:\r\nGeForce RTX 2080 Ti 32Gb\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\n\r\nRun from Anaconda\r\n\r\n== check python ===================================================\r\n\r\n== check os platform ===============================================\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\n./tf_env_collect.sh: line 102: c++: command not found\r\n\r\n== check pips ===================================================\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n\r\n== tensorflow import ============================================\r\nTraceback (most recent call last):\r\n\r\n  File \"<string>\", line 1, in <module>\r\n\r\nModuleNotFoundError: No module named 'tensorflow'\r\n\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 8, 2, 'final', 0)\r\n\r\n\r\n== bazel version  ===============================================\r\n\r\n== check python ===================================================\r\npython version: 3.6.10\r\n\r\npython branch: \r\n\r\npython build version: ('default', 'Mar 23 2020 17:58:33')\r\n\r\npython compiler version: MSC v.1916 64 bit (AMD64)\r\n\r\npython implementation: CPython\r\n\r\n\r\n\r\n\r\n== check os platform ===============================================\r\nos: Windows\r\n\r\nos kernel version: 10.0.18362\r\n\r\nos release version: 10\r\n\r\nos platform: Windows-10-10.0.18362-SP0\r\n\r\nlinux distribution: ('', '', '')\r\n\r\nlinux os distribution: ('', '', '')\r\n\r\nmac version: ('', ('', '', ''), '')\r\n\r\nuname: uname_result(system='Windows', node='DESKTOP-5O07H5P', release='10', version='10.0.18362', machine='AMD64', processor='Intel64 Family 6 Model 158 Stepping 13, GenuineIntel')\r\n\r\narchitecture: ('64bit', 'WindowsPE')\r\n\r\nmachine: AMD64\r\n\r\n\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nbash: c++: command not found\r\n\r\n== check pips ===================================================\r\nnumpy                         1.18.1                                        \r\n\r\nnumpydoc                      0.9.2                                         \r\n\r\nprotobuf                      3.11.4                                        \r\n\r\ntensorflow-docs               0.0.084618e76cde6edf23c2e71fd1d126c3d18ba53d5-\r\n\r\ntensorflow-gpu                2.1.0                                         \r\n\r\ntensorflow-gpu-estimator      2.1.0                                         \r\n\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.1.0\r\n\r\ntf.version.GIT_VERSION = v2.1.0-rc2-17-ge5bf8de410\r\n\r\ntf.version.COMPILER_VERSION = MSVC 192428314\r\n\r\n2020-05-16 10:55:03.809733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nbash: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 6, 10, 'final', 0)\r\n\r\n\r\n== bazel version  ===============================================\r\n\r\n\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen building a model containing my custom layer I get this message: I don't get it on the first time the code is run.\r\n\r\nPython 3.6.10 |Anaconda, Inc.| (default, Mar 23 2020, 17:58:33) [MSC v.1916 64 bit (AMD64)]\r\nTensorFlow version: 2.1.0\r\nv2.1.0-rc2-17-ge5bf8de410 2.1.0\r\nKeras version: 2.2.4-tf\r\ntf.Tensor([[150. 150. 150. 150. 150.]], shape=(1, 5), dtype=float32)\r\nINFO:tensorflow:Converted call: <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>>\r\n    args: (<tf.Tensor 'Placeholder:0' shape=(1, 5) dtype=float32>,)\r\n    kwargs: {}\r\n\r\nINFO:tensorflow:Not whitelisted: <method-wrapper '__call__' of method object at 0x00000299AA71E088>: default rule\r\nINFO:tensorflow:Not whitelisted: <class 'layers.Linear'>: default rule\r\nINFO:tensorflow:Not whitelisted: <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>>: default rule\r\nINFO:tensorflow:Cache hit for entity <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>> key <code object call at 0x00000299CF180780, file \"C:\\Users\\ruper\\Versioning\\PCTSoftware\\Libraries\\python\\tensorflow\\pct\\pctdl\\layers.py\", line 24> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x00000299CF196F28>, frozenset()): _ConvertedEntityFactoryInfo(tf__call in tmph641gc44)\r\nINFO:tensorflow:Error transforming entity <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>>\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\ruper\\.conda\\envs\\pct-tf\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 526, in converted_call\r\n    converted_f = conversion.convert(target_entity, program_ctx)\r\n  File \"C:\\Users\\ruper\\.conda\\envs\\pct-tf\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\conversion.py\", line 328, in convert\r\n    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)\r\n  File \"C:\\Users\\ruper\\.conda\\envs\\pct-tf\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\conversion.py\", line 266, in _instantiate\r\n    factory = converted_entity_info.get_factory()\r\n  File \"C:\\Users\\ruper\\.conda\\envs\\pct-tf\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\conversion.py\", line 92, in get_factory\r\n    assert self.module_name in sys.modules\r\nAssertionError\r\nWARNING:tensorflow:AutoGraph could not transform <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: \r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI don't want the error/warning message (not sure what it means).\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n[test-layer.txt](https://github.com/tensorflow/tensorflow/files/4638168/test-layer.txt)\r\n[layers.txt](https://github.com/tensorflow/tensorflow/files/4638169/layers.txt)\r\n\r\nI have added the test code. Change to .py extension.\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@ruperty,\r\nI did not observe the `WARNING:tensorflow:AutoGraph could not transform` warning on running the code with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/b3c2f0ec76039aebde67d10b788dc0d7/39598-2-1.ipynb) and [TF v2.2](https://colab.research.google.com/gist/amahendrakar/b773f46a9f7ae8ff83be435fc21d3be1/39598.ipynb). Please find the attached gist.\r\n\r\nPlease take a look at [this](https://github.com/tensorflow/tensorflow/issues/38691) similar issue and set [tf.autograph.set_verbosity](\r\nhttps://www.tensorflow.org/api_docs/python/tf/autograph/set_verbosity#for_example) to a lower value to disable the logs. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Yes, I did set verbosity lower to get rid of it. I was just reporting it \nbecause the message said it was a bug.\n\nI didn't see any attachment. What is a gist?\n\nRegards,\nRupert\n\n", "@ruperty,\r\nThank you for reporting the bug, there is already a [PR submitted](https://github.com/tensorflow/tensorflow/issues/38947#issuecomment-620923249) for the fix.\r\n\r\nI ran the code on my end and attached the links of the Python notebook (which are saved as GitHub gist). Here are the direct links for the notebook files\r\n- TF v2.1 - https://colab.research.google.com/gist/amahendrakar/b3c2f0ec76039aebde67d10b788dc0d7/39598-2-1.ipynb\r\n\r\n- TF v2.2 -\r\nhttps://colab.research.google.com/gist/amahendrakar/b773f46a9f7ae8ff83be435fc21d3be1/39598.ipynb\r\n\r\nPlease take a look at the links and close the issue if resolved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39597, "title": "[INTEL MKL] Added input name in the bfloat16 namescope interface so t\u2026", "body": "\u2026hat it can create correct node name when it is needed. The default value of the name is '', which will make sure the change is backward compatible, and won't affect existing models which suse bfloat16 namescope", "comments": ["@cuixiaom Can you please fix build failures ? Thanks!", "> @cuixiaom Can you please fix build failures ? Thanks!\r\n\r\nYes, I have fixed the format which caused CI failure, the other failures were not caused by my changes.", "Since this changes `defaults` from `None` to `[]` it now needs a review from API owners", "> @reedwm - is there a way to do this with v2 APIs? We want to make sure we're not locking people in to v1 APIs.\r\n\r\nThe `tf.keras.mixed_precision` API is the recommended way of using bfloat16 in TF 2. Altneratively, \r\nthis can be done with a grappler pass once I update and merge #34504 (I'll do this soon), but we strongly recommend `tf.keras.mixed_precision` over the grappler pass.", "Thanks Reed. That leaves the question Karmel asked earlier.", "For @tensorflow/api-owners:\r\n\r\n@cuixiaom Note that this will not be included in any v1 release. Is there a particular reason we want to update the v1 APIs here?"]}, {"number": 39596, "title": "tf.keras.layers.Lambda can't infer output dimension of tf.slice", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\ntf.keras.layers.Lambda can't infer output dimension of tf.slice even if the input dimension is partially known.\r\n\r\n**Describe the expected behavior**\r\n\r\nthe output dimension should be partially known as input dimension.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\n#!/usr/bin/python3\r\nimport tensorflow as tf;\r\ninputs=tf.keras.Input((None,32));\r\nresults=tf.keras.layers.Lambda(lambda x: tf.slice(x, (0,0,0),(-1,tf.shape(x)[1] - 1,-1)))(inputs);\r\nprint(results.shape); # the last dimension should be 32\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@breadbread1984 \r\nI ran the code shared by you, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/efe9afccaf94dfd6e50f8d0e53e36f9d/untitled183.ipynb) and let us know id this confirms your issue", "yes it confirms the reported issue. the last dimension should be 32 but none.", "@breadbread1984 We need to use static shape (`x.shape`) instead of dynamic shape (`tf.shape(x)`) as dynamic shape doesn't support `None` in any dimension. check [this](https://stackoverflow.com/questions/37096225/how-to-understand-static-shape-and-dynamic-shape-in-tensorflow) resource to understand the difference between static and dynamic shapes of tensors. \r\n\r\nI have updated your code to \r\n\r\n```\r\ninputs=tf.keras.Input((2,32));\r\nresults=tf.keras.layers.Lambda(lambda x: tf.slice(x, (0,0,0),(-1,x.shape[1] - 1,-1)))(inputs);\r\nprint(results.shape); # the last dimension should be 32\r\n# the above line prints (None, 1, 32)\r\n```\r\n\r\n\r\nPlease check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/dd43e6edc936bad2663943147df2a247/untitled183.ipynb). Thanks! \r\n\r\nPlease close the issue if this was resolved for you. Thanks!", "I met the problem when I implemented wavenet with tf2. the input tensor has input shape (batch, None, 1). Slicing on the unknown dimension (axis = 1) will cause the output dimeision become all unknown. therefore, I give the above code which can reproduce the issue.\r\n\r\nthe code you provide doesn't do slice on the known dimension.", "Was able to  reproduce the issue using  TF 2.5 . Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/ec82c562ba62908a97387768334aa8b1/untitled80.ipynb).Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39596\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39596\">No</a>\n"]}, {"number": 39595, "title": "Pixel range issue with `image_dataset_from_directory` after applying `convert_image_dtype`", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.3.0-dev20200514\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nAfter creating a dataset with `image_dataset_from_directory` I am mapping it to `tf.image.convert_image_dtype` for scaling the pixel values to the range of [0, 1] and also to convert them to `tf.float32` data-type. Looks like the value range is not getting changed. \r\n\r\nHere's the self-contained code:\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\n\r\n# Get the flowers dataset\r\nflowers = tf.keras.utils.get_file(\r\n    'flower_photos',\r\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\r\n    untar=True)\r\n\r\ndef scale(image, label):\r\n    return tf.image.convert_image_dtype(image, tf.float32), label\r\n\r\nbatch = tf.keras.preprocessing.image_dataset_from_directory(flowers)\r\nbatch = batch.map(scale)\r\n\r\nimage_batch, label_batch = next(iter(batch))\r\n\r\nprint(tf.reduce_max(image_batch[1,:])) # outputs: tf.Tensor(248.96571, shape=(), dtype=float32)\r\nprint(image_batch.dtype) # outputs: <dtype: 'float32'>\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIf my understanding is correct, then `batch = batch.map(scale)` should already take care of the scaling step. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n[Colab Gist](https://colab.research.google.com/gist/sayakpaul/831f2a584f92cdcc97f81bf0eb344ad1/scratchpad.ipynb)\r\n", "comments": ["@jvishnuvardhan", "I have tried in colab with TF nIghtly version (`2.3.0-dev20200516`) and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/9d665bc0d9e4cfaea247e310eed3a8e8/untitled897.ipynb).Thanks!", "`tf.image.convert_image_dtype` expects the image to be between 0,1 if the type is float which is your case. \r\nI am attaching the excerpt from the [link](https://www.tensorflow.org/api_docs/python/tf/image/convert_image_dtype)\r\n**Images that are represented using floating point values are expected to have values in the range [0,1). Image data stored in integer data types are expected to have values in the range [0,MAX], where MAX is the largest positive representable number for the data type.**\r\n\r\nNow coming back to your issue. Since `image_dataset_from_directory` does not provide rescaling option either you can use `ImageDataGenerator` which provides rescaling option and then convert it to `tf.data.Dataset` object using `tf.data.Dataset.from_generator` or process the output from `image_dataset_from_directory` as follows:\r\n```\r\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling\r\ndataset = image_dataset_from_directory(directory)\r\nrescale = Rescaling(scale=1.0/255)\r\nrescaled_dataset = dataset.map(lambda image,label:(rescale(image),label))\r\n```\r\nIn your case map your `batch` with this `rescale` layer. Also check the documentation for Rescaling [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Rescaling)\r\n", "Makes sense, thank you. I am aware of the other options you suggested. I am gonna close this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39595\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39595\">No</a>\n"]}, {"number": 39594, "title": "Simple Quantization aware training (TF Tutorial) throws a warning", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- TensorFlow installed from (source or binary): 2.2\r\n- TensorFlow version (or github SHA if from source):\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/quantization/training_example.ipynb\r\n\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:465: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\r\nInstructions for updating:\r\nSimply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:105: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: /tmp/tmpjvd7obgu/assets\r\n```\r\n\r\n**Failure details**\r\nThere is no failure as such. But there is a warning which is not clear to any common user. It would be helpful if the warning guides the user to root-cause of the issue so that user will update `training` argument.\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["We're currently working on removing these warnings and will update this issue once it's resolved.", "Issue has been resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39594\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39594\">No</a>\n", "I have also on TensorFlow 2.2 with Linux 18.04 and trying transfer learning with a config file. I got the same warning and the process just stopped but not terminating.\r\n```\r\nWARNING:tensorflow:From /root/.local/lib/python3.6/site-packages/object_detection/model_lib_v2.py:355: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\r\nInstructions for updating:\r\nSimply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\r\nW1102 19:19:05.185284 140450597959424 deprecation.py:323] From /root/.local/lib/python3.6/site-packages/object_detection/model_lib_v2.py:355: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.\r\nInstructions for updating:\r\nSimply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\r\n```\r\n ", "@sammilei As this issue is related to a warning that we print in the console, it should ideally not cause issues when training a model. Please file a [New issue](https://github.com/tensorflow/tensorflow/issues/new/choose) with more details. "]}, {"number": 39593, "title": "How does delayed restoration of variables in tensorflow checkpoint restore work?", "body": "I am trying to follow the checkpointing from tensorflow's checkpoint [guide][1]. I am failing to understand the exact semantics of how the variable restoration takes place. The issue that i am facing is best shown by the code bit below:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n#defining a simple subclass of keras model. Just one dense layer is present we will try to restore the kernel and bias of this layer using the `tf.train.Checkpoint`\r\n\r\nclass Net(tf.keras.Model):\r\n  \"\"\"A simple linear model.\"\"\"\r\n\r\n  def __init__(self):\r\n    super(Net, self).__init__()\r\n    self.l1 = tf.keras.layers.Dense(5)\r\n  def call(self, x):\r\n    return self.l1(x)\r\n\r\nnet = Net()\r\n\r\n#initializing the net by running it on a demo input\r\n_ = net(np.arange(4).reshape(-1,1)) #model has been initalized\r\n\r\n#checking the weights\r\nprint(net.l1.bias,net.l1.kernel)\r\n\r\n#saving the model parameters\r\nckpt = tf.train.Checkpoint(netin=net)\r\nmgr = tf.train.CheckpointManager(ckpt,'./tf_ckpt',max_to_keep=1)\r\nmgr.save()\r\n\r\n#trying to load just the bias of the net's parameters\r\ninitializer = tf.keras.initializers.Constant(value=1.)\r\nrestore_bias_here = tf.Variable(initial_value=initializer(shape=(5,)))\r\nckpt_layer = tf.train.Checkpoint(bias = restore_bias_here)\r\nckpt_net = tf.train.Checkpoint(l1 = ckpt_layer)\r\nckpt1 = tf.train.Checkpoint(netin=ckpt_net)\r\n#queing up the restores\r\nckpt1.restore(tf.train.latest_checkpoint('./tf_ckpt'))\r\n```\r\nNow, here is the problem. I can restore the kernel in a delayed fashion both from `ckpt_net` and from `ckpt_layer` but it seems i can do it only once. And the behavior shifts silently. Code to show the problem:\r\n\r\n```\r\n#and now trying to load just the kernel but from 1. ckpt_net created previously\r\ndelayed_restore_kernel_here = tf.Variable(initial_value=initializer(shape=(1,5)))\r\nprint(delayed_restore_kernel_here) #all 1's tensor\r\nckpt_layer.kernel = delayed_restore_kernel_here\r\nprint(delayed_restore_kernel_here) #the variable is loaded up perfectly\r\n\r\n#2. from #ckpt_layer created previously. But this wont succeed\r\ndelayed_restore_kernel_here = tf.Variable(initial_value=initializer(shape=(1,5)))\r\nprint(delayed_restore_kernel_here) #all 1's tensor\r\nckpt_net.l1.kernel = delayed_restore_kernel_here\r\nprint(delayed_restore_kernel_here) #all 1's tensor\r\n```\r\nI can run the last two steps in any order and only the first one will run. Can somebody explain to me the exact semantics of how the delayed restoration of variables in tensorflow works under the hood? Many thanks in advance\r\n\r\n\r\n  [1]: https://www.tensorflow.org/guide/checkpoint", "comments": ["@nitinmnsn \r\nCan you please the tensorflow version on which error is faced.\r\nIf possible please share colab gist with error faced.", "tensorflow 2.2.0\r\nubuntu 20.04\r\nNormal installation", "@nitinmnsn \r\nI ran the code shared and both the spinets mentioned run with an output, can you please refer to [this gist](https://colab.sandbox.google.com/gist/Saduf2019/66d17cb4a3b823d90d6bf0a6bcaf16b5/untitled187.ipynb) and let us know if this confirms your issue.", "@nitinmnsn\r\nplease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 39592, "title": "tensorflow.keras deadlock when using model's predict() in parallel", "body": "\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Ubuntu 20.04\r\n- Mobile device if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.10\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nIn the toy code below, I am trying to call `predict()` from a `tf.keras` model running in parallel. However, it is getting stuck because of a deadlock.\r\n\r\n**Describe the expected behavior**\r\n\r\nAll processes should have returned the result of `predict()` without getting stuck.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n You can find a minimal reproducible code in the gist below:\r\nhttps://gist.github.com/leandrocouto/a11534688dc568ea23f20171a5dc643f\r\n\r\n\r\n**Other info / logs**\r\n\r\nThere are two functions (`make_keras_picklable()` and `_unpack()`) that are a hotfix in order to make a tf.keras pickable. Apparently, when running code in parallel, the objects you pass as parameters needs to be pickled. I am using  `concurrent.futures` for the parallel calls. \r\n", "comments": ["I have tried in colab with TF version 2.2.0 and was able to reproduce the issue.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/c5a3a1595bfd728ec2b36855be893c7f/untitled895.ipynb).Thanks!", "Issue still persists in TF 2.5 . Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/4a4a126de447a862406f27319e24df3c/untitled80.ipynb).Thanks!", "Hi There,\n\n This is a stale issue. As you are using an older version of tensorflow, we are checking to see if you still need help on this issue. Please test the issue with the latest TensorFlow (TF2.7 and tf-nightly). If the issue still persists with the newer versions of TF, please feel free to open it in [keras-team/keras](https://github.com/keras-team/keras/issues) repository by providing details about the issue and a standalone code to reproduce the issue. Thanks! \n\n Please note that Keras development has moved to a separate Keras-team/keras repository to focus entirely on only Keras. Thanks! ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39592\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39592\">No</a>\n"]}, {"number": 39591, "title": "installed but cannot run", "body": "windows 10\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\ninstalled using pip3 20.1\r\npython 3.7\r\n\r\nimport tensorflow\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Shiyi\\OneDrive\\python\\tensorflow\\tshirt.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\__init__.py\", line 50, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 69, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Shiyi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: \u52a8\u6001\u94fe\u63a5\u5e93(DLL)\u521d\u59cb\u5316\u4f8b\u7a0b\u5931\u8d25\u3002\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "thanks. i switched to google colab", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39591\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39591\">No</a>\n"]}, {"number": 39590, "title": "undeclared identifier 'ABSL_FALLTHROUGH_INTENDED'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OSX 10.15.3\r\n- source\r\n- 2.2.0\r\n- Python version:3.8.2\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): bazelisk --version -> bazel 2.0.0\r\n- Clang10 and gcc10 + Apple gcc +Apple clang\r\n\r\n> tf2.2/tensorflow/core/lib/io/BUILD:211:1: C++ compilation of rule '//tensorflow/core/lib/io:cache' failed (Exit 1)\r\n> \r\n> tensorflow/core/lib/io/cache.cc:430:9: error: use of undeclared identifier 'ABSL_FALLTHROUGH_INTENDED'\r\n>         ABSL_FALLTHROUGH_INTENDED;\r\n>         ^\r\n> tensorflow/core/lib/io/cache.cc:433:9: error: use of undeclared identifier 'ABSL_FALLTHROUGH_INTENDED'\r\n>         ABSL_FALLTHROUGH_INTENDED;\r\n\r\n\r\nbazelisk build --config=opt --config=macos --config=monolithic --config=v2 --config=mkl   //tensorflow/tools/pip_package:build_pip_package\r\n", "comments": ["I met the same issue with yours on Windows. Finally I found that I let the compiler (MSVC 2019) to compile with C++ 17 standard in configuration file. And the default setting is to use C++ 14 standard, it's OK.\r\n\r\n**Seems that my present was not clear. So it should be emphasized:**\r\n**I succeeded when the compiler using C++ 14 standard.**", "> I met the same issue with yours on Windows. Finally I found that I let the compiler (MSVC 2019) to compile with C++ 17 standard in configuration file. And the default setting is to use C++ 14 standard, it's OK.\r\n\r\n@museum-future Could you try this suggestion and feedback?", "> > I met the same issue with yours on Windows. Finally I found that I let the compiler (MSVC 2019) to compile with C++ 17 standard in configuration file. And the default setting is to use C++ 14 standard, it's OK.\r\n> \r\n> @museum-future Could you try this suggestion and feedback?\r\n\r\nSorry for late reply.\r\n\r\nI removed intel MKL library \r\n\r\nI tried with -std=c++17 and  -std=gnu++17\r\nwith clang version 10.0.0 \r\n\r\nwhen building the options are overriden\r\n\r\n`tensorflow-2.2.0/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14`\r\n\r\n\r\nexternal/com_google_absl/absl/base/macros.h:149:49: note: expanded from macro 'ABSL_DEPRECATED'\r\n#define ABSL_DEPRECATED(message) __attribute__((deprecated(message)))\r\n\r\n\r\ntensorflow/python/lib/core/bfloat16.cc:635:8: error: no matching function for call to object of type '(lambda at tensorflow/python/lib/core/bfloat16.cc:609:25)'\r\n  if (!register_ufunc(\"equal\", CompareUFunc<Bfloat16EqFunctor>,\r\n", "@museum-future\r\nOK, we got it.\r\nThere is still problem when disable MKL.", "@museum-future \r\n\r\nIt's not officially support TF+MKL with OSX even though oneDNN supports. It could be that oneDNN is not compiled with openMP so oneDNN code is not threaded.\r\n\r\nIn the latest master and also TF2.4, there are some changes to use an opensource openMP, it has been tested on windows and Linux but not Mac. \r\n\r\nCould you try these branches?", "@museum-future \r\nHave you try with TF2.4?\r\nIf it's working, could you close this issue?", "@museum-future \r\n\r\nCould you feedback the suggestion?\r\n\r\nThank you!", "@museum-future Could you please let us know if you have tried as per the  [comment](https://github.com/tensorflow/tensorflow/issues/39590#issuecomment-718295491) and is it still an issue with the latest stable version of TF 2.6.0 ?Thank you! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39590\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39590\">No</a>\n"]}, {"number": 39589, "title": "Why am I getting a no gradient provided for any variable error?", "body": "I'm trying to do a style transfer program, and when I get to the optimization I get the no gradient provided for any variable.\r\n[notebook](\r\nhttps://github.com/RodrigoPina407/StyleTransfer/blob/master/StyleTransferV2.ipynb)\r\n\r\n```\r\nepochs=200\r\n\r\nmelhor_loss = 2000000000\r\nmelhor_imagem = None\r\n\r\nmin_value = MEAN\r\nmax_value = 255 + MEAN\r\nloss = 0.0\r\n\r\nfor e in range(epochs):\r\n  with tf.GradientTape() as tape:\r\n    tape.watch(output_processado)\r\n    output_feats = modelo(output_processado)\r\n\r\n    loss = total_loss(content_feats, style_feats, output_feats)\r\n\r\n    grad = tape.gradient(loss, output_processado)\r\n    optimizer.apply_gradients(zip([grad],[output_processado]))\r\n\r\n    clip = tf.clip_by_value(output, min_value, max_value)\r\n    output_processado.assign(clip)\r\n  if loss < melhor_loss:\r\n    melhor_imagem = output_processado\r\n    melhor_loss = loss\r\n    print(\"Epoch: \" + e + \"Loss diminui para \" + melhor_loss)\r\n```\r\n\r\n\r\n> ---------------------------------------------------------------------------\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-236-4b717f5a772e> in <module>()\r\n>      16 \r\n>      17     grad = tape.gradient(loss, output_processado)\r\n> ---> 18     optimizer.apply_gradients(zip([grad],[output_processado]))\r\n>      19 \r\n>      20     clip = tf.clip_by_value(output, min_value, max_value)\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)\r\n>     470       ValueError: If none of the variables have gradients.\r\n>     471     \"\"\"\r\n> --> 472     grads_and_vars = _filter_grads(grads_and_vars)\r\n>     473     var_list = [v for (_, v) in grads_and_vars]\r\n>     474 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)\r\n>    1217   if not filtered:\r\n>    1218     raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n> -> 1219                      ([v.name for _, v in grads_and_vars],))\r\n>    1220   if vars_with_empty_grads:\r\n>    1221     logging.warning(\r\n> \r\n> ValueError: No gradients provided for any variable: ['Variable:0']\r\n", "comments": ["@RodrigoPina407 \r\n\r\nI have tried in colab with TF version 2.1.0 ,2.2.0 and i am seeing the below error message.\r\n`OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.`.Please, find the gist [here](https://colab.sandbox.google.com/gist/ravikyram/1f70784c1aac0b07f41b10f3c17d26ef/untitled898.ipynb).Thanks!", "That problem is caused by the if statement `if loss < melhor_loss:` but if I remove it and its contents I get the error I described above, \"No gradiants provided for any variable\"", "I have tried in colab with TF 2.1, 2.2 and was able to reproduce the issue.Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/88d65fc8ed4bf16c23f58685598d354f/untitled902.ipynb).Thanks!", "Do you have a suggestion on how to solve it?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}]