[{"number": 34766, "title": "Error with tensorflow lite interpreter", "body": "Hi, I am trying to create a wrapper library that takes tensorflow lite model, returns interpreter, which is used later to invoke on specific inputs\r\n\r\nI am using the mimimal.cc example code and modified it to my needs:\r\n\r\nIn order to use interpreter from one function to the next, make sure to keep other components like model and resolver alive with it.\r\n", "comments": ["Hi,\r\n\r\nThis is probably better to ask on [StackOverflow](https://stackoverflow.com)\r\n\r\nBut it looks like your `unique_ptr<Interpreter>` created in `LoadModel` has no references after exiting the function so is deleted (possibly right after printing inputs).", "@dsp-ml I had the same error (always segfaults when trying to do smthg with the interpreter), then I considered that the tflite::FlatbufferModel has to have the same lifetime as the interpreter and now it works (see https://www.tensorflow.org/lite/api_docs/cc/class/tflite/interpreter-builder)"]}, {"number": 34765, "title": "Keras' predict method should be compatible with TensorFlow Probability", "body": "**System information**\r\n- TensorFlow version (you are using): 2.\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, I cannot call the [`predict`](https://keras.io/models/model/) method to perform predictions, when using a [`DistributionLambda`](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DistributionLambda) as the output layer of a Keras model. See, for example, the issues https://github.com/tensorflow/probability/issues/427, https://github.com/tensorflow/probability/issues/538 and https://github.com/tensorflow/tensorflow/issues/31695. So, currently, the apparent alternative is to use  `my_model(input)`, which is a syntax that is not even documented anywhere (AFAIK), to get the predictions.\r\n\r\n**Will this change the current api? How?**\r\n\r\nI don't know exactly because I am not very familiar with the Keras APIs.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nEveryone interested in Bayesian deep learning.", "comments": ["@jvishnuvardhan, @fchollet Actually, it seems that, at least in one case, you can call `model.predict`, when the output of your model is a distribution. See, for example, the comment https://github.com/tensorflow/probability/issues/538#issuecomment-561113039, where the following code is provided\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ndef make_distribution_fn(t):\r\n    return tfp.distributions.Normal(loc=t, scale=t, validate_args=True)\r\n\r\naction_model = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(2, input_dim=2),\r\n    tf.keras.layers.Lambda(tf.abs),\r\n    tfp.layers.DistributionLambda(\r\n        make_distribution_fn=make_distribution_fn\r\n    )\r\n])\r\n\r\ntensor = tf.constant([\r\n    [1.0, 2.0]\r\n], dtype='float32')\r\n\r\nout = action_model.predict(tensor)\r\n\r\nprint(out)\r\n```\r\n\r\nwhich can be executed with TF 2.1 and TFP 0.9 (or 0.8) without getting any errors. However, if I use TF 2.0 with TFP 0.8, I get the error `AttributeError: 'Normal' object has no attribute 'type'`. TFP 0.9 is not compatible with TF 2.0, so that combination cannot be tested.\r\n\r\nWhich commit in TF and/or TFP solved this issue? It seems that the commit that solved this issue was in TF (and not TFP). If this issue was really solved, maybe it can be closed, but I would appreciate if you at least tell me which commit solved the issue.", "The issue still persists if I set the `compile`'s parameter `experimental_run_tf_function` to `False`, which is **necessary in order to train TensorFlow Probability models with Keras' `fit`** (see this comment https://github.com/tensorflow/tensorflow/issues/33729#issuecomment-562656807, the issues https://github.com/tensorflow/tensorflow/issues/33729 and https://github.com/tensorflow/probability/issues/620 and [this Stack Overflow question](https://stackoverflow.com/q/58565913/3924118)). For example, if you execute the following code, you will get the error `AttributeError: 'Normal' object has no attribute 'type'`.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\ndef make_distribution_fn(t):\r\n    return tfp.distributions.Normal(loc=t, scale=t, validate_args=True)\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.layers.Dense(2, input_dim=2),\r\n    tf.keras.layers.Lambda(tf.abs),\r\n    tfp.layers.DistributionLambda(\r\n        make_distribution_fn=make_distribution_fn\r\n    )\r\n])\r\n\r\n# If you use experimental_run_tf_function=False (as a parameter), you will get the error\r\n# AttributeError: 'Normal' object has no attribute 'type'\r\nmodel.compile(experimental_run_tf_function=False, loss=\"mse\")\r\n\r\ntensor = tf.constant([\r\n    [1.0, 2.0]\r\n], dtype='float32')\r\n\r\nout = model.predict(tensor)\r\n\r\nprint(out)\r\n```", "It would be nice to be able to use .fit with tfp, but for now I found I had to use custom training talked about [here](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseFlipout)  in the tfp documentation and [here](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough) for the custom training.", "I have the same issue. \r\n```\r\nTypeError: An op outside of the function building code is being passed\r\n    a \"Graph\" tensor. It is possible to have Graph tensors\r\n    leak out of the function building context by including a\r\n    tf.init_scope in your function building code.\r\n    For example, the following function will fail:\r\n      @tf.function\r\n      def has_init_scope():\r\n        my_constant = tf.constant(1.)\r\n        with tf.init_scope():\r\n          added = my_constant * 2\r\n    The graph tensor has name: model/pconv2d_enc_1/Shape:0\r\n```\r\ndid try experimental_run_tf_function=False but not working still\r\n\r\nHelp....", "I had the same issue and it seems to be solved in tensorflow-probability 0.11.1.", "@nbro,\r\nThe code mentioned in [this comment](https://github.com/tensorflow/tensorflow/issues/34765#issuecomment-580359337) works fine in **`Tensorflow Version 2.4.1`** and in **`TFP Version 0.12.1`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/50e8bebffe57a05d725f13e01a111f50/gh_34765.ipynb) of the working code. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34764, "title": "Set --incompatible_remove_legacy_whole_archive to False", "body": "A roll-forward of cl/281126040\r\nThe windows build failure that caused the rollback is addressed in cl/282539273\r\n\r\nPiperOrigin-RevId: 282833339\r\nChange-Id: I36a4ea4b188880265a80cc52f229e26004b56b17", "comments": []}, {"number": 34763, "title": "MLIR-based convertor fails to convert Universal Sentence Encoder model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (or github SHA if from source): tf-nightly 2.1.0.dev20191202 also tried tf-nightly-gpu 2.1.0.dev20191202\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nsaved_model_dir = '... local path to https://tfhub.dev/google/universal-sentence-encoder-large/4 '\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.experimental_new_converter = True\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2019-12-02 11:06:35.679573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3020 MB memory) -> physical GPU (device: 0, name: Quadro M2200, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2019-12-02 11:06:36.258120: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2019-12-02 11:06:36.258307: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 3003 nodes (0), 3003 edges (0), time = 149.21ms.\r\n2019-12-02 11:06:36.258515: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 3003 nodes (0), 3003 edges (0), time = 144.701ms.\r\n2019-12-02 11:06:36.258724: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: __inference_pruned_16231\r\n2019-12-02 11:06:36.258972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-02 11:06:36.259294: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-12-02 11:06:39.543465: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2019-12-02 11:06:39.544669: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2019-12-02 11:06:39.546506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: Quadro M2200 computeCapability: 5.2\r\ncoreClock: 1.036GHz coreCount: 8 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 82.08GiB/s\r\n2019-12-02 11:06:39.546854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2019-12-02 11:06:39.547033: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2019-12-02 11:06:39.547210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2019-12-02 11:06:39.547386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2019-12-02 11:06:39.547560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2019-12-02 11:06:39.547739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2019-12-02 11:06:39.547922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-02 11:06:39.548392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2019-12-02 11:06:39.548564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-02 11:06:39.548735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2019-12-02 11:06:39.548842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2019-12-02 11:06:39.549462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3020 MB memory) -> physical GPU (device: 0, name: Quadro M2200, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2019-12-02 11:06:39.665521: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize\r\n2019-12-02 11:06:39.665699: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 3003 nodes (0), 3003 edges (0), time = 39.697ms.\r\n2019-12-02 11:06:39.665895: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 3003 nodes (0), 3003 edges (0), time = 35.538ms.\r\nTraceback (most recent call last):\r\n  File \"C:/Users/User/PycharmProjects/tf_models/main.py\", line 70, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\tf_models\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\", line 474, in convert\r\n    **converter_kwargs)\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\tf_models\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 475, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"C:\\Users\\User\\anaconda3\\envs\\tf_models\\lib\\site-packages\\tensorflow_core\\lite\\python\\convert.py\", line 215, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2019-12-02 11:06:42.830207: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-12-02 11:06:42.830437: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n...\r\n...\r\n2019-12-02 11:06:43.425042: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-12-02 11:06:43.425241: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-12-02 11:06:43.425462: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20\r\n2019-12-02 11:06:43.426297: I tensorflow/lite/toco/import_tensorflow.cc:1324] Converting unsupported operation: StatefulPartitionedCall\r\n2019-12-02 11:06:43.427219: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"CPU\"') for unknown op: WrapDatasetVariant\r\n2019-12-02 11:06:43.427479: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"WrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: WrapDatasetVariant\r\n2019-12-02 11:06:43.427895: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"CPU\"') for unknown op: UnwrapDatasetVariant\r\n2019-12-02 11:06:43.428165: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: \"UnwrapDatasetVariant\" device_type: \"GPU\" host_memory_arg: \"input_handle\" host_memory_arg: \"output_handle\"') for unknown op: UnwrapDatasetVariant\r\n2019-12-02 11:06:43.428599: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\r\n\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://tfhub.dev/google/universal-sentence-encoder-large/4\r\n```\r\n", "comments": ["I believe this is a duplicate of #26626, but I'm checking on it with the v2 converter.", "> I believe this is a duplicate of #26626, but I'm checking on it with the v2 converter.\r\n\r\n@daverim that issue is for the old version of converter. The error also looks different.", "The issue in 26626 was unsupported ops, which I'm seeing in your error messages as well, but yes, your path and model seem to be different so I am taking a look at it.", "@daverim have you had a chance to take a look?", "@daverim Did you get a chance to look at it.\r\nI am also facing the same issue in converting the [Universal Sentence Encoder Large model](https://tfhub.dev/google/universal-sentence-encoder-large/5) and my `tensorflow` version is `2.1.0`\r\n\r\nHere is the error I am getting\r\n\r\n```\r\nConverterError: See console for info.\r\n2020-04-23 11:06:59.248348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-23 11:07:23.991599: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-04-23 11:07:24.071588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020-04-23 11:07:24.157016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2070 with Max-Q Design computeCapability: 7.5\r\ncoreClock: 1.305GHz coreCount: 36 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 357.69GiB/s\r\n2020-04-23 11:07:24.157935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020-04-23 11:07:24.193097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-04-23 11:07:24.239225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020-04-23 11:07:24.244705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020-04-23 11:07:24.280229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020-04-23 11:07:24.296901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020-04-23 11:07:24.359551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-04-23 11:07:24.362297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020-04-23 11:07:27.678001: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-04-23 11:07:27.678432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-04-23 11:07:27.678674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n2020-04-23 11:07:27.682591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6304 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-04-23 11:07:30.334652: E tensorflow/lite/toco/import_tensorflow.cc:2252] tensorflow::ImportGraphDef failed with status: Invalid argument: Input 0 of node Func/StatefulPartitionedCall_1/input/_185 was passed float from Embeddings/sharded_0:0 incompatible with expected resource.\r\n2020-04-23 11:07:32.067931: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: StatefulPartitionedCall\r\n2020-04-23 11:07:32.068967: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)\r\nFatal Python error: Aborted\r\n\r\nCurrent thread 0x0000485c (most recent call first):\r\n  File \"C:\\Users\\anu10961\\.conda\\envs\\document-similarity\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 56 in execute\r\n  File \"C:\\Users\\anu10961\\.conda\\envs\\document-similarity\\lib\\site-packages\\absl\\app.py\", line 250 in _run_main\r\n  File \"C:\\Users\\anu10961\\.conda\\envs\\document-similarity\\lib\\site-packages\\absl\\app.py\", line 299 in run\r\n  File \"C:\\Users\\anu10961\\.conda\\envs\\document-similarity\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40 in run\r\n  File \"C:\\Users\\anu10961\\.conda\\envs\\document-similarity\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 93 in main\r\n  File \"C:\\Users\\anu10961\\.conda\\envs\\document-similarity\\Scripts\\toco_from_protos-script.py\", line 10 in <module>\r\n```", "any solutions? I met the same problem with TF2.0\r\n", "Hi this seems to be working with the latest tf release:\r\n\r\nThere are a number of ops which do not have TFLite equivalents, so you must use flex: \r\n\r\nSee here for an working colab gist:\r\n\r\nhttps://gist.github.com/daverim/35a8429affed658b2be97679e76e81c1#file-convert_sentence_encoder-ipynb", "Hi @wboleksii ! Have you checked the gist from the above comment yet ? Attaching threads on [Supported Ops](https://www.tensorflow.org/lite/guide/op_select_allowlist) and [Select Ops](https://www.tensorflow.org/lite/guide/ops_select) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 34762, "title": "AttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 2.0.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.27.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.4.0\r\n- **CUDA/cuDNN version**: 10.2\r\n- **GPU model and memory**: NVIDIA 970\r\n- **Exact command to reproduce**: python3 legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config\r\n\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nTrying tensorflow's trainning appears an error message about a non existed attribute **(register_op_list')** . I checked in the file that is supposed to have it and it really doesn't have it. \r\nI looked for that attribute in every single path I thought it could be but I didn't get anything at all.\r\nI have no clue of how I can resolve this.\r\n\r\n\r\nTo resolve the contrib issue of tf2.0 i used [tf-slim](https://github.com/adrianc-a/tf-slim) but if you have a way to resolve it completely that doesn't trigger this error please let me know because I'm quite sure that everything comes because of the \"contrib\" problem.\r\nThanks\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\n\r\n**### python3 legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v1_pets.config**\r\n\r\n2019-12-02 13:59:36.908351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\r\nTraceback (most recent call last):\r\n  File \"legacy/train.py\", line 48, in <module>\r\n    from tensorflow.contrib import framework as contrib_framework\r\n  File \"/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/__init__.py\", line 31, in <module>\r\n    from tensorflow.contrib import cloud\r\n  File \"/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/__init__.py\", line 24, in <module>\r\n    from tensorflow.contrib.cloud.python.ops.bigquery_reader_ops import *\r\n  File \"/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py\", line 21, in <module>\r\n    from tensorflow.contrib.cloud.python.ops import gen_bigquery_reader_ops\r\n  File \"/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py\", line 369, in <module>\r\n    _op_def_lib = _InitOpDefLibrary(b\"\\n\\355\\001\\n\\016BigQueryReader\\032\\024\\n\\rreader_handle\\030\\007\\200\\001\\001\\\"\\027\\n\\tcontainer\\022\\006string\\032\\002\\022\\000\\\"\\031\\n\\013shared_name\\022\\006string\\032\\002\\022\\000\\\"\\024\\n\\nproject_id\\022\\006string\\\"\\024\\n\\ndataset_id\\022\\006string\\\"\\022\\n\\010table_id\\022\\006string\\\"\\027\\n\\007columns\\022\\014list(string)\\\"\\027\\n\\020timestamp_millis\\022\\003int\\\"\\034\\n\\016test_end_point\\022\\006string\\032\\002\\022\\000\\210\\001\\001\\n\\331\\001\\n GenerateBigQueryReaderPartitions\\032\\016\\n\\npartitions\\030\\007\\\"\\024\\n\\nproject_id\\022\\006string\\\"\\024\\n\\ndataset_id\\022\\006string\\\"\\022\\n\\010table_id\\022\\006string\\\"\\027\\n\\007columns\\022\\014list(string)\\\"\\027\\n\\020timestamp_millis\\022\\003int\\\"\\025\\n\\016num_partitions\\022\\003int\\\"\\034\\n\\016test_end_point\\022\\006string\\032\\002\\022\\000\")\r\n  File \"/home/adele/.local/lib/python3.6/site-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py\", line 277, in _InitOpDefLibrary\r\n    _op_def_registry.register_op_list(op_list)\r\n**AttributeError: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'**", "comments": ["@rmbadl, Provide the complete standalone code to replicate the issue. Thanks!", "I have no standalone code, the \"train.py\" code that I use is the tensorflow's one. I was just following this [page ](https://pythonprogramming.net/introduction-use-tensorflow-object-detection-api-tutorial/) . \r\nI downloaded tensorflow and bazel and followed each ones guides to install them.. (from their own pages)", "@rmbadl, Please find the [link](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) for Tensorflow Object detection tutorial. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "@gadagashwini I have the same problem.. I'm using Tensorflow 1.14.0 and python 3.7", "@bjente,\r\nCan you please post a new issue by providing the information asked by the template?\r\nThe reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks!", "same error using tflearn package ==0.3.2 and tenserflow ==1.14 \r\n\r\nerror:'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'\r\n", "Is anybody working on this? Getting the same error. ", "> Is anybody working on this? Getting the same error.\r\n\r\nrestart the kernel ...maybe that help\r\n", "Restart the kernel if you recently changed tensorflow versions", "> @gadagashwini I have the same problem.. I'm using Tensorflow 1.14.0 and python 3.7\r\n\r\nrestart kernel it worked for me.", "I changed the version of python to 3.7 from 3.6 it worked", "1. Use Tensorflow 1.11 or greater. However, don't go on to use Tensorflow 2.*.\r\n2. Also, for the current latest version of tensorflow-hub 0.10.0, you need tf >= 1.15.0.\r\nThus, best to use 1.15.0 <= tf < 2.* along with tf-hub latest(0.10.0).\r\n"]}, {"number": 34761, "title": "TFLite object detection output values", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\n\r\nHi, \r\n\r\nI am using TFLite in my project, which includes object detection in python. I am working on a raspberry pi so TFLite was suitable for this. I could successsfully invoke the interpreter and get some numbers as the output:\r\n\r\n```\r\n[[[ 1.66415479e-02  5.48024022e-04  8.67791831e-01  3.35325867e-01]\r\n  [ 7.41335377e-02  3.22245747e-01  9.64617252e-01  9.71388936e-01]\r\n  [-2.11861148e-03  5.41743517e-01  2.60241032e-01  7.02846169e-01]\r\n  [-5.67546487e-03  3.26282382e-01  8.59034657e-01  6.30770981e-01]\r\n  [ 7.27111334e-03  7.90268779e-01  2.86753297e-01  9.56545353e-01]\r\n  [ 2.07318692e-03  7.96441555e-01  5.48386931e-01  9.96111989e-01]\r\n  [-1.04907183e-02  2.38761827e-01  6.75976276e-01  7.01156497e-01]\r\n  [ 3.12007014e-02  1.34294275e-02  5.82291842e-01  3.10949832e-01]\r\n  [-1.95578858e-03  7.05318868e-01  9.18281525e-02  7.96184599e-01]\r\n  [-5.43205580e-03  3.23292404e-01  6.34427786e-01  5.68508685e-01]]]\r\n```\r\n\r\nHow do I convert these numbers to classes and then to thier location in the image?\r\n\r\nHere is my code if I did something wrong:\r\n\r\n```python\r\nimport tensorflow as tf \r\nimport numpy as np\r\nimport cv2\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=\"/content/drive/My Drive/detect.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nprint(input_details)\r\nprint(output_details)\r\n\r\ninput_shape = input_details[0]['shape']\r\nim = cv2.imread(\"/content/drive/My Drive/doggy.jpg\")\r\nim_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\r\nim_rgb = cv2.resize(im_rgb, (input_shape[1], input_shape[2]))\r\ninput_data = np.expand_dims(im_rgb, axis=0)\r\nprint(input_data.shape)\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data.shape)\r\nprint()\r\nprint(output_data)\r\n```\r\nOutput:\r\n```\r\n[{'name': 'normalized_input_image_tensor', 'index': 175, 'shape': array([  1, 300, 300,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0078125, 128)}]\r\n[{'name': 'TFLite_Detection_PostProcess', 'index': 167, 'shape': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:1', 'index': 168, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:2', 'index': 169, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}, {'name': 'TFLite_Detection_PostProcess:3', 'index': 170, 'shape': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n\r\n(1, 300, 300, 3)\r\n\r\n(1, 10, 4)\r\n\r\n[[[ 1.66415479e-02  5.48024022e-04  8.67791831e-01  3.35325867e-01]\r\n  [ 7.41335377e-02  3.22245747e-01  9.64617252e-01  9.71388936e-01]\r\n  [-2.11861148e-03  5.41743517e-01  2.60241032e-01  7.02846169e-01]\r\n  [-5.67546487e-03  3.26282382e-01  8.59034657e-01  6.30770981e-01]\r\n  [ 7.27111334e-03  7.90268779e-01  2.86753297e-01  9.56545353e-01]\r\n  [ 2.07318692e-03  7.96441555e-01  5.48386931e-01  9.96111989e-01]\r\n  [-1.04907183e-02  2.38761827e-01  6.75976276e-01  7.01156497e-01]\r\n  [ 3.12007014e-02  1.34294275e-02  5.82291842e-01  3.10949832e-01]\r\n  [-1.95578858e-03  7.05318868e-01  9.18281525e-02  7.96184599e-01]\r\n  [-5.43205580e-03  3.23292404e-01  6.34427786e-01  5.68508685e-01]]]\r\n```\r\n\r\nThanks", "comments": ["Hi,\r\n\r\nYou should post this question on [StackOverFlow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nJust as an FYI:\r\n\r\nThe TFLite_Detection_PostProcess custom op node has four outputs\r\ndetection_boxes: a tensor of shape [1, num_boxes, 4] with normalized coordinates\r\ndetection_classes: a tensor of shape [1, num_boxes] containing class prediction for each box\r\ndetection_scores: a tensor of shape [1, num_boxes]\r\nnum_boxes: a tensor of size 1 containing the number of detected boxes\r\n\r\n```\r\ndetection_boxes = interpreter.get_tensor(output_details[0]['index'])\r\ndetection_classes = interpreter.get_tensor(output_details[1]['index'])\r\ndetection_scores = interpreter.get_tensor(output_details[2]['index'])\r\nnum_boxes = interpreter.get_tensor(output_details[3]['index'])\r\nfor i in range(num_boxes):\r\n  if detection_scores[0, i] > .5:\r\n       x = detection_boxes[0, i, [1, 3]] * img_width\r\n       y = detection_boxes[0, i, [0, 2]] * img_height\r\n       rectangle = [x[0], y[0], x[1], y[1]]\r\n       class_id = detection_classes[0, i]\r\n       ...\r\n```\r\n      \r\n", "Thanks for the reply with the answer I needed. Sorry I mad an issue here :) \r\nFor those who are getting an error in the for loop, change the for loop to :\r\n```python\r\nfor i in range(int(num_boxes[0])):\r\n```\r\nThanks Again", "why my output_detail have just 1-dimention \r\noutput [{'name': 'output_boxes', 'index': 104, 'shape': array([   1, 5415,   16]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]", "> why my output_detail have just 1-dimention\r\n> output [{'name': 'output_boxes', 'index': 104, 'shape': array([ 1, 5415, 16]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0)}]\r\n\r\nI got the same error..."]}, {"number": 34760, "title": "ValueError: Dimensions must be equal, but are 32 and 64 for 'conv2/Conv2D' (op: 'Conv2D') with input shapes: [?,?,?,32], [3,3,64,128].", "body": "Hello There,\r\nI am Working on a project of converting given image to its latex form . \r\nI have cloned this github repo->\r\n**https://github.com/yixuanzhou/image2latex.git**\r\n\r\nI am using Python-2 and tensorflow_version = 0.12.1 with a GPU backend on **Google-Colab**.\r\nI have resotred  the Weights  correctly and tried to run the file **predict.py** Without calling the function **predict** inside that file I am getting no error . But when I am calling that function with an input image ,it showed me an error that Data Format must be NHWC and the file **ops.py** under the tflib folder in that repo used data format of NCHW . So i simply changed the data format from NCHW to NHWC.\r\nAfter that when I run the program **predict.py** I am getting this error :\r\n\r\nTraceback (most recent call last):\r\n  File \"predict.py\", line 32, in <module>\r\n    ctx = tflib.network.im2latex_cnn(X,NUM_FEATS_START,False)\r\n  File \"/content/image2latex/tflib/network.py\", line 79, in im2latex_cnn\r\n    X = tf.nn.relu(tflib.ops.conv2d('conv2', X, 3, 1, num_feats, num_feats*2, pad = 'SAME', bias=False))\r\n  File \"/content/image2latex/tflib/ops.py\", line 202, in conv2d\r\n    out = tf.nn.conv2d(input, filters, strides=[1, 1, stride, stride], padding=pad, data_format='NHWC')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 396, in conv2d\r\n    data_format=data_format, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2242, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1617, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1568, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 610, in call_cpp_shape_fn\r\n    debug_python_shape_fn, require_shape_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py\", line 675, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Dimensions must be equal, but are 32 and 64 for 'conv2/Conv2D' (op: 'Conv2D') with input shapes: [?,?,?,32], [3,3,64,128].\r\n\r\nI have also tried a solution proposed on StackOverflow :\r\n-> **https://stackoverflow.com/questions/37689423/convert-between-nhwc-and-nchw-in-tensorflow**\r\n\r\nKindly help me regarding this issue .\r\n-->Thanks<--\r\n\r\n ", "comments": ["@sarthakforwet, Could you post your Google-colab gist to analyze the reported issue. Thanks!", "https://colab.research.google.com/gist/sarthakforwet/cd7ff133d0e68cf21571de3790010543/untitled0.ipynb\r\n\r\nHere is my Colab gist. Kindly have a look at it .\r\nThanks", "@sarthakforwet, Is there specific reason to use two frameworks such as Theano and Tenosrflow.  Tensorflow version is very old, Can you try with latest Tensorflow version 1.15. Let us know if it solves your problem. Thanks! ", "Theano and Tensorflow both are used  in this particular repo and some functions I found in theano for whom any equivalent was unable to be found out in tensorflow.\r\n\r\nSecondly , even if I use **Tensorflow-1.15** , same issue persists and if I change the code from original one to mentioned below i am getting new error i.e \r\n\r\n**ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.**\r\n\r\n_Before  :-_ \r\n\r\ntflib/ops.py in conv2d\r\n**out = tf.nn.conv2d(input, filters, strides=[1, 1, stride, stride], padding=pad, data_format='NHWC')**\r\n\r\ntflib/ops.py in max_pool\r\n\r\n**return  tf.nn.max_pool(l_input, ksize=[1, 1, k1, k2], strides=[1, 1, s1, s2],\r\n                          padding='SAME', name=name, data_format='NCHW')**\r\n\r\n_After :-_ \r\n\r\ntflib/ops.py in conv2d\r\n**out = tf.nn.conv2d(input, filters, strides=[1, stride, stride, 1], padding=pad, data_format='NHWC')**\r\n\r\ntflib/ops.py in max_pool\r\n\r\n**return  tf.nn.max_pool(l_input, ksize=[1, k1, k2, 1], strides=[1, s1, s2, 1],\r\n                          padding='SAME', name=name, data_format='NCHW')**\r\n\r\n", "@sarthakforwet, Could you provide the code snippet with only Tensorflow, indeed it will hep us to analyze the issue faster.Thanks!", "@sarthakforwet, Any update on issue!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34759, "title": "Using tensorflow gpu 2.1 with Cuda 10.2", "body": "- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from: `pip`\r\n- TensorFlow version: 2.1.0rc0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: `pip`\r\n- CUDA/cuDNN version: 10.2\r\n- GPU model and memory: Quadro P5000, 16GB\r\n\r\n**Describe the problem**\r\n\r\nI want to use `tensorflow-gpu==2.1.0rc0` with cuda 10.2 and it seems that it can't work right now.\r\nWhen I use `tensorflow-gpu=2.0.0` it works perfectly fine.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nmkdir tests2 &&\\\r\ncd tests2 &&\\\r\nvirtualenv -p /usr/bin/python3.6 venv &&\\\r\nsource venv/bin/activate &&\\\r\npip install tensorflow-gpu==2.1.0rc0 &&\\\r\npython -c 'import tensorflow'\r\n```\r\nWhich gives the following warnings:\r\n```\r\n2019-12-02 15:23:46.869198: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n2019-12-02 15:23:46.869227: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n2019-12-02 15:23:47.516321: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n2019-12-02 15:23:47.516433: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n2019-12-02 15:23:47.516449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n```\r\n\r\n**Any other info / logs**\r\nWhen I do `locate libcudart.so`, I get the following:\r\n```\r\n/usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0\r\n/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n/usr/local/cuda-10.2/doc/man/man7/libcudart.so.7\r\n/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so\r\n/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2\r\n/usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2.89\r\n```\r\n\r\n`locate libnvinfer_plugin.so` is empty.\r\n\r\n\r\n", "comments": ["As printed in the stack trace ```Could not load dynamic library 'libcudart.so.10.1'```. You have to roll back to cuda 10.1 in order to use TF 2.1 binary. For using cuda 10.2 you have to install TF from sources.\r\nAlso, tensorflow pip package (TF 2.1) now includes GPU support by default (same as tensorflow-gpu) for both Linux and Windows. ", "Thanks for your answer.\r\nI am a bit surprised though because I am able to use tf 2.0.0 just fine (with cuda 10.2). How is it possible?", "You are able to import tf cpu version in tf 2.0\r\nReason being when you installed tf 2.0.0 without specifying accelerator(gpu) it installed both CPU and GPU support.\r\nTo check this you may try printing;\r\n```python\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n```", "No I installed specifically tensorflow gpu, didn't get the warning at import and more than simply using this function to test, I monitored the GPU usage during training (and my training was way faster than when I was masking the GPU).", "When I look at the logs for tf 2.0.0, I see that it's loading packages from the 10.0 version of Cuda. These packages are certainly legacy packages that I still have, and so that's why it's working even though my main Cuda is 10.2.\r\nI'll roll back to 10.1 to use tf 2.1.0", "@zaccharieramzi, Were you able to install Tensorflow-gpu 2.1 with Cuda 10.1. ", "Hi @gadagashwini , I didn't have time to try. I can't really try right now, cos I am too afraid to mess up my conda environment atm(haha). But I can let you know in a few days.", "Thanks @zaccharieramzi. Please try and let us know how it progresses. Thanks!", "@zaccharieramzi They got Cuda 10.1 working in another thread: https://github.com/tensorflow/tensorflow/issues/34429#issuecomment-562961299. Could you verify?", "I managed to build master with **--config=v1** and cuda 10.2(gcc), numpy 1.17.4, however I get reproducible segfaults on code that runs fine with tf 1.x/cuda 10.1. OS is a fresh new Ubuntu 18.04, cuda/cudnn from debs. Hardware is a xeon 2695v2, 128GB RAM, 1060ti:\r\n\r\n```\r\n2019-12-12 09:53:39.102734: F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)\r\n[xeon:22138] *** Process received signal ***\r\n[xeon:22138] Signal: Aborted (6)\r\n[xeon:22138] Signal code:  (-6)\r\n[xeon:22138] [ 0] /lib/x86_64-linux-gnu/libc.so.6(+0x3ef20)[0x7f650fe53f20]\r\n[xeon:22138] [ 1] /lib/x86_64-linux-gnu/libc.so.6(gsignal+0xc7)[0x7f650fe53e97]\r\n[xeon:22138] [ 2] /lib/x86_64-linux-gnu/libc.so.6(abort+0x141)[0x7f650fe55801]\r\n[xeon:22138] [ 3] /home/i/.local/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so(+0xaa769c7)[0x7f6464a8b9c7]\r\n[xeon:22138] [ 4] /home/i/.local/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow7functor9ApplyAdamIN5Eigen9GpuDeviceEfEclERKS3_NS2_9TensorMapINS2_6TensorIfLi1ELi1ElEELi16ENS2_11MakePointerEEESB_SB_NS7_INS2_15TensorFixedSizeIKfNS2_5SizesIJEEELi1ElEELi16ESA_EESH_SH_SH_SH_SH_NS7_INS8_ISD_Li1ELi1ElEELi16ESA_EEb+0x40f)[0x7f646322fb7f]\r\n[xeon:22138] [ 5] /home/i/.local/lib/python3.6/site-packages/tensorflow_core/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow11ApplyAdamOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE+0x52c)[0x7f646315015c]\r\n[xeon:22138] [ 6] /home/i/.local/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2(_ZN10tensorflow13BaseGPUDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE+0xe6)[0x7f6475574b76]\r\n[xeon:22138] [ 7] /home/i/.local/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2(+0xf75665)[0x7f64755df665]\r\n[xeon:22138] [ 8] /home/i/.local/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2(+0xf75d2f)[0x7f64755dfd2f]\r\n[xeon:22138] [ 9] /home/i/.local/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2(_ZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi+0x4b1)[0x7f64756cdbc1]\r\n[xeon:22138] [10] /home/i/.local/lib/python3.6/site-packages/tensorflow_core/python/../libtensorflow_framework.so.2(_ZNSt17_Function_handlerIFvvEZN10tensorflow6thread16EigenEnvironment12CreateThreadESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data+0x43)[0x7f64756caed3]\r\n[xeon:22138] [11] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xbd66f)[0x7f648ff7266f]\r\n[xeon:22138] [12] /lib/x86_64-linux-gnu/libpthread.so.0(+0x76db)[0x7f650fbfd6db]\r\n[xeon:22138] [13] /lib/x86_64-linux-gnu/libc.so.6(clone+0x3f)[0x7f650ff3688f]\r\n[xeon:22138] *** End of error message ***\r\n```\r\n\r\nWeirdest thing I'm doing in the code is to have tensors with a dimension axis at 0 (ie: shape@(40,4,0,50)). This happens because I'm doing a network architecture search process.\r\n\r\nI'm rolling back to 10.1 meanwhile, Regards.", "@pisiiki Did your build include this patch? https://github.com/tensorflow/tensorflow/pull/34885", "@EwoutH it think so, I did a pull on master yesterday and this was merged 6 days ago.\r\n\r\nedit:\r\n\r\nI have just crashed with a prebuilt conda tf-gpu 1.15 package. It seems that my code may crash with a segfault or get some numerical overflows depending on driver, python version, tf version, packages, etc. Conda has its own cuda 10.0 runtime for this package. I'm still on 440.33.01(cuda 10.2) driver btw.\r\n\r\nI didn't notice before because my previous setup (py3.6 tf 1.15 cuda 10.1) simply threw exceptions. It turned some vars into infs on a train call and I raised the exceptions myself. I suposed all of these were regular instability/overflows but now I think some may be related with 0 sized dimensions at some tensors.\r\n\r\nI will rollback the driver to 10.1, however I guess this is worth checking out. I will also try to reproduce the  error with minimal code.", "@zaccharieramzi, Any update on this issue!", "Sorry, I didn't get a chance to check on this with the holiday season and all.\r\nWould you rather have me try to build with cuda 10.2 or try and install cuda 10.1?", "> * OS Platform and Distribution: Linux Ubuntu 16.04\r\n> * TensorFlow installed from: `pip`\r\n> * TensorFlow version: 2.1.0rc0\r\n> * Python version: 3.6.8\r\n> * Installed using virtualenv? pip? conda?: `pip`\r\n> * CUDA/cuDNN version: 10.2\r\n> * GPU model and memory: Quadro P5000, 16GB\r\n> \r\n> **Describe the problem**\r\n> \r\n> I want to use `tensorflow-gpu==2.1.0rc0` with cuda 10.2 and it seems that it can't work right now.\r\n> When I use `tensorflow-gpu=2.0.0` it works perfectly fine.\r\n> \r\n> **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n> \r\n> ```\r\n> mkdir tests2 &&\\\r\n> cd tests2 &&\\\r\n> virtualenv -p /usr/bin/python3.6 venv &&\\\r\n> source venv/bin/activate &&\\\r\n> pip install tensorflow-gpu==2.1.0rc0 &&\\\r\n> python -c 'import tensorflow'\r\n> ```\r\n> \r\n> Which gives the following warnings:\r\n> \r\n> ```\r\n> 2019-12-02 15:23:46.869198: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n> 2019-12-02 15:23:46.869227: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n> 2019-12-02 15:23:47.516321: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n> 2019-12-02 15:23:47.516433: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n> 2019-12-02 15:23:47.516449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n> ```\r\n> \r\n> **Any other info / logs**\r\n> When I do `locate libcudart.so`, I get the following:\r\n> \r\n> ```\r\n> /usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n> /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so\r\n> /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0\r\n> /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n> /usr/local/cuda-10.2/doc/man/man7/libcudart.so.7\r\n> /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so\r\n> /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2\r\n> /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2.89\r\n> ```\r\n> \r\n> `locate libnvinfer_plugin.so` is empty.\r\n\r\nI have similar issues here. The way help me out is to build tensorflow from source. It seems the prebuild tensorflow is not compatible with cuda10.2 quite well.", "I rolled back to cuda 10.1, and everything seems to work fine. I am going to close this since tensorflow 2.1 is not supposed to be directly usable with cuda 10.2, and from @lijiaying 's comment, I understand there is no issue with building from source.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34759\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34759\">No</a>\n", "> > * OS Platform and Distribution: Linux Ubuntu 16.04\r\n> > * TensorFlow installed from: `pip`\r\n> > * TensorFlow version: 2.1.0rc0\r\n> > * Python version: 3.6.8\r\n> > * Installed using virtualenv? pip? conda?: `pip`\r\n> > * CUDA/cuDNN version: 10.2\r\n> > * GPU model and memory: Quadro P5000, 16GB\r\n> > \r\n> > **Describe the problem**\r\n> > I want to use `tensorflow-gpu==2.1.0rc0` with cuda 10.2 and it seems that it can't work right now.\r\n> > When I use `tensorflow-gpu=2.0.0` it works perfectly fine.\r\n> > **Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n> > ```\r\n> > mkdir tests2 &&\\\r\n> > cd tests2 &&\\\r\n> > virtualenv -p /usr/bin/python3.6 venv &&\\\r\n> > source venv/bin/activate &&\\\r\n> > pip install tensorflow-gpu==2.1.0rc0 &&\\\r\n> > python -c 'import tensorflow'\r\n> > ```\r\n> > \r\n> > \r\n> > Which gives the following warnings:\r\n> > ```\r\n> > 2019-12-02 15:23:46.869198: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n> > 2019-12-02 15:23:46.869227: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n> > 2019-12-02 15:23:47.516321: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n> > 2019-12-02 15:23:47.516433: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda/extras/CUPTI/lib64\r\n> > 2019-12-02 15:23:47.516449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n> > ```\r\n> > \r\n> > \r\n> > **Any other info / logs**\r\n> > When I do `locate libcudart.so`, I get the following:\r\n> > ```\r\n> > /usr/local/cuda-10.0/doc/man/man7/libcudart.so.7\r\n> > /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so\r\n> > /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0\r\n> > /usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130\r\n> > /usr/local/cuda-10.2/doc/man/man7/libcudart.so.7\r\n> > /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so\r\n> > /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2\r\n> > /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2.89\r\n> > ```\r\n> > \r\n> > \r\n> > `locate libnvinfer_plugin.so` is empty.\r\n> \r\n> I have similar issues here. The way help me out is to build tensorflow from source. It seems the prebuild tensorflow is not compatible with cuda10.2 quite well.\r\n\r\ndid you get any solution for your problem??", "@aloksingh3110 like I said in [the comment just above](https://github.com/tensorflow/tensorflow/issues/34759#issuecomment-570176629), I rolled back to cuda 10.1. [This person](https://github.com/tensorflow/tensorflow/issues/34759#issuecomment-564993689) has run into problems when building from source with cuda 10.2, so I advise you to roll back to 10.1. ", "I have the same problem, but using `CUDA 10.1`:\r\n```\r\nPython 3.5.2 (default, Oct  8 2019, 13:06:37) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-01-21 16:33:12.576855: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.27' not found (required by /usr/lib/x86_64-linux-gnu/libnvinfer.so.6)\r\n2020-01-21 16:33:12.577361: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.27' not found (required by /usr/lib/x86_64-linux-gnu/libnvinfer.so.6)\r\n2020-01-21 16:33:12.577381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n>>> tf.test.is_gpu_available()\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2020-01-21 16:33:15.483537: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3491610000 Hz\r\n2020-01-21 16:33:15.484394: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4de73f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-21 16:33:15.484448: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-21 16:33:15.487966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-21 16:33:15.518212: I tensorflow/compiler/xla/service/platform_util.cc:205] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0\r\n2020-01-21 16:33:15.518325: I tensorflow/compiler/jit/xla_gpu_device.cc:136] Ignoring visible XLA_GPU_JIT device. Device number is 0, reason: Internal: no supported devices found for platform CUDA\r\n2020-01-21 16:33:15.518765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:05:00.0 name: Quadro K2000 computeCapability: 3.0\r\ncoreClock: 0.954GHz coreCount: 2 deviceMemorySize: 1.94GiB deviceMemoryBandwidth: 59.60GiB/s\r\n2020-01-21 16:33:15.518997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-01-21 16:33:15.520344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-21 16:33:15.521455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-01-21 16:33:15.521690: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-01-21 16:33:15.523131: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-01-21 16:33:15.523940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-01-21 16:33:15.527679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-21 16:33:15.528518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1651] Ignoring visible gpu device (device: 0, name: Quadro K2000, pci bus id: 0000:05:00.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\r\n2020-01-21 16:33:15.528552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-21 16:33:15.528566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 \r\n2020-01-21 16:33:15.528579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N \r\n\r\n```", "I'm relatively very new to this, so I might be wrong, but @SalahAdDin it seems like you're missing the libnvinfer library found in TensorRT (see [here](https://docs.nvidia.com/deeplearning/sdk/tensorrt-install-guide/index.html#installing)). CUDA-10.1 seems to have loaded fine.", "@hd1090. I tried to install tensorRT in cuda 10.1. My gpu is working, but tensorRT installation is erroring out on me with the following message. Do you know what could be wrong here?\r\n(base) prompt$:~$ sudo apt install tensorrt\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nSome packages could not be installed. This may mean that you have\r\nrequested an impossible situation or if you are using the unstable\r\ndistribution that some required packages have not yet been created\r\nor been moved out of Incoming.\r\nThe following information may help to resolve the situation:\r\n\r\nThe following packages have unmet dependencies:\r\n tensorrt : Depends: libnvinfer6 (= 6.0.1-1+cuda10.1) but 6.0.1-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-plugin6 (= 6.0.1-1+cuda10.1) but 6.0.1-1+cuda10.2 is to be installed\r\n            Depends: libnvparsers6 (= 6.0.1-1+cuda10.1) but 6.0.1-1+cuda10.2 is to be installed\r\n            Depends: libnvonnxparsers6 (= 6.0.1-1+cuda10.1) but 6.0.1-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-bin (= 6.0.1-1+cuda10.1) but it is not going to be installed\r\n            Depends: libnvinfer-dev (= 6.0.1-1+cuda10.1) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-plugin-dev (= 6.0.1-1+cuda10.1) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvparsers-dev (= 6.0.1-1+cuda10.1) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvonnxparsers-dev (= 6.0.1-1+cuda10.1) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-samples (= 6.0.1-1+cuda10.1) but it is not going to be installed\r\n            Depends: libnvinfer-doc (= 6.0.1-1+cuda10.1) but it is not going to be installed\r\nE: Unable to correct problems, you have held broken packages.\r\n", "@maximuslee1226 \r\nTry doing this:\r\n```\r\n!sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.0 \\\r\n    libnvinfer-dev=6.0.1-1+cuda10.0 \\\r\n    libnvinfer-plugin6=6.0.1-1+cuda10.0\r\n```\r\n\r\nUsing `CUDA 10.1`.", "@zaccharieramzi  did you find any other solution other then rolling back to 10.1?", "@alihamid996 I didn't try anything else, so I couldn't tell you myself. @lijiaying 's [comment](https://github.com/tensorflow/tensorflow/issues/34759#issuecomment-570102033) suggests that it's possible to build tf 2.1 from source with cuda 10.2 though.", "My head hurts just to see it is such a pain in the rear to get all the moving pieces exactly right to have GPU support. NVidia obviously wants you to install 10.2... and here goes Tensorflow only works with 10.1 out of box. Now I have to recompile the OpenCV from scratch.. Just wonderful.", "Could you please clarify. are there any plans to support 10.2 in the next release of Tensorflow?\r\n\r\nUnfortunately there are no CUDA 10.1 packages for modern RHEL based repos like Centos 8 or Fedora 30+. Only CUDA 10.2 is available for these distributives.", "New sdcard image for Jetson Nano comes with CUDA 10.2 preinstalled and older images cannot be downloaded anymore. Not sure if is possible to downgrade because it seams nVidia blacklist older packages.", "> You are able to import tf cpu version in tf 2.0\r\n> Reason being when you installed tf 2.0.0 without specifying accelerator(gpu) it installed both CPU and GPU support.\r\n> To check this you may try printing;\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> tf.test.is_gpu_available()\r\n> ```\r\n\r\nIs there any **tensorflow** installation solution for those person which have **CUDA** _10.2_ installed, **gcc** _7.5.0_ and **ubuntu18.04**? if so, only with **tensorflow** 2.1?", "https://github.com/tensorflow/tensorflow/issues/38194#issuecomment-629801937 seem to have a solution", "I had a similar problem with 10.2 using `tf.config.experimental.list_physical_devices('GPU')`. Cuda v10.2 was installed using this command after installing the ubuntu 18.04 `cuda` and `nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb` repos per the TensorFlow documetation at https://www.tensorflow.org/install/gpu\r\n\r\n```\r\napt-get install -y --no-install-recommends \\\r\ncuda-10-2 \\ \r\nlibcudnn7=7.6.5.32-1+cuda10.2  \\\r\nlibcudnn7-dev=7.6.5.32-1+cuda10.2 \\\r\nlibnvinfer7=7.0.0-1+cuda10.2 \\\r\nlibnvinfer-dev=7.0.0-1+cuda10.2 \\\r\nlibnvinfer-plugin7=7.0.0-1+cuda10.2    \r\n```\r\n\r\nThe error when trying `tf.config.experimental.list_physical_devices('GPU')` can be seen below:\r\n\r\n```\r\n$ python3\r\nPython 3.8.2 (default, Apr 27 2020, 15:53:34) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf                            \r\n>>> tf.config.experimental.list_physical_devices('GPU')\r\n2020-05-20 14:02:50.885725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-05-20 14:02:50.903802: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-05-20 14:02:50.904116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1070 Ti computeCapability: 6.1\r\ncoreClock: 1.683GHz coreCount: 19 deviceMemorySize: 7.92GiB deviceMemoryBandwidth: 238.66GiB/s\r\n2020-05-20 14:02:50.904232: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-05-20 14:02:50.905355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-05-20 14:02:50.906434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-05-20 14:02:50.906614: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-05-20 14:02:50.907827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-05-20 14:02:50.908474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-05-20 14:02:50.910932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-05-20 14:02:50.910947: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[]\r\n>>> \r\n```\r\nThis error `Could not load dynamic library 'libcudart.so.10.1'` was the clue.\r\n\r\nThis resolved it:\r\n\r\n`apt-get -y install cuda-cudart-10-1`\r\n", "UPDATE: WARNING below https://github.com/tensorflow/tensorflow/issues/34759#issuecomment-633819017\r\n\r\nFYI, we have a tested work-around (symlink _fake_ `libcudart.so.10.2` to _real_ `libcudart.so.10.1`)  for TF 2.2 and cuda 10.2 for Ubuntu 20.04 and Windows in https://github.com/tensorflow/tensorflow/issues/38194#issuecomment-629801937\r\nand https://github.com/tensorflow/tensorflow/issues/38194#issuecomment-633492769\r\n\r\nI am using this for days now, mainly with CPU and I never got trouble with it (so libcudart 10.1 and 10.2 really seem compatible, as was promised higher up in that thread NOT SURE !!).", "> so libcudart 10.1 and 10.2 really seem compatible, as was promised higher up in that thread\r\n\r\nI don't think libcudart 10.1 and 10.2 are ABI compatible ([doc](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility)).  Symlinking 10.2 to 10.1 may _seem to_ work, but there is no guarantee that e.g. this will not fry your GPU.", "> > so libcudart 10.1 and 10.2 really seem compatible, as was promised higher up in that thread\r\n> \r\n> I don't think libcudart 10.1 and 10.2 are ABI compatible ([doc](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility)). Symlinking 10.2 to 10.1 may _seem to_ work, but there is no guarantee that e.g. this will not fry your GPU.\r\n\r\nWow, that would be bad ... \r\n\r\nTo be clear, I am doing most of the actual work on the CPU (this smallish `GM107GLM [Quadro M1200 Mobile]` GPU with 2.5G free RAM, faces OOM very quickly for any real work, and when it does not face OOM, it seems a factor 2 slower than the 8 core CPU).\r\n", "Can anyone tell about when tensorflow gpu would be able to run with cuda 10.2?", "At least after the TF 2.4 release", "> Can anyone tell about when tensorflow gpu would be able to run with cuda 10.2?\r\n\r\nOur current plan is to use move TF 2.4 to CUDA 11.", "Managed to install tensorflow-gpu 2.3 with cudatoolkit 10.1 on my **cuda 10.2 driver**(Jan 19th,2021)\r\n\r\n```\r\nconda create --name tf2 python=3.8.3\r\nconda install cudnn==7.6.4\r\npip install tensorflow-gpu=2.3\r\n```"]}, {"number": 34758, "title": "Docker Install instructions page is broken now that v2.0 is the latest image", "body": "\r\n##  URL(s) with the issue:\r\nhttps://www.tensorflow.org/install/docker\r\n\r\n## Documentation page with the issue:\r\nhttps://www.tensorflow.org/install/docker\r\n\r\n## Description of issue (what needs changing):\r\nSince the latest docker builds are now tensorflow v2.x instead of v1.x the python example script on this page doesn't work out of the box. If the user copy and pastes the commands from this page they'll get a \"AttributeError: module 'tensorflow' has no attribute 'enable_eager_execution'\" error when trying to verify their docker container.\r\nThe python example script can be changing to this to fix it:\r\n\r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.enable_eager_execution();\r\nprint(tf.reduce_sum(tf.random_normal([1000, 1000])))\r\n```\r\n\r\nA better alternative however would be to change it to a TF v2.0 native example.\r\n\r\n", "comments": ["Thanks. Fixed in https://github.com/tensorflow/docs/pull/1261 and tensorflow/org will get updated next site push.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34758\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34758\">No</a>\n", "The docker installation verification code is still broken. There is still a version 1.x API call in there. I've opened PR https://github.com/tensorflow/docs/pull/1310 which fixes this. @lamberta Hopefully this can get merged and sorted easily."]}, {"number": 34757, "title": "Multiple inputs for iOS benchmark app", "body": "Hi.\r\nI'm trying to find a way in the \"benchmark_params.json\":\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/ios/README.md\r\n\r\nTo define multiple inputs to the network. Is it even possible?\r\n\r\n", "comments": ["I found a solution. It is possible. I found the explanation in the source code error message:\r\n    LOG(ERROR) << \"There must be the same number of items in --input_layer,\"\r\n               << \" --input_layer_shape, and --input_layer_type, for example\"\r\n               << \" --input_layer=input1,input2 --input_layer_type=float,float \"\r\n               << \" --input_layer_shape=1,224,224,4:1,20\";\r\n\r\nin the file:\r\nhttps://github.com/tensorflow/tensorflow/blob/d1002d97efdbb173e3798cba7e11dd0eb89bdbe7/tensorflow/tools/benchmark/benchmark_model.cc\r\n\r\n\r\nI believe this explanation should be added to the documentation", "@Ori226 Thanks for flagging this! Will keep this issue open until the doc is updated.", "@Ori226 \r\nIs this still an issue.", "The multiple input explanation has been added to the [README file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/README.md) since. Closing."]}, {"number": 34756, "title": "Cannot load model in TF2.1.0 AttributeError: 'h5py.h5f.FileID' object has no attribute 'endswith'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.1.0rc0\r\n- Keras version: 2.2.4-tf\r\n- Python version: 3.6.1\r\n- CUDA/cuDNN version: 10.1 / 7.6.5\r\n- GPU model and memory: RTX 2060 6GB\r\n\r\n**Describe the current behavior**\r\nI cannot load a model that has been trained with previous versions of the TensorFlow 1.x\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\project\\model.py\", line 56, in load_weights\r\n    weights = model.load_weights(file, by_name=True)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 234, in load_weights\r\n    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 1183, in load_weights\r\n    if _is_hdf5_filepath(filepath):\r\n  File \"C:\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 1500, in _is_hdf5_filepath\r\n    return (filepath.endswith('.h5') or filepath.endswith('.keras') or\r\nAttributeError: 'h5py.h5f.FileID' object has no attribute 'endswith'\r\n```", "comments": ["@kiflowb777 ,\r\nCan you please provide the code to reproduce the error faced ?Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@kiflowb777 ,\r\nAny update on the issue ?Thanks!", "Need help phone is not att\n\n\nOn Wed, Dec 18, 2019, 04:22 oanush <notifications@github.com> wrote:\n\n> @kiflowb777 <https://github.com/kiflowb777> ,\n> Any update on the issue ?Thanks!\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/34756?email_source=notifications&email_token=AOBPXOT3A3XTL6A7G3PD4NTQZH2XHA5CNFSM4JTUGZYKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHFT5KQ#issuecomment-566967978>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AOBPXOT5YWKUZT3ZMAIUP7TQZH2XHANCNFSM4JTUGZYA>\n> .\n>\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I am facing this issue. What is the ideal solution this. Using Tensorflow 2.3"]}, {"number": 34753, "title": "libcublas.so.10.0: cannot open shared object file", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu server 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.13.1\r\n- TensorFlow version:1.13.1\r\n- Python version:3.6.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:it is saying nvidia-smi cuda 10 but on nvcc -version it is not showing \r\n- GPU model and memory:p40\r\n\r\n\r\n\r\ncannot run \r\nimport tensorflow \r\n\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n*\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["i installed tensorflow-gpu==2.0.2\r\nit is working correctly but i want to install  tensorflow-gpu==1.13.1 which is not working", "@game-sys, Tensorflow-gpu uses CUADa and cuDNN. if you have installed them, make sure the LD_LIBRARY_PATH is correctly set and pointing to where the cudnn libraries are located. Please check the Tensorflow [tested build configuration](https://www.tensorflow.org/install/source#gpu) for GPU support. Thanks! ", "Done \r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Wed_Oct_23_19:24:38_PDT_2019\r\nCuda compilation tools, release 10.2, V10.2.89\r\n\r\nBut still having same error:\r\n\r\nusing TensorFlow backend.\r\nTraceback (most recent call last):\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"objectdetecton.py\", line 1, in <module>\r\n    from imageai.Detection import ObjectDetection\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/imageai/Detection/__init__.py\", line 2, in <module>\r\n    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/imageai/Detection/keras_retinanet/models/resnet.py\", line 19, in <module>\r\n    import keras\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import utils\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/keras/utils/__init__.py\", line 6, in <module>\r\n    from . import conv_utils\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/keras/utils/conv_utils.py\", line 9, in <module>\r\n    from .. import backend as K\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/keras/backend/__init__.py\", line 1, in <module>\r\n    from .load_backend import epsilon\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/keras/backend/load_backend.py\", line 90, in <module>\r\n    from .tensorflow_backend import *\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 5, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/fast/anaconda3/envs/imageai/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help\r\n", "Solved the issue by uninstalling nvidia driver , CUDA, CuDNN and from scracth installed driver with CUDA 10.0 ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34753\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34753\">No</a>\n"]}, {"number": 34752, "title": "MultiWorkerMirroredStrategy  Performance is low (2gpu, 2node)  X1.3   speed-up", "body": "System information\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n    OS Platform and Distribution: Ubuntu 18.04\r\n    TensorFlow installed from (source or binary): pip install tensorflow-gpu\r\n    TensorFlow version (use command below): 2.0\r\n    Python version: 3.6.9\r\n    CUDA/cuDNN version: 10/7.6.4.38\r\n    GPU model and memory: Tesla P4  8G\r\n\r\nDescribe the current behavior\r\nI  run the code described below:\r\n\r\n**TEST 1:   (two machine)**\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:12345\", \"server2:12345\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n\r\nIn the other machine\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:12345\", \"server2:12345\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 1}\r\n})\r\n\r\nWhen the script start processing the first epoch it crashes,\r\n\r\n**Describe the expected behavior**\r\n\r\n15s/epoch  is so slow\r\n\r\n<img width=\"1162\" alt=\"\u56fe\u7247\" src=\"https://user-images.githubusercontent.com/12653212/69707374-942a6780-1134-11ea-8dd1-994fd7e41451.png\">\r\n\r\n\r\n**TEST 2:   (one machine)**\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': [\"server1:12345\"]\r\n    },\r\n    'task': {'type': 'worker', 'index': 0}\r\n})\r\n\r\n**Describe the expected behavior**\r\n\r\n5s/epoch      same as use  strategy = tf.distribute.MirroredStrategy()  one GPU card\r\n\r\n<img width=\"1072\" alt=\"\u56fe\u7247\" src=\"https://user-images.githubusercontent.com/12653212/69707387-9b517580-1134-11ea-976a-73c9d36fdc2b.png\">\r\n\r\n\r\n**CODE**\r\n\r\n```\r\nimport ssl\r\nimport os\r\nimport json\r\nimport argparse\r\nimport time\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\nssl._create_default_https_context = ssl._create_unverified_context\r\n\r\n\r\ndef configure_cluster(worker_hosts=None, task_index=-1):\r\n    \"\"\"Set multi-worker cluster spec in TF_CONFIG environment variable.\r\n    Args:\r\n      worker_hosts: comma-separated list of worker ip:port pairs.\r\n    Returns:\r\n      Number of workers in the cluster.\r\n    \"\"\"\r\n    tf_config = json.loads(os.environ.get('TF_CONFIG', '{}'))\r\n    if tf_config:\r\n        num_workers = len(tf_config['cluster'].get('worker', []))\r\n    elif worker_hosts:\r\n        workers = worker_hosts.split(',')\r\n        num_workers = len(workers)\r\n        if num_workers > 1 and task_index < 0:\r\n            raise ValueError('Must specify task_index when number of workers > 1')\r\n        task_index = 0 if num_workers == 1 else task_index\r\n        os.environ['TF_CONFIG'] = json.dumps({\r\n            'cluster': {\r\n                'worker': workers\r\n            },\r\n            'task': {'type': 'worker', 'index': task_index}\r\n        })\r\n    else:\r\n        num_workers = 1\r\n    return num_workers\r\n\r\n\r\nparser = argparse.ArgumentParser(description='TensorFlow Benchmark',\r\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\nparser.add_argument('--num-epochs', type=int, default=5, help='input batch size')\r\nparser.add_argument('--batch-size-per-replica', type=int, default=32, help='input batch size')\r\nparser.add_argument('--worker-method', type=str, default=\"NCCL\")\r\nparser.add_argument('--worker-hosts', type=str, default=\"localhost:23456\")\r\nparser.add_argument('--worker-index', type=int, default=0)\r\n\r\nargs = parser.parse_args()\r\n\r\nworker_num = configure_cluster(args.worker_hosts, args.worker_index)\r\nbatch_size = args.batch_size_per_replica * worker_num\r\nprint('Batch Size: %d' % batch_size)\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nprint(\"Physical GPU Devices Num:\", len(gpus))\r\nfor gpu in gpus:\r\n    tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\nif args.worker_method == \"AUTO\":\r\n    communication = tf.distribute.experimental.CollectiveCommunication.AUTO\r\nelif args.worker_method == \"RING\":\r\n    communication = tf.distribute.experimental.CollectiveCommunication.RING\r\nelse:\r\n    communication = tf.distribute.experimental.CollectiveCommunication.NCCL\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\r\n    communication=communication)\r\n\r\n\r\n# logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n# print(\"Logical GPU Devices Num:\", len(gpus))\r\n\r\n\r\ndef resize(image, label):\r\n    image = tf.image.resize(image, [128, 128]) / 255.0\r\n    return image, label\r\n\r\n\r\n# if as_supervised is True\uff0creturn image abd label\r\ndataset, info = tfds.load(\"tf_flowers\", split=tfds.Split.TRAIN, with_info=True, as_supervised=True)\r\ndataset = dataset.map(resize).repeat().shuffle(1024).batch(batch_size)\r\n\r\n# options = tf.data.Options()\r\n# options.experimental_distribute.auto_shard = False\r\n# dataset = dataset.with_options(options)\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, [3, 3], activation='relu'),\r\n        tf.keras.layers.Conv2D(64, [3, 3], activation='relu'),\r\n        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\r\n        tf.keras.layers.Dropout(0.25),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dropout(0.5),\r\n        tf.keras.layers.Dense(info.features['label'].num_classes, activation='softmax')\r\n    ])\r\n    model.compile(\r\n        opt=tf.keras.optimizers.Adam(learning_rate=0.0001),\r\n        loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n        metrics=[tf.keras.metrics.sparse_categorical_accuracy]\r\n    )\r\n    return model\r\n\r\n\r\nwith strategy.scope():\r\n    multi_worker_model = build_and_compile_cnn_model()\r\nprint(\"Now training the distributed model\")\r\n\r\n\r\nclass TimeHistory(tf.keras.callbacks.Callback):\r\n    def on_train_begin(self, logs={}):\r\n        self.times = []\r\n        self.totaltime = time.time()\r\n\r\n    def on_train_end(self, logs={}):\r\n        self.totaltime = time.time() - self.totaltime\r\n\r\n    def on_epoch_begin(self, batch, logs={}):\r\n        self.epoch_time_start = time.time()\r\n\r\n    def on_epoch_end(self, batch, logs={}):\r\n        self.times.append(time.time() - self.epoch_time_start)\r\n\r\n\r\ntime_callback = TimeHistory()\r\nsteps_per_epoch = 100\r\nprint('Running benchmark...')\r\nmulti_worker_model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=args.num_epochs, callbacks=[time_callback])\r\nper_epoch_time = np.mean(time_callback.times[1:])\r\nprint(\"per_epoch_time:\", per_epoch_time)\r\nimg_sec = batch_size * steps_per_epoch / per_epoch_time\r\nprint(\"Result:  {:.1f} pic/sec\".format(img_sec))\r\n\r\n\r\n```\r\n\r\nIn TEST 2:    only 1 worker,   440pic/sec  \uff08batch_szie = 128\uff09\r\n\r\nIn TEST 1:  2 workers,     610 pic/sec   \uff08batch_szie = 128*2\uff09   [Expect  440 *2 = 800+]\r\n\r\n**Question1:** \r\nwith dist MultiWorkerMirroredStrategy  worker nums > 1,   why Training is so slow\r\n\r\nExpect\r\n\r\n", "comments": ["There are many reasons why your models can be slow: networking, reading data, threads contention, etc. You can profile your program to see which part is the bottleneck: https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34752\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34752\">No</a>\n", "Closing now. Feel free to reopen or file a new issue if you see obvious problems on your profile."]}, {"number": 34751, "title": "Added Support for Person Detection for ESP32", "body": "Req- Labels : **comp: lite** , **Work In Progress**\r\nFix Some Build Issues\r\n- [x]  1) remove  the file \"person_model_grayscale/person_detect_model_data.cc\u201d from CMakeLists.txt of main and add it in CMakeLists.txt of components/tfmicro/\r\n\r\n- [x] 2) change the header person_detect_model_data.h in file  \"person_model_grayscale/person_detect_model_data.cc\u201d to its appropriate name\r\n\r\n- [x] 3) Add support in current build system to add esp specific files to espressif projects such as sdkconfig.defaults , Kconfig.projbuilds, partitions.csv( if required) .\r\n\r\n- [x] 4) Add esp specific third party components when building esp project for e.x. [esp32-camera](https://github.com/espressif/esp32-camera) for this project\r\n\r\nTo build this project successfully follow below steps till build issues are fixed \r\n\r\n1) Generate esp project using \r\n`make -f tensorflow/lite/experimental/micro/tools/make/Makefile TARGET=esp generate_person_detection_esp_project`\r\n\r\n2) In esp-idf/main/CMakeLists.txt remove following line from SRCS var of idf_component_register `tensorflow/lite/experimental/micro/tools/make/downloads/person_model_grayscale/person_detect_model_data.cc` and add it to the SRCS var of comopnents/tfmicro/CMakeLists.txt  \r\n\r\n3) in `person_detect_model_data.cc` (which is located at path given in step 2) change the included header file `person_detect_model_data` to its respective path.\r\n\r\n4) Clone component  [esp32-camera](https://github.com/espressif/esp32-camera) into components/\r\n5) Copy ` sdkconfig.defaults` from `person_detection/esp` and put it into newly generated `esp-idf` folder ( where we will build the project ) , also copy `Kconfig.projbuild` file from `person_detection/esp` into `esp-idf/main`\r\n6) the idf at `$IDF_PATH`  must be checked out at release/v4.0 ", "comments": ["@fredrec , the `CMakelists.txt.tpl` had used wrong method to add compile flags so flag `-O3` ( along with some other flags) was not being applied while compiling, I have made the respective changes, Please test this example on ESP-EYE , I will add respective changes to the micro speech MR as well.", "@AdityaHPatwardhan Could you please resolve the conflicts? Thanks!", "@gbaned @fredrec , did you get a chance to review this MR , It provides correct output within 1 sec of time, Please provide the patch which fixes the build issues. also If there are no issues pending please help merge it ASAP, Thank You", "Hi Aditya,\r\n\r\nI sent you a PR (https://github.com/AdityaHPatwardhan/tensorflow/pull/2) that should fix the build issues.\r\n\r\nAfter generating the example (`make V=1 -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp clean generate_person_detection_esp_project`), I was able to build by just adding the `esp32-camera` component and calling `idf.py`:\r\n```\r\ngit clone https://github.com/espressif/esp32-camera.git components/esp32-camera\r\nidf.py build\r\n```\r\n\r\n", "@fredrec I have made the necessary changes, Thank you for the build fixes, and I think this should be ready to merge.", "> @fredrec I have made the necessary changes, Thank you for the build fixes, and I think this should be ready to merge.\r\n\r\nI just noticed that I mistakenly hard-coded some path in the make file. Can you please apply this fix: https://github.com/fredrec/tensorflow/commit/0cfd78254f36d3825e4164fd27aa0144fe762691", "@petewarden: This PR seems to be done, it builds and works fine on an ESP-EYE board. Can you please merge it ?\r\n", "@fredrec I have made the suggested changes to build fixes. Thank you for your help in fixing the build issues.", "I fixed the compile warnings, when building the example, It runs successfully without any error.", "Hi @AdityaHPatwardhan,\r\n\r\nThere are some problems merging this PR. Most likely it is because some files (`app_camera_esp.c`, `app_camera_esp.h`) have the wrong licence headers (currently `ESPRESSIF MIT License`). Can you please update it to the Tensorflow licence header ?", "@fredrec , Yes will do it ASAP.", "@fredrec, I have made the respective changes regarding license headers as you requested, Along with that I have also Fixed README for the person_detection and micro_speech example.\r\n\r\nAlso,\r\nWhen @tensorflower-gardener merged the micro_speech PR , I think they added a line \r\n``` ifeq ($(TARGET, \u201cesp\u201d)```  which causes build error in build micro_speech example on master branch ,\r\nwhich should have been ```ifeq ($(TARGET, esp) ```, I have fixed that error in this branch with \r\n[this-commit](https://github.com/tensorflow/tensorflow/commit/8978def1b7e6870edfc4dd20149e2c9e32b97dc6).\r\n\r\nAlong with this, \r\nThere is a new error message  printed at the `generate_esp_project` command because of `ESP_PROJECT_FILES` which we added in the build fix of micro_speech.\r\nWhen `generateing_esp_project` tensorflow is executing all the makefiles and internally the `ESP_PROJECT_FILES` is getting replaced with values for other examples which prints an error message while building a particular example (say hello_world)\r\nActually project is generated correctly , it just that `generate_esp_project` command prints failure.\r\nCan you Please Take a look,\r\nThank You", "[[@AdityaHPatwardhan I used exmail.qq.com to send email. I changed pem and builded seccessfully. But I can't receive emails.\r\n\r\nThe LOG was:\r\n```\r\nperson detected\r\nI (1212591) smtp_client: Connecting to smtp.exmail.qq.com:465...\r\nI (1212641) smtp_client: Connected.\r\nE (1272711) smtp_client: mbedtls_net_recv failed with error -0x0\r\n\r\nI (1272711) smtp_client: Email sent successfully !\r\n```\r\n", "@rocLv, if you are talking about the [doorbell_camera](https://github.com/espressif/tensorflow/tree/master/tensorflow/lite/micro/examples/doorbell_camera) example then I dont think this would be a proper platform to discuss the issue. As the example you are referring is in a different repository ( surely forked from this one, but still a different one ). If you have any issue regarding the `doorbell_camera` you may email me and we will take it up from there.\r\nMy email id - aditya.patwardhan@espressif.com\r\nThank you."]}, {"number": 34750, "title": "estimator API may not save the latest variable value", "body": "**System information**\r\n- TensorFlow version :tf1.10.0\r\n\r\n**Describe the current behavior**\r\n\r\nhello,I build a model with estimator API, According to this tutorial: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator ,I use parameter server distribute strategy to train my model. I devide the training data into several parts on average, the chief get one part, each worker get one part also. I didn't set the max train step,so the training will end when the data used up. in practical, I found that some worker is slower than chief(because of mechine performace),so the chief finish early than some workers. when the chief finish, it will dump the last checkpoint, and the training result(by other unfinished worker) after this time node will be lost.I think it may not expected.\r\n**Describe the expected behavior**\r\nI expect the model satisfy \u201call the training data used exactly once\u201d\uff0cno mater which mechine run faster. Is there any way to make the chief wait until all worker finished\uff0cthen dump the last checkpoint\uff1f\r\n\r\n", "comments": ["@SeekerYb ,\r\nCan you please provide the standalone code used ?Thanks!", "@SeekerYb ,\r\nAny update on the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34749, "title": "how to convert tensorflow1.13 trained model ckpt to tflite use tensorflow2.0?", "body": "i have a model trained use tensorflow1.13 and i can export model to pb from checkpoints and then convert to tflite file by tf.lite.TFLiteConverter.from_frozen_graph. but in tensorflow 2.0 there is no this method. my code trained use slim, so is very hard to change to tf2.0 retrain the model, are there any method to read from the original ckpt files to convert tf2.0 tflite file? ", "comments": ["You may use ```tf.compat.v1.lite.TFLiteConverter``` in this case.\r\nSee https://www.tensorflow.org/api_docs/python/tf/compat/v1/lite/TFLiteConverter#from_frozen_graph ", "my approach is first to load ckpt and then use saved_model builder to save a format of saved_model, then update tensorflow to 2.1, then use tfliteconverter v2 method from_saved_model to generate tflite"]}, {"number": 34748, "title": "TFLite micro hello_world sketch fails to build", "body": "**System information**\r\n- OS Platform and Distribution: 4.19.67-2rodete2-amd64\r\n- Target platform: Arduino 1.8.10 w/ `nano33ble` target\r\n- TensorFlow version: 1.15.0-ALPHA\r\n\r\n**Describe the problem**\r\n\r\n`hello_world` sketches fails to build due to a missing header file (not included in the Arduino libraries distribution).\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n- Install `Arduino_TensorFlowLite@1.15.0-ALPHA`\r\n- Open `hello_world` example sketch\r\n- Click the `Verify` \u2713 button.\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n/usr/local/google/home/proppy/.arduino15/packages/arduino/tools/arm-none-eabi-gcc/7-2017q4/bin/arm-none-eabi-g++ -c -w -g -Os -nostdlib @/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/variants/ARDUINO_NANO33BLE/defines.txt @/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/variants/ARDUINO_NANO33BLE/cxxflags.txt -DARDUINO_ARCH_NRF52840 -mcpu=cortex-m4 -w -x c++ -E -CC -DARDUINO=10810 -DARDUINO_ARDUINO_NANO33BLE -DARDUINO_ARCH_MBED -I/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/cores/arduino -I/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/variants/ARDUINO_NANO33BLE -I/usr/local/google/home/proppy/Arduino/libraries/Arduino_TensorFlowLite/src -I/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/cores/arduino/api/deprecated -iprefix/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/cores/arduino @/usr/local/google/home/proppy/.arduino15/packages/arduino/hardware/mbed/1.1.3/variants/ARDUINO_NANO33BLE/includes.txt /tmp/arduino_build_987011/sketch/hello_world.ino.cpp -o /dev/null\r\nAlternatives for tensorflow/lite/c/common.h: []\r\nResolveLibrary(tensorflow/lite/c/common.h)\r\n  -> candidates: []\r\nMultiple libraries were found for \"TensorFlowLite.h\"\r\nIn file included from /tmp/arduino_modified_sketch_134477/hello_world.ino:21:0:\r\n Used: /usr/local/google/home/proppy/Arduino/libraries/Arduino_TensorFlowLite\r\noutput_handler.h:19:10: error: tensorflow/lite/c/common.h: No such file or directory\r\n Not used: /usr/local/google/home/proppy/Arduino/libraries/tensorflow_lite\r\n #include \"tensorflow/lite/c/common.h\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n```\r\n\r\n/cc @dansitu @khanhlvg", "comments": ["Simply removing the following include workaround the issue.\r\n```\r\n#include \"tensorflow/lite/c/common.h\"\r\n```", "@advaitjain - Assigning since you're reviewing https://github.com/tensorflow/tensorflow/pull/34747", "This works for me from the tip of tree. I think @proppy is running into a mismatch between the example and library version.\r\n\r\nCan you try manually generating the latest tensorflow_lite arduino library with these steps:\r\n\r\n```\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile clean\r\nmake -f tensorflow/lite/experimental/micro/tools/make/Makefile \\\r\n  TARGET=arduino \\\r\n  TAGS=\"portable_optimized\" \\\r\n  generate_arduino_zip\r\n```\r\n\r\nRemove the existing tensorflow_lite library (you can also move it if you prefer that):\r\n```\r\nrm -rf ~/Arduino/libraries/tensorflow_lite\r\n```\r\n\r\nInstall the newly created Arduino library:\r\n```\r\nunzip tensorflow/lite/experimental/micro/tools/make/gen/arduino_x86_64/prj/tensorflow_lite.zip -d ~/Arduino/libraries/\r\n```\r\n\r\nAnd then try the build.\r\n\r\nThese steps worked for me so I'm assuming that it is only some configuration issue at your end.\r\n\r\nAdding @petewarden and @dansitu to this conversation.", "@advaitjain Spot on!\r\n\r\nI thought this was happening against a new installation of Arduino `1.8.10` against the latest published version of the library `1.15.0-ALPHA`. But after testing against a fresh install of the library after cleaning up `~/Arduino/libraries`, the `hello_world` example compile successfully.\r\n\r\nI think this was due to example being picked up from a stale installation in `tensorflow_lite` (as shown in the compile log) while include files where being looked up in another directory `Arduino_TensorFlowLite`.\r\n\r\nSorry for the noise!\r\n\r\n"]}, {"number": 34747, "title": "lite/third_party/kiss_fft: fix include patches", "body": "- remove commented out `string.h` include\r\n- add missing `stdint.h` include\r\n\r\nThis fixes compilation with the esp32 core arduino toolchain.\r\n\r\nTested that the `nano33ble` build still work (modulo #34748), after manually applying the patches.\r\n\r\n/cc @dansitu @khanhlvg ", "comments": ["Thanks for the pull request.\r\n\r\nThe lines edited here were previously changed in this commit:\r\nhttps://github.com/tensorflow/tensorflow/commit/91bcbd69225a51d283aa0a918e2449cc66b03078\r\n\r\n@proppy: Could you please add some context about the error that you are seeing (and fixing with this issue? Ideally make a github issue with the steps to reproduce the error that you are seeing and reference that from this PR.\r\n\r\n@dansitu: It seems like you had explicitly commented out string.h and removed the additional stdint include. Could you make an issue for what problem that was causing.\r\n\r\nI want to make sure that we find a solution that doesn't break something else.\r\n\r\nI will fix #34748 independent of this PR -- thanks for creating that issue.", "All I remember is that the build previously failed for the Arduino Nano 33 BLE Sense if those `string.h` and `stdint.h` includes were present. Maybe they were not available in an earlier version of the board support package?\r\n\r\nIf the build works for that board with these new changes, then the changes look good to me.", "@proppy Can you please resolve conflicts? Thanks!", "PTAL, rebased", "Hi @proppy, it looks like this PR might need to be rebased one more time. Again, apologies for the delay.", "@proppy Can you please check advaitjain's comments and keep us posted. Thanks!", "@proppy Any update on this PR? Please. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I believe this is handled internally , will close this PR, feel free to reopen."]}, {"number": 34746, "title": "lite/microfrontend: fix FilterbankState unsigned type missmatch", "body": "`FilterbankState` `work` is `uint64_t*`, casting a signed type prevent the\r\nlibraries to compile w/ the esp32 arduino core toolchain.\r\n\r\n/cc @dansitu @khanhlvg ", "comments": []}, {"number": 34745, "title": "Update init_ops_v2.py", "body": "Issue #34712 the way i understood the docs reference (https://www.tensorflow.org/community/contribute/docs_ref) we need to remove backticks(```) before >>> and put (...) for indentation.", "comments": ["Please open the PR against master branch, not the `r2.0` one.\r\n\r\nThe `rx.y` branches are release branches. They branch from `master` when we start the release process, we only accept critical cherry-picks and, once the release is done, they are not merged back into master. Hence, if you want your change to be useful, it should go into `master`.", "But the problem is not there in the master branch.\r\nDocumentation -- https://www.tensorflow.org/api_docs/python/tf/constant_initializer#used_in_the_tutorials\r\n\r\nCode URL ( The one given on the documentation page)-- https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/init_ops_v2.py#L123-L215\r\n\r\nIn the master branch the code is fine, the documentation is reading from the r2.0 branch\r\n\r\nMaster branch url -- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops_v2.py\r\n\r\n@mihaimaruseac  Please check once.\r\n", "The documentation for 2.0 won't be upgraded as we already released 2.0.\r\n\r\nThe documentation for 2.1 should update soon to be the one from r2.1 branch"]}, {"number": 34744, "title": "Nested TensorArray using tf.function. Write->concat->write", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n\r\n **Yes**\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n\r\n**Windows 10 Pro 1809. OS Build: 17763.503**\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n\r\n**Not applicable**\r\n\r\n- TensorFlow installed from (source or binary):\r\n\r\n**binary, via pip\r\npip3 install --user --upgrade tensorflow-gpu**\r\n\r\n- TensorFlow version (use command below):\r\n\r\n**v2.0.0-rc2-26-g64c3d382ca 2.0.0**\r\n\r\n- Python version:\r\n\r\n**3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]**\r\n\r\n- Bazel version (if compiling from source):\r\n\r\n**Not applicable**\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n**Not applicable**\r\n\r\n- CUDA/cuDNN version:\r\n\r\n**Cuda compilation tools, release 10.0, V10.0.130\r\ncuDNN: cudnn-10.0-windows10-x64-v7.6.5.32**\r\n\r\n- GPU model and memory:\r\n\r\n**GTX 1060 Ti, 6Gb**\r\n\r\n**Describe the current behavior**\r\n\r\nI have two functions, one calling another, in the inner function, I'm accumulating some data into a TensorArray, then return TensorArray.concat() result to the outer function. In the outer function I have a loop that calls the inner function. On each call, it retrieves the result of the inner function and writes it to it's own TensorArray. The operations could be summarized as write->concat->write. The whole program runs some random number of iterations before crashing, likely due to some non-deterministic execution of the graph, perhaps something to do with control dependencies?\r\n\r\nObservations:\r\n- The higher the number of iterations in the outer function (currently 50), the more quickly it fails.\r\n- It appears 3 is a minimum number of iterations inside the outer function for the issue to occur\r\n- Issue occurs only on GPU (CPU does fine, using ``os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\")``. It does not occur in eager execution mode.\r\n- Code generates a series of different errors, sporadically (see below)\r\n- The functions do not need to be nested for the issue to occur (see reduced code version below)\r\n- You can remove ``infer_shape=False, element_shape=(3,)`` and see it generate a different error:\r\n``tensorflow.python.framework.errors_impl.InvalidArgumentError:  Trying to concat list with only uninitialized tensors but element_shape_except_first_dim_ is not fully defined: []\t \r\n[[{{node while/body/_1/TensorListConcatV2}}]] [Op:__inference_computeElement_71]\r\n``\r\n- It may take many runs to encounter an error (up to a 1000 on my machine)\r\n\r\n**Describe the expected behavior**\r\n\r\nCode producing same result and running indefinitely.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n@tf.function\r\ndef computeElement_byBin():\r\n\tc = tf.TensorArray(tf.int64, size=1, infer_shape=False, element_shape=(3,))\r\n\tconst = tf.cast(tf.constant([1, 2, 3]), tf.int64)\r\n\tc = c.write(0, const)\r\n\tc_c = c.concat()\r\n\treturn c_c\r\n\r\n@tf.function\r\ndef computeElement():\r\n\tc = tf.TensorArray(tf.int64, size=50, infer_shape=False, element_shape=(3,))\r\n\tfor x in tf.range(50):\r\n\t\tbyBinVariant = computeElement_byBin()\r\n\t\tc = c.write(x, byBinVariant)\r\n\r\n\treturn tf.constant([1, 2, 3])\r\n\r\nk=0\r\nwhile True:\r\n\tk+=1\r\n\tr = computeElement()\r\n\tprint('iteration: %s, result: %s'%(k,r))\r\n```\r\n\r\nHere's a reduced version of code, without nesting (behaves similarly):\r\n\r\n``` python\r\n@tf.function\r\ndef computeElement():\r\n\ttensorArray1 = tf.TensorArray(tf.int32, size=50, infer_shape=False, element_shape=(3,))\r\n\ttensorArray2 = tf.TensorArray(tf.int32, size=1, infer_shape=False, element_shape=(3,))\r\n\tconst = tf.constant([1, 2, 3])\r\n\tfor x in tf.range(50):\r\n\t\ttensorArray2 = tensorArray2.write(0, const)\r\n\t\tconcat = tensorArray2.concat()\r\n\t\ttensorArray1=tensorArray1.write(x, concat)\r\n\treturn tf.constant(1)\r\n\r\nk=0\r\nwhile True:\r\n\tk+=1\r\n\tr = computeElement()\r\n\tprint('iteration: %s, result: %s'%(k,r))\r\n```\r\n\r\n**Other info / logs**\r\nOutput (example):\r\n\r\n```\r\n2019-12-01 23:29:49.834579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll\r\n2019-12-01 23:29:50.917651: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-01 23:29:50.988907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:01:00.0\r\n2019-12-01 23:29:50.992256: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-01 23:29:50.995402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-01 23:29:50.996693: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-12-01 23:29:50.999987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: GeForce GTX 1660 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.77\r\npciBusID: 0000:01:00.0\r\n2019-12-01 23:29:51.001975: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.\r\n2019-12-01 23:29:51.003600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2019-12-01 23:29:51.501197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-01 23:29:51.503618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2019-12-01 23:29:51.505070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2019-12-01 23:29:51.506907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4629 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n\r\niteration: 1, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)\r\niteration: 2, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)\r\niteration: 3, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)\r\niteration: 4, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)\r\niteration: 5, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)\r\niteration: 6, result: tf.Tensor([1 2 3], shape=(3,), dtype=int32)\r\nr=outer()\r\n  File \"Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 457, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 494, in _call\r\n    results = self._stateful_fn(*args, **kwds)\r\n  File \"Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1823, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1141, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1224, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager)\r\n  File \"Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 511, in call\r\n    ctx=ctx)\r\n  File \"Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError:  Tried to set a tensor with incompatible shape at a list index. Item element shape: [3,3] list shape: [3]\r\n\t [[{{node while/body/_1/TensorArrayV2Write/TensorListSetItem}}]] [Op:__inference_computeElement_79]\r\n```\r\n\r\nOther errors that code may generate:\r\n```\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n(No stack trace)\r\n```\r\nor\r\n```\r\n2019-12-01 23:10:55.194128: F tensorflow/core/framework/tensor_shape.cc:445] Check failed: end <= dims() (1 vs. 0)\r\n\r\nProcess finished with exit code -1073740791 (0xC0000409)\r\n```\r\n\r\nCheers!", "comments": ["New observation: using stack() instead of concat() eliminates the issue (I would like to use concat though, because in reality my tensors are un-even on one dimension). \r\n\r\n``` python\r\n@tf.function\r\ndef computeElement():\r\n\ttensorArray1 = tf.TensorArray(tf.int32, size=50)#, infer_shape=False, element_shape=(3,))\r\n\ttensorArray2 = tf.TensorArray(tf.int32, size=1)#, infer_shape=False, element_shape=(3,))\r\n\tconst = tf.constant([1, 2, 3])\r\n\tfor x in tf.range(50):\r\n\t\ttensorArray2 = tensorArray2.write(0, tf.transpose(const))\r\n\t\tconcat = tensorArray2.stack()\r\n\t\ttensorArray1=tensorArray1.write(x, concat)\r\n\treturn tf.constant(1)\r\n```", "Issue is replicating with Tf 2.0.\r\nPlease see the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/6a1fd005c737b2f40e87756864b049bc/untitled288.ipynb). Thanks!", "I played around with it more and narrowed it down a bit:\r\n\r\n``` python\r\n@tf.function\r\ndef computeElement():\r\n    tarr = tf.TensorArray(tf.int32, size=1,clear_after_read=False)\r\n    tarr = tarr.write(0, [1])\r\n    concat = tarr.concat()\r\n\r\n    # PROBLEM HERE\r\n    for x in tf.range(50):\r\n        concat = tarr.concat()\r\n\r\n    return concat\r\n```\r\n\r\nIf you set ``tf.config.threading.set_inter_op_parallelism_threads(1)`` the bug goes away, which means it's to do with parallelization of the unrolled tensorflow loop. Knowing that tensorflow unrolls statically when looping over a python variable rather than a tensor, I could confirm that this code worked:\r\n\r\n``` python\r\n@tf.function\r\ndef computeElement(arr):\r\n    tarr = tf.TensorArray(tf.int32, size=1)\r\n    tarr = tarr.write(0, [1])\r\n    concat = tarr.concat()\r\n\r\n    a = 0\r\n    while a<arr:\r\n        concat = tarr.concat()\r\n        a+=1\r\n\r\n    return concat\r\n\r\nk = 0\r\nwhile True:\r\n    k += 1\r\n    r = computeElement(50)\r\n```\r\n\r\nSo solution for now is to loop over a python variable rather than a tensor. Perhaps would be nice to get an error when invoking a non-thread safe method inside a thread, but syntactically I don't see anything wrong with this. I guess one concern with current solution would be performance, as whatever else might be in the loop would be single-threaded.", "@ostruk Sorry for the late response. Is this still an issue for you? I ran your code and it was not crashing for more than 490K iterations. Manually interrupted the code from execution. Please take a look at the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/2583297555553625054f7339b0e41bd1/untitled288.ipynb). Thanks!\r\n\r\n```\r\niteration: 492551, result: tf.Tensor(1, shape=(), dtype=int32)\r\niteration: 492552, result: tf.Tensor(1, shape=(), dtype=int32)\r\niteration: 492553, result: tf.Tensor(1, shape=(), dtype=int32)\r\niteration: 492554, result: tf.Tensor(1, shape=(), dtype=int32)\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34744\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34744\">No</a>\n"]}, {"number": 34743, "title": "Will TF2.0 build with Python3 ONLY, without Python2? ", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.10\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: pip \r\n- Bazel version (if compiling from source):1.1.0\r\n- GCC/Compiler version (if compiling from source): 9.2.1\r\n- CUDA/cuDNN version:10.2/7.6.5\r\n- GPU model and memory: GeForce GTX 980M/4035MiB\r\n\r\n\r\n\r\n```console\r\nERROR: ~/Downloads/....../tensorflow/python/keras/api/BUILD:129:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 771, in <module>\r\n    main()\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 767, in main\r\n    lazy_loading, args.use_relative_imports)\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 625, in create_api_files\r\n    api_version, compat_api_versions, lazy_loading, use_relative_imports)\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 502, in get_api_init_text\r\n    _, attr = tf_decorator.unwrap(attr)\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 219, in unwrap\r\n    elif _has_tf_decorator_attr(cur):\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/tf_decorator.py\", line 124, in _has_tf_decorator_attr\r\n    hasattr(obj, '_tf_decorator') and\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 62, in __getattr__\r\n    module = self._load()\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/python/util/lazy_loader.py\", line 45, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 28, in <module>\r\n    _wrap_py_utils = swig_import_helper()\r\n  File \"~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/wrap_py_utils.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_wrap_py_utils', fp, pathname, description)\r\n  File \"/usr/lib/python3.7/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.7/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 696, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 670, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 583, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 1043, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: ~/.cache/bazel/_bazel_longervision/90066176d51f3058b5ce7c4e1b3a40d7/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_2_keras_python_api_gen_compat_v2.runfiles/org_tensorflow/tensorflow/compiler/tf2tensorrt/_wrap_py_utils.so: undefined symbol: _ZN15stream_executor14StreamExecutor18EnablePeerAccessToEPS0_\r\n----------------\r\nNote: The failure of target //tensorflow/python/keras/api:create_tensorflow.python_api_2_keras_python_api_gen_compat_v2 (with exit code 1) may have been caused by the fact that it is a Python 2 program that was built in the host configuration, which uses Python 3. You can change the host configuration (for the entire build) to instead use Python 2 by setting --host_force_python=PY2.\r\n\r\nIf this error started occurring in Bazel 0.27 and later, it may be because the Python toolchain now enforces that targets analyzed as PY2 and PY3 run under a Python 2 and Python 3 interpreter, respectively. See https://github.com/bazelbuild/bazel/issues/7899 for more information.\r\n----------------\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: ~/....../tensorflow/tools/pip_package/BUILD:40:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v2 failed (Exit 1)\r\nINFO: Elapsed time: 12747.266s, Critical Path: 300.17s\r\nINFO: 16213 processes: 16213 local.\r\nFAILED: Build did NOT complete successfully\r\n\u279c  tensorflow git:(master) \u2717 \r\n```", "comments": ["Any update on this?", "Could this be related to https://github.com/tensorflow/tensorflow/issues/35584 ?\r\nThat - using bazel 0.26.1 solved my build issue ", "Looking for info about this as well, trying to build in a Py3-only environment.  TF 2.1.0 seems to require Bazel >= 0.27.1 (.bazelversion says 0.29.1), which means the workaround noted above no longer applies.\r\n\r\nIs there any way to override this, or just specify to not build the Py2 code sections?", "TensorFlow 2.1.0 is the last release for Python 2.7\r\nTo know more see https://groups.google.com/a/tensorflow.org/g/announce/c/w1Xu7dMa_Do\r\nThannks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34743\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34743\">No</a>\n"]}, {"number": 34742, "title": "libtensorflow_framework.so.2 segfault in OPENSSL_strcasecmp", "body": "**System information**\r\n- OS Platform and Distribution Ubuntu 18.04 on jetson xavier\r\n- TensorFlow installed from source\r\n- TensorFlow version: 2.0.0\r\n- Python version: 3.6.9\r\n- Bazel version: 0.26.1\r\n- GCC/Compiler version: 7.4.0\r\n- CUDA/cuDNN version: 10.0.326-1/ 7.5.0.56 installed from nvidia repo\r\n\r\nafter compiling tensorflow 2.0 from source my application segfaults this way:\r\n\r\n```\r\n(gdb) bt\r\n#0  0x0000007fa56656a8 in OPENSSL_strcasecmp () at /usr/local/lib/libtensorflow_framework.so.2\r\n#1  0x0000007fa5623d44 in EVP_get_cipherbyname () at /usr/local/lib/libtensorflow_framework.so.2\r\n#2  0x0000007f77b0258c in  () at /usr/lib/aarch64-linux-gnu/libssl.so.1.1\r\n#3  0x0000007f77b5c88c in  () at /usr/lib/aarch64-linux-gnu/libssl.so.1.1\r\n```\r\n\r\nopenssl version is 1.1.1-1ubuntu2.1~18.04.5.\r\n\r\nI'm using tensorflow inside a Qt application but probably this make no difference.\r\n\r\nis there any way to prevent this issue and/or remove openssl dependency while building?\r\n\r\nlibtensorflow_framework.so.2 is not linked to openssl\r\n\r\n```\r\nldd /usr/local/lib/libtensorflow_framework.so.2 \r\nlinux-vdso.so.1 (0x0000007f84044000)\r\nlibrt.so.1 => /lib/aarch64-linux-gnu/librt.so.1 (0x0000007f826d2000)\r\nlibpthread.so.0 => /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000007f826a6000)\r\nlibdl.so.2 => /lib/aarch64-linux-gnu/libdl.so.2 (0x0000007f82691000)\r\nlibm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000007f825d7000)\r\nlibstdc++.so.6 => /usr/lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000007f82444000)\r\nlibgcc_s.so.1 => /lib/aarch64-linux-gnu/libgcc_s.so.1 (0x0000007f82420000)\r\nlibc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000007f822c7000)\r\n/lib/ld-linux-aarch64.so.1 (0x0000007f84019000)\r\n```\r\n\r\nthanks\r\n", "comments": ["I won't be able to look into this until ~20th December. But will investigate", "Though, do you have a minimal reproducer?", "@mihaimaruseac I don't have access to a jetson xavier anymore but I'm getting a very similar issue on my laptop (x86_64). I'm using tensorflow and Qt from Arch Linux repo, here are my versions:\r\n\r\n```\r\npacman -Q | grep tenso\r\npython-tensorflow-estimator 2.1.0-1\r\npython-tensorflow-opt 2.1.0-4\r\ntensorflow-opt 2.1.0-4\r\n```\r\n\r\n```\r\npacman -Q | grep qt\r\nqt5-base 5.14.1-3\r\nqt5-declarative 5.14.1-1\r\nqt5-location 5.14.1-1\r\nqt5-multimedia 5.14.1-1\r\nqt5-quickcontrols 5.14.1-1\r\nqt5-quickcontrols2 5.14.1-1\r\nqt5-script 5.14.1-1\r\nqt5-sensors 5.14.1-1\r\nqt5-svg 5.14.1-1\r\nqt5-tools 5.14.1-1\r\nqt5-wayland 5.14.1-1\r\nqt5-webchannel 5.14.1-1\r\nqt5-webengine 5.14.1-2\r\nqt5-webkit 5.212.0alpha4-1\r\nqt5-websockets 5.14.1-1\r\nqt5-x11extras 5.14.1-1\r\n```\r\n\r\nand here a minimal reproducer\r\n\r\n[TFQtReproducer.zip](https://github.com/tensorflow/tensorflow/files/4403925/TFQtReproducer.zip)\r\n\r\nplease compile it using:\r\n\r\n```\r\nqmake\r\nmake\r\n```\r\n\r\nif I try to run it I get this crash\r\n\r\n```\r\ngdb ./TFQtReproducer \r\nGNU gdb (GDB) 9.1\r\nCopyright (C) 2020 Free Software Foundation, Inc.\r\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\r\nThis is free software: you are free to change and redistribute it.\r\nThere is NO WARRANTY, to the extent permitted by law.\r\nType \"show copying\" and \"show warranty\" for details.\r\nThis GDB was configured as \"x86_64-pc-linux-gnu\".\r\nType \"show configuration\" for configuration details.\r\nFor bug reporting instructions, please see:\r\n<http://www.gnu.org/software/gdb/bugs/>.\r\nFind the GDB manual and other documentation resources online at:\r\n    <http://www.gnu.org/software/gdb/documentation/>.\r\n\r\nFor help, type \"help\".\r\nType \"apropos word\" to search for commands related to \"word\"...\r\nReading symbols from ./TFQtReproducer...\r\n(No debugging symbols found in ./TFQtReproducer)\r\n(gdb) run\r\nStarting program: /tmp/TFQtReproducer/TFQtReproducer \r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/usr/lib/libthread_db.so.1\".\r\nTensorFlow version: 2.1.0\r\n[New Thread 0x7fffdfb80700 (LWP 53899)]\r\n[New Thread 0x7fffdf262700 (LWP 53900)]\r\n\r\nThread 1 \"TFQtReproducer\" received signal SIGSEGV, Segmentation fault.\r\n0x00007fffeac3a7c4 in pthread_rwlock_wrlock () from /usr/lib/libpthread.so.0\r\n(gdb) bt\r\n#0  0x00007fffeac3a7c4 in pthread_rwlock_wrlock () from /usr/lib/libpthread.so.0\r\n#1  0x00007fffec362b79 in CRYPTO_STATIC_MUTEX_lock_write () from /usr/lib/libtensorflow_framework.so.2\r\n#2  0x00007fffec3250ea in CRYPTO_get_ex_new_index () from /usr/lib/libtensorflow_framework.so.2\r\n#3  0x00007fffeb2da1a3 in ?? () from /usr/lib/libQt5Network.so.5\r\n#4  0x00007fffeb2ca5ea in ?? () from /usr/lib/libQt5Network.so.5\r\n#5  0x00007fffeb2a8740 in ?? () from /usr/lib/libQt5Network.so.5\r\n#6  0x00007fffeb29b55f in QSslConfiguration::defaultConfiguration() () from /usr/lib/libQt5Network.so.5\r\n#7  0x00007fffeb1e1d89 in QNetworkRequest::sslConfiguration() const () from /usr/lib/libQt5Network.so.5\r\n#8  0x00007fffeb2414c3 in ?? () from /usr/lib/libQt5Network.so.5\r\n#9  0x00007fffeb1d16e3 in QNetworkAccessManager::createRequest(QNetworkAccessManager::Operation, QNetworkRequest const&, QIODevice*) ()\r\n   from /usr/lib/libQt5Network.so.5\r\n#10 0x00007fffeb1ce5a5 in QNetworkAccessManager::get(QNetworkRequest const&) () from /usr/lib/libQt5Network.so.5\r\n#11 0x00005555555553c6 in main ()\r\n```\r\n\r\nif I set the url to `http://www.google.it` instead of `https://www.google.it` no crash happen. It seems something really weird happen in `libtensorflow_framework.so.2` when Qt try to inizialize ssl configuration.", "I will try to prioritize this in the next quarter. Thank you for the reproduction instructions", "> I will try to prioritize this in the next quarter. Thank you for the reproduction instructions\r\n\r\nThanks, I can confirm that the problem does not happen using tensorflow 1.x (tested with 1.12)", "Hi again, \r\n\r\nI retested using 2.3.0-rc2 Arch Linux package, still the same issue but now Qt 5.15 give a better error message:\r\n\r\n```\r\nTensorFlow version: 2.3.0-rc2\r\nqt.network.ssl: QSslSocket: OpenSSL >= 1.1.1 is required; BoringSSL was found instead\r\n```\r\n\r\nso can I build tensorflow without boringssl? What are the dependencies that I need to disable at build time? S3? Others? Can this be documented somewhere? \r\n \r\nDo tensorflow support building against system openssl instead of boringssl?\r\n\r\nIn my app I need to link both system provided openssl and tensorflow, thanks", "This is fixed compiling with `--config=noaws --config=nogcp --config=nohdfs`", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34742\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34742\">No</a>\n"]}, {"number": 34741, "title": "NameError: name 'x_train' is not defined", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (GPU)\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version (use command below): 1.15.0\r\n- Python version: 3.6\r\n\r\n\r\n**Describe the current behavior**\r\nError when using `fit_generator()`:  \r\n`ModuleNotFoundError: No module named 'tensorflow_core.compat'`\r\n\r\n**Describe the expected behavior**\r\nThe model should begin training.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom google.colab import drive\r\ndrive.mount('/content/drive')\r\n\r\n!pip install --quiet tensorflow==2.0.0-rc0\r\n!pip install --quiet neural-structured-learning\r\n\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\n# TensorFlow and tf.keras\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import datasets, layers, models\r\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n\r\nprint(tf.__version__)\r\n\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport cv2\r\nfrom tqdm import tqdm\r\nimport pathlib\r\nimport random\r\n\r\ntrain_data_dir = \"/content/drive/My Drive/Resize/train\"\r\ntrain_label_dir = pathlib.Path(train_data_dir)\r\n\r\ntest_data_dir = \"/content/drive/My Drive/Resize/test\"\r\ntest_label_dir = pathlib.Path(test_data_dir)\r\n\r\n\r\n\r\nCATEGORIES = np.array([item.name for item in train_label_dir.glob('*') if item.name != \"LICENSE.txt\"])\r\nclass_names = CATEGORIES\r\nprint(CATEGORIES)\r\n\r\n\r\ndef createdataset(DATADIR, label_dir, CATEGORIES, img_size):\r\n    image_count = len(list(label_dir.glob('*/*.jpg')))\r\n    print(image_count)\r\n\r\n    # for category in CATEGORIES:  # do dogs and cats\r\n    #     path = os.path.join(DATADIR,category)  # create path to dogs and cats\r\n    #     for img in os.listdir(path):  # iterate over each image per dogs and cats\r\n    #         img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\r\n    #     #     break  # we just want one for now so break\r\n    #     # break  #...and one more!\r\n\r\n    IMG_SIZE = img_size\r\n\r\n    datalist = []\r\n\r\n    for category in CATEGORIES:  # do dogs and cats\r\n\r\n        path = os.path.join(DATADIR,category)  # create path to dogs and cats\r\n        class_num = np.where(CATEGORIES == category)\r\n\r\n        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\r\n            try:\r\n                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_COLOR)  # convert to array\r\n                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\r\n                datalist.append([new_array, class_num])  # add this to our training_data\r\n            except Exception as e:  # in the interest in keeping the output clean...\r\n                pass\r\n            #except OSError as e:\r\n            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\r\n            #except Exception as e:\r\n            #    print(\"general exception\", e, os.path.join(path,img))\r\n    return datalist\r\n\r\n\r\ntraining_dataset = createdataset(train_data_dir, train_label_dir, CATEGORIES, 200)\r\ntesting_dataset = createdataset(test_data_dir, test_label_dir, CATEGORIES, 200)\r\n\r\nprint(len(training_dataset))\r\nprint(len(testing_dataset))\r\n\r\n\r\ndef dataset(datasets):\r\n    xdata = []\r\n    ylabels = []\r\n    # random.shuffle(datasets)\r\n    for datas,labels in datasets:\r\n        xdata.append(datas)\r\n        ylabels.append(labels)\r\n    return xdata, ylabels\r\n\r\ntrain_images, train_labels = dataset(training_dataset)\r\ntest_images, test_labels = dataset(testing_dataset)\r\nprint(len(train_images))\r\nprint(len(train_labels))\r\nprint(len(test_images))\r\nprint(len(test_labels))\r\n\r\n\r\n\r\ntrain_images = np.array(train_images)\r\ntest_images = np.array(test_images)\r\ntrain_labels = np.array(train_labels)\r\ntest_labels = np.array(test_labels)\r\n\r\nprint(train_images.shape)\r\nprint(test_images.shape)\r\n\r\n# print(train_labels)\r\n# print(test_labels)\r\n\r\n\r\n\r\ntrain_images = train_images / 255.0\r\ntest_images = test_images / 255.0\r\n\r\n# plt.figure(figsize=(10,10))\r\n# for i in range(25):\r\n#     plt.subplot(5,5,i+1)\r\n#     plt.xticks([])\r\n#     plt.yticks([])\r\n#     plt.grid(False)\r\n#     plt.imshow(train_images[i], cmap=plt.cm.binary)\r\n#     plt.xlabel(class_names[train_labels[i]])\r\n# plt.show()\r\n\r\n\r\nepochs = 5\r\nbatch_size = 50\r\n\r\ndatagen = ImageDataGenerator(\r\n    featurewise_center=True,\r\n    featurewise_std_normalization=True,\r\n    rotation_range=20,\r\n    width_shift_range=0.2,\r\n    height_shift_range=0.2,\r\n    horizontal_flip=True)\r\n\r\nprint(\"Augmenting the shitty dataset\")\r\ndatagen.fit(train_images, augment=True, rounds=5)\r\n\r\n\r\n\r\n\r\n\r\nmodel = models.Sequential()\r\n\r\nmodel.add(layers.Conv2D(32, (3, 3), input_shape=(200,200,3)))\r\nmodel.add(layers.BatchNormalization(axis=-1))\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.Conv2D(32, (3, 3)))\r\nmodel.add(layers.BatchNormalization(axis=-1))\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(layers.Conv2D(64,(3, 3)))\r\nmodel.add(layers.BatchNormalization(axis=-1))\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.Conv2D(64, (3, 3)))\r\nmodel.add(layers.BatchNormalization(axis=-1))\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2,2)))\r\n\r\nmodel.add(layers.Flatten())\r\n\r\n# Fully connected layer\r\nmodel.add(layers.Dense(512))\r\nmodel.add(layers.BatchNormalization())\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.Dropout(0.2))\r\nmodel.add(layers.Dense(4))\r\n\r\nmodel.add(layers.Activation('softmax'))\r\n\r\n\r\nmodel.summary()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\r\n                    steps_per_epoch=len(x_train) / 32, epochs=epochs)\r\n\r\n\r\ntest_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\r\n\r\nprint('\\nTest accuracy:', test_acc)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nERROR:root:Internal Python error in the inspect module.\r\nBelow is the traceback from this internal error.\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-22-ce52d11f71b2>\", line 1, in <module>\r\n    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\r\nNameError: name 'x_train' is not defined\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\r\n    stb = value._render_traceback_()\r\nAttributeError: 'NameError' object has no attribute '_render_traceback_'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\r\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\r\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\r\n  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\r\n    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\r\n  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\r\n    filename = getsourcefile(frame) or getfile(frame)\r\n  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\r\n    if getattr(getmodule(object, filename), '__loader__', None) is not None:\r\n  File \"/usr/lib/python3.6/inspect.py\", line 733, in getmodule\r\n    if ismodule(module) and hasattr(module, '__file__'):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\r\nModuleNotFoundError: No module named 'tensorflow_core.compat'\r\n```", "comments": ["The real error is\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-22-ce52d11f71b2>\", line 1, in <module>\r\n    model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\r\nNameError: name 'x_train' is not defined\r\n```", "This points to some lines of code missing in your code sample. You should define values for `x_train` and `y_train`", "Great that worked.", "Hi, I am getting the same error can you tell me how you resolved it ? @jonathanrjpereira "]}, {"number": 34740, "title": "Autograph failure with tf.ragged.boolean_mask", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, providing source\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.1, most likely irrelevant.\r\n- TensorFlow installed from (source or binary): binary from pip\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d382ca 2.0.0\r\n- Python version: 3.7.2\r\n- CUDA/cuDNN version: using CPU only.\r\n\r\n**Describe the current behavior**\r\nI have a function computing a loss for a Keras model that produces multiple outputs of different sizes, and chooses one of them based on an index received as input from a tf.data.Dataset. Since the desired outputs have different sizes, they are stored in the dataset as ragged tensors and tf.ragged.boolean_mask is used to perform appropriate selections based on the index.\r\n\r\n**Describe the expected behavior**\r\nThe function works fine in eager mode, but raises the following exception when trying to apply @tf.function on it:\r\n\r\n> 2019-12-01 22:06:26.103694: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: Input to reshape is a tensor with 388 values, but the requested shape has 408\r\n> Traceback (most recent call last):\r\n>   File \"ragged_bug.py\", line 50, in <module>\r\n>     loss, grads = get_loss(model, inputs, outputs, index)\r\n>   File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 457, in __call__\r\n>     result = self._call(*args, **kwds)\r\n>   File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\", line 494, in _call\r\n>     results = self._stateful_fn(*args, **kwds)\r\n>   File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1823, in __call__\r\n>     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n>   File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1141, in _filtered_call\r\n>     self.captured_inputs)\r\n>   File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 1224, in _call_flat\r\n>     ctx, args, cancellation_manager=cancellation_manager)\r\n>   File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 511, in call\r\n>     ctx=ctx)\r\n>   File \".../lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\", line 67, in quick_execute\r\n>     six.raise_from(core._status_to_exception(e.code, message), None)\r\n>   File \"<string>\", line 3, in raise_from\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 388 values, but the requested shape has 408\r\n> \t [[node RaggedMask_1/RaggedMask/boolean_mask/Reshape (defined at .../lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_get_loss_1681]\r\n\r\nI can confirm that making all model output sizes the same removes the problem, suggesting an issue with the use of ragged tensors in this situation. Also, the actual exception values change on every run since they depend on the random data being generated.\r\n\r\n**Code to reproduce the issue**\r\n[ragged_bug.py.txt](https://github.com/tensorflow/tensorflow/files/3908709/ragged_bug.py.txt)\r\n\r\n", "comments": ["@leandro-gracia-gil \r\n\r\nCan you please try with recent nightly version(`!pip install tf-nightly==2.1.0dev20191201`). I am not seeing any issue with recent nightly version. Please, find the gist [here.](https://colab.sandbox.google.com/gist/ravikyram/6d90edaa50eeaefff46c676b020ca2a0/untitled434.ipynb)Thanks!", "@ravikyram \r\n\r\nThe recent nightly version does seem to fix the problem. Thanks!"]}, {"number": 34739, "title": "Embedding visualization in TensorFlow 2.0 not supported", "body": "This is the way I create a Tensorboard callback in the Keras:\r\n\r\n```\r\nfrom keras.callbacks import TensorBoard\r\ntbCallBack = TensorBoard(log_dir='./Graph', \r\n                         histogram_freq=1, \r\n                         embeddings_freq=1, \r\n                         embeddings_data=embedding_matrix, \r\n                         write_graph=True, \r\n                         write_images=True)\r\n```\r\n\r\nWhat I get is:\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/keras/callbacks/tensorboard_v2.py:102: UserWarning: The TensorBoard callback does not support embeddings display when using TensorFlow 2.0. Embeddings-related arguments are ignored.\r\n  warnings.warn('The TensorBoard callback does not support '\r\n```\r\n\r\nWhen I launched Tensorboard, `Projector` is under `Inactive`\r\n\r\n", "comments": ["Related PR for the issue #[33291](https://github.com/tensorflow/tensorflow/pull/33291).Thanks!", "@oanush  , I checked the linked issue but it seems open, not merged yet. How can I get a quick way of seeing the embeddings now?", "@DuyguA Closing as duplicate. \r\n#33230", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34739\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34739\">No</a>\n"]}, {"number": 34738, "title": "Delete init_op option", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.15.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI am using HiAI-DDK in order to deploy my model on mobile phone. The model transform failed with debug information that the _init_op_ is not supported. I think this should be due to the redundant part in the initialization procedure for trained model. So I wonder if there exists the feature that allows me to delete a node in a graph (especially tf_rep). \r\nI am using onnx-tf to do the transformation. \r\n\r\n**Will this change the current api? How?**\r\nAdd node deleting interface, (not sure)\r\n\r\n**Who will benefit with this feature?**\r\nEngineers working with model deployment.\r\n\r\n**Any Other info.**\r\n", "comments": ["@MARMOTatZJU, Could you post your sample code snippet for us to analyze the issue better?", "@MARMOTatZJU, Could you elaborate feature request with code snippet and if any use case. Thanks!  ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34737, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 294912 values, but the requested shape has 73728", "body": "I downloaded the precomputed image features from \r\n[https://github.com/kuanghuei/SCAN](url)\r\n\r\nand then tried to store them in a tfrecord file and trying to read them from that file but then i get this array. any help will be appreciated.\r\n", "comments": ["Please provide minimal code snippet to reproduce the reported behavior. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 34736, "title": "2.1.0rc0 doesn't find GPU libraries", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 1909\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.1.0rc0\r\n- Python version: 3.7.5\r\n- Installed using virtualenv? pip? conda?: pip/virtualenv\r\n- CUDA/cuDNN version: Cuda 10.0, cudnn v7.6.4\r\n- GPU model and memory: rtx 2060 6gb\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n2.1.0rc0 and nightly builds doesn't see any of the libraries below, and it defaults to use CPU. \r\n\r\n\r\n**Any other info / logs**\r\n\r\n\r\n2019-12-01 20:12:55.597639: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2019-12-01 20:12:55.601941: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\ntimestamp divided..\r\ntimestamp divided..\r\n(1112,)\r\n2019-12-01 20:12:59.141649: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2019-12-01 20:12:59.168743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2060 computeCapability: 7.5\r\ncoreClock: 1.695GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s\r\n2019-12-01 20:12:59.176962: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2019-12-01 20:12:59.181864: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cublas64_10.dll'; dlerror: cublas64_10.dll not found\r\n2019-12-01 20:12:59.186511: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2019-12-01 20:12:59.191583: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2019-12-01 20:12:59.197264: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found\r\n2019-12-01 20:12:59.202050: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cusparse64_10.dll'; dlerror: cusparse64_10.dll not found\r\n2019-12-01 20:12:59.216645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2019-12-01 20:12:59.219553: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing \r\nlibraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2019-12-01 20:12:59.232902: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not \r\ncompiled to use: AVX2\r\n2019-12-01 20:12:59.236973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-12-01 20:12:59.246142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]\r\n<RepeatDataset shapes: ((None, 720, 1), (None,)), types: (tf.float64, tf.float64)>\r\n(720, 1)\r\nTrain for 200 steps, validate for 50 steps\r\n", "comments": ["Closed by mistake", "Try using a video driver >= 418.96 and Cuda 10.1", "> Try using a video driver >= 418.96 and Cuda 10.1\r\n\r\nI thought it doesn't support Cuda 10.1, it doesn't mention on release notes.", "Please see https://github.com/tensorflow/tensorflow/pull/34494\r\nIndeed, the next RC should probably have this aspect mentioned in the release notes", "> Please see #34494\r\n\r\nOh thank you a lot that's great, I'm upgrading now. Closing it."]}, {"number": 34735, "title": "tensorflow-gpu 2.0 cannot be imported", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Ubuntu 18.04)\r\n- TensorFlow installed from (source or binary)\r\n- TensorFlow version: tensorflow-gpu 2.0.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: installed using pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0 / 7.6.4\r\n- GPU model and memory: GeForce GTX, driveri 410.104\r\n\r\ni have tensorflow-estimator 2.0 & tensorflow-gpu 2.0 installed, and it shows under 'pip list' and 'conda list'.\r\n\r\nbut import tensorflow is not working as attached picture;\r\n\r\n\r\n![capture](https://user-images.githubusercontent.com/46334448/69915721-9888b480-1495-11ea-8593-f93413cc9ccd.JPG)\r\n\r\nplease help..!! thank you all for your help in advance!", "comments": ["@songheej,\r\nBelow are the instructions to install Tensorflow 2.0 using PIP\r\n**Note:** Make sure CUDA and cuDNN path are set before installing Tenosrflow-gpu.\r\n#Install tensorflow using pip virtual env \r\n```\r\n$pip install virtualenv\r\n$virtualenv tf_2.0.0   # tf_2.0.0 is virtual env name\r\n$source tf_2.0.0/bin/activate\r\ntf_2.0.0 $ pip install tensorflow-gpu==2.0.0\r\ntf_2.0.0 $ python\r\n>>import tensorflow as tf\r\n>>tf.__version__\r\n2.0.0\r\n```\r\n\r\nLet us know how it progresses. Thanks!", "@songheej, Is this still an issue!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34735\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34735\">No</a>\n"]}]