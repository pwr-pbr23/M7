[{"number": 24719, "title": "Updates so that Demo app can build in current Android Studio.", "body": "This has been tested on OS X Android Studio 3.2.1.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "Corrected committer email.", "CLAs look good, thanks!\n\n<!-- ok -->", "@qu1j0t3  Could you please rebase to resolve the branch conflicts. Thanks !", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 24718, "title": "Update README.md", "body": "Removed typos and increased readability.", "comments": []}, {"number": 24716, "title": "Building Pip package keeps failing", "body": "**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device: Lenovo Z50 Laptop\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: i'm not sure\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: i don't have it installed i cloned the github repository though i ran through issues and been told to not have tensorflow installed before building pip packages\r\n- Bazel version (if compiling from source): 0.18.0\r\n- GCC/Compiler version: i don't know where to find that\r\n- CUDA/cuDNN version: 9.0/7.4.1\r\n- GPU model and memory: NVIDIA GEFORCE 850M\r\n\r\n\r\n\r\n**Describe the problem**\r\n  So After i cloned the tensorflow repository i typed python ./configure.py but then following the steps it didn't show \"configuration finished\" at the end and neither any errors, so i tried to build the pip packages with gpu support since my gpu supports cuda and that stuff but then it abored due to the issues below but before that it keeps spamming a certain line, and also where am i failing it's been 5 days of purely tensorflow installation i just wanna make things learn!!\r\n\r\n```\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded)\r\n    Fetching https://mirror.bazel.build/.../archive/7f634429a04abc48e2eb041c81c5235816c96514.tar.gz; 360,763b 13s\r\nFAILED: Build did NOT complete successfully (305 packages loaded)\r\n```", "comments": ["hey! so i've bypassed some issues but i'm still getting that spammed line with the fetching below it", "Is this still an issue for you? Can you please share the error message?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24715, "title": "Mobilenet transfer learning has a higher final test accuracy than Inception V3 and Inception ResNet v2", "body": "I tried and did some experimenting on running the [retrain.py](https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py) script for transfer learning. I did the transfer learning of images with 512px x 384px, of the same learning rate (used 0.01) and training steps (used 10000) using [Inception V3](https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1), [Mobilenet_v2_140_224](https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/2), and [Inception ResNet V2](https://tfhub.dev/google/imagenet/inception_resnet_v2/feature_vector/1). I found out that while and after retraining, Mobilenet_v2_140_224 has a higher accuracy compared to Inception V2 and Inception ResNet V3. Below are the final test accuracy of the retraining:\r\n\r\n- Mobilenet_v2_140_224: **0.97790337 (97.9%)**\r\n- Inception V3: **0.93268245 (93.3%)**\r\n- Inception ResNet V2: **0.9116136 (91.2%)**\r\n\r\nWhat could possibly be the cause of this? Thanks a lot.", "comments": ["Kinda overfitting probably. Mobilenet V2 has much fewer parameters than the other two, so this looks reasonable to me.", "Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. Github is mainly for addressing bugs in installation and performance. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@marcreyesph I have same question about myself dastaset, what's the reason?"]}, {"number": 24714, "title": "Tensor conversion requested dtype float32_ref for Tensor with dtype float32?", "body": "Hi:\r\nmy program have bug in these code line:\r\n     \r\n               word_embeddings = tf.scatter_nd_update(var_output, error_word_f, sum_all)\r\n                word_embeddings_2 = tf.nn.dropout(word_embeddings, self.dropout_pl)\r\n\r\nThe error hint as follows:\r\nValueError: Tensor conversion requested dtype float32_ref for Tensor with dtype float32: 'Tensor(\"dropout:0\", shape=(), dtype=float32)\r\n  ========================================\r\n   it looks  like word_embeddings 's dtype is float32_ref but actual the function tf.nn.dropout  need word_embeddings dtype float32  ,how can i convert word_embeddings's dtype float32_ref to float32 before run  tf.nn.dropout(word_embeddings, self.dropout_pl)?\r\n   \r\n", "comments": ["Have you tried using [tf.cast](https://www.tensorflow.org/api_docs/python/tf/dtypes/cast) function?\r\n", "Closing since this is now a duplicate of latest posted issue. Lets track it there."]}, {"number": 24713, "title": "Update flatbuffers to the most recent release version of 1.10.0", "body": "While working on flatbuffer bazel build https://github.com/google/flatbuffers/pull/5104, I noticed that the\r\nflatbuffers library included in tensorflow used to be a non-release\r\ncommit 1f5eae5d. A newer release version (1.10.0) has recently been\r\nreleased so this fix updates the flatbuffers library to 1.10.0.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@angersson  Could you PTAL and approve.", "@aselle @jdduke TF Lite is the primary user for flatbuffers in TF. Can we accept this safely?", "The change itself looks good, but we'll have to hold off on landing this until our internal flatbuffer references can also be updated to 1.10. We'll keep you posted, though unfortunately there's no ETA on that update.", "@yongtang could you please resolve the conflicts? ", "@gbaned Thanks. The PR has been updated.", "@yongtang could you please resolve the conflicts?", "We've since updated to flatbuffer 1.11, thanks anyway for the contribution!"]}, {"number": 24712, "title": "tf.nn.embedding_lookup return NaN, i think it is not correct!", "body": "here is my test case code:\r\n-----------------------------------start\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.enable_eager_execution()\r\na = np.zeros([2,4,2],dtype=float)\r\nout = tf.nn.embedding_lookup(a,[0],'div',None,-1)\r\n-------------------------------------end\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:3.5\r\n- Bazel version (if compiling from source):NA\r\n- GCC/Compiler version (if compiling from source):NA\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nit returns :\r\n[[[nan nan]\r\n   [nan nan]\r\n   [nan nan]\r\n   [nan nan]]]\r\n**Describe the expected behavior**\r\nwhen i study the source code in \r\ntensorflow/python/ops/embedding_ops.py\r\ntensorflow/tensorflow/python/ops/clip_ops.py:line148-152\r\ni think it should returns \r\n[[[0. 0.]\r\n   [0. 0.]\r\n   [0. 0.]\r\n   [0. 0.]]]\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nconfig = tf.ConfigProto(\r\n        device_count = {'GPU': 0}\r\n    )\r\ntf.enable_eager_execution(config=config)\r\na = np.zeros([2,4,2],dtype=float)\r\nout = tf.nn.embedding_lookup(a,[0],'div',None,-1)\r\n\r\nprint(tf.__version__)\r\nprint(out)\r\n```\r\n```\r\n1.11.0\r\ntf.Tensor(\r\n[[[0. 0.]\r\n  [0. 0.]\r\n  [0. 0.]\r\n  [0. 0.]]], shape=(1, 4, 2), dtype=float64)\r\n```\r\nGuess I cannot reproduce.", "@hanayashiki   please try again with tensorflow version=1.12.0", "@gitdavida I was able execute your code snippet successfully in TF 1.12.\r\n1.12.0\r\ntf.Tensor(\r\n[[[0. 0.]\r\n  [0. 0.]\r\n  [0. 0.]\r\n  [0. 0.]]], shape=(1, 4, 2), dtype=float64)\r\n\r\nCan you please confirm it by running your code using [google colab](https://colab.research.google.com/notebooks/welcome.ipynb)? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I came across the same problem. Still have no idea about it. My environment is tensorflow-gpu 1.13, cuda 10.0, ubuntu 16.04"]}, {"number": 24711, "title": "Additional release cherrypicks - cuda 10 docker image & nomac tag handling.", "body": "", "comments": []}, {"number": 24710, "title": "Upgrade ci docker images to cuda 10.", "body": "These really should be gone already...\r\n\r\nPiperOrigin-RevId: 227914258", "comments": []}, {"number": 24709, "title": "TFLite c++ code with TFLite binary gives error using BuildFromBuffer()", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac High Sierra 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): r.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): 0.21\r\n- GCC/Compiler version (if compiling from source): Apple LLVM version 9.1.0 (clang-902.0.39.2)\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA 2080TI\r\n\r\n**Describe the current behavior**\r\nHigh-Level: I'm trying to built a TFLite Interpreter in C++ running on a Pixel 2. I have successfully loaded my model.tflite binary using the BuildFromFile() API. However, I now want to read the model from the assets/ directory. To do this I believe I am supposed to use the  BuildFromBuffer() API.\r\n\r\nI'm using the BuildFromBuffer() API in model.h using a mmapped buffer and not getting a valid model back. Am I supposed to supply some other kind of buffer? See my code below. I get an error about a bad model identifier.\r\n\r\n**Describe the expected behavior**\r\nI expected a valid FlatBufferModel to be returned.\r\n\r\n**Code to reproduce the issue**\r\n`\r\n            Code that errors:\r\n\r\n            AAsset *asset = AAssetManager_open(mgr, tflite_path.c_str(), O_RDONLY);\r\n            off_t start = 0;\r\n            off_t length = AAsset_getLength(asset);\r\n            int mmap_fd_ = AAsset_openFileDescriptor(asset, &start, &length);\r\n            if (mmap_fd_ < 0) {\r\n                LOGE(\"Failed to open file: %s\", tflite_path.c_str());\r\n                AAsset_close(asset);\r\n                return false;\r\n            }\r\n            // MMAP TFLite Binary\r\n            struct stat sb;\r\n            fstat(mmap_fd_, &sb);\r\n            char *mmapped_buffer_ = (char *) mmap(nullptr, sb.st_size, PROT_READ, MAP_SHARED, mmap_fd_,\r\n                                                  0);\r\n            if (mmapped_buffer_ == MAP_FAILED) {\r\n                LOGE(\"Mmap failed on file: %s\", tflite_path.c_str());\r\n                AAsset_close(asset);\r\n                return false;\r\n            }\r\n\r\n            // Build the Model\r\n            using namespace tflite;\r\n            std::unique_ptr<FlatBufferModel> model = FlatBufferModel::BuildFromBuffer(mmapped_buffer_,\r\n                                                                                      strlen(mmapped_buffer_));\r\n            if (!model) {\r\n                LOGE(\"Failed to create FlatBufferModel with file: %s\", tflite_path.c_str()); // My code errors here.\r\n                AAsset_close(asset);\r\n                return false;\r\n            }`\r\n\r\n\r\n`\r\n            Code that runs successfully: \r\n\r\n            const char *external_path = env->GetStringUTFChars(jExternalPath, JNI_FALSE);\r\n            const char *relative_path = \"/Download/tflite/1052model.ckpt-303187.tflite\";\r\n            char tflite_path[strlen(external_path) + strlen(relative_path)];\r\n            strcpy(tflite_path, external_path);\r\n            strcat (tflite_path, relative_path);\r\n\r\n            std::unique_ptr<tflite::FlatBufferModel> model(\r\n                    tflite::FlatBufferModel::BuildFromFile(tflite_path));\r\n\r\n            if(!model){\r\n                printf(\"Failed to mmap model\\n\");\r\n                exit(0);\r\n            }\r\n\r\n            tflite::ops::builtin::BuiltinOpResolver resolver;\r\n            std::unique_ptr<tflite::Interpreter> interpreter;\r\n            if (tflite::InterpreterBuilder(*model, resolver)(&interpreter) == kTfLiteOk) {\r\n                interpreter->Invoke();\r\n                LOGD(\"TFLiteOk!\");\r\n            } else {\r\n                LOGD(\"TFLite -- NOT -- Ok!\");\r\n            }`\r\n**Other info / logs**\r\nError logs:\r\n19-01-04 12:38:14.953 25181-25181/com.google.sample.oboe.hearsight E/tflite: Model provided has model identifier '\u0014\r\n2019-01-04 12:38:14.953 25181-25181/com.google.sample.oboe.hearsight E/AUDIO-APP: Failed to create FlatBufferModel with file: 1052model.ckpt-303187.tflite", "comments": ["One hacky workaround is to read the *.tflite file from the assets directory and write it into a normal *.tflite file in getFilesDir(), as done here: https://github.com/google/oboe/blob/master/tests/UnitTestRunner/app/src/main/java/com/google/oboe/tests/unittestrunner/MainActivity.java\r\n\r\nThen you can use the buildFromFile() API again instead of trying to use buildFromBuffer().", "I'm glad that you found a workaround.", "@tofulawrence Can we expect that to work, or is there something blocking this feature fundamentally?", "I'll let @miaout17 reply.", "Hello,\r\n\r\nHad the same problem. \r\nFor debugging, I tried to wrote the buffer I read from the assets back to a file on the internal storage.\r\nHowever when writing the buffer directly, only one byte was written. That surprised me.\r\nSo I tried to copy the buffer byte-wise to a string prior to writing it to a file. \r\nAnd that worked! So I created a buffer and copied the data over, and built the FlatBuffer from it. \r\nLater I noticed that although my models were loaded correctly, they did not work properly.\r\nThe problem was that I deleted the buffer (that I temporarily created) after creating the interpreter.\r\nBut it seems FlatBuffers does not copy any data from it but rather works on the buffer itself. So the buffer has to exist alongside  the FlatBuffer model as long as you want to use it.\r\nFinally, I came up with this:\r\n\r\n```\r\n// Open the file from assets in BUFFER_MODE\r\n// Streaming might work too?\r\nAAsset* asset =\r\n            AAssetManager_open(assetManager, this->modelFile.c_str(), AASSET_MODE_BUFFER);\r\n\r\nif(asset == NULL)\r\n{\r\n    Logger::printfln(\"Asset is NULL, probably the asset file that shall\"\r\n                                                  \"be loaded was not packed into the apk.\");\r\n    return false;\r\n}\r\noff_t start;\r\noff_t length;\r\nconst int fd = AAsset_openFileDescriptor(asset, &start, &length);\r\n\r\n\r\noff_t  dataSize = AAsset_getLength(asset);\r\nconst void* const memory = AAsset_getBuffer(asset);\r\n\r\n// Use as const char*\r\nconst char* const memChar = (const char*) memory;\r\n\r\n// Create a new Buffer for the FlatBuffer with the size needed.\r\n// It has to exist alongside the FlatBuffer model as long as the model shall exist!\r\n// char* flatBuffersBuffer; (declared in the header file of the class in which I use this).\r\nthis->flatBuffersBuffer = new char[dataSize];\r\n\r\nLogger::printfln(\"Copying assets buffer to flatbuffers buffer, size: %d\", dataSize);\r\nfor(int i = 0; i < dataSize; i++)\r\n{\r\n\tthis->flatBuffersBuffer[i] = memChar[i];\r\n}\r\n\r\n// Build your tflite model from the buffer.\r\nthis->model = tflite::FlatBufferModel::BuildFromBuffer(this->flatBuffersBuffer, dataSize);\r\n\r\n// Make sure NOT to delete the flatBuffersBuffer now! If you would do it your models won't work!\r\n// As long as you want to use the FlatBuffer model, the flatBuffersBuffer has to exist!\r\n\r\nif (this->model)\r\n{\r\n    // And so on...\r\n}\r\n```\r\n\r\n\r\n\r\n", "You don't need to mmap() and call FlatBufferModel::BuildFromBuffer().\r\nYou can use simply FlatBufferModel::BuildFromFile() since it tried mmap() internally.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/model_builder.cc#L52\r\n\r\nFYI, you need to set noCompress option for \"tflite\" extension to use mmap() feature.\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/build.gradle#L21"]}, {"number": 24708, "title": "improve docs of depthwise conv", "body": "State that the documentation is for \"NHWC\" format.\r\nOtherwise, the documentation can be confusing: it confuses one user at https://github.com/tensorpack/tensorpack/issues/1029#issuecomment-451552581.", "comments": ["@av8ramit  could you please verify failing check once ?", "Unrelated. We are in the process of fixing them at HEAD."]}, {"number": 24707, "title": "Update dependency on rules_closure", "body": "This makes this command succeed:\r\n  `bazel build //tensorflow/tools/pip_package:build_pip_package --incompatible_disallow_data_transition`\r\n\r\nThis is needed for a future change in Bazel.", "comments": []}, {"number": 24706, "title": "LIBXSMM in TensorFlow broken due to TensorFlow API changes", "body": "* Register sparse matmul-op to use LIBXSMM's Bfloat16 functionality (functionality was there since ever but registration was lacking).\r\n* Updated conv-integration with LIBXSMM (there is much more possible: https://github.com/hfp/libxsmm/issues/281).\r\n* Generalized libxsmm.BUILD using glob/patterns.\r\n\r\nThis issue refers to https://github.com/hfp/libxsmm/issues/295", "comments": ["@jewillco do you mind taking a look at this PR? Thanks!", "Is there any action needed from my side to get this issue closed?", "@hfp I'm so sorry for our delay! I'll review this soon.", "> @hfp I'm so sorry for our delay! I'll review this soon.\r\n\r\nNo worries!\r\n\r\nI have updated the PR to the current TF/master. I also (temporarily) white-listed \"libxsmm_archive\" to allow a single URL for this dependency. When the PR is taken/squashed, then (1)&#160;the change in `third_party/repo.bzl` can/should be omitted, (2)&#160;LIBXSMM archive to be mirrored, and (3)&#160;`tensorflow/workspace.bzl` updated according to the mirror.", "@hfp can you please resolve conflicts.", "@hfp Thank you for letting us know! Please take your time. :) \r\nGood news: I don't think there's anything blocking the merge now.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes.", "Hi @hfp,\r\n\r\nBased on the comment in https://github.com/hfp/libxsmm/pull/355#issuecomment-544238088, I realized there might have been a misunderstanding.\r\n\r\nFirst off, I apologize again for our delay in reviewing your PR. We are grateful for your valuable time and it wasn't our intention to make you feel neglected. We realized our response time has been very slow for many PRs and we have been trying to improve that. \r\n\r\nSecondly, I wanted to clarify that the PR has been closed strictly because of TF's PR policy. We close PRs if they have been inactive for longer than a certain period of time, just to make the number of active PRs we are monitoring manageable. This doesn't mean that we are dismissing the closed PRs, the authors are encouraged to reopen the PR when they are ready to make adjustments again. I think this is where the misunderstanding happened. I just realized that the PR status 'Closed/Rejected' is not very descriptive and that we could have explained the reasoning of automatically closing PRs better. I'm so sorry if this has caused you unnecessary frustrations.\r\n\r\nI'm reopening the PR in case you'd still like to contribute. This PR was very close to being merged, it just needed minor formatting edits to conform with our C++ style guide. (We might need more changes now since TF code has also changed quite a bit in the past several months.) Please let me know if I can help with anything. ", "https://github.com/hfp/libxsmm/pull/355#issuecomment-546622659", "I can update the PR to current TF/master. I think format changes should be a separate task. Perhaps otherwise the diff will be dominated with unrelated changes (I haven't done anything in this PR to make the format worse or non-conforming).", "@hfp gentle ping to resolve conflicts.", "I have force-pushed an update. Currently, tests are running."]}, {"number": 24705, "title": "TensorFlow 2.0 installation issue on Windows (CPU)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (Home Edition)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: tf-nightly-2.0-preview\r\n- Python version: 3.6.x\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nOn a Windows 10 machine, anytime I try to install the TF 2.0 preview by:\r\n\r\n```pip install tf-nightly-2.0-preview```\r\n\r\nI obtain the following error message:\r\n\r\n**Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\xxx\\\\AppData\\\\Local\\\\Temp\\\\pip-install-3rfhl_nx\\\\tf-nightly-2.0-preview\\\\tf_nightly_2.0_preview-1.13.0.dev20181226.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'**\r\n\r\nI tried on three different machines with the same OS, fresh install of Anaconda, fresh environment with all possible 3.6.x versions of Python. I also tried to install directly by the whl file after downloading it from PyPy. Any ideas what I am doing wrong?", "comments": ["I forgot to mention that a similar error message arises here: #24694.", "I'm having the same exact issue on Windows. Any updates?", "@sscardapane and @danaugrs Could you follow this [solution](https://github.com/tensorflow/tensorflow/issues/24886) and let us know how it progresses? Thanks!", "@jvishnuvardhan Thanks! The Windows path length limit was the issue. I edited the Windows Registry following the instructions on [this page](https://www.howtogeek.com/266621/how-to-make-windows-10-accept-file-paths-over-260-characters/) to remove the limit.", "@sscardapane Can you follow solution provided by @danaugrs. Please let me know how it progresses or close the issue if you already resolved. Thanks @danaugrs. ", "Thanks, that was the issue.", "had a similar issue but on windows 7. after seeing the environment variable underlying issue, tried pip install --user ... which also worked for anyone else who encounters this."]}, {"number": 24704, "title": "Can't load JNI library and Python bindings in same process", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.11.0\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.20.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to load both the JNI library (libtensorflow_jni.so) and the Python bindings into the same process. I've built both from source via the //tensorflow/tools/pip_package:build_pip_package and //tensorflow/java:libtensorflow_jni targets. When I load the JNI library after importing tensorflow in a Python process, I get the following error:\r\n```python\r\n>>> import tensorflow as tf\r\n>>> import ctypes\r\n>>> ctypes.CDLL('tensorflow/java/libtensorflow_jni.so')\r\n2019-01-04 16:07:57.226191: F tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/cc/saved_model/load_attempt_count\r\nAborted\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n- Built TensorFlow JNI library and pip package from source\r\n- Installed whl file via pip\r\n- Ran the steps above\r\n\r\nMy guess is one or both of _pywrap_tensorflow.so and libtensorflow_jni.so are statically linking that initialization code. Are they intended to be usable in the same process? If not, is there a way to build both objects to have them both dynamically link against the offending section?", "comments": ["The two libraries aren't setup to be linked together - not that we specifically don't want them to, just wasn't something we were aiming for.\r\n\r\nI'd be curious to understand your use case here - why do you need to load both libraries? That said, the bottleneck here is that the operations and kernel registrations (the `REGISTER_OP` and `REGISTER_KERNEL` calls in source) are part of both libraries. You could perhaps try building one of them without this (e.g., removing the dependency on `//tensorflow/core:all_kernels` and `//tensorflow/core:ops` from https://github.com/tensorflow/tensorflow/blob/2703d8e84aced78a05ec7ad1ea0e5f4d0fe70f9f/tensorflow/java/src/main/native/BUILD#L43)", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "BTW, this is now possible with the Python-enabled builds of the JavaCPP Presets for TensorFlow.\r\nThere is an example of that there:\r\nhttps://github.com/bytedeco/javacpp-presets/blob/master/tensorflow/samples/pom.xml\r\nhttps://github.com/bytedeco/javacpp-presets/blob/master/tensorflow/samples/KerasMNIST.java"]}, {"number": 24703, "title": "Tensorflow Feature Column V2 test cases failing due to Out of Memory for CUDA", "body": "I am trying to run feature_column_v2_test script using  the bazel test command :\r\nbazel test tensorflow/python/feature_column:feature_column_v2_test\r\nI get an error of i.e out of memory for CUDA\r\n---------------------------------------------------------------------------------------------------\r\n```\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nE.2019-01-04 11:57:48.246499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1464] Ignoring visible gpu device 0 whose executor is in invalid state: Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 6373572608\r\n2019-01-04 11:57:48.248421: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 6373572608\r\n```\r\n-----------------------------------------------------------------------------------------------\r\n**My System information**\r\n- Build Tensorflow via Bazel no change from the original repository:\r\n- OS is Ubuntu 18.04\r\n-  Dell G7 15 GTX 1060 Nvidia Cuda 9.0:\r\n- TensorFlow installed from (source or binary):\r\n- 1.18 Tensorflow version:\r\n- Python version is 3.6.7:\r\n- Bazel version  0.19.2\r\n- CUDA/cuDNN version: 9/7\r\n- GPU model and memory: GTX 1060 6GB\r\n\r\nPls help\r\n\r\nRegards\r\nAmit", "comments": ["Hello @amitisgr8 , Please mention the correct TensorFlow version you are building from.\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)\r\nAlso if you are using the current version 1.12.0, please step back to bazel 0.15.0, and build again.", "Thanks for the reply @msymp , yes you are right my tensorflow version is 1.12.0 , but tensorflow does not allow to downgrade the bazel version below 0.19, I tried to by pass some checks to make it compile but in the end they are too many checks and i am not sure if this is the cause of the problem, i tried the same configuration on my server with 12GB GPU RAM instead of 6GB in my machine and to my surprise the same configuration worked on server, so this has to be tensorflow problem.\r\nI find it very strange that tensorflow basic test cases which uses GPU cannot be run on my laptop with the above configurations, any help in this regards is highly appreciated.\r\n\r\nRegards\r\nAmit", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "@amitisgr8 , as the error says please use the new FeatureColumn APIs in your code and try again:\r\nhttps://www.tensorflow.org/guide/feature_columns\r\nhttps://stackoverflow.com/questions/51965094/whats-the-difference-between-tf-feature-column-input-layer-and-tf-layers-input", "@msymp , this is just a warning, also i have not modified anything in the Testcases, the least you can expect is the test case to run without modification, Any help in this regard will be highly appriciated.", "Apparently there is no space in GPU, you would like to kill one process and re-run you code, or if you have more than one gpu, set the fresh gpu number you want to use.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24702, "title": "Errors of the form \"In rule 'core_test', size 'medium' is not a valid size.\" ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): MSVC - Visual Studio 2015 Update 3\r\n- CUDA/cuDNN version: Cuda Toolkit 9.0/cuDNN 7.4.2\r\n- GPU model and memory: Geforce 1070 GTX, 8 GB\r\n\r\nI am trying to build Tensorflow in Windows 10, from the source code. In order to that; I followed the instructions at https://www.tensorflow.org/install/source_windows . After I give the `bazel build` command, the building ends with errors of the type:\r\n\r\n`ERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:66:1: In rule 'core_test', size 'medium' is not a valid size.\r\nERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:66:1: In rule 'core_test', timeout 'illegal' is not a valid timeout.\r\nERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:168:1: In rule 'ops_test', size 'medium' is not a valid size.\r\nERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/labeled_tensor/BUILD:168:1: In rule 'ops_test', timeout 'illegal' is not a valid timeout.\r\nERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/python/debug/BUILD:620:1: In rule 'framework_test', size 'medium' is not a valid size.\r\nERROR: D:/tensorflowinstallation/tensorflow_build/tensorflow/tensorflow/contrib/eager/python/BUILD:37:1: In rule 'tfe_test', size 'medium' is not a valid size.`\r\n\r\n\r\nHere is a screenshot of the command prompt:\r\n![image](https://user-images.githubusercontent.com/17207330/50683013-b0f01500-1021-11e9-8bba-eed36813df21.png)\r\n\r\n\r\nI searched for this type of errors but I found no mention about them at all. I think I may be missing something very fundamental during the build preparation phase but I followed every step as mentioned in the above link. So, I am clueless what these errors are about and turned here for help.\r\n\r\n", "comments": ["Hello @ufukcbicici , Please step back to bazel 0.15.0 for the source version tensorflow_gpu-1.12.0 and build again, and let us know. Thanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Changing the system language solved my issue. Check the issue here https://github.com/tensorflow/tensorflow/issues/34833#issuecomment-611476283\r\n\r\n"]}, {"number": 24701, "title": "Cannot reproduce tf.contrib.autograph's RNN Colorbot example", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No (Used script provided in Tensorflow)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): r1.12\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nI tried to test some autograph's features with RNN.\r\nAt first, I tried tensorflow's official [example](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/autograph/examples/notebooks/rnn_keras_estimator.ipynb) with Python 3.6 Colab Notebook with tensorflow version as r1.12.0.\r\nHowever, it emits type error as\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    454                 preferred_dtype=default_dtype,\r\n--> 455                 as_ref=input_arg.is_ref)\r\n    456             if input_arg.number_attr and len(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1210             preferred_dtype=preferred_dtype,\r\n-> 1211             ctx=ctx))\r\n   1212   return ret\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1145     if ret is None:\r\n-> 1146       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1147 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    228   _ = as_ref\r\n--> 229   return constant(v, dtype=dtype, name=name)\r\n    230 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    207       tensor_util.make_tensor_proto(\r\n--> 208           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n    209   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)\r\n    441     else:\r\n--> 442       _AssertCompatible(values, dtype)\r\n    443       nparray = np.array(values, dtype=np_dt)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)\r\n    352       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\r\n--> 353                       (dtype.name, repr(mismatch), type(mismatch).__name__))\r\n    354 \r\n\r\nTypeError: Expected int64, got range(0, 64) of type 'range' instead.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-8-e180019d193a> in <module>()\r\n     14 regressor.train(\r\n     15     input_fn=lambda: input_fn(data_dir, train_url, params),\r\n---> 16     steps=100)\r\n     17 eval_results = regressor.evaluate(\r\n     18     input_fn=lambda: input_fn(data_dir, test_url, params, training=False),\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    352 \r\n    353       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 354       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    355       logging.info('Loss for final step: %s.', loss)\r\n    356       return self\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1205       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1206     else:\r\n-> 1207       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1208 \r\n   1209   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1235       worker_hooks.extend(input_hooks)\r\n   1236       estimator_spec = self._call_model_fn(\r\n-> 1237           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n   1238       global_step_tensor = training_util.get_global_step(g)\r\n   1239       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n   1193 \r\n   1194     logging.info('Calling model_fn.')\r\n-> 1195     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1196     logging.info('Done calling model_fn.')\r\n   1197 \r\n\r\n<ipython-input-6-eed6cb165519> in model_fn(features, labels, mode, params)\r\n     10 \r\n     11   if mode == tf.estimator.ModeKeys.TRAIN:\r\n---> 12     predictions = colorbot(inputs, training=True)\r\n     13     loss = loss_fn(labels, predictions)\r\n     14 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    755       if not in_deferred_mode:\r\n    756         self._in_call = True\r\n--> 757         outputs = self.call(inputs, *args, **kwargs)\r\n    758         self._in_call = False\r\n    759         if outputs is None:\r\n\r\n/tmp/tmpwq7vt5r_.py in call(self, inputs, training)\r\n     83       seq = self._rnn_layer(seq, self.upper_cell, batch_size, training)\r\n     84       indices = length - 1, ag__.range_(batch_size)\r\n---> 85       indices = tf.stack(indices, 1)\r\n     86       sequence_ends = tf.gather_nd(seq, indices)\r\n     87       return self.relu_layer(sequence_ends)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in stack(values, axis, name)\r\n    872                                                       expanded_num_dims))\r\n    873 \r\n--> 874   return gen_array_ops.pack(values, axis=axis, name=name)\r\n    875 \r\n    876 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in pack(values, axis, name)\r\n   4873     axis = _execute.make_int(axis, \"axis\")\r\n   4874     _, _, _op = _op_def_lib._apply_op_helper(\r\n-> 4875         \"Pack\", values=values, axis=axis, name=name)\r\n   4876     _result = _op.outputs[:]\r\n   4877     _inputs_flat = _op.inputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    481                                 (prefix, dtype.name))\r\n    482               else:\r\n--> 483                 raise TypeError(\"%s that don't all match.\" % prefix)\r\n    484             else:\r\n    485               raise TypeError(\"%s that are invalid.\" % prefix)\r\n\r\nTypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int64, <NOT CONVERTIBLE TO TENSOR>] that don't all match.\r\n```\r\n\r\nMoreover, if I try to run the original colab notebook example, which run tensorflow 1.13 dev in python2.7, it raises graph construction error as\r\n\r\n```\r\nGraphConstructionErrorTraceback (most recent call last)\r\n<ipython-input-15-e180019d193a> in <module>()\r\n     14 regressor.train(\r\n     15     input_fn=lambda: input_fn(data_dir, train_url, params),\r\n---> 16     steps=100)\r\n     17 eval_results = regressor.evaluate(\r\n     18     input_fn=lambda: input_fn(data_dir, test_url, params, training=False),\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    356 \r\n    357       saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 358       loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    359       logging.info('Loss for final step: %s.', loss)\r\n    360       return self\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\r\n   1122       return self._train_model_distributed(input_fn, hooks, saving_listeners)\r\n   1123     else:\r\n-> 1124       return self._train_model_default(input_fn, hooks, saving_listeners)\r\n   1125 \r\n   1126   def _train_model_default(self, input_fn, hooks, saving_listeners):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc in _train_model_default(self, input_fn, hooks, saving_listeners)\r\n   1152       worker_hooks.extend(input_hooks)\r\n   1153       estimator_spec = self._call_model_fn(\r\n-> 1154           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\r\n   1155       global_step_tensor = training_util.get_global_step(g)\r\n   1156       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc in _call_model_fn(self, features, labels, mode, config)\r\n   1110 \r\n   1111     logging.info('Calling model_fn.')\r\n-> 1112     model_fn_results = self._model_fn(features=features, **kwargs)\r\n   1113     logging.info('Done calling model_fn.')\r\n   1114 \r\n\r\n<ipython-input-13-eed6cb165519> in model_fn(features, labels, mode, params)\r\n     10 \r\n     11   if mode == tf.estimator.ModeKeys.TRAIN:\r\n---> 12     predictions = colorbot(inputs, training=True)\r\n     13     loss = loss_fn(labels, predictions)\r\n     14 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/base_layer.pyc in __call__(self, inputs, *args, **kwargs)\r\n    554           if not self.dynamic:\r\n    555             try:\r\n--> 556               outputs = self.call(inputs, *args, **kwargs)\r\n    557             except TypeError as e:\r\n    558               messages = ['`tf.Tensor` as a Python `bool` is not allowed',\r\n\r\n/tmp/tmpxLZSMS.py in call(self, inputs, training)\r\n    101         return ag__.converted_call('relu_layer', self, autograph.ConversionOptions(recursive=False, verbose=0, strip_decorators=(defun, autograph.convert, autograph.do_not_convert, autograph.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), sequence_ends)\r\n    102     except:\r\n--> 103       ag__.rewrite_graph_construction_error(ag_source_map__)\r\n    104 \r\n    105 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/autograph/core/errors.pyc in rewrite_graph_construction_error(source_map)\r\n    135     raise original_error\r\n    136   else:\r\n--> 137     raise new_error\r\n    138   finally:\r\n    139     # Addresses warning https://docs.python.org/2/library/sys.html#sys.exc_info.\r\n\r\nGraphConstructionError: Traceback (most recent call last):\r\n  File \"<ipython-input-12-87e5f229a162>\", line 59, in call\r\n    seq = self._rnn_layer(seq, self.lower_cell, batch_size, training)\r\n  File \"<ipython-input-12-87e5f229a162>\", line 27, in _rnn_layer\r\n    cell_output, (state, output) = cell.call(ch, (state, output))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 200, in converted_call\r\n    return f(*args, **kwargs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/rnn/python/ops/lstm_ops.py\", line 426, in call\r\n    wci = wcf = wco = array_ops.zeros([self._num_units], dtype=self.dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1793, in zeros\r\n    dtype = dtypes.as_dtype(dtype).base_dtype\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/dtypes.py\", line 719, in as_dtype\r\n    raise TypeError(\"Cannot convert value %r to a TensorFlow DType.\" % type_value)\r\n\r\nCannot convert value None to a TensorFlow DType.\r\n```\r\n\r\nIt says that it cannot convert value **None** to a Tensorflow DType, but I cannot find any None value if I print values related to erroneous call stack.\r\n\r\n**Describe the expected behavior**\r\n\r\nIn the original notebook example, it prints out evaluation loss without error\r\n\r\n**Code to reproduce the issue**\r\n[link](https://colab.research.google.com/drive/1fqr0wiNctLm0M07xss-Ra_iGC7DCojjs) to my python3 colab notebook", "comments": ["I found that for python 3.6 environment, autograph cannot convert range(batch_size) in \r\ncall function of RnnColorbot.\r\n\r\nIt runs without error if change range(batch_size) into tf.Tensor via \r\n```\r\ntf.convert_to_tensor(range(batch_size), tf.uint64)\r\n```\r\n\r\nHowever, still raise error in python 2.7, the original tutorial environment", "Thank you for bringing this to our attention! I believe we fixed the problems in the master version - if you have a chance to give it another try, please reopen this thread if you encounter any more issues."]}, {"number": 24700, "title": "Update api_def_RegexReplace.pbtxt", "body": "Documentation issue fixed", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 24699, "title": "Fix the issue of dense layer failure with initializer in 2.0", "body": "This fix tries to address the issue raised in #24573 where the dense layer fails with kernel_initializer=tf.keras.initializers.Zeros() in 2.0 mode.\r\n\r\nThe reason was that the initializer passed could be v1 or v2 depending on the mode of the build, but only v1 mode is assumed (which has different signature with v2). This fix fixes the issue.\r\n\r\nThis fix fixes #24573.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@martinwicke can you please verify test failures and also branch has some conflicts.", "@yongtang the function has changed quite significantly (in particular partition_info is gone), so maybe this change is not needed any more or needs to be updated?", "Thanks @martinwicke, I took a look and it seems that the bug was fixed separately in another commit a couple of days ago https://github.com/tensorflow/tensorflow/commit/2f7e56a65104aeca0a7fcc98a7b10fb2cff9b1a3\r\n\r\nI will close this PR, but thanks all for the help \ud83d\udc4d \r\n"]}, {"number": 24698, "title": "C++ include files when linking against headers with wrong type, no long int in code base, erroring about long int type.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes I have written code which takes an image from a OpenFX framework and passes it as input to a Tensorflow session in C++, runs the feed forward and returns the result to a pixel buffer\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux CentOS 6.10, using gcc/4.8.5 and bazel 0.11.0 with CUDA 9.1 and cuDNN 7.1.2\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nfrom source v1.6.0 zip here:\r\nhttps://github.com/tensorflow/tensorflow/archive/v1.6.0.zip\r\n- TensorFlow version (use command below):\r\nNA\r\n\r\n- Python version:\r\n2.7.13 NA\r\n- Bazel version (if compiling from source):\r\n0.11.0\r\n- GCC/Compiler version (if compiling from source):\r\ngcc 4.8.5\r\n- CUDA/cuDNN version:\r\nCUDA 9.1 with 3 updates, cuDNN 7.1.3\r\n- GPU model and memory:\r\nGTX 1060 6Gb\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n**Describe the current behavior**\r\nthe code will no longer compile when I moved to a machine with a GPU \r\n```\r\n\r\n  CXX      Linux-64-debug/rotobot.o\r\nrotobot.cpp: In function \u2018void drawMasks(OpenImageIO::v1_6::ImageBuf&, std::unique_ptr<tensorflow::Session>&, OpenImageIO::v1_6::ImageBuf&, std::string&, bool, const bool*, const bool*, const bool*, double, double, double, double, double, double, bool)\u2019:\r\nrotobot.cpp:435:68: error: \u2018inputTensor2\u2019 was not declared in this scope\r\n  tensorflow::Status run_status = tfSession->Run({ { \"input_image\", inputTensor2 },{ \"input_image_meta\", inputMetadataTensor } },\r\n                                                                    ^\r\nrotobot.cpp:439:11: error: no matching function for call to \u2018tensorflow::Session::Run(<brace-enclosed initializer list>, <brace-enclosed initializer list>, <brace-enclosed initializer list>, std::vector<tensorflow::Tensor>*)\u2019\r\n   &outputs);\r\n           ^\r\nrotobot.cpp:439:11: note: candidates are:\r\nIn file included from rotobot.cpp:32:0:\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:121:18: note: virtual tensorflow::Status tensorflow::Session::Run(const std::vector<std::pair<std::basic_string<char>, tensorflow::Tensor> >&, const std::vector<std::basic_string<char> >&, const std::vector<std::basic_string<char> >&, std::vector<tensorflow::Tensor>*)\r\n   virtual Status Run(const std::vector<std::pair<string, Tensor> >& inputs,\r\n                  ^\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:121:18: note:   no known conversion for argument 1 from \u2018<brace-enclosed initializer list>\u2019 to \u2018const std::vector<std::pair<std::basic_string<char>, tensorflow::Tensor> >&\u2019\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:150:18: note: virtual tensorflow::Status tensorflow::Session::Run(const tensorflow::RunOptions&, const std::vector<std::pair<std::basic_string<char>, tensorflow::Tensor> >&, const std::vector<std::basic_string<char> >&, const std::vector<std::basic_string<char> >&, std::vector<tensorflow::Tensor>*, tensorflow::RunMetadata*)\r\n   virtual Status Run(const RunOptions& run_options,\r\n                  ^\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:150:18: note:   candidate expects 6 arguments, 4 provided\r\nIn file included from /home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/tensor.h:23:0,\r\n                 from /home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:24,\r\n                 from rotobot.cpp:32:\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/types.h: In instantiation of \u2018struct tensorflow::DataTypeToEnum<long int>\u2019:\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/tensor.h:566:46:   required from \u2018typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::tensor() [with T = long int; long unsigned int NDIMS = 3ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<long int, 3, 1, long int>, 16, Eigen::MakePointer>]\u2019\r\nrotobot.cpp:1743:53:   required from here\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/types.h:356:3: error: static assertion failed: Specified Data Type not supported\r\n   static_assert(IsValidDataType<T>::value, \"Specified Data Type not supported\");\r\n   ^\r\nIn file included from /home/sam/dev/tensorflow-1.6.0/tensorflow/core/public/session.h:24:0,\r\n                 from rotobot.cpp:32:\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/tensor.h: In instantiation of \u2018typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::tensor() [with T = long int; long unsigned int NDIMS = 3ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<long int, 3, 1, long int>, 16, Eigen::MakePointer>]\u2019:\r\nrotobot.cpp:1743:53:   required from here\r\n/home/sam/dev/tensorflow-1.6.0/tensorflow/core/framework/tensor.h:566:46: error: \u2018v\u2019 is not a member of \u2018tensorflow::DataTypeToEnum<long int>\u2019\r\n\r\n```\r\n**Describe the expected behavior**\r\nThe code was compiling previously\r\n\r\nThe previous build environment was v1.6.0.2-gcbc6580\r\n\r\nBut was built without CUDA options.\r\n\r\nI am linking against \r\n`\r\n-ltensorflow -ltensorflow_cc and -ltensorflow_framework -lnsync`\r\n\r\nand a few more\r\n\r\nbut I am not compiling yet.\r\n\r\nlines 31,32,33 respectively are:\r\n\r\n#include <tensorflow/core/platform/init_main.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/framework/tensor_shape.h>\r\n\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nlines 31,32,33 respectively are:\r\n\r\n#include <tensorflow/core/platform/init_main.h>\r\n#include <tensorflow/core/public/session.h>\r\n#include <tensorflow/core/framework/tensor_shape.h>\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nsee:\r\nhttps://stackoverflow.com/questions/54033100/tensorflow-1-6-0-with-cuda-support-on-centos-6-10-c-linking-against-libtensorf", "comments": ["I have an idea now to recompile on the new machine with identical SHA hash from the github revision without CUDA support and see if I still get the same error.", "OK I can compile some old code but not the identical source as is compiling in the otherwise identical virtual machine which is CentOS 6.9\r\nIs there a way to run that virtual machine with access to the GPU, the end goal is to come up with some libraries that are glibc 2.12 compliant.", "Hello @samhodge , Your query is very systems related.\r\n\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nThanks!\r\n", "turns out it was a duplicate of https://github.com/tensorflow/tensorflow/issues/20586 but I assumed it was the system, so after installing CentOS 7 CUDA 9.2 and cuDNN 7.3.1 I was able to reproduce the error and then found it was in my code. For the record long int is not supported changing the template from int64_t to int64 made the compiler error go away. Still working on testing the software with GPU acceleration."]}, {"number": 24697, "title": "Fix typos", "body": "the the -> the\r\nthat that -> that", "comments": []}, {"number": 24696, "title": "Fix build failure with v2 config", "body": "While trying to build tensorflow with v2 config:\r\n```\r\nbazel build -s --verbose_failures --config=opt --config=v2 \\\r\n//tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nThe following failure was encountered:\r\n```\r\nERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:6800:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:summary_kernels':\r\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/summary_kernels.cc':\r\n  'tensorflow/contrib/tensorboard/db/schema.h'\r\n  'tensorflow/contrib/tensorboard/db/summary_db_writer.h'\r\n  'tensorflow/core/kernels/summary_interface.h'\r\n  'tensorflow/contrib/tensorboard/db/summary_file_writer.h'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\n\r\nThe issue was that summary_kernels.cc still depends on tensorflow/contrib/tensorboard\r\nno matter if v1 or v2 is selected.\r\n\r\nNOTE: The `tf.summary.create_file_writer` that is exposed to `v2` calls `create_summary_file_writer` in `tensorflow/contrib/tensorboard` anyway.\r\n\r\nThis fix fixes the issue so that it is possible to build tensorflow with v2 config.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@martinwicke  The `tf.summary.create_db_writer` is not exposed in 2.0 so removing `tf.summary.create_db_writer` could drop the dependency of `\"//tensorflow/contrib/tensorboard/db:summary_db_writer\"`.\r\n\r\nHowever, `tf.summary.create_file_writer` is exposed in 2.0, and eventually calls `SummaryFileWriter` in `tensorflow/contrib/tensorboard`. So a dependency to `\"//tensorflow/contrib/tensorboard\"` will always be there.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/735d26a821d9f69f58ad12ea841da9bc8c5a65ee/tensorflow/python/ops/summary_ops_v2.py#L237-L238\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/735d26a821d9f69f58ad12ea841da9bc8c5a65ee/tensorflow/core/kernels/summary_kernels.cc#L52\r\n", "@martinwicke I am wondering if `tf.summary.create_file_writer` and `tf.db.summary.create_file_writer` should be dropped from 2.0 completely? Then the dependency could be removed.\r\n\r\nI can update the PR if this is the case.\r\n", "@nfelt WDYT", "I'm working on moving the contrib/tensorboard files into core today, actually.  Once that is done this won't be a problem.", "@yongtang Has https://github.com/tensorflow/tensorflow/commit/021a8fe8a70b880031958f596b6c96e819e14386 solved the issue? If so, can we close the PR? If not, let's continue the discussion.", "@penpornk @nfelt @martinwicke I tried locally with the last master, and is able to build successfully with v2 config. The issue has been resolved. Thanks all for the help! \ud83c\udf89 "]}, {"number": 24695, "title": "Incompatible checkpoint being restored on estimator with keras model", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Archlinux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.12.0\r\n- Python version: 3.7.1\r\n- CUDA/cuDNN version: 10.0.130-2/7.4.1.5-2\r\n- GPU model and memory: 11GB GTX 1080 Ti\r\n\r\n**Describe the current behavior**\r\n\r\nI am running a Keras Model within a custom estimator and giving the size of a hidden layer as a parameter.\r\nAfter each run with the estimator, a checkpoint is stored, saving the weights of that run. \r\n\r\nHowever, if the architecture of the model is changed between runs, instead of receiving an error due to incompatible checkpoints, tensorflow is loading the old checkpoint into the new model even though new dimension is being considered.\r\n\r\n**Describe the expected behavior**\r\n\r\nAccording to the [guide](https://www.tensorflow.org/guide/checkpoints#avoiding_a_bad_restoration), \"restoring a model's state from a checkpoint only works if the model and checkpoint are compatible\" and it is given an example of how the described situation (changes in the model architecture) should generate an `InvalidArgumentError`.\r\n\r\nHowever no error is raised, being unclear the behaviour - are the new weights (in case of increased size) initialised and trained? Are they simply ignored?\r\n\r\n**Code to reproduce the issue**\r\n\r\nBelow is a minimum working example to reproduce the problem:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras import layers\r\n\r\n\r\ndef main(argv):\r\n    steps = 1000\r\n    # Generate random data\r\n    train_x = np.random.randn(100, 4).astype('float32')\r\n    train_y = np.random.randint(0, 3, 100).astype('int32')\r\n\r\n    # First estimator, with *10* hidden units\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        model_dir=\"tmp\",\r\n        params={'hidden': 10}\r\n    )\r\n\r\n    # train first model and store checkpoints on \"tmp\"\r\n    classifier.train(\r\n        input_fn=lambda: tf.data.Dataset.from_tensors((train_x, train_y)),\r\n        steps=steps)\r\n\r\n    # second estimator, with *20* hidden units and same model_dir output\r\n    # BUG: should break as model_dir is the same as previous estimator!!!\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=model_fn,\r\n        model_dir=\"tmp\",\r\n        params={'hidden': 20}\r\n    )\r\n\r\n    classifier.train(\r\n        input_fn=lambda: tf.data.Dataset.from_tensors((train_x, train_y)),\r\n        steps=steps)\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n\r\n    # create model with params[hidden] units on hidden layer\r\n    model = tf.keras.Sequential([layers.Dense(params[\"hidden\"], activation='relu'),\r\n                                 layers.Dense(3)])\r\n    logits = model(features)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.app.run(main)\r\n```\r\n\r\n\r\n\r\n**Other info / logs**\r\nLog output (abbreviated)\r\n\r\n~~~\r\n# RUN 1\r\n...\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 0 into tmp/model.ckpt.\r\nINFO:tensorflow:loss = 1.1404053, step = 0\r\nINFO:tensorflow:Saving checkpoints for 1 into tmp/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.1404053.\r\n...\r\n# RUN 2\r\nINFO:tensorflow:Calling model_fn.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Graph was finalized.\r\n\r\nINFO:tensorflow:Restoring parameters from tmp/model.ckpt-1\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:Saving checkpoints for 1 into tmp/model.ckpt.\r\nINFO:tensorflow:loss = 1.112431, step = 1\r\nINFO:tensorflow:Saving checkpoints for 2 into tmp/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.112431.\r\n~~~\r\n", "comments": ["Removing comp:gpu since this does not look like a GPU specific issue.  Please add it back if you disagree.", "Just an educated guess, but Keras Layers are numbered globally within the Python process. So if you re-create the Model you'll get totally new variable names each time. Then there's no conflict: it just looks like there are a bunch of new variables and the old ones don't match. Sad but a known incompatibility between Keras and name-based variable matching.\r\n\r\nI'm somewhat more confused as to why Estimator isn't complaining or at least warning about variables that aren't in the checkpoint (given that it is restoring a checkpoint). Is that expected behavior? @k-w-w @karmel ", "All the checkpoint values are being loaded, but as a result the variable shapes are changed. \r\nIt appears to be an unintentional bug from tf.train.Saver, not just a quirk of Estimator or Keras. The restore op doesn't do any shape checking. We should add a warning to saver.restore if any of the variables are reshaped.", "Assignment ops definitely do shape checking, though. Restores into variables with different shapes do fail.", "tf.train.Saver uses RestoreV2/RestoreTensorsV2 instead of assign ops. Is this a bug with the restore ops?", "RestoreV2 just returns a Tensor with the value of the variable. Then that feeds into an assignment here: https://github.com/tensorflow/tensorflow/blob/55e68264ba670301fb861aa4d28f9642b251c6fc/tensorflow/python/training/saving/saveable_object_util.py#L113\r\n\r\nThere is a reshape, but in this case there'd be a different number of elements, so even if restored_shapes wasn't None (I think it is) that'd be an error.", "I ran this and you're right, the restored_shapes is None. \r\nLooks like it is being set to None here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L336\r\n\r\nSo fixing this would just require Estimator using savers with reshape=True.", "I don't think we want to reshape. shape_safe_assign_variable_handle does a shape check, so if we passed a restored_shapes argument we'd be doing less checking than we are now (exact shape match vs. only checking the number of elements). I've seen that error come up a bunch when refactoring models...", "Ah, I see. The shapes that come from RestoreV2 are completely undefined (None) so they're compatible with the variable's shape when in a tf.Graph().\r\n\r\nGiven that we always run RestoreV2 eagerly in 2.x except for Estimator, we should probably just make whichever tweak fixes this for Estimator. Maybe you're right that reshape=True is an easy workaround.", "@allenlavoie Do any of the TF releases include a fix for this issue?", "No, and AFAIK nobody is working on it. Since it only affects Estimator (in 2.x), maybe @karmel has someone who could take a look. I have a queue of other things that look like higher priorities at the moment.\r\n\r\nMost of the work is going to be working to migrate existing code which breaks with new shape checking (or maybe we get lucky and it's fine). As @k-w-w mentioned, the Saver flag change itself is a pretty simple fix to try.", "@allenlavoie Thanks", "@kurka,\r\nDocumentation of [Estimators](https://www.tensorflow.org/guide/estimator#whats_supported_now) state:\r\n\r\n> There is limited support for training with Estimator using all strategies except TPUStrategy. Basic training and evaluation should work, but a number of advanced features such as v1.train.Scaffold do not. There may also be a number of bugs in this integration and there are no plans to actively improve this support (the focus is on Keras and custom training loop support).\r\n\r\nCan you please confirm if this issue is still relevant, as `Estimators` are not actively supported in `Tensorflow 2.x`? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24695\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/24695\">No</a>\n"]}, {"number": 24694, "title": "Tensorflow-gpu installation issue ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: idk\r\n- Python version: Python 3.6.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: CUDA v.10.0/cuDNN 7.4.0\r\n- GPU model and memory: GeForce 940MX\r\n\r\n\r\nI first installed tensorflow-gpu using the command \"pip install tensorflow-gpu\"\r\nhowever when I tested it using the commands:\r\n>>>import tensorflow as tf\r\n >>>print(tf.__version__)\r\n\"Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\"\r\n\r\nSo, I checked \r\n>>> dir(tf)\r\n['__doc__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\r\n\r\nPrior to my attempt to install and use tensorflow-gpu, I was using tensorflow supported using cpu and it was okay. \r\n\r\nI tried uninstalling it using the command \"pip uninstall tensorflow-gpu\" but I cannot do so due to error:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-ba9tnm4x\\\\users\\\\lenovo\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorsyclconverttodeviceexpression.h' \r\n\r\nResult of uninstallation is:\r\nUninstalling tensorflow-gpu-1.12.0:\r\n  Would remove:\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclconverttodeviceexpression.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclexprconstructor.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclextractaccessor.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclextractfunctors.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclfunctors.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclleafcount.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclplaceholderexpr.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsyclrun.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorsycltuple.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensortrace.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensortraits.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensoruint128.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensor\\tensorvolumepatch.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensorsymmetry\\dynamicsymmetry.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensorsymmetry\\staticsymmetry.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensorsymmetry\\symmetry.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\tensorsymmetry\\util\\templategrouptheory.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\eventcount.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\nonblockingthreadpool.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\runqueue.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\simplethreadpool.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\threadcancel.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\threadenvironment.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\threadlocal.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\threadpoolinterface.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\threadpool\\threadyield.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\cxx11meta.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\cxx11workarounds.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\emulatearray.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\emulatecxx11meta.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\src\\util\\maxsizevector.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\tensor\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\tensorsymmetry\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\cxx11\\threadpool\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\fft\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\kroneckerproduct\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\matrixfunctions\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\specialfunctions\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\fft\\ei_fftw_impl.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\fft\\ei_kissfft_impl.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\kroneckerproduct\\kroneckertensorproduct.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\matrixfunctions\\matrixexponential.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\matrixfunctions\\matrixfunction.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\matrixfunctions\\matrixlogarithm.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\matrixfunctions\\matrixpower.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\matrixfunctions\\matrixsquareroot.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\matrixfunctions\\stemfunction.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\specialfunctions\\arch\\cuda\\cudaspecialfunctions.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\specialfunctions\\specialfunctionsarrayapi.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\specialfunctions\\specialfunctionsfunctors.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\specialfunctions\\specialfunctionshalf.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\specialfunctions\\specialfunctionsimpl.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\include\\external\\eigen_archive\\unsupported\\eigen\\src\\specialfunctions\\specialfunctionspacketmath.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\blas.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_activation.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_blas.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_diagnostics.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_dnn.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_driver.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_event.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_fft.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_gpu_executor.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_helpers.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_kernel.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_platform.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_platform_id.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_rng.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_stream.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cuda_timer.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\cuda\\cudnn_version.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\device_description.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\device_memory.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\device_options.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\dnn.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\dso_loader.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\event.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\executor_cache.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\fft.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\gpu_launch_dim.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\host\\host_gpu_executor.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\host\\host_platform.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\host\\host_platform_id.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\host\\host_stream.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\host\\host_timer.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\host_buffer.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\host_or_device_scalar.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\kernel.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\kernel_cache_config.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\kernel_spec.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\launch_dim.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\array_slice.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\casts.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\demangle.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\env.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\error.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\human_readable.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\initialize.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\inlined_vector.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\mathutil.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\notification.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\numbers.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\path.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\process_state.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\ptr_util.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\stacktrace.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\static_threadlocal.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\status.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\status_macros.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\statusor.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\statusor_internals.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\str_util.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\strcat.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\stringpiece.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\stringprintf.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\thread_options.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\lib\\threadpool.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\module_spec.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\multi_platform_manager.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\platform.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\platform\\default\\mutex.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\platform\\logging.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\platform\\mutex.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\platform\\port.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\platform\\thread_annotations.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\plugin.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\plugin_registry.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\rng.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\scratch_allocator.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\shared_memory_config.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\stream.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\stream_executor.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\stream_executor_internal.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\stream_executor_pimpl.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\temporary_device_memory.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\temporary_memory_manager.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\timer.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\tensorflow\\stream_executor\\trace_listener.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\eigen\\cholesky\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\eigen\\core\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\eigen\\eigenvalues\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\eigen\\lu\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\eigen\\qr\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\eigen\\svd\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\license\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\fixedpoint\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\fixedpointtypes.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\matmatproduct.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\matmatproductavx2.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\matmatproductneon.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\matvecproduct.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\packetmathavx2.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\packetmathavx512.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\typecastingavx2.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\src\\fixedpoint\\typecastingavx512.h\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\tensor\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\cxx11\\threadpool\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\matrixfunctions\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\include\\third_party\\eigen3\\unsupported\\eigen\\specialfunctions\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\*\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\tools\\*\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_gpu-1.12.0.dist-info\\*\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\scripts\\freeze_graph.exe\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\scripts\\saved_model_cli.exe\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\scripts\\tensorboard.exe\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\scripts\\tflite_convert.exe\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\scripts\\toco.exe\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\scripts\\toco_from_protos.exe\r\n  Would not remove (might be manually added):\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\notebook.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session_benchmark.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\debug\\examples\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\debug\\examples\\debug_errors.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\debug\\examples\\debug_fibonacci.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\debug\\examples\\debug_mnist.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\debug\\examples\\debug_tflearn_iris.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\debug\\ops\\gen_debug_ops.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\graph_callable.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\dnn_testing_utils.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\estimator\\canned\\linear_testing_utils.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\grappler\\controller.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\grappler\\cost_analyzer.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\grappler\\cost_analyzer_tool.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\grappler\\graph_placer.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\grappler\\hierarchical_controller.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\grappler\\model_analyzer.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\kernel_tests\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\kernel_tests\\boosted_trees\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\kernel_tests\\distributions\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\kernel_tests\\linalg\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\kernel_tests\\random\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\kernel_tests\\reduction_ops_test_big.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\kernel_tests\\testdata\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\lib\\core\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\accumulate_n_benchmark.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\batch_norm_benchmark.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\concat_benchmark.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\conv2d_benchmark.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\matmul_benchmark.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\split_benchmark.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\transpose_benchmark.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\profiler\\internal\\model_analyzer_testlib.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\profiler\\pprof_profiler.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.lib\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\data_structures_base.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\util\\protobuf\\compare_test_pb2.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\tools\\api\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\tools\\api\\generator\\__init__.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\tools\\api\\generator\\create_python_api.py\r\n    c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\tools\\api\\generator\\doc_srcs.py\r\nProceed (y/n)? y\r\nException:\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\shutil.py\", line 544, in move\r\n    os.rename(src, real_dst)\r\nFileNotFoundError: [WinError 3] The system cannot find the path specified: 'c:\\\\users\\\\lenovo\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorsyclconverttodeviceexpression.h' -> 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-ba9tnm4x\\\\users\\\\lenovo\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorsyclconverttodeviceexpression.h'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pip\\_internal\\basecommand.py\", line 228, in main\r\n    status = self.run(options, args)\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pip\\_internal\\commands\\uninstall.py\", line 68, in run\r\n    auto_confirm=options.yes, verbose=self.verbosity > 0,\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pip\\_internal\\req\\req_install.py\", line 661, in uninstall\r\n    uninstalled_pathset.remove(auto_confirm, verbose)\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pip\\_internal\\req\\req_uninstall.py\", line 219, in remove\r\n    renames(path, new_path)\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pip\\_internal\\utils\\misc.py\", line 273, in renames\r\n    shutil.move(old, new)\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\shutil.py\", line 558, in move\r\n    copy_function(src, real_dst)\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\shutil.py\", line 257, in copy2\r\n    copyfile(src, dst, follow_symlinks=follow_symlinks)\r\n  File \"c:\\users\\lenovo\\appdata\\local\\programs\\python\\python36\\lib\\shutil.py\", line 121, in copyfile\r\n    with open(dst, 'wb') as fdst:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-ba9tnm4x\\\\users\\\\lenovo\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\tensorflow\\\\include\\\\tensorflow\\\\include\\\\external\\\\eigen_archive\\\\unsupported\\\\eigen\\\\cxx11\\\\src\\\\tensor\\\\tensorsyclconverttodeviceexpression.h'  \r\n", "comments": ["Alternatively, when I rerun it, sometimes this happens\r\n\r\n>>>import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\n\r\nI really need all the help I can get. Thanks in advance", "Hello @MariAmon , can you please provide us the TensorFlow Windows 10 binary version you are installing.\r\n[New Issue Template](https://github.com/tensorflow/tensorflow/issues/new)\r\nThanks.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Not anymore. I was able to resolve this by reinstalling everything, including the cuda and cudnn. Thanks"]}, {"number": 24693, "title": "Fix incorrect documentation in batch_normalization", "body": "This fix fixes the incorrect in batch_normalization\r\n`output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\r\n->\r\n`output = (x - mean) / sqrt(var + epsilon) * gamma + beta`\r\n\r\nThis fix fixes #24690.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["The original doc string is actually correct see: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L1106\r\n\r\nThe `batch_normalization` function from TensorFlow nn_impl.py is used under the hood by the tf.keras backend.", "I don't think that's right;  in the code you're pointing to, the +epsilon is inside the rsqrt, which doesn't match the original docstring.\r\n\r\nAlso, if you execute it, the behavior doesn't match the original docstring either:\r\n````\r\nIn [15]: import tensorflow.keras.backend as K                                                                                                                                                \r\nIn [16]: K.eval(K.batch_normalization( \r\n    ...:     x=K.constant(1.0), \r\n    ...:     mean=K.constant(0.0), \r\n    ...:     var=K.constant(0.0), \r\n    ...:     beta=K.constant(0.0), \r\n    ...:     gamma=K.constant(1.0), \r\n    ...:     epsilon=100.0))                                                                                                                                                                 \r\nOut[16]: 0.1\r\n````\r\nIf the original docstring was correct, this would be 0.01 [1.0/(sqrt(0)+100.0)], not 0.1 [1.0/sqrt(0+100)]\r\n", "The calculation is:\r\n```\r\ninv = math_ops.rsqrt(variance + variance_epsilon)\r\n```\r\n\r\n@caisq can you take a look?"]}, {"number": 24692, "title": "Build with CUDA 10 by default", "body": "", "comments": []}, {"number": 24691, "title": "Move VariantTensorDataWriter and VariantTensorDataReader to dataset_utils", "body": "This PR moves the class `VariantTensorDataWriter` and `VariantTensorDataReader` from `iterator_ops.cc` to `dataset_utils.{h, cc}`, so that other files can reuse these two classes.\r\n\r\ncc: @jsimsa ", "comments": ["Sure, will add the tests.", "@jsimsa The test cases have been added. Could you have a look at the change?", "Thanks for your review @jsimsa! Yes, there is a clang-format issue as you point out. The test results also detect this issue. A commit is submitted to address your comments. Could you please have a look? ", "@jsimsa The clang-format check still fails. The following block is from the result of `clang-format check`. The indentation of the upper piece of code is suggested by running `clang-format tensorflow/core/kernels/data/dataset_utils.cc --style=google > ~/Desktop/my_cc_file.cc` on my laptop. I thought it should be consistent with the `Experimental clang-format Check` on Google. Do you have any advice about how to check the format?\r\n\r\n```\r\n-    status_ = errors::InvalidArgument(\r\n-        \"Unmatched number of keys and tensors: \", num_entries, \" vs. \",\r\n-        data_->tensors_size());\r\n+    status_ =\r\n+        errors::InvalidArgument(\"Unmatched number of keys and tensors: \",\r\n+                                num_entries, \" vs. \", data_->tensors_size());\r\n```\r\n\r\n`Ubuntu Sanity` failed, but it seems to be unrelated:\r\n```\r\n=== Sanity check step 5 of 12: do_bazel_nobuild (bazel nobuild) ===\r\n\r\n$TEST_TMPDIR defined: output root default is '/tmpfs/bazel_output' and max_idle_secs default is '15'.\r\nStarting local Bazel server and connecting to it...\r\nLoading: \r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    currently loading: tensorflow/lite/examples/android ... (19 packages)\r\nERROR: error loading package 'tensorflow/lite/experimental/swift': Extension file not found. Unable to load file '@bazel_skylib//lib:new_sets.bzl': file doesn't exist or isn't a file\r\nINFO: Elapsed time: 3.309s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (140 packages loaded)\r\nFAILED: Build did NOT complete successfully (140 packages loaded)\r\n\r\nFAIL: bazel build --nobuild  -- //tensorflow/... -//tensorflow/lite/java/demo/app/... -//tensorflow/lite/examples/android/... -//tensorflow/lite/schema/...\r\n  This is due to invalid BUILD files. See lines above for details.\r\n```", "I am not sure why the auto-formatting does not work. My advice is to just change it manually.", "@jsimsa I have submitted a commit to change the indentation as suggested by `clang-format check` test. Could you trigger the test again? Thanks!", "@jsimsa The failure is of `Ubuntu Sanity` is caused by `ERROR: error loading package 'tensorflow/lite/experimental/swift': Extension file not found. Unable to load file '@bazel_skylib//lib:partial.bzl': file doesn't exist or isn't a file`.\r\n\r\nThe failure of `Ubuntu Python2` is caused by `//tensorflow/python/keras:unified_gru_test`.\r\n\r\nBoth of them seem to be unrelated. Are there anything else I need to do?", "no", "My change has been submitted as https://github.com/tensorflow/tensorflow/commit/f602445c23feb67f2e35256ffdf24e090195fab6. Please merge the master into your PR.", "@jsimsa Thanks for solving the issue of the proto header! There are some edge cases that the latest code handles in a different way with the old one:\r\n\r\n- Empty key: [this line](https://github.com/tensorflow/tensorflow/blob/f602445c23feb67f2e35256ffdf24e090195fab6/tensorflow/core/kernels/data/iterator_ops.cc#L288) removes the empty key. The old implementation supports the empty key, as the tests show [here](https://github.com/tensorflow/tensorflow/blob/d140313d99afac25f181ae4f5ae8fedc475e55ff/tensorflow/core/kernels/data/dataset_utils_test.cc#L49). Do we need to consider the empty key?\r\n\r\n- Invalid metadata: do we need to consider the `VariantTensorData` with the invalid metadata, such as the metadata without `kDelimiter`, like [here](https://github.com/tensorflow/tensorflow/blob/d140313d99afac25f181ae4f5ae8fedc475e55ff/tensorflow/core/kernels/data/dataset_utils_test.cc#L86-L88)?\r\n\r\n- UnmatchedKeys: do we need to consider the case that the number of keys and values in `VariantTensorData` doesn't match with each other, like [here](https://github.com/tensorflow/tensorflow/blob/d140313d99afac25f181ae4f5ae8fedc475e55ff/tensorflow/core/kernels/data/dataset_utils_test.cc#L92-L95)?", "You do not need to test any of that anymore.", "@jsimsa Your changes have been merged to this PR by [this commit](https://github.com/tensorflow/tensorflow/pull/24691/commits/0be3b45806074bc386d3e72113dc48f2a332abe6). Could you have a look at it?", "@jsimsa This [commit](https://github.com/tensorflow/tensorflow/pull/24691/commits/06444e2c956f1aa6265bf467ebb8b3072a280b7f) is submitted to move `kDelimiter` to an anonymous namespace.", "@jsimsa The test failures look like unrelated. ", "Your PR fails to build with:\r\n\r\ntensorflow/core/kernels/data/dataset_utils_test.cc:55:3: error: use of undeclared identifier 'EXPECT_OK'\r\n  EXPECT_OK(reader.status());"]}, {"number": 24690, "title": "tf.keras.backend.batch_normalization documentation equation does not match implementation", "body": "**System information**\r\n- TensorFlow version: r1.12\r\n- Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/backend/batch_normalization\r\n\r\n**Describe the documentation issue**\r\nThe documentation describes the op as computing \r\n\r\n~~~\r\noutput = (x - mean) / (sqrt(var) + epsilon) * gamma + beta\r\n~~~\r\n\r\nthe op actually computes\r\n\r\n~~~\r\noutput = (x - mean) / (sqrt(var + epsilon)) * gamma + beta\r\n~~~\r\n\r\nwith `epsilon` inside the sqrt\r\n\r\nPractically, it doesn't really matter where the epsilon is added in actual use.  (I only noticed because I was comparing the output of each layer when translating something from caffe to tensorflow)\r\n\r\n**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**\r\n\r\nMaybe eventually, would take me awhile to get around to it", "comments": ["@chrisdahlberg Created a PR #24693 for the fix.", "The PR #24693 for the fix was closed. Hence we close this issue. Thanks."]}, {"number": 24689, "title": "Cherry pick build fixes.", "body": "", "comments": []}]