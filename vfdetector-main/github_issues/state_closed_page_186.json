[{"number": 49167, "title": "Could not load dynamic library 'libcusolver.so.10:No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64", "body": "TF2.4\r\nCUDA 11.2\r\n\r\n\r\nWhen I import tensorflow and list devices, I get this:\r\n\r\n`>>> tf.config.list_physical_devices()   \r\n2021-03-18 10:56:30.410381: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-18 10:56:30.411387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-18 10:56:30.451723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\r\n2021-03-18 10:56:30.451771: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2021-03-18 10:56:30.454059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2021-03-18 10:56:30.454124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2021-03-18 10:56:30.454855: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-18 10:56:30.455023: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-18 10:56:30.455150: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64\r\n2021-03-18 10:56:30.455675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2021-03-18 10:56:30.455764: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2021-03-18 10:56:30.455776: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\r\n`\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):2.4\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:11.2\r\n- GPU model and memory:\r\n\r\n", "comments": ["@ajinkeya007 \r\n\r\nplease refer the similar [issue](https://github.com/tensorflow/tensorflow/issues/47874) .You may also refer to this [link](https://www.tensorflow.org/install/source_windows) and let us know if it helps.Thanks\r\n\r\n", "@ajinkeya007 \r\n\r\nThank you for your update, glad its working fine for you, If yes kindly move this issue to closed status as it is resolved.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49167\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49167\">No</a>\n"]}, {"number": 49166, "title": "Resolves coredump caused by `tf.data.experimental.save` with prefetch", "body": "Repeat and prefetch in combination cause the snapshot reader Initialize function to be invoked multiple times.\r\nHowever, there is nothing to prefetch on the very last iteration. This results in Prefetch issuing a CancelThreads call while the snapshot thread is trying to initialize. See https://github.com/tensorflow/tensorflow/blob/6446dda92eaadf11d22377e2354307642d739d73/tensorflow/core/kernels/data/prefetch_dataset_op.cc#L151\r\n\r\nCurrently the dataset reference counting is done asymmetrically. The reference increment happens at the end of initialization, where as the reference decrement\r\nhappens in a destructor. When prefetch cancels the snapshot thread, it errors out of the initialization function. And stops calling the reference increment. However, the reference decrement happens regardless, as it is in the destructor which always is invoked during cleanup. This results in an attempt to decrement the null dataset pointer, and therefore a segmentation fault.\r\nThis is different from all other dataset ops, where the dataset reference increment happens in the constructor and the decrement happens in the destructor, which are symmetric.\r\n\r\nThe solution to this is to ensure that the dataset reference is always initialized to nullptr, and to check for null when decrementing the dataset reference.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49166) for more info**.\n\n<!-- need_author_consent -->", "Can you merge this? I plan to cherry-pick to 2.4 and 2.5 soon.\n\nOn Thu, May 13, 2021 at 10:06 PM Yang Chen ***@***.***> wrote:\n\n> ***@***.**** approved this pull request.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/49166#pullrequestreview-659521212>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAFISPMV363SILP7VD2UEI3TNSVV5ANCNFSM4423HJWQ>\n> .\n>\n", "I have approved your change internally. It requires approval from another reviewer. ", "Can we request @jsimsa for a review? He was helping me with another PR.", "Your change has been approved. It will be submitted soon. Currently, an unrelated already-failing test blocks the submission. It will be retried.", "Update: The test is still failing. I'll manually import your pull request and submit it."]}, {"number": 49165, "title": "`tf.data.experimental.load` segfaults when using repeat and prefetch", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Centos 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): 2.4.0\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nUsing the following simple script, we can see a segmentation fault:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndataset1 = tf.data.Dataset.from_tensor_slices(np.random.rand(16, 1024))\r\ntf.data.experimental.save(dataset1, 'snapshot')\r\ndataset = tf.data.experimental.load('snapshot')\r\ndataset = dataset.shuffle(buffer_size=16)\r\ndataset = dataset.batch(16)\r\ndataset = dataset.repeat()\r\ndataset = dataset.prefetch(1)\r\ndef run(dataset):\r\n    iterator = iter(dataset)\r\n    for _ in range(30):\r\n        next(iterator)\r\nfor _ in range(10):\r\n    run(dataset) \r\n```\r\nIf we run it with Tensorflow 2.4.0 (or Tensorflow 2.4.1), the output is:\r\n```\r\n...\r\n2021-05-04 11:04:17.989897: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-05-04 11:04:17.990504: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2596985000 Hz\r\nSegmentation fault (core dumped)\r\n```\r\nIf either of `load` or `repeat` or `prefetch` is removed, this would not occur.\r\n\r\n**Describe the expected behavior**\r\nThe expected behavior is that there would not be a segmentation fault\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - yes\r\nBriefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndataset1 = tf.data.Dataset.from_tensor_slices(np.random.rand(16, 1024))\r\ntf.data.experimental.save(dataset1, 'snapshot')\r\ndataset = tf.data.experimental.load('snapshot')\r\ndataset = dataset.shuffle(buffer_size=16)\r\ndataset = dataset.batch(16)\r\ndataset = dataset.repeat()\r\ndataset = dataset.prefetch(1)\r\ndef run(dataset):\r\n    iterator = iter(dataset)\r\n    for _ in range(30):\r\n        next(iterator)\r\nfor _ in range(10):\r\n    run(dataset)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nAnalyzing the core dump, this is the truncated stack trace:\r\n```\r\n        tensorflow::data::experimental::LoadDatasetOp::Dataset::Iterator::~Iterator()\r\n        tensorflow::data::DatasetBase::MakeIterator(tensorflow::data::IteratorContext*, tensorflow::data::IteratorBase const*, std::string const&, std::unique_ptr<tensorflow::data::IteratorBase, std::default_delete<tensorflow::data::IteratorBase> >*) const\r\n        tensorflow::data::ShuffleDatasetOpBase::ShuffleDatasetBase::Iterator::GetNextInternal(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*)\r\n        tensorflow::data::DatasetBaseIterator::GetNext(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*)\r\n        tensorflow::data::BatchDatasetOp::Dataset::Iterator::GetNextInternal(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*)\r\n        tensorflow::data::DatasetBaseIterator::GetNext(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*)\r\n        tensorflow::data::RepeatDatasetOp::Dataset::ForeverIterator::GetNextInternal(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*)\r\n        tensorflow::data::DatasetBaseIterator::GetNext(tensorflow::data::IteratorContext*, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, bool*)\r\n        tensorflow::data::PrefetchDatasetOp::Dataset::Iterator::PrefetchThread(std::shared_ptr<tensorflow::data::IteratorContext> const&)\r\n\r\n        std::_Function_handler<void (), std::_Bind<void (*(std::function<void ()>, std::shared_ptr<tensorflow::Notification>))(std::function<void ()> const&, std::shared_ptr<tensorflow::Notification>)> >::_M_invoke(std::_Any_data const&)\r\n        tensorflow::UnboundedWorkQueue::PooledThreadFunc()\r\n```", "comments": ["@yangustc07 ", "I have a fix for this in: https://github.com/tensorflow/tensorflow/pull/49166", "@ashahab \r\nAs #49166 is merged, please confirm if the issue is resolved and  move this to closed status.", "@Saduf2019 I have verified this works. Thank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49165\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49165\">No</a>\n"]}, {"number": 49163, "title": "[XLA] Move AMDGPU to not be compiled by default.", "body": "It lower the number of files compiled.", "comments": []}, {"number": 49162, "title": "Failure to load model files.", "body": "I have a tensorflow model that I train using customizing the model.fit() as in following link\r\nhttps://keras.io/guides/customizing_what_happens_in_fit/\r\nNow when I save the model using ModelCheckpoint callback and then later try to reload the model (using tf.keras.models.Model.load_model) or weights (model.load_weights), I am unable to do that, with the following errors:\r\n\r\nwith _.load_model()_\r\n> ValueError: No model found in config file. \r\n\r\nwith _.load_weights()_\r\n> ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\r\n\r\nMy models are defined using FunctionalAPI (https://keras.io/guides/functional_api/).\r\n\r\nI am currently working with tensorflow==2.4.1 but I have faced a similar issue with 2.2.0 before.\r\n\r\nBack then, to solve this issue, I wrote my own custom callback to save the model (using `.save()`) and then reload with `.load_model()`, as I discuss in this article [link](https://medium.com/dive-into-ml-ai/customization-of-model-fit-to-write-a-combined-discriminator-generator-gan-trainer-in-keras-524bce10cf66). But I was wondering if there is a default way to this.", "comments": ["@Anuj040 \r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet/ colab gist to reproduce the issue reported here and  Could you please [try](https://www.tensorflow.org/guide/keras/save_and_serialize) this, if it helps.Thanks!\r\n", "Here is the [gist](https://gist.github.com/Anuj040/77a5ec315d8c5693f7e5e8dcb1072484).\r\nI tried both the methods _load_model()_ method in the gist. \r\nso my question is what is the best way to save and load model if I am working with custom model.fit().\r\nAll help will be immensely appreciated.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Any updates/recommendations?", "@Anuj040 \r\n\r\nlooking at the error log,seems this is similar to the issue  [#29545](https://github.com/tensorflow/tensorflow/issues/29545), and let us know if it helps. Thanks", "@jvishnuvardhan  Please take a look at this issue\r\n\r\nI was able to reproduce the error.please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/4b973157398c7bae0c1d59fdac99474f/untitled68.ipynb) here.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Sorry for the delay in my response. \r\nUnfortunately, the suggested  issue #29545 does not seem to be connected to my issue, since I am working with _ModelCheckpoint_ callback.\r\n\r\nEven if I remove the _file extension_ from the save file path, on trying to use _KM.load_model_ with the saved model object(s), throws the following error\r\n![image](https://user-images.githubusercontent.com/66895104/120911185-f831d280-c6bf-11eb-8b29-670c66724ec8.png)\r\n\r\nFurther help would be immensely appreciated.", "Just following up. ", "@Anuj040 Sorry for the late response. I could reproduce the issue. I was able to save and load simple models using ModelCheckPoint callback but with the custom training and custom metrics, it is throwing an error. \r\n\r\nAs Keras moved to completely different repo to focus only on keras related issues, code owners are looking into the issues opened in new repo https://github.com/keras-team/keras/issues. Would you mind opening there and close it here. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@jvishnuvardhan Sorry for my late response and thanks a lot for getting back on this. I will move the issue to the mentioned repo."]}, {"number": 49161, "title": "Progbar prints invalid metrics value when using multiple thresholds", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- TensorFlow version: 2.4.1\r\n- Python version: 3.8.8\r\n\r\n**Describe the current behavior**\r\nProgbar prints mean metric value of all thersholds\r\n\r\n**Describe the expected behavior**\r\nProgbar prints first value or value for default threshold (0.5)\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://github.com/tensorflow/tensorflow/blob/r2.4/tensorflow/python/keras/utils/generic_utils.py#L625\r\nhttps://colab.research.google.com/drive/1zEa5GB9e8IrufWlHVw2Cc35euc8aXGKy?usp=sharing\r\n", "comments": ["@ferryvg Can you please share a standalone code to reproduce the issue? Please note that the above standalone code doesn't use `Progabar`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/10e46351800000340e577b0e5a0f96f4/untitled.ipynb) is a gist with MNIST data where `Progbar` results matches well with the values in the `history` object.  Thanks!", "@jvishnuvardhan\r\nYou can just run common training on MNIST dataset, for example. You should adds FP/FN/TP/TF metrics to model, set verbose to 1 and compare `Progbar` results with `History` metrics", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49161\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49161\">No</a>\n"]}, {"number": 49160, "title": "No operations will run on the GPU for mobilenet_v1_1.0_224_quant.tflite using C++", "body": "Hello.\r\n\r\nI tired to load mobilenet_v1_1.0_224_quant.tflite from tflite example (tensorflow/lite/java/demo/app/src/main/), but it print errors to log:\r\n\r\n```\r\nFollowing operations are not supported by GPU delegate:\r\n    AVERAGE_POOL_2D: OP is supported, but tensor type isn't matched!\r\n    CONV_2D: OP is supported, but tensor type isn't matched!\r\n    DEPTHWISE_CONV_2D: OP is supported, but tensor type isn't matched!\r\n    RESHAPE: OP is supported, but tensor type isn't matched!\r\n    SOFTMAX: OP is supported, but tensor type isn't matched!\r\n    No operations will run on the GPU, and all 31 operations will run on the CPU.\r\n```\r\n\r\nI use this code to load:\r\n\r\n```\r\n  tflite::InterpreterBuilder interpreter_builder(flatbuffer, op_resolver);\r\n  if (interpreter_builder(&interpreter) != kTfLiteOk || !interpreter) {\r\n    return absl::InternalError(\"Unable to prepare TfLite interpreter.\");\r\n  }\r\n  TfLiteGpuDelegateOptionsV2 options = TfLiteGpuDelegateOptionsV2Default();\r\n  TfLiteDelegate* delegate = TfLiteGpuDelegateV2Create(&options);\r\n\r\n  DelegateContext::DelegateData delegate_data{interpreter->inputs(),\r\n                                              interpreter->outputs(), graph};\r\n\r\n  delegate->data_ = &delegate_data;\r\n  delegate->flags = kTfLiteDelegateFlagsNone;\r\n  delegate->Prepare = DelegatePrepare;\r\n  delegate->CopyFromBufferHandle = nullptr;\r\n  delegate->CopyToBufferHandle = nullptr;\r\n  delegate->FreeBufferHandle = nullptr;\r\n\r\n  if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk)  // Error prints in in this call.\r\n```\r\n\r\nWhy TFlite cannot run mobilenet_v1_1.0_224_quant.tflite on GPU? Or is it ok?\r\n\r\nIf I load mobilenet_v1_1.0_224_quant.tflite using Java interface I don't have any errors in log.\r\n\r\nI have built TFlite by my self, run on Android Google Pixel 2.", "comments": ["@srjoglekar246 could you take a look at this?", "Can you check which version of TFLite you are using? Last year we added support for quantization in our GPU delegate.", "I built using sources from master revision:\r\n[XLA:GPU] Log error if LLVM IR parsing fails.\te6986d372fa\tChris Jones <cjfj@google.com>\t29 Apr 2021, 11:46\r\n\r\nI didn't find ready to use binary(for C++ API) and built them by myself. Maybe I took unstable changes and I have to take sources from release branch. \r\n\"graph\" from code sample uses OpenGL.", "Also I tried to build from r2.5 branch and I have the same problem. I built 3 modules:  libtensorflowlite_gpu_delegate.so, libtensorflowlite_gpu_gl.so, libtensorflowlite.so. Did I forget any modules?", "@lu-wang-g Can you check if @UnickSoft 's method of building the application is correct?\r\n\r\n@UnickSoft Does the model run w/ GPU delegate with the Java API? Or you just tried the CPU path on Java?", "The way of building the TFLite .so files looks good to me.\r\n\r\nThe error seems to come from the code [here](https://github.com/tensorflow/tensorflow/blob/5854c4a9c86ccb44d7bc43cfd2143ea5a56b2d8c/tensorflow/lite/delegates/gpu/common/model_builder.cc#L3048-L3069), where I think `allow_quant_ops` is not turned on (be default it's not, see [enable_quantization](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu#advanced-usage-delegate-options)), so Interpreter cannot handle quantized models.  However, seems TfLiteGpuDelegateOptionsV2Default() sets enable_quant as default. See [here](https://github.com/tensorflow/tensorflow/blob/5854c4a9c86ccb44d7bc43cfd2143ea5a56b2d8c/tensorflow/lite/delegates/gpu/delegate.cc#L451).\r\n\r\n@talumbau are you familiar with the GPU settings?", "@srjoglekar246 I tried to use Java API with GPU. With Java API it loads and work correct (because I don't see any messages in log and GPU version is work much faster). I haven't build Java module by myself. I will try to add additional log to find a reason for a problem.\r\n\r\n", "We have found a reason of problem. Sorry for wasting time. It is my initialization code:\r\n\r\n```\r\n  delegate->data_ = &delegate_data;\r\n  delegate->flags = kTfLiteDelegateFlagsNone;\r\n  delegate->Prepare = DelegatePrepare;\r\n  delegate->CopyFromBufferHandle = nullptr;\r\n  delegate->CopyToBufferHandle = nullptr;\r\n  delegate->FreeBufferHandle = nullptr;\r\n```\r\n\r\nI override delegate->Prepare. In this function I don't pass second parameter to function GetOpsToReplace (by default this parameter is false). \r\n\r\nMaybe I should take code from official docs, because I took this code from tensorflow/lite/delegates/gpu/common/testing/tflite_model_reader.cc and some other code from mediapipe.\r\nAlso I found potencial problem in file tensorflow/lite/delegates/gpu/gl_delegate.cc. The function DelegatePrepare from this file don't pass second parameter into GetOpsToReplace.\r\n\r\nTo fix my problem I just pass second paramert into GetOpsToReplace in DelegatePrepare function. And I think we can close this issue.", "@UnickSoft The delegate code in delegates/gpu/delegate.h is what we recommend to use as the GPU delegate, since that is the well-maintained backend. Some of these delegate codebases are pretty complex, so modify with care :-). GLad that you found a fix!"]}, {"number": 49159, "title": "Checkpoint` was expecting a trackable object", "body": "\r\n\r\n**System information**\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 18.04.5\r\n- Python version: 3.7\r\n- tensorflow_gpu: 2.4.1\r\n- CUDA/cuDNN version: 11.3/7.6.5\r\n\r\n\r\n\r\n**Description**\r\nAll models are written in tf.keras.\r\nI wanted to do a hyperparameter tuning for the model and vary the number of convultional layers, bottleneck layers, but now the discriminator model seems to be no longer trackable.\r\nCan anyone help?\r\n\r\n**Output**\r\n```python\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-124-88c04dabe8f8> in <module>\r\n     21           print('--- Starting trial: %s' % run_name)\r\n     22           print({h.name: hparams[h] for h in hparams})\r\n---> 23           run('logs/hparam_tuning/' + run_name, hparams,train_256=train256_dataset,test_256=test256_dataset,EPOCHS=5)\r\n     24           session_num += 1\r\n\r\n<ipython-input-72-992a1cf189f8> in run(run_dir, hparams, train_256, test_256, EPOCHS)\r\n      3      with tf.summary.create_file_writer(run_dir).as_default():\r\n      4         hp.hparams(hparams)\r\n----> 5         gen_l1_loss,disc_loss=train_test(hparams,train_256,test_256,EPOCHS=EPOCHS)\r\n      6         tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\r\n      7         tf.summary.scalar('disc_loss', disc_loss, step=epoch)\r\n\r\n<ipython-input-123-120ded423bb9> in train_test(hparams, train_256, test_256, EPOCHS)\r\n    129                                      discriminator_optimizer=discriminator_optimizer,\r\n    130                                      generator=generator,\r\n--> 131                                      discriminator=discriminator)\r\n    132     log_dir=\"logs/\"\r\n    133     sum_log=f\"{log_dir} {model_name}/{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\r\n\r\n~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py in __init__(self, root, **kwargs)\r\n   1927       # v to a Trackable data structure when v is a list/dict/tuple.\r\n   1928       converted_v = getattr(self, k)\r\n-> 1929       _assert_trackable(converted_v)\r\n   1930 \r\n   1931       if root:\r\n\r\n~/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py in _assert_trackable(obj)\r\n   1413         \"object should be trackable (i.e. it is part of the \"\r\n   1414         \"TensorFlow Python API and manages state), please open an issue.\"\r\n-> 1415         .format(obj))\r\n   1416 \r\n   1417 \r\n\r\nValueError: `Checkpoint` was expecting a trackable object (an object derived from `TrackableBase`), got <function train_test.<locals>.Discriminator at 0x7fa5a8174f80>. If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.\r\n```\r\n\r\n\r\n", "comments": ["Can you share a very minimal standalone code example or Colab to reproduce this?", "I ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49159\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49159\">No</a>\n", "I only forgot the brackets ()"]}, {"number": 49157, "title": "Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.", "body": " I use tf.image.combined_non_max_suppression in my model. I can transfer it to tflite. But when I use it in Android or use python tf.lite.interpreter, I got the issue.\r\n![118105343-d752c600-b40e-11eb-9810-5a12f18e1ce3](https://user-images.githubusercontent.com/54882489/118111091-cd809100-b415-11eb-9aca-f6ee3fd0b5ef.png)\r\nHow can I resolve it?", "comments": ["Looks like the model is converted with the Select TF option. In such case, the android application requires an extra dependency. See also https://www.tensorflow.org/lite/guide/ops_select.\r\n\r\n```\r\ndependencies {\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly-SNAPSHOT'\r\n    // This dependency adds the necessary TF op support.\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly-SNAPSHOT'\r\n}\r\n```", "@abattery    I did it. I ran it successfully. Thank you very much", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49157\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49157\">No</a>\n"]}, {"number": 49156, "title": "Convert from TF 1.8 checkpoint to TF2 SavedModel : variables not visible", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): From dockerhub\r\n- TensorFlow version (use command below): 2.41\r\n- Python version: '3.6.9 (default, Oct  8 2020, 12:12:24) \\n[GCC 8.4.0]'\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: 1080ti, 10GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n```\r\n2021-05-13 09:38:26.223909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\nv2.4.0-49-g85c8b2a817f 2.4.1\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nThe converted and loaded FuncGraph's variables is not visible.\r\n\r\nI converted it by the script below. as you can see, I ran `tf.compat.v1.enable_resource_variables()\r\n` before building the model.\r\n```python\r\nimport os\r\nfrom tensorflow.python.saved_model import signature_constants\r\nfrom tensorflow.python.saved_model import tag_constants\r\ntf.compat.v1.enable_resource_variables()\r\n\r\n\r\ntrained_checkpoint_prefix = '../pretrained/model.ckpt'\r\nexport_dir = os.path.join('../total_exported')\r\n\r\ntarget_tensors = {}\r\nall_tensors = []\r\nall_nodes = []\r\nall_placeholders = []\r\ngraph = tf.Graph()\r\ntensor_info_inputs = []\r\ntensor_info_outputs = []\r\n\r\nwith tf.compat.v1.Session(graph=graph) as sess:\r\n    # Restore from checkpoint\r\n    loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')\r\n    loader.restore(sess, trained_checkpoint_prefix)\r\n    # all_nodes = [n for n in tf.compat.v1.get_default_graph().as_graph_def().node]\r\n    print(graph)\r\n    all_nodes = [n for n in graph.as_graph_def().node]\r\n    all_tensors = all_tensors + [tensor for op in graph.get_operations() for tensor in op.values()]\r\n    all_placeholders = [placeholder for op in graph.get_operations() if op.type=='Placeholder' for placeholder in op.values()]\r\n    \r\n    for tensor in all_tensors:\r\n        if tensor.name == 'resnet_v2_101/predictions/Reshape_1:0':\r\n            target_tensors['feature'] = tensor\r\n        elif tensor.name == 'Placeholder:0':\r\n            target_tensors['img'] = tensor\r\n        elif tensor.name == 'Placeholder_1:0':\r\n            target_tensors['kp_map_gt'] = tensor\r\n        elif tensor.name == 'Placeholder_2:0':\r\n            target_tensors['short_offset_gt'] = tensor\r\n        elif tensor.name == 'Placeholder_3:0':\r\n            target_tensors['mid_offset_gt'] = tensor\r\n        elif tensor.name == 'Placeholder_4:0':\r\n            target_tensors['long_offset_gt'] = tensor\r\n        elif tensor.name == 'Placeholder_5:0':\r\n            target_tensors['seg_mask_gt'] = tensor\r\n        elif tensor.name == 'Placeholder_6:0':\r\n            target_tensors['crowd_mask'] = tensor\r\n        elif tensor.name == 'Placeholder_7:0':\r\n            target_tensors['unannotated_mask'] = tensor\r\n        elif tensor.name == 'Placeholder_8:0':\r\n            target_tensors['overlap_mask'] = tensor\r\n        elif tensor.name == 'short_offsets/BiasAdd:0':\r\n            target_tensors['short_offset'] = tensor\r\n        elif tensor.name == 'mid_offsets/BiasAdd:0':\r\n            target_tensors['mid_offset'] = tensor\r\n        elif tensor.name == 'long_offsets/BiasAdd:0':\r\n            target_tensors['long_offset'] = tensor\r\n        elif tensor.name == 'kp_maps/Sigmoid:0':\r\n            target_tensors['kp_map'] = tensor\r\n        elif tensor.name == 'seg_mask/Sigmoid:0':\r\n            target_tensors['seg_mask'] = tensor\r\n\r\n    tensor_info_inputs = {\r\n              'img': tf.compat.v1.saved_model.build_tensor_info(target_tensors['img']),  \r\n              'kp_map_gt': tf.compat.v1.saved_model.build_tensor_info(target_tensors['kp_map_gt']),\r\n              'short_offset_gt': tf.compat.v1.saved_model.build_tensor_info(target_tensors['short_offset_gt']),\r\n              'mid_offset_gt': tf.compat.v1.saved_model.build_tensor_info(target_tensors['mid_offset_gt']),\r\n              'long_offset_gt': tf.compat.v1.saved_model.build_tensor_info(target_tensors['long_offset_gt']),\r\n              'seg_mask_gt': tf.compat.v1.saved_model.build_tensor_info(target_tensors['seg_mask_gt']),\r\n              'crowd_mask': tf.compat.v1.saved_model.build_tensor_info(target_tensors['crowd_mask']),\r\n              'unannotated_mask': tf.compat.v1.saved_model.build_tensor_info(target_tensors['unannotated_mask']),\r\n              'overlap_mask': tf.compat.v1.saved_model.build_tensor_info(target_tensors['overlap_mask']),\r\n    }\r\n    tensor_info_outputs = {\r\n        'feature' : tf.compat.v1.saved_model.build_tensor_info(target_tensors['feature']),\r\n        'short_offset' : tf.compat.v1.saved_model.build_tensor_info(target_tensors['short_offset']),\r\n        'mid_offset' : tf.compat.v1.saved_model.build_tensor_info(target_tensors['mid_offset']),\r\n        'long_offset' : tf.compat.v1.saved_model.build_tensor_info(target_tensors['long_offset']),\r\n        'kp_map' : tf.compat.v1.saved_model.build_tensor_info(target_tensors['kp_map']),\r\n        'seg_mask' : tf.compat.v1.saved_model.build_tensor_info(target_tensors['seg_mask']),\r\n    }\r\n\r\n    detection_signature = (\r\n            tf.compat.v1.saved_model.build_signature_def(\r\n                  inputs=tensor_info_inputs,\r\n                  outputs=tensor_info_outputs,\r\n            method_name='total'))\r\n\r\n    # Export checkpoint to SavedModel\r\n    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_dir)\r\n    builder.add_meta_graph_and_variables(\r\n              sess, [tf.saved_model.SERVING],\r\n              signature_def_map={\r\n                  'total': detection_signature,\r\n              },\r\n          )\r\n    \r\n#     builder.add_meta_graph_and_variables(sess,\r\n#                                          [tf.saved_model.TRAINING, tf.saved_model.SERVING],\r\n#                                          strip_default_attrs=True)\r\n    builder.save()\r\n```\r\n\r\nAnd warn message shows up.\r\n```python\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/conv1/biases:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/conv1/biases:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/conv1/biases:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/conv1/weights:0' shape=(7, 7, 3, 64) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/conv1/biases:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/gamma:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/beta:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\nWARNING:tensorflow:Unable to create a python object for variable <tf.Variable 'resnet_v2_101/block1/unit_1/bottleneck_v2/preact/moving_mean:0' shape=(64,) dtype=float32_ref> because it is a reference variable. It may not be visible to training APIs. If this is a problem, consider rebuilding the SavedModel after running tf.compat.v1.enable_resource_variables().\r\n```\r\n\r\nwith no visible variables.\r\n```python\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_resource_variables()\r\nmodel = tf.compat.v1.saved_model.load_v2('../total_exported')\r\nf = model.signatures['total']\r\noutputs = f(**inputs)\r\n\r\nmodel.graph.variables\r\n# ()\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nLoaded model has graph with visible variables.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi @cybaj, it's likely that the metagraph loaded in `loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')` contains reference variables. We can't directly convert reference to resource variables so you may have to re-export it in order to get the correct graph. ", "Marking this as closed, but feel free to ask this question in StackOverflow since this is neither a bug nor feature request.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49156\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49156\">No</a>\n"]}, {"number": 49155, "title": "problem using tf.compat.v1.get_variable in tf2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos mojave 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):no\r\n- GCC/Compiler version (if compiling from source):no\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nreference\uff1ahttps://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable?hl=en\r\nI referenced the above links and got the following results\r\nv1:<tf.Variable 'foo/v:0' shape=(1,) dtype=float32, numpy=array([1.3727096], dtype=float32)>\r\nv2:<tf.Variable 'foo/v:0' shape=(1,) dtype=float32, numpy=array([-1.0603657], dtype=float32)>\r\n\r\n**Describe the expected behavior**\r\nv1 should be equal to v2\r\nWhen I use tf1, I get the following results\uff1a\r\nv1:[0.91463006]\r\nv2:[0.91463006]\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): no\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nreference\uff1ahttps://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable?hl=en\r\ntf2 version:\r\n```import tensorflow as tf\r\ndef foo():\r\n    with tf.compat.v1.variable_scope(\"foo\", reuse=tf.compat.v1.AUTO_REUSE):\r\n        v = tf.compat.v1.get_variable(\"v\", [1], initializer=tf.compat.v1.glorot_uniform_initializer())\r\n        return v\r\nv1 = foo()\r\nv2 = foo()\r\nprint(v1)\r\nprint(v2)\r\n```\r\ntf1 version:\r\n```import tensorflow as tf\r\ndef foo():\r\n    with tf.compat.v1.variable_scope(\"foo\", reuse=tf.compat.v1.AUTO_REUSE):\r\n        v = tf.compat.v1.get_variable(\"v\", [1], initializer=tf.compat.v1.glorot_uniform_initializer())\r\n        return v\r\nv1 = foo()\r\nv2 = foo()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    print(f'v1:{sess.run(v1)}')\r\n    print(f'v2:{sess.run(v2)}')\r\n```", "comments": ["In TF1 you are in graph mode right? \r\n\r\nSo in TF2\r\n\r\n```\r\nimport tensorflow as tf\r\n@tf.function\r\ndef foo():\r\n  with tf.compat.v1.variable_scope(\"foo\", reuse=tf.compat.v1.AUTO_REUSE):\r\n    v = tf.compat.v1.get_variable(\"v\", [1])\r\n  return v\r\n\r\nv1 = foo()  # Creates v.\r\nv2 = foo()  # Gets the same, existing v.\r\nassert v1 == v2\r\n````", "> In TF1 you are in graph mode right?\r\n> \r\n> So in TF2\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> @tf.function\r\n> def foo():\r\n>   with tf.compat.v1.variable_scope(\"foo\", reuse=tf.compat.v1.AUTO_REUSE):\r\n>     v = tf.compat.v1.get_variable(\"v\", [1])\r\n>   return v\r\n> \r\n> v1 = foo()  # Creates v.\r\n> v2 = foo()  # Gets the same, existing v.\r\n> assert v1 == v2\r\n> ```\r\n\r\nWhy do I get different values when I use  Eager Execution  and Graph Execution", "It Is documented. \r\nCheck:\r\nhttps://github.com/tensorflow/tensorflow/issues/45876\r\nhttps://github.com/tensorflow/tensorflow/issues/46133\r\n\r\n", "@neuronblack ,\r\n\r\nPlease refer the issues mentioned and let us know if it helps.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49155\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49155\">No</a>\n"]}, {"number": 49154, "title": "TensorBuffer does not support data type: INT32", "body": "\r\nI used tf.image.combined_non_max_suppression in my model to post-process my output.   And I transfer it to tflite file.  When I use the tflite file in the android app.  I found an issue. \r\nProcess: ncu.edu.mao, PID: 26529\r\n    java.lang.AssertionError: TensorBuffer does not support data type: INT32\r\n        at org.tensorflow.lite.support.tensorbuffer.TensorBuffer.createFixedSize(TensorBuffer.java:79)\r\n\r\n![image](https://user-images.githubusercontent.com/54882489/118098482-8b038800-b406-11eb-9b08-dccd78b248aa.png)\r\n", "comments": ["#Android code\r\n protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {\r\n        super.onActivityResult(requestCode, resultCode, data);\r\n        if(requestCode == REQUEST_CODE_FOR_CAPTURE){\r\n            if(resultCode == Activity.RESULT_OK){\r\n                try{\r\n                    imageProcessor = new ImageProcessor.Builder()\r\n                            .add(new ResizeOp(300, 300, ResizeOp.ResizeMethod.NEAREST_NEIGHBOR))\r\n                            .add(new NormalizeOp(IMAGE_MEAN,IMAGE_STD)).build();\r\n\r\n                    Bitmap bm = BitmapFactory.decodeFile(imagePath + imageName).copy(Bitmap.Config.ARGB_8888,true);\r\n\r\n                    inputImageBuffer = new TensorImage(DataType.FLOAT32);\r\n\r\n                    AddSsd model = AddSsd.newInstance(NewActivity.this);\r\n                    inputImageBuffer.load(bm);\r\n\r\n                    // Creates inputs for reference.\r\n                    TensorBuffer inputFeature0 = TensorBuffer.createFixedSize(new int[]{1, 300, 300, 3}, DataType.FLOAT32);\r\n                    inputFeature0.loadBuffer(imageProcessor.process(inputImageBuffer).getBuffer());\r\n\r\n                    // Runs model inference and gets result.\r\n                    AddSsd.Outputs outputs = model.process(inputFeature0);\r\n                    TensorBuffer outputFeature0 = outputs.getOutputFeature0AsTensorBuffer();\r\n                    TensorBuffer outputFeature1 = outputs.getOutputFeature1AsTensorBuffer();\r\n                    TensorBuffer outputFeature2 = outputs.getOutputFeature2AsTensorBuffer();\r\n                    TensorBuffer outputFeature3 = outputs.getOutputFeature3AsTensorBuffer();\r\n\r\n                    // Releases model resources if no longer used.\r\n                    model.close();\r\n\r\n\r\n\r\n                }catch (Exception e){\r\n                    e.printStackTrace();\r\n                }\r\n            }\r\n        }\r\n\r\n    }", "`def get_real_result(input):\r\n    mbox_loc = input[0]\r\n    mbox_conf = input[1][:, :, 1:]\r\n    mbox_priorbox = input[2][:,:,:4]\r\n    variances = input[2][:, :, -4:]\r\n\r\n    # \u83b7\u5f97\u5148\u9a8c\u6846\u7684\u5bbd\u4e0e\u9ad8\r\n    prior_width = mbox_priorbox[:,:, 2] - mbox_priorbox[:,:, 0]\r\n    prior_height = mbox_priorbox[:,:, 3] - mbox_priorbox[:,:, 1]\r\n    # \u83b7\u5f97\u5148\u9a8c\u6846\u7684\u4e2d\u5fc3\u70b9\r\n    prior_center_x = 0.5 * (mbox_priorbox[:,:, 2] + mbox_priorbox[:,:, 0])\r\n    prior_center_y = 0.5 * (mbox_priorbox[:,:, 3] + mbox_priorbox[:,:, 1])\r\n\r\n    # \u771f\u5b9e\u6846\u8ddd\u79bb\u5148\u9a8c\u6846\u4e2d\u5fc3\u7684xy\u8f74\u504f\u79fb\u60c5\u51b5\r\n    decode_bbox_center_x = mbox_loc[:,:, 0] * prior_width * variances[:,:, 0]\r\n    decode_bbox_center_x += prior_center_x\r\n    decode_bbox_center_y = mbox_loc[:,:, 1] * prior_height * variances[:,:, 1]\r\n    decode_bbox_center_y += prior_center_y\r\n\r\n    # \u771f\u5b9e\u6846\u7684\u5bbd\u4e0e\u9ad8\u7684\u6c42\u53d6\r\n    decode_bbox_width = tf.math.exp(mbox_loc[:,:, 2] * variances[:,:, 2])\r\n    decode_bbox_width *= prior_width\r\n    decode_bbox_height = tf.math.exp(mbox_loc[:,:, 3] * variances[:,:, 3])\r\n    decode_bbox_height *= prior_height\r\n\r\n    # \u83b7\u53d6\u771f\u5b9e\u6846\u7684\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0b\u89d2\r\n    decode_bbox_xmin = decode_bbox_center_x - 0.5 * decode_bbox_width\r\n    decode_bbox_ymin = decode_bbox_center_y - 0.5 * decode_bbox_height\r\n    decode_bbox_xmax = decode_bbox_center_x + 0.5 * decode_bbox_width\r\n    decode_bbox_ymax = decode_bbox_center_y + 0.5 * decode_bbox_height\r\n    # \u771f\u5b9e\u6846\u7684\u5de6\u4e0a\u89d2\u4e0e\u53f3\u4e0b\u89d2\u8fdb\u884c\u5806\u53e0\r\n    decode_bbox = tf.concat((decode_bbox_xmin[:,:, None],\r\n                            decode_bbox_ymin[:,:, None],\r\n                            decode_bbox_xmax[:,:, None],\r\n                            decode_bbox_ymax[:,:, None]), axis=-1)\r\n    decode_bbox = tf.reshape(decode_bbox,shape=[-1,8732,1,4])\r\n\r\n    # \u9632\u6b62\u8d85\u51fa0\u4e0e1\r\n    decode_bbox = tf.minimum(tf.maximum(decode_bbox, 0.0), 1.0)\r\n    boxes,scores,classes,count = tf.image.combined_non_max_suppression(decode_bbox,mbox_conf,\r\n                                          max_output_size_per_class=100,max_total_size=200,\r\n                                          iou_threshold=0.5,score_threshold=0.5,clip_boxes=False)\r\n\r\n    return boxes,scores,classes,count\r\n\r\ndef SSD300(input_shape, num_classes=21, anchors_size=[30,60,111,162,213,264,315],training = True):\r\n    #---------------------------------#\r\n    #   \u5178\u578b\u7684\u8f93\u5165\u5927\u5c0f\u4e3a[300,300,3]\r\n    #---------------------------------#\r\n    input_tensor = Input(shape=input_shape)\r\n    \r\n    # net\u53d8\u91cf\u91cc\u9762\u5305\u542b\u4e86\u6574\u4e2aSSD\u7684\u7ed3\u6784\uff0c\u901a\u8fc7\u5c42\u540d\u53ef\u4ee5\u627e\u5230\u5bf9\u5e94\u7684\u7279\u5f81\u5c42\r\n    net = VGG16(input_tensor)\r\n    # net = mobilenet(input_tensor)\r\n\r\n    #-----------------------\u5c06\u63d0\u53d6\u5230\u7684\u4e3b\u5e72\u7279\u5f81\u8fdb\u884c\u5904\u7406---------------------------#\r\n    # \u5bf9conv4_3\u7684\u901a\u9053\u8fdb\u884cl2\u6807\u51c6\u5316\u5904\u7406 \r\n    # 38,38,512\r\n    net['conv4_3_norm'] = Normalize(20, name='conv4_3_norm')(net['conv4_3'])\r\n    num_priors = 4\r\n    # \u9884\u6d4b\u6846\u7684\u5904\u7406\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0c4\u662fx,y,h,w\u7684\u8c03\u6574\r\n    net['conv4_3_norm_mbox_loc'] = Conv2D(num_priors * 4, kernel_size=(3,3), padding='same', name='conv4_3_norm_mbox_loc')(net['conv4_3_norm'])\r\n    net['conv4_3_norm_mbox_loc_flat'] = Flatten(name='conv4_3_norm_mbox_loc_flat')(net['conv4_3_norm_mbox_loc'])\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0cnum_classes\u662f\u6240\u5206\u7684\u7c7b\r\n    net['conv4_3_norm_mbox_conf'] = Conv2D(num_priors * num_classes, kernel_size=(3,3), padding='same',name='conv4_3_norm_mbox_conf')(net['conv4_3_norm'])\r\n    net['conv4_3_norm_mbox_conf_flat'] = Flatten(name='conv4_3_norm_mbox_conf_flat')(net['conv4_3_norm_mbox_conf'])\r\n\r\n    priorbox = PriorBox(input_shape, anchors_size[0], max_size=anchors_size[1], aspect_ratios=[2],\r\n                        variances=[0.1, 0.1, 0.2, 0.2],\r\n                        name='conv4_3_norm_mbox_priorbox')\r\n    net['conv4_3_norm_mbox_priorbox'] = priorbox(net['conv4_3_norm'])\r\n    \r\n    # \u5bf9fc7\u5c42\u8fdb\u884c\u5904\u7406 \r\n    # 19,19,1024\r\n    num_priors = 6\r\n    # \u9884\u6d4b\u6846\u7684\u5904\u7406\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0c4\u662fx,y,h,w\u7684\u8c03\u6574\r\n    net['fc7_mbox_loc'] = Conv2D(num_priors * 4, kernel_size=(3,3),padding='same',name='fc7_mbox_loc')(net['fc7'])\r\n    net['fc7_mbox_loc_flat'] = Flatten(name='fc7_mbox_loc_flat')(net['fc7_mbox_loc'])\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0cnum_classes\u662f\u6240\u5206\u7684\u7c7b\r\n    net['fc7_mbox_conf'] = Conv2D(num_priors * num_classes, kernel_size=(3,3),padding='same',name='fc7_mbox_conf')(net['fc7'])\r\n    net['fc7_mbox_conf_flat'] = Flatten(name='fc7_mbox_conf_flat')(net['fc7_mbox_conf'])\r\n\r\n    priorbox = PriorBox(input_shape, anchors_size[1], max_size=anchors_size[2], aspect_ratios=[2, 3],\r\n                        variances=[0.1, 0.1, 0.2, 0.2],\r\n                        name='fc7_mbox_priorbox')\r\n    net['fc7_mbox_priorbox'] = priorbox(net['fc7'])\r\n\r\n    # \u5bf9conv6_2\u8fdb\u884c\u5904\u7406\r\n    # 10,10,512\r\n    num_priors = 6\r\n    # \u9884\u6d4b\u6846\u7684\u5904\u7406\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0c4\u662fx,y,h,w\u7684\u8c03\u6574\r\n    x = Conv2D(num_priors * 4, kernel_size=(3,3), padding='same',name='conv6_2_mbox_loc')(net['conv6_2'])\r\n    net['conv6_2_mbox_loc'] = x\r\n    net['conv6_2_mbox_loc_flat'] = Flatten(name='conv6_2_mbox_loc_flat')(net['conv6_2_mbox_loc'])\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0cnum_classes\u662f\u6240\u5206\u7684\u7c7b\r\n    x = Conv2D(num_priors * num_classes, kernel_size=(3,3), padding='same',name='conv6_2_mbox_conf')(net['conv6_2'])\r\n    net['conv6_2_mbox_conf'] = x\r\n    net['conv6_2_mbox_conf_flat'] = Flatten(name='conv6_2_mbox_conf_flat')(net['conv6_2_mbox_conf'])\r\n\r\n    priorbox = PriorBox(input_shape, anchors_size[2], max_size=anchors_size[3], aspect_ratios=[2, 3],\r\n                        variances=[0.1, 0.1, 0.2, 0.2],\r\n                        name='conv6_2_mbox_priorbox')\r\n    net['conv6_2_mbox_priorbox'] = priorbox(net['conv6_2'])\r\n\r\n    # \u5bf9conv7_2\u8fdb\u884c\u5904\u7406\r\n    # 5,5,256\r\n    num_priors = 6\r\n    # \u9884\u6d4b\u6846\u7684\u5904\u7406\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0c4\u662fx,y,h,w\u7684\u8c03\u6574\r\n    x = Conv2D(num_priors * 4, kernel_size=(3,3), padding='same',name='conv7_2_mbox_loc')(net['conv7_2'])\r\n    net['conv7_2_mbox_loc'] = x\r\n    net['conv7_2_mbox_loc_flat'] = Flatten(name='conv7_2_mbox_loc_flat')(net['conv7_2_mbox_loc'])\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0cnum_classes\u662f\u6240\u5206\u7684\u7c7b\r\n    x = Conv2D(num_priors * num_classes, kernel_size=(3,3), padding='same',name='conv7_2_mbox_conf')(net['conv7_2'])\r\n    net['conv7_2_mbox_conf'] = x\r\n    net['conv7_2_mbox_conf_flat'] = Flatten(name='conv7_2_mbox_conf_flat')(net['conv7_2_mbox_conf'])\r\n\r\n    priorbox = PriorBox(input_shape, anchors_size[3], max_size=anchors_size[4], aspect_ratios=[2, 3],\r\n                        variances=[0.1, 0.1, 0.2, 0.2],\r\n                        name='conv7_2_mbox_priorbox')\r\n    net['conv7_2_mbox_priorbox'] = priorbox(net['conv7_2'])\r\n\r\n    # \u5bf9conv8_2\u8fdb\u884c\u5904\u7406\r\n    # 3,3,256\r\n    num_priors = 4\r\n    # \u9884\u6d4b\u6846\u7684\u5904\u7406\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0c4\u662fx,y,h,w\u7684\u8c03\u6574\r\n    x = Conv2D(num_priors * 4, kernel_size=(3,3), padding='same',name='conv8_2_mbox_loc')(net['conv8_2'])\r\n    net['conv8_2_mbox_loc'] = x\r\n    net['conv8_2_mbox_loc_flat'] = Flatten(name='conv8_2_mbox_loc_flat')(net['conv8_2_mbox_loc'])\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0cnum_classes\u662f\u6240\u5206\u7684\u7c7b\r\n    x = Conv2D(num_priors * num_classes, kernel_size=(3,3), padding='same',name='conv8_2_mbox_conf')(net['conv8_2'])\r\n    net['conv8_2_mbox_conf'] = x\r\n    net['conv8_2_mbox_conf_flat'] = Flatten(name='conv8_2_mbox_conf_flat')(net['conv8_2_mbox_conf'])\r\n\r\n    priorbox = PriorBox(input_shape, anchors_size[4], max_size=anchors_size[5], aspect_ratios=[2],\r\n                        variances=[0.1, 0.1, 0.2, 0.2],\r\n                        name='conv8_2_mbox_priorbox')\r\n    net['conv8_2_mbox_priorbox'] = priorbox(net['conv8_2'])\r\n\r\n    # \u5bf9conv9_2\u8fdb\u884c\u5904\u7406\r\n    # 1,1,256\r\n    num_priors = 4\r\n    # \u9884\u6d4b\u6846\u7684\u5904\u7406\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0c4\u662fx,y,h,w\u7684\u8c03\u6574\r\n    x = Conv2D(num_priors * 4, kernel_size=(3,3), padding='same',name='conv9_2_mbox_loc')(net['conv9_2'])\r\n    net['conv9_2_mbox_loc'] = x\r\n    net['conv9_2_mbox_loc_flat'] = Flatten(name='conv9_2_mbox_loc_flat')(net['conv9_2_mbox_loc'])\r\n    # num_priors\u8868\u793a\u6bcf\u4e2a\u7f51\u683c\u70b9\u5148\u9a8c\u6846\u7684\u6570\u91cf\uff0cnum_classes\u662f\u6240\u5206\u7684\u7c7b\r\n    x = Conv2D(num_priors * num_classes, kernel_size=(3,3), padding='same',name='conv9_2_mbox_conf')(net['conv9_2'])\r\n    net['conv9_2_mbox_conf'] = x\r\n    net['conv9_2_mbox_conf_flat'] = Flatten(name='conv9_2_mbox_conf_flat')(net['conv9_2_mbox_conf'])\r\n    \r\n    priorbox = PriorBox(input_shape, anchors_size[5], max_size=anchors_size[6], aspect_ratios=[2],\r\n                        variances=[0.1, 0.1, 0.2, 0.2],\r\n                        name='conv9_2_mbox_priorbox')\r\n\r\n    net['conv9_2_mbox_priorbox'] = priorbox(net['conv9_2'])\r\n\r\n    # \u5c06\u6240\u6709\u7ed3\u679c\u8fdb\u884c\u5806\u53e0\r\n    #\u6bcf\u4e2a\u6846\u7684\u4f4d\u7f6e\r\n    net['mbox_loc'] = Concatenate(axis=1, name='mbox_loc')([net['conv4_3_norm_mbox_loc_flat'],\r\n                             net['fc7_mbox_loc_flat'],\r\n                             net['conv6_2_mbox_loc_flat'],\r\n                             net['conv7_2_mbox_loc_flat'],\r\n                             net['conv8_2_mbox_loc_flat'],\r\n                             net['conv9_2_mbox_loc_flat']])     #34928\r\n                            \r\n    net['mbox_conf'] = Concatenate(axis=1, name='mbox_conf')([net['conv4_3_norm_mbox_conf_flat'],\r\n                              net['fc7_mbox_conf_flat'],\r\n                              net['conv6_2_mbox_conf_flat'],\r\n                              net['conv7_2_mbox_conf_flat'],\r\n                              net['conv8_2_mbox_conf_flat'],\r\n                              net['conv9_2_mbox_conf_flat']])\r\n                             \r\n    z = net['mbox_priorbox'] = Concatenate(axis=1, name='mbox_priorbox')([net['conv4_3_norm_mbox_priorbox'],\r\n                                  net['fc7_mbox_priorbox'],\r\n                                  net['conv6_2_mbox_priorbox'],\r\n                                  net['conv7_2_mbox_priorbox'],\r\n                                  net['conv8_2_mbox_priorbox'],\r\n                                  net['conv9_2_mbox_priorbox']])\r\n                                  \r\n    # 8732,4\r\n    x = net['mbox_loc'] = Reshape((-1, 4),name='mbox_loc_final')(net['mbox_loc'])\r\n    # 8732,21\r\n    y = net['mbox_conf'] = Reshape((-1, num_classes),name='mbox_conf_logits')(net['mbox_conf'])\r\n    # 8732,8\r\n    net['mbox_conf'] = Activation('softmax',name='mbox_conf_final')(net['mbox_conf'])     #\u4f7f\u7528\u4e86softmax\u6fc0\u6d3b\u51fd\u6570\r\n    # 8732,33\r\n    net['predictions'] = Concatenate(axis=2, name='predictions')([net['mbox_loc'],\r\n                               net['mbox_conf'],\r\n                               net['mbox_priorbox']])\r\n    if(training == True):\r\n         return Model(net['input'], net['predictions'])\r\n    else:\r\n        net['result'] = Lambda(get_real_result)([x,y,z])\r\n        return Model(net['input'],net['result'])`", "`dependencies {\r\n\r\n    implementation 'androidx.appcompat:appcompat:1.2.0'\r\n    implementation 'com.google.android.material:material:1.3.0'\r\n    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'\r\n    implementation 'org.tensorflow:tensorflow-lite-metadata:0.1.0-rc1'\r\n    testImplementation 'junit:junit:4.+'\r\n    androidTestImplementation 'androidx.test.ext:junit:1.1.2'\r\n    androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'\r\n    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.1.0-rc1'\r\n}`", "I try to not return the vaild_detections. But it occurs a new issue.\r\n\r\nInternal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 142 (FlexCombinedNonMaxSuppression) failed to prepare.", "![image](https://user-images.githubusercontent.com/54882489/118107288-2699f600-b411-11eb-84e8-19a61e589461.png)\r\n", "Duplication of https://github.com/tensorflow/tensorflow/issues/49157"]}, {"number": 49153, "title": "using sampled softmax is slow in 'double-tower' like model #49145", "body": "I'm trying to implement a 'double tower' like recommendation model, as something described in paper 'Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations'. It mentioned to use 'in batch softmax' to learn the embedding. I think this 'in batch softmax' is very likely to the 'sampled softmax' used in word2vec. So I tried to use the tf.nn.nce_loss to implement this logic. I wrote some code try to train a simple model.\r\n\r\n    \r\n    graph = tf.Graph()\r\n    vocabulary_size = 300000\r\n    embedding_size = 128\r\n    dense_size = 64\r\n    batch_size = 256\r\n    \r\n    import math\r\n    \r\n    with graph.as_default():\r\n    # Input data.\r\n\r\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\r\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\r\n    \r\n\r\n    # Ops and variables pinned to the CPU because of missing GPU implementation\r\n    with tf.device('/cpu:0'):\r\n        # Look up embeddings for inputs.\r\n        embeddings = tf.Variable(\r\n            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\r\n        user = tf.nn.embedding_lookup(embeddings, train_inputs)\r\n        user = tf.layers.dense(user, dense_size, activation='relu')\r\n        user = tf.nn.l2_normalize(user, axis=-1)\r\n        print(user)\r\n        \r\n        right_embedding = tf.Variable(\r\n            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\r\n        right_hidden = tf.layers.dense(right_embedding, dense_size, activation='relu')\r\n        \r\n        \r\n        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),dtype=tf.float32,trainable=False)\r\n\r\n        loss = tf.reduce_mean(\r\n            tf.nn.nce_loss(weights=right_hidden,biases=nce_biases, inputs=user, labels=train_labels,\r\n                 num_sampled=num_sampled, num_classes=vocabulary_size))\r\n        \r\n        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\r\n        init = tf.global_variables_initializer()\r\n\r\nand I found that the training is really slow. **I doubt that it's because every time tensorflow need to calculate the right_hidden = tf.layers.dense(right_embedding, dense_size, activation='relu'), and the sampled softmax only applies to the last matrix \"right_hidden\", but we still need to calculate the entire right_hidden matrix beforehand**. So am I right? or anyone can give a better solution to deal with the 'sampled softmax' or 'in batch softmax'. thanks \r\n\r\nAnd here is the colab link to reproduce the result: \r\nhttps://colab.research.google.com/drive/11prB0qNvxXAiE-zzD9TtBn4OS8ozVeY_?usp=sharing", "comments": ["And here is the colab link to reproduce the result:\r\nhttps://colab.research.google.com/drive/11prB0qNvxXAiE-zzD9TtBn4OS8ozVeY_?usp=sharing", "I found tensorflow recommenders have already implement in batch softmax. https://www.tensorflow.org/recommenders"]}, {"number": 49152, "title": "[Intel MKL] Parallelize UnsortedSegmentSum on CPU deivce", "body": "`UnsortedSegmentSum` is a single thread op on CPU, here use the Eigen device to parallelize it. It's updated from an old PR #26427 .\r\n\r\nI resubmit this PR because the original one got an internal crash and was reverted. Because I can't get the failure case, I just went through the old code and found a potential rounding issue here: https://github.com/Intel-tensorflow/tensorflow/blob/52a6cfddef9b51b608b4a554b77a10e1522d56ec/tensorflow/core/kernels/segment_reduction_ops.cc#L419-L425\r\n\r\nThis PR uses a simple and reasonable way to balance workload like what TF usually did:\r\n```c++\r\n    // Each output row may contain different size of reduction from inputs,\r\n    // balance the workload to a task group. Each task is output row index.\r\n    const int64 kMaxTaskBlock =\r\n        std::min(num_reductions, (int64)cpu_device.numThreads());\r\n    const int64 kAverTaskSize = (N + kMaxTaskBlock - 1) / kMaxTaskBlock\r\n```\r\nOther parts are the same as the original one.\r\n\r\n\r\nPerformance improvement:\r\n* Origin\r\n```\r\nBM_UnsortedSegmentSum_4096_128_1_ 300315 2279 6983.2MB/s\r\nBM_UnsortedSegmentSum_4096_128_128_ 306408 2268 6844.3MB/s\r\n```\r\n\r\n* Optimized\r\n```\r\nBM_UnsortedSegmentSum_4096_128_1_ 302769 2268 6926.6MB/s\r\nBM_UnsortedSegmentSum_4096_128_128_ 116179 6112 18051.1MB/s\r\n```\r\n\r\nThis patch tries to parallel UnsortSegmentReduction with num_segments, so the performance has no change if num_segments is 1, otherwise it has ~3X improvement in the benchmark.", "comments": ["Here's a simple introduce for the optimization:\r\n1. `row_counter` stores how many rows will be reduce to each output row, `num_reductions` is the real `num_segments` which excluded 0 count rows, it also decides the degree of parallelism.\r\n2. Loop `row_counter` to pick each task to `block_range`. `block_range` records task index for shard function.\r\n![image](https://user-images.githubusercontent.com/38638514/56342254-125f7c80-61ea-11e9-9e31-856b50995d77.png)\r\n ", "@ezhulenev can you help to review this again?", "It was rolled back because of internal failures, looks like it produces nans. Any ideas? Will try to get better error messages tomorrow.\r\n\r\nError message:\r\n\r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [Condition x >= y did not hold element-wise:] [x (<...internal_node_name...>/Sum_2:0) = ] [-nan -nan -nan...]\r\n```", "Thank for your reminding, I found it may cause accuracy issue(including NaN) when `task_index` is 0 but exited from here: https://github.com/tensorflow/tensorflow/pull/49152/files#diff-d29812caca45335066ddacf8fb06afc416f1f557bc446068ebaa163495e0b03bR388-R390. The older PR started from index 1 but the new PR started from index 0 to simplify the code, and seems it caused new issue.\r\nI will make a related case to test locally first, and update it later."]}, {"number": 49151, "title": "tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f8ef0275730> triggered tf.function retracing.", "body": "`tf version: 2.4.1`\r\n```\r\n\r\nimport tensorflow as tf\r\nimport pandas as pd \r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.callbacks import *\r\nimport tensorflow_hub as hub\r\nimport tensorflow_text as text\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow import keras\r\n\r\nclass MultiHeadSelfAttention(layers.Layer):\r\n    def __init__(self, embed_dim, num_heads=8):\r\n        super(MultiHeadSelfAttention, self).__init__()\r\n        self.embed_dim = embed_dim\r\n        self.num_heads = num_heads\r\n        assert embed_dim % num_heads == 0\r\n        #if embed_dim % num_heads != 0:\r\n        #    raise ValueError(f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\")\r\n        self.projection_dim = embed_dim // num_heads\r\n        self.query_dense = layers.Dense(embed_dim)\r\n        self.key_dense = layers.Dense(embed_dim)\r\n        self.value_dense = layers.Dense(embed_dim)\r\n        self.combine_heads = layers.Dense(embed_dim)\r\n\r\n    def attention(self, query, key, value):\r\n        score = tf.matmul(query, key, transpose_b=True)\r\n        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\r\n        scaled_score = score / tf.math.sqrt(dim_key)\r\n        weights = tf.nn.softmax(scaled_score, axis=-1)\r\n        output = tf.matmul(weights, value)\r\n        return output, weights\r\n\r\n    def separate_heads(self, x, batch_size):\r\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, inputs):\r\n        # x.shape = [batch_size, seq_len, embedding_dim]\r\n        batch_size = tf.shape(inputs)[0]\r\n        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\r\n        query = self.separate_heads(\r\n            query, batch_size\r\n        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n        key = self.separate_heads(\r\n            key, batch_size\r\n        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n        value = self.separate_heads(\r\n            value, batch_size\r\n        )  # (batch_size, num_heads, seq_len, projection_dim)\r\n        attention, weights = self.attention(query, key, value)\r\n        attention = tf.transpose(\r\n            attention, perm=[0, 2, 1, 3]\r\n        )  # (batch_size, seq_len, num_heads, projection_dim)\r\n        concat_attention = tf.reshape(\r\n            attention, (batch_size, -1, self.embed_dim)\r\n        )  # (batch_size, seq_len, embed_dim)\r\n        output = self.combine_heads(\r\n            concat_attention\r\n        )  # (batch_size, seq_len, embed_dim)\r\n        return output\r\n\r\nclass TransformerBlock(layers.Layer):\r\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\r\n        super(TransformerBlock, self).__init__()  \r\n        if tf.__version__.startswith('2.4') or tf.__version__.startswith('2.5'):        \r\n            self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\r\n        else:\r\n            self.att = MultiHeadSelfAttention(num_heads=num_heads, embed_dim=embed_dim)\r\n        self.ffn = keras.Sequential(\r\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\r\n        )\r\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\r\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\r\n        self.dropout1 = layers.Dropout(rate)\r\n        self.dropout2 = layers.Dropout(rate)\r\n\r\n    def call(self, inputs, training):\r\n        if tf.__version__.startswith('2.4') or tf.__version__.startswith('2.5'): \r\n            attn_output = self.att(inputs, inputs)\r\n        else:\r\n            attn_output = self.att(inputs)\r\n        attn_output = self.dropout1(attn_output, training=training)\r\n        out1 = self.layernorm1(inputs + attn_output)\r\n        ffn_output = self.ffn(out1)\r\n        ffn_output = self.dropout2(ffn_output, training=training)\r\n        return self.layernorm2(out1 + ffn_output)\r\n\r\n\r\nclass TokenAndPositionEmbedding(layers.Layer):\r\n    def __init__(self, maxlen, vocab_size, embed_dim):\r\n        super(TokenAndPositionEmbedding, self).__init__()\r\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\r\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\r\n\r\n    def call(self, x):\r\n        maxlen = tf.shape(x)[-1]\r\n        positions = tf.range(start=0, limit=maxlen, delta=1)\r\n        positions = self.pos_emb(positions)\r\n        x = self.token_emb(x)\r\n        return x + positions\r\n\r\npreprocessor_file = \"./albert_en_preprocess_3\" # https://tfhub.dev/tensorflow/albert_en_preprocess/3\r\npreprocessor_layer = hub.KerasLayer(preprocessor_file)\r\n\r\n\r\ndef get_model_transormer(num_classes):\r\n    embed_dim = 32  # Embedding size for each token\r\n    num_heads = 2  # Number of attention heads\r\n    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\r\n    \r\n    preprocessor = hub.load(preprocessor_file)\r\n    vocab_size = preprocessor.tokenize.get_special_tokens_dict()['vocab_size'].numpy()\r\n\r\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string) \r\n\r\n    encoder_inputs = preprocessor_layer(text_input)['input_word_ids']\r\n\r\n    embedding_layer = TokenAndPositionEmbedding(encoder_inputs.shape[1], vocab_size, embed_dim)\r\n    x = embedding_layer(encoder_inputs)\r\n    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\r\n    x = transformer_block(x)\r\n    x = layers.GlobalAveragePooling1D()(x)\r\n    x = layers.Dense(32, activation=\"relu\")(x)\r\n    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\r\n\r\n    #outputs = layers.Dense(1, activation=\"sigmoid\")(x)\r\n    model = keras.Model(inputs=text_input, outputs=outputs)\r\n\r\n    model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"acc\"])\r\n    #model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\r\n    return model\r\n```\r\nwhen I run the initialization code below\r\n`model = get_model_transormer(4)`\r\n\r\nthere comes the warning info:\r\n\r\n> WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7f51284a4ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\r\n\r\n\r\n\r\n\r\nHow to fix my code to avoid this warning ?", "comments": ["@yananchen1989 , The reason for `re-tracing `might be due to the `variable input lengths/sequence lengths` or `variable batch size`(last batch size might be smaller). \r\nTo avoid `re-tracing` you can use `input_signature` in your `tf.function`.\r\nYou can follow [this](https://www.tensorflow.org/tutorials/text/transformer#training_and_checkpointing) document which has the example for same.", "I cannot reproduce on Colab with `pip install --upgrade tensorflow-text==2.5.0-rc0 tensorflow-hub`", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49151\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49151\">No</a>\n"]}, {"number": 49147, "title": "Training grouped Conv2D is slow", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f, 2.4.1\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 11 / 8.0\r\n- GPU model and memory: Titan RTX, 24 GB\r\n\r\n**Describe the current behavior**\r\nTraining the two networks defined below takes almost the same amount of time, despite GROUPED using groups=8, which should cut down the FLOPS by ~8x compared to REGULAR. \r\n\r\n**Describe the expected behavior**\r\nTraining the two networks should take radically different amounts of time, something closer to ~3 seconds for GROUPED vs ~19 seconds for REGULAR. \r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport sys\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(f'sys.version_info: {sys.version_info}')  # sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\r\nprint(f'tf.version.VERSION: {tf.version.VERSION}')  # 2.4.1\r\nprint(f'tf.version.GIT_VERSION: {tf.version.GIT_VERSION}')  # v2.4.0-49-g85c8b2a817f\r\n\r\nsteps = 50\r\nbatch_size = 16\r\ninput_w = 224\r\ninput_shape = (input_w, input_w, 3)\r\n\r\nmodel_grouped = tf.keras.Sequential(layers=[\r\n    tf.keras.layers.Input(shape=input_shape, batch_size=batch_size),\r\n    tf.keras.layers.Conv2D(128, 1, padding='same'),\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 0\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 1\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 2\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 3\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 4\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 5\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 6\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 7\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 8\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 9\r\n    tf.keras.layers.GlobalMaxPooling2D(),\r\n    tf.keras.layers.Dense(2),\r\n    tf.keras.layers.Activation('softmax')], name=\"Grouped\")\r\n\r\nmodel_regular = tf.keras.Sequential(layers=[\r\n    tf.keras.layers.Input(shape=input_shape, batch_size=batch_size),\r\n    tf.keras.layers.Conv2D(128, 1, padding='same'),\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 0\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 1\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 2\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 3\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 4\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 5\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 6\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 7\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 8\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 9\r\n    tf.keras.layers.GlobalMaxPooling2D(),\r\n    tf.keras.layers.Dense(2),\r\n    tf.keras.layers.Activation('softmax')], name=\"Regular\")\r\n\r\nmodel_grouped.compile(optimizer=tf.optimizers.SGD(0.1, 0.8, True), loss=tf.keras.losses.BinaryCrossentropy())\r\nmodel_regular.compile(optimizer=tf.optimizers.SGD(0.1, 0.8, True), loss=tf.keras.losses.BinaryCrossentropy())\r\n\r\nimages = np.full((2 * batch_size, input_w, input_w, 3), 0.5)\r\nlabels = np.full((2 * batch_size, 2), 1)\r\n\r\nprint(\"Warming up\")\r\nmodel_grouped.fit(images, labels, batch_size=batch_size)\r\nmodel_regular.fit(images, labels, batch_size=batch_size)\r\nprint(\"Warmup completed\")\r\n\r\nimages = np.full((steps * batch_size, input_w, input_w, 3), 0.5)\r\nlabels = np.full((steps * batch_size, 2), 1)\r\n\r\nprint(\"Training GROUPED model\")\r\nt0 = time.time()\r\nmodel_grouped.fit(images, labels,\r\n                  batch_size=batch_size)  # 50/50 [==============================] - 16s 317ms/step - loss: 0.0573\r\ndelta = time.time() - t0\r\nprint(f\"Trained GROUPED in {delta: .3f} seconds\")  # Trained GROUPED in  16.019 seconds\r\n\r\nprint(\"Training REGULAR model\")\r\nt0 = time.time()\r\nmodel_regular.fit(images, labels,\r\n                  batch_size=batch_size)  # 50/50 [==============================] - 19s 387ms/step - loss: 0.0000e+00\r\ndelta = time.time() - t0\r\nprint(f\"Trained REGULAR in {delta: .3f} seconds\")  # Trained REGULAR in  19.492 seconds\r\n```\r\n\r\n**Other info / logs** \r\n```\r\nsys.version_info: sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\r\ntf.version.VERSION: 2.4.1\r\ntf.version.GIT_VERSION: v2.4.0-49-g85c8b2a817f\r\n2021-05-12 11:07:17.540364: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-05-12 11:07:17.541021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-05-12 11:07:17.567324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-05-12 11:07:17.567480: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-05-12 11:07:17.572104: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-05-12 11:07:17.572193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-05-12 11:07:17.574848: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-05-12 11:07:17.575756: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-05-12 11:07:17.581969: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-05-12 11:07:17.583916: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-05-12 11:07:17.584412: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-05-12 11:07:17.584543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-05-12 11:07:17.584822: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-05-12 11:07:17.586613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: TITAN RTX computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-05-12 11:07:17.587022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-05-12 11:07:17.587220: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-05-12 11:07:17.587398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-05-12 11:07:17.587488: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-05-12 11:07:17.587565: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-05-12 11:07:17.587640: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2021-05-12 11:07:17.587707: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-05-12 11:07:17.587779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-05-12 11:07:17.587909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2021-05-12 11:07:18.069972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-05-12 11:07:18.070055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \r\n2021-05-12 11:07:18.070100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \r\n2021-05-12 11:07:18.070261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21843 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:08:00.0, compute capability: 7.5)\r\n2021-05-12 11:07:18.070704: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nWarming up\r\n2021-05-12 11:07:18.849525: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-05-12 11:07:19.185194: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-05-12 11:07:19.547236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-05-12 11:07:19.554048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-05-12 11:07:20.340878: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2021-05-12 11:07:20.377477: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n\r\n2/2 [==============================] - 6s 311ms/step - loss: 0.6704\r\n2/2 [==============================] - 2s 388ms/step - loss: 0.5845\r\nWarmup completed\r\nTraining GROUPED model\r\n50/50 [==============================] - 16s 317ms/step - loss: 0.0573\r\nTrained GROUPED in  16.019 seconds\r\nTraining REGULAR model\r\n50/50 [==============================] - 19s 387ms/step - loss: 0.0000e+00\r\nTrained REGULAR in  19.492 seconds\r\n\r\nProcess finished with exit code 0\r\n```\r\n", "comments": ["Inference is also slow-ish, but not quite as drastic: \r\n```python\r\nt0 = time.time()\r\nmodel_grouped.predict(images, batch_size=batch_size, verbose=1)  # 50/50 [==============================] - 4s 78ms/step\r\nprint(f\"Predicted GROUPED in {time.time() - t0: .3f} seconds\")  # Predicted GROUPED in  4.148 seconds\r\n\r\nt0 = time.time()\r\nmodel_regular.predict(images, batch_size=batch_size,\r\n                      verbose=1)  # 50/50 [==============================] - 7s 135ms/step\r\nprint(f\"Predicted REGULAR in {time.time() - t0: .3f} seconds\")  # Predicted REGULAR in  6.825 seconds\r\n```\r\n\r\n78 / 135 = 58%, when it could probably be closer to 20%?\r\n", "Have you tried with 2.5rc/nightly?", "I have not, no. \r\n\r\nRunning the provided script in existing environment: 1-3 minutes. \r\n\r\nInstalling 2.5rc/nightly + correct CUDA & cuDNN + get it to work and hopefully not bork my current setup: 1-3 hours.\r\n\r\n:-/", "You could try to run it in the nightly container https://www.tensorflow.org/install/docker", "Alas, I have never used docker... :-/", "Does google colab have tf2.5rc/nightly?", "Yes but you need to run `pip install tf-nightly`", "I added a code block with: \r\n```python\r\npip install tf-nightly\r\n```\r\nand it ran seemingly fine: \r\n```\r\nCollecting tf-nightly\r\n  Downloading https://files.pythonhosted.org/packages/a5/47/b6c6ee722ed748fb184f5ecbde0fc1c49422defd8f6cb890d70b5ffeecc0/tf_nightly-2.6.0.dev20210512-cp37-cp37m-manylinux2010_x86_64.whl (453.3MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 453.3MB 29kB/s \r\nRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.36.2)\r\nRequirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12.1)\r\nRequirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.7.4.3)\r\nCollecting tb-nightly~=2.6.0.a\r\n  Downloading https://files.pythonhosted.org/packages/48/a2/6ca4c89580a0fc0d42f7ef47233c2d51163bbea21ed0d9623608b6b28b85/tb_nightly-2.6.0a20210512-py3-none-any.whl (5.9MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.9MB 46.3MB/s \r\nRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.12.0)\r\nRequirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.12)\r\nCollecting grpcio<2.0,>=1.37.0\r\n  Downloading https://files.pythonhosted.org/packages/31/d8/1bfe90cc49c166dd2ec1be46fa4830c254ce702004a110830c74ec1df0c0/grpcio-1.37.1-cp37-cp37m-manylinux2014_x86_64.whl (4.2MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.2MB 45.5MB/s \r\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.2)\r\nCollecting keras-nightly~=2.6.0.dev\r\n  Downloading https://files.pythonhosted.org/packages/52/d4/c4edc8d4060e2ab33205c50dc2098f8eccf49bb7660a5cf14064b0ee0888/keras_nightly-2.6.0.dev2021051200-py2.py3-none-any.whl (1.3MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.3MB 44.3MB/s \r\nCollecting tf-estimator-nightly~=2.5.0.dev\r\n  Downloading https://files.pythonhosted.org/packages/24/6c/9bf4a6004d18c8e543845d3416e50f36dd09d272161e2fb0db5678132dfd/tf_estimator_nightly-2.5.0.dev2021032601-py2.py3-none-any.whl (462kB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 471kB 48.5MB/s \r\nRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.12.4)\r\nRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.19.5)\r\nRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.15.0)\r\nRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (3.3.0)\r\nRequirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.6.3)\r\nRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (1.1.0)\r\nCollecting h5py~=3.1.0\r\n  Downloading https://files.pythonhosted.org/packages/9d/74/9eae2bedd8201ab464308f42c601a12d79727a1c87f0c867fdefb212c6cf/h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0MB 39.3MB/s \r\nCollecting gast==0.4.0\r\n  Downloading https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl\r\nRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly) (0.2.0)\r\nRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.0.1)\r\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\r\n  Downloading https://files.pythonhosted.org/packages/60/f9/802efd84988bffd9f644c03b6e66fde8e76c3aa33db4279ddd11c5d61f4b/tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.9MB 34.7MB/s \r\nRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.30.0)\r\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (3.3.4)\r\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (56.1.0)\r\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (1.8.0)\r\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (0.4.4)\r\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.6.0.a->tf-nightly) (2.23.0)\r\nCollecting cached-property; python_version < \"3.8\"\r\n  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\r\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.2.2)\r\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (4.7.2)\r\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.2.8)\r\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (4.0.1)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (1.3.0)\r\nRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2.10)\r\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (2020.12.5)\r\nRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (3.0.4)\r\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.6.0.a->tf-nightly) (1.24.3)\r\nRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tb-nightly~=2.6.0.a->tf-nightly) (0.4.8)\r\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly~=2.6.0.a->tf-nightly) (3.4.1)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.6.0.a->tf-nightly) (3.1.0)\r\nERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.4.0 which is incompatible.\r\nERROR: tensorflow 2.4.1 has requirement grpcio~=1.32.0, but you'll have grpcio 1.37.1 which is incompatible.\r\nERROR: tensorflow 2.4.1 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\r\nInstalling collected packages: tensorboard-data-server, grpcio, tb-nightly, keras-nightly, tf-estimator-nightly, cached-property, h5py, gast, tf-nightly\r\n  Found existing installation: grpcio 1.32.0\r\n    Uninstalling grpcio-1.32.0:\r\n      Successfully uninstalled grpcio-1.32.0\r\n  Found existing installation: h5py 2.10.0\r\n    Uninstalling h5py-2.10.0:\r\n      Successfully uninstalled h5py-2.10.0\r\n  Found existing installation: gast 0.3.3\r\n    Uninstalling gast-0.3.3:\r\n      Successfully uninstalled gast-0.3.3\r\nSuccessfully installed cached-property-1.5.2 gast-0.4.0 grpcio-1.37.1 h5py-3.1.0 keras-nightly-2.6.0.dev2021051200 tb-nightly-2.6.0a20210512 tensorboard-data-server-0.6.1 tf-estimator-nightly-2.5.0.dev2021032601 tf-nightly-2.6.0.dev20210512\r\n```\r\n\r\nThen I ran the script: \r\n```python\r\nimport sys\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(\r\n    f'sys.version_info: {sys.version_info}')  # sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\r\nprint(f'tf.version.VERSION: {tf.version.VERSION}')  # 2.4.1\r\nprint(f'tf.version.GIT_VERSION: {tf.version.GIT_VERSION}')  # v2.4.0-49-g85c8b2a817f\r\n\r\nsteps = 50\r\nbatch_size = 16\r\ninput_w = 224\r\ninput_shape = (input_w, input_w, 3)\r\n\r\nmodel_grouped = tf.keras.Sequential(layers=[\r\n    tf.keras.layers.Input(shape=input_shape, batch_size=batch_size),\r\n    tf.keras.layers.Conv2D(128, 1, padding='same'),\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 0\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 1\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 2\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 3\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 4\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 5\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 6\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 7\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 8\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', groups=8),  # 9\r\n    tf.keras.layers.GlobalMaxPooling2D(),\r\n    tf.keras.layers.Dense(2),\r\n    tf.keras.layers.Activation('softmax')], name=\"Grouped\")\r\n\r\nmodel_regular = tf.keras.Sequential(layers=[\r\n    tf.keras.layers.Input(shape=input_shape, batch_size=batch_size),\r\n    tf.keras.layers.Conv2D(128, 1, padding='same'),\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 0\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 1\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 2\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 3\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 4\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 5\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 6\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 7\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 8\r\n    tf.keras.layers.Conv2D(128, 3, padding='same'),  # 9\r\n    tf.keras.layers.GlobalMaxPooling2D(),\r\n    tf.keras.layers.Dense(2),\r\n    tf.keras.layers.Activation('softmax')], name=\"Regular\")\r\n\r\nmodel_grouped.compile(optimizer=tf.optimizers.SGD(0.1, 0.8, True), loss=tf.keras.losses.BinaryCrossentropy())\r\nmodel_regular.compile(optimizer=tf.optimizers.SGD(0.1, 0.8, True), loss=tf.keras.losses.BinaryCrossentropy())\r\n\r\nimages = np.full((2 * batch_size, input_w, input_w, 3), 0.5)\r\nlabels = np.full((2 * batch_size, 2), 1)\r\n\r\nprint(\"Warming up\")\r\nmodel_grouped.fit(images, labels, batch_size=batch_size)\r\nmodel_regular.fit(images, labels, batch_size=batch_size)\r\nprint(\"Warmup completed\")\r\n\r\nimages = np.full((steps * batch_size, input_w, input_w, 3), 0.5)\r\nlabels = np.full((steps * batch_size, 2), 1)\r\n\r\nprint(\"Training GROUPED model\")\r\nt0 = time.time()\r\nmodel_grouped.fit(images, labels,\r\n                  batch_size=batch_size)  # 50/50 [==============================] - 16s 317ms/step - loss: 0.0573\r\nprint(f\"Trained GROUPED in {time.time() - t0: .3f} seconds\")  # Trained GROUPED in  16.019 seconds\r\n\r\nprint(\"Training REGULAR model\")\r\nt0 = time.time()\r\nmodel_regular.fit(images, labels,\r\n                  batch_size=batch_size)  # 50/50 [==============================] - 19s 387ms/step - loss: 0.0000e+00\r\nprint(f\"Trained REGULAR in {time.time() - t0: .3f} seconds\")  # Trained REGULAR in  19.492 seconds\r\n\r\nt0 = time.time()\r\nmodel_grouped.predict(images, batch_size=batch_size, verbose=1)  # 50/50 [==============================] - 4s 78ms/step\r\nprint(f\"Predicted GROUPED in {time.time() - t0: .3f} seconds\")  # Predicted GROUPED in  4.148 seconds\r\n\r\nt0 = time.time()\r\nmodel_regular.predict(images, batch_size=batch_size,\r\n                      verbose=1)  # 50/50 [==============================] - 7s 135ms/step\r\nprint(f\"Predicted REGULAR in {time.time() - t0: .3f} seconds\")  # Predicted REGULAR in  6.825 seconds\r\n```\r\n\r\nwhich didn't work: \r\n```\r\nsys.version_info: sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\r\ntf.version.VERSION: 2.6.0-dev20210512\r\ntf.version.GIT_VERSION: v1.12.1-56494-gf1b217420e9\r\nWARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\r\nWARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\r\nWarming up\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\n<ipython-input-2-dde23b833467> in <module>()\r\n     55 \r\n     56 print(\"Warming up\")\r\n---> 57 model_grouped.fit(images, labels, batch_size=batch_size)\r\n     58 model_regular.fit(images, labels, batch_size=batch_size)\r\n     59 print(\"Warmup completed\")\r\n\r\n6 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nUnimplementedError:  Fused conv implementation does not support grouped convolutions for now.\r\n\t [[node Grouped/conv2d_1/BiasAdd (defined at <ipython-input-2-dde23b833467>:57) ]] [Op:__inference_train_function_1398]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\n\r\nSo I guess it went from being not as fast as it ought to be to not working at all ;) ", "I tried `use_bias=False`: \r\n```python\r\nimport sys\r\nimport time\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(\r\n    f'sys.version_info: {sys.version_info}')  # sys.version_info(major=3, minor=7, micro=7, releaselevel='final', serial=0)\r\nprint(f'tf.version.VERSION: {tf.version.VERSION}')  # 2.4.1\r\nprint(f'tf.version.GIT_VERSION: {tf.version.GIT_VERSION}')  # v2.4.0-49-g85c8b2a817f\r\n\r\nsteps = 50\r\nbatch_size = 16\r\ninput_w = 224\r\ninput_shape = (input_w, input_w, 3)\r\n\r\nmodel_grouped = tf.keras.Sequential(layers=[\r\n    tf.keras.layers.InputLayer(input_shape=input_shape, batch_size=batch_size),\r\n    tf.keras.layers.Conv2D(128, 1, padding='same', use_bias=False),\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 0\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 1\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 2\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 3\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 4\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 5\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 6\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 7\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 8\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False, groups=8),  # 9\r\n    tf.keras.layers.GlobalMaxPooling2D(),\r\n    tf.keras.layers.Dense(2),\r\n    tf.keras.layers.Activation('softmax')], name=\"Grouped\")\r\n\r\nmodel_regular = tf.keras.Sequential(layers=[\r\n    tf.keras.layers.InputLayer(input_shape=input_shape, batch_size=batch_size),\r\n    tf.keras.layers.Conv2D(128, 1, padding='same', use_bias=False),\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 0\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 1\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 2\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 3\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 4\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 5\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 6\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 7\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 8\r\n    tf.keras.layers.Conv2D(128, 3, padding='same', use_bias=False),  # 9\r\n    tf.keras.layers.GlobalMaxPooling2D(),\r\n    tf.keras.layers.Dense(2),\r\n    tf.keras.layers.Activation('softmax')], name=\"Regular\")\r\n\r\nmodel_grouped.compile(optimizer=tf.optimizers.SGD(0.1, 0.8, True), loss=tf.keras.losses.BinaryCrossentropy())\r\nmodel_regular.compile(optimizer=tf.optimizers.SGD(0.1, 0.8, True), loss=tf.keras.losses.BinaryCrossentropy())\r\n\r\nimages = np.full((2 * batch_size, input_w, input_w, 3), 0.5)\r\nlabels = np.full((2 * batch_size, 2), 1)\r\n\r\nprint(\"Warming up\")\r\nmodel_grouped.fit(images, labels, batch_size=batch_size)\r\nmodel_regular.fit(images, labels, batch_size=batch_size)\r\nprint(\"Warmup completed\")\r\n\r\nimages = np.full((steps * batch_size, input_w, input_w, 3), 0.5)\r\nlabels = np.full((steps * batch_size, 2), 1)\r\n\r\nprint(\"Training GROUPED model\")\r\nt0 = time.time()\r\nmodel_grouped.fit(images, labels,\r\n                  batch_size=batch_size)  # 50/50 [==============================] - 16s 317ms/step - loss: 0.0573\r\nprint(f\"Trained GROUPED in {time.time() - t0: .3f} seconds\")  # Trained GROUPED in  16.019 seconds\r\n\r\nprint(\"Training REGULAR model\")\r\nt0 = time.time()\r\nmodel_regular.fit(images, labels,\r\n                  batch_size=batch_size)  # 50/50 [==============================] - 19s 387ms/step - loss: 0.0000e+00\r\nprint(f\"Trained REGULAR in {time.time() - t0: .3f} seconds\")  # Trained REGULAR in  19.492 seconds\r\n\r\nt0 = time.time()\r\nmodel_grouped.predict(images, batch_size=batch_size, verbose=1)  # 50/50 [==============================] - 4s 78ms/step\r\nprint(f\"Predicted GROUPED in {time.time() - t0: .3f} seconds\")  # Predicted GROUPED in  4.148 seconds\r\n\r\nt0 = time.time()\r\nmodel_regular.predict(images, batch_size=batch_size,\r\n                      verbose=1)  # 50/50 [==============================] - 7s 135ms/step\r\nprint(f\"Predicted REGULAR in {time.time() - t0: .3f} seconds\")  # Predicted REGULAR in  6.825 seconds\r\n```\r\nBut ran into another error: \r\n```\r\nsys.version_info: sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\r\ntf.version.VERSION: 2.6.0-dev20210512\r\ntf.version.GIT_VERSION: v1.12.1-56494-gf1b217420e9\r\nWarming up\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-3-69340df0fa27> in <module>()\r\n     55 \r\n     56 print(\"Warming up\")\r\n---> 57 model_grouped.fit(images, labels, batch_size=batch_size)\r\n     58 model_regular.fit(images, labels, batch_size=batch_size)\r\n     59 print(\"Warmup completed\")\r\n\r\n6 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError:  filter_size does not have enough elements, requested 147456, got 18432\r\n\t [[node gradient_tape/Grouped/conv2d_32/Conv2D/Conv2DBackpropFilter (defined at <ipython-input-3-69340df0fa27>:57) ]] [Op:__inference_train_function_2432]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\nI'm not sure what to make of that error - the code as quoted runs on my machine (version details in original post) without any problems, and the error sounds like it's \"under the hood\" so to speak. ", "It seem broken in 2.5.0-rc3 and not working on tf-nightly.\r\n\r\n\r\nOn Colab 2.4.1 the diff it is not so huge.\r\n\r\n```\r\nsys.version_info: sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\r\ntf.version.VERSION: 2.4.1\r\ntf.version.GIT_VERSION: v2.4.1-0-g85c8b2a817f\r\nWarming up\r\n2/2 [==============================] - 41s 852ms/step - loss: 0.6857\r\n2/2 [==============================] - 4s 819ms/step - loss: 0.5777\r\nWarmup completed\r\nTraining GROUPED model\r\n50/50 [==============================] - 43s 870ms/step - loss: 0.2190\r\nTrained GROUPED in  43.703 seconds\r\nTraining REGULAR model\r\n50/50 [==============================] - 41s 826ms/step - loss: 0.0000e+00\r\nTrained REGULAR in  41.516 seconds\r\n50/50 [==============================] - 14s 289ms/step\r\nPredicted GROUPED in  14.391 seconds\r\n50/50 [==============================] - 14s 281ms/step\r\nPredicted REGULAR in  14.068 seconds\r\n\r\n```\r\n", "With tf-nightly on CPU I get:\r\n```\r\ntensorflow.python.framework.errors_impl.UnimplementedError:  Fused conv implementation does not support grouped convolutions for now.\r\n```\r\nThat I think it is https://github.com/tensorflow/tensorflow/issues/41107\r\n\r\n", "> On Colab 2.4.1 the diff it is not so huge.\r\n\r\nThat's the problem! Grouped should be almost 8x faster than regular, _yet it's actually slower_ in your test!", "Tried your code in Tensorflow 2.7 and having different error, I think it is related to [this](https://stackoverflow.com/questions/61796021/) issue, could you please update the same and let us know if the issue still persists. [Here](https://colab.research.google.com/gist/sachinprasadhs/ded4792be03d3b237f3603420b543fa6/49147.ipynb) is the gist for reference.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49146, "title": "tf.gradients with variable shaped inputs returns error (inverse extract volume patches)", "body": "**System information**\r\n- Google Colab\r\n- TF v2.4.1-0-g85c8b2a817f\r\n\r\nUsing the gradients (as suggested in [#6743](https://github.com/tensorflow/tensorflow/issues/6743#issuecomment-271969125) and on [stackoverflow](https://stackoverflow.com/a/51785735/12528320)) to compute the inverse of `tf.extract_volume_patches` works for statically known input shapes, however it does returns an error when used with variably input shapes.\r\n\r\nAn minimal example for synthetic 3D MNIST dataset, (it works when `input=(28,28,28,1)` however I need the extraction on variable sized data input (large biomedical images)):\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef gaussian(x, amp=1, mu=None, sig=None):\r\n    \"\"\" Gaussian function over d dimensions of x\r\n    \"\"\"\r\n    if mu is None:\r\n        mu = np.zeros_like(x)\r\n    if sig is None:\r\n        sig = np.ones_like(x)\r\n    return amp * np.exp(-np.sum(np.square(x - mu) / (2 * np.square(sig))))\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\nx_train = x_train.astype('float32') / 255.\r\nx_test = x_test.astype('float32') / 255.\r\n\r\n\r\nmultiplier = np.zeros((28,), dtype=np.float32)\r\nfor i in range(4):\r\n    multiplier[13-i] = round(gaussian(i),2)\r\n    multiplier[14+i] = round(gaussian(i),2)\r\nprint(multiplier)\r\n\r\nx_train = np.einsum('bhw,d->bdhw', x_train, multiplier)[...,np.newaxis]\r\n\r\n\r\nclass ExtractPatches(tf.keras.layers.Layer):\r\n    def __init__(self, ksizes, strides, shape):\r\n        super(ExtractPatches, self).__init__()\r\n        self.ksizes = ksizes\r\n        self.strides = strides\r\n        self.shape = shape\r\n\r\n    def call(self, inputs):\r\n        patches = tf.extract_volume_patches(inputs,\r\n                                        ksizes=self.ksizes,\r\n                                        strides=self.strides,\r\n                                        padding=\"VALID\")\r\n        return tf.reshape(patches, self.shape), tf.shape(inputs)\r\n\r\nclass CombinePatches(tf.keras.layers.Layer):\r\n    def __init__(self, ksizes, strides):\r\n        super(CombinePatches, self).__init__()\r\n        self.ksizes = ksizes\r\n        self.strides = strides\r\n\r\n    def call(self, patches, inputs):\r\n        target_volume = tf.zeros_like(inputs)\r\n        target_patches = tf.extract_volume_patches(\r\n            target_volume,\r\n            ksizes=self.ksizes,\r\n            strides=self.strides,\r\n            padding=\"VALID\"\r\n        )\r\n        # Creates list of gradient mappings from patches to target shape\r\n        # Patches without overlap get 1, elements that overlap receive 1 \r\n        # times the number of overlaps.\r\n        target_grad_mapping = tf.gradients(target_patches, target_volume)[0]\r\n\r\n        # Computes gradients again and dividing by grad, otherwise its just summed.\r\n        return tf.gradients(target_patches, target_volume, patches)[0] / target_grad_mapping\r\n\r\n\r\ndef create_model():\r\n    inputs = tf.keras.layers.Input(shape=(None,None,None,1))\r\n    patches, shape = ExtractPatches(ksizes=[1,14,14,14,1], strides=[1,14,14,14,1], shape=(-1,14,14,14,1))(inputs)\r\n    encoded = tf.keras.layers.Conv3D(filters=28, kernel_size=(14,14,14), strides=(14,14,14))(patches)\r\n    decoded = tf.keras.layers.Conv3DTranspose(filters=1, kernel_size=(14,14,14), strides=(14,14,14))(encoded)\r\n    merged = CombinePatches(ksizes=[1,14,14,14,1], strides=[1,14,14,14,1])(decoded, inputs)\r\n\r\n    return tf.keras.models.Model(inputs=inputs, outputs=merged)\r\n\r\nae = create_model()\r\n\r\nae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\r\n           loss=tf.keras.losses.MeanSquaredError(),\r\n           metrics=['accuracy'])\r\n\r\n\r\ntest_history = ae.fit(x_train,\r\n                       x_train,\r\n                       batch_size=1,\r\n                       epochs=1,\r\n                       callbacks=None)\r\n\r\n```\r\n## Stacktrace\r\n```python\r\n---------------------------------------------------------------------------\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n\r\n<ipython-input-5-733ae79232be> in <module>()\r\n     55 \r\n     56 \r\n---> 57 ae = create_model()\r\n     58 \r\n     59 ae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\r\n\r\n5 frames\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    668       except Exception as e:  # pylint:disable=broad-except\r\n    669         if hasattr(e, 'ag_error_metadata'):\r\n--> 670           raise e.ag_error_metadata.to_exception(e)\r\n    671         else:\r\n    672           raise\r\n\r\nTypeError: in user code:\r\n\r\n    <ipython-input-5-733ae79232be>:41 call  *\r\n        target_grad_mapping = tf.gradients(target_patches, target_volume)[0]\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_impl.py:318 gradients_v2  **\r\n        unconnected_gradients)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:684 _GradientsHelper\r\n        lambda: grad_fn(op, *out_grads))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:340 _MaybeCompile\r\n        return grad_fn()  # Exit early\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:684 <lambda>\r\n        lambda: grad_fn(op, *out_grads))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_grad.py:1067 _ExtractVolumePatchesGrad\r\n        input_indices_num = 1 + planes_in * rows_in * cols_in\r\n\r\n    TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'\r\n```\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49146\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49146\">No</a>\n"]}, {"number": 49145, "title": "using sampled softmax is slow in 'double-tower' like model", "body": "I'm trying to implement a 'double tower' like recommendation model, as something described in paper 'Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations'. It mentioned to use 'in batch softmax' to learn the embedding. I think this 'in batch softmax' is very likely to the 'sampled softmax' used in word2vec. So I tried to use the tf.nn.nce_loss to implement this logic. I wrote some code try to train a simple model.\r\n\r\n    \r\n    graph = tf.Graph()\r\n    vocabulary_size = 300000\r\n    embedding_size = 128\r\n    dense_size = 64\r\n    batch_size = 256\r\n    \r\n    import math\r\n    \r\n    with graph.as_default():\r\n    # Input data.\r\n\r\n    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\r\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\r\n    \r\n\r\n    # Ops and variables pinned to the CPU because of missing GPU implementation\r\n    with tf.device('/cpu:0'):\r\n        # Look up embeddings for inputs.\r\n        embeddings = tf.Variable(\r\n            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\r\n        user = tf.nn.embedding_lookup(embeddings, train_inputs)\r\n        user = tf.layers.dense(user, dense_size, activation='relu')\r\n        user = tf.nn.l2_normalize(user, axis=-1)\r\n        print(user)\r\n        \r\n        right_embedding = tf.Variable(\r\n            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\r\n        right_hidden = tf.layers.dense(right_embedding, dense_size, activation='relu')\r\n        \r\n        \r\n        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),dtype=tf.float32,trainable=False)\r\n\r\n        loss = tf.reduce_mean(\r\n            tf.nn.nce_loss(weights=right_hidden,biases=nce_biases, inputs=user, labels=train_labels,\r\n                 num_sampled=num_sampled, num_classes=vocabulary_size))\r\n        \r\n        optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\r\n        init = tf.global_variables_initializer()\r\n\r\nand I found that the training is really slow. I doubt that it's because every time tensorflow need to calculate the right_hidden = tf.layers.dense(right_embedding, dense_size, activation='relu'), and the sampled softmax only applies to the last matrix \"right_hidden\", but we still need to calculate the entire right_hidden matrix beforehand. So am I right? or anyone can give a better solution to deal with the 'sampled softmax' or 'in batch softmax'. thanks ", "comments": ["@jaydone78 ,\r\n\r\nPlease, fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n", "I've upload the training data and code in the colab: \r\nhttps://colab.research.google.com/drive/11prB0qNvxXAiE-zzD9TtBn4OS8ozVeY_?usp=sharing"]}, {"number": 49144, "title": "TFLM: Enable FVP target in benchmarks", "body": "* Enable FVP target with and without Ethos-U for person detect benchmarks.\r\n* Use uint32 instead of int32 in TicksToMs to avoid overflow.\r\n* Add GetCurrentTicks for FVP target.\r\n* Use Vela converted model in person detect benchmarks for Ethos-U.\r\n\r\nThis is fixing: https://github.com/tensorflow/tensorflow/issues/47071", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@advaitjain ping for review"]}, {"number": 49143, "title": "Facilitate building on Windows", "body": "Updates to facilitate building on Windows systems, via for instance\r\nGit Bash.\r\n- fix: Changed use of TMPDIR to TEMPDIR in flatbuffers download.\r\n       Variable TMPDIR have special meaning in mktemp, and if set in\r\n       the environment it will be used as template for the second call\r\n       to mktemp. As the temporary directory is removed before the\r\n       second call it would fail if TMPDIR was present in the\r\n       environment.\r\n - Added download of the Windows version of GNU Arm Embedded Toolchain.\r\n - In the cortex_m target files, do not download the GNU Arm Embedded\r\n   Toolchain if a TARGET_TOOLCHAIN_ROOT is set.\r\n\r\nSigned-off-by: Jonas Ohlsson <jonas.ohlsson@arm.com>\r\nThis is fixing: https://github.com/tensorflow/tensorflow/issues/49142", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "Done!"]}, {"number": 49142, "title": "Not possible to build cortex_m_generic target on Windows", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- Tensorflow version (commit SHA if source): ce84c661ef8e223a7d76d69d79ce0f26c216c3b9\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):\r\n\r\n**Describe the problem**\r\nBuilding the microlite lib for the cortex_m_generic target on Windows is not working.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nFor example using Git Bash follow the instructions in tensorflow/lite/micro/cortex_m_generic/README.md.\r\n", "comments": []}, {"number": 49141, "title": "Docs: Added documentation to some serialize and deserialize functions in keras", "body": "Documentation/Example for:\r\n* tf.keras.optimizers.schedules.serialize\r\n* tf.keras.optimizers.schedules.deserialize\r\n* tf.keras.layers.serialize\r\n* tf.keras.layers.deserialize", "comments": ["This is a nice PR, thanks @bizzyvinci ", "I hope the last commit resolves all feedback. But, I don't know why the check failed.", "This PR has been merged: https://github.com/keras-team/keras/commit/95c2966d9dd8b5ba67c9473bb1099a407f3b9bd7"]}, {"number": 49138, "title": "Normalization using TensorFlow Keras", "body": "Our task is as below not sure what we done mistake in it -\r\n\r\n```\r\nimport numpy as np\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.wrappers.scikit_learn import KerasClassifier\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.model_selection import train_test_split\r\nfrom matplotlib import pyplot\r\nimport seaborn as sns\r\nimport pandas as pd\r\nfrom keras.layers import BatchNormalization\r\nfrom keras.models import model_from_json\r\nimport tensorflow as tf\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn import metrics\r\nimport random as rn\r\nfrom keras import backend as K\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nfrom keras.layers import Activation\r\n```\r\n\r\n```\r\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\r\nsd = 22 # Here sd means seed.\r\nnp.random.seed(sd)\r\nrn.seed(sd)\r\nos.environ['PYTHONHASHSEED']=str(sd)\r\n```\r\n\r\n```\r\nconfig = tf.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\r\ntf.set_random_seed(sd)\r\nsess = tf.Session(graph=tf.get_default_graph(), config=config)\r\nK.set_session(sess)\r\n```\r\n\r\nLoad the breast cancer dataset using load_breast_cancer function\r\n\r\n`X, y = load_breast_cancer(return_X_y=True)`\r\n\r\n\r\n`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=500)`\r\n\r\n- Create a sequential model\r\n- The model expects rows of data with 30 variables (the input_dim=30 argument)\r\n- The first hidden layer has 90 nodes and uses the relu activation function.\r\n- The second hidden layer has 60 nodes and uses the relu activation function.\r\n- The third hidden layer has 30 nodes and uses the relu activation function.\r\n- The output layer has 1 node and uses the sigmoid activation function.\r\n\r\n```\r\nWhile comipling the model pass the following parameters -\r\n\r\n     -optimizer as Adagrad\r\n     -loss as binary cross entropy \r\n     -metrics as accuracy\r\n\r\n```\r\n  ```\r\n  model = Sequential()\r\n    model.add(Dense(30, input_dim=30))\r\n    model.add(Dense(90, activation='relu'))\r\n    model.add(Dense(60, activation='relu'))\r\n    model.add(Dense(30, activation='relu'))\r\n    model.add(Dense(1, activation='sigmoid'))\r\n    model.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\r\n```\r\n\r\nfit the model with X_train, y_train, epochs=50, batch_size=50,shuffle=False,validation_split=0.2,verbose=1 and save it in history\r\n\r\n`history = model.fit(X_train, y_train, epochs=50, batch_size=50,shuffle=False,validation_split=0.2,verbose=1 )`\r\n\r\n```\r\n_, train_acc = model.evaluate(X_train, y_train, verbose=0)\r\n_,test_acc=  model.evaluate(X_test, y_test, verbose=0)\r\nprint('Train: %.2f, Test: %.2f' % (train_acc, test_acc))\r\n```\r\nTrain: 0.61, Test: 0.69\r\n\r\n```\r\ndef plot_history(history):\r\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\r\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\r\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\r\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\r\n    \r\n    if len(loss_list) == 0:\r\n        print('Loss is missing in history')\r\n        return \r\n    \r\n    ## As loss always exists\r\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\r\n    \r\n    ## Loss\r\n    plt.figure(1)\r\n    for l in loss_list:\r\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\r\n    for l in val_loss_list:\r\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\r\n    \r\n    plt.title('Loss')\r\n    plt.xlabel('Epochs')\r\n    plt.ylabel('Loss')\r\n    plt.legend()\r\n    \r\n    ## Accuracy\r\n    plt.figure(2)\r\n    for l in acc_list:\r\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\r\n    for l in val_acc_list:    \r\n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\r\n\r\n    plt.title('Accuracy')\r\n    plt.xlabel('Epochs')\r\n    plt.ylabel('Accuracy')\r\n    plt.legend()\r\n    plt.show()\r\n# Calling the function\r\nplot_history(history)\r\n\r\n```\r\n\r\n- Create a sequential model1\r\n- The model1 expects rows of data with 30 variables (the input_dim=30 argument)\r\n- The first hidden layer has 90 nodes.\r\n- Add Batch Normalization using BatchNormalization function to the model1.\r\n- Add the activation function as relu\r\n- The second hidden layer has 60 nodes.\r\n- Add Batch Normalization using BatchNormalization function to the model1.\r\n- Add the activation function as relu\r\n- The third hidden layer has 30 nodes.\r\n- Add Batch Normalization using BatchNormalization function to the model1.\r\n- Add the activation function as relu\r\n- The output layer has 1 node and uses the sigmoid activation function.\r\n\r\n\r\n```\r\nWhile comipling the model1 pass the following parameters -\r\n\r\n     -optimizer as Adagrad\r\n     -loss as binary cross entropy \r\n     -metrics as accuracy\r\n\r\n```   \r\n   ```\r\n model1 =  Sequential()\r\n    model1.add(Dense(30, input_dim=30))\r\n    model1.add(Dense(90))\r\n    model1.add(Dense(60))\r\n    model1.add(Dense(30))\r\n    model1.add(BatchNormalization())\r\n    model1.add(Activation('relu'))\r\n    model1.add(Dense(1, activation='sigmoid'))\r\n    model1.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\r\n```\r\n\r\n`history1 =  model1.fit(X_train, y_train, epochs=50, batch_size=50,shuffle=False,validation_split=0.2,verbose=1 )\r\n`\r\n```\r\n_, train_acc1 = model1.evaluate(X_train, y_train, verbose=0)\r\n_,test_acc1 =  model1.evaluate(X_test, y_test, verbose=0)\r\nprint('Train: %.2f, Test: %.2f' % (train_acc1, test_acc1))\r\n```\r\nTrain: 0.93, Test: 0.99\r\n\r\n\r\n```\r\nplt.figure(1)\r\npyplot.plot(history1.history['accuracy'], label='train')\r\npyplot.plot(history1.history['val_accuracy'], label='test')\r\n\r\nplt.figure(2)\r\npyplot.plot(history1.history['loss'], label='train')\r\npyplot.plot(history1.history['val_loss'], label='test')\r\npyplot.legend()\r\npyplot.show()\r\n```\r\n\r\n\r\n- Create a sequential model2\r\n- The model2 expects rows of data with 30 variables (the input_dim=30 argument)\r\n- The first hidden layer has 90 nodes and uses the relu activation function.\r\n- Add Batch Normalization using BatchNormalization function to the model2.\r\n- The second hidden layer has 60 nodes and uses the relu activation function.\r\n- Add Batch Normalization using BatchNormalization function to the model2.\r\n- The third hidden layer has 30 nodes and uses the relu activation function.\r\n- Add Batch Normalization using BatchNormalization function to the model2.\r\n- The output layer has 1 node and uses the sigmoid activation function.\r\n\r\n```\r\nWhile comipling the model2 pass the following parameters -\r\n\r\n     -optimizer as Adagrad\r\n     -loss as binary cross entropy \r\n     -metrics as accuracy\r\n```  \r\n   \r\n\r\n   ```\r\nmodel2 =  Sequential()\r\n    model2.add(Dense(30, input_dim=30))\r\n    model2.add(Activation('relu'))\r\n    model2.add(BatchNormalization())\r\n    model2.add(Dense(90))\r\n    model2.add(Dense(60))\r\n    model2.add(Dense(30))\r\n    model2.add(Dense(1, activation='sigmoid'))\r\n    model2.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\r\n\r\n```\r\n\r\n`history2 =  model2.fit(X_train, y_train, epochs=50, batch_size=50,shuffle=False,validation_split=0.2,verbose=1 )`\r\n\r\n```\r\n_, train_acc2 = model2.evaluate(X_train, y_train, verbose=0)\r\n_,test_acc2 =  model2.evaluate(X_test, y_test, verbose=0)\r\nprint('Train: %.2f, Test: %.2f' % (train_acc2, test_acc2))\r\n\r\n```\r\nTrain: 0.92, Test: 0.91\r\n\r\n```\r\nplt.figure(1)\r\npyplot.plot(history2.history['accuracy'], label='train')\r\npyplot.plot(history2.history['val_accuracy'], label='test')\r\n\r\nplt.figure(2)\r\npyplot.plot(history2.history['loss'], label='train')\r\npyplot.plot(history2.history['val_loss'], label='test')\r\npyplot.legend()\r\npyplot.show()\r\n\r\n```\r\n", "comments": ["We think issue is with before and after implementation code in normalization part", "any expertise advice :-)  ", "@ashishsme14 Could you please confirm your TF version?  I tried to reproduce the issue but facing different error. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/ffaee10384289f40f682fba383a99d6f/untitled75.ipynb).If possible,Please provide the colab link to reproduce the issue at our end.Thanks\r\n\r\n\r\nNote: tf.ConfigProto is deprecated in TF 2.0, In that case, use tf.compat.v1.ConfigProto() instead.\r\n\r\n\r\n\r\n\r\n", "tensorflow==1.13.1\r\nnumpy==1.14.5\r\npandas==0.24.2\r\nscipy==1.1.0\r\nscikit-learn==0.19.1\r\npytest==5.4.1\r\nblinker==1.4.0\r\njupyter==1.0.0\r\njupyter-client==6.1.2\r\njupyter-console==6.1.0\r\njupyter-core==4.6.3\r\npickleshare==0.7.5\r\nipynb==0.5.1\r\nmatplotlib==3.0.3\r\nseaborn==0.9.1\r\nkeras==2.3.1\r\nsix==1.14.0\r\nsetuptools==46.1.3", "@ashishsme14 Could you please upgrade your Tensoflow version to 2.x  since 1.x is  no more supported actively.Thanks!", "tensorflow==1.13.1 is fixed for us", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ashishsme14 ,\r\n\r\nPlease try to install 2.x and use tf.compat.v1.ConfigProto() in that version as 1.x is not actively supported.\r\n\r\nThanks!", "tensorflow==2.1.1\r\nnumpy==1.16.0\r\npandas==0.24.2\r\nscipy==1.4.1\r\nscikit-learn==0.19.1\r\npytest==5.4.1\r\nblinker==1.4.0\r\njupyter==1.0.0\r\njupyter-client==6.1.2\r\njupyter-console==6.1.0\r\njupyter-core==4.6.3\r\npickleshare==0.7.5\r\nipynb==0.5.1\r\nmatplotlib==3.0.3\r\nseaborn==0.9.1\r\nkeras==2.3.1\r\nsix==1.14.0\r\nsetuptools==46.1.3\r\n\r\nWe now using above version still thing not working -\r\n\r\n[Normalization_Question (5).zip](https://github.com/tensorflow/tensorflow/files/6562474/Normalization_Question.5.zip)\r\n", "@ashishsme14 ,\r\n\r\nCan you please try to execute your code in tensorflow stable version v2.4.1 or 2.5.0 and let us know if you are still facing the same issue.\r\n\r\nThanks!", "tried same result.. can u please check code for before and after normalization. seems it is not correct.", "@ashishsme14 ,\r\n\r\nPlease check this links for more information on Normalization.[Link1](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization),[Link2](https://keras.io/api/layers/normalization_layers/batch_normalization/),[Link3](https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras).\r\n\r\nCan you pease consider posting this issue in [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow)  as it is not either bug or feature request, where there is a large community to help and support each other. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Please let us know below part is correct\r\n\r\n```\r\nmodel1 =  Sequential()\r\n model1.add(Dense(30, input_dim=30))\r\n model1.add(Dense(90))\r\n model1.add(Dense(60))\r\n model1.add(Dense(30))\r\n model1.add(BatchNormalization())\r\n model1.add(Activation('relu'))\r\n model1.add(Dense(1, activation='sigmoid'))\r\n model1.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\r\n```", "@ashishsme14 ,\r\n\r\nPlease refer this link for more information on [model](https://www.tensorflow.org/guide/keras/train_and_evaluate#the_compile_method_specifying_a_loss_metrics_and_an_optimizer) and add tensorflow.keras before as mentioned in this [link](https://www.tensorflow.org/api_docs/python/tf/keras/Model#expandable-1).\r\n\r\nAlso please feel free to move this issue to closed status and please submit a new issue from this [link](https://github.com/tensorflow/models/issues/new/choose).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49138\">No</a>\n"]}, {"number": 49137, "title": "Training on tf distribute takes long time ->  Complete shape not known for Adam for SparseTensor", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No mobile\r\n- TensorFlow installed from (source or binary): Rejected\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): Negative\r\n- GCC/Compiler version (if compiling from source): Negative\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am trying to run my model with MultiWorkerMirroredStrategy() but I am facing this issue:\r\n`2021-05-07 19:47:04.704974: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:455] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce_3\r\n2021-05-07 19:47:04.705015: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1138] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce_3\r\n2021-05-07 19:47:04.705027: E tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:1155] ScopedAllocatorOptimizer: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce_3\r\n2021-05-07 19:47:04.705033: W tensorflow/core/grappler/optimizers/scoped_allocator_optimizer.cc:928] error: Aborted: Complete shape not known for Adam/allreduce/CollectiveReduce_3\r\n`\r\nMy code in the tf.distribute takes more than 1 hour per epoch whereas running it normally it takes 4 minutes per epoch. The difference is humongous, I am suspecting its maybe due to the warning msg above. \r\n\r\nFor the dataset I am using sklearn's CountVectorizer to transform text to feature vector and LabelEncoder to encode the class names. I then convert x to SparseTensor. \r\n\r\n\r\n```\r\nx # coo_matrix with 300k observations and 42000 feature size\r\ny # array containing integer value of size 300k x 1 \r\nBATCH_SIZE = 256\r\nNUM_WORKERS = 2\r\n\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\r\n\r\ndef build_model():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.Dense(512,  activation='relu'))\r\n    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\r\n    model.compile(optimizer=\"adam\",\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=[\"accuracy\"])\r\n\r\n    return model\r\n\r\ndef coo_to_tensor(coo):\r\n    return tf.sparse.SparseTensor(list(zip(coo.row, coo.col)), coo.data, coo.shape)\r\n\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA \r\nds = tf.data.Dataset.from_tensor_slices((coo_to_tensor(x),y)).batch(GLOBAL_BATCH_SIZE).with_options(options)..batch(GLOBAL_BATCH_SIZE)\r\n\r\nwith strategy.scope():\r\n    multi_worker_model = build_model()\r\n\r\nmulti_worker_model.fit(ds, epochs=args.epochs, batch_size=BATCH_SIZE)\r\n```\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have been playing around with the from_generator function as well\r\n\r\n\r\n```\r\nfeature_size = 42000 -> feature size after vectorizing text data \r\n\r\ndef build_model():\r\n    model = tf.keras.models.Sequential()\r\n    model.add(tf.keras.layers.InputLayer(input_shape=(feature_size,), dtype=tf.int64, sparse=True))\r\n    model.add(tf.keras.layers.Dense(512,  activation='relu'))\r\n    model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\r\n    model.compile(optimizer=\"adam\",\r\n                  loss='sparse_categorical_crossentropy',\r\n                  metrics=[\"accuracy\"])\r\n\r\n    return model\r\n\r\nds = tf.data.Dataset.from_generator(my_generator, output_signature=(tf.SparseTensorSpec(shape=(1,feature_size) ,dtype=tf.int64), tf.TensorSpec(shape=(1,), dtype=tf.int32)))\r\n\r\nwith strategy.scope():\r\n    multi_worker_model = build_model()\r\n\r\nmulti_worker_model.fit(ds, epochs=args.epochs, batch_size=BATCH_SIZE , steps_per_epoch=my_steps)\r\n\r\n```\r\n\r\nHowever issues still prevail, this time I am getting such an error message \r\n`\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"cond_1/Identity:0\", shape=(None, 3), dtype=int64, device=/job:worker/replica:0/task:0/device:CPU:0), values=Tensor(\"sequential/dense/Cast:0\", shape=(None,), dtype=float32, device=/job:worker/replica:0/task:0/device:CPU:0), dense_shape=Tensor(\"cond_1/Identity_2:0\", shape=(3,), dtype=int64, device=/job:worker/replica:0/task:0/device:CPU:0)). Consider casting elements to a supported type.`\r\n", "@sat2000pts \r\n\r\nSorry for the delayed response.\r\nLooking at the error log,seems like this is similar to  the issue [#25980](https://github.com/tensorflow/tensorflow/issues/25980), and let me know if it helps.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49137\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49137\">No</a>\n"]}, {"number": 49136, "title": "[manylinux2014][aarch64][git] fatal error: AArch64GenO0PreLegalizeGICombiner.inc: No such file or directory", "body": "**System information**\r\n- OS Platform and Distribution: manylinux2014 (CentOS 7)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: commit db73cf59f471354ace196bb2ccb1f5c951b6cec1 \r\n- Python version: 3.6\r\n- Installed using virtualenv\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc (GCC) 9.3.1 20200408 (Red Hat 9.3.1-2)\r\n- CUDA/cuDNN version: CUDA disabled\r\n- GPU model and memory: no GPU\r\n\r\n**Describe the problem**\r\n\r\nI am working on building manylinux2014 compatible wheels of TensorFlow for AArch64. So far built 1.15.5 and 2.4.1 versions. But git HEAD fails:\r\n```\r\n    Execution platform: @local_execution_config_platform//:platform                                                                                                                    \r\n    external/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp:45:10: fatal error: AArch64GenO0PreLegalizeGICombiner.inc: No such file or directory\r\n       45 | #include \"AArch64GenO0PreLegalizeGICombiner.inc\"                                                                                                                           \r\n          |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                                                                           \r\n    compilation terminated.   \r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\n      bazel clean --expunge\r\n      export BAZEL_LINKLIBS=-l%:libstdc++.a\r\n      bazel build --config=noaws --config=nogcp --config=nonccl \\\r\n            //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n    + bazel build --config=noaws --config=nogcp --config=nonccl //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n    Starting local Bazel server and connecting to it...\r\n    INFO: Options provided by the client:\r\n      Inherited 'common' options: --isatty=0 --terminal_columns=80\r\n    INFO: Reading rc options for 'build' from /tmp/workspace6/tensorflow-git/.bazelrc:\r\n      Inherited 'common' options: --experimental_repo_remote_exec\r\n    INFO: Reading rc options for 'build' from /tmp/workspace6/tensorflow-git/.bazelrc:\r\n      'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --defi\r\nne=no_hdfs_support=true\r\n    INFO: Reading rc options for 'build' from /tmp/workspace6/tensorflow-git/.tf_configure.bazelrc:\r\n      'build' options: --action_env PYTHON_BIN_PATH=/tmp/workspace6/venv-cp36-cp36m/bin/python3 --action_env PYTHON_LIB_PATH=/tmp/workspace6/venv-cp36-cp36m/lib/python3.6/site-packages --python_path=/tmp/workspace6/venv-cp36-cp36m/bin/python3\r\n    INFO: Found applicable config definition build:short_logs in file /tmp/workspace6/tensorflow-git/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\n    INFO: Found applicable config definition build:v2 in file /tmp/workspace6/tensorflow-git/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\n    INFO: Found applicable config definition build:noaws in file /tmp/workspace6/tensorflow-git/.bazelrc: --define=no_aws_support=true\r\n    INFO: Found applicable config definition build:nogcp in file /tmp/workspace6/tensorflow-git/.bazelrc: --define=no_gcp_support=true\r\n    INFO: Found applicable config definition build:nonccl in file /tmp/workspace6/tensorflow-git/.bazelrc: --define=no_nccl_support=true\r\n    INFO: Found applicable config definition build:linux in file /tmp/workspace6/tensorflow-git/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\n    INFO: Found applicable config definition build:dynamic_kernels in file /tmp/workspace6/tensorflow-git/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\n    Loading:\r\n    Loading: 0 packages loaded\r\n    Loading: 0 packages loaded\r\n    DEBUG: /root/.cache/bazel/_bazel_root/5ad6f84b01b6d62f14bf0104cde27024/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\n    Loading: 0 packages loaded\r\n    Loading: 0 packages loaded\r\n    Loading: 0 packages loaded\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded)\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 0 targets configured)\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (54 packages loaded, 14 targets configured)\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (55 packages loaded, 14 targets configured)\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (194 packages loaded, 3759 targets configured)\r\n    DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\n    DEBUG: Repository io_bazel_rules_docker instantiated at:\r\n      /tmp/workspace6/tensorflow-git/WORKSPACE:23:14: in <toplevel>\r\n      /tmp/workspace6/tensorflow-git/tensorflow/workspace0.bzl:108:34: in workspace\r\n      /root/.cache/bazel/_bazel_root/5ad6f84b01b6d62f14bf0104cde27024/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\n    Repository rule git_repository defined at:\r\n      /root/.cache/bazel/_bazel_root/5ad6f84b01b6d62f14bf0104cde27024/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (230 packages loaded, 3969 targets configured)\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (232 packages loaded, 3969 targets configured)\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (382 packages loaded, 11241 targets configured)\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (404 packages loaded, 20479 targets configured)\r\n    Analyzing: target //tensorflow/tools/pip_package:build_pip_package (404 packages loaded, 20479 targets configured)\r\n    WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/6351993da72e298b3f79218e4f129a9bbde3e679.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\n    INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (410 packages loaded, 31658 targets configured).\r\n    INFO: Found 1 target...\r\n    [0 / 511] [Prepa] Expanding template tensorflow/lite/tools/visualize ... (4 actions, 0 running)\r\n    [40 / 511] Compiling com_google_protobuf/src/google/protobuf/descriptor.cc [for host]; 7s local ... (16 actions, 15 running)\r\n    [67 / 511] Compiling com_google_protobuf/src/google/protobuf/descriptor.cc [for host]; 17s local ... (16 actions, 15 running)\r\n    [108 / 511] Compiling com_google_protobuf/src/google/protobuf/descriptor.cc [for host]; 28s local ... (16 actions, 15 running)\r\n    [167 / 511] Compiling com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_enum.cc [for host]; 7s local ... (16 actions, 15 running)\r\n    [312 / 1,088] Compiling com_google_protobuf/src/google/protobuf/compiler/cpp/cpp_message.cc [for host]; 13s local ... (16 actions running)\r\n    [536 / 1,180] Compiling tensorflow/core/framework/op_gen_lib.cc [for host]; 10s local ... (16 actions running)\r\n    [763 / 2,128] Compiling tensorflow/python/util/tf_stack.cc [for host]; 17s local ... (16 actions running)\r\n    [1,179 / 2,128] Compiling boringssl/src/crypto/fipsmodule/bcm.c [for host]; 10s local ... (16 actions running)\r\n    [1,423 / 2,128] Compiling com_github_grpc_grpc/src/core/lib/security/credentials/tls/grpc_tls_credentials_options.cc [for host]; 1s local ... (16 actions running)\r\n    [1,588 / 6,871] Compiling tensorflow/core/kernels/scatter_nd_op_cpu_impl_2.cc [for host]; 23s local ... (16 actions running)\r\n    [1,686 / 6,871] Compiling llvm-project/llvm/utils/TableGen/GlobalISelEmitter.cpp [for host]; 24s local ... (16 actions running)\r\n    [1,851 / 6,871] Compiling llvm-project/llvm/lib/Support/ItaniumManglingCanonicalizer.cpp [for host]; 7s local ... (16 actions running)\r\n    [1,966 / 7,128] Compiling llvm-project/llvm/utils/TableGen/GlobalISelEmitter.cpp [for host]; 23s local ... (16 actions, 15 running)\r\n    [2,699 / 7,660] Compiling llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp [for host]; 10s local ... (16 actions running)\r\n    [2,958 / 7,881] Compiling llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp [for host]; 17s local ... (16 actions running)\r\n    [3,036 / 7,881] Compiling tensorflow/core/kernels/reduction_ops_mean.cc [for host]; 62s local ... (16 actions running)\r\n    [3,099 / 7,881] Compiling tensorflow/core/kernels/linalg/matrix_square_root_op.cc [for host]; 92s local ... (16 actions running)\r\n    [3,177 / 7,881] Compiling tensorflow/core/kernels/linalg/matrix_square_root_op.cc [for host]; 178s local ... (16 actions running)\r\n    [3,342 / 7,881] Compiling tensorflow/core/kernels/linalg/matrix_square_root_op.cc [for host]; 277s local ... (16 actions running)\r\n    [3,504 / 7,882] Compiling llvm-project/mlir/lib/Rewrite/ByteCode.cpp [for host]; 18s local ... (16 actions running)\r\n    [3,667 / 7,882] Compiling tensorflow/core/kernels/ragged_tensor_to_tensor_op.cc [for host]; 46s local ... (16 actions running)\r\n    [3,800 / 7,886] Compiling tensorflow/core/kernels/conv_grad_filter_ops.cc [for host]; 144s local ... (16 actions running)\r\n    [3,955 / 7,898] Compiling tensorflow/compiler/xla/service/hlo_evaluator_typed_visitor_bfloat16.cc [for host]; 53s local ... (16 actions running)\r\n    [4,125 / 7,912] Compiling tensorflow/core/kernels/reverse_op.cc [for host]; 78s local ... (16 actions running)\r\n    [4,369 / 7,912] Compiling tensorflow/core/kernels/reverse_sequence_op.cc [for host]; 65s local ... (16 actions running)\r\n    ERROR: /root/.cache/bazel/_bazel_root/5ad6f84b01b6d62f14bf0104cde27024/external/llvm-project/llvm/BUILD:816:11: C++ compilation of rule '@llvm-project//llvm:AArch64CodeGen' failed (Exit 1): gcc failed: error executing command\r\n      (cd /root/.cache/bazel/_bazel_root/5ad6f84b01b6d62f14bf0104cde27024/execroot/org_tensorflow && \\\r\n      exec env - \\\r\n        LD_LIBRARY_PATH=/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/usr/local/lib64 \\\r\n        PATH=/tmp/workspace6/venv-cp36-cp36m/bin:/opt/rh/devtoolset-9/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n        PWD=/proc/self/cwd \\\r\n      /opt/rh/devtoolset-9/root/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/host/bin/external/llvm-project/llvm/_objs/AArch64CodeGen/AArch64O0PreLegalizerCombiner.pic.d '-frandom-seed=bazel-out/host/bin/external/llvm-project/llvm/_objs/AArch64CodeGen/AArch64O0PreLegalizerCombiner.pic.o' -fPIC -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D\r\n__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquoteexternal/llvm-project -iquotebazel-out/host/bin/external/llvm-project -iquoteexternal/zlib -iquotebazel-out/host/bin/external/zlib -isystem external/llvm-project/llvm/lib/Target/AArch64 -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/AArch64 -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out\r\n/host/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Target/AMDGPU -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/AMDGPU -g0 -w -g0 '-std=c++14' -Iexternal/llvm-project/llvm/lib/Target/AArch64 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp -o bazel-out/host/bin/external/llvm-project/llvm/_objs/AArch64CodeGen/AArch64O\r\n0PreLegalizerCombiner.pic.o)\r\n    Execution platform: @local_execution_config_platform//:platform\r\n    external/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp:45:10: fatal error: AArch64GenO0PreLegalizeGICombiner.inc: No such file or directory\r\n       45 | #include \"AArch64GenO0PreLegalizeGICombiner.inc\"\r\n          |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    compilation terminated.\r\n    Target //tensorflow/tools/pip_package:build_pip_package failed to build\r\n    ERROR: /tmp/workspace6/tensorflow-git/tensorflow/tools/pip_package/BUILD:69:10 C++ compilation of rule '@llvm-project//llvm:AArch64CodeGen' failed (Exit 1): gcc failed: error executing command\r\n      (cd /root/.cache/bazel/_bazel_root/5ad6f84b01b6d62f14bf0104cde27024/execroot/org_tensorflow && \\\r\n      exec env - \\\r\n        LD_LIBRARY_PATH=/opt/rh/devtoolset-9/root/usr/lib64:/opt/rh/devtoolset-9/root/usr/lib:/opt/rh/devtoolset-9/root/usr/lib64/dyninst:/opt/rh/devtoolset-9/root/usr/lib/dyninst:/usr/local/lib64 \\\r\n        PATH=/tmp/workspace6/venv-cp36-cp36m/bin:/opt/rh/devtoolset-9/root/usr/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n        PWD=/proc/self/cwd \\\r\n      /opt/rh/devtoolset-9/root/usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/host/bin/external/llvm-project/llvm/_objs/AArch64CodeGen/AArch64O0PreLegalizerCombiner.pic.d '-frandom-seed=bazel-out/host/bin/external/llvm-project/llvm/_objs/AArch64CodeGen/AArch64O0PreLegalizerCombiner.pic.o' -fPIC -DLLVM_ENABLE_STATS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D\r\n__STDC_FORMAT_MACROS -DLLVM_BUILD_GLOBAL_ISEL -iquoteexternal/llvm-project -iquotebazel-out/host/bin/external/llvm-project -iquoteexternal/zlib -iquotebazel-out/host/bin/external/zlib -isystem external/llvm-project/llvm/lib/Target/AArch64 -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/AArch64 -isystem external/llvm-project/llvm/include -isystem bazel-out/host/bin/external/llvm-project/llvm/include -isystem external/zlib -isystem bazel-out/host/bin/external/zlib -isystem external/llvm-project/llvm/include/llvm/IR -isystem bazel-out\r\n/host/bin/external/llvm-project/llvm/include/llvm/IR -isystem external/llvm-project/llvm/lib/Target/AMDGPU -isystem bazel-out/host/bin/external/llvm-project/llvm/lib/Target/AMDGPU -g0 -w -g0 '-std=c++14' -Iexternal/llvm-project/llvm/lib/Target/AArch64 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp -o bazel-out/host/bin/external/llvm-project/llvm/_objs/AArch64CodeGen/AArch64O\r\n0PreLegalizerCombiner.pic.o)\r\n    Execution platform: @local_execution_config_platform//:platform\r\n    INFO: Elapsed time: 1979.907s, Critical Path: 313.84s\r\n    INFO: 4575 processes: 520 internal, 4055 local.\r\n    FAILED: Build did NOT complete successfully\r\n    FAILED: Build did NOT complete successfully\r\n```", "comments": ["When it comes to released versions we have this coverage on AArch64 builds (using manylinux2014 image):\r\n\r\nTF version | Py 3.6 |  Py 3.7 | Py 3.8 | Py 3.9\r\n-|-|-|-|-\r\n1.15.5 | OK | OK | -- | --\r\n2.4.1 | OK | OK | OK | OK\r\n2.5 | OK | OK | OK | OK\r\n\r\nhttps://snapshots.linaro.org/ldcg/python/tensorflow-manylinux/4/ for 1.15.5 and 2.4.1 wheels\r\nhttps://snapshots.linaro.org/ldcg/python/tensorflow-manylinux/11/ for 2.5.0 ones.\r\n\r\ngit HEAD is what fails on any Python version", "It seems it was recently introduced by @aemerson  in LLVM with https://github.com/llvm/llvm-project/commit/5b158093e2469dec16a070019c6432d26bf7be9b", "@bhack thanks!\r\n\r\nSo LLVM in TF (or bazel) needs update to newer revision?", "From quick look I could blame commit e9e496d551457701614e5c435919911928b44502 from 10th May as 9th May is the last successful build on Linaro CI.\r\n\r\nBut we lack builds on 10-11 May so it could be any changes done between 9th and 12th May.", "Is that after that LLVM commit we are missing the new entry for `AArch64O0PreLegalizerCombinerHelper` in:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3fd3ae1fbb10961dd1aa6805280674c781fd4609/third_party/llvm/llvm.autogenerated.BUILD#L483-L485\r\n\r\nYou could try to send a PR.", "@mihaimaruseac Do you know where Is the generator? Is it an internal tool?", "Probably this would be enough but I cannot test it at the moment.\r\n\r\n```diff\r\n14:04 (s) hrw@puchatek:llvm$ git diff\r\ndiff --git a/third_party/llvm/llvm.autogenerated.BUILD b/third_party/llvm/llvm.autogenerated.BUILD\r\nindex d3f9990c2e1..08201f5fc8c 100644\r\n--- a/third_party/llvm/llvm.autogenerated.BUILD\r\n+++ b/third_party/llvm/llvm.autogenerated.BUILD\r\n@@ -480,6 +480,7 @@ llvm_target_list = [\r\n             (\"-gen-dag-isel\", \"lib/Target/AArch64/AArch64GenDAGISel.inc\"),\r\n             (\"-gen-fast-isel\", \"lib/Target/AArch64/AArch64GenFastISel.inc\"),\r\n             (\"-gen-global-isel\", \"lib/Target/AArch64/AArch64GenGlobalISel.inc\"),\r\n+            (\"-gen-global-isel-combiner -combiners=AArch64O0PreLegalizerCombinerHelper\", \"lib/Target/AArch64/AArch64GenO0PreLegalizeGICombiner.inc\"),\r\n             (\"-gen-global-isel-combiner -combiners=AArch64PreLegalizerCombinerHelper\", \"lib/Target/AArch64/AArch64GenPreLegalizeGICombiner.inc\"),\r\n             (\"-gen-global-isel-combiner -combiners=AArch64PostLegalizerCombinerHelper\", \"lib/Target/AArch64/AArch64GenPostLegalizeGICombiner.inc\"),\r\n             (\"-gen-global-isel-combiner -combiners=AArch64PostLegalizerLoweringHelper\", \"lib/Target/AArch64/AArch64GenPostLegalizeGILowering.inc\"),\r\n```", "commit 04b415a fixed the problem", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49136\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49136\">No</a>\n", "/cc @hawkinsp", "Yes, it seems like we came up with the same fix. Thanks for the PR!", "@hawkinsp Do you know where the generator is?", "@bhack The generator is (was?) internal to Google. However, I think at this point we just manually update the BUILD file.\r\n\r\nThe BUILD file for LLVM is likely soon going to go away, in lieu of https://github.com/llvm/llvm-www/blob/main/proposals/LP0002-BazelBuildConfiguration.md"]}, {"number": 49135, "title": "How to reduce false positives when training a model using tensorflow object detection API?", "body": "I have trained a tensorflow object detection model which will detect a box, bag and crate. when i tested that trained model it is giving more false positives. I trained that with the cleaned datas with proper annotations, though it is giving more false positives. Is there any way to reduce the false positives?\r\n\r\nI am using ssd_mobilenet_v1_coco to train a model. Do i need to change anything in the pipeline.config file to reduce the false positives?", "comments": ["@Shobana1230  Could you please provide some sample images.", "@Shobana1230  You can refer [this](https://stackoverflow.com/questions/45666499/best-strategy-to-reduce-false-positives-googles-new-object-detection-api-on-sa) StackOverflow answer for general context on reducing false positives in Object detection APIs.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49135\">No</a>\n"]}, {"number": 49134, "title": "RFC for memory allocation updates in TFLM.", "body": "Signed-off-by: Vijay Pawar <vpawar@cadence.com>", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "With TFLM moving to its own GitHub repository, we are not going to be merging any TFLM specific pull requests in the TensorFlow repository starting today.\r\n\r\nI am closing the current PR but please feel free to open a new PR in https://github.com/tensorflow/tflite-micro/.\r\n\r\nhttps://groups.google.com/a/tensorflow.org/g/micro/c/W4DACgjPmOE\r\n"]}, {"number": 49132, "title": "RFC for memory allocation updates in TFLM", "body": "RFC for memory allocation updates in TFLM", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49132) for more info**.\n\n<!-- need_sender_cla -->", "@vp-cad  Can you please sign CLA. Thanks!", "@vp-cad  Any update on this PR? Please. Thanks!", "Closing this PR in favor of https://github.com/tensorflow/tensorflow/pull/49134"]}, {"number": 49131, "title": "Cannot import TensorFlow in Docker running at M1 (with AMD64 emulation)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 in Docker (running at Apple M1)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): pip3\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: M1\r\n\r\n```zsh\r\n8:04 martin@martins-MacBook-Pro /Users/martin/Projects/rl-toolkit\r\n% docker run -it -p 8000:8000 --rm markub3327/rl-toolkit bash\r\nWARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\r\n+ Xvfb -screen 0 1024x768x24\r\n+ export DISPLAY=:0\r\n+ DISPLAY=:0\r\n+ display=0\r\n+ file=/tmp/.X11-unix/X0\r\n++ seq 1 10\r\n+ for i in $(seq 1 10)\r\n+ '[' -e /tmp/.X11-unix/X0 ']'\r\n+ echo 'Waiting for /tmp/.X11-unix/X0 to be created (try 1/10)'\r\nWaiting for /tmp/.X11-unix/X0 to be created (try 1/10)\r\n+ sleep 1\r\n+ for i in $(seq 1 10)\r\n+ '[' -e /tmp/.X11-unix/X0 ']'\r\n+ break\r\n+ '[' -e /tmp/.X11-unix/X0 ']'\r\n+ exec bash\r\nroot@215c42188dbc:~/rl-toolkit# python3\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-05-12 06:04:33.062104: F tensorflow/core/lib/monitoring/sampler.cc:42] Check failed: bucket_limits_[i] > bucket_limits_[i - 1] (0 vs. 10)\r\nqemu: uncaught target signal 6 (Aborted) - core dumped\r\nAborted\r\nroot@215c42188dbc:~/rl-toolkit# \r\n```\r\n\r\nWhy I cannot import the TensorFlow package on Ubuntu 20.04 running in a Docker container (arch AMD64)? I use Apple M1 with Rosetta 2, which allows the run of AMD64 containers.", "comments": ["@markub3327 \r\n\r\nCould you please share the following details with us, so that we can look into the issue\r\n\r\noutput of pip list command\r\noutput of conda list command\r\nThanks!", "The output of `pip list`:\r\n```zsh\r\nroot@d6da516955d3:~/rl-toolkit# pip list\r\nPackage                 Version  \r\n----------------------- ---------\r\nabsl-py                 0.12.0   \r\nastunparse              1.6.3    \r\nBox2D                   2.3.10   \r\ncachetools              4.2.2    \r\ncertifi                 2020.12.5\r\nchardet                 4.0.0    \r\nclick                   7.1.2    \r\ncloudpickle             1.6.0    \r\nconfigparser            5.0.2    \r\ndecorator               5.0.7    \r\ndm-reverb               0.2.0    \r\ndm-tree                 0.1.6    \r\ndocker-pycreds          0.4.0    \r\nflatbuffers             1.12     \r\nfuture                  0.18.2   \r\ngast                    0.3.3    \r\ngitdb                   4.0.7    \r\nGitPython               3.1.14   \r\ngoogle-auth             1.30.0   \r\ngoogle-auth-oauthlib    0.4.4    \r\ngoogle-pasta            0.2.0    \r\ngrpcio                  1.32.0   \r\ngym                     0.18.0   \r\nh5py                    2.10.0   \r\nidna                    2.10     \r\nKeras-Preprocessing     1.1.2    \r\nMarkdown                3.3.4    \r\nnumpy                   1.17.4   \r\noauthlib                3.1.0    \r\nopencv-python           4.5.2.52 \r\nopt-einsum              3.3.0    \r\npathtools               0.1.2    \r\nPillow                  7.2.0    \r\npip                     20.0.2   \r\nportpicker              1.3.1    \r\npromise                 2.3      \r\nprotobuf                3.16.0   \r\npsutil                  5.8.0    \r\npyasn1                  0.4.8    \r\npyasn1-modules          0.2.8    \r\npybullet                3.1.7    \r\npyglet                  1.5.0    \r\npython-dateutil         2.8.1    \r\nPyYAML                  5.4.1    \r\nrequests                2.25.1   \r\nrequests-oauthlib       1.3.0    \r\nrsa                     4.7.2    \r\nscipy                   1.6.3    \r\nsentry-sdk              1.1.0    \r\nsetuptools              45.2.0   \r\nshortuuid               1.0.1    \r\nsix                     1.15.0   \r\nsmmap                   4.0.0    \r\nsubprocess32            3.5.4    \r\ntensorboard             2.5.0    \r\ntensorboard-data-server 0.6.1    \r\ntensorboard-plugin-wit  1.8.0    \r\ntensorflow              2.4.1    \r\ntensorflow-estimator    2.4.0    \r\ntensorflow-probability  0.12.2   \r\ntermcolor               1.1.0    \r\ntyping-extensions       3.7.4.3  \r\nurllib3                 1.26.4   \r\nwandb                   0.10.30  \r\nWerkzeug                1.0.1    \r\nwheel                   0.36.2   \r\nwrapt                   1.12.1\r\n```\r\n\r\nThe installation packages, I was doing with my [requirements.txt](https://github.com/markub3327/rl-toolkit/blob/master/requirements.txt).\r\n\r\nI'm not using a Conda in my container:\r\n```zsh\r\nroot@d6da516955d3:~/rl-toolkit# conda list\r\nbash: conda: command not found\r\nroot@d6da516955d3:~/rl-toolkit# \r\n```\r\nThanks!", "With `TF 2.5.0` the same output:\r\n```zsh\r\nPython 3.8.5 (default, Jan 27 2021, 15:41:15) \r\n[GCC 9.3.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2021-05-14 11:23:17.684148: F tensorflow/core/lib/monitoring/sampler.cc:42] Check failed: bucket_limits_[i] > bucket_limits_[i - 1] (0 vs. 10)\r\nqemu: uncaught target signal 6 (Aborted) - core dumped\r\nAborted\r\nroot@767f5a944abf:~/rl-toolkit# \r\n```\r\n", "Modern, or say, recent TF  x86_64 pip wheel packages have AVX enabled. Please check if your emulator has AVX enabled.", "Here is a detailed info about my CPU in the container (Docker).\r\n```bash\r\nroot@53ac39fad5ef:~/rl-toolkit# cat /proc/cpuinfo \r\nprocessor\t: 0\r\nBogoMIPS\t: 48.00\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\nCPU implementer\t: 0x41\r\nCPU architecture: 8\r\nCPU variant\t: 0x1\r\nCPU part\t: 0xd07\r\nCPU revision\t: 0\r\n\r\nprocessor\t: 1\r\nBogoMIPS\t: 48.00\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\nCPU implementer\t: 0x41\r\nCPU architecture: 8\r\nCPU variant\t: 0x1\r\nCPU part\t: 0xd07\r\nCPU revision\t: 0\r\n\r\nprocessor\t: 2\r\nBogoMIPS\t: 48.00\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\nCPU implementer\t: 0x41\r\nCPU architecture: 8\r\nCPU variant\t: 0x1\r\nCPU part\t: 0xd07\r\nCPU revision\t: 0\r\n\r\nprocessor\t: 3\r\nBogoMIPS\t: 48.00\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\nCPU implementer\t: 0x41\r\nCPU architecture: 8\r\nCPU variant\t: 0x1\r\nCPU part\t: 0xd07\r\nCPU revision\t: 0\r\n\r\nprocessor\t: 4\r\nBogoMIPS\t: 48.00\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\nCPU implementer\t: 0x41\r\nCPU architecture: 8\r\nCPU variant\t: 0x1\r\nCPU part\t: 0xd07\r\nCPU revision\t: 0\r\n\r\nprocessor\t: 5\r\nBogoMIPS\t: 48.00\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\nCPU implementer\t: 0x41\r\nCPU architecture: 8\r\nCPU variant\t: 0x1\r\nCPU part\t: 0xd07\r\nCPU revision\t: 0\r\n\r\nprocessor\t: 6\r\nBogoMIPS\t: 48.00\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\nCPU implementer\t: 0x41\r\nCPU architecture: 8\r\nCPU variant\t: 0x1\r\nCPU part\t: 0xd07\r\nCPU revision\t: 0\r\n\r\nprocessor\t: 7\r\nBogoMIPS\t: 48.00\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\nCPU implementer\t: 0x41\r\nCPU architecture: 8\r\nCPU variant\t: 0x1\r\nCPU part\t: 0xd07\r\nCPU revision\t: 0\r\n```", "```\r\nFeatures\t: fp asimd evtstrm aes pmull sha1 sha2 crc32 fphp asimdhp cpuid dit\r\n```\r\nit seems you are emulating AArch64/ARM64/ARMv8 instead of AMD64. What does `uname -a` show?\r\n", "```bash\r\nroot@a8fce6a50ff8:~/rl-toolkit# uname -a\r\nLinux a8fce6a50ff8 5.10.25-linuxkit #1 SMP PREEMPT Tue Mar 23 09:24:45 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux\r\nroot@a8fce6a50ff8:~/rl-toolkit# \r\n```\r\n\r\n`uname -a` showed that I'm emulating AMD64. The container has a CPU instruction set like the M1 chip that I have, but It's emulating Ubuntu AMD64 ..... So interesting.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Any solution?", "I am sorry, but we can't provide support for emulation errors. Our docker containers don't support this use case.\r\n\r\nIf you can replicate this on a native x86 CPU, please open another issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49131\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49131\">No</a>\n"]}, {"number": 49130, "title": "[ERROR] TF Object Detection API: \u201ctensorflow.python.framework.errors_impl.FailedPreconditionError: HashTable has different value for same key. Key item { has 0 and trying to add value 4 [Op:InitializeTableFromTextFileV2]\u201d", "body": "I am trying to work with this [yolov4 in tensorflow 2.0 model](https://github.com/zzh8829/yolov3-tf2), it requires the dataset to be in Tensorflow Object Detection API format. Since I had no prior experience with this api so I was following Adrian Rosebrock's tutorial in his ImageNet bundle book. I barely changed the code in tutorial other than changing paths and file names, the code files and more in depth detail of error is posted in my this [Stack Overflow post](https://stackoverflow.com/questions/67497274/tensorflow-object-detection-api-failedpreconditionerror-hashtable-has-differe).\r\nBut when I try to use the acquired tf record files with the yolov4 tensorflow model mentioned above, I get this error. I couldnt find any working solution on the web, so here I am. If you can pls help, I'll be more grateful than you can imagine! :)\r\n\r\nError: \r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 195, in <module>\r\n    app.run(main)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"train.py\", line 64, in main\r\n    FLAGS.dataset, FLAGS.classes, FLAGS.size)\r\n  File \"/content/yolov3-tf2/yolov3_tf2/dataset.py\", line 124, in load_tfrecord_dataset\r\n    class_file, tf.string, 0, tf.int64, LINE_NUMBER, delimiter=\"\\n\"), -1)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 314, in __init__\r\n    super(StaticHashTable, self).__init__(default_value, initializer)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 185, in __init__\r\n    self._init_op = self._initialize()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 188, in _initialize\r\n    return self._initializer.initialize(self)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\", line 744, in initialize\r\n    -1 if self._vocab_size is None else self._vocab_size, self._delimiter)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_lookup_ops.py\", line 363, in initialize_table_from_text_file_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: HashTable has different value for same key. Key item { has 0 and trying to add value 4 [Op:InitializeTableFromTextFileV2]\r\n```", "comments": ["@KhawajaAbaid Could you please  submit  a new issue on models [repo](https://github.com/tensorflow/models/issues/new/choose) since the issue is more related TF OD API.Thanks!"]}]