[{"number": 6573, "title": "Error with bazel and protobuf building a project that depends on tensorflow in Windows", "body": "We have a project that depends on Tensorflow, using the following in our WORKSPACE file:\r\nlocal_repository(\r\n  name = \"org_tensorflow\",\r\n  path = __workspace_dir__ + \"/Libraries/TensorFlow/\",\r\n)\r\nand \r\n\"@org_tensorflow//tensorflow/core:tensorflow\"\r\nunder deps in the BUILD file.\r\nIt builds correctly on osX.\r\n\r\nWe also have a Windows machine that we've set up to build tensorflow (Standard set up: VS 2015 community, java 8, Anaconda Python 3.5, etc) and running the simple build (e.g. build a PIP wheel) works.\r\n\r\nHowever, trying to build our project on Windows fails when it hits tensorflow's protos.\r\nThe precise error message is\r\nERROR: C:/tmp/Bazel/D6SVld$i/external/org_tensorflow/tensorflow/core/kernels/BUILD:272:1: null failed: protoc.exe failed: error executing command\r\n  cd C:/tmp/Bazel/D6SVld$i/execroot/face_tf_desktop\r\nbazel-out/host/bin/external/protobuf/protoc.exe --cpp_out=bazel-out/vc_14_0_x64-opt/genfiles/external/org_tensorflow -Iexternal/org_tensorflow -Ibazel-out/vc_14_0_x64-opt/genfiles/external/org_tensorflow -Iexternal/protobuf/src -Ibazel-out/vc_14_0_x64-opt/genfiles/external/protobuf/src external/org_tensorflow/tensorflow/core/kernels/reader_base.proto: com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status -1073741515.\r\n\r\nTrying to run that command manually in the shell produces: \r\nbazel-out/vc_14_0_x64-opt/genfiles/external/protobuf/src: warning: directory does not exist.\r\n(Accurately: the protobuf folder doesn't exist)\r\n\r\nI'm not sure whether this is a bug in tensorflow or in Bazel (or, for that matter, whether the local_repository approach isn't meant to work and only works as a fluke on the Mac.)\r\n\r\nI have tried building with the newest Bazel release 0.4.3, and previously with 0.4.1.", "comments": ["Update: I have just noticed that my Mac was sitting on an old Tensorflow commit on master (d9c173f from the 5th of December), and that changing it to later commits introduces the same or similar proto failures on the Mac.\r\n\r\nHowever, setting Windows back to that commit still fails with a (different) proto error. \r\n\r\nI suspect this means it is not a Bazel problem, and will have another look for errors in my build system."]}, {"number": 6572, "title": "Server finished RPC without an explicit exit code -bash: Server: command not found when use bazel build tensorflow", "body": "### Environment info\r\nOperating System:\r\ncentos 6.5 x86_64\r\nldd (GNU libc) 2.12\r\ngcc (GCC) 4.8.2\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n [****tensorflow]# git rev-parse HEAD\r\nac28ae043df4bc6f112f964f6df22845b8a05390\r\n2. The output of `bazel version`\r\nbazel  0.4.3\r\nI build bazel and install successfully\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nbazel build -c opt //tensorflow/tools/pip_package:build_pip_package                                               \r\n\r\n\r\n### What other attempted solutions have you tried?\r\n1: rebuild and still have errors\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nINFO: From Compiling tensorflow/core/distributed_runtime/rpc/grpc_worker_service_impl.cc:\r\ntensorflow/core/distributed_runtime/rpc/grpc_worker_service_impl.cc: In function 'const char* tensorflow::GrpcWorkerMethodName(tensorflow::GrpcWorkerMethod)':\r\ntensorflow/core/distributed_runtime/rpc/grpc_worker_service_impl.cc:50:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nINFO: From Compiling tensorflow/core/debug/debug_io_utils.cc:\r\ntensorflow/core/debug/debug_io_utils.cc:212:15: warning: 'tensorflow::Status tensorflow::CloseDebugURL(const string&)' defined but not used [-Wunused-function]\r\n static Status CloseDebugURL(const string& debug_url) { return Status::OK(); }\r\n               ^\r\nServer finished RPC without an explicit exit code", "comments": ["@anddelu Hi, have you solved the issue? I saw a similar error message \"Server finished RPC without an explicit exit code\" when installing tensorflow from source. Details [here](https://github.com/tensorflow/tensorflow/issues/6678). Please let me know if you have any suggestions. Thanks in advance! :-)", "@sugartom I have tried to rebuild the source, but I added some parameters \"--local_resources 2048,.5,1.0\" when I used bazel. You can fix 2048 to 8096 or else based on your environment. I once tried 2048, it take long time to do. I suggest you can set bigger if you want to fast. Good luck!  ", "This problem may be caused by the rpc timeout. Bazel has such a problem with some type of Java Runtime. It maps all processes it spawned to a single core of your machine. So all threads will run slow and at some point, the bazel server will experience a RPC timeout and shutdown.\r\n\r\nTo fix this problem:\r\n1) Use --batch mode instead of server/client mode of Bazel. add --batch --batch_cpu_scheduling as startup options. Like, bazel --batch --batch_cpu_scheduling build -c opt...\r\n2) Increase rpc timeout using other startup options.", "I will close this issue. I don't see how we can do anything about this on the TensorFlow side. The workarounds should work.", "Switching to batch mode seems to get the process Killed (I'm guessing memory?) so i'm looking at the other option- You mentioned you can increase the 'rpc timeout' , how do you do this? I can't seem to find documentation on doing so."]}, {"number": 6571, "title": "Android nightly fixes", "body": "This time tested for real:\r\nhttp://ci.tensorflow.org/job/experimental-android/5/\r\n\r\nPS: 25 minutes on a 32 CPU machine, wow this build is heavy!", "comments": ["s/android_nightly/android_full/ sgtm\r\n\r\nI worry android_build_only is misleading because they both only build (for now), and in the future we may want to throw some quick tests into the presubmit. What about android_presubmit or android_quick?", "+1 s/android_nightly/android_full/ \r\nFor the quicker android build, I suggest we just keep the current name \"android\".", "http://ci.tensorflow.org/job/experimental-android/TF_BUILD_CONTAINER_TYPE=ANDROID_FULL,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=NO_PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/13/console\r\n\r\nI am seeing some failures for my experimental build on this, but not sure what it means:\r\n\r\n```\r\nINFO: Reading 'startup' options from /var/lib/jenkins/workspace/experimental-android/TF_BUILD_CONTAINER_TYPE/ANDROID_FULL/TF_BUILD_IS_OPT/OPT/TF_BUILD_IS_PIP/NO_PIP/TF_BUILD_PYTHON_VERSION/PYTHON2/label/cpu-slave/bazel-ci_build-cache/.bazelrc: --batch\r\nTF_BUILD_INFO = {container_type: \"android\", command: \"/tmp/tf_build.sh\", source_HEAD: \"9bdea3de08242f40a0fd391e8c41726dd56cc478\", source_remote_origin: \"https://github.com/gunan/tensorflow.git\", OS: \"Linux\", kernel: \"4.4.0-36-generic\", architecture: \"x86_64\", processor: \"Intel(R) Xeon(R) CPU @ 2.50GHz\", processor_count: \"32\", memory_total: \"123776972 kB\", swap_total: \"0 kB\", Bazel_version: \"Build label: 0.4.2\", Java_version: \"1.8.0_111\", Python_version: \"2.7.6\", gpp_version: \"g++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4\", swig_version: \"\", NVIDIA_driver_version: \"\", CUDA_device_count: \"0\", CUDA_device_names: \"\", CUDA_toolkit_version: \"\"}\r\n/tmp/tf_build.sh: line 2: -c: command not found\r\n```\r\n\r\n@caisq how do we construct tf_build.sh? What should be there before `-c`?\r\n"]}, {"number": 6570, "title": "Update version string to 1.0.0-alpha", "body": "", "comments": ["@tensorflow-jenkins test this please."]}, {"number": 6569, "title": "Branch 143206951", "body": "", "comments": []}, {"number": 6568, "title": "Get rid of setdlopen() in tests", "body": "We need to find a new strategy for loading `.so` files at runtime that depend on other `.so` files. This is so we can remove the dlopen hack, which is now present in >100 files.\r\n\r\nThe solution needs to meet the requirement that each .py file fully specifies its dependencies and does not rely on code in `__init__.py` files to run beforehand. This requirement is necessary in order to maintain the work we've done to: a) remove hourglass imports so affected tests can be calculated better; and b) so Python modules can be loaded lazily like Java.\r\n\r\n## Background\r\n\r\nWe've refactored TensorFlow to remove hourglass imports in https://github.com/tensorflow/tensorflow/commit/5866e065bc95c1d7de8a27413b368016941889a6, https://github.com/tensorflow/tensorflow/commit/58201a058853de647b37ddb0ccf63d89b2357f03, etc. Since tests no longer depend on the master `__init__.py` file, we've needed to add code like this to the tops of _test.py files that load custom ops defined by contrib libraries:\r\n\r\n```python\r\nimport sys\r\n\r\nif hasattr(sys, \"getdlopenflags\") and hasattr(sys, \"setdlopenflags\"):\r\n  import ctypes\r\n  sys.setdlopenflags(sys.getdlopenflags() | ctypes.RTLD_GLOBAL)\r\n\r\nfrom google3.third_party.tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl\r\nfrom google3.third_party.tensorflow.python.framework import ops\r\n...\r\n```\r\n\r\nOtherwise we get errors like `undefined symbol: _ZTIN10tensorflow8OpKernelE` e.g.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.py\", line 26, in <module>\r\n    from tensorflow.contrib.slim.python.slim.nets import inception_v2\r\n  File \"bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/contrib/slim/python/slim/nets/inception_v2.py\", line 21, in <module>\r\n    from tensorflow.contrib import layers\r\n  File \"bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/contrib/layers/__init__.py\", line 118, in <module>\r\n    from tensorflow.contrib.layers.python.layers import *\r\n  File \"bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/__init__.py\", line 22, in <module>\r\n    from tensorflow.contrib.layers.python.layers.embedding_ops import *\r\n  File \"bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/embedding_ops.py\", line 21, in <module>\r\n    from tensorflow.contrib.layers.python.ops import sparse_feature_cross_op\r\n  File \"bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/ops/sparse_feature_cross_op.py\", line 31, in <module>\r\n    resource_loader.get_path_to_datafile(\"_sparse_feature_cross_op.so\"))\r\n  File \"bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/contrib/util/loader.py\", line 42, in load_op_library\r\n    ret = load_library.load_op_library(path)\r\n  File \"bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/python/framework/load_library.py\", line 64, in load_op_library\r\n    None, None, error_msg, error_code)\r\ntensorflow.python.framework.errors_impl.NotFoundError: bazel-out/local-fastbuild/bin/tensorflow/contrib/slim/python/slim/nets/inception_v2_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/ops/_sparse_feature_cross_op.so: undefined symbol: _ZTIN10tensorflow8OpKernelE\r\n```\r\n\r\nCC: @gunan @martinwicke @yifeif ", "comments": ["Passing on the assignment to @keveman, since he came up with the library-loading code in the first place.\r\n\r\nI don't understand why `setdlopenflags()` needs to appear in so many test files in the first place.\r\nIt seems (at least superficially) that the problem is easy to fix: we just need to ensure that all imports of `tensorflow.python.pywrap_tensorflow` are guarded by the same flag-setting-and-unsetting logic that appears in [`tensorflow/python/__init__.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/__init__.py#L58). So we can either:\r\n\r\n1. Add the logic to the approximately 20 places where this module is imported, or\r\n2. Add a new wrapper module that is the only module allowed to import `tensorflow.python.pywrap_tensorflow` (perhaps there's a cunning way to enforce this?), implement the flag-setting logic in there, and then replace the 20 direct imports of `tensorflow.python.pywrap_tensorflow` with an import of the wrapper module.", "29 files import that module. Even more probably import it indirectly via init files, which we might unroll in the future. I'm also considering breaking up our giant swig module into a bunch of tiny swig modules. Having to have the try/setdlopen-import/except/setdlopen every time we import a swig module isn't going to be great. A delegate module could work. But there might be a better option:\r\n\r\n3. Ask SWIG to to put the setdlopenflags() stuff into the generated `pywrap_tensorflow.py` file. See: https://www.mail-archive.com/users@lists.open-mpi.org/msg09998.html", "Another thing we need to figure out how to do is situations like these:\r\n\r\n```python\r\nfrom tensorflow.python import pywrap_tensorflow\r\nfrom tensorflow.contrib.tfprof.python.tools.tfprof import pywrap_tensorflow_print_model_analysis_lib\r\n```\r\n\r\nThe first swig module needs to be loaded in order for the second one to load. So we need to be able to generate code inside `pywrap_tensorflow_print_model_analysis_lib.py` that imports `pywrap_tensorflow`.\r\n\r\nNow this seems like a build system issue. Maybe we could change our swig Bazel rule to mangle the `.i` input file to inject the code we want into the generated `.py` binding.", "/CC @drpngx \r\n\r\nI think I like the idea of wrapping pywrap_tensorflow with a different module, and importing that wrapper in all locations pywrap_tensorflow is needed. I think this approach has some parallels with @drpngx 's approach for sealing the python API. Moreover, I think pywrap_tensorflow is still not sealed, so the wrapper can also do the job to seal it. What do you think?\r\n\r\nHowever, once we replace big monolithic swig with a nicer mechanism that lets us split up the wrapped library, I am not sure how much work wrapping approach will need, as we will potentially have many wrapped libraries now.\r\n\r\nFor the 2nd issue @jart raised, bazel magic scares me a little, but it is quite powerful. So I think the solution you suggested can work, but I am not sure I like it.\r\n", "I left the pywrap unsealed, and under the tf namespace, because it's in the main package, and packages are different from modules, which are easier. I recently discovered a way to remove it from the main package, so I might try that. (The issue was that if you delete from the main package, you can't import it anymore. I found out that you can actually import it using `from ... python import foo` but not `from .. python.foo import bar`)", "This didn't fixed it for me. Actually that was a custom user op in my case. I think #13607 describes my issue. I wonder a bit that on my local system (Ubuntu 16.04), this doesn't seem to be needed but on Travis, it seems to be needed."]}, {"number": 6567, "title": "Make *args in example iterable", "body": "", "comments": ["Can one of the admins verify this patch?", "This is an autogenerated file.\r\nIt is generated from the inline comments in file `tensorflow/python/training/supervisor.py`\r\nCould you instead modify that file? Otherwise website release will simply override your changes.", "Aha, thanks!"]}, {"number": 6566, "title": "Branch 143204614", "body": "", "comments": ["let's try this again"]}, {"number": 6565, "title": "Branch 143201255", "body": "", "comments": []}, {"number": 6564, "title": "Update tf.nn.conv2d_transpose.md", "body": "This seems wrong.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "This is an autogenerated file which is generated from this:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1089\r\n\r\nPlease edit the inline comments instead of the documentation, it will be picked up in our next website release.\r\n\r\nAlso, please sign the Google CLA, and use the same email to create the git commits for CLA bot to resolve the CLA.\r\n\r\nI am closing this PR. Please address the above issues, then we can create a new PR or reopen this one."]}, {"number": 6563, "title": "Tensorflow 0.12 on Windows - TypeError: init() got an unexpected keyword argument 'dense_shape' running example", "body": "I downloaded the latest Windows native version of Tensorflow and having problems running some examples. I have Windows 10 (TF 0.12) with python 3.5.2.\r\n\r\nThe issue is with keyword dense_shape while running the wide_n_deep_tutorial shipped with the package. I pulled the latest version of TF as well but still didn't resolve the problem:\r\n\r\nTypeError: init() got an unexpected keyword argument 'dense_shape'\r\n\r\nSpecifically, the issue is in the following section:\r\n\r\n  categorical_cols = {\r\n     k: tf.SparseTensor(\r\n          indices=[[i, 0] for i in range(df[k].size)],\r\n          values=df[k].values,\r\n          dense_shape=[df[k].size, 1])\r\n      for k in CATEGORICAL_COLUMNS}\r\n\r\nPer documentation SparseTensor does not seem to understand dense_shape but IndexedSlices does. But I would like examples to work as is, without modification.\r\n\r\nCannot comment whether this happens on Unix also. Any ideas on how to resolve? Thanks!", "comments": ["The `tf.SparseTensor` API has changed between the 0.12 release and the git master. You should run the version of `wide_n_deep_tutorial.py` from the `r0.12` branch when you have TensorFlow 0.12 installed:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/examples/learn/wide_n_deep_tutorial.py", "I'm closing this because it probably doesn't need a response... feel free to reopen if it still doesn't work.", "Thanks, helped a lot."]}, {"number": 6562, "title": "More numerically stable gradients for division.", "body": "This PR improves the numerical stability of gradients for the division operator. \r\n\r\nLet `f = x / y` such that `df/dy = - x / y ** 2`. But if `y` is of small magnitude, `y ** 2` can underflow, and if `y` is of large magnitude, `y ** 2` can overflow. It is thus better to compute the derivative as `df/dy - (x / y) / y`. The changes are illustrated in the figure below using 32-bit floating point representations.\r\n\r\n![divgrad](https://cloud.githubusercontent.com/assets/966348/21547770/0fcae1f8-cde0-11e6-9090-524f24bff1a1.png)\r\n\r\nThe coverage of the new implementation is strictly greater than the coverage of the old implementation, where coverage is the domain in which the gradient is defined.", "comments": ["Can one of the admins verify this patch?", "Here is a [notebook](https://gist.github.com/tillahoffmann/e2e639335f89f2e5e270bb18edd11ded) to reproduce the figure above.", "Jenkins, test this please.", "Ignoring unrelated errors."]}, {"number": 6561, "title": "Android Camera Demo Error when trying to replace default inception model  ", "body": "Hey, \r\n\r\nI am trying tu run the android camera demo with different model.\r\nTo get a simple use case, I started with a retrained inception model produced from the TensorFlow for Poets codelab tutorial classifying flowers.\r\n\r\nWhat I have done ?\r\nI place the new model pb file and the new label txt file in the assets directory.\r\nI followed the recommendations specified in the ClassifierActivity.java file \r\n1/ set IMAGE_SIZE = 299, IMAGE_MEAN = 128, IMAGE_STD = 128\r\n2/ set INPUT_NAME = \"Mul:0\", and OUTPUT_NAME = \"final_result:0\"\r\nI bazel build and install the apk file.\r\n\r\nRunning the app, I got this error in the logcat \r\n\r\n_**E/native: tensorflow_inference_jni.cc:138 Could not create Tensorflow Graph: Invalid argument: No OpKernel was registered to support Op 'DecodeJpeg' with these attrs.  Registered devices: [CPU], Registered kernels: <no registered kernels>  [[Node: DecodeJpeg = DecodeJpeg [ acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false](DecodeJpeg/contents)]]**_\r\n\r\nTo my understanding this node is preceding my input node and is useless in this case.\r\nWhat is the solution to get rid of it ?\r\n\r\nThanks in advance for your answer.\r\nAlex", "comments": ["@AlexandreBriot You can run [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) on your GraphDef to strip out the unsupported-on-mobile nodes like DecodeJpeg.", "Thanks for your answer !\r\nI tried to run optimze_for_inference.py.\r\nIt works to remove the unneeded input nodes but it also removed other nodes at the end of the GraphDef before the output node. Do you have any idea about what I could have done wrong ? \r\n\r\nHere is how I proceed\r\n```\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\nwith tf.gfile.FastGFile(\"/home/alexandre/tf_files/retrained_graph.pb\", 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def, name='')\r\noutput_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n    graph_def,\r\n    ['Mul'],\r\n    ['final_result'],\r\n    tf.float32.as_datatype_enum)\r\n```\r\nThen printing the first and last nodes names of my out_graph_def \r\n```\r\nprint(output_graph_def.node[0].name)\r\nprint(output_graph_def.node[-1].name)\r\n```\r\nI get \r\n\r\n```\r\nMul\r\nmixed_10/tower_2/conv/batchnorm\r\n```\r\n\r\nWhich is OK for input but KO for output\r\n\r\n\r\n", "@AlexandreBriot What parameters did you pass to optimize_for_inference? Are you certain your that the final_result node has actually been removed? AFAIK there is no guarantee that the nodes will be defined in the GraphDef in run order.", "@andrewharp Thanks for your help !!!\r\nI got confused by the list order of the nodes which is modified during inference optimizing.\r\n"]}, {"number": 6560, "title": "No tf.batch_matmul operation on source built 12.1 distribution ", "body": "Hi all,\r\nI've build from sources the latest version (0.12.1) on my Ubuntu 16 machine.\r\nThen I've found out that I no longer have tf.batch_mat:\r\n\r\nimport tensorflow as tf\r\ntf.batch_matmul\r\n\r\nTraceback (most recent call last):\r\n  File \"<ipython-input-2-95fcb8b3e629>\", line 1, in <module>\r\n    tf.batch_matmul\r\nAttributeError: module 'tensorflow' has no attribute 'batch_matmul'\r\n\r\nAny ideas for a solution?\r\n\r\nSome details:\r\n\r\n ls -l /usr/local/cuda-8.0\r\ntotal 64\r\ndrwxr-xr-x  3 root root 4096 Sep 28 12:39 bin\r\ndrwxr-xr-x  5 root root 4096 Sep 28 12:38 doc\r\ndrwxr-xr-x  5 root root 4096 Sep 28 12:38 extras\r\ndrwxr-xr-x  5 root root 4096 Sep 28 12:44 include\r\ndrwxr-xr-x  5 root root 4096 Sep 28 12:38 jre\r\ndrwxr-xr-x  3 root root 4096 Nov  8 09:46 lib64\r\ndrwxr-xr-x  8 root root 4096 Sep 28 12:38 libnsight\r\ndrwxr-xr-x  7 root root 4096 Sep 28 12:38 libnvvp\r\ndrwxr-xr-x  3 root root 4096 Sep 28 12:38 nvml\r\ndrwxr-xr-x  7 root root 4096 Sep 28 12:38 nvvm\r\ndrwxr-xr-x  2 root root 4096 Sep 28 12:39 pkgconfig\r\ndrwxr-xr-x 11 root root 4096 Sep 28 12:39 samples\r\ndrwxr-xr-x  3 root root 4096 Sep 28 12:38 share\r\ndrwxr-xr-x  2 root root 4096 Sep 28 12:38 src\r\ndrwxr-xr-x  2 root root 4096 Sep 28 12:38 tools\r\n-rw-r--r--  1 root root   20 Sep 28 12:38 version.txt\r\n\r\n\r\ngit rev-parse HEAD\r\nac28ae043df4bc6f112f964f6df22845b8a05390\r\n\r\n\r\n bazel version\r\nBuild label: 0.4.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 12:31:25 2016 (1482409885)\r\nBuild timestamp: 1482409885\r\nBuild timestamp as int: 1482409885\r\n\r\n\r\nThanks!\r\n\r\n\r\n", "comments": ["The `tf.batch_matmul()` op was removed in https://github.com/tensorflow/tensorflow/commit/3a88ec0b8e5a9b8f3f3efc4d9a8f88d47b3f4042. You can now use [`tf.matmul()`](https://www.tensorflow.org/api_docs/python/math_ops/matrix_math_functions#matmul) to perform batch matrix multiplications (i.e. for tensors with rank > 2).", "Well, it currently breaks Keras with the new version of TensorFlow, but I guess that it should be solved there.\r\nThanks :)"]}, {"number": 6559, "title": "Fix: make sure android_nightly is a recognized container type in ci scripts", "body": "", "comments": ["@tensorflow-jenkins test this please."]}, {"number": 6558, "title": "`TensorContractionThreadPool.h' file not found` when using `--copt=-march=native`", "body": "When trying to run \r\n\r\n```\r\n$ ./configure\r\n........\r\n$ bazel build --copt=-march=native -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\n\r\nit fails with the `./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:10: fatal error: 'src/Tensor/TensorContractionThreadPool.h' file not found` error.\r\n\r\nSimilar problem was seen in #4680, #580, \r\n\r\nI am on Mac OS X Sierra 10.12.2 with no CUDA and cuDNN installed. I am using virtual environment on Python 2.7\r\n\r\nHere is the checked commit and `bazel` version\r\n```\r\n$ git rev-parse HEAD\r\nac28ae043df4bc6f112f964f6df22845b8a05390\r\n$ bazel version\r\nBuild label: 0.4.3-homebrew\r\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 15:20:15 2016 (1482420015)\r\nBuild timestamp: 1482420015\r\nBuild timestamp as int: 1482420015\r\n```\r\n\r\nThis works fine:\r\n```\r\n$ git checkout remotes/origin/r0.12\r\n$ ./configure # With defaults\r\n.........\r\n$ bazel build --copt=-march=native -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n# This finishes correctly\r\n```\r\n\r\nHere is the full terminal log when `bazel` is run on the `master` branch:\r\n```\r\n$ git checkout master\r\n$ ./configure # With defaults\r\n.........\r\n$ bazel build --copt=-march=native -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\nINFO: Found 1 target...\r\nERROR: /Users/zafar/GitHub/tensorflow/tensorflow/core/kernels/BUILD:856:1: C++ compilation of rule '//tensorflow/core/kernels:gather_functor' failed: cc_wrapper.sh failed: error executing command\r\n  (cd /private/var/tmp/_bazel_zafar/c40dcca6773e4515fd11c724e0ebae51/execroot/tensorflow && \\\r\n  exec env - \\\r\n    PATH=/Users/zafar/.virtualenvs/tf-dev-env/bin:/Users/zafar/.rbenv/shims:/Users/zafar/.node/bin:/Users/zafar/bin:/Users/zafar/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/MacGPG2/bin:/Library/TeX/texbin \\\r\n    TMPDIR=/var/folders/yx/nf9nvmpn4_79r7nbzkn8cz8h0000gn/T/ \\\r\n  external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++0x' -MD -MF bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.d '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o' -fPIC -DEIGEN_MPL2_ONLY -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local-opt/genfiles/external/local_config_sycl -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/local-opt/genfiles/external/eigen_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/core/kernels/gather_functor.cc -o bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/gather_functor/tensorflow/core/kernels/gather_functor.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from tensorflow/core/kernels/gather_functor.cc:50:\r\nIn file included from ./tensorflow/core/kernels/gather_functor.h:22:\r\nIn file included from ./tensorflow/core/framework/type_traits.h:22:\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:25:\r\n./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:10: fatal error: 'src/Tensor/TensorContractionThreadPool.h' file not found\r\n#include \"src/Tensor/TensorContractionThreadPool.h\"\r\n         ^\r\n1 error generated.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 34.692s, Critical Path: 25.26s\r\n```\r\n", "comments": ["I can attach the `pip freeze` of my virtual environment if required, but I think this is an issue with `Eigen` not being downloaded", "2016-12-29 16:04 GMT+08:00 Zafar Takhirov <notifications@github.com>:\n\n> I can attach the pip freeze of my virtual environment if required, but I\n> think this is an issue with Eigen not being downloaded\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6558#issuecomment-269595095>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKyIBkkGUlvgP3653ijP9iN8p53mEdv-ks5rM2mggaJpZM4LXaje>\n> .\n>\n", "If it helps, I've \"resolved\" the issue for me by tinkering around a bit and adding the missing header:\r\n\r\n1. Add [`TensorContractionThreadPool.h`](https://bitbucket.org/eigen/eigen/raw/9ba936354ee8b73fb1966dcb2d3506387bb357f1/unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h) from [Eigen](https://bitbucket.org/eigen/eigen) \"itself\" to TF's `third_party/eigen3/unsupported/Eigen/CXX11/src/Tensor` directory (note that the leaf directory `Tensor` has to be created first).\r\n1. Update the `hdrs` parameter of the cc_library function in the Bazel `third_party/eigen3/BUILD` file to include the newly added file by inserting the following line in the list of files there: `\"unsupported/Eigen/CXX11/src/Tensor/TensorContractionThreadPool.h\",`\r\n\r\nAfter that, my TF builds with AVX2 optimizations pass and I can nicely use a fully optimized, CUDA-enabled TF (so far, at least...).\r\n\r\nBut I have exactly zero idea if that's indeed the right fix... YMMV ;-)", "@fnl I can get it to work by using the `r0.12` branch, so there is no need for tinkering and manually placing the files. Also, `r0.12` has no major differences with the `master` (ac28ae043df4bc6f112f964f6df22845b8a05390). The reason I opened an issue is because it might affect the future releases, and we need to find what caused it.", "I have had the same issue (except with --copt=-mavx2 instead of --copt=-march=native), which is now present due to changes since the `r0.12` branch.\r\n\r\nIt seems that removing the two include lines, #include \"src/Tensor/TensorContractionThreadPool.h\", in [FixedPoint](https://github.com/tensorflow/tensorflow/blob/master/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint) from this commit (https://github.com/benoitsteiner/tensorflow/commit/8c943af3b139f07aa1f3ba5c976e2f0765e75960) allows bazel to compile, but when I try to use it after installation it fails:\r\n\r\n```\r\nPython 3.5.2 (default, Nov 17 2016, 17:05:23) \r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"/home/s.antol/cv/tf/tensorflow/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name 'pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/s.antol/cv/tf/tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/s.antol/cv/tf/tensorflow/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/s.antol/cv/tf/tensorflow/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\nImportError: cannot import name 'pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n```\r\n\r\nPerhaps someone more familiar with the build system would know how to properly remedy this.\r\n\r\nEdit:\r\nGood catch, @zafartahirov ! I switched to a non-tensorflow directory and the python import was successful and I was able to run the mnist convolutional example without any problems. I still don't know if removing those two ``#include``s is the \"right\" thing to do, but it seems to work for now. Hopefully a tf dev can comment on this.\r\n\r\n@zafartahirov , if you think Eigen hasn't been downloaded, in your cloned directory, run ``ls $(bazel info output_base)/external`` and see if eigen_archive is there and all the correct files are there. Based on my \"fix\", I think it will have downloaded fine and it's just a bug in the build/include setup.", "@StanislawAntol I don't think your error is related -- you are running TF from the same directory where you cloned the repo. Try going to any other directory, the error will be different", "So I'm running into same problem when trying to compile with avx/avx2. The following works on \"tags/0.12.0-rc1\" tag, but doesn't work on today's head. @benoitsteiner -- something in the last 2 weeks broke avx/avx2 compilation, what do you think about @fnl's solution in https://github.com/tensorflow/tensorflow/issues/6558#issuecomment-269643996 ?\r\n\r\n```\r\nexport flags=\"-c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --config=cuda\"\r\nbazel build $flags //tensorflow/tools/pip_package:build_pip_package\r\n\r\n...\r\ncompilation terminated.\r\nERROR: /local_home/yaroslav/tensorflow_avx2.git/tensorflow/tensorflow/core/kernels/BUILD:582:1: Couldn't build file tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op_cpu_impl_1.pic.o: C++ compilation of rule '//tensorflow/core/kernels:mirror_pad_op' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 126 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:25:0,\r\n                 from ./tensorflow/core/framework/register_types.h:20,\r\n                 from ./tensorflow/core/kernels/mirror_pad_op_cpu_impl.h:21,\r\n                 from tensorflow/core/kernels/mirror_pad_op_cpu_impl_1.cc:17:\r\n./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:52: fatal error: src/Tensor/TensorContractionThreadPool.h: No such file or directory\r\n #include \"src/Tensor/TensorContractionThreadPool.h\"\r\n```", "I have same problem.. when installing Tensorflow from source on Python3.6\r\n\r\n```\r\nbazel build --copt=-march=native  -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n\r\n```\r\nERROR: /home/anderson/Downloads/tensorflow/tensorflow/core/kernels/BUILD:300:1: C++ compilation of rule '//tensorflow/core/kernels:reader_base' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 115 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from ./tensorflow/core/framework/numeric_types.h:25:0,\r\n                 from ./tensorflow/core/framework/allocator.h:23,\r\n                 from ./tensorflow/core/framework/op_kernel.h:22,\r\n                 from ./tensorflow/core/framework/queue_interface.h:22,\r\n                 from ./tensorflow/core/kernels/reader_base.h:21,\r\n                 from tensorflow/core/kernels/reader_base.cc:16:\r\n./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:42:52: fatal error: src/Tensor/TensorContractionThreadPool.h: No such file or directory\r\ncompilation terminated.\r\n```\r\n\r\n", "Same issue, I was able to work around it following @fnl directions in the post above, I do not know enough about the build system used here to know if this is the \"correct\" way of fixing it.\r\n\r\n`\r\nbazel build -c opt --copt=-march=native --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Found 1 target...\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 629.828s, Critical Path: 392.63s\r\n`", "Same here, back to 0.12.1 would work well.", "Does #6759 Fix this issue?\r\nAre we OK to close this?", "It looks like it is working for `bazel build --copt=-march=native -c opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\nThe compilation is still going, but it was throwing the error almost immediately before", "I was also able to compile the most recent version of master with the following:\r\n`bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nAlso, I find it odd that the [nightly build](https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu-mavx/) doesn't seem to be working. I would think it's using similar commands.", "Nightlies are able to build, but they are having test failures.\r\nWe are investigating them, but occasional test failures in nightlies are expected.\r\n@zafartahirov Once (if)your build succeeds, could you close this issue.\r\nIf it does not, please share the new failures so we can continue debugging.", "Complete (with warnings about `-pthread`). Closing"]}, {"number": 6557, "title": "Remove manual nvidia-docker instructions.", "body": "", "comments": ["@tensorflow-jenkins test this please.", "A user added these lines (back), which seems to indicate that these commands could be useful to some users. If the concern is users asking more questions about vanilla docker not working with GPU, maybe we can just emphasize that this is not supported workflow?", "Are these instructions not simply manually setting everything nvidia-docker sets?\r\nI tried to write that if nvidia-docker does not work, do this and that, however these kind of things are usually diverting users from simple issues in nvidia-docker to a rather complicated one and returning to us with more questions than reducing questions.\r\n\r\nAs far as I can tell from the dialogue in the PR, the user was trying to get rid of docker_run_gpu script. Most of the wording here is leftover from when we had that script.\r\nI am still in favor of removing complicated instructions. Nvidia docker is much better than what it was earlier this year, and they are providing very timely support with questions."]}, {"number": 6556, "title": "[minor] Silenced the 'pushd'", "body": "It doesn't fix the #6555, it just silences the `pushd` during the `./configure` step.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 6555, "title": "[minor] Non-silenced `pushd` followed by `popd` in `tensorflow/configure`", "body": "`tensorflow/configure` has the following lines:\r\n\r\n```bash\r\n#!/usr/bin/env bash\r\n\r\nset -e\r\nset -o pipefail\r\n\r\n# Find out the absolute path to where ./configure resides\r\npushd `dirname $0` #> /dev/null\r\nSOURCE_BASE_DIR=`pwd -P`\r\npopd > /dev/null\r\n```\r\n\r\nI am not sure what is the purpose of the `pushd` followed by `popd` without change in directory. I think this is not necessary, and distracts when reading the source codes. Unless it is allowed to run the `configure` from the directory other than the tensorflow root, it is completely redundant.\r\n\r\nAlso, the `> /dev/null` is commented out which causes the current directory to print when running the script:\r\n\r\n```bash\r\n$ ./configure\r\n  ~/GitHub/tensorflow ~/GitHub/tensorflow\r\n  Please specify the location of python. [Default is ~/.virtualenvs/tf-dev-env/bin/python]:\r\n  .........\r\n```", "comments": ["@aselle looks like you added the lines in question?\r\n\r\nI believe this would be a sanity check, just making sure we are at the root.", "done\n\nOn Wed, Dec 28, 2016 at 11:20 PM gunan <notifications@github.com> wrote:\n\n> @aselle <https://github.com/aselle> looks like you added the lines in\n> question?\n>\n> I believe this would be a sanity check, just making sure we are at the\n> root.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6555#issuecomment-269591588>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAT52uw5qZBa5nQq04cAnDuRb_9K6rWhks5rM19IgaJpZM4LXZRG>\n> .\n>\n", "The `pushd` is silenced, but do we actually need it? I can see the use for it if we allow users to run `configure` from custom defined source directories. However, even in that case I would probably use something like\r\n\r\n```bash\r\nSOURCE_BASE_DIR=`pwd -P`\r\npushd ${PWD} >/dev/null\r\ncd `dirname $0`\r\n#\r\n# Perform some actions in the directory where 'configure' is\r\n#\r\npopd >/dev/null\r\n#\r\n# Perform some actions in the directory where the scipt was called from\r\n#\r\n```", "I'm confused. That code does seems to have an extra line and capture SOURCE_BASE_DIR of where you run it from rather than where configure is.\r\n", "@aselle That what the original script was doing -- I assumed it was on purpose -- the only reason it would be required is if there are two different locations that the script would have to work with: one where `SOURCE_BASE_DIR` is, and another where the `configure` is located. Assuming TF allows them to be in different locations, it makes sense to keep one of them on a stack, while working in the other. Otherwise, I think the script should be \r\n\r\n```bash\r\n#!/usr/bin/env bash\r\n\r\nset -e\r\nset -o pipefail\r\n\r\nSOURCE_BASE_DIR=`pwd -P`\r\n```\r\n\r\nI was trying to see the `Blame` for the `popd`, `pushd` lines, and this is the only thing there:\r\n\r\n```\r\nIt is necessary to symlink in files from .git/ in order to make\r\nbazel aware of changes to the current head. As it is this is not\r\ncompletely reliable when git repositories are in a dirty index\r\nstate. First class support for bazel git a reported bug but\r\nnot a high priority.\r\n\r\n./configure sets up the symlinks by calling the gen_git_source.py\r\na bazel genrule calls gen_git_source.py to generate version_info.cc\r\n\r\nAlso changed cmake and make to build this properly.\r\nChange: 132328009\r\n```", "@zafartahirov I think for using tensorflow as is, you have a point. However, when tensorflow is an external dependency, such as  how tensorflow/models or tensorflow/serving use TF, I believe users may need to run `configure` differently.\r\n\r\nAlso, as the pushd/popd lines have a comment describing what they do, I do not see a problem in terms of code readability.\r\n\r\nTherefore, I am inclined to close this issue. Let me know if the above explanations do not address your concerns. We can reopen the issue if you disagree with my assessment above."]}, {"number": 6554, "title": "Bug or feature - Confusing shapes of Tensor resulted from `tf.decode_csv`", "body": "\r\n```\r\nreader = tf.TextLineReader()\r\nkey, value = reader.read(filename_queue)\r\nparsed = tf.decode_csv(value, [[0.0]] * 24)\r\nlabel = parsed[0]\r\nweight = parsed[1]\r\nfeature = tf.pack(parsed[2:])\r\nlabel_tensor, weight_tensor, feature_tensor = tf.train.shuffle_batch(\r\n      [label, weight, feature],\r\n      batch_size=200,\r\n      capacity=capacity,\r\n      min_after_dequeue=min_after_dequeue,\r\n      num_threads=num_threads)\r\nprint(\"Parsed tensors: %s, %s, %s\" % )\r\n```\r\n\r\nThe shapes of `label_tensor`, `weight_tensor` and `feature_tensor` are **`(200, )`**, **`(200, )`** and **`(200, 22)`** respectively.\r\n\r\nIf we put an extra comma at the end of line 4 and 5, like this:\r\n```\r\nlabel = parsed[0],\r\nweight = parsed[1],\r\n```\r\nThen the shapes become **`(200, 1)`**, **`(200, 1)`** and **`(200, 26)`** respectively.\r\n\r\nSo an extra comma determines the shape of the resulted tensor. I am not quite familiar with Python and am new to TensorFlow, but is it supposed so?\r\n\r\n", "comments": ["The extra comma is significant: it turns a `tf.Tensor` object (e.g. `parsed[0]`) into a single-element `tuple` (e.g. `(parsed[0],)`).  When you pass them to APIs that expect a single `tf.Tensor`, TensorFlow automatically packs lists and tuples of `tf.Tensor` objects into a single `tf.Tensor` with an additional dimension. Therefore, since `parsed[0]` is a scalar (a 0-dimensional tensor), `parsed[0],` is treated as a single-element vector (a 1-dimensional tensor).\r\n\r\nThe `tf.train.shuffle_batch(..., batch_size=200, ...)` method accumulates a batch of 200 labels, weights, and features, by packing together 200 randomly sampled parsed inputs. Batching 200 scalars gives a shape of `(200,)`, whereas batching 200 single-element vectors gives a shape of `(200, 1)`.\r\n\r\nHope this helps. Note that in future, queries like this one are better handled on Stack Overflow, so that they're more easily searchable.", "Thanks Derek for the detailed explanation. My question is resolved.\n\nOn Thu, Dec 29, 2016 at 3:59 PM, Derek Murray <notifications@github.com>\nwrote:\n\n> The extra comma is significant: it turns a tf.Tensor object (e.g.\n> parsed[0]) into a single-element tuple (e.g. (parsed[0],)). When you pass\n> them to APIs that expect a single tf.Tensor, TensorFlow automatically\n> packs lists and tuples of tf.Tensor objects into a single tf.Tensor with\n> an additional dimension. Therefore, since parsed[0] is a scalar (a\n> 0-dimensional tensor), parsed[0], is treated as a single-element vector\n> (a 1-dimensional tensor).\n>\n> The tf.train.shuffle_batch(..., batch_size=200, ...) method accumulates a\n> batch of 200 labels, weights, and features, by packing together 200\n> randomly sampled parsed inputs. Batching 200 scalars gives a shape of\n> (200,), whereas batching 200 single-element vectors gives a shape of (200,\n> 1).\n>\n> Hope this helps. Note that in future, queries like this one are better\n> handled on Stack Overflow, so that they're more easily searchable.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6554#issuecomment-269594672>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAOttIsfFLVZuPEVjC9IyaNm3-ljd51Nks5rM2hygaJpZM4LXVX->\n> .\n>\n"]}, {"number": 6553, "title": "Issue with tf.InteractiveSession()", "body": "I try to train a neural net with nested loop `tf.while_loop`. If I start the session with `sess = tf.Session()`, the training process will start with the following log for one batch. There seems to be some memory issue, but it still works.\r\n[log1.txt](https://github.com/tensorflow/tensorflow/files/676557/log1.txt)\r\n\r\n\r\nBut if I start with `sess = tf.InteractiveSession()`, there will be errors. The log is as follows. `tf.InteractiveSession()` isn't really different from `tf.Session()` by tutorial. I just wonder what is wrong here.\r\n[log2.txt](https://github.com/tensorflow/tensorflow/files/676555/log2.txt)\r\n\r\n\r\n### Environment info\r\n- Ubuntu  14.04.5 LTS\r\n- Python 2.7.6\r\n- cuda 8, V8.0.44\r\n- cudnn 5.1.3\r\n- TensorFlow 0.12.0-rc0\r\n- GeForce GTX 1080\r\n\r\n", "comments": ["You can see [from implementation](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/python/client/session.py#L1315) that `InteractiveSession` uses regular `Session`, and registers it to be the default session. So I can't see how the difference would cause DeviceAssignment error you are seeing. If this is still a problem, can you provide a reproducible example?\r\n\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'gradients/while/while/TensorArrayWriteV2_grad/TensorArrayReadV2/f_acc': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nTensorArrayWriteV2: GPU CPU \r\nTensorArrayReadV2: GPU CPU \r\nTensorArrayScatterV2: GPU CPU \r\nTensorArrayGatherV2: GPU CPU \r\nRange: GPU CPU \r\nTensorArraySizeV2: GPU CPU \r\nTensorArrayV2: GPU CPU \r\nStack: GPU CPU \r\nStackPop: GPU CPU \r\nControlTrigger: GPU CPU \r\nConst: GPU CPU \r\nRefEnter: GPU CPU \r\nEnter: GPU CPU \r\nTensorArrayGradV2: GPU CPU \r\nIdentity: GPU CPU \r\nStackPush: GPU CPU \r\nExit: GPU CPU \r\n\t [[Node: gradients/while/while/TensorArrayWriteV2_grad/TensorArrayReadV2/f_acc = Stack[_class=[\"loc:@while/TensorArray_2\", \"loc:@while/while/Identity_1\"], elem_type=DT_INT32, stack_name=\"\"]()]]\r\n```", "Closing due to lack of recent activity. We will reopen when additional information becomes available. Thanks!"]}, {"number": 6552, "title": "Issue with tf.matrix_diag() in 0.12.0 in Windows GPU mode (CUDA_ERROR_ILLEGAL_ADDRESS)", "body": "Here is my code:\r\n```\r\nwith tf.Session() as sess:\r\n    # with tf.device(\"/cpu:0\"):\r\n    x = tf.ones(shape=[3, 3])\r\n    x_diag = tf.diag_part(x)\r\n    x_diag_matrix = tf.matrix_diag(x_diag)\r\n    print(sess.run(x_diag_matrix))\r\n```\r\nIt works ok on a CPU but fails in a GPU mode with the following 'CUDA_ERROR_ILLEGAL_ADDRESS' error:\r\n\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cublas64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cudnn64_5.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library cufft64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library nvcuda.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\dso_loader.cc:128] successfully opened CUDA library curand64_80.dll locally\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:885] Found device 0 with properties: \r\n> name: Tesla K40m\r\n> major: 3 minor: 5 memoryClockRate (GHz) 0.745\r\n> pciBusID 0000:27:00.0\r\n> Total memory: 11.16GiB\r\n> Free memory: 11.09GiB\r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:906] DMA: 0 \r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:916] 0:   Y \r\n> I c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:27:00.0)\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:586] Could not identify NUMA node of /job:localhost/replica:0/task:0/gpu:0, defaulting to 0.  Your kernel may not have been built with NUMA support.\r\n> E c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n> F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:198] Unexpected Event status: 1\r\n\r\nI have tried \r\n```\r\nx = tf.ones(shape=[3, 3])\r\nx_diag = tf.diag_part(x)\r\n```\r\nand `x_diag_matrix = tf.matrix_diag([1., 1., 1.])` , Both work ok in a GPU mode. Maybe the tensor couldn't be input of  tf.matrix_diag() in a Windows GPU mode?\r\n", "comments": ["This may be related to #6509. DId you try updating to the newest drivers as suggested there?", "Closing to merge duplicate issues.\r\nPlease follow #6509 for updates."]}, {"number": 6551, "title": "Can't restore a partitioned variable", "body": "I have a large matrix.\r\n\r\nI use bellow method create this variable as number of shards.\r\n```\r\nsoftmax_w = tf.get_variable(\"softmax_w\", [hps.vocab_size, hps.projected_size],\r\n                            partitioner=tf.fixed_size_partitioner(hps.num_shards, 0))\r\n```\r\ncreate log:\r\n```\r\nmodel/softmax_w/part_0:0 (99184, 512) /cpu:0\r\nmodel/softmax_w/part_1:0 (99184, 512) /cpu:0\r\nmodel/softmax_w/part_2:0 (99184, 512) /cpu:0\r\nmodel/softmax_w/part_3:0 (99184, 512) /cpu:0\r\nmodel/softmax_w/part_4:0 (99184, 512) /cpu:0\r\nmodel/softmax_w/part_5:0 (99184, 512) /cpu:0\r\nmodel/softmax_w/part_6:0 (99183, 512) /cpu:0\r\nmodel/softmax_w/part_7:0 (99183, 512) /cpu:0\r\n```\r\nI can training and save it success. But when I try to restore model I got this error:\r\n\r\nrestore log\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_7 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_6 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_5 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_4 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_3 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_2 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_1 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_0 not found in checkpoint\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: Key model/softmax_w/part_7 not found in checkpoint\r\n```\r\n\r\nI found tensorflow save the variable as a whole part. The saved parameter just have one softmax_w. No longer a partitioned variable, I can use bellow code restore that variable. \r\n```\r\nsoftmax_w = tf.get_variable(\"softmax_w\", [hps.vocab_size, hps.projected_size])\r\n```", "comments": ["@ilblackdragon have you used Saver with your fixed size partitioner successfully?", "Hm, weird - restoring should work as it works with other partitioners. \r\nI can take a quick look tomorrow, may be something changed with Saver v2.", "This looks like it's been fixed.\r\n\r\nI can `saver.save` and `saver.restore` a partitioned variable without trouble. If this is still a problem please post a minimal example of the code that is not working for you.\r\n\r\nInteresting to note, your line:\r\n\r\n    softmax_w = tf.get_variable(\"softmax_w\", [hps.vocab_size, hps.projected_size])\r\n\r\nFails for me with:\r\n\r\n    ValueError: No partitioner was provided, but a partitioned version of the variable was found:\r\n    softmax_w/part_0. Perhaps a variable of the same name was already created with partitioning?\r\n\r\n"]}, {"number": 6550, "title": "classify_image.py file not found at tensorflow/models/image/imagenet", "body": "It seems the tutorials are out of date. ", "comments": ["I believe this is fixed; the directory is now listed on this page:\r\n\r\nhttps://www.tensorflow.org/tutorials/image_recognition\r\n\r\nreferences this repo: \r\n\r\nhttps://github.com/tensorflow/models\r\n\r\nand asks you to find classify_image.py:\r\n\r\ncd models/tutorials/image/imagenet\r\n\r\nwhich lines up with this: \r\n\r\nhttps://github.com/tensorflow/models/tree/master/tutorials/image/imagenet\r\n\r\nPlease reopen or refile if this is in error.  Thanks!", "I work in windows, python, tensorflow\r\nclone models from https://github.com/tensorflow/models\r\nand I found classify_image.py in  \\ models-master \\ tutorials \\ image \\ imagenet directory", "Where is the file?"]}, {"number": 6549, "title": "keep_checkpoint_every_n_hours is ignored", "body": "I think that when creating a new saver [`here`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L249), the `keep_checkpoint_every_n_hours` flag from the RunConfig should be passed to it.", "comments": ["Thanks for reporting! I've submitted the fix. "]}, {"number": 6548, "title": "'No module named tensorflow' after installing via pip", "body": "I'm running Anaconda on win64. \r\nThe files are in the correct site-packages directory, with an `__init__.py`, but python doesn't seem to be able to recognize the module, regardless of whether I'm working in a virtual environment or the root directory. \r\nNone of the following has changed this result:\r\n[This.](https://www.tensorflow.org/get_started/os_setup#pip_installation_on_windows)\r\n[This.](https://github.com/tensorflow/tensorflow/issues/6136)\r\n[This.](https://www.reddit.com/r/MachineLearning/comments/3tcxs6/something_does_not_work_for_me_i_get_no_module/)\r\n", "comments": ["I originally tried setting it up by generating an environment.yml file:\r\n```\r\nname: tensorflow\r\ndependencies:\r\n- python=3\r\n- pip\r\n- numpy\r\n- scipy\r\n- matplotlib\r\n- pandas\r\n- pip:\r\n    - tensorflow\r\n```\r\nIn the same directory, I use `conda env create`, followed by `activate tensorflow`. Running any `import tensorflow` statement then prints `ImportError: No module named 'tensorflow'`.\r\n\r\nI've now found that I manually add 'C:\\Program Files\\Anaconda3\\envs\\tensorflow\\Lib\\site-packages' to my PYTHONPATH environment variable, I get past that error, though this workaround seems like it could cause problems with other environments. The next error that pops up is `ImportError: cannot import name 'multiarray'`, from numpy. ", "There are some known anaconda issues.\r\nIn your pip install command, could you try adding the `--ignore_installed` flag", "I tried adding the `--ignore-installed` flag, and it doesn't change the result at all. ", "Yes, I'm using anaconda 64-bit, and there are no errors. \r\n```\r\n(tensorflow) E:\\Tristan\\Mython\\TensorFlowTutorial>pip install --ignore-installed tensorflow\r\nCollecting tensorflow\r\n  Using cached tensorflow-0.12.1-cp35-cp35m-win_amd64.whl\r\nCollecting wheel>=0.26 (from tensorflow)\r\n  Using cached wheel-0.29.0-py2.py3-none-any.whl\r\nCollecting numpy>=1.11.0 (from tensorflow)\r\n  Using cached numpy-1.11.3-cp35-none-win_amd64.whl\r\nCollecting six>=1.10.0 (from tensorflow)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting protobuf>=3.1.0 (from tensorflow)\r\n  Using cached protobuf-3.1.0.post1-py2.py3-none-any.whl\r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow)\r\n  Using cached setuptools-32.3.1-py2.py3-none-any.whl\r\nInstalling collected packages: wheel, numpy, six, setuptools, protobuf, tensorflow\r\nSuccessfully installed numpy-1.11.3 protobuf-3.1.0.post1 setuptools-32.3.1 six-1.10.0 tensorflow-0.12.1 wheel-0.29.0\r\n```", "OK, one observation, you are installing tensorflow inside a conda environment, so tensorflow is only available in that conda environment.\r\nby any chance, are you trying to run tensorflow outside this environment?\r\n\r\nAnother possible issue, is it possible you have multiple python distributions installed?", "Couldn't reproduce the error. It's really something on your side.\r\n```\r\nC:\\WINDOWS\\system32>activate tensorflow\r\n\r\n(tensorflow) C:\\WINDOWS\\system32>pip install --ignore-installed tensorflow\r\nCollecting tensorflow\r\n  Using cached tensorflow-0.12.1-cp35-cp35m-win_amd64.whl\r\nCollecting numpy>=1.11.0 (from tensorflow)\r\n  Downloading numpy-1.11.3-cp35-none-win_amd64.whl (7.6MB)\r\n    100% |################################| 7.6MB 123kB/s\r\nCollecting six>=1.10.0 (from tensorflow)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting protobuf>=3.1.0 (from tensorflow)\r\n  Downloading protobuf-3.1.0.post1-py2.py3-none-any.whl (347kB)\r\n    100% |################################| 348kB 907kB/s\r\nCollecting wheel>=0.26 (from tensorflow)\r\n  Downloading wheel-0.29.0-py2.py3-none-any.whl (66kB)\r\n    100% |################################| 71kB 1.1MB/s\r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow)\r\n  Downloading setuptools-32.3.1-py2.py3-none-any.whl (479kB)\r\n    100% |################################| 481kB 415kB/s\r\nInstalling collected packages: numpy, six, setuptools, protobuf, wheel, tensorflow\r\nSuccessfully installed numpy-1.11.3 protobuf-3.1.0.post1 setuptools-32.3.1 six-1.10.0 tensorflow-0.12.1 wheel-0.29.0\r\n\r\n(tensorflow) C:\\WINDOWS\\system32>python\r\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> hello = tf.constant('Hello, testing TensorFlow!')\r\n>>> sess = tf.Session()\r\n>>> print(sess.run(hello))\r\nb'Hello, testing TensorFlow!'\r\n```", "I might be mistaken, but I always thought that Anaconda has its own package and environment control and didn't use `pip` or `venv`. Have you tried following the [`conda` instructions](https://www.tensorflow.org/get_started/os_setup#anaconda_installation)? I am not sure if the instructions are the same for Windows, but it should as simple as\r\n\r\n```bash\r\nconda create -n tensorflow python=2.7\r\n```", "virtualenv can still be made available when using anaconda, but you are right.\r\nAnaconda has conda.\r\n\r\nHowever, one important detail. We only have python 3.5 support in windows. so the `python=2.7` part in the command wont work in windows.", "@tristanbrown did these suggestions help?", "The only other python distribution I have installed is WinPython, but I've already removed the path variables for that installation. \r\n\r\nI have followed the conda instructions. That line only creates the new environment, named \"tensorflow.\" \r\n\r\nI have tried both:\r\n1. Installing tensorflow within a conda environment and running it within that environment. \r\n2. Installing tensorflow to the root environment and running it outside of any environment. \r\n\r\nBoth of these options render the same error. \r\n\r\nI can install and run other modules (e.g. theano, yahoo_finance, pandas_datareader) without issue, so this problem is unique to Tensorflow. ", "What is your python installed with WinPython and Anaconda?\r\nAlso, Could you confirm all your environment/path variables refer to your anaconda python?", "@tristanbrown Could you also copy and paste your command line session into this bug similar to what @Carmezim did above? That should hopefully help us get a better idea of what's going on, since we can see the error message and the commands you're running.", "@jart I followed his same [steps](https://github.com/tensorflow/tensorflow/issues/6548#issuecomment-269576043) to try reporduce the error:\r\n- Created new anaconda env \r\n- Activated it\r\n- Installed clean TensorFlow installation through `pip` with ignore flag \r\n- Imported TensorFlow (where his error occurred) and ran snippet successfully\r\n\r\nSo I assumed is on his side. The difference is I have only one Python distribution which is Anaconda's, while as @gunan pointed out @tristanbrown apparently has WinPython and Anaconda so is something to consider.\r\n\r\n", "In that case I'll trust your judgement and close this one out. Thank you for your time @Carmezim.", "@jart **edit**: oh, my bad, I misread you asked him to post the cmd session and thought you asked me then replied that. So sorry ", "@Carmezim  I was also able to do the same things as you have shown in your post on 29th Dec. However, I am not being able to run a '.py' file. Anaconda gives the error that there is no module names tensorflow. ", "Hi @ayushya2531, sorry you are having problems. Would you mind to please open a new issue filling the template with the necessary info and commenting me so we can better asses what is happening?", "I installed using pip3, and then it dint work on `python` prompt. Used `python3` prompt and it worked like a charm!", "@Carmezim \r\n\r\nI am on Fedora 23 Workstation (will upgrade soon with full reinstall) and --ignore_installed worked perfectly for me, with anaconda installed.  In my next reinstall on this system, I would NOT install anaconda because of how it places the site-packages separately other than from root, during pip.  \r\n\r\nSince I also confirmed pip install --ignore-installed tensorflow-gpu worked without a hitch and I confirmed the GPU ran with import tensorflow-gpu I'll repeat your answer (worth repeating):\r\n\r\nIn your pip install command, could you try adding the --ignore_installed flag\r\n\r\nQuestion: why did --ignore_installed work?  (for the future)\r\n", "I am having the same issue.. @tristanbrown were u able to solve ur issue?", "The error came from the multiple Python distributions I had installed. I removed all the other ones from my PATH file, and the issue was resolved. ", "For me it was similar with TensorFlow built from source. In the end I uninstalled `tensorflow` and `numpy` and then reinstalled both (`tensorflow` first, then `numpy` via conda); this solved the issue.", "For me, the error went I away when I went into the anaconda prompt and activated tensorflow using the pre-installation command: `activate tensorflow`\r\nBut I still have no idea why that worked :/", "@tristanbrown how do we solve the error, when I don't have any python related lines in my PATH file ?\r\n\r\nI have multiple python versions(2.7 &  3.5) installed in various envs in Anaconda. ", "I was having the same problem. I'm running Windows 10 Build 17074 and how I got it running was by opening a Command Prompt (CMD - Not PowerShell) as Administrator, and following[ the installation tutorial.](https://www.tensorflow.org/install/install_windows) ", "@superjose I tried it in CMD, nothing different. Installation finishes without any error, but I am not able to import tensorflow", "@madhusudangr Did you try running CMD as Administrator?", "@madhusudangr I don't know if this will help you, but here's my Path's content inside Environment Variables:\r\n![image](https://user-images.githubusercontent.com/8230270/35292896-f1cd91e4-003f-11e8-96f2-8f6e0c8c5c68.png)\r\n\r\nIf you're using Windows 7/8,x, you can always do:\r\n\r\n1) Go to Computer\r\n2) Click System Properties.\r\n3) Click Advanced System Settings\r\n4) Under the Advanced Tab (Should be already selected) click Environment Variables.\r\n\r\nOR:\r\nPress WinKey + Break.\r\n\r\nIn Windows 10:\r\n1) Go to Computer and Click System Properties OR Go to Settings and Click System and go to the About tab.\r\n2) On the right panel, click \"System info\" which appears under Related Settings. \r\n3) Click \"Advanced System Settings\"\r\n4) Click on \"Environment Variables\"\r\n\r\nCheck this image to see if it helps:\r\n![image](https://user-images.githubusercontent.com/8230270/35293090-a25ac400-0040-11e8-9ac7-93aded5a59da.png)\r\n\r\n", "This is an old post but I had this same issue 8/2018 with conda environments and tensorflow (SE led me here). Maybe this will help someone else:\r\n\r\n![image](https://user-images.githubusercontent.com/7524999/44933291-28d5c800-ad2e-11e8-9b67-e4ae013214e5.png)\r\n\r\nAlthough in the conda environment as seen in the picture. And tensorflow being installed in the conda environment. I had to execute the python from the environment folder in order for python to recognize tensorflow. \r\n\r\nThis is hobby work for me so I cannot verify why this is.", "I met with the same problem after I upgrade Tensorflow(1.1->1.4) in Conda environment via `pip --ignore_installed`. Then I install via `conda install tensorflow==1.4` again. The tensorflow installed via pip seems to be covered by the conda installed one. Then the problem disappears. I dont know what is different between these two installation methods. ", "I have installed conda on centos7 ..later when i installed tensorflow by executing command \r\nconda create -y -n tensorflow python=2.7 \r\nsource activate tensorflow\r\nconda install tensorflow\r\n\r\ni logged in to container to test the tensorflow installation \r\n\r\npython\r\nPython 2.7.16 |Anaconda, Inc.| (default, Aug 22 2019, 16:00:36)\r\n[GCC 7.3.0] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n\r\nam getting output without any error if i execute \"source activate tensorflow\" in the beginning else when i execute  import tensorflow as tf i will get \"ImportError: No module named tensorflow\r\n\" so is it necessary that i should use  \"source activate tensorflow\" or any mistake is there in my installation", "I solved it by removing other PATH in .bashrc, then restart the shell.", "I solved this problem by updating `$PATH`. In my case, I installed Anaconda3 on Mac OSX. I found that Anaconda add its `bin` path to the beginning of $PATH, therefore you use its python. So you may update your $PATH to use maybe `/usr/local/bin/python` instead.", "I solved it installing by 'conda install tensorflow', running CMD as Administrator."]}, {"number": 6547, "title": "Branch 143142615", "body": "", "comments": ["Both failures are legitimate issues.\r\nImportError: cannot import name test_ops, likely a missing all_files rule.\r\n\r\nwindows_cmake:\r\nImportError: cannot import name 'nn'  : likely missing include in cmake rules.", "Close this and push again once failures are fixed internally."]}, {"number": 6546, "title": "TensorFlow Implementation in MapReduce", "body": "Hello guys,\r\n\r\nFirst, i'm sorry if i break the rules. But, i really stuck right now. So, this is my last hope to ask this question.\r\n\r\nI just want to ask something. Can i implement TensorFlow in MapReduce Programming Model within Hadoop ?\r\n\r\nYes, i know that TensorFlow can be runned in distributed mode, according this reference https://www.tensorflow.org/versions/r0.11/how_tos/distributed/ . But, it doesn't explain more detail how to use it in MapReduce.\r\n\r\nOne last thing, can i use TensorFlow to predict something a lot of data ? Because, i only know that TensorFlow can be used to classify similar images.\r\n\r\nI'm sorry if my question look dumb. Well, there are many thing in the world that i don't know. So, when i don't know, i will ask a question :smile:\r\nThanks for your attention", "comments": ["We try to keep the Github issues focused on bugs and feature requests. Such a question is better suited for [stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow). (Hence I'll close this issue)\r\n\r\nWhen asking on stackoverflow, you might want to elaborate on what you need from TensorFlow. For example, do you want to train a model or run inference on a pre-trained model over a large set of examples? (In which case, something like the [Java API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java) might make it easier to work within the Hadoop ecosystem.)"]}, {"number": 6545, "title": "Enable running android_nightly.sh through ci_parameterized_build.sh", "body": "", "comments": []}, {"number": 6544, "title": "Android: Copy libs built by bazel in android_nightly.sh to output dir", "body": "This will make them easier to grab and expose via Jenkins.", "comments": []}]