[{"number": 8088, "title": "Branch 149211889", "body": "", "comments": []}, {"number": 8087, "title": "SystemError: <built-in function TF_Run> returned a result with an error set using tf.contrib.layers", "body": "(Using `tf 1.0.0`)\r\nI tried to follow the following example (https://www.tensorflow.org/get_started/tflearn) from the TF website and I get the following error:\r\n\r\n```\r\nINFO:tensorflow:Using default config.\r\nINFO:tensorflow:Using config: {'_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9cfb283e80>, '_task_id': 0, '_keep_checkpoint_every_n_hours': 10000, '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_task_type': None, '_environment': 'local', '_tf_config': gpu_options {\r\n  per_process_gpu_memory_fraction: 1.0\r\n}\r\n, '_master': '', '_save_checkpoints_secs': 600, '_is_chief': True, '_tf_random_seed': None, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_evaluation_master': ''}\r\nWARNING:tensorflow:From <ipython-input-131-b49d002a31c2>:14: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From <ipython-input-131-b49d002a31c2>:14: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\r\nInstructions for updating:\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\nWARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\r\n  equality = a == b\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nTypeError: expected bytes, tuple found\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nSystemError                               Traceback (most recent call last)\r\n<ipython-input-131-b49d002a31c2> in <module>()\r\n     12 classifier.fit(x=results,\r\n     13                y=labels,\r\n---> 14                steps=2000)\r\n     15 \r\n     16 # Evaluate accuracy.\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    278             _call_location(), decorator_utils.get_qualified_name(func),\r\n    279             func.__module__, arg_name, date, instructions)\r\n--> 280       return func(*args, **kwargs)\r\n    281     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\r\n    282         func.__doc__, date, instructions)\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\r\n    408     _verify_input_args(x, y, input_fn, None, batch_size)\r\n    409     if x is not None:\r\n--> 410       SKCompat(self).fit(x, y, batch_size, steps, max_steps, monitors)\r\n    411       return self\r\n    412 \r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, batch_size, steps, max_steps, monitors)\r\n   1351                         steps=steps,\r\n   1352                         max_steps=max_steps,\r\n-> 1353                         monitors=all_monitors)\r\n   1354     return self\r\n   1355 \r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    278             _call_location(), decorator_utils.get_qualified_name(func),\r\n    279             func.__module__, arg_name, date, instructions)\r\n--> 280       return func(*args, **kwargs)\r\n    281     new_func.__doc__ = _add_deprecated_arg_notice_to_docstring(\r\n    282         func.__doc__, date, instructions)\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in fit(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\r\n    424       hooks.append(basic_session_run_hooks.StopAtStepHook(steps, max_steps))\r\n    425 \r\n--> 426     loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n    427     logging.info('Loss for final step: %s.', loss)\r\n    428     return self\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py in _train_model(self, input_fn, hooks)\r\n    982         loss = None\r\n    983         while not mon_sess.should_stop():\r\n--> 984           _, loss = mon_sess.run([model_fn_ops.train_op, model_fn_ops.loss])\r\n    985       summary_io.SummaryWriterCache.clear()\r\n    986       return loss\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    460                           feed_dict=feed_dict,\r\n    461                           options=options,\r\n--> 462                           run_metadata=run_metadata)\r\n    463 \r\n    464   def should_stop(self):\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    784                               feed_dict=feed_dict,\r\n    785                               options=options,\r\n--> 786                               run_metadata=run_metadata)\r\n    787       except errors.AbortedError:\r\n    788         logging.info('An AbortedError was raised. Closing the current session. '\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n    742 \r\n    743   def run(self, *args, **kwargs):\r\n--> 744     return self._sess.run(*args, **kwargs)\r\n    745 \r\n    746 \r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    889                                   feed_dict=feed_dict,\r\n    890                                   options=options,\r\n--> 891                                   run_metadata=run_metadata)\r\n    892 \r\n    893     for hook in self._hooks:\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/training/monitored_session.py in run(self, *args, **kwargs)\r\n    742 \r\n    743   def run(self, *args, **kwargs):\r\n--> 744     return self._sess.run(*args, **kwargs)\r\n    745 \r\n    746 \r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    765     try:\r\n    766       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 767                          run_metadata_ptr)\r\n    768       if run_metadata:\r\n    769         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n    963     if final_fetches or final_targets:\r\n    964       results = self._do_run(handle, final_targets, final_fetches,\r\n--> 965                              feed_dict_string, options, run_metadata)\r\n    966     else:\r\n    967       results = []\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1013     if handle is None:\r\n   1014       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\r\n-> 1015                            target_list, options, run_metadata)\r\n   1016     else:\r\n   1017       return self._do_call(_prun_fn, self._session, handle, feed_dict,\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)\r\n   1020   def _do_call(self, fn, *args):\r\n   1021     try:\r\n-> 1022       return fn(*args)\r\n   1023     except errors.OpError as e:\r\n   1024       message = compat.as_text(e.message)\r\n\r\n/opt/conda/lib/python3.5/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1002         return tf_session.TF_Run(session, options,\r\n   1003                                  feed_dict, fetch_list, target_list,\r\n-> 1004                                  status, run_metadata)\r\n   1005 \r\n   1006     def _prun_fn(session, handle, feed_dict, fetch_list):\r\n\r\nSystemError: <built-in function TF_Run> returned a result with an error set\r\n```\r\n\r\nTo replicate:\r\n(For better accuracy use Docker image `jupyter/datascience-notebook` and install tf)\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nimport tensorflow as tf\r\n\r\nresults = np.ndarray(10,2632)\r\nlabels = [0,1] * 5\r\n\r\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\r\n\r\n\r\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\r\n                                            hidden_units=[2623,2],\r\n                                            n_classes=2,\r\n                                            model_dir='./model')\r\n\r\n# Fit model.\r\nclassifier.fit(x=results,\r\n               y=labels,\r\n               steps=2000)\r\n\r\n# Evaluate accuracy.\r\naccuracy_score = classifier.evaluate(x=test_set.data,\r\n                                     y=test_set.target)[\"accuracy\"]\r\nprint('Accuracy: {0:f}'.format(accuracy_score))\r\n```\r\n\r\nI think this might be a problem with tensorflow badly interacting with the underlying file system? Not sure.", "comments": ["@martinwicke Has this tutorial broken with 1.0?  (There seem to be some deprecation warnings in tf.learn estimators code with dates from last year.)", "I believe this is obsolete now (I believe it has been fixed)."]}, {"number": 8086, "title": "Fix some typos in the documation (tensorflow/docs_src)", "body": "This fix fixes some typos in the documentation (`tensorflow/docs_src/**.md`)", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8085, "title": "Running Multiple instances.", "body": "Operating System: Ubuntu 16.10\r\nInstalled version of CUDA and cuDNN: 8.0 and cuDNN 5.2\r\nTensorflow Version : 1.0.0-rc2\r\n\r\n\r\nI have a website that will take an image of a number plate, the website passes the numberplate to tensor for to be detected, sensor flow then returns the detected text back to the website to be inserted into the database. \r\n\r\nI found this issue when using a dropzone to detect many number plates at once when I upload an image one by one, the system is quite reliable. When using the dropzone the system is very unreliable. \r\n\r\nI tested this by opening 6 terminal sessions and running the following command in each instance:\r\n\r\n```\r\n/usr/bin/python3 /home/chad/Documents/anpr-python/detect.py /var/www/Honours/public/number_plate/fec5e719-ee8f-4b7c-a259-0cf8ddc5a28f.jpeg /home/chad/Documents/anpr-python/weights.npz out.jpg\r\n```\r\n\r\nI need to be able to fix this as the website needs to allow for mass upload of images to be detected.\r\n\r\nCould you help me figure out the best way to fix this?\r\n\r\n\r\n```chad@chad-GA-990XA-UD3:/var/www/Honours/public/number_plate$ /usr/bin/python3 /home/chad/Documents/anpr-python/detect.py /var/www/Honours/public/number_plate/fec5e719-ee8f-4b7c-a259-0cf8ddc5a28f.jpeg /home/chad/Documents/anpr-python/weights.npz out.jpg\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1070\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 126.19MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 126.19M (132317184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\nAborted (core dumped)\r\nchad@chad-GA-990XA-UD3:/var/www/Honours/public/number_plate$ \r\n\r\n```\r\n\r\n\r\n```\r\nchad@chad-GA-990XA-UD3:/var/www/Honours/public/number_plate$ /usr/bin/python3 /home/chad/Documents/anpr-python/detect.py /var/www/Honours/public/number_plate/fec5e719-ee8f-4b7c-a259-0cf8ddc5a28f.jpeg /home/chad/Documents/anpr-python/weights.npz out.jpg\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1070\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.683\r\npciBusID 0000:01:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 82.19MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 82.19M (86179840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 15.62M (16374016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 15.62M (16374016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 256.00MiB was 256.00MiB, Chunk State: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec00000 of size 1280\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec00500 of size 4864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec01800 of size 1039104\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ecff300 of size 307200\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a300 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a700 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a800 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a900 of size 819200\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee12900 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee12b00 of size 8192\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee14b00 of size 2072576\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030f00eb00 of size 65551360\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 256 totalling 512B\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1024 totalling 1.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 4864 totalling 4.8KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 8192 totalling 8.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 307200 totalling 300.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 819200 totalling 800.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1039104 totalling 1014.8KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2072576 totalling 1.98MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 65551360 totalling 62.51MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 66.57MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: \r\nLimit:                    86179840\r\nInUse:                    69805824\r\nMaxInUse:                 69805824\r\nNumAllocs:                      12\r\nMaxAllocSize:             65551360\r\n\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ******************************************************************************xxxxxxxxxxxxxxxxxxxxxx\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 256.00MiB.  See logs for memory state.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304):   Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608):   Total Chunks: 1, Chunks in use: 0 15.61MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 47.56MiB was 32.00MiB, Chunk State: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec00000 of size 1280\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec00500 of size 4864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ec01800 of size 1039104\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ecff300 of size 307200\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a300 of size 1024\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a700 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a800 of size 256\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ed4a900 of size 819200\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee12900 of size 512\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee12b00 of size 8192\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030ee14b00 of size 2072576\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x1030f00eb00 of size 65551360\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x10313800000 of size 4864\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x10313801300 of size 16369152\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: \r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 256 totalling 512B\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1024 totalling 1.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 4864 totalling 9.5KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 8192 totalling 8.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 307200 totalling 300.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 819200 totalling 800.0KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1039104 totalling 1014.8KiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2072576 totalling 1.98MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 65551360 totalling 62.51MiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 66.58MiB\r\n```\r\n", "comments": ["Taking a shot in the dark.\r\n\r\nI do not think you want start TensorFlow each time.  You might get away with it by setting [Allow GPU Memory To Grow](https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth).  I think a likely better solution would be to leave TensorFlow running, e.g. the python script, and have it pick up the images to process from a directory queue or whatever.  Even better might be to check out [TensorFlow Serving](https://github.com/tensorflow/serving).  I would put the time into setting up TensorFlow Serving if possible that would give you a robust solution.  \r\n", "The errors are clearly coming from running out of GPU memory by trying to run multiple TF computations simultaneously.  As @tfboyd says, it looks like you are creating a separate TF process and runtime for each, and they are all trying to grab some GPU memory until you run out.  Being separate processes, the TF instances have no way of coordinating with each other (other than maybe blocking than waiting to allocate GPU memory).  If you are wanting to make this reliable you should probably be running some kind of service with concurrency control (i.e. limit the number of concurrent operations in flight).  \r\n\r\n@nfiedel Any suggestions here wrt tf.serving?\r\n\r\nFailing that, I'd suggest posting a question on StackOverflow, since this is more of a generic programming question.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 8084, "title": "Difference of allocate_persistent() and allocate_temp()", "body": "This is a question on the usage of the two functions which are used in Compute() method for defining new Op.\r\n\r\nMy understanding is that allocate_temp() are used **only and once in constructing Op** and cannot be used during traning or testing steps later on; whereas allocate_persistent() can be used in **both Op construction and traning or testing steps later on**. Am I correct?\r\n\r\nWhen I was defining my own new Op, I need to allocate a temporay GPU memory buffer in my Compute() method. The buffer will be used every time I call Compute(), however, I don't need to store the value in the temporary buffer for future reuse. In this case, which function should I use?\r\n\r\nWhen calling the two functions, do we need to call corresponding functions like free_persistent() or free_temp()?\r\n\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8083, "title": "Update retrain.py", "body": "Fixed example bazel build in description (removed unnecessary 'third-party' from path)", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8082, "title": "Slicing a tensor by an index tensor in Tensorflow", "body": "Hi the tensorflow team in Github,\r\n\r\nI am not sure if it is appropriate to ask questions in Issue. If questions should not be here, please let me know so that I can close it (I already asked this question on [Stackoverflow](http://stackoverflow.com/questions/42597520/slicing-a-tensor-by-an-index-tensor-in-tensorflow) but I think It would be better to ask this right here in Tensorflow?)\r\n\r\nI have two following tensors (note that they are both Tensorflow tensors which means they are still virtually symbolic at the time I construct the following slicing op before I launch a tf.Session()):\r\n\r\n`params`: has shape (64,784, 256)\r\n`indices`: has shape (64, 784)\r\nand I want to construct an op that returns the following tensor:\r\n\r\n`output`: has shape (64,784) where\r\n`output[i,j] = params_tensor[i,j, indices[i,j] ]`\r\n\r\nWhat is the most efficient way in Tensorflow to do so?\r\n\r\nMany thanks.\r\n-Bests", "comments": ["Since I think it is more appropriate to discuss this kind of question in Stackoverflow. I close it. If you want to check out the question section in Stackoverflow, follow the link above."]}, {"number": 8081, "title": "Fixed build error.", "body": "  The errors were introduced in commit d97f45ba949b11d721aa5b81c12d655a8487aa38.", "comments": ["Can one of the admins verify this patch?", "@cwhipkey please review. This issue was caused by my last PR.", "@tensorflow-jenkins test this please"]}, {"number": 8080, "title": "Merge pull request #1 from tensorflow/master", "body": "Update", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 8079, "title": "Support read data from ViewFs.", "body": "Support read data from [ViewFs](http://hadoop.apache.org/docs/r2.6.5/hadoop-project-dist/hadoop-hdfs/ViewFs.html).", "comments": ["Can one of the admins verify this patch?", "@gunan @jhseu ViewFs implements the Hadoop file system interface just like HDFS and the local file system. Our Hadoop cluster use this feature. thanks.", "How is the viewfs cluster name being picked up? The change seems to ignore that component.", "For example, how would you distinguish between:\r\n`viewfs://clusterx/path/to/foo` vs. `viewfs://clustery/path/to/foo`", "Jenkins, test this please"]}, {"number": 8078, "title": "error: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file", "body": "Hello, now my cuda is 7.5 , so I install tf from the source, but when I run the command 'bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg', it shows error:\r\n`Sun Mar 5 02:09:40 CST 2017 : === Using tmpdir: /tmp/tmp.4Bm642hjyI\r\n~/ztgong/dl-tools/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles ~/ztgong/dl-tools/tensorflow\r\n~/ztgong/dl-tools/tensorflow\r\n/tmp/tmp.4Bm642hjyI ~/ztgong/dl-tools/tensorflow\r\nSun Mar 5 02:09:41 CST 2017 : === Building wheel\r\nerror: can't copy 'tensorflow/python/pywrap_tensorflow.py': doesn't exist or not a regular file\r\n`\r\n\r\nI still not fix this error, how to solve it?\r\nthank you in advance!!", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@prb12 Thank you very much for your reply. I use ubuntu14.04, x86_64. and I use tf r1.0 by  `git clone https://github.com/tensorflow/tensorflow `and `git checkout r1.0`.", "When you previously ran the command \r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`, did it complete successfully?", "@prb12 yes, it did success. And running the command `bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package` also success.", "@IvyGongoogle  your error message includes the string ~/ztgong/dl-tools/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles` \r\n\r\nIs it possibly the case that you are running this binary from a directory other than the root of your tensorflow git client?\r\n\r\nIf so, then the _relative_ pathname of `tensorflow/python/pywrap_tensorflow.py` would not be found.  (The `build_pip_package` command expects to be run in your build tree.)", "I met the same problems", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "I meet the same problem with Ubuntu 16.04 + CUDA 8.0"]}, {"number": 8077, "title": "change tf.Supervisor to tf.train.Supervisor after api changed", "body": "@maciekcc @terrytangyuan  review this pr please", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "PR merged. Thanks, @fanlu !"]}, {"number": 8076, "title": "New Build causes Illegal instruction on Import in Python 2.7 and 3.4", "body": "I built tensorflow with gpu support for python 2.7 and 3.4 using CUDA 8.0 and CUDNN 5.1, running oo Linux.\r\n\r\nOn import of tensorflow installed with the wheels I built I get an 'Illegal Instruction' error and python quits.\r\n\r\nCan someone confirm if this is an error on my part or if is an issue with the current build?", "comments": ["The problem was that the wheels were compiled on a machine with newer kernel than the machine it was being installed on.", "How did you fix it?"]}, {"number": 8075, "title": "Update the github docs link in the error msg upon import failure", "body": "Hi,\r\nsince the docs have moved from `g3doc` to `docs_src` (and have been refactored), I thought it would be good to update the link in the error message that comes up upon import failures.", "comments": ["Can one of the admins verify this patch?", "good call @caisq . I prob. had python's original pep-8 and users with 80-char terminals in mind, but yeah, it's 2017 :). ", "@tensorflow-jenkins test this please", "PR merged. Thanks for the contribution, @rasbt !"]}, {"number": 8074, "title": "Add Stereogram into /contrib/image", "body": "Build PULL package for Tensorflow single_image_random_dot_stereograms", "comments": ["Can one of the admins verify this patch?", "@martinwicke please take a look to see if I did this right, there are a few strange un-related files that have no changes in it and I don't understand why.\r\n\r\nThis is related to the issue listed here.  Again, sorry, this is my first real pull request and thought this was going to another branch within my repository before it got here.\r\nhttps://github.com/tensorflow/tensorflow/issues/8022\r\n\r\n-Greg", "The code has been updated and a new pull request has been submitted:\r\nhttps://github.com/tensorflow/tensorflow/pull/8327"]}, {"number": 8073, "title": "Invalid value with \"sigmoid_cross_entropy_with_logits\"", "body": "Hello, I started learning tensorflow and trying to solve a lot of sample program and lessons, \r\nThen I faced the problem with \"sigmoid_cross_entropy_with_logits\" methods,\r\nI attached the error & code which I used.\r\nPlease help me to figure out this,,, Thank you very much in advance.\r\n--------\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\n\r\n#placeholders of input & output\r\nx = tf.placeholder(tf.float32, shape=(None,2), name=\"x\")\r\ny_ = tf.placeholder(tf.float32, shape=(None,1), name=\"y\")\r\n\r\n#model parameter\r\na = tf.Variable(-10 * tf.ones((2,1)), name=\"a\")\r\nb = tf.Variable(200., name=\"b\")\r\n\r\n#model function\r\nu = tf.matmul(x, a) + b\r\ny = tf.sigmoid(u)\r\n\r\nloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(u, y_))\r\n\r\ntrain_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\r\n\r\ntrain_x = np.array([[2.,3.], [0., 16.], [3., 1.][2., 8.]])\r\ntrain_y = np.array([1.,1.,0.,0.]).reshape(4,1)\r\nprint(\"x=\", train_x)\r\nprint(\"y=\", train_y)\r\n\r\nsess = tf.Session()\r\ninit = tf.global_variables_initializer()\r\nsess.run(init)\r\n\r\nfor i in range(1000):\r\n    _, l, a_, b_ = sess.run([train_step, loss, a, b], feed_dict={x:train_x, y:train_y})\r\n    if (i+1) % 100 == 0:\r\n        print(\"Step=%3d, a1=%6.2f, a2=%6.2f, b=%6.2f, loss=%.2f\" % (i+1, a_[0], a_[1], b_, l))\r\n\r\n        \r\nest_a, est_b = sess.run([a, b], feed_dict={x: train_x, y_: train_y})\r\nprint(\"Estimated: a1=%6.2f, a2=%6.2f, b=%6.2f\" % (est_a[0], est_a[1], est_b))\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-2a36f4933eac> in <module>()\r\n     15 y = tf.sigmoid(u)\r\n     16 \r\n---> 17 loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(u, y_))\r\n     18 \r\n     19 train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\r\n\r\n/Users/GenkiA/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py in sigmoid_cross_entropy_with_logits(_sentinel, labels, logits, name)\r\n    144   # pylint: disable=protected-access\r\n    145   nn_ops._ensure_xent_args(\"sigmoid_cross_entropy_with_logits\",\r\n--> 146                            _sentinel, labels, logits)\r\n    147   # pylint: enable=protected-access\r\n    148 \r\n\r\n/Users/GenkiA/.pyenv/versions/3.5.2/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py in _ensure_xent_args(name, sentinel, labels, logits)\r\n   1531   if sentinel is not None:\r\n   1532     raise ValueError(\"Only call `%s` with \"\r\n-> 1533                      \"named arguments (labels=..., logits=..., ...)\" % name)\r\n   1534   if labels is None or logits is None:\r\n   1535     raise ValueError(\"Both labels and logits must be provided.\")\r\n\r\nValueError: Only call `sigmoid_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)", "comments": ["This error message is saying you'd call ```sigmoid_cross_entropy_with_logits``` like this:\r\n```py\r\nsigmoid_cross_entropy_with_logits(labels=xxx, logits=..., name=...,...)\r\n```"]}, {"number": 8072, "title": "change learn.metric_spec.MetricSpec to learn.MetricSpec due to API change  in version 1.0", "body": "As mentioned in #7569.", "comments": ["Can one of the admins verify this patch?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->", "@terrytangyuan Sorry, maybe I used another usename and email to do this PR. I don't know how to add a CLA for it.  Maybe I should create another PR with the username which I had a CLA?", "Sure", "CLAs look good, thanks!\n\n<!-- ok -->", "@terrytangyuan I pushed with another username, it should be OK now.", "@tensorflow-jenkins test this please", "Android test failure is unrelated. Merging PR."]}, {"number": 8071, "title": "Downloads and Setups in Tutorial appears to be 404 error", "body": "Just as the headline, I can not open this page, who can fix this porblem ?", "comments": ["Just came here to report the same.", "#7990 seems to fix this issue", "Thank you very much ~ Thank you all ~"]}, {"number": 8070, "title": "Cherry-pick: Add the graphdef version to InferenceContext and to ShapeRefiner::Add\u2026", "body": "\u2026Node.\r\n\r\nUse this to allow loading reductions saved with older graphdefs.\r\n\r\nChange GraphConstructor to not increase the version when importing, but instead take the min of all versions.\r\nChange: 149152437", "comments": ["Jenkins, test this please"]}, {"number": 8069, "title": "1.0.1 patch (update version)", "body": "Running the update_version.sh script to update all version #s to 1.0.1.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 8068, "title": "Update head for StereoGram Pull Request to TensorFlow repository", "body": "Update Local copy to latest TensorFlow prior to StereoGram pull request.", "comments": ["Can one of the admins verify this patch?", "Wanted to pull current Tensorflow to my repo.  Error, closing."]}, {"number": 8067, "title": "Fix Linux pip install paths and instructions", "body": "I updated the wheel URLs (the old ones give 404s) and fixed the wording for the pip install instructions.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please.", "/CC @jugglerix "]}, {"number": 8066, "title": "Branch 149155199", "body": "", "comments": ["@andrewharp fyi"]}, {"number": 8065, "title": "1.0 cherrypicks", "body": "", "comments": ["CC @av8ramit These are the cherrypicks for the patch release."]}, {"number": 8064, "title": "Missing images in docs", "body": "The page https://www.tensorflow.org/api_docs/python/tf/gather has a broken link to an image: https://www.tensorflow.org/api_docs/images/Gather.png\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/scatter_add also has a broken link https://www.tensorflow.org/api_docs/images/ScatterAdd.png\r\n\r\nIn addition, the images seem to be missing from the tensorflow repo.  Could those be put in there? It is hard to build the docs locally for offline use without the images.", "comments": ["It looks like those images were deleted from [this folder](https://github.com/tensorflow/tensorflow/tree/9c3043ff3bf31a6a81810b4ce9e87ef936f1f529/tensorflow/g3doc/api_docs/images) in [this commit](https://github.com/tensorflow/tensorflow/commit/854f49bd43588c062b046384f239f64a3d819702).  It looks like those images should be restored to a new tensorflow/docs_src/api_guides/images folder (or, since the URL is being resolved from documentation that is generated from the source code, maybe it should be in a new tensorflow/docs_src/images folder), and the links in the source code documentation need to be changed (e.g., for the ScatterAdd.png image, the image source in [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/state_ops.cc#L289) would need to be changed to the correct link.)", "Thanks for filing the issue @christopherhesse \r\n\r\nAssigning to @dr4b ", "The images will likely never get uploaded to GitHub.  The best solution, which does not have a timeline, would be to replace the image links in the GitHub mark down to point to tensorflow.org.  It might be more work than you want to do, but you could try to script it yourself.  Putting the images in GitHub bloats the repo, or so I am told.   Image links should never break over a line due to internal style guides, so a simple find replace might cover a large percent of the situation.  Our publishing process is a big convoluted due to a desire to accept community contributions.     ", "We should restore the images in our internal image repo and fix those links to point to /images.  We need to fix this internally, so I will take this."]}, {"number": 8063, "title": " convert_graphdef_memmapped_format does not work on graph quantized with mode=\"weights\"", "body": "Using the `quantize_graph` tool and choosing `mode='weights'` produces a graph that when run through `convert_graphdef_memmapped_format` reports:\r\n```\r\ntensorflow/contrib/util/convert_graphdef_memmapped_format_lib.cc:168] Converted 0 nodes\r\n```\r\nGraphs quantized with other modes work (for example `weights_rounded`):\r\n```\r\ntensorflow/contrib/util/convert_graphdef_memmapped_format_lib.cc:168] Converted 10 nodes\r\n```", "comments": ["NM. Looks like tensors did not match the `--min_conversion_tensor_size` threshold."]}, {"number": 8062, "title": "Branch 149132162", "body": "@andrewharp fyi", "comments": []}, {"number": 8061, "title": "Multi-GPU Inference using FIFO Queues issue in TF v1.0.0", "body": "Here is an example multi-gpu inference code which produces incorrect Output in TF v1.0.0 but produces correct output in v0.11.0 (single GPU produces correct output in both versions). Please refer to the attached code for more details.\r\n\r\n[InferenceTest.txt](https://github.com/tensorflow/tensorflow/files/817729/InferenceTest.txt)\r\n[Inference.txt](https://github.com/tensorflow/tensorflow/files/817730/Inference.txt)\r\n\r\n\r\n**Correct Output (TF v0.11) -- 2 GPUs** \r\npython InferenceTest.py 2\r\n-------------------------\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n**************** Using 2 GPUs *****************\r\nTensorFlow version:  0.11.0\r\nGPU ID List:  [0, 1]\r\nDevice:  /gpu:0\r\nDevice:  /gpu:1\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 0 with properties: \r\nname: Tesla M40\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\r\npciBusID 0000:00:05.0\r\nTotal memory: 11.21GiB\r\nFree memory: 11.09GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x37b9dc0\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:951] Found device 1 with properties: \r\nname: Tesla M40\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\r\npciBusID 0000:00:09.0\r\nTotal memory: 11.21GiB\r\nFree memory: 11.09GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 0 to device ordinal 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:855] cannot enable peer access from device ordinal 1 to device ordinal 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:972] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:982] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)\r\nInput Value 0\r\nInput Value 1\r\nInput Value 2\r\nInput Value 3\r\nInput Value 4\r\nInput Value 5\r\nInput Value 6\r\nInput Value 7\r\nInput Value 8\r\nInput Value 9\r\nInput Queue Size=  10\r\nResults Queue Size=  0\r\nProcessing: Input Queue Size=  8\r\nProcessing: Results Queue Size=  2\r\nProcessing: Input Queue Size=  6\r\nProcessing: Results Queue Size=  4\r\nProcessing: Input Queue Size=  4\r\nProcessing: Results Queue Size=  6\r\nProcessing: Input Queue Size=  2\r\nProcessing: Results Queue Size=  8\r\nProcessing: Input Queue Size=  0\r\nProcessing: Results Queue Size=  10\r\n************* Dequeue Results ********************\r\nResults Queue Size=  10\r\nDequeue Results: Results Output  1:\r\nDequeue Results: Results Queue Size=  9\r\nDequeue Results: Results Output  2:\r\nDequeue Results: Results Queue Size=  8\r\nDequeue Results: Results Output  3:\r\nDequeue Results: Results Queue Size=  7\r\nDequeue Results: Results Output  4:\r\nDequeue Results: Results Queue Size=  6\r\nDequeue Results: Results Output  6:\r\nDequeue Results: Results Queue Size=  5\r\nDequeue Results: Results Output  5:\r\nDequeue Results: Results Queue Size=  4\r\nDequeue Results: Results Output  7:\r\nDequeue Results: Results Queue Size=  3\r\nDequeue Results: Results Output  8:\r\nDequeue Results: Results Queue Size=  2\r\nDequeue Results: Results Output  9:\r\nDequeue Results: Results Queue Size=  1\r\nDequeue Results: Results Output  10:\r\nDequeue Results: Results Queue Size=  0\r\nFinal Input Queue Size=  0\r\nFinal Results Queue Size=  0\r\n\r\n***************************************************************************************************\r\n**Wrong Output (TF v1.0.0) -- 2 GPUs** \r\npython InferenceTest.py 2\r\n-------------------------\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n**************** Using 2 GPUs *****************\r\nTensorFlow version:  1.0.0\r\nGPU ID List:  [0, 1]\r\nDevice:  /gpu:0\r\nDevice:  /gpu:1\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Tesla M40\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\r\npciBusID 0000:00:05.0\r\nTotal memory: 11.21GiB\r\nFree memory: 11.09GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x1999d60\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: Tesla M40\r\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.112\r\npciBusID 0000:00:09.0\r\nTotal memory: 11.21GiB\r\nFree memory: 11.09GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla M40, pci bus id: 0000:00:05.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla M40, pci bus id: 0000:00:09.0)\r\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:1 for node 'Tower_1/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Dequeue_Input_Data/fifo_queue_Dequeue' because the input edge from 'Input_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:669] Ignoring device specification /GPU:0 for node 'Tower_0/Results_Enqueue/fifo_queue_enqueue' because the input edge from 'Results_FIFO_Queue/fifo_queue' is a reference connection and already has a device field set to /CPU:0\r\nInput Value 0\r\nInput Value 1\r\nInput Value 2\r\nInput Value 3\r\nInput Value 4\r\nInput Value 5\r\nInput Value 6\r\nInput Value 7\r\nInput Value 8\r\nInput Value 9\r\nInput Queue Size=  10\r\nResults Queue Size=  0\r\nProcessing: Input Queue Size=  9\r\nProcessing: Results Queue Size=  2\r\nProcessing: Input Queue Size=  8\r\nProcessing: Results Queue Size=  4\r\nProcessing: Input Queue Size=  7\r\nProcessing: Results Queue Size=  6\r\nProcessing: Input Queue Size=  6\r\nProcessing: Results Queue Size=  8\r\nProcessing: Input Queue Size=  5\r\nProcessing: Results Queue Size=  10\r\nProcessing: Input Queue Size=  4\r\nProcessing: Results Queue Size=  12\r\nProcessing: Input Queue Size=  3\r\nProcessing: Results Queue Size=  14\r\nProcessing: Input Queue Size=  2\r\nProcessing: Results Queue Size=  16\r\nProcessing: Input Queue Size=  1\r\nProcessing: Results Queue Size=  18\r\nProcessing: Input Queue Size=  0\r\nProcessing: Results Queue Size=  20\r\n************* Dequeue Results ********************\r\nResults Queue Size=  20\r\nDequeue Results: Results Output  1:\r\nDequeue Results: Results Queue Size=  19\r\nDequeue Results: Results Output  1:\r\nDequeue Results: Results Queue Size=  18\r\nDequeue Results: Results Output  2:\r\nDequeue Results: Results Queue Size=  17\r\nDequeue Results: Results Output  2:\r\nDequeue Results: Results Queue Size=  16\r\nDequeue Results: Results Output  3:\r\nDequeue Results: Results Queue Size=  15\r\nDequeue Results: Results Output  3:\r\nDequeue Results: Results Queue Size=  14\r\nDequeue Results: Results Output  4:\r\nDequeue Results: Results Queue Size=  13\r\nDequeue Results: Results Output  4:\r\nDequeue Results: Results Queue Size=  12\r\nDequeue Results: Results Output  5:\r\nDequeue Results: Results Queue Size=  11\r\nDequeue Results: Results Output  5:\r\nDequeue Results: Results Queue Size=  10\r\nDequeue Results: Results Output  6:\r\nDequeue Results: Results Queue Size=  9\r\nDequeue Results: Results Output  6:\r\nDequeue Results: Results Queue Size=  8\r\nDequeue Results: Results Output  7:\r\nDequeue Results: Results Queue Size=  7\r\nDequeue Results: Results Output  7:\r\nDequeue Results: Results Queue Size=  6\r\nDequeue Results: Results Output  8:\r\nDequeue Results: Results Queue Size=  5\r\nDequeue Results: Results Output  8:\r\nDequeue Results: Results Queue Size=  4\r\nDequeue Results: Results Output  9:\r\nDequeue Results: Results Queue Size=  3\r\nDequeue Results: Results Output  9:\r\nDequeue Results: Results Queue Size=  2\r\nDequeue Results: Results Output  10:\r\nDequeue Results: Results Queue Size=  1\r\nDequeue Results: Results Output  10:\r\nDequeue Results: Results Queue Size=  0\r\nFinal Input Queue Size=  0\r\nFinal Results Queue Size=  0\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System: CentOS 7.2.1511\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n![cudaversion](https://cloud.githubusercontent.com/assets/21690396/23567127/a771fd10-0009-11e7-88bb-daa27a1b28c2.PNG)\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0-cp35-cp35m-linux_x86_64.whl \r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nSee the output above for 2 different tensor flow versions\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["I think this is an instance of a previous issue: #7038. This was fixed at `HEAD` but not in the 1.0 release. Can you try a nightly build and see if that fixes your problem? Thanks!\r\n\r\n/cc @alextp to confirm.", "Yes, this looks like that issue.", "I am trying to build tensorflow with the latest master branchbut getting the following error. Any ideas on how this could be fixed?\r\n\r\nERROR: /home/agupta2/TensorFlow_Sources/tensorflow/tensorflow/stream_executor/BUILD:39:1: C++ compilation of rule '//tensorflow/stream_executor:cuda_platform' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /root/.cache/bazel/_bazel_root/56ec560c84df85845595d71cca739b3f/execroot/tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/lib64/:/root/anaconda3/lib/:/usr/local/cuda/lib64/:/usr/local/cuda/extras/CUPTI/lib64/: \\\r\n    PATH=/home/agupta2/Bazel/output:/root/anaconda3/bin:/usr/local/cuda/bin:/sbin:/usr/sbin:/usr/local/sbin:/root/bin:/usr/local/bin:/usr/bin:/bin:/usr/bin/X11:/usr/games:/opt/ibutils/bin \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-march=native' '-std=c++11' '-march=native' -MD -MF bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_blas.pic.d '-frandom-seed=bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_blas.pic.o' -fPIC -DEIGEN_MPL2_ONLY -DTENSORFLOW_USE_JEMALLOC -iquote . -iquote bazel-out/local_linux-py3-opt/genfiles -iquote external/jemalloc -iquote bazel-out/local_linux-py3-opt/genfiles/external/jemalloc -iquote external/bazel_tools -iquote bazel-out/local_linux-py3-opt/genfiles/external/bazel_tools -iquote external/protobuf -iquote bazel-out/local_linux-py3-opt/genfiles/external/protobuf -iquote external/eigen_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/local_linux-py3-opt/genfiles/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/local_linux-py3-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive -iquote external/highwayhash -iquote bazel-out/local_linux-py3-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda -isystem external/jemalloc/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/jemalloc/include -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/protobuf/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/protobuf/src -isystem external/eigen_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/local_linux-py3-opt/genfiles/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/local_linux-py3-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/local_linux-py3-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/include -isystem bazel-out/local_linux-py3-opt/genfiles/external/local_config_cuda/cuda/include -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c tensorflow/stream_executor/cuda/cuda_blas.cc -o bazel-out/local_linux-py3-opt/bin/tensorflow/stream_executor/_objs/cuda_platform/tensorflow/stream_executor/cuda/cuda_blas.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In member function 'virtual bool perftools::gputools::cuda::CUDABlas::GetBlasGemmAlgorithms(std::vector<long long int>*)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:9: error: 'CUBLAS_GEMM_ALGO5' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n         ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:28: error: 'CUBLAS_GEMM_ALGO6' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                            ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:47: error: 'CUBLAS_GEMM_ALGO7' was not declared in this scope\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                                               ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:1916:64: error: unable to deduce 'std::initializer_list<_Tp>&&' from '{CUBLAS_GEMM_DFALT, CUBLAS_GEMM_ALGO0, CUBLAS_GEMM_ALGO1, CUBLAS_GEMM_ALGO2, CUBLAS_GEMM_ALGO3, CUBLAS_GEMM_ALGO4, <expression error>, <expression error>, <expression error>}'\r\n         CUBLAS_GEMM_ALGO5, CUBLAS_GEMM_ALGO6, CUBLAS_GEMM_ALGO7}) {\r\n                                                                ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasOperation_t perftools::gputools::cuda::{anonymous}::CUDABlasTranspose(perftools::gputools::blas::Transpose)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:410:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasFillMode_t perftools::gputools::cuda::{anonymous}::CUDABlasUpperLower(perftools::gputools::blas::UpperLower)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:421:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasDiagType_t perftools::gputools::cuda::{anonymous}::CUDABlasDiagonal(perftools::gputools::blas::Diagonal)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:432:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cudaDataType_t perftools::gputools::cuda::{anonymous}::CUDAComputationType(perftools::gputools::blas::ComputationType)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:519:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\ntensorflow/stream_executor/cuda/cuda_blas.cc: In function 'cublasSideMode_t perftools::gputools::cuda::{anonymous}::CUDABlasSide(perftools::gputools::blas::Side)':\r\ntensorflow/stream_executor/cuda/cuda_blas.cc:443:1: warning: control reaches end of non-void function [-Wreturn-type]\r\n }\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n\r\n", "It looks like that I have an older version of Cuda 8 libraries. Please ignore my previous email. ", "@agupta74  Please let us know if building at head solves your problem.", "I was able to build successfully from sources using master branch. The issue is fixed. We can close this issue now. Thanks !", "Great\u2014thanks for verifying!", "I would like to know in which version of TensorFlow release will the fix for this issue be delivered?  Based on the release notes of the latest TensorFlow release (TensorFlow 1.1.0- rc1), I do not see the fix for this issue delivered yet.", "Getting this error in 1.3\r\n\r\nstream_executor:cuda_platform' failed"]}, {"number": 8060, "title": "Hi, I am using TF 1.0.0 on ubuntu 16.04 64bit os , I also meets this error, can you help to resolve it, thank you.", "body": "```\r\nbazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=32 --train_dir=/data/imagenet-train --data_dir=/data/imagenet-data                                                              \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally             \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally                \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally              \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally                 \r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally             \r\nTraceback (most recent call last):                                                                                    \r\n  File \"/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py\", line 41, in <module>                                                                                          \r\n    tf.app.run()                                                                                                      \r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run                    \r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))                                                                \r\n  File \"/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/imagenet_train.py\", line 37, in main                                                                                              \r\n    inception_train.train(dataset)                                                                                    \r\n  File \"/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/inception_train.py\", line 217, in train                                                                                           \r\n    num_preprocess_threads=num_preprocess_threads)                                                                    \r\n  File \"/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py\", line 136, in distorted_inputs                                                                               \r\n    num_readers=FLAGS.num_readers)                                                                                    \r\n  File \"/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py\", line 490, in batch_inputs\r\n    example_serialized)\r\n  File \"/root/tensorflow/models/inception/bazel-bin/inception/imagenet_train.runfiles/inception/inception/image_processing.py\", line 397, in parse_example_proto\r\n    bbox = tf.concat(0, [ymin, xmin, ymax, xmax])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 1029, in concat\r\n    dtype=dtypes.int32).get_shape(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 637, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 702, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 110, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py\", line 99, in constant\r\n    tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 367, in make_tensor_proto\r\n    _AssertCompatible(values, dtype)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py\", line 302, in _AssertCompatible\r\n    (dtype.name, repr(mismatch), type(mismatch).__name__))\r\nTypeError: Expected int32, got list containing Tensors of type '_Message' instead.\r\n```", "comments": ["I did a quick test and I get the same error.  The models repo is being rebuilt, but I will try to take a look at the possible root cause.   I suspect an issue when it was upgraded (or not) to TF 1.0 but that is just an off-the-cuff wild guess.  Someone with more knowledge might also help but if not I will take a look.", "Responding with half of an answer because if you are like me, you are playing on the weekend and want this to work.  I will try to do a PR tomorrow but I am virtually sure the issues starts here:\r\n\r\n`inception/image_processing.py\", line 397, in parse_example_proto bbox = tf.concat(0, [ymin, xmin, ymax, xmax])`\r\n\r\nConcat changed in TF 1.0, the 0 goes at the end.  But there is more, tf.image_summary also changed.  I suggest running the tf [migration script](https://www.tensorflow.org/install/migration) on the file.  I believe the other files have been updated.  ", "Took me a week rather than one day.  Here is the PR.  I tested it locally and works with TF 1.0. Sorry tensorflow/models is a bit tough.  We are working on it.  \r\n\r\nhttps://github.com/tensorflow/models/pull/1150\r\n\r\nMarking closed."]}, {"number": 8059, "title": "Unable to run tensorflow on Android through android studio", "body": "The build is successful but the app crashes immediately when it's brought up on any device.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/7256\r\nhttps://github.com/tensorflow/tensorflow/issues/3444 (assembleDebug error - resolved)\r\n\r\n### Environment info\r\nOperating System:\r\nRunning Android Studio 2.2.3 on Mac with Sierra 10.12.3\r\nTrying to run on a virtual Nexus 7 with Marshmallow x86\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI followed the instructions in the tensorflow GitHub readme [here]([https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android]).\r\n\r\n### What other attempted solutions have you tried?\r\nI am able to compile using bazel build but I cannot get it running through Android Studio. I tried using a Pixel C and a Visual Land Prestige.\r\n\r\n### Logs or other output that would be helpful\r\n03/03 10:27:22: Launching android\r\n$ adb shell am start -n \"org.tensorflow.demo/org.tensorflow.demo.ClassifierActivity\" -a android.intent.action.MAIN -c android.intent.category.LAUNCHER\r\nConnected to process 4001 on device emulator-5554\r\nI/InstantRun: Instant Run Runtime started. Android package is org.tensorflow.demo, real application class is null.\r\n\r\n              [ 03-03 10:27:22.381  1648: 1669 D/         ]\r\n              HostConnection::get() New Host Connection established 0x9d43f2c0, tid 1669\r\nD/tensorflow: CameraActivity: onCreate org.tensorflow.demo.ClassifierActivity@9ad6111\r\nD/tensorflow: CameraActivity: onStart org.tensorflow.demo.ClassifierActivity@9ad6111\r\nD/tensorflow: CameraActivity: onResume org.tensorflow.demo.ClassifierActivity@9ad6111\r\nD/OpenGLRenderer: Use EGL_SWAP_BEHAVIOR_PRESERVED: true\r\n\r\n                  [ 03-03 10:27:22.710  4001: 4001 D/         ]\r\n                  HostConnection::get() New Host Connection established 0xaa7bfb80, tid 4001\r\n\r\n\r\n                  [ 03-03 10:27:22.840  4001: 4017 D/         ]\r\n                  HostConnection::get() New Host Connection established 0xb2d1a000, tid 4017\r\nI/OpenGLRenderer: Initialized EGL, version 1.4\r\nW/OpenGLRenderer: Failed to choose config with EGL_SWAP_BEHAVIOR_PRESERVED, retrying without...\r\nI/CameraManagerGlobal: Connecting to camera service\r\nI/tensorflow: CameraConnectionFragment: Adding size: 640x480\r\nI/tensorflow: CameraConnectionFragment: Not adding size: 352x288\r\nI/tensorflow: CameraConnectionFragment: Not adding size: 320x240\r\nI/tensorflow: CameraConnectionFragment: Not adding size: 176x144\r\nI/tensorflow: CameraConnectionFragment: Chosen size: 640x480\r\nI/TensorFlowImageClassifier: Reading labels from: imagenet_comp_graph_label_strings.txt\r\nE/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\nI/TensorFlowInferenceInterface: Loading tensorflow_inference.\r\nD/AndroidRuntime: Shutting down VM\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n                  Process: org.tensorflow.demo, PID: 4001\r\n                  java.lang.RuntimeException: Error initializing TensorFlow!\r\n                      at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:131)\r\n                      at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:158)\r\n                      at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:394)\r\n                      at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:411)\r\n                      at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:63)\r\n                      at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:94)\r\n                      at android.view.TextureView.getHardwareLayer(TextureView.java:368)\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15151)\r\n                      at android.view.View.draw(View.java:15948)\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15169)\r\n                      at android.view.View.draw(View.java:15948)\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                      at android.view.View.draw(View.java:16181)\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15174)\r\n                      at android.view.View.draw(View.java:15948)\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15169)\r\n                      at android.view.View.draw(View.java:15948)\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15169)\r\n                      at android.view.View.draw(View.java:15948)\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\r\n                      at android.view.View.draw(View.java:16181)\r\n                      at com.android.internal.policy.PhoneWindow$DecorView.draw(PhoneWindow.java:2690)\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15174)\r\n                      at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:281)\r\n                      at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:287)\r\n                      at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:322)\r\n                      at android.view.ViewRootImpl.draw(ViewRootImpl.java:2615)\r\n                      at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2434)\r\n                      at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2067)\r\n                      at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1107)\r\n                      at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6013)\r\n                      at android.view.Choreographer$CallbackRecord.run(Choreographer.java:858)\r\n                      at android.view.Choreographer.doCallbacks(Choreographer.java:670)\r\n                      at android.view.Choreographer.doFrame(Choreographer.java:606)\r\n                      at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:844)\r\n                      at android.os.Handler.handleCallback(Handler.java:739)\r\n                      at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                      at android.os.Looper.loop(Looper.java:148)\r\n                      at android.app.ActivityThread.main(ActivityThread.java:5417)\r\n                      at java.lang.reflect.Method.invoke(Native Method)\r\n                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)\r\n                   Caused by: java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present and loaded.\r\n                      at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:57)\r\n                      at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:101)\r\n                      at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:121)\r\n                      at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:158)\u00a0\r\n                      at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:394)\u00a0\r\n                      at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:411)\u00a0\r\n                      at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:63)\u00a0\r\n                      at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:94)\u00a0\r\n                      at android.view.TextureView.getHardwareLayer(TextureView.java:368)\u00a0\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15151)\u00a0\r\n                      at android.view.View.draw(View.java:15948)\u00a0\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\u00a0\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\u00a0\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15169)\u00a0\r\n                      at android.view.View.draw(View.java:15948)\u00a0\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\u00a0\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\u00a0\r\n                      at android.view.View.draw(View.java:16181)\u00a0\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15174)\u00a0\r\n                      at android.view.View.draw(View.java:15948)\u00a0\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\u00a0\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\u00a0\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15169)\u00a0\r\n                      at android.view.View.draw(View.java:15948)\u00a0\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\u00a0\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\u00a0\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15169)\u00a0\r\n                      at android.view.View.draw(View.java:15948)\u00a0\r\n                      at android.view.ViewGroup.drawChild(ViewGroup.java:3609)\u00a0\r\n                      at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3399)\u00a0\r\n                      at android.view.View.draw(View.java:16181)\u00a0\r\n                      at com.android.internal.policy.PhoneWindow$DecorView.draw(PhoneWindow.java:2690)\u00a0\r\n                      at android.view.View.updateDisplayListIfDirty(View.java:15174)\u00a0\r\n                      at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:281)\u00a0\r\n                      at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:287)\u00a0\r\n                      at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:322)\u00a0\r\n                      at android.view.ViewRootImpl.draw(ViewRootImpl.java:2615)\u00a0\r\n                      at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2434)\u00a0\r\n                      at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2067)\u00a0\r\n                      at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1107)\u00a0\r\n                      at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6013)\u00a0\r\n                      at android.view.Choreographer$CallbackRecord.run(Choreographer.java:858)\u00a0\r\n                      at android.view.Choreographer.doCallbacks(Choreographer.java:670)\u00a0\r\n                      at android.view.Choreographer.doFrame(Choreographer.java:606)\u00a0\r\n                      at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:844)\u00a0\r\n                      at android.os.Handler.handleCallback(Handler.java:739)\u00a0\r\n                      at android.os.Handler.dispatchMessage(Handler.java:95)\u00a0\r\n                      at android.os.Looper.loop(Looper.java:148)\u00a0\r\n                      at android.app.ActivityThread.main(ActivityThread.java:5417)\u00a0\r\n                      at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:726)\u00a0\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:616)\u00a0\r\nApplication terminated.\r\n", "comments": ["Relevant line from Exception (it's sort of hidden in the middle):\r\n\r\n> Caused by: java.lang.RuntimeException: Native TF methods not found; check that the correct native libraries are present and loaded.\r\n\r\nThe most likely cause is that you're only packaging armeabi-v7a libs for an x86 emulated device. Bazel builds multiple ABIs, but the gradle.build only specifies armeabi-v7a, so you should change that to x86 or x86_64.", "I did change the cpuType in the build.gradle to x86. I also tried to install the demo on a Pixel C using the armeabi-v7a cpuType.", "Hmm, what do you see when you run `unzip -v tensorflow_demo.apk`? (change the filename to whatever the actual output location from gradle is)", "unzip -v android-debug.apk \r\nArchive:  android-debug.apk\r\n Length   Method    Size  Cmpr    Date    Time   CRC-32   Name\r\n--------  ------  ------- ---- ---------- ----- --------  ----\r\n    3444  Defl:N     1033  70% 00-00-1980 00:00 9cf79666  AndroidManifest.xml\r\n     773  Defl:N      602  22% 00-00-1980 00:00 fdcf1a2f  META-INF/CERT.RSA\r\n    4775  Defl:N     2220  54% 00-00-1980 00:00 0676efa1  META-INF/CERT.SF\r\n    4713  Defl:N     2180  54% 00-00-1980 00:00 5de00cd7  META-INF/MANIFEST.MF\r\n     461  Defl:N      292  37% 00-00-1980 00:00 eafcece9  assets/BUILD\r\n   11416  Defl:N     4452  61% 00-00-1980 00:00 c04c6f58  assets/LICENSE\r\n   10492  Defl:N     5554  47% 00-00-1980 00:00 bdb9b97a  assets/imagenet_comp_graph_label_strings.txt\r\n   42335  Defl:N     4901  88% 00-00-1980 00:00 90dd4646  assets/multibox_location_priors.txt\r\n18556306  Defl:N 16704579  10% 00-00-1980 00:00 cdeb207e  assets/multibox_model.pb\r\n  563897  Defl:N   270926  52% 00-00-1980 00:00 7d7f4ff3  assets/stylize_quantized.pb\r\n53884595  Defl:N 50111288   7% 00-00-1980 00:00 6d3b8d15  assets/tensorflow_inception_graph.pb\r\n  131978  Stored   131978   0% 00-00-1980 00:00 3228d14c  assets/thumbnails/style0.jpg\r\n  113303  Stored   113303   0% 00-00-1980 00:00 13596907  assets/thumbnails/style1.jpg\r\n  151688  Stored   151688   0% 00-00-1980 00:00 a3e9c196  assets/thumbnails/style10.jpg\r\n  181154  Stored   181154   0% 00-00-1980 00:00 adb4f0a3  assets/thumbnails/style11.jpg\r\n  107888  Stored   107888   0% 00-00-1980 00:00 79dbb7d0  assets/thumbnails/style12.jpg\r\n  198586  Stored   198586   0% 00-00-1980 00:00 c97505a9  assets/thumbnails/style13.jpg\r\n  104584  Stored   104584   0% 00-00-1980 00:00 5832132a  assets/thumbnails/style14.jpg\r\n  152269  Stored   152269   0% 00-00-1980 00:00 005ad28f  assets/thumbnails/style15.jpg\r\n  168251  Stored   168251   0% 00-00-1980 00:00 6bccba6c  assets/thumbnails/style16.jpg\r\n  146736  Stored   146736   0% 00-00-1980 00:00 132cbdcb  assets/thumbnails/style17.jpg\r\n  173496  Stored   173496   0% 00-00-1980 00:00 9a6a37a7  assets/thumbnails/style18.jpg\r\n  227300  Stored   227300   0% 00-00-1980 00:00 a11b35b4  assets/thumbnails/style19.jpg\r\n  140121  Stored   140121   0% 00-00-1980 00:00 34147cc2  assets/thumbnails/style2.jpg\r\n  134506  Stored   134506   0% 00-00-1980 00:00 3df5a425  assets/thumbnails/style20.jpg\r\n  138136  Stored   138136   0% 00-00-1980 00:00 0754c15d  assets/thumbnails/style21.jpg\r\n  168478  Stored   168478   0% 00-00-1980 00:00 25998a8e  assets/thumbnails/style22.jpg\r\n  108525  Stored   108525   0% 00-00-1980 00:00 91f2c6b0  assets/thumbnails/style23.jpg\r\n  131414  Stored   131414   0% 00-00-1980 00:00 ead622b6  assets/thumbnails/style24.jpg\r\n  190320  Stored   190320   0% 00-00-1980 00:00 b39eb2ce  assets/thumbnails/style25.jpg\r\n  181100  Stored   181100   0% 00-00-1980 00:00 a86a5141  assets/thumbnails/style3.jpg\r\n  181565  Stored   181565   0% 00-00-1980 00:00 9dfc14ff  assets/thumbnails/style4.jpg\r\n  162432  Stored   162432   0% 00-00-1980 00:00 2e8636d3  assets/thumbnails/style5.jpg\r\n  167919  Stored   167919   0% 00-00-1980 00:00 560fdd11  assets/thumbnails/style6.jpg\r\n   97875  Stored    97875   0% 00-00-1980 00:00 684b2c5b  assets/thumbnails/style7.jpg\r\n  121177  Stored   121177   0% 00-00-1980 00:00 d552dee2  assets/thumbnails/style8.jpg\r\n  208059  Stored   208059   0% 00-00-1980 00:00 83817fe1  assets/thumbnails/style9.jpg\r\n     740  Defl:N      471  36% 00-00-1980 00:00 481c4d20  classes.dex\r\n   64752  Defl:N    32965  49% 00-00-1980 00:00 f7b87071  classes2.dex\r\n  484649  Defl:N   180245  63% 00-00-1980 00:00 ff23ba2f  instant-run.zip\r\n  742980  Defl:N   306495  59% 00-00-1980 00:00 60d11c46  lib/x86/libtensorflow_demo.so\r\n     234  Stored      234   0% 00-00-1980 00:00 c560f719  res/drawable-hdpi-v4/tile.9.png\r\n     728  Stored      728   0% 00-00-1980 00:00 9be31f46  res/drawable-xhdpi-v4/ic_action_info.png\r\n    5855  Stored     5855   0% 00-00-1980 00:00 284931e6  res/drawable-xhdpi-v4/ic_launcher.png\r\n     404  Defl:N      228  44% 00-00-1980 00:00 97e57137  res/layout/activity_camera.xml\r\n     960  Defl:N      395  59% 00-00-1980 00:00 0df7a886  res/layout/camera_connection_fragment.xml\r\n    1356  Defl:N      501  63% 00-00-1980 00:00 fd74248b  res/layout/camera_connection_fragment_stylize.xml\r\n     784  Defl:N      319  59% 00-00-1980 00:00 3cce775d  res/layout/camera_connection_fragment_tracking.xml\r\n    4616  Stored     4616   0% 00-00-1980 00:00 be2ae3b9  resources.arsc\r\n--------          -------  ---                            -------\r\n78380125         71629939   9%                            49 files", "You're missing libtensorflow_inference.so for some reason. Check the bazel logs and bazel-bin dir to see if you can figure out what happened to it?", "I found it in the bazel-bin at /tensorflow/contrib/android/libtensorflow_inference.so.", "Ok -- is that from the command-line bazel build or from the gradle build inside Android Studio (if the former then make sure you `bazel clean` first)?\r\n\r\nDoes it make its way into the libs/ dir in the project when you build, and does the Android Studio/gradle log give any indication why it's not being packaged?", "I'm assuming you mean the jniLibs directory? If so then, no, I only see the libtensorflow_demo.so not the libtensorflow_inference.so library. \r\n\r\nI did do a bazel clean before running in android studio and the libtensorflow_inference.so is built at the location /tensorflow/contrib/android/libtensorflow_inference.so. I don't see any indication as to why it's not being packaged in the log files. Is there a particular log file (what are the steps to output the log file) that you're talking about?", "@SeanTyco I was able to reproduce this locally -- the problem is that gradle is only looking for .so files under tensorflow/examples/android and not tensorflow/contrib/android as well (where libtensorflow_inference.so resides).\r\n\r\nIf you change add another \"from\" directive to \"copyNativeLibs\" so that it looks like the following:\r\n```\r\ntask copyNativeLibs(type: Copy) {\r\n    from('../../../bazel-bin/tensorflow/contrib/android/') { include '*.so' }\r\n    from('../../../bazel-bin/tensorflow/examples/android/') { include '*.so' }\r\n    into nativeDir\r\n    duplicatesStrategy = 'include'\r\n    dependsOn 'buildNative'\r\n}\r\n```\r\nit should work.\r\n\r\nI also had to switch my local gradle plugin defined in the build.gradle from 2.2.0 to 2.3.0 to deal with a !zip.isFile() problem, just in case you run into that too.\r\n\r\nWill check in a full fix soon after I figure out how to remove the read-only attribute on the files so that the build can be repeated without manually deleting.", "In case anyone else is looking, I initially got the following, even when doing a 'bazel clean' before building:\r\n```\r\n:copyNativeLibs FAILED\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':copyNativeLibs'.\r\n> Could not copy file '.../1a4c05cdb2ca1de9a943143330f40f73/execroot/tensorflow/bazel-out/x86-4.9-gnu-libstdcpp-opt/bin/tensorflow/examples/android/libtensorflow_demo.so' to '.../tensorflow/tensorflow/examples/android/libs/x86/libtensorflow_demo.so'.\r\n```\r\nYou have to open permissions on the .so file in the `.../tensorflow/tensorflow/examples/android/libs/x86/`  directory.", "Yes, it turns out that bazel creates the files as read-only by default. To prevent this from happening on subsequent builds you can add a \"fileMode 0644\" to the copyNativeLibs task in build.gradle (I'm in the process of submitting a fix for this currently).", "Have similar problems starting on real device (LP5.1) successfully created apk with Android studio.\r\nApk created by Bazel works fine. Comparing two apks (Old - android-debug.apk from Android studio; New - tensorflow_demo.apk from Bazel) shows the next:\r\n![image](https://cloud.githubusercontent.com/assets/14268320/23840746/a72deb64-079f-11e7-8285-292d33820226.png)\r\nLooks like existence of libtensorflow_inference.so doesn't metter. It's not exist at tensorflow_demo.apk from Bazel and apk work fine.\r\n\r\nPS Logcat (through Android Monitor in Android Studio) when starting android-debug.apk shows next error:\r\n03-12 23:53:26.525 26147-26147/org.tensorflow.demo E/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                     Process: org.tensorflow.demo, PID: 26147\r\n                                                                     java.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \"/data/app/org.tensorflow.demo-1/base.apk\", zip file \"/data/app/org.tensorflow.demo-1/split_lib_slice_0_apk.apk\", ....<censored>... ],nativeLibraryDirectories=[/data/app/org.tensorflow.demo-1/lib/arm64, /vendor/lib64, /system/lib64]]] couldn't find \"libtensorflow_demo.so\"\r\nLooks like libtensorflow_demo.so is included in Libs but not linked for some reason... Will keep investigating.", "@ArtsiomCh You need to build both `tensorflow/examples/android:libtensorflow_demo.so` and `tensorflow/contrib/android:libtensorflow_inference.so` and make sure they both make it into the APK. The former contains the image pre-processing and tracking code used by the examples, and the latter contains TensorFlow itself.", "I thought so too. But the trick is that apk built by bazel (`bazel build -c opt //tensorflow/examples/android:tensorflow_demo`) does not contain `libtensorflow_inference.so` either but only `libtensorflow_demo.so` in /lib. And its work fine on real device.\r\nI would presume `libtensorflow_demo.so` include functionality of `libtensorflow_inference.so`.\r\n\r\n", "I am having a similar issue. I am using my own retrained model (which I have stripped) and put in assets. I have updated ClassifierActivity with appropriate values. \r\n\r\nI am running on a real android device (API 24) and I get \"No implementation found for long org.tensorflow.contrib.android.RunStats.allocate()\" from logcat. Both lib/armeabi-v7a/libtensorflow_demo.so and lib/armeabi-v7a/libtensorflow_inference.so are in the resulting APK.\r\n\r\nFull stack:\r\n`\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n                                                                     Process: org.tensorflow.demo, PID: 13456\r\n                                                                     java.lang.UnsupportedOperationException: Op BatchNormWithGlobalNormalization is not available in GraphDef version 21. It has been removed in version 9. Use tf.nn.batch_normalization().\r\n                                                                         at org.tensorflow.Graph.importGraphDef(Native Method)\r\n                                                                         at org.tensorflow.Graph.importGraphDef(Graph.java:118)\r\n                                                                         at org.tensorflow.Graph.importGraphDef(Graph.java:102)\r\n                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.load(TensorFlowInferenceInterface.java:414)\r\n                                                                         at org.tensorflow.contrib.android.TensorFlowInferenceInterface.initializeTensorFlow(TensorFlowInferenceInterface.java:95)\r\n                                                                         at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:106)\r\n                                                                         at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:118)\r\n                                                                         at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:158)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:408)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:415)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)\r\n                                                                         at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)\r\n                                                                         at android.view.TextureView.getHardwareLayer(TextureView.java:387)\r\n                                                                         at android.view.TextureView.draw(TextureView.java:325)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:16116)\r\n                                                                         at android.view.View.draw(View.java:16900)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3764)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3550)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:16111)\r\n                                                                         at android.view.View.draw(View.java:16900)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3764)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3550)\r\n                                                                         at android.view.View.draw(View.java:17137)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:16116)\r\n                                                                         at android.view.View.draw(View.java:16900)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3764)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3550)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:16111)\r\n                                                                         at android.view.View.draw(View.java:16900)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3764)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3550)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:16111)\r\n                                                                         at android.view.View.draw(View.java:16900)\r\n                                                                         at android.view.ViewGroup.drawChild(ViewGroup.java:3764)\r\n                                                                         at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3550)\r\n                                                                         at android.view.View.draw(View.java:17137)\r\n                                                                         at com.android.internal.policy.DecorView.draw(DecorView.java:751)\r\n                                                                         at android.view.View.updateDisplayListIfDirty(View.java:16116)\r\n                                                                         at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:656)\r\n                                                                         at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:662)\r\n                                                                         at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:770)\r\n                                                                         at android.view.ViewRootImpl.draw(ViewRootImpl.java:2830)\r\n                                                                         at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2638)\r\n                                                                         at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2245)\r\n                                                                         at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1272)\r\n                                                                         at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6396)\r\n                                                                         at android.view.Choreographer$CallbackRecord.run(Choreographer.java:871)\r\n                                                                         at android.view.Choreographer.doCallbacks(Choreographer.java:683)\r\n                                                                         at android.view.Choreographer.doFrame(Choreographer.java:619)\r\n                                                                         at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:857)\r\n                                                                         at android.os.Handler.handleCallback(Handler.java:751)\r\n                                                                         at android.os.Handler.dispatchMessage(Handler.java:95)\r\n                                                                         at android.os.Looper.loop(Looper.java:154)\r\n                                                                         at android.app.ActivityThread.main(ActivityThread.java:6209)\r\n                                                                         at java.lang.reflect.Method.invoke(Native Method)\r\n                                                                         at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:865)\r\n03-15 08:16:54.573 13456-13456/org.tensorflow.demo E/AndroidRuntime:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:755)`\r\n\r\n\r\nAlso - I'm trying to build this all through Android Studio (ie I've not done a bazel build first)\r\n Anyone help?", "Hi, @rtjarvis. \r\nLooks like you are using some deprecated function in your code:\r\n`BatchNormWithGlobalNormalization is not available in GraphDef version 21. It has been removed in version 9. Use tf.nn.batch_normalization().`\r\nAlso I would suggest to use Bazel to build first. Looks like building with Android studio is still a bit experimental and need some fine tuning.", "@ArtsiomCh - thanks. I guessed that this might be the challenge but I've only just built my model by following the tutorial for retaining the inception classifier. I was expecting it to be compatible!\r\n\r\nIn the end I fixed this by running the optimise_for_inference.py script on my model which removes this operation and it works.\r\n\r\nBuild via Android Studio works fine for me.\r\n\r\n", "Somewhat along those lines, what function should be used in Python to output a custom model for the demo code to load the graph and weights? I am using tf.train.Saver(), which outputs an index, a meta and a checkpoint file. However, I'm not sure what to use in the demo android code in place of tensorflow_inception_graph.p?", "@SeanTyco \r\nLook into the script freeze_graph.py published by TensorFlow"]}]