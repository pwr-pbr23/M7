[{"number": 13592, "title": "Fixing the name of the disabled test.", "body": "", "comments": []}, {"number": 13591, "title": "GPU Allocation and Results Unexpected for Simple Test codes", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/ 6.0\r\n- **GPU model and memory**:two of Nvidia Quadro M4000\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI have two Nvidia Quadro M4000 GPUs; each has a 8G Memory. My regular memory has 64G space. \r\n\r\nI tested a simple GPU memory allocation for tensorflow and found that the allocation seems larger than the GPU memory. My test codes are as below:\r\n\r\n    GPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\r\n\r\n    # Number of float32 elements (4 bytes) that consume 7/8 of GPU memory\r\n    NUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)\r\n\r\n\tdef Test1():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt = tf.ones([2, NUM_ELEMS])\r\n\t  s = tf.reduce_sum(t)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tprint(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\r\n\r\n\tdef Test2():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt0 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU0's memory\r\n\t\ts0 = tf.reduce_sum(t0)\r\n\t  with tf.device(\"/gpu:1\"):\r\n\t\tt1 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU1's memory\r\n\t\ts1 = tf.reduce_sum(t1)\r\n\t  s = tf.add(s0, s1)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0,1'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tprint(sess.run(s))\r\n\r\nI expect the Test2() should work well but Test1() should fail as one GPU only has 8G memory. However, both test functions succeeded but the results are incorrect!\r\n\r\nNUM_ELEMS is about 1.879*e+9, and I expected that it should be half of the output of Test2() (Test1() should fail). \r\n\r\nHowever, I got Test1's output as 1.07374e+09, and Test2's output as 2.14748e+09.\r\n\r\nIt seems that Both Test1 and Test2 clip the number of elements to 2^30! As NUM_ELEMS is int32, even clipping should be clipped to 2^31 but not 2^30. \r\n\r\nIn Test1(), if I change the first dimension in (tf.ones([2, NUM_ELEMS])) from 2 to 15, the outputs are always 1.07374e+09; if the first dimension is no less than 16, it starts to crash showing out of GPU memory.\r\n\r\nMy questions are:\r\n\r\n 1. Why the GPU allocation did not crash for larger than 8G for single GPU?\r\n\r\n 2. Why the output results are clipped to 2^30 for single GPU and 2^31 for two GPUs\r\n\r\n 3. How could I get the correct outputs? ", "comments": ["To answer your first question, the GPU allocation did not crash, because the optimizer did constant folding, which is run on the CPU. Constant folding evaluated the `tf.reduce_sum` call. The following code should crash on TensorFlow 1.3:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\n\r\nGPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\r\n\r\n# Number of float32 elements (4 bytes) that consume 7/8 of GPU memory\r\nNUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)\r\n\r\ndef Test1():\r\n  with tf.device(\"/gpu:0\"):\r\n    t = tf.ones([2, NUM_ELEMS])\r\n  s = tf.reduce_sum(t)\r\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\r\n                          allow_soft_placement=False)\r\n  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n  # config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\r\n  with tf.Session(config=config) as sess:\r\n    print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\r\n\r\nTest1()\r\n```\r\n\r\nOn master, the `config.graph_options.rewrite_options.constant_folding` line must also be uncommented to cause a crash.\r\n\r\nAs for your second question, when I run `Test1` without disabling optimizations, I get 3.7581e+09, which is the correct answer. Perhaps you're getting a different result due to thread scheduling. @ekelsen do you know if reductions on the CPU consistency compute the reduce_sum of 3,758,096,384 float32 ones in a numerically stable way?", "If the datatype is float32, the summation on the cpu would not give the correct answer (you stop being able to add 1 to a floating point number at 2^24) and the CPU algorithm uses a very small number of accumulators.  If the datatype is int32 then it should give the correct result everytime.", "Thank you both for your help. After I change the float32 to int32, the number now overflows normally. \r\nAfter I added the tf.OptimizerOptions.L0 as @reedwm suggested, I encountered the following weird issues:\r\n\r\n1. Test1() started to show output message \" CUDA_ERROR_OUT_OF_MEMORY\" as soon as NUM_ELEMS is greater than int((4 * GPU_MEMORY_BYTES / 8) / 4); this is reasonable. But it did not crash and still outputs the overflowed value! If the NUM_ELEMS is smaller than that, no \"CUDA_ERROR_OUT_OF_MEMORY\" is shown.\r\n\r\n2. Test2() is exactly the same output as Test1()! When NUM_ELEMS is greater than int((4 * GPU_MEMORY_BYTES / 8) / 4), it shows \"CUDA_ERROR_OUT_OF_MEMORY\" but does not crash. When the NUM_ELEMS is greater than GPU_MEMORY_BYTES/4, it started to crash. This is really weird.\r\n\r\nI noticed that in both cases, the regular computer memory cost a lot; don't know whether this is related to the problem. I am confused about the outputs. What may cause these weird issues? Thank you.", "I cannot reproduce. For me, changing the type to `int32` does not cause out of memory errors, and outputs the result. I am running the code \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\n\r\nGPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\r\n\r\n# Number of float32 elements (4 bytes) that consume 7/8 of GPU memory\r\nNUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)\r\n\r\ndef Test1():\r\n  with tf.device(\"/gpu:0\"):\r\n    t = tf.ones([2, NUM_ELEMS], dtype=tf.int32)\r\n  s = tf.reduce_sum(t)\r\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\r\n                          allow_soft_placement=False)\r\n  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n  # config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\r\n  with tf.Session(config=config) as sess:\r\n    print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\r\n\r\nTest1()\r\n```\r\n\r\nThe reason there is no OOM error is that for `int32` types, many ops are done on the CPU with CPU memory, not GPU memory. For example, see the [`Const` op kernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/constant_op.cc#L133) (used by `tf.ones`) and the [`Sum` op kernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_sum.cc#L48) (used by `tf.reduce_sum`). See [this StackOverflow answer](https://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op) for more information.\r\n\r\nCan you post the code that causes the error message to show but still outputs a value? Also can you post the output when you run that code?", "@reedwm My previous test codes are exactly the same as yours, and it did not crash when NUM_ELEMS= int((7 * GPU_MEMORY_BYTES / 8) / 4). The codes' outputs are:\r\n\r\n\t2017-10-11 10:58:27.944911: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n\t2017-10-11 10:58:27.945590: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n\t2017-10-11 10:58:28.179010: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties:\r\n\tname: Quadro M4000\r\n\tmajor: 5 minor: 2 memoryClockRate (GHz) 0.7725\r\n\tpciBusID 0000:04:00.0\r\n\tTotal memory: 7.93GiB\r\n\tFree memory: 7.87GiB\r\n\t2017-10-11 10:58:28.179994: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0\r\n\t2017-10-11 10:58:28.180991: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y\r\n\t2017-10-11 10:58:28.181884: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M4000, pci bus id: 0000:04:00.0)\r\n\t2017-10-11 10:58:45.303652: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:955] failed to alloc 17179869184 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\r\n\t2017-10-11 10:58:45.304306: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 17179869184\r\n\t2017-10-11 10:58:52.164328: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:955] failed to alloc 15461881856 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\r\n\t2017-10-11 10:58:52.164914: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 15461881856\r\n\t2017-10-11 10:58:57.874112: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:955] failed to alloc 13915693056 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\r\n\t2017-10-11 10:58:57.874684: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 13915693056\r\n\t2017-10-11 10:59:03.462710: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:955] failed to alloc 12524123136 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\r\n\t2017-10-11 10:59:03.463291: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 12524123136\r\n\t2017-10-11 10:59:08.392877: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:955] failed to alloc 11271710720 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\r\n\t2017-10-11 10:59:08.393505: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 11271710720\r\n\t2017-10-11 10:59:12.736219: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:955] failed to alloc 10144539648 bytes on host: CUDA_ERROR_OUT_OF_MEMORY\r\n\t2017-10-11 10:59:12.736467: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow/core/common_runtime/gpu/pool_allocator.h:195] could not allocate pinned host memory of size: 10144539648\r\n\t-536870912\r\n\r\nThanks to your explanation, I think the int32 type question is clear now. Now the problem comes from only float32 issue.\r\n\r\nWhen I change the type to float32 (all other codes are fixed, I also copy them below), the single GPU version can only support NUM_ELEMS = int((1.8 * GPU_MEMORY_BYTES / 8) / 4) but will crash when NUM_ELEMS = int((2 * GPU_MEMORY_BYTES / 8) / 4). However, I check many times to make sure that my GPU usage before calling is very low as shown:\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 376.51                 Driver Version: 376.51                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro M4000       WDDM  | 0000:03:00.0      On |                  N/A |\r\n| 48%   45C    P8    21W / 120W |    772MiB /  8192MiB |      1%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Quadro M4000        TCC  | 0000:04:00.0     Off |                  N/A |\r\n| 49%   46C    P8    21W / 120W |      0MiB /  8121MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\nWhen using two GPUs, the codes run successfully when NUM_ELEMS = int((3 * GPU_MEMORY_BYTES / 8) / 4), but will crash when NUM_ELEMS = int((4 * GPU_MEMORY_BYTES / 8) / 4). \r\n\r\nThis phenomenon seems indicating that Tensorflow always need double size of the actual data in the memory. Is this a bug in Tensorflow? Is there any way to fix this issue? Thank you.\r\n\r\nMy test codes are:\r\n\r\n\tdef Test4():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt = tf.ones([2, NUM_ELEMS], tf.float32)\r\n\t  s = tf.reduce_sum(t)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tprint(sess.run(s))\r\n\r\n\tdef Test5():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt0 = tf.ones([NUM_ELEMS], tf.float32)\r\n\t\ts0 = tf.reduce_sum(t0)\r\n\t  with tf.device(\"/gpu:1\"):\r\n\t\tt1 = tf.ones([NUM_ELEMS], tf.float32)\r\n\t\ts1 = tf.reduce_sum(t1)\r\n\t  s = tf.add(s0, s1)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0,1'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tprint(sess.run([s0, s1, s]))\r\n\r\nThe error messages when crashing for single gpu (test4) are:\r\n\r\n\t2017-10-11 11:17:28.183277: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n\t2017-10-11 11:17:28.183968: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n\t2017-10-11 11:17:28.477173: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties:\r\n\tname: Quadro M4000\r\n\tmajor: 5 minor: 2 memoryClockRate (GHz) 0.7725\r\n\tpciBusID 0000:04:00.0\r\n\tTotal memory: 7.93GiB\r\n\tFree memory: 7.87GiB\r\n\t2017-10-11 11:17:28.478131: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0\r\n\t2017-10-11 11:17:28.478488: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y\r\n\t2017-10-11 11:17:28.478857: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M4000, pci bus id: 0000:04:00.0)\r\n\t2017-10-11 11:17:39.968193: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4B.  Current allocation summary follows.\r\n\t2017-10-11 11:17:39.968928: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (256):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.969705: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (512):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.970396: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (1024):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.971082: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (2048):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.971800: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (4096):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.972469: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (8192):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.973140: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (16384):        Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.973810: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (32768):        Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.974515: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (65536):        Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.975196: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (131072):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.975882: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (262144):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.976544: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (524288):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.977217: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (1048576):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.978161: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (2097152):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.978837: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (4194304):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.979513: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (8388608):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.980190: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (16777216):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.980896: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (33554432):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.985346: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (67108864):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.986052: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (134217728):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.986728: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (268435456):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:17:39.987393: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:660] Bin for 256B was 256B, Chunk State:\r\n\t2017-10-11 11:17:39.987850: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:678] Chunk at 0000001403A40000 of size 1280\r\n\t2017-10-11 11:17:39.988368: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:678] Chunk at 0000001403A40500 of size 8029879040\r\n\t2017-10-11 11:17:39.988810: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:693]      Summary of in-use Chunks by size:\r\n\t2017-10-11 11:17:39.989287: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.3KiB\r\n\t2017-10-11 11:17:39.989747: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:696] 1 Chunks of size 8029879040 totalling 7.48GiB\r\n\t2017-10-11 11:17:39.990202: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:700] Sum Total of in-use chunks: 7.48GiB\r\n\t2017-10-11 11:17:39.990649: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:702] Stats:\r\n\tLimit:                  8029880320\r\n\tInUse:                  8029880320\r\n\tMaxInUse:               8029880320\r\n\tNumAllocs:                       2\r\n\tMaxAllocSize:           8029879040\r\n\r\n\t2017-10-11 11:17:39.991605: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:277] ******************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n\t2017-10-11 11:17:39.992210: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[]\r\n\tTraceback (most recent call last):\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n\t\treturn fn(*args)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\r\n\t\tstatus, run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\contextlib.py\", line 88, in __exit__\r\n\t\tnext(self.gen)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n\t\tpywrap_tensorflow.TF_GetCode(status))\r\n\ttensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[]\r\n\t\t\t [[Node: Sum = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](ones, Const)]]\r\n\r\n\tDuring handling of the above exception, another exception occurred:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"Test_two_GPUs.py\", line 100, in <module>\r\n\t\ttf.app.run()\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n\t\t_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\t  File \"Test_two_GPUs.py\", line 96, in main\r\n\t\tTest4()\r\n\t  File \"Test_two_GPUs.py\", line 77, in Test4\r\n\t\tprint(sess.run(s))\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n\t\trun_metadata_ptr)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n\t\tfeed_dict_tensor, options, run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n\t\toptions, run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n\t\traise type(e)(node_def, op, message)\r\n\ttensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[]\r\n\t\t\t [[Node: Sum = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](ones, Const)]]\r\n\r\n\tCaused by op 'Sum', defined at:\r\n\t  File \"Test_two_GPUs.py\", line 100, in <module>\r\n\t\ttf.app.run()\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n\t\t_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\t  File \"Test_two_GPUs.py\", line 96, in main\r\n\t\tTest4()\r\n\t  File \"Test_two_GPUs.py\", line 72, in Test4\r\n\t\ts = tf.reduce_sum(t)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1278, in reduce_sum\r\n\t\tname=name)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2656, in _sum\r\n\t\tkeep_dims=keep_dims, name=name)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n\t\top_def=op_def)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n\t\toriginal_op=self._default_original_op, op_def=op_def)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n\t\tself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\tResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[]\r\n\t\t\t [[Node: Sum = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](ones, Const)]]\r\n\r\nThe error message for two GPUs (test5) are:\r\n\r\n\t2017-10-11 11:19:20.563267: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n\t2017-10-11 11:19:20.563990: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n\t2017-10-11 11:19:20.843673: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 0 with properties:\r\n\tname: Quadro M4000\r\n\tmajor: 5 minor: 2 memoryClockRate (GHz) 0.7725\r\n\tpciBusID 0000:04:00.0\r\n\tTotal memory: 7.93GiB\r\n\tFree memory: 7.87GiB\r\n\t2017-10-11 11:19:21.098788: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:523] A non-primary context 0000000EF7D9B640 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.\r\n\t2017-10-11 11:19:21.099703: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:955] Found device 1 with properties:\r\n\tname: Quadro M4000\r\n\tmajor: 5 minor: 2 memoryClockRate (GHz) 0.7725\r\n\tpciBusID 0000:03:00.0\r\n\tTotal memory: 8.00GiB\r\n\tFree memory: 6.71GiB\r\n\t2017-10-11 11:19:21.100621: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 0 and 1\r\n\t2017-10-11 11:19:21.101004: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0\r\n\t2017-10-11 11:19:21.101448: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:976] DMA: 0 1\r\n\t2017-10-11 11:19:21.101779: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 0:   Y N\r\n\t2017-10-11 11:19:21.102121: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:986] 1:   N Y\r\n\t2017-10-11 11:19:21.102487: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro M4000, pci bus id: 0000:04:00.0)\r\n\t2017-10-11 11:19:21.102945: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Quadro M4000, pci bus id: 0000:03:00.0)\r\n\t2017-10-11 11:19:34.108912: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:273] Allocator (GPU_1_bfc) ran out of memory trying to allocate 4B.  Current allocation summary follows.\r\n\t2017-10-11 11:19:34.109688: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (256):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.110365: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (512):  Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.111035: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (1024):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.111702: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (2048):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.112383: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (4096):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.113054: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (8192):         Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.113725: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (16384):        Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.114401: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (32768):        Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.115132: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (65536):        Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.115808: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (131072):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.116466: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (262144):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.117129: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (524288):       Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.117864: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (1048576):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.118552: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (2097152):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.119231: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (4194304):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.119925: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (8388608):      Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.120600: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (16777216):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.121283: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (33554432):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.121969: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (67108864):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.122698: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (134217728):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.123359: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:643] Bin (268435456):    Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n\t2017-10-11 11:19:34.124031: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:660] Bin for 256B was 256B, Chunk State:\r\n\t2017-10-11 11:19:34.124492: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:678] Chunk at 00000004E5E80000 of size 1280\r\n\t2017-10-11 11:19:34.125033: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:678] Chunk at 00000004E5E80500 of size 6843439616\r\n\t2017-10-11 11:19:34.125652: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:693]      Summary of in-use Chunks by size:\r\n\t2017-10-11 11:19:34.126135: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.3KiB\r\n\t2017-10-11 11:19:34.126603: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:696] 1 Chunks of size 6843439616 totalling 6.37GiB\r\n\t2017-10-11 11:19:34.127074: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:700] Sum Total of in-use chunks: 6.37GiB\r\n\t2017-10-11 11:19:34.127530: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:702] Stats:\r\n\tLimit:                  6843440988\r\n\tInUse:                  6843440896\r\n\tMaxInUse:               6843440896\r\n\tNumAllocs:                       2\r\n\tMaxAllocSize:           6843439616\r\n\r\n\t2017-10-11 11:19:34.128516: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:277] *******************************************************xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\r\n\t2017-10-11 11:19:34.129120: W C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Resource exhausted: OOM when allocating tensor with shape[]\r\n\tTraceback (most recent call last):\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1327, in _do_call\r\n\t\treturn fn(*args)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1306, in _run_fn\r\n\t\tstatus, run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\contextlib.py\", line 88, in __exit__\r\n\t\tnext(self.gen)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n\t\tpywrap_tensorflow.TF_GetCode(status))\r\n\ttensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[]\r\n\t\t\t [[Node: Sum_1 = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](ones_1, Const_1)]]\r\n\r\n\tDuring handling of the above exception, another exception occurred:\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"Test_two_GPUs.py\", line 100, in <module>\r\n\t\ttf.app.run()\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n\t\t_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\t  File \"Test_two_GPUs.py\", line 96, in main\r\n\t\tTest5()\r\n\t  File \"Test_two_GPUs.py\", line 91, in Test5\r\n\t\tprint(sess.run([s0, s1, s]))\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n\t\trun_metadata_ptr)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n\t\tfeed_dict_tensor, options, run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n\t\toptions, run_metadata)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n\t\traise type(e)(node_def, op, message)\r\n\ttensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[]\r\n\t\t\t [[Node: Sum_1 = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](ones_1, Const_1)]]\r\n\r\n\tCaused by op 'Sum_1', defined at:\r\n\t  File \"Test_two_GPUs.py\", line 100, in <module>\r\n\t\ttf.app.run()\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n\t\t_sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n\t  File \"Test_two_GPUs.py\", line 96, in main\r\n\t\tTest5()\r\n\t  File \"Test_two_GPUs.py\", line 85, in Test5\r\n\t\ts1 = tf.reduce_sum(t1)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1278, in reduce_sum\r\n\t\tname=name)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 2656, in _sum\r\n\t\tkeep_dims=keep_dims, name=name)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n\t\top_def=op_def)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n\t\toriginal_op=self._default_original_op, op_def=op_def)\r\n\t  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n\t\tself._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\tResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[]\r\n\t\t\t [[Node: Sum_1 = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](ones_1, Const_1)]]", "When allocating half the GPU memory, the BFC allocator will actually allocate all the memory. This is because of an internal design decision of the BFC allocator: When a chunk is found for an allocation, it will only be split if the allocation is less than half the chunk size. When you allocate 4 GB on an 8 GB GPU, you are allocating a bit more than half the free memory, so the chunk holding almost all the free memory will not be split.\r\n\r\nAs a result, allocating more than half the free GPU memory will often end up allocating all the GPU memory, causing future allocations to fail until memory is freed. `tf.reduce_sum(t)` requires 4 bytes to be allocated for the output, which causes the OOM error. In practice, it is rare that a single allocation allocates more than half the GPU memory.\r\n\r\nSee [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/bfc_allocator.cc#L304) for the logic of when to split a chunk.\r\n\r\nI have been assigned an internal bug to fix this for a few months, but unfortunately fixing this is not a priority right now. Fixing it would require doing a large amount of benchmarking to ensure no performance regressions from the change.\r\n\r\n", "@reedwm I made another test as below, where I removed `tf.reduce_sum(t)`. And my test verifies your conclusion. When no  `tf.reduce_sum(t)` is required, the ` NUM_ELEMS = int((3.5 * GPU_MEMORY_BYTES / 8) / 4 )` will run without any problem for single GPU, and the `NUM_ELEMS = int((6 * GPU_MEMORY_BYTES / 8) / 4 )` will run successfully for two GPUs.\r\n\r\nI do think this is a serious problem. When I am training my data, I can only use batch size of 1/4 or even 1/8 compared with people using other platforms. Batch size is so important for performance. If you could fix this memory allocation issue asap, it would be really great, especially for regular users, like me, who do not have so big GPUs as Google has. Thank you.\r\n\r\n    def Test7():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt = tf.ones([2, NUM_ELEMS], tf.float32)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tsess.run(t)\r\n\r\n\tdef Test8():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt = tf.ones([1, NUM_ELEMS], tf.float32)\r\n\t  with tf.device(\"/gpu:1\"):\r\n\t\tt1 = tf.ones([1, NUM_ELEMS], tf.float32)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tsess.run([t, t1])", "@ybsave what model and data are you using where you can only use a batch size of 1/4 compared to another platform? This issue only affects allocations that are multiple gigabytes, and most models have significantly smaller allocations than that.", "@reedwm This happened when I was testing the ImageNet training using Resnet101 model. I can only use a batch size no more than 48 images in my one 8G GPU.", "@ybsave What implementation of ResNet101 were you using? When I run ResNet101 using [tf_cnn_benchmarks.py](https://github.com/tensorflow/benchmarks/tree/master/scripts/tf_cnn_benchmarks), the maximum batch size I can run with on my 8GB GTX 1080 is 61. The command line I use is\r\n\r\n```\r\npython tf_cnn_benchmarks.py --model=resnet101 --batch_size=61\r\n```\r\n\r\nAlso, you mentioned other frameworks could be run with higher batch sizes. What are those frameworks?\r\n\r\nWhen I run ResNet1010 using a batch size of 62, the logs have\r\n\r\n```\r\n2017-10-12 12:49:09.307354: W tensorflow/stream_executor/cuda/cuda_dnn.cc:2223] \r\n2017-10-12 12:49:19.307640: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) ran out of memory trying to allocate 23.73MiB.  Current allocation summary follows.\r\n2017-10-12 12:49:19.307857: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (256): \tTotal Chunks: 467, Chunks in use: 225. 116.8KiB allocated for chunks. 56.2KiB in use in bin. 22.0KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.307889: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (512): \tTotal Chunks: 107, Chunks in use: 96. 53.5KiB allocated for chunks. 48.0KiB in use in bin. 48.0KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.307913: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (1024): \tTotal Chunks: 603, Chunks in use: 601. 603.2KiB allocated for chunks. 601.2KiB in use in bin. 601.0KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.307936: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (2048): \tTotal Chunks: 136, Chunks in use: 134. 273.5KiB allocated for chunks. 269.5KiB in use in bin. 268.0KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.307957: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (4096): \tTotal Chunks: 291, Chunks in use: 291. 1.14MiB allocated for chunks. 1.14MiB in use in bin. 1.14MiB client-requested in use in bin.\r\n2017-10-12 12:49:19.307978: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (8192): \tTotal Chunks: 47, Chunks in use: 46. 379.0KiB allocated for chunks. 371.0KiB in use in bin. 368.0KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308019: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (16384): \tTotal Chunks: 2, Chunks in use: 2. 32.0KiB allocated for chunks. 32.0KiB in use in bin. 32.0KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308038: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (32768): \tTotal Chunks: 1, Chunks in use: 1. 36.8KiB allocated for chunks. 36.8KiB in use in bin. 36.8KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308056: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (65536): \tTotal Chunks: 13, Chunks in use: 13. 897.2KiB allocated for chunks. 897.2KiB in use in bin. 804.8KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308074: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (131072): \tTotal Chunks: 7, Chunks in use: 7. 1.06MiB allocated for chunks. 1.06MiB in use in bin. 976.0KiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308092: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (262144): \tTotal Chunks: 15, Chunks in use: 15. 3.75MiB allocated for chunks. 3.75MiB in use in bin. 3.64MiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308109: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (524288): \tTotal Chunks: 13, Chunks in use: 12. 7.55MiB allocated for chunks. 6.56MiB in use in bin. 6.50MiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308127: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (1048576): \tTotal Chunks: 90, Chunks in use: 90. 90.25MiB allocated for chunks. 90.25MiB in use in bin. 90.00MiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308145: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (2097152): \tTotal Chunks: 48, Chunks in use: 48. 109.25MiB allocated for chunks. 109.25MiB in use in bin. 107.00MiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308164: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (4194304): \tTotal Chunks: 25, Chunks in use: 25. 129.02MiB allocated for chunks. 129.02MiB in use in bin. 121.59MiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308181: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (8388608): \tTotal Chunks: 102, Chunks in use: 101. 1.20GiB allocated for chunks. 1.19GiB in use in bin. 1.14GiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308200: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (16777216): \tTotal Chunks: 20, Chunks in use: 20. 481.38MiB allocated for chunks. 481.38MiB in use in bin. 462.82MiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308217: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (33554432): \tTotal Chunks: 65, Chunks in use: 65. 2.95GiB allocated for chunks. 2.95GiB in use in bin. 2.90GiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308236: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (67108864): \tTotal Chunks: 9, Chunks in use: 9. 854.44MiB allocated for chunks. 854.44MiB in use in bin. 854.44MiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308253: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (134217728): \tTotal Chunks: 9, Chunks in use: 9. 1.62GiB allocated for chunks. 1.62GiB in use in bin. 1.62GiB client-requested in use in bin.\r\n2017-10-12 12:49:19.308270: I tensorflow/core/common_runtime/bfc_allocator.cc:627] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n```\r\n\r\nSince for each bin, the \"client-requested in use in bin\" is almost as high as \"use in bin\", this issue of only using half a chunk is not occurring.", "@reedwm I used the Resnet implementation at https://github.com/tensorflow/models/blob/master/official/resnet/resnet_model.py; I made a few modifications by using slim to simplify the model definitions.\r\n\r\nAnother person told me that he used Caffe on Resnet 101 for ImageNet with about batch size up to 100. But I am not aware what implementations he used. Sorry about that.\r\n\r\nFrom your posted output, it seems that the Tensorflow GPU memory allocations are performed in several sub-steps by allocating 2*previous memory one by one. So, it means as long as the single biggest allocation, e.g. 1.62G in your post, is smaller than half of the GPU, e.g., 4G, the problem should not appear, no matter of the total allocation so far, right?", "Yes, the problem will almost certainly not occur if the biggest allocation is less than half the GPU. It can theoretically occur in any case where an allocation is about half the size of the chunk its assigned to, but such cases are rare.\r\n\r\nNote all models I know of will not allocate more than half the memory it uses in a single allocation. So I don't think this problem occurs in practice, but feel free to update this bug if you find out it does.\r\n\r\nIf you find out the Caffe Resnet50 implementation and find that it supports higher batch sizes then the Resnet implementation on the same hardware, update this issue. I will update this issue when I fix the issue involving a whole chunk being allocated if just half of it is needed.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "The allocation issue has been fixed (although not by me). Closing this issue.", "@reedwm In which commit was this fixed ? I'm still noticing weird behavior regarding allocation. For instance, it's impossible to allocate a tensor filled with zeros that's above half the total GPU memory. I have a GPU with 12GB of vram yet I can't allocate more than ~5.5GB. Does not make any sense.\r\n\r\nSee https://github.com/tensorflow/tensorflow/issues/18380#issuecomment-380110126", "This was fixed in 9fe7c29605a5ee1519cceba8e67a6d8413444fac. "]}, {"number": 13590, "title": "Tensorflow Session Connects to Multiple Targets", "body": "I posted a question on [SO](https://stackoverflow.com/questions/46626879/can-one-tensorflow-session-connect-to-two-targets-at-the-same-time), where I wonder if a Tensorflow Session could connect to two (or more than two) targets at the same time. It looks the feature is not supported yet. Can we make a feature request on it?", "comments": ["What's the semantics of supporting multiple masters for tf.Session? Which master would execute `sess.run`? ", "@yaroslavvb The point is to support one session to connect multiple server targets (or masters), and the goal is to make some of tensor computations run on one server, and others run on different servers. `sess.run` may specify which master (or target) to connect to. I wonder if it is possible.", "I think it's a little vague for a feature request. Lets move it to SO"]}, {"number": 13589, "title": "tensorflow batch norm used when rank = 4?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: pip install \r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: Titan X Pascal\r\n- **Exact command to reproduce**: See code snippet\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI am using slim.batch_norm from layers and trying to understand the code flow in my use case. It looks to me like the logic that decides whether to use _fused_batch_norm() or the base class will only use the _fused_batch_norm() in my case if the input rank is 2. The code description sounds like it should also be used if rank is 4 and the function itself (_fused_batch_norm()) supports rank of 4, but the logic seems to prevent calling it. \r\n\r\nIf my input is rank 4, it looks like the code will use the fused implementation in normalization_layers.BatchNormalization Is my understanding of the logic correct?\r\n\r\nIs this the expected and proper behavior? I am wondering if the the condition rank==2 should actually be rank in [2,4]? If the latter is correct, then this would be a potential bug. If the original is correct, then why have rank in [2,4] for determining feature_supported ?\r\n\r\nIssue posted to stack overflow and cited as a bug to report: [which tensorflow batch norm..](https://stackoverflow.com/questions/44809342/which-tensorflow-batch-norm-code-is-used-when-input-rank-is-4/46620919#46620919)\r\n\r\n### Source code / logs\r\n```python\r\n  # Only use _fused_batch_norm (1) if fused is set True or if it is\r\n  # possible to use (currently it doesn't support batch weights,\r\n  # renorm, and the case when rank is neither 2 nor 4),\r\n  # and (2) if used with zero_debias_moving_mean, or an input shape of rank 2,\r\n  # or non-default updates_collections (not implemented in\r\n  # normalization_layers.BatchNormalization yet); otherwise use the fused\r\n  # implementation in normalization_layers.BatchNormalization.\r\n  inputs = ops.convert_to_tensor(inputs)\r\n  rank = inputs.get_shape().ndims\r\n  feature_supported = batch_weights is None and not renorm and rank in [2, 4]\r\n  possible_to_fuse = fused is None and feature_supported\r\n  if (fused or possible_to_fuse) and (\r\n      zero_debias_moving_mean or rank == 2 or\r\n      updates_collections is not ops.GraphKeys.UPDATE_OPS):\r\n      return _fused_batch_norm(...)\r\n```", "comments": ["You pasted a slightly older version of the chunk of code. The code is currently\r\n\r\n```python\r\n  # Only use _fused_batch_norm if all of the following three\r\n  # conditions are true:\r\n  # (1) fused is set True;\r\n  # (2) it is possible to use (currently it doesn't support batch weights,\r\n  #   renorm, and the case when rank is neither 2 nor 4);\r\n  # (3) it is used with zero_debias_moving_mean, or an input shape of rank 2,\r\n  #   or non-default updates_collections (not implemented in\r\n  #   normalization_layers.BatchNormalization yet); otherwise use the fused\r\n  #   implementation in normalization_layers.BatchNormalization.\r\n  inputs = ops.convert_to_tensor(inputs)\r\n  rank = inputs.get_shape().ndims\r\n  possible_to_fuse = batch_weights is None and not renorm and rank in [2, 4]\r\n  if fused and possible_to_fuse and (\r\n      zero_debias_moving_mean or rank == 2 or\r\n      updates_collections is not ops.GraphKeys.UPDATE_OPS):\r\n```\r\n\r\nNow, the comment and the code match. Note the rank can be 4, as long as `zero_debias_moving_mean` is True or if non-default update collections are used."]}, {"number": 13588, "title": "Unable to open table file. Data loss: file is too short to be an sstable", "body": "I am trying simple save and restore operation in tensorflow. Here is link to Jupyter Notebook.[https://github.com/BrazilForever11/tf_error/blob/master/reproducible%20tf%20saving%20restoring%20error.ipynb](url)\r\n\r\n\r\n\r\nThe following line generates error:\r\n`new_all_saver.restore(sess, data_path)`\r\n\r\nHere is error message:\r\n\r\n```\r\nINFO:tensorflow:Restoring parameters from C:\\tmp\\tmp\\.data-00000-of-00001\r\n---------------------------------------------------------------------------\r\nDataLossError                             Traceback (most recent call last)\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1326     try:\r\n-> 1327       return fn(*args)\r\n   1328     except errors.OpError as e:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)\r\n   1305                                    feed_dict, fetch_list, target_list,\r\n-> 1306                                    status, run_metadata)\r\n   1307 \r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\contextlib.py in __exit__(self, type, value, traceback)\r\n     65             try:\r\n---> 66                 next(self.gen)\r\n     67             except StopIteration:\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py in raise_exception_on_not_ok_status()\r\n    465           compat.as_text(pywrap_tensorflow.TF_Message(status)),\r\n--> 466           pywrap_tensorflow.TF_GetCode(status))\r\n    467   finally:\r\n\r\nDataLossError: Unable to open table file C:\\tmp\\tmp\\.data-00000-of-00001: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\r\n\t [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nDataLossError                             Traceback (most recent call last)\r\n<ipython-input-2-4f763d68be26> in <module>()\r\n      6 \r\n      7 sess=tf.Session()\r\n----> 8 new_all_saver.restore(sess, data_path)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py in restore(self, sess, save_path)\r\n   1558     logging.info(\"Restoring parameters from %s\", save_path)\r\n   1559     sess.run(self.saver_def.restore_op_name,\r\n-> 1560              {self.saver_def.filename_tensor_name: save_path})\r\n   1561 \r\n   1562   @staticmethod\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    893     try:\r\n    894       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 895                          run_metadata_ptr)\r\n    896       if run_metadata:\r\n    897         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1122     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1123       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1124                              feed_dict_tensor, options, run_metadata)\r\n   1125     else:\r\n   1126       results = []\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1319     if handle is None:\r\n   1320       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1321                            options, run_metadata)\r\n   1322     else:\r\n   1323       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py in _do_call(self, fn, *args)\r\n   1338         except KeyError:\r\n   1339           pass\r\n-> 1340       raise type(e)(node_def, op, message)\r\n   1341 \r\n   1342   def _extend_graph(self):\r\n\r\nDataLossError: Unable to open table file C:\\tmp\\tmp\\.data-00000-of-00001: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\r\n\t [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\r\n\r\nCaused by op 'save/RestoreV2_1', defined at:\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\r\n    ioloop.IOLoop.instance().start()\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\r\n    super(ZMQIOLoop, self).start()\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\r\n    handler_func(fd_obj, events)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\r\n    self._handle_recv()\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\r\n    self._run_callback(callback, msg)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\r\n    callback(*args, **kwargs)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\r\n    return self.dispatch_shell(stream, msg)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\r\n    handler(stream, idents, msg)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\r\n    user_expressions, allow_stdin)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-4f763d68be26>\", line 5, in <module>\r\n    new_all_saver = tf.train.import_meta_graph(meta_path)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1698, in import_meta_graph\r\n    **kwargs)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\", line 656, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 313, in import_graph_def\r\n    op_def=op_def)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"C:\\Users\\some_user\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): Unable to open table file C:\\tmp\\tmp\\.data-00000-of-00001: Data loss: file is too short to be an sstable: perhaps your file is in a different file format and you need to use a different restore operator?\r\n\t [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]\r\n\r\n```\r\nHere link to data file [https://github.com/BrazilForever11/tf_error/blob/master/.data-00000-of-00001](url)\r\n\r\nIt is not clear to me what is wrong. Error is reproducible.\r\n\r\nHere some details of my setup:\r\nOS: Win 7, 64 bit\r\nTensorflow installed through anaconda enviroment\r\nPython 3.5\r\nTensorflow version 1.3.0\r\nNo GPU, \r\n2 cores CPU\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13587, "title": "TF fails to build on PowerPC. Issues with BoringSSL.", "body": "\r\n------------------------\r\nBoringSSL doesn't seem to have their own issue board. So I'm guessing here's the closest alternative.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from (source or binary)**: source (github master, 10c871ed92a1d9b36c5e2e3a674d5812c67e82a1)\r\n- **TensorFlow version (use command below)**:  (doesn't compile)\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.6.1\r\n- **CUDA/cuDNN version**: (not relevant)\r\n- **GPU model and memory**: Nvidia K40\r\n- **Exact command to reproduce**: ```bazel --host_jvm_args=-XX:+IgnoreUnrecognizedVMOptions build -c opt --config=cuda  //tensorflow/tools/pip_package:build_pip_package --verbose_failures```\r\n\r\n\r\n### Describe the problem\r\nLatest TF master branch does not build on PowerPC. \r\nProblem seems to be with BoringSSL.\r\nThis [link](https://github.com/google/boringssl/blob/f21650709a6f76e829ddcc77fe221c9d6a5c12de/crypto/fipsmodule/bcm.c#L91) points to a file in BoringSSL that has the following inclusions:\r\n```\r\n#include \"sha/sha1-altivec.c\"\r\n#include \"sha/sha1.c\"\r\n```\r\n\r\nBoth files included seem to have their own definition of `sha1_block_data_order` and both are enabled on PowerPC. This seems to cause GCC 4.8.5 to freak out (did not test other versions of gcc). My temporary fix is to remove the altivec version of sha1 and it works. Hopefully someone working on BoringSSL can take a look.\r\n\r\n\r\nBelow is the exact error message:\r\n```\r\nERROR: /gsa/yktgsa-h2/05/tjin/.cache/bazel/_bazel_tjin/5ebd35a31f2e08b1acf4a588141b13f1/external/boringssl/BUILD:118:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n  (cd /gsa/yktgsa-h2/05/tjin/.cache/bazel/_bazel_tjin/5ebd35a31f2e08b1acf4a588141b13f1/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-8.0 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64: \\\r\n    PATH=/usr/lib/jvm/java-8-openjdk-ppc64el/bin:/localhd/tjin/tensorflow_latest/bin:/gsa/yktgsa/home/t/j/tjin/anaconda2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/local/cuda/bin:/localhd/tjin/tensorflow_latest/bazel/output:/usr/local/cuda/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/localhd/tjin/tensorflow_latest/bin/python \\\r\n    PYTHON_LIB_PATH=/localhd/tjin/tensorflow_latest/lib/python2.7/site-packages \\\r\n    TF_CUDA_CLANG=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2 \\\r\n    TF_CUDA_VERSION=8.0 \\\r\n    TF_CUDNN_VERSION=5.1.5 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/local_linux-opt/bin/external/boringssl/_objs/crypto/external/boringssl/src/crypto/fipsmodule/bcm.pic.d -fPIC -iquote external/boringssl -iquote bazel-out/local_linux-opt/genfiles/external/boringssl -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -isystem external/boringssl/src/include -isystem bazel-out/local_linux-opt/genfiles/external/boringssl/src/include -isystem external/bazel_tools/tools/cpp/gcc3 -Wa,--noexecstack '-D_XOPEN_SOURCE=700' -Wall -Werror '-Wformat=2' -Wsign-compare -Wmissing-field-initializers -Wwrite-strings -Wshadow -fno-common -DOPENSSL_NO_ASM '-std=c11' -Wmissing-prototypes -Wold-style-definition -Wstrict-prototypes -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers -c external/boringssl/src/crypto/fipsmodule/bcm.c -o bazel-out/local_linux-opt/bin/external/boringssl/_objs/crypto/external/boringssl/src/crypto/fipsmodule/bcm.pic.o).\r\nIn file included from external/boringssl/src/crypto/fipsmodule/bcm.c:92:0:\r\nexternal/boringssl/src/crypto/fipsmodule/sha/sha1.c:125:6: error: static declaration of 'sha1_block_data_order' follows non-static declaration\r\n void sha1_block_data_order(uint32_t *state, const uint8_t *data, size_t num);\r\n      ^\r\nIn file included from external/boringssl/src/crypto/fipsmodule/bcm.c:91:0:\r\nexternal/boringssl/src/crypto/fipsmodule/sha/sha1-altivec.c:190:6: note: previous definition of 'sha1_block_data_order' was here\r\n void sha1_block_data_order(uint32_t *state, const uint8_t *data, size_t num) {\r\n      ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 311.401s, Critical Path: 8.71s\r\n```", "comments": ["@gunan can you comment? Do we support PowerPC?", "There is no official support for powerPC architecture for TensorFlow.\r\nYou will need to check with the community or reach out to stackoverflow for support.\r\n\r\nBefore I leave, two things that may help:\r\n- @nayana-ibm how similar is powerPC to s390x?\r\n- Boringssl should only be needed for GCS support or a very limited number of kernels. You could try modifying our build files and cut boringssl off and try building again. But no guarantees.", "@gunan, I understand there's no official support for PowerPC for TF. In fact I'm trying to make sure that people can use Tensorflow on PowerPC. Also the problem is with OpenSSL.\r\n\r\nMore importantly, looking at the history of the relevant OpenSSL file, especially [here](https://github.com/google/boringssl/commit/5f04b6bc3a46662323b67be31bc9a28be38b2844); I think the problem is not that there is no support for PPC; the problem is the support seems only half-way there. Tensorflow with BoringSSL that is totally without support for PPC works just fine (which is what I did by removing the altivec version of sha1). Since a PR to the TF repository will unlikely be of any help, are you suggesting that the best course of action is to debug OpenSSL and submit a PR to OpenSSL repository?", "@Nayana-ibm already tried to submit fixes to boringSSL to support big-endian systems. Boring SSL team firmly refused and said they will never support big endian systems.\r\nIf you are working on PowerPC running in little-endian systems, even that is questionable for boring ssl team to accept.\r\nBut this question is better asked to them. At the moment, on TF side all you can do is not build the ops that require openssl, as I said above. ", "@gunan, thanks for your information. I use a little-endian version of PowerPC. Optimally I would like TF to work out of the box and it has more or less been the case in the past, it would be a pity that we can no longer do it because of a less-than-often-used part. Can you tell me what's the best way to reach the BoringSSL team?", "All I have about boringssl is here:\nhttps://boringssl.googlesource.com/boringssl/\n\nOn Mon, Oct 9, 2017 at 11:14 AM, Tian Jin <notifications@github.com> wrote:\n\n> @gunan <https://github.com/gunan>, thanks for your information. Optimally\n> I would like TF to work out of the box and it has more or less been the\n> case in the past, it would be a pity that we can no longer do it because of\n> a less-than-often-used part. Can you tell me what's the best way to reach\n> the BoringSSL team?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13587#issuecomment-335241938>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOf6_ahnJbReq7w4__rsnTdumg0Ygks5sqmKfgaJpZM4Pyu64>\n> .\n>\n", "This sounds like the issue we fixed here:\r\nhttps://boringssl.googlesource.com/boringssl/+/e7136a978f6f9d5b3f392b661e573e0793965f97\r\n\r\nThe issue affected the ppc64le + no-asm build.", "@davidben looks like that's it! It appears in the master-with-bazel branch. Should I do a PR and update the boringssl archive [here](https://github.com/tensorflow/tensorflow/blob/a2d9b3bf5f9e96bf459074d079b01e1c74b25afa/tensorflow/workspace.bzl#L593)?", "Sounds reasonable (though I can't speak for Tensorflow).", "Please feel free to send a pill request to update the boringssl library we\ndepend on.\nIf you send it to me for review, I can also make sure the mirror is set up\nbefore merging the pull request.\n\nOn Tue, Oct 10, 2017 at 9:14 AM, David Benjamin <notifications@github.com>\nwrote:\n\n> Sounds reasonable (though I can't speak for Tensorflow).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13587#issuecomment-335525105>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOTId6ejjzS-25VIgsc4nLzIO1WNLks5sq5fsgaJpZM4Pyu64>\n> .\n>\n", "@tjingrant, I have the same boringssl error on ppc64le Redhat 7.4 (AC922) with python3.  Can you pls let me know how this issue was addressed ?   IBM's technical preview edition for TF1.4 is for python2 only, so it does not help much."]}, {"number": 13586, "title": "update from origin", "body": "get the latest commits from original tensorflow", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "sorry, please ignore the request, it is a wrong operation.\r\nFrank\r\n\u53d1\u81ea\u6211\u7684 iPhone\r\n\r\n\u5728 2017\u5e7410\u67089\u65e5\uff0c22:38\uff0cTensorflow Jenkins <notifications@github.com<mailto:notifications@github.com>> \u5199\u9053\uff1a\r\n\r\n\r\nCan one of the admins verify this patch?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/13586#issuecomment-335176481>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AIOWy-yXxgPds4_ActnoTCAWfve58Vgkks5sqi9YgaJpZM4PylE5>.\r\n"]}, {"number": 13585, "title": "AttributeError: 'SummaryMetadata' object has no attribute 'display_name' ; windows10 (64bit), install tensorflow (1.3.0rc0) in Anaconda python36  with tensorflow-tensorboard (0.1.7) on win10", "body": "My laptop OS is windows10 (64bit), install tensorflow (1.3.0rc0) in Anaconda python36  with tensorflow-tensorboard (0.1.7) in it. when run command \" tensorboard --logdir=\"path//to//logs\", met below error.\r\n\r\n```\r\nException in thread Reloader:\r\nTraceback (most recent call last):\r\n  File \"c:\\soft_app\\anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"c:\\soft_app\\anaconda3\\lib\\threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"c:\\soft_app\\anaconda3\\lib\\site-packages\\tensorboard\\backend\\application.py\", line 327, in _reload_forever\r\n    reload_multiplexer(multiplexer, path_to_run)\r\n  File \"c:\\soft_app\\anaconda3\\lib\\site-packages\\tensorboard\\backend\\application.py\", line 301, in reload_multiplexer\r\n    multiplexer.Reload()\r\n  File \"c:\\soft_app\\anaconda3\\lib\\site-packages\\tensorboard\\backend\\event_processing\\plugin_event_multiplexer.py\", line 195, in Reload\r\n    accumulator.Reload()\r\n  File \"c:\\soft_app\\anaconda3\\lib\\site-packages\\tensorboard\\backend\\event_processing\\plugin_event_accumulator.py\", line 189, in Reload\r\n    self._ProcessEvent(event)\r\n  File \"c:\\soft_app\\anaconda3\\lib\\site-packages\\tensorboard\\backend\\event_processing\\plugin_event_accumulator.py\", line 335, in _ProcessEvent\r\n    value = data_compat.migrate_value(value)\r\n  File \"c:\\soft_app\\anaconda3\\lib\\site-packages\\tensorboard\\data_compat.py\", line 57, in migrate_value\r\n    return handler(value) if handler else value\r\n  File \"c:\\soft_app\\anaconda3\\lib\\site-packages\\tensorboard\\data_compat.py\", line 69, in _migrate_histogram_value\r\n    display_name=value.metadata.display_name or value.tag,\r\nAttributeError: 'SummaryMetadata' object has no attribute 'display_name'\r\n```", "comments": ["Please file this issue in the TensorBoard repository [here](https://github.com/tensorflow/tensorboard)."]}, {"number": 13584, "title": "AttributeError: 'SummaryMetadata' object has no attribute 'display_name' ( OS:windows, Python36, tensorflow (1.3.0rc0) tensorflow-tensorboard (0.1.7)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@SusieZ Sorry, I don't understand the question.\r\n\r\nPlease follow the instructions in the `New issue` template.\r\n\r\nAnd please note: if this is a tensorboard question, please file it here:\r\nhttps://github.com/tensorflow/tensorboard/issues"]}, {"number": 13583, "title": "InvalidArgumentError : Expected image (JPEG, PNG, or GIF), got unknown format starting with '255'", "body": "### System information\r\n-  **OS Platform and Distribution**  :Linux Ubuntu 16.04\r\n-  **TensorFlow installed from**  :binary\r\n-  **TensorFlow version**  :1.1.0\r\n-  **Python version** : 2.7.12\r\n\r\n\r\n### Describe the problem\r\ntrying to feed a model an image encoded in string as the model require that as an input string\r\n```\r\nsignature_def {\r\n  key: \"serving_default\"\r\n  value {\r\n    inputs {\r\n      key: \"image_bytes\"\r\n      value {\r\n        name: \"Placeholder:0\"\r\n        dtype: DT_STRING\r\n        tensor_shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n        }\r\n      }\r\n    }\r\n    inputs {\r\n      key: \"key\"\r\n      value {\r\n        name: \"Placeholder_1:0\"\r\n        dtype: DT_STRING\r\n        tensor_shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n        }\r\n      }\r\n    }\r\n    outputs {\r\n      key: \"key\"\r\n      value {\r\n        name: \"Identity:0\"\r\n        dtype: DT_STRING\r\n        tensor_shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n        }\r\n      }\r\n    }\r\n    outputs {\r\n      key: \"prediction\"\r\n      value {\r\n        name: \"ArgMax:0\"\r\n        dtype: DT_INT64\r\n        tensor_shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n        }\r\n      }\r\n    }\r\n    outputs {\r\n      key: \"scores\"\r\n      value {\r\n        name: \"final_ops/softmax:0\"\r\n        dtype: DT_FLOAT\r\n        tensor_shape {\r\n          dim {\r\n            size: -1\r\n          }\r\n          dim {\r\n            size: 3\r\n          }\r\n        }\r\n      }\r\n    }\r\n    method_name: \"tensorflow/serving/predict\"\r\n  }\r\n}\r\n```\r\nHere is the code :-\r\n```\r\ndef load_image( infilename ) :\r\n    img = Image.open( infilename )\r\n    img.load()\r\n    data = np.asarray( img, dtype=\"string\" )\r\n    return data\r\n\r\nexport_dir = '.'\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n\tmodel = tf.saved_model.loader.load(sess, ['serve'], export_dir)\r\n\tinput_dict, output_dict =_signature_def_to_tensors(model.signature_def['serving_default'])\r\n\tout = sess.run(output_dict, feed_dict={input_dict['image_bytes']: load_image(\"fullsize.jpeg\").flatten()})\r\n\tprint(input_dict)\r\n```\r\nError returns  is :\r\n```\r\nInvalidArgumentError (see above for traceback): Expected image (JPEG, PNG, or GIF), got unknown format starting with '255'\r\n\t [[Node: map/while/DecodeJpeg = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](map/while/TensorArrayReadV3)]]\r\n```\r\n\r\nhow i can solve that , any help ", "comments": ["The error message suggests that the input isn't a valid JPEG image.\r\nAre you sure that the image you're feeding (which seems to be a 3 byte file) is a valid JPEG image?", "I feed the Image using `np.array `and also tried to feed it with` tf.image.decode_jpeg` getting an error in both cases . when i am using `tf.image.decode_jpeg` as \r\n```\r\nimage_contents = tf.read_file(\"fullsize.jpeg\")\r\nimage = tf.image.decode_jpeg(image_contents, channels=3)\r\n```\r\ni get this error \r\n```\r\n    raise TypeError('The value of a feed cannot be a tf.Tensor object. '\r\nTypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.\r\n```\r\nhow to validate if the image is valid or not and also the best way to feed the image . \r\n\r\nThanks In Advance \r\n\r\n", "Sorry , **Edited the Error Message** i tried it again with the same values but represent this error , seems to be problem in the encoding \r\nusing this library to load the image \r\n\r\n> from PIL import Image\r\n\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said, a few things to note:\r\n\r\n- `tf.image.decode_jpeg` expects the encoded-JPEG bytes (i.e., the contents of the file). The use of `PIL` and in particular `PIL.Image.load()` suggests that the numpy array you're creating consists of the decoded contents (decoded by `PIL`). Which is why the `sess.run()` call is failing as it expects an encoded JPEG but you're providing it a decoded one.\r\n\r\n- To provide the raw contents of the file as a feed, you'd want to do something like so:\r\n\r\n```python\r\ndef load_image(filename):\r\n  with open('fullsize.jpeg') as f:\r\n    return np.array(f.read())\r\n```\r\nWhich will create the numpy array of the encoded JPEG bytes that you can feed to your session.\r\n\r\nHope that helps.", "@engahmed1190 ,  I meet a silimar question with you when i train models on google TensorFlow Object Detection API, May i ask if the question has been solved? what's the problem and how to solved? thank you!", "I too get an issue similarly,Someone please help me to resolve this.\r\nThanks in advance!\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expected image (JPEG, PNG, or GIF), got unknown format starting with 'RIFF\\342:\\000\\000WEBPVP8 '\r\n\t [[Node: DecodeJpeg_1 = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DecodeJPGInput_0_0)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 370, in create_bottleneck_file\r\n    resized_input_tensor, bottleneck_tensor)\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 305, in run_bottleneck_on_image\r\n    {image_data_tensor: image_data})\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expected image (JPEG, PNG, or GIF), got unknown format starting with 'RIFF\\342:\\000\\000WEBPVP8 '\r\n\t [[Node: DecodeJpeg_1 = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DecodeJPGInput_0_0)]]\r\n\r\nCaused by op 'DecodeJpeg_1', defined at:\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 1009, in main\r\n    model_info['input_std'])\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 955, in add_jpeg_decoding\r\n    decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/ops/gen_image_ops.py\", line 953, in decode_jpeg\r\n    name=name)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Expected image (JPEG, PNG, or GIF), got unknown format starting with 'RIFF\\342:\\000\\000WEBPVP8 '\r\n\t [[Node: DecodeJpeg_1 = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DecodeJPGInput_0_0)]]\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 1025, in main\r\n    bottleneck_tensor, FLAGS.architecture)\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 476, in cache_bottlenecks\r\n    resized_input_tensor, bottleneck_tensor, architecture)\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 418, in get_or_create_bottleneck\r\n    bottleneck_tensor)\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 373, in create_bottleneck_file\r\n    str(e)))\r\nRuntimeError: Error during processing file tf_files/data/Organic Waste/organic89.jpg (Expected image (JPEG, PNG, or GIF), got unknown format starting with 'RIFF\\342:\\000\\000WEBPVP8 '\r\n\t [[Node: DecodeJpeg_1 = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DecodeJPGInput_0_0)]]\r\n\r\nCaused by op 'DecodeJpeg_1', defined at:\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 1326, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 1009, in main\r\n    model_info['input_std'])\r\n  File \"/home/karthi/tensorflow-for-poets-2/scripts/retrain.py\", line 955, in add_jpeg_decoding\r\n    decoded_image = tf.image.decode_jpeg(jpeg_data, channels=input_depth)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/ops/gen_image_ops.py\", line 953, in decode_jpeg\r\n    name=name)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/home/karthi/tf1/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Expected image (JPEG, PNG, or GIF), got unknown format starting with 'RIFF\\342:\\000\\000WEBPVP8 '\r\n\t [[Node: DecodeJpeg_1 = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method=\"\", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_DecodeJPGInput_0_0)]]\r\n)\r\n\r\n", "@keyankarthisdk , maybe the reason is some picture is not the JPEG format. You can use tf.image.decode_jpeg() to test which picture is not the right format for tensorflow and then discard. I use this way solve the problem. ", "Thank you so much\n\nOn Mon, 11 Feb 2019, 3:22 pm clovking <notifications@github.com wrote:\n\n> @keyankarthisdk <https://github.com/keyankarthisdk> , maybe the reason is\n> some picture is not the JPEG format. You can use tf.image.decode_jpeg() to\n> test which picture is not the right format for tensorflow and then discard.\n> I use this way solve the problem.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13583#issuecomment-462267255>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AoFJhC4L3sn7W6llk8rtJD----IaO94Dks5vMT11gaJpZM4PyhJ8>\n> .\n>\n", "> @keyankarthisdk , maybe the reason is some picture is not the JPEG format. You can use tf.image.decode_jpeg() to test which picture is not the right format for tensorflow and then discard. I use this way solve the problem.\r\n\r\nHello,\r\nI'am currently working with Application API and where should i Insert the decode_jpeg function my code is as follows\r\n```\r\nbase_model=tf.keras.applications.EfficientNetB2(include_top=False, input_shape=(224, 224, 3))\r\nbase_model.trainable=False\r\ninputs=tf.keras.layers.Input(shape=(224, 224, 3))\r\nx=tf.keras.layers.GlobalAveragePooling2D()(inputs)\r\noutputs=tf.keras.layers.Dense(2, activation='softmax')(x)\r\nmodel_0=tf.keras.Model(inputs, outputs)\r\n```\r\n```\r\nmodel_0.compile(loss=tf.keras.losses.binary_crossentropy,\r\n                optimizer=tf.keras.optimizers.Adam(),\r\n                metrics=['accuracy'])\r\nmodel_0.fit(data_gen, epochs=5)\r\n```\r\nAnd when I executed this code I was getting this error\r\n```\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-101-6769bac1fcee> in <module>()\r\n      2                 optimizer=tf.keras.optimizers.Adam(),\r\n      3                 metrics=['accuracy'])\r\n----> 4 model_0.fit(data_gen, epochs=5)\r\n\r\n6 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nInvalidArgumentError: 2 root error(s) found.\r\n  (0) Invalid argument:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\r\n\t [[{{node decode_image/DecodeImage}}]]\r\n\t [[IteratorGetNext]]\r\n\t [[IteratorGetNext/_4]]\r\n  (1) Invalid argument:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.\r\n\t [[{{node decode_image/DecodeImage}}]]\r\n\t [[IteratorGetNext]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_216368]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function\r\n```\r\n\r\nPlease help me with this \r\nThanks in advance", "@rohitkumar9989  you can solve this problem with the below code:\r\n\r\n    import cv2\r\n    import tensorflow as tf\r\n    import os\r\n\r\n    def check_images(s_dir, ext_list):\r\n        bad_images=[]\r\n        bad_ext=[]\r\n        s_list= os.listdir(s_dir)\r\n        for klass in s_list:\r\n            klass_path=os.path.join (s_dir, klass)\r\n            print ('processing class directory ', klass)\r\n            if os.path.isdir(klass_path):\r\n                file_list=os.listdir(klass_path)\r\n                for f in file_list:               \r\n                    f_path=os.path.join (klass_path,f)\r\n                    index=f.rfind('.')\r\n                    ext=f[index+1:].lower()\r\n                    if ext not in ext_list:\r\n                        print('file ', f_path, ' has an invalid extension ', ext)\r\n                        bad_ext.append(f_path)\r\n                    if os.path.isfile(f_path):\r\n                        try:\r\n                            img=cv2.imread(f_path)\r\n                            shape=img.shape\r\n                            image_contents = tf.io.read_file(f_path)\r\n                            image = tf.image.decode_jpeg(image_contents, channels=3)\r\n                        except Exception as e:\r\n                            print('file ', f_path, ' is not a valid image file')\r\n                            print(e)\r\n                            bad_images.append(f_path)\r\n                    else:\r\n                        print('*** fatal error, you a sub directory ', f, ' in class directory ', klass)\r\n            else:\r\n                print ('*** WARNING*** you have files in ', s_dir, ' it should only contain sub directories')\r\n        return bad_images, bad_ext\r\n\r\n\r\nFor execution of the above code:\r\n\r\n    source_dir =r'/data/validation/'\r\n    good_exts=['jpg','jpeg'] # list of acceptable extensions\r\n    bad_file_list, bad_ext_list=check_images(source_dir, good_exts)\r\n    if len(bad_file_list) !=0:\r\n        print('improper image files are listed below')\r\n    \r\n    print(bad_file_list)\r\n    print(bad_ext_list)\r\n\r\n", "Yes thank you sir....  I've actually made a helper function that converts\nall the images into JPEG format... So I wasn't able to see any error....\n\n\nOn Thu, 10 Jun 2021, 16:01 Deep Modi, ***@***.***> wrote:\n\n> @rohitkumar9989 <https://github.com/rohitkumar9989> you can solve this\n> problem with the below code:\n>\n> import cv2\n> import tensorflow as tf\n> import os\n>\n> def check_images(s_dir, ext_list):\n>     bad_images=[]\n>     bad_ext=[]\n>     s_list= os.listdir(s_dir)\n>     for klass in s_list:\n>         klass_path=os.path.join (s_dir, klass)\n>         print ('processing class directory ', klass)\n>         if os.path.isdir(klass_path):\n>             file_list=os.listdir(klass_path)\n>             for f in file_list:\n>                 f_path=os.path.join (klass_path,f)\n>                 index=f.rfind('.')\n>                 ext=f[index+1:].lower()\n>                 if ext not in ext_list:\n>                     print('file ', f_path, ' has an invalid extension ', ext)\n>                     bad_ext.append(f_path)\n>                 if os.path.isfile(f_path):\n>                     try:\n>                         img=cv2.imread(f_path)\n>                         shape=img.shape\n>                         image_contents = tf.io.read_file(f_path)\n>                         image = tf.image.decode_jpeg(image_contents, channels=3)\n>                     except Exception as e:\n>                         print('file ', f_path, ' is not a valid image file')\n>                         print(e)\n>                         bad_images.append(f_path)\n>                 else:\n>                     print('*** fatal error, you a sub directory ', f, ' in class directory ', klass)\n>         else:\n>             print ('*** WARNING*** you have files in ', s_dir, ' it should only contain sub directories')\n>     return bad_images, bad_ext\n>\n> For execution of the above code:\n>\n> source_dir =r'/data/validation/'\n> good_exts=['jpg','jpeg'] # list of acceptable extensions\n> bad_file_list, bad_ext_list=check_images(source_dir, good_exts)\n> if len(bad_file_list) !=0:\n>     print('improper image files are listed below')\n>\n> print(bad_file_list)\n> print(bad_ext_list)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13583#issuecomment-858508478>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQS5W25IOPN4KRIJ4CF2PO3TSCIBNANCNFSM4D6KCJ6A>\n> .\n>\n", "@rohitkumar9989 Okay, That's great. :)", "Awesome, thanks!\nThanks for your handy help...\n\n\nOn Thu, 10 Jun 2021, 16:07 Deep Modi, ***@***.***> wrote:\n\n> @rohitkumar9989 <https://github.com/rohitkumar9989> Okay, That's great. :)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13583#issuecomment-858511996>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AQS5W23LQD2EAC3XBJHOGQTTSCIX3ANCNFSM4D6KCJ6A>\n> .\n>\n", "\r\n\r\n@rohitkumar9989  could you please tell me how did you build the helper class? \r\nDoes this mean if I only have .jpg files to train on,I should be able to do?"]}, {"number": 13582, "title": "Branch 171489827", "body": "", "comments": ["The test failures are unrelated.", "Closing in favor of https://github.com/tensorflow/tensorflow/pull/13606"]}, {"number": 13581, "title": "Assign requires shapes of both tensors to match", "body": "im trying to run the voice recognition example\r\n\r\nfor some reason specifying a clip_duration_ms diffren from 1000 generate an error while freezing the model.\r\n\r\nso running\r\npython tensorflow/examples/speech_commands/freeze.py \r\n--wanted_words=yes \r\n--clip_duration_ms=2800 --sample_rate=16000 --window_size_ms=20 \r\n--start_checkpoint=/notebooks/yesmodel/conv.ckpt-10 \r\n--output_file=/notebooks/yesmodel/conv_frozen.pb\r\n\r\ngenerate the following message\r\nError: Assign requires shapes of both tensors to match. lhs shape= [320000,3] rhs shape= [62720,3]\r\n\r\nany idea what im doing wrong?", "comments": ["This appears to be a duplicate of #13487"]}, {"number": 13580, "title": "Equeued values to Queue get chopped-off if the Queue isn't instantiated properly.", "body": "### Describe the problem\r\n\r\nIn QueueBase._check_enqueue_dtypes, the following code is run:\r\n`tensors = []\r\n    for i, (val, dtype) in enumerate(zip(vals, self._dtypes)):\r\n      tensors.append(ops.convert_to_tensor(val, dtype=dtype,\r\n          name=\"component_%d\" % i))`\r\n\r\nThe problem is if the user feeds a list of tensors as `val` (the input) which is longer than the `_dtypes`-construction argument of QueueBase. \r\n\r\nIf the user hasn't specified a length of `_dtypes`, e.g if he constructs a Queue like this:\r\n`tf.FIFOQueue(100,dtypes=tf.int64)`\r\nThe FIFOQueue will have a default length 1. This means that the zip-function in the code above will essentialy **cut-off** any data that is longer than 1 (or whatever length `_dtypes` is).\r\n\r\nI think an exception should be thrown if the user tries to enqueue a list of tensors that is of unexpected length.\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/data_flow_ops.py\r\nLine 270, 271, 272 and 273\r\n", "comments": ["want to send a PR to fix? :)", "@yaroslavvb Will do", "Great! BTW, besides `contributing.md` (tells how to run linters), here are my notes for \"quick\" pull requests in TF\r\n\r\n```\r\n1. Making  Pull Request\r\n\r\nFirst, click \"Fork\" on github tensorflow page\r\n\r\nThen in Terminal:\r\n\r\nexport user=yaroslavvb\r\nexport branch_name\r\ngit clone https://github.com/$user/tensorflow.git\r\ncd tensorflow\r\ngit remote add y https://github.com/$user/tensorflow\r\ngit remote add tf https://github.com/tensorflow/tensorflow.git\r\ngit fetch tf\r\ngit checkout tf/master -b $branch_name\r\ngit push --set-upstream y\r\n\r\nThen in \"bugfix\" branch of your tensorflow fork on Github, find file, click \"Edit\" button, edit file, then click commit,\r\nthen click \"Pull Requests\" and select tensorflow/master on left and /tensorflow:bugfix on right. If master head moves while\r\nthe PR is going through, you can do\r\n\r\ngit rebase tf/master\r\ngit push -f y\r\n\r\n# Never do a git pull in your PR branch, that'll dump all the new commits into your PR\r\n\r\n```\r\n\r\nIf it gets too complicated to to eyeball for correctness, then may need to set up dev environment like [here](https://medium.com/@yaroslavvb/setting-up-tensorflow-dev-environment-sep-19-fd27b321de14)", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I'm assuming this is fixed since @bnoreus stated he would send a PR. If someone still has this issue, please reopen."]}, {"number": 13579, "title": "[XLA] Reorder the parameters in a map inline operation according to the parameter number", "body": "The inliner (not typically in use) has a fault where it doesn't spot that the parameters within the mapped operation are not in the same order as the actual parameter numbers on the operands.\r\n\r\ni.e. the mapped operation looks like:\r\n\r\n```\r\n  Param(1)         Param(0)\r\n      |              |\r\n      -- Binary-op ---\r\n            |\r\n```\r\n\r\nPreviously the operands were passed to the clone in the order that they were supplied to the binary op - not the order as defined by the parameter numbers.\r\n\r\nThis fixes the issue (which could be potentially be called `b/35786417`)\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "I don't think that the test failures are related to my change.", "sure thing", "added\r\n", "@tensorflow-jenkins test this please", "Can one of the admins verify this patch?", "@tensorflow-jenkins test this please"]}, {"number": 13578, "title": "Using an LSTM-CTC Tensorflow Model in Android", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 8.1\r\n- **TensorFlow installed from (source or binary)**:\r\nAnaconda Install. [Prebuilt libraries](https://ci.tensorflow.org/view/Nightly/job/nightly-android/) were used for Tensorflow Android.\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n- **Python version**: \r\nPython 3.5.3 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n### Describe the problem\r\nI have succeeded in training my bi-lstm-ctc tensorflow model and now I want to use it for my handwriting recognition android application. Here's the part of the code that defines the graph I used:\r\n\r\n```\r\nself.inputs = tf.placeholder(tf.float32, [None, None, network_config.num_features], name=\"input\")\r\nself.labels = tf.sparse_placeholder(tf.int32, name=\"label\")\r\nself.seq_len = tf.placeholder(tf.int32, [None], name=\"seq_len_input\")\r\n\r\nlogits = self._bidirectional_lstm_layers(\r\n   network_config.num_hidden_units,\r\n   network_config.num_layers,\r\n   network_config.num_classes\r\n)\r\n\r\nself.global_step = tf.Variable(0, trainable=False)\r\nself.loss = tf.nn.ctc_loss(labels=self.labels, inputs=logits, sequence_length=self.seq_len)\r\nself.cost = tf.reduce_mean(self.loss)\r\n\r\nself.optimizer = tf.train.AdamOptimizer(network_config.learning_rate).minimize(self.cost)\r\nself.decoded, self.log_prob = tf.nn.ctc_beam_search_decoder(inputs=logits, sequence_length=self.seq_len, merge_repeated=False)\r\nself.dense_decoded = tf.sparse_tensor_to_dense(self.decoded[0], default_value=-1, name=\"output\")\r\n\r\n```\r\n\r\nI also succeeded in freezing and optimizing the graph using this code:\r\n\r\n```\r\ndef freeze(input_graph_path, checkpoint_path, output_node_names, input_saver_def_path=\"\", input_binary=False,\r\n           restore_op_name=\"save/restore_all\", filename_tensor_name=\"save/Const:0\",\r\n           output_frozen_graph_name=\"frozen_output.pb\",\r\n           clear_devices=True):\r\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path, input_binary,\r\n                              checkpoint_path, output_node_names, restore_op_name, filename_tensor_name,\r\n                              output_frozen_graph_name, clear_devices, \"\")\r\n\r\n\r\ndef optimize_graph(graph_path, input_nodes, output_nodes):\r\n    input_graph_def = tf.GraphDef()\r\n    with tf.gfile.Open(graph_path, \"rb\") as f:\r\n        data = f.read()\r\n        input_graph_def.ParseFromString(data)\r\n\r\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n        input_graph_def,\r\n        input_nodes,\r\n        output_nodes,\r\n        tf.float32.as_datatype_enum\r\n    )\r\n\r\n    f = tf.gfile.FastGFile(\"optimized_\" + graph_path, \"w\")\r\n    f.write(output_graph_def.SerializeToString())\r\n```\r\n\r\nAnd here's the part of the android code that is supposed to run the model:\r\n\r\n```\r\nbitmap = Bitmap.createScaledBitmap(bitmap, 1024, 128, true);\r\nint[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];\r\nbitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\nfloat[] floatValues = new float[bitmap.getWidth() * bitmap.getHeight()];\r\nfor (int i = 0; i < intValues.length; ++i) {\r\n    final int val = intValues[i];\r\n    floatValues[i] = (((val >> 16) & 0xFF));\r\n}\r\nfloat[] result = new float[80];\r\nlong[] INPUT_SIZE = new long[]{1, bitmap.getHeight(), bitmap.getWidth()};\r\ninferenceInterface.feed(config.getInputName(), floatValues, INPUT_SIZE);\r\ninferenceInterface.feed(\"seq_len_input\", new int[]{bitmap.getWidth()}, 1);\r\ninferenceInterface.run(config.getOutputNames());\r\ninferenceInterface.fetch(config.getOutputNames()[0], result);\r\n\r\nreturn result.toString();\r\n```\r\n\r\nHowever, I encounter these problems depending on the model I use. If I use the frozen graph, I encounter this error:\r\n\r\n```\r\nCaused by: java.lang.IllegalArgumentException: No OpKernel was registered to support\r\nOp 'SparseToDense' with these attrs.  Registered devices: [CPU], Registered kernels:\r\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n                                                                                            \r\n[[Node: output = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true](CTCBeamSearchDecoder, CTCBeamSearchDecoder:2, CTCBeamSearchDecoder:1, output/default_value)]]\r\n```\r\n\r\nIf I use the optimized frozen graph, I encounter this error:\r\n\r\n```\r\njava.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs \r\nspecified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; \r\nNodeDef: stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/add/y = Const[dtype=DT_INT32, \r\nvalue=Tensor<type: int32 shape: [] values: 1>](stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Switch:1)\r\n```\r\n\r\nI have no ideas on what these error messages tell me nor how to resolve these.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Updated the post."]}, {"number": 13577, "title": "Size of TFRecord is much more larger than CSV format", "body": "### Testing Data:\r\n `adult.data` in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py\r\n### Problem:\r\nTo use a `tf.contrib.data.TFRecordDataset`, i tried to convert `adult.data` in CSV into a TFRecord, but i just found that the **TFRecord** after converted is about **12MB**, while the original **CSV** is only about **3MB**, oops, **why the storage efficiency for TFRecord is so poor\uff1f**\r\n\r\n### Source code for converting CSV to TFRecord:\r\n```\r\ndef _int64_feature(value):\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\r\n\r\ndef _bytes_feature(value):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\r\n\r\ndef csv2proto(\r\n        data_source,\r\n        all_cols,\r\n        categorical_cols=[],\r\n        continuous_cols=[],\r\n        multi_values_cols=[],\r\n        inner_delimiter=';'):\r\n    if not os.path.isfile(data_source):\r\n        raise ValueError('data file passed do not exist or not a file')\r\n\r\n    file_name = os.path.splitext(data_source)[0] + '.tfrecords'\r\n    writer = tf.python_io.TFRecordWriter(file_name)\r\n    with open(data_source) as f:\r\n        reader = csv.DictReader(f, fieldnames=all_cols)\r\n        for row in reader:\r\n            feature = dict()\r\n            for col in categorical_cols:\r\n                feature.update({col: _bytes_feature([row[col]])})\r\n            for col in continuous_cols:\r\n                feature.update({col: _int64_feature([int(row[col])])})\r\n            for col in multi_values_cols:\r\n                feature.update({col: _bytes_feature(row[col].split(inner_delimiter))})\r\n\r\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\r\n            writer.write(example.SerializeToString())\r\n\r\n        writer.close()\r\n```\r\n\r\n### System information:\r\n- **OS Platform**:  Mac OS X 10.12.5\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: None\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None", "comments": ["Please include the code that calls `csv2proto`, so I have a self-contained example I can run to generate the TFRecord.", "@reedwm pls check the full code as below,\r\n\r\n```\r\nimport csv\r\nimport os\r\nimport tempfile\r\n\r\nfrom six.moves import urllib\r\nfrom datetime import datetime\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nMB_SIZE = 1024.0*1024.0\r\nCSV_FORMAT = 'adult.data'\r\n\r\ncategorical_columns = [\r\n    \"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\",\r\n    \"race\", \"gender\", \"native_country\", \"label\"\r\n]\r\ncontinuous_columns = [\r\n    \"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\",\r\n    \"hours_per_week\"\r\n]\r\nall_columns = [\r\n    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\r\n    \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\r\n    \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"label\"\r\n]\r\n\r\n\r\ndef _int64_feature(value):\r\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\r\n\r\n\r\ndef _bytes_feature(value):\r\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\r\n\r\n\r\ndef csv2proto(\r\n        data_source,\r\n        all_cols,\r\n        categorical_cols=[],\r\n        continuous_cols=[],\r\n        multi_values_cols=[],\r\n        inner_delimiter=';'\r\n):\r\n    if not os.path.isfile(data_source):\r\n        raise ValueError('data file passed do not exist or not a file')\r\n\r\n    csv_size = os.path.getsize(data_source)/MB_SIZE\r\n    file_name = os.path.splitext(data_source)[0] + '.tfrecords'\r\n    writer = tf.python_io.TFRecordWriter(file_name)\r\n    start = datetime.now()\r\n    with open(data_source) as f:\r\n        reader = csv.DictReader(f, fieldnames=all_cols)\r\n        for row in reader:\r\n            feature = dict()\r\n            for col in categorical_cols:\r\n                feature.update({col: _bytes_feature([row[col]])})\r\n            for col in continuous_cols:\r\n                feature.update({col: _int64_feature([int(row[col])])})\r\n            for col in multi_values_cols:\r\n                feature.update({col: _bytes_feature(row[col].split(inner_delimiter))})\r\n\r\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\r\n            writer.write(example.SerializeToString())\r\n\r\n        writer.close()\r\n        end = datetime.now()\r\n        proto_size = os.path.getsize(file_name)/MB_SIZE\r\n\r\n        convert_logs = \"\\n| Time Elapsed: %d\" % (end-start).seconds + \" s |\"\r\n        convert_logs += \"\\n| csv size: \" + \"%.2f\" % csv_size + \" MB |\"\r\n        convert_logs += \"\\n| proto size: \" + \"%.2f\" % proto_size + \" MB |\\n\"\r\n\r\n        print convert_logs\r\n\r\n\r\ndef main():\r\n    urllib.request.urlretrieve(\r\n        \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\r\n        CSV_FORMAT)\r\n    csv2proto(data_source=CSV_FORMAT,\r\n              all_cols=all_columns,\r\n              categorical_cols=categorical_columns,\r\n              continuous_cols=continuous_columns)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nLogs of my side(Mac OS X 10.12.5) is:\r\n```\r\n| Time Elapsed: 89 s |\r\n| csv size: 3.79 MB |\r\n| proto size: 12.68 MB |\r\n```\r\n\r\n", "@davidsoergel, do you know why `tf.train.Example`s are significantly larger than the equivalent row of a CSV file?", "I'm running into a similar situation using images. \r\n\r\nI uploaded my code here:\r\n\r\nhttps://github.com/moondra2017/Testing_Cat_classifier_Tensorflow/blob/master/Creating_tf_records_of_small_set_fishes.py", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Its a good way to serialize a libsvm into tf.Example protos in this way:\r\n```\r\ndef libsvm2proto(data_source, target_dir, delimiter='\\t'):\r\n    \"\"\"\r\n    a single file should not contain lines more than 1000,000\r\n    :param data_source: libsvm file path\r\n    :param target_dir: dir to storage the serialize proto file\r\n    :param delimiter: delimiter for csv reader\r\n    :return:\r\n    \"\"\"\r\n    if not os.path.isfile(data_source):\r\n        raise ValueError('data file passed do not exist or not a file')\r\n\r\n    file_name = os.path.join(target_dir, os.path.splitext(\r\n        os.path.basename(data_source))[0] + '.tfrecords')\r\n    writer = tf.python_io.TFRecordWriter(file_name)\r\n    start = datetime.now()\r\n    with open(data_source, 'rb') as rf:\r\n        f_reader = csv.reader(rf, delimiter=delimiter, quotechar='|')\r\n        for row in f_reader:\r\n            feature = dict()\r\n            indexes = []\r\n            values = []\r\n            feature.update({'label': _float_feature([float(row[0])])})\r\n            for e in row[1:]:\r\n                index, value = e.split(':')\r\n                indexes.append(int(index))\r\n                values.append(float(value))\r\n                feature.update({'index': _int64_feature(indexes)})\r\n                feature.update({'value': _float_feature(values)})\r\n\r\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\r\n            writer.write(example.SerializeToString())\r\n\r\n        writer.close()\r\n        end = datetime.now()\r\n\r\n        print(\"- consumed time: %ds for %s\" % ((end-start).seconds, data_source))\r\n```\r\nThe size of TFRecord after converted is just half of the source libsvm \ud83d\udc4d .  That's what we really expect.\r\n"]}, {"number": 13576, "title": "sparse_softmax_cross_entropy_with_logits wrong annotation", "body": "https://github.com/tensorflow/tensorflow/blob/107cc777af7880c140d089e44ad898a6ba929286/tensorflow/python/ops/nn_ops.py#L1661\r\n\r\nIt should be `If logits are scalars (need to have rank >= 1) or if the rank\r\n      of the labels is not equal to the rank of the logits minus one.`", "comments": ["Thanks for pointing that out, will send a fix.\r\n"]}, {"number": 13575, "title": "Reading .tfrecords files greater than 64mb brings up errors", "body": "I have already read this thread: https://github.com/tensorflow/tensorflow/issues/7311 but I am still encountering this issue with version 1.3.0. Was wondering if anyone knows why?\r\n\r\n\r\nTested on 2 systems:\r\nOS: Windows 10, Ubuntu 16.04\r\nTensorflow installed from source,\r\nTensorflow version: 1.3.0\r\nOne system running tensorflow non gpu and another system running CUDA 8 and cuDNN v6\r\nGPU model: none and GTX Titan Z\r\n\r\nThe error I get is:\r\n\r\n`  File \"C:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Could not parse example input, value: '\r\n\ufffd\u05dd9\r\n\u047b\u0001\r\n\u0003csv\u0012\u023b\u0001\r\n\u013b\u0001\r\n\ufffd\ufffd\u0001@CQ\ufffd\u0014.\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdM\ufffdB\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\write_to_tfrecords.py\", line 581, in <module>\r\n    main()\r\n  File \"D:\\write_to_tfrecords.py\", line 577, in main\r\n    read_dataset_from_tfrecords()\r\n  File \"D:\\write_to_tfrecords.py\", line 554, in read_dataset_from_tfrecords\r\n    image_dataset, csv_dataset = sess.run([image_out_reshaped, csv_out_reshaped])\r\n  File \"C:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Could not parse example input, value: '\r\n\ufffd\u05dd9\r\n\u047b\u0001\r\n\u0003csv\u0012\u023b\u0001\r\n\u013b\u0001\r\n\ufffd\ufffd\u0001@CQ\ufffd\u0014.\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffdM\ufffdB\r\n>>> \r\n`\r\n\r\n\r\nWhen I try to read .tfrecords files that are less than 64mb this error does not occur.\r\n\r\nMy code is here:\r\n`        with tf.Session() as sess:\r\n            try:\r\n                feature = {'images': tf.FixedLenFeature([], tf.string),\r\n                           'csv': tf.FixedLenFeature([], tf.string)\r\n                           }\r\n\r\n                filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)\r\n\r\n                reader = tf.TFRecordReader()\r\n\r\n                _, serialized_example = reader.read(filename_queue)\r\n\r\n                features = tf.parse_single_example(serialized_example, features=feature)\r\n\r\n                image_out = tf.decode_raw(features['images'], tf.uint8)\r\n                csv_out = tf.decode_raw(features['csv'], tf.float32)\r\n\r\n                image_out_reshaped = tf.reshape(image_out, [1000, 200, 200, 3])\r\n                csv_out_reshaped = tf.reshape(csv_out, [1000, 6])\r\n\r\n                sess.run(tf.global_variables_initializer())\r\n                sess.run(tf.local_variables_initializer())\r\n\r\n                # Create a coordinator and run all QueueRunner objects\r\n                coord = tf.train.Coordinator()\r\n                threads = tf.train.start_queue_runners(coord=coord)\r\n\r\n                image_dataset, csv_dataset = sess.run([image_out_reshaped, csv_out_reshaped])\r\n\r\n                coord.request_stop()\r\n                coord.join(threads)\r\n`", "comments": ["Can confirm this issue though it only occurs on windows. To reproduce you can use [this][1] modified 3d volume from the RIRE dataset with following decoder:\r\n\r\n```python\r\nclass Decoder:\r\n  \"\"\"Decodes a tfrecord volume to tensor.\"\"\"\r\n\r\n  def __init__(self):\r\n    self.features = {\r\n        'volume/encoded': tf.FixedLenFeature((), tf.string),\r\n        'volume/shape': tf.VarLenFeature(tf.int64),\r\n        'volume/vmin': tf.FixedLenFeature((), tf.int64),\r\n        'volume/vmax': tf.FixedLenFeature((), tf.int64),\r\n    }\r\n\r\n  def decode(self, example):\r\n    \"\"\"Returns tensor from tfrecord string.\r\n\r\n    Args:\r\n      example: tfrecord string\r\n    Returns:\r\n      volume: volume tensor\r\n    \"\"\"\r\n    with tf.name_scope('decode'):\r\n      features = tf.parse_single_example(example, features=self.features)\r\n\r\n      return tf.reshape(tf.decode_raw(features['volume/encoded'], tf.int32),\r\n                        features['volume/shape'].values)\r\n```\r\n\r\nthis script should read volume and let the error occur:\r\n\r\n```python\r\ndecoder = Decoder()\r\n\r\ndataset = tf.contrib.data.TFRecordDataset('mr.tfrecord').map(decoder.decode)\r\n\r\nvolume = dataset.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as session:\r\n  session.run(volume)\r\n```\r\n\r\nyielding:\r\n\r\n```\r\n2017-10-09 17:30:13.903542: W C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: Could not parse example input, value: '\r\n\u00ab\u00b5\u00bd'\r\n\u2580\u03c3\u00bd'\r\n\u000evolume/encoded\u0012\u2569\u03c3\u00bd'\r\n\u253c\u03c3\u00bd'\r\n\u2514\u03c3\u00bd'\r\n2017-10-09 17:30:14.626344: W C:\\tf_jenkins\\home\\workspace\\tf-nightly-windows\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\framework\\op_kernel.cc:1192] Invalid argument: Could not parse example input, value: '\r\n\u00ab\u00b5\u00bd'\r\n\u2580\u03c3\u00bd'\r\n\u000evolume/encoded\u0012\u2569\u03c3\u00bd'\r\n\u253c\u03c3\u00bd'\r\n\u2514\u03c3\u00bd'\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Bodo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\Bodo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Users\\Bodo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\contextlib.py\", line 88, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\Bodo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 467, in raise_exception_on_not_ok_status\r\n    c_api.TF_GetCode(status.status))\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Could not parse example input, value: '\r\n\\udcae\\udce6\\udcab'\r\n\\udcdf\\udce5\\udcab'\r\n\u000evolume/encoded\u0012\\udcca\\udce5\\udcab'\r\n\\udcc5\\udce5\\udcab'\r\n\\udcc0\\udce5\\udcab'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Bodo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\Bodo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\Bodo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"C:\\Users\\Bodo\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Could not parse example input, value: '\r\n\\udcae\\udce6\\udcab'\r\n\\udcdf\\udce5\\udcab'\r\n\u000evolume/encoded\u0012\\udcca\\udce5\\udcab'\r\n\\udcc5\\udce5\\udcab'\r\n\\udcc0\\udce5\\udcab'\r\n```\r\n\r\nTensorflow versions where this occurs latest `tf-nightly`, `tf-nightly-gpu` and 1.3.0 gpu all on windows 10\r\n\r\n[1]: https://www.dropbox.com/s/ka9ximgj65dibtr/mr.tfrecord?dl=0", "```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nSIZE = int(64e6)\r\n\r\ndef _bytes_feature(value):\r\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\r\n\r\n\r\ndef decode(example):\r\n  feature_keys = {'encoded': tf.FixedLenFeature((), tf.string)}\r\n  features = tf.parse_single_example(example, features=feature_keys)\r\n  return tf.decode_raw(features['encoded'], tf.int32)\r\n\r\n\r\ndef encode(array):\r\n  return tf.train.Example(features=tf.train.Features(feature={\r\n      'encoded': _bytes_feature([array.astype(np.int32).tobytes()]),\r\n    })).SerializeToString()\r\n\r\n\r\nwith tf.python_io.TFRecordWriter('foo.tfrecord') as writer:\r\n  writer.write(encode(np.ones([SIZE], np.int32)))\r\n\r\n\r\ndataset = tf.data.TFRecordDataset('foo.tfrecord').map(decode)\r\n\r\ndata = dataset.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as session:\r\n  print(session.run(data).shape)\r\n```\r\n\r\nThis should fails too on windows.", "@martinwicke #7311 (tfrecord limit) is not resolved on windows", "@mrry Does Windows Protobuf still have the 64M limit? \r\n\r\n@bodokaiser You built from source, can you check your protobuf version?", "@martinwicke I used `tf-nightly-gpu` package [built by the tensorflow team](http://ci.tensorflow.org/view/tf-nightly/job/tf-nightly-windows/M=windows-gpu,PY=36/). Protobuf version is `3.4.0` (as on `tf-nightly` for macOS)\r\n\r\nHowever I noticed I had to do `from google import protobuf; protobuf.__version__` on the windows machine as `import google; google.protobuf.__version__` would fail with `module \"google\" has no attribute \"protobuf\"` what however works under macOS (could this be relevant somehow?).", "@martinwicke I believe we're using the same version of the library on both platforms. Is it possible that there's a missing configuration option needed to disable the limit?", "The default limit starting at 3.2 is 2G -- there doesn't seem to be anything platform related in that code.", "@martinwicke what do u suggest where the error originates from?", "I believe the error \"Could not parse example input\" originates [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/example_parsing_ops.cc#L341). \r\n\r\nThis would also mean you're parsing Sequence Examples, which your code doesn't match, and is confusing.", "What about [here][1]?\r\n\r\nI followed `tf.parse_single_example` to [here][2] however `tensorflow.python.ops.gen_parsing_ops` seems to be a generated file. Any idea what generates this?\r\n\r\n[1]: https://github.com/tensorflow/tensorflow/blob/e062447136faa0a3513e3b0690598fee5c16a5db/tensorflow/core/util/example_proto_fast_parsing.cc#L461-L462\r\n[2]: https://github.com/tensorflow/tensorflow/blob/a2d9b3bf5f9e96bf459074d079b01e1c74b25afa/tensorflow/python/ops/parsing_ops.py#L698", "You are right of course. gen_parsing_ops is generated from parsing_ops.cc. The core you pointed to is the ultimate source of the error.", "~~Any suggestion on how I could trace tensorflow core calls in C++ land so we can identify the problem?~~\r\n\r\n~~I tried dtrace and tfdbg, however dtrace got me only syscalls and tfdbg only python / graph related things.~~\r\n\r\nOkay so `tf.parse_single_example` or `tf.parse_example` etc. pp. (see comment below), all finally end up at [`ExampleParserOp`][1] which uses [`FastParseSerializedExample`][2] and [`ParseExample`][3]  where our error occur.\r\n\r\n[1]: https://github.com/tensorflow/tensorflow/blob/b126091bbb65e614d43d6ccd984e728561778a1a/tensorflow/core/kernels/example_parsing_ops.cc#L142\r\n[2]: https://github.com/tensorflow/tensorflow/blob/e062447136faa0a3513e3b0690598fee5c16a5db/tensorflow/core/util/example_proto_fast_parsing.cc#L461-L462\r\n[3]: https://github.com/tensorflow/tensorflow/blob/e062447136faa0a3513e3b0690598fee5c16a5db/tensorflow/core/util/example_proto_fast_parsing.cc#L298-L321", "Okay this is really weird.\r\n\r\nIf I replace `tf.parse_single_example` with `tf.parse_single_sequence_example` and reference to the resulting features with `features[0]['encoded']` instead of `features['encoded']` I do not get an error but the decoded bytes are just about 1/5 of the size it should be. On macOS it works as expected.\r\n\r\n**Update:**\r\n\r\n~~Using `tf.parse_example` also gives the same error on windows. Can it be that there is some allocation limit going on on windows?~~\r\n\r\nThere are just two possibilities ([1][1], [2][2]) where `ParseExample` returns `false`. The first one ends at [`SkipExtraenousTag`][3] calling [`CodedInputStream::ReadVarint32`][4]. The second one ends at [`ParseFeatures`][5] where there are multiple possible points of failure. As this code is coupled very tight with the protobuf library it may make sense to write a protobuf only test case.\r\n\r\n[1]: https://github.com/tensorflow/tensorflow/blob/e062447136faa0a3513e3b0690598fee5c16a5db/tensorflow/core/util/example_proto_fast_parsing.cc#L307\r\n\r\n[2]: https://github.com/tensorflow/tensorflow/blob/e062447136faa0a3513e3b0690598fee5c16a5db/tensorflow/core/util/example_proto_fast_parsing.cc#L310\r\n\r\n[3]: https://github.com/tensorflow/tensorflow/blob/e062447136faa0a3513e3b0690598fee5c16a5db/tensorflow/core/util/example_proto_fast_parsing.cc#L229\r\n\r\n[4]: https://github.com/google/protobuf/blob/fa5a69e73b0dd667ff15062adbc170310d440ee9/src/google/protobuf/io/coded_stream.cc#L401\r\n\r\n[5]: https://github.com/tensorflow/tensorflow/blob/e062447136faa0a3513e3b0690598fee5c16a5db/tensorflow/core/util/example_proto_fast_parsing.cc#L281", "@martinwicke @mrry are there any updates?\r\n\r\nI found that you can use i.e.\r\n\r\n```python\r\nimport numpy as np\r\n\r\nfrom tensorflow.core.example import example_pb2\r\n\r\ndef read_tfrecord(filename):\r\n  with open(filename) as f:\r\n    example = example_pb2.Example.FromString(f.read())\r\n    feature = example.features.feature\r\n\r\n    encoded = feature['encoded'].bytes_list.value[0]\r\n\r\n  return np.fromstring(encoded, np.int32)\r\n\r\nplaceholder = tf.placeholder(tf.int32)\r\n\r\ndataset = tf.data.Dataset.from_tensors(placeholder)\r\n# do data proprocessing as usual\r\n\r\nwith tf.Session() as session:\r\n  sess.run(..., feed_dict={placeholder: read_tfrecord('foo.tfrecord')})\r\n```\r\n\r\nPS:\r\n\r\n**Is there anything else I could do to help to push this further?**", "Thanks @bodokaiser  for the detailed repro examples. I was able to reproduce using this example on Windows.\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nSIZE = int(64e6)\r\ndef _bytes_feature(value):\r\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\r\n\r\ndef decode(example):\r\n  feature_keys = {'encoded': tf.FixedLenFeature((), tf.string)}\r\n  features = tf.parse_single_example(example, features=feature_keys)\r\n  return tf.decode_raw(features['encoded'], tf.int32)\r\n\r\ndef encode(array):\r\n  return tf.train.Example(features=tf.train.Features(feature={\r\n      'encoded': _bytes_feature([array.astype(np.int32).tobytes()]),\r\n    })).SerializeToString()\r\n\r\nwith tf.python_io.TFRecordWriter('foo.tfrecord') as writer:\r\n  writer.write(encode(np.ones([SIZE], np.int32)))\r\n\r\ndataset = tf.data.TFRecordDataset('foo.tfrecord').map(decode)\r\n\r\ndata = dataset.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as session:\r\n  print(session.run(data).shape)\r\n```\r\n\r\nHowever, changing from `BytesList` to `Int64` seems to work (for this example at least). I am looking into this further.", "@saxenasaurabh yes indeed using `Int64List` does work as intended on windows.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nSIZE = int(64e6)\r\ndef _int64_feature(value):\r\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\r\n\r\ndef decode(example):\r\n  feature_keys = {'encoded': tf.VarLenFeature(tf.int64)}\r\n  features = tf.parse_single_example(example, features=feature_keys)\r\n  return tf.sparse_tensor_to_dense(features['encoded'])\r\n\r\ndef encode(array):\r\n  return tf.train.Example(features=tf.train.Features(feature={\r\n      'encoded': _int64_feature(array.astype(np.int64)),\r\n    })).SerializeToString()\r\n\r\nwith tf.python_io.TFRecordWriter('foo.tfrecord') as writer:\r\n  writer.write(encode(np.ones([SIZE], np.int64)))\r\n\r\ndataset = tf.data.TFRecordDataset('foo.tfrecord').map(decode)\r\n\r\ndata = dataset.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as session:\r\n  print(session.run(data).shape)\r\n```\r\n\r\nIn this case the problem may not be caused by the decoding part but by the [type switch](https://github.com/tensorflow/tensorflow/blob/e062447136faa0a3513e3b0690598fee5c16a5db/tensorflow/core/util/example_proto_fast_parsing.cc#L566-L570). What do you think?", "Even this fails with `SIZE = int(64e7)` on my machine :(", "@saxenasaurabh unfortunately can confirm too for `SIZE = int(64e7)`...", "@saxenasaurabh Can you give an honest estimate on when (or if) this issue will get resolved? It may make sense for me to pursue the path of somehow pressing a linux workstation.", "Assigning to @saxenasaurabh as part of triage.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@saxenasaurabh any update?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I am unable to reproduce this even with `SIZE = int(64e7)`. I am using protobuf version `3.5.1`.\r\nClosing for now. Please re-open if you are still seeing this."]}, {"number": 13574, "title": "Protobuf must be compiled with the same version", "body": "Added CXX=g++-4.8 to make command. On Raspberry PI protobuf must be compiled with the same gcc version as tensorflow, otherwise compile stops with error see tensorflow/tensorflow#5684.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@gmamaladze Please take care of the CLA.\r\n\r\n@tensorflow-jenkins test this please", "We can't accept this PR without CLA. I will close it. Please reopen under CLA if you want to contribute this change."]}, {"number": 13573, "title": "Fix broken link in performance models", "body": "This fix fixes broken link in performance models as models repo moved inception to `models/research/inception`:\r\n`https://github.com/tensorflow/models/tree/master/inception#getting-started`\r\n->\r\n`https://github.com/tensorflow/models/tree/master/research/inception#getting-started`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13572, "title": "Adding --verify_images to example image retrainer.", "body": "Adding --verify_images to allow soft checking of image files before bottleneck creation.\r\nWhen enabled, invalid images are ignored with a warning instead of throwing an exception.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please\r\n", "@ianmaddox any updates? Marking as stalled for now.", "@ianmaddox Feel free to reopen when you get a chance to take a look."]}, {"number": 13571, "title": "top level estimator docs update to match dnn.py", "body": "see #13570", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13570, "title": "update dnn.py to clarify outputs arg to predict fn", "body": "It is unclear what the valid values are for predict(). Since the 2 variants are predict_classes and predict_proba, it's easy to think that the valid values are \"classes\" and \"proba\", which is untrue.\r\nI _think_ I've listed all the relevant values for DNNClassifier -- at least that's what the error msg told me when I used \"proba\" as my arg :D", "comments": ["Can one of the admins verify this patch?", "All of the contrib Estimator will be dropped in favor of tf.estimator.Estimator, which does not have a predict_classes methods, but retains the predict method returning all the outputs. I think while this change looks like a good idea, it would be confusing in this context."]}, {"number": 13569, "title": "Initial Haiku support", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@martinwicke WDYT?", "Instead of making PIC depend on Haiku directly, can you add a generic option to turn off PIC, and make haiku use that option? \r\n\r\nSince this is limited to PIC and some error codes (for which we already have FreeBSD exceptions) I'm fine with this change otherwise.", "@martinwicke : Sadly currently i have no idea how to create a variable what can propagate through the cmake files, but i'll look into it. Have you got an example maybe?", "I think you can just do `option()`.\n\nOn Oct 10, 2017 00:49, \"miqlas\" <notifications@github.com> wrote:\n\n> @martinwicke <https://github.com/martinwicke> : Sadly currently i have no\n> idea how to create a variable what can propagate through the cmake files,\n> but i'll look into it. Have you got an example maybe?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13569#issuecomment-335389460>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_ai2H143GhJ-8xAJDdgs2yHcgEx1ks5sqyF3gaJpZM4PxufZ>\n> .\n>\n", "thanks! Will try at weekend!", "@martinwicke : Tried to do it as good as possible,but i have not too much experiences with cmake.Can you look into it please?\r\n\r\nThanks!", "There seems to be a merge conflict. could you rebase your change on top of master?", "@gunan : conflict solved.", "Jenkins, test this please.", "Checks looks ok \ud83d\udc4d ", "Any news here?", "Hi @extrowerk !\r\n\r\nCould you please share the reason as to why you disabled PIC for Haiku builds? Just curious!", "@nehaljwani i disabled PIE, not PIC.\r\nI cannot explain you correctly, i don't know much about technologies behind the curtain in Haiku, but it is required, without it it results just linking problems."]}, {"number": 13568, "title": "batch_flatten gives unpredictable results when batch_size is 1", "body": "## Problem\r\n\r\nWhen I attempt to use `tf.contrib.keras.backend.batch_flatten` or more readily `keras.backend.batch_flatten` on a Tensor of shape (1, a, b) it produces a Tensor with shape (None, None).\r\n\r\n## MCVE\r\n\r\nNote, I'll use keras tensorflow backend because that is how I noticed it. \r\n\r\n```\r\nt = K.zeros((1,2,2))\r\nprint(K.int_shape(K.reshape(t,(-1,2*2))))\r\nprint(K.int_shape(K.batch_flatten(t)))\r\n```\r\nprints out `(1,4)` and `(None, None)`.\r\nIf you replace the first dimension to 2 or more it works as expected.\r\n\r\n## System\r\n\r\ntensorflow 1.2.0 \r\nkeras 2.0.8 python 3.6 installed from git repo\r\n\r\n\r\n\r\n\r\n", "comments": ["cc @fchollet \r\n(is there a separate list for keras issues?)", "This was caused by https://github.com/fchollet/keras/pull/3253. The repository also has a list for keras issues.", "Maybe PR https://github.com/tensorflow/tensorflow/pull/13561 fixes it.", "Nagging Assignee @tatianashp: It has been 331 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 13567, "title": "Hello every one, please i got an error in my code, here is the error, please can someone help me how to fix it...thanks.... \"but saw tensor: %s\" % p) ValueError: prefix tensor must be either a scalar or vector, but saw tensor: Tensor(\"batch_size:0\", dtype=int32)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\nIt must be a bug or a feature request.\r\nThe form below must be filled out.\r\nIt shouldn't be a TensorBoard issue. Those go here."]}, {"number": 13566, "title": "tf.estimator Quickstart web doc needs syncing with GitHub", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.11.6\r\n- **TensorFlow installed from (source or binary)**: conda-forge\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: Copy and paste code sample and run on a Jupter Notebook. The code sample on GitHub works (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md). The code sample on Tensorflow documentation doesn't (https://www.tensorflow.org/get_started/estimator). My guess is that the web documentation requires \"syncing\" with Github version? \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThis is regarding (possibly out-dated) web documentation not \"synced\" with the correct GitHub version.\r\n\r\n- The code sample on Tensorflow documentation doesn't run on Python 3.6 / TensorFlow v1.3 (https://www.tensorflow.org/get_started/estimator). \r\n- The code sample on GitHub works (probably more up to date)\r\n (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md). \r\n\r\n### Source code / logs\r\nIf you copy and paste the code sample from web documentation and run on a Jupter notebook, you get:\r\n\r\n```\r\nAttributeError: module 'urllib' has no attribute 'urlopen'\r\n```\r\n\r\n(this has been addressed in GitHub repo sample code. Just not the web doc).\r\n\r\nAlso, the line that reads (for both training and test):\r\n\r\n```.py\r\nwith open(IRIS_TRAINING, \"w\") as f:\r\n```\r\n\r\nshould be corrected to:\r\n\r\n```.[py\r\nwith open(IRIS_TRAINING, \"wb\") as f:\r\n```\r\n\r\n(again, this has been addressed in GitHub repo sample code. Just not the web doc).\r\n\r\nSo possible just need a refresh of the code sample on the web doc?\r\n\r\ni.e. replace the [current web doc](https://www.tensorflow.org/get_started/estimator) with the [GitHub doc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/get_started/estimator.md)?\r\n\r\nThanks!", "comments": ["The docs refer to the most recent stable branch. So I'm guessing the recent stable branch (1.3) has a bug it. Your workaround used master where the bug was fixed. Release 1.4 seems to be forthcoming shortly which would address this.", "@MarkDaoust : Yeah, I think this will be addressed with the next website refresh.", "I just had a look at the [tensorflow version](https://www.tensorflow.org/versions/) page - it says:\r\n\r\n> The docs at root (i.e. in tensorflow.org/api_docs) refer to the most recent stable branch (in this case, r1.2).\r\n\r\nThis probably explains it. Though I guess it \"might\" be a good idea to highlight at the top of web documentation on this? I have a gut feeling a chunk of new comers would have tensorflow v1.3 installed, while using the v1.2 web documentation.)\r\n\r\nI presume this will belong to a \"feature request\" bucket so will leave this trial open for now.\r\n\r\nThanks!", "Hi, \r\n\r\nThanks for reporting this.\r\n\r\nI've updated the master docs, which fixed this problem: \r\n\r\nhttps://www.tensorflow.org/versions/master/get_started/estimator\r\n\r\nAlso: 1.3 is the current stable branch. It's that 'versions' page is wrong -> fixed.\r\n"]}, {"number": 13565, "title": "Bug in Estimator tutorial?", "body": "This is the tutorial for the Estimator-class:\r\n\r\nhttps://www.tensorflow.org/extend/estimators\r\n\r\nYou have the following code:\r\n\r\n    my_nn = tf.estimator.DNNClassifier(feature_columns=[age, height, weight],\r\n                                       hidden_units=[10, 10, 10],\r\n                                       activation_fn=tf.nn.relu,\r\n                                       dropout=0.2,\r\n                                       n_classes=3,\r\n                                       optimizer=\"Adam\")\r\n\r\nand the following:\r\n\r\n    input_layer = tf.feature_column.input_layer(\r\n        features=features, feature_columns=[age, height, weight])\r\n\r\nIf I understand correctly, the feature-columns use the __variables__ `age`, `height` and `weight`. However, these __variables__ are not defined anywhere in the source-code for the tutorial.\r\n\r\nThe complete source-code is available in `abalone.py` as well:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/estimators/abalone.py\r\n\r\nBut here you no longer have the `input_layer` that uses the feature-columns. Instead you have the following which pulls out `\"x\"` from the `features`-dict:\r\n\r\n    # Connect the first hidden layer to input layer\r\n    # (features[\"x\"]) with relu activation\r\n    first_hidden_layer = tf.layers.dense(features[\"x\"], 10, activation=tf.nn.relu)\r\n\r\nSo I'm a bit confused how this is supposed to work?\r\n\r\nIn general, why don't you make the tutorials as Jupyter Notebooks instead? It would be immensely more helpful than your current tutorial style which is very confusing.\r\n\r\nAnd please remember, that for each hour you spend polishing the code, you will likely save several hours of head-aches for each person trying to understand your code. Multiplied by the many thousands of TensorFlow users, this is a tremendous amount of work-hours that is freed up for the community!\r\n", "comments": ["The reason for using files rather than notebooks is probably for testing reasons. IE, [test_tutorials.sh](https://github.com/tensorflow/tensorflow/blob/d040289dcf825fe2ac9579f4b0175a23833a57fa/tensorflow/tools/ci_build/builds/test_tutorials.sh) runs all the files and make sure they don't crash.", "@sandersk, can you comment why `input_layer` is not used in the source file?\r\n\r\nThe issue #11785 suggests having Jupyter Notebook tutorials. Pinging @wolffg on that issue.", "We're working on improved estimator docs now.  In the interim, another\nsource of information is Magnus's blog post:\nhttps://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html\n\nOn Mon, Oct 9, 2017 at 9:51 AM, Reed <notifications@github.com> wrote:\n\n> @sandersk <https://github.com/sandersk>, can you comment why input_layer\n> is not used in the source file?\n>\n> The issue #11785 <https://github.com/tensorflow/tensorflow/issues/11785>\n> suggests having Jupyter Notebook tutorials. Pinging @wolffg\n> <https://github.com/wolffg> on that issue.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13565#issuecomment-335215872>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AA1YDqgZMm-f4t9m7Yy6oJ72Ew9lPswzks5sqk8agaJpZM4PxsVR>\n> .\n>\n", "Looks like the docs may have gotten a bit out of sync with recent code updates to align with the latest version of the Estimator API. As wolffg mentioned, we are in the process of updating the estimator docs now.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Assigning to @wolffg based on the earlier note about working on improved estimator docs.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This doc has been replaced by these two files:\r\n\r\nhttps://www.tensorflow.org/versions/master/get_started/custom_estimators\r\nhttps://github.com/tensorflow/models/blob/master/samples/core/get_started/custom_estimator.py\r\n\r\nIt is 100% true we should remove the outdated sample still in tensorflow/tensorflow, but outdated sample cleanup is being tracked elsewhere, so I'm closing this.  Thanks for reporting this."]}, {"number": 13564, "title": "Add RDMA verbs configuration", "body": "RDAM configuration is now possible with the following environment variables:  RDMA_DEVICE, RDMA_DEVICE_PORT, RDMA_GID_INDEX,  RDMA_PKEY, RDMA_QUEUE_DEPTH, RDMA_TIMEOUT, RDMA_RETRY_CNT, RDMA_SL, RDMA_MTU and RDMA_TRAFFIC_CLASS.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@junshi15, @poxvoculi, @jhseu", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "I signed it!", "It looks good to me.", "All the authors have signed the CLA.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I'm ok okay with my commits being contributed to this project.", "Hi @poxvoculi @jhseu, \r\nHave you add the chance to take a look ?", "Thanks @poxvoculi @jhseu \r\nCan we go forward with the merge ? ", "@poxvoculi, we want to rearrange the commits of the PR (all of the commits are duplicated, probably something with the merges got mixed up). We will fix it tomorrow plz do not merge if you see this. ", "@drpngx @poxvoculi @jhseu - We fixed the duplicated commits and now all looks good.\r\nNo change in code since @poxvoculi review.\r\nThanks @noaezra @dariavel for the great work.", "@drpngx ? ", "Sounds good, let's see if the test suite passes.", "Could you pull rebase and push again? The different commits appear to be foiling the CLA bot.", "Jenkins, test this please.", "Thanks @gunan @drpngx \r\nI guess the cla/google is fooled by - commits were authored by someone other than the pull request submitter ? \r\nIf so, rebasing on top master on this PR will help ? \r\nShould we post a new PR with the changes ? ", "Yes rebase usually works\n\nOn Wed, Nov 1, 2017 at 1:28 AM, Ido Shamay <notifications@github.com> wrote:\n\n> Thanks @gunan <https://github.com/gunan> @drpngx\n> <https://github.com/drpngx>\n> I guess the cla/google is fooled by - commits were authored by someone\n> other than the pull request submitter ?\n> If so, rebasing on top master on this PR will help ?\n> Should we post a new PR with the changes ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/13564#issuecomment-341032108>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbVAisThR5OH-VtlhVmFGqJEoS_E7ks5syCuzgaJpZM4Pxp_k>\n> .\n>\n", "Jenkins, test this please.", "Looks like all the authors did sign the CLA, but CLA bot is confused because of multiple commits from different authors. Once the tests pass, I can merge this PR.", "@gunan , looks like there problem with the CI, not related to this patch.\r\nCan we re-run the jenkins ? ", "You are right. I think last night we ran out of daily quota on the backend, it should be fixed now."]}, {"number": 13563, "title": "What the wrong with sess.run ? ", "body": "I want to test a single test sample into a graph , but  unfortunately , I got nothing . If i just put a batch size of test sample ,  the result is good, why is it?  Furthermore , if i just copy a single test sample multiple times, the result is also nothing.\r\n\r\nx_reconstruction = sess.run(t.x_r, feed_dict={t.z_r: z_batch})\r\nx_reconstruction[0]\r\nOut[42]: \r\narray([ -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n        -1.00000000e+00,  -1.00000000e+00,  -1.00000000e+00,\r\n\r\nI just take the a test sample from z_batch, running the graph\r\n\r\nx_reconstruction_1 = sess.run(t.x_r, feed_dict={t.z_r: z_batch[0].reshape(1,2)})\r\nx_reconstruction_1\r\nOut[44]: \r\narray([[ -2.21053764e-01,  -2.20187426e-01,  -2.38173127e-01,\r\n         -2.24671751e-01,  -2.32440352e-01,  -2.28797898e-01,\r\n         -2.25955158e-01,  -2.28772879e-01,  -2.28901237e-01,\r\n         -2.22546220e-01,  -2.15402722e-01,  -2.31919050e-01,\r\n         -2.24671602e-01,  -2.24030137e-01,  -2.37917259e-01,\r\n         -2.35338598e-01,  -2.11188301e-01,  -2.30172306e-01,\r\n         -2.26653352e-01,  -2.27616981e-01,  -2.25351438e-01,\r\n         -2.26480648e-01,  -2.29957879e-01,  -2.28425398e-01,\r\n\r\nWhy is there a big difference between the same implementation?", "comments": ["x_reconstruction_1 = sess.run(t.x_r, feed_dict={t.z_r: z_batch[:2]})\r\nx_reconstruction_1\r\nOut[50]: \r\narray([[-1., -1., -1., ..., -1., -1., -1.],\r\n       [-1., -1., -1., ..., -1., -1., -1.]], dtype=float32)\r\nx_reconstruction_1.shape\r\nOut[51]: \r\n(2, 784)\r\n\r\nThe result is good again???    ", "if i just copy a single test sample multiple times, the result is also nothing.\r\n\r\nx_reconstruction_1 = sess.run(t.x_r, feed_dict={t.z_r: np.tile(z_batch[0],(100,1))})\r\nx_reconstruction_1\r\nOut[61]: \r\narray([[-0.22105289, -0.22018659, -0.23817246, ..., -0.23409246,\r\n        -0.22799131, -0.23101638],\r\n       [-0.22105289, -0.22018659, -0.23817246, ..., -0.23409246,\r\n        -0.22799131, -0.23101638],\r\n       [-0.22105289, -0.22018659, -0.23817246, ..., -0.23409246,\r\n        -0.22799131, -0.23101638],\r\n       ..., \r\n       [-0.22105289, -0.22018659, -0.23817246, ..., -0.23409246,\r\n        -0.22799131, -0.23101638],\r\n       [-0.22105289, -0.22018659, -0.23817246, ..., -0.23409246,\r\n        -0.22799131, -0.23101638],\r\n       [-0.22105289, -0.22018659, -0.23817246, ..., -0.23409246,\r\n        -0.22799131, -0.23101638]], dtype=float32)\r\n", "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\nIt must be a bug or a feature request.\r\nThe form below must be filled out.\r\nIt shouldn't be a TensorBoard issue. Those go here."]}]