[{"number": 9735, "title": "Tensorflow searching for outdated version of cudnn64_6.dll (Import Error)", "body": "So I tried my first install of tensorflow-gpu on Window 7x64 with a GTX1080 and was met with a DLL import error python exception that wasn't very helpful at all (that message needs to be updated to indicate what DLL it can't find!).\r\n\r\nAnyways, I decided to attempt the import tensorflow function with proc_mon watching all the file I/O from python... Turns out, Tensorflow is searching for 'cudnn64_5.dll' ... The latest 'cudnn-8.0-windows7-x64-v6.0' installs the following dll: 'cudnn64_6.dll' ... By copying the newer DLL to the DLL name that tensorflow expects, I was able to get past this issue and run tensorflow.\r\n\r\nTensorflow needs to be updated to support the latest cudnn64_6 dll that NVidia provides by default.", "comments": ["Thanks @iysaleh, noted.", "[...cuda_dnn.cc:352] Loaded runtime CuDNN library: 6021 (compatibility version 6000) but source was compiled with 5105 (compatibility version 5100).  If using a binary install, upgrade your CuDNN library to match.  If building from sources, make sure the library loaded at runtime matches a compatible version specified during compile configuration.\r\n\r\nI got the above error just now ... looks like just tricking it to use the newer library wasn't enough... I guess I'll try to install an older version until this is resolved.", "Hello man, I am looking for cudnn64_6.dll, can you send me an address where  I can down it? Thank you", "cudnn64_6.dll is included with 'cudnn-8.0-windows7-x64-v6.0' (as stated in the original description).\r\n\r\nTo get it, download that version of cudnn from the NVidia website: https://developer.nvidia.com/cudnn", "Thank you man\n\n2017-09-13 3:07 GMT+08:00 iysaleh <notifications@github.com>:\n\n> cudnn64_6.dll is included with 'cudnn-8.0-windows7-x64-v6.0' (as stated in\n> the original description).\n>\n> To get it, download that version of cudnn from the NVidia website:\n> https://developer.nvidia.com/cudnn\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9735#issuecomment-328949815>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT2kox6k73-o3N8gmu3lpNMhqaBfG5P8ks5shtaJgaJpZM4NTA9S>\n> .\n>\n", "Closing this because the DLL not found error has been improved to indicate what DLL is missing."]}, {"number": 9734, "title": "`tf.while_loop` Bug", "body": "I found that `tf.while_loop(cond...)` has issues on its termination condition which `cond` is set by using method `tf.logical_or(t1, t2)`. If one of conditions `t1` or `t2` is satisfied, then while loop should break. But I noticed this is not true. The following is the code for testing and reproducing:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\ndef condition(i, val):\r\n    # Termination condition.\r\n    return tf.logical_or(tf.less(i, 10), tf.less(val, 1))\r\n\r\n\r\ndef body(i, val):\r\n    # Mimic the body loop.\r\n    i += 1\r\n    val -= 0.2\r\n    return i, val\r\n\r\n# Build up graph.\r\ni = tf.Variable(0, trainable=False)\r\nvalue = tf.Variable(0.0, trainable=False)\r\ntrain_ops = tf.while_loop(condition, body, [i, value])\r\ninit_op = tf.global_variables_initializer()\r\n\r\n# Run tf session.\r\nsess = tf.Session()\r\nsess.run(init_op)\r\nprint(sess.run(train_ops))\r\n```\r\nThe code logic is quite simple: Inside the termination condition, either `i >= 10` or `value >= 1` is satisfied, the loop should be terminated. Since `i` increments by 1 each iteration with initial `0`, and second condition is always `True`, so it should terminate after the 10th iteration. However, if you run the above code, you could see the while loop goes infinity. I also tried other termination condition like `tf.logical_or(tf.less(i, 10), tf.less(val, -0.5))` and got the same result (but should terminate after the third iteration, in which `val` decreases by 0.2 each iteration). But if you try `cond` like`tf.logical_or(tf.less(i, 10), tf.less(val, -100))`, it would terminate at the 10th iteration which is also not expected, because `val` initially is 0, which is greater -100 at the beginning. This means that one of conditions for `tf.logical_or(t1, t2)` may have bugs when it works with `tf.while_loop()`.\r\n\r\nP.S The above code runs on MacOS 10.12.4 with Python 3.5, and the issue can be reproduced with Tensorflow 0.12/1.0/1.1", "comments": ["I don't think there's a bug here. Unless I'm mistaken, the function `condition` always evaluates to `true`. For a while loop to terminate, the condition must eventually evaluate to `false`.", "OK. It looks like we need a pass condition for `tf.while_loop()` instead of termination condition... going to close the ticket. "]}, {"number": 9733, "title": "Fix comments in EncodeTensorToByteBuffer", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 9732, "title": "better handling of MKL check in configure", "body": "not prompted to ask MKL download if the var is set already\r\nremove unexpected exit when input is given by user", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "PR merged. Thank you, @jhjin !"]}, {"number": 9731, "title": "More description for optimizers Adagrad, Adadelta, FTRL...", "body": "Is it necessary to add more description for optimizers such as Adagrad, Adadelta, FTRL and so on just as what [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) does? Since these are several quite new optimizers and I think it's better to show users more details about these optimizers so that they can understand why do these optimizers work better than SGD in some situations.\r\n\r\nIf more descriptions are welcomed, I'm glad to make new PRs to do this.\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@wolffg @aselle ", "What kind of extra language would you want?", "Just English, but to show more details of these optimization algorithms, such as how are the parameters updated, how do these methods adapt their learning rates.", "@wolffg I'll let you decide how much we want to do here.", "Hello, \r\nI have a question about \"AdagradOptimizer\", I created a node for the learning rates to be updated but I don't see any change from the initial learning rate. \r\n\r\nThis is the code for the optimizer within the Trainer object that I'm using\r\n\r\nself.learning_rate_node = tf.Variable(learning_rate)\r\noptimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate).minimize(self.net.cost, global_step=global_step)\r\n\r\nDo I have to specify something else in \"AdagradOptimizer\" for the learning rate to be updated? Do I have to define the decay that I want to use when I create \"self.learning_rate_node\"? Thanks in advance.\r\n\r\nBest regards, Amelia.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 9730, "title": "make usage of is_training more clear", "body": "This is the continuation of #9723.", "comments": []}, {"number": 9729, "title": "AttributeError: module 'tensorflow.contrib.seq2seq' has no attribute 'BeamSearchDecoder'", "body": "I don't know why I am getting this issue:\r\n\r\n ```\r\nAttributeError: module 'tensorflow.contrib.seq2seq' has no attribute 'BeamSearchDecoder'\r\n```\r\n\r\nI am using version 1.1.0, and BeamSearchDecoder is in the API master for Python: https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/seq2seq/BeamSearchDecoder", "comments": []}, {"number": 9728, "title": "Error code 1 : While installing TensorFlow on windows 10 with pip", "body": "I was trying to install TensorFlow  on windows 10 with pip \r\nWhen I type : \r\n>pip3 install tensorflow \r\n\r\nI get the following error : \r\n>ImportError : cannot import name 'setup'\r\n>Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\Dell\\AppData\\Local\\Temp\\pip-build-qzzubrm_\\protobuf\\\r\n", "comments": ["Could you check the resolution [here](https://github.com/python/mypy/issues/980)?", "Closing issue due to inactivity."]}, {"number": 9727, "title": "Python package for macOS missing.", "body": "The python 3 gpu package listed on: https://www.tensorflow.org/install/install_mac#the_url_of_the_tensorflow_python_package is missing.\r\n\r\n```\r\n$ curl https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.1.0-py3-none-any.whl\r\n<?xml version='1.0' encoding='UTF-8'?><Error><Code>NoSuchKey</Code><Message>The specified key does not exist.</Message></Error>\r\n```\r\n", "comments": ["Duplicate of #9610 "]}, {"number": 9726, "title": "Import errors ", "body": "### System information\r\n- Trying to run tutorial code\r\n- Win8.1\r\n- pip3 install --upgrade tensorflow \r\n- can't run programm to write version (can't run any code with import tensorflow)\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/home-pc/PycharmProjects/untitled6/b.py\", line 1, in <module>\r\n    import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\home-pc\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.", "comments": ["I have the same problem. I already tried this solution(#97 installign VS C++ 2015) but it did not help.", "@Androbin, nope this is also not helping me :/", "@Androbin it's solution for ubuntu, i using windows8.1", "It's a windows issue, most likely due to missing dependencies. Could you run the dependency walker on the `pywrap_internal_tensorflow` library? I am guessing it's CUDA. Please double check that you have those installed, including CUPTI.", "I'm not sure if this is it at all, but I believe the general consensus is that you don't attempt to use 32-bit python with Tensorflow on Windows... I only say that you are based on your folder name 'Python35-32'", "@drpngx, CUDA could not install, so i install tensorflow-cpu-only with anaconda and it's fix this problems. Example prject works, but now i have some warnings\r\n\r\n2017-05-07 14:37:35.667977: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-07 14:37:35.668324: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-07 14:37:35.668662: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-07 14:37:35.668993: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-07 14:37:35.669359: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-07 14:37:35.669736: W c:\\tf_jenkins\\home\\workspace\\release-win\\device\\cpu\\os\\windows\\tensorflow\\core\\platform\\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n\r\nWhat are these instructions? How i may use them?", "Don't worry about the warnings. They are just saying that we are not using the special SIMD instructions, so the code will be slower."]}, {"number": 9725, "title": "Patch/170506 misspell", "body": "Fixed some misspells :)", "comments": ["Can one of the admins verify this patch?", "English is hard.", "@vrv Thanks \ud83d\udc4d  Have a good weekend :)"]}, {"number": 9724, "title": "Freeze_graph results in very poor accuracy compared to manually exporting and freezing the graph?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**: 4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX 860M\r\n\r\n### Describe the problem\r\nFrom using the official freeze_graph.py file from TF, I am getting a very low accuracy in prediction as compared to manually exporting the graph using a file I wrote called `write_pb.py`, I get a much higher accuracy.\r\n\r\nTo be specific, here are the differences:\r\n\r\n**Differences:**\r\n1. Using `write_pb.py` to manually export the graph converted way more variables to constants, even with the same checkpoint files.\r\n2.  It takes a long, long time for `freeze_graph.py` to actually complete the exporting.\r\n3. Very importantly: I get a very low accuracy from using freeze_graph. Meanwhile, by exporting the graph manually, I get a nearly identical accuracy as if I predicted an image right from the checkpoint without freezing.\r\n4. Manually exporting the graph results in a smaller file size (just 1-2MB of difference).\r\n5. The manually exported graph has a faster inference time than the graph obtained from `freeze_graph.py`.\r\n\r\nThis is the freeze_graph.py file I got from the TF repo: https://gist.github.com/kwotsin/b9dae8246a30371a1a10690e2fa27cb7\r\n\r\nThis is the `write_pb` file I wrote:\r\nhttps://gist.github.com/kwotsin/8e43f5db4815e1f1af37da70d0933d8b\r\n\r\n\r\n### Source code / logs\r\nUsing freeze_graph with this command: `python freeze_graph.py --input_checkpoint=model.ckpt-18279 --input_graph=graph.pbtxt --output_graph=frozen_model_from_freeze_graph.pb --output_node_names=InceptionResnetV2/Logits/Predictions`\r\n\r\nI get this output:\r\n```\r\n2017-05-06 22:27:22.967898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-06 22:27:22.968268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 860M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.34GiB\r\n2017-05-06 22:27:22.968283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-06 22:27:22.968288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-06 22:27:22.968306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\r\n2017-05-06 22:27:33.499491: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-05-06 22:27:33.499519: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-06 22:27:33.509659: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ced220 executing computations on platform Host. Devices:\r\n2017-05-06 22:27:33.509678: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-06 22:27:33.509827: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-05-06 22:27:33.509837: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-06 22:27:33.512265: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ce6210 executing computations on platform CUDA. Devices:\r\n2017-05-06 22:27:33.512279: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\r\nConverted 490 variables to const ops.\r\n7871 ops in the final graph.\r\n```\r\nMeanwhile, if I manually freeze the graph using `write_pb.py`,\r\n\r\nI get this output:\r\n```\r\n2017-05-06 22:39:00.197711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-06 22:39:00.198096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 860M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.29GiB\r\n2017-05-06 22:39:00.198120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-06 22:39:00.198125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-06 22:39:00.198143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\r\n2017-05-06 22:39:00.709417: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-05-06 22:39:00.709452: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-06 22:39:00.710238: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef16320 executing computations on platform Host. Devices:\r\n2017-05-06 22:39:00.710252: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-06 22:39:00.710374: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-05-06 22:39:00.710384: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-06 22:39:00.710645: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef18180 executing computations on platform CUDA. Devices:\r\n2017-05-06 22:39:00.710654: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\r\nExporting graph...\r\nConverted 898 variables to const ops.\r\n```\r\n\r\n\r\n\r\n\r\n\r\n", "comments": ["It's hard to say without knowing how the graph was exported, but in your script you set `is_training=False` when creating the Slim model. This will create a different model with some training-only variables and ops removed, which can be important for good results when running the graph.", "Same Problem.   \r\n@petewarden \r\n\r\nI use freeze_graph to export my model to a file named \"frozen.pb\". But Found that the accuracy of predictions on frozen.pb is very poor.\r\n\r\nI know the problem maybe MovingAverage not included in frozen.pb or not loaded in prediction.\r\n\r\nWhen I use model.ckpt files to restore model for evaluating, if i call tf.train.ExponentialMovingAverage(0.999); variables_to_restore = variable_averages.variables_to_restore(); saver = tf.train.Saver(variables_to_restore) , then the accuracy is good as expected, else the accuracy is bad.\r\n\r\nSo How To export a binary model which performance is the same as the one restored from checkpoint files? I want to use \".pb\" files in Android Devices.\r\n\r\nThank you!\r\n\r\nFreeze Command:\r\n```\r\n~/bazel-bin/tensorflow/python/tools/freeze_graph \\\r\n  --input_graph=./graph.pbtxt \\\r\n  --input_checkpoint=./model.ckpt-100000 \\\r\n  --output_graph=frozen.pb \\\r\n  --output_node_names=output  \\\r\n  --restore_op_name=save/restore_all \\\r\n  --clear_devices\r\n```\r\n[More code at StackOverflow](https://stackoverflow.com/questions/44690363/how-to-use-tf-train-exponentialmovingaverage-in-android-ios)\r\n", "@kwotsin \r\nThank you !   Your code is also helpful for me, it works.", "I had the exact same issue and have been hopelessly confused about this, thinking that the `freeze_graph.py` script couldn't be the breaking point.\r\n\r\nIn my case, fine-tuning off MobileNet, the accuracy dropped to the abyss and the network outputted form `freeze_graph.py` was extremely degenerate \u2013 tiny variations in the input image would lead to different top classes being predicted. I printed and various weights and they matched.\r\n\r\nI confirmed @petewarden's suspicion that if a network exported directly (like in @kwotsin's script) works iff is_training=False is passed. I haven't debugged further, but even if this doesn't have much to do with the `freeze_graph.py` script itself, some docs should probably be adjusted.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!", "Is there a fix for this issue yet. having the same problem. I've tried @kwotsin's script . It still gives me inaccurate results when loaded with the graph.pb file\r\n", "Hello!\r\nI've the same problem!\r\n@Karthik777  any news?", "I meet the same problem ,any solutions?", "@ambr89 : I fixed it by importing meta graph and applying transformations on graph with TransformGraph.\r\n```\r\ninput_checkpoint = \"chckpoint path\"\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n        # We import the meta graph in the current default Graph\r\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n\r\n        # We restore the weights\r\n        saver.restore(sess, input_checkpoint)\r\n\r\n        from tensorflow.tools.graph_transforms import TransformGraph\r\n        transforms = ['add_default_attributes',\r\n                      'remove_nodes(op=Identity, op=CheckNumerics)',\r\n                      'fold_batch_norms', 'fold_old_batch_norms',\r\n                      'strip_unused_nodes', 'sort_by_execution_order']\r\n        transformed_graph_def = TransformGraph(tf.get_default_graph().as_graph_def(),'Placeholder', output_node_names.split(\",\"), transforms)\r\n\r\n# We use a built-in TF helper to export variables to constants\r\n       output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess,  # The session is used to retrieve the weights\r\n            transformed_graph_def,  # The graph_def is used to retrieve the nodes\r\n            output_node_names.split(\",\")  # The output node names are used to select the useful nodes\r\n        )\r\n      with tf.gfile.GFile(\"optimised_model.pb\", \"wb\") as f:\r\n            f.write(output_graph_def.SerializeToString())\r\n```\r\nChange Placeholder to input node name and fill in your output nodes.", "> @ambr89 : I fixed it by importing meta graph and applying transformations on graph with TransformGraph.\r\n> \r\n> ```\r\n> input_checkpoint = \"chckpoint path\"\r\n> with tf.Session(graph=tf.Graph()) as sess:\r\n>         # We import the meta graph in the current default Graph\r\n>         saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n> \r\n>         # We restore the weights\r\n>         saver.restore(sess, input_checkpoint)\r\n> \r\n>         from tensorflow.tools.graph_transforms import TransformGraph\r\n>         transforms = ['add_default_attributes',\r\n>                       'remove_nodes(op=Identity, op=CheckNumerics)',\r\n>                       'fold_batch_norms', 'fold_old_batch_norms',\r\n>                       'strip_unused_nodes', 'sort_by_execution_order']\r\n>         transformed_graph_def = TransformGraph(tf.get_default_graph().as_graph_def(),'Placeholder', output_node_names.split(\",\"), transforms)\r\n> \r\n> # We use a built-in TF helper to export variables to constants\r\n>        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n>             sess,  # The session is used to retrieve the weights\r\n>             transformed_graph_def,  # The graph_def is used to retrieve the nodes\r\n>             output_node_names.split(\",\")  # The output node names are used to select the useful nodes\r\n>         )\r\n>       with tf.gfile.GFile(\"optimised_model.pb\", \"wb\") as f:\r\n>             f.write(output_graph_def.SerializeToString())\r\n> ```\r\n> Change Placeholder to input node name and fill in your output nodes.\r\n\r\nYes, you're right to do that. This particular function literally just converts all the variables into constants without doing any kind of transformations to it. This is a bug I found to mess completely with my accuracy as well. So, it expects a subgarph to be fed in with only those inputs and outputs that are necessary. I suggest using [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) since this strips the graphs of any Restore Tensor ops and file loading calls. ", "Thank you @ktiwary2 for your solution. I still have difficulty to find proper output nodes of SSD-Mobilenet when I loaded it from checkpoints. There are lots of nodes there 5000+ which I cant check them one by one. I checked some 1000+ last layers, but could not found. Any idea for this?  -Thankyou ", "> Thank you @ktiwary2 for your solution. I still have difficulty to find proper output nodes of SSD-Mobilenet when I loaded it from checkpoints. There are lots of nodes there 5000+ which I cant check them one by one. I checked some 1000+ last layers, but could not found. Any idea for this? -Thankyou\r\n\r\nI would use the `summarize_graph` feature in graph transforms. Here's a [link](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md). it will basically give you all possible input/output nodes. ", "It seems this takes frozen graph as input, I wanted the same thing for meta graph \"checkpoint.meta\" file.\r\nDo we have the same thing for meta graphs?\r\n", "> @kwotsin\r\n> Thank you ! Your code is also helpful for me, it works.\r\n\r\nhi,  please tell me how to solve this probleme, i tried many solutions but it is not useful", "> @kwotsin\r\n> Thank you ! Your code is also helpful for me, it works.\r\n\r\n\r\n\r\n> Same Problem.\r\n> @petewarden\r\n> \r\n> I use freeze_graph to export my model to a file named \"frozen.pb\". But Found that the accuracy of predictions on frozen.pb is very poor.\r\n> \r\n> I know the problem maybe MovingAverage not included in frozen.pb or not loaded in prediction.\r\n> \r\n> When I use model.ckpt files to restore model for evaluating, if i call tf.train.ExponentialMovingAverage(0.999); variables_to_restore = variable_averages.variables_to_restore(); saver = tf.train.Saver(variables_to_restore) , then the accuracy is good as expected, else the accuracy is bad.\r\n> \r\n> So How To export a binary model which performance is the same as the one restored from checkpoint files? I want to use \".pb\" files in Android Devices.\r\n> \r\n> Thank you!\r\n> \r\n> Freeze Command:\r\n> \r\n> ```\r\n> ~/bazel-bin/tensorflow/python/tools/freeze_graph \\\r\n>   --input_graph=./graph.pbtxt \\\r\n>   --input_checkpoint=./model.ckpt-100000 \\\r\n>   --output_graph=frozen.pb \\\r\n>   --output_node_names=output  \\\r\n>   --restore_op_name=save/restore_all \\\r\n>   --clear_devices\r\n> ```\r\n> [More code at StackOverflow](https://stackoverflow.com/questions/44690363/how-to-use-tf-train-exponentialmovingaverage-in-android-ios)\r\nhi,  can you  tell me how to solve this problem, i tried many solutions but it si not useful"]}, {"number": 9723, "title": "suggest is_training not known at construction time", "body": "", "comments": ["Can one of the admins verify this patch?", "@vrv My concern is, that the phrase \"being **built** for training\" suggests knowledge of `is_training` at **construction** time. This is why I proposed replacing \"**build**\" with \"**run**\".", "Oh, sorry, I didn't exactly parse the commit message properly, and the existing\r\ndocumentation was already pretty confusing -- how about:\r\n\r\n* Layers that behave differently during training should take\r\n   an `is_training` boolean Tensor as input to conditionally\r\n   choose different computations paths (e.g., using `tf.cond`)\r\n   during execution.\r\n\r\nI can send the PR if you'd like, but would like to give you the opportunity first.\r\n\r\n", "@vrv **SGTM** but it is good style to accept both Python `Booleans` and `bool` `Tensor`s.", "A python boolean would make it a static graph-build-time decision, not a dynamic one though, but I guess that's up to the caller to decide.  (Will you send the PR?)", "Continued at #9730"]}, {"number": 9722, "title": "How to know which will be supported wheel for my platform? (Installing Tensorflow)", "body": "C:\\Users\\Sudhit>pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.1.0-cp35-cp35m-win_amd64.whl\r\ntensorflow-1.1.0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.\r\n\r\nhow will I know which will be supported wheel for my platform?", "comments": ["@yifeif could you clarify? If it's not already in the docs, maybe we can improve there.", "I ran into this error while attempting to install Tensorflow ... For me it was because I was trying to install Tensorflow in Windows with Python 3.6 ... Tensorflow only supports Python3.5!", "@sudhirshahu51 , you can check what wheel tags are compatible on your system with the following command:\r\n`python -c \"from pip import pep425tags;print(pep425tags.supported_tags)\"`, or python3. \r\n\r\nWe don't plan to add this to our doc as this should be strictly following pep425 standard.", "@yifeif thanx for this and can I know what features of Laptop do this supported tags depend upon as in my case I am finding it to be python2 and py27", "reinstall tensorflow based on python 3.5xx and not 3.6 which is the default for anaconda\r\nhttp://www.stefangordon.com/install-tensorflow-in-anaconda-on-windows/", "Thanks @yifeif \r\n", "@yifeif  it doesn't work for me :/\r\n\r\n", "@SofiaAmel what error/issue are you getting?", "it doesn't work  for me too:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nImportError: cannot import name 'pep425tags'\r\n```", "@yifeif  the issue was \" this wheel is not compatible with the platform \" , but now evrything is working ! I downgrade my TF version with pip install tensorflow==1.6 and it works , I did the same for TensorBoard, but unfortunately I'm having a new issue, and it's about prediction , it's in the picture below, I'll be grateful if you can help. thanks in advance\r\n![image](https://user-images.githubusercontent.com/25504507/39185900-10518ce4-47c9-11e8-9b56-389280b897d3.png)\r\n", "Same issue here -- When installing on ARMv7l platform:\r\n    `pip install tensorflow-1.9.0rc0-cp34-none-linux_armv7l.whl `\r\n    _tensorflow-1.9.0rc0-cp34-none-linux_armv7l.whl is not a supported wheel on this platform._\r\nMy pep425 tags are as follows (thank you @yifeif!):\r\n    _[('cp35', 'cp35m', 'linux_armv7l'), ('cp35', 'abi3', 'linux_armv7l'), ('cp35', 'none', 'linux_armv7l'),\r\n     ('cp34', 'abi3', 'linux_armv7l'), ('cp33', 'abi3', 'linux_armv7l'), ('cp32', 'abi3', 'linux_armv7l'), \r\n     ('py3', 'none', 'linux_armv7l'), ('cp35', 'none', 'any'), ('cp3', 'none', 'any'), ('py35', 'none', 'any'),\r\n     ('py3', 'none', 'any'), ('py34', 'none', 'any'), ('py33', 'none', 'any'), ('py32', 'none', 'any'), \r\n     ('py31', 'none', 'any'), ('py30', 'none', 'any')]_\r\nDoesn't ('cp34', 'abi3', 'linux_armv7l') match the wheel?  The target OS is Rasbian stretch.  The wheel is from [http://ci.tensorflow.org/view/Nightly/job/nightly-pi-python3/].  Many thanks for your input!\r\n\r\n\r\n\r\n", "@nikisalli @yifeif @SofiaAmel \r\nTry this one(pip18, python3.6.6):\r\n`python3 -c \"import wheel.pep425tags as w; print(w.get_supported())\" | sed -zE 's/\\),/),\\n/g'`", "Here's my modification, for those who found this via web search, like me:\r\n```\r\npython -c \"import wheel.pep425tags as w; print('\\n'.join(['-'.join(p) for p in w.get_supported()]))\"\r\n```", "reinstall tensorflow based on python 3.5xx and not 3.6 which is the default for anaconda\r\nhttp://www.stefangordon.com/install-tensorflow-in-anaconda-on-windows/", "pygame-2.1.0: pygame version\r\n-cp39-cp39-: python version\r\nwin32.: system 32bit\r\nwhl: file type\r\ntype python to get info .\r\nIt maybe works."]}, {"number": 9721, "title": "Feature request: Add 'axis' option for 'tf.boolean_mask()'", "body": "`tf.boolean_mask()` is extremely useful when it comes to training.\r\n\r\nHowever, the present `tf.boolean_mask()` does not have an `axis` option, thus masking some specific dimension of an array could be troublesome. \r\n\r\nAppreciate your consideration.", "comments": ["This is a good feature request, but unfortunately it's blocked by the fact that gather is also missing an axis parameter.", "Added a PR #11558 for `axis` support of `boolean_mask()` as now `gather` supports `axis` through (#11223)"]}, {"number": 9720, "title": "tf.while_loop swap_memory=True when not running train_op, does it always swap?", "body": "From the documentation of `tf.while_loop`:\r\n> For training, TensorFlow remembers the tensors that are produced in the forward inference but needed in back propagation. These tensors can be a main source of memory consumption and often cause OOM problems when training on GPUs. When the flag swap_memory is true, we swap out these tensors from GPU to CPU. This for example allows us to train RNN models with very long sequences and large batches.\r\n\r\nBut **after** having trained, is TensorFlow's engine smart enough to avoid swapping even if `swap_memory=True` or should [optimize_for_inference.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) flip swap_memory (i.e. make the equivalent modifications to the graph)?\r\n\r\nNaturally this only applies to running inference on a GPU.\r\n\r\n@yuanbyu", "comments": ["@yuanbyu @hawkinsp could you take a look at the first question?  If the answer is no, we could reassign to someone to take a look at `optiomize_for_inference.py` or related tools.", "while_loop will not do any memory swapping if it is inference only. In training, it only performs swapping on tensors that are produced in the forward but used in the backprop.", "Thanks for clarifying! I hope this could be briefly mentioned in the documentation of tf.while_loop as well (e.g. that caching outputs during a forward pass is only done if tf.Session.run will be needing it for gradients and backprop) as it's not a clear behavior from a pure programming perspective in the more general case not related to machine learning."]}, {"number": 9719, "title": "Fix mixed use of bytes and unicode string in gc.py", "body": null, "comments": ["Can one of the admins verify this patch?", "Perhaps you could write a test for this so it doesn't regress?\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md#general-guidelines-and-philosophy-for-contribution", "I wrote a test. Could you run it?", "Jenkins, test this please.", "I cannot access to ci.tensorflow.org to see what is wrong with my code. My Chrome shows NET::ERR_CERT_REVOKED. What is going on?", "Can one of the admins verify this patch?", "Jenkins, test this please.\r\n", "Fixed an error.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@raviqqe The test seems to be failing, can you check?", "Yes, I did.", "@tensorflow-jenkins test this please", "@raviqqe it appears there are some linter errors in your change, please fix:\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/4357/consoleFull", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please", "@raviqqe looks like you haave a few more bugs to squash...", "@raviqqe Nope, unit test failing.", "@tensorflow-jenkins test this please", "Is the error related to this change? Should I rebase this branch to master or do something?", "@raviqqe nevermind, the failures in the latest tests are unrelated to your change. This is ready to merge. Thanks for the contribution.", "Finally! Thank you for your assistance!"]}, {"number": 9718, "title": "compile error C2064 when I use cmake(vs2015) to make windows binary", "body": "I follows the instructions (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake  ) to compile the tensorflow in windows ( compiler vs2015)\r\n\r\nI use \r\n    cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=\"D:/Program Files/Python 3.5/python.exe\" -DPYTHON_LIBRARIES=\"D:/Program Files/Python 3.5/libs/python35.lib\" -Dtensorflow_ENABLE_GPU=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=OFF  -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX\r\nto generate project files.\r\n\r\nafter: MSBuild /p:Configuration=Release tf_tutorials_example_trainer.vcxproj\r\n\r\nit complains following :\r\n\r\n[in Chinese , the cl output]\r\nh:\\tensorflow_c\\tensorflow-1.1.0\\tensorflow\\core\\lib\\gtl\\array_slice_internal\r\n.h(89): error C2064: \u9879\u4e0d\u4f1a\u8ba1\u7b97\u4e3a\u63a5\u53d7 0 \u4e2a\u53c2\u6570\u7684\u51fd\u6570 (\u7f16\u8bd1\u6e90\u6587\u4ef6 H:\\tensorflow_c\r\n\\tensorflow-1.1.0\\tensorflow\\core\\lib\\strings\\str_util.cc) [H:\\tensorflow_c\\tensorflow-1.1.0\\tensorflow\\contrib\\cmake\\build_vc\\tf_core_lib.vcxproj]\r\n\r\n[in English ]\r\nh:\\tensorflow_c\\tensorflow-1.1.0\\tensorflow\\core\\lib\\gtl\\array_slice_internal\r\n.h(89): error C2064: term does not evaluate to a function taking 0 arguments ( compiling  H:\\tensorflow_c\r\n\\tensorflow-1.1.0\\tensorflow\\core\\lib\\strings\\str_util.cc) [H:\\tensorflow_c\\tensorflow-1.1.0\\tensorflow\\contrib\\cmake\\build_vc\\tf_core_lib.vcxproj]\r\n  ", "comments": ["CC @guschmue @mrry \r\n\r\nThe error is in the array slice implementation. We'd like to see the call site. Could you put the full compiler log in pastebin?", "I find the bugs come from the vs2015. When I update the vs2015 to vs2015update3 version , It compiles OK.\r\nI suggest that the instructions ( (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake )  need to emphasize the use of **vs2015 update3** ", "Thanks! Could you send a PR with the change to the MD file?", "yes, needs to be update3 for sure. I thought this was in the README.md but just checked - really not there."]}, {"number": 9717, "title": "feature request: support placeholder for parameter k in tf.nn.in_top_k", "body": "Currently [`tf.nn.in_top_k`](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) requires an int as `k`, while [`tf.nn.top_k`](https://www.tensorflow.org/api_docs/python/tf/nn/top_k) does support a 0-D int32 Tensor as parameter `k`. This is kind of inconsistent and inconvenient.\r\n\r\nIs it possible to allow `k` to be a 0-D int32 Tensor in `tf.nn.in_top_k`?", "comments": ["Yes, it should be [supported](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/topk_op.cc#L37).\r\n\r\nI think it's just a matter of removing `k=` in the [code](https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1946).\r\n\r\n@girving WDYT?", "Yes, this requires a similar `InTopKV2` op to the `TopKV2` op I added.\r\n\r\n@soloice Would be up for submitting a PR?", "@girving Okay, I'll have a try.", "Thanks!  Let me know if you have questions.", "Added a PR #11197 for this issue.", "@yongtang Thanks for your contribution. I'm kind of busy these days and almost forget this issue..."]}, {"number": 9716, "title": "applying different batch_sizes for each buckets in bucketd_rnn?", "body": "I've found an obvious fact that, in seq2seq model with buckets(especially in NMT applications), the shorter buckets takes extremely smaller memory size in GPU than the longer buckets in same batch_size in both training and inferencing; i.e 64 sentences for (5, 10) takes extremely smaller memory size than 64 sentences for (30, 40).\r\n\r\nThen why don't we have different bucket sizes for each bucket? \r\n\r\nActually i've run my experiments in these way and it really helps cutting down my training time.\r\n\r\nThere may be some issues in having different bucket sizes like sampling frequency for each buckets, the issue from recent paper in ICLR that claiming the smaller batch size make the model better in generalization, etc...\r\n\r\nYet, my preemptive concern was the training time so i managed different batch sizes like 1024 sentences for (5, 10) bucket and 64 sentences for (30, 40) bucket that maximize my GPU's utility.\r\n\r\n(absolutely this cannot be done with dynamic_rnn structure.)\r\n\r\nwhat do you guys think about this idea?", "comments": ["Sorry, I don't exactly follow what the specific questions here are.  The ideas you describe are perfectly reasonable, and I think they are widely applied in the context of sequence models.  Are you trying to file a specific bug against some RNN implementation in TensorFlow?", "no, this was not a question or feature request.\r\n\r\ni'm just opening a discussion thread curious about what others thinking.\r\n\r\ni guess this wasn't appropriate in this community and closing the thread.", "@yhg0112 Thanks.  As mentioned I think using different batch sizes for different buckets is totally reasonable.  People usually analyze the sequence lengths over the training set, and could cut off at, say, 20%, 40%, ..., 80%-tiles, etc."]}, {"number": 9715, "title": "[inputs] instead of inputs for rnn_cell?", "body": "In current rnn_cell implementation, It gets inputs as 2-d tensor for cell inputs,\r\n\r\nHence embedding layer before the actual rnn_cell gets a little bit annoying for making 'decoder'.\r\n\r\nI really like the \"wrapper\" concept in current implementation that expand by each layer with embedding, residual net, or dropouts. \r\n\r\nIf rnn_cell gets list of '2-d' tensor like [inputs1, inputs2, ...], then the concept of wrapper can be fully used in decoder as well as simple rnn or encoder;\r\n\r\nfor a little more concretely, \r\nif \"embedding wrapped cell\" gets input list [inputs, context_inputs_from_attention],\r\nthen the \"embedding wrapped cell\" can just pass [embedded_inputs, context_inputs_from_attention] to the real \"LSTM_cell\" and \"LSTM_cell\" can weight sum those inputs in inputs list.\r\n\r\nwhat do you think about this idea?", "comments": ["@ebrevdo does that make sense?\r\n\r\n@yhg0112 would you be willing to send a PR?", "Does the new attention + decoder API in tf.contrib.seq2seq (in the\nnightlies) do what you want?\n\nIt seems like what you're currently proposing is a pretty major overhaul of\nthe LSTM and embedding wrapper APIs that would not be backwards\ncompatible.  I think we try to avoid doing this in the new AttentionWrapper\nin tf.contrib.seq2seq, and provide a variety of decoding mechanisms there\nas well.\n\nOn Sat, May 6, 2017 at 12:06 PM, drpngx <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> does that make sense?\n>\n> @yhg0112 <https://github.com/yhg0112> would you be willing to send a PR?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9715#issuecomment-299659814>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1QWmzf7BDeEFTeAkTGhU93XUSLDks5r3MS3gaJpZM4NSpk6>\n> .\n>\n", "do you mean BeamSearchDecoder in tf.contrib.seq2seq.python.ops ? \r\nThis do the job yet require \"embedding\" outside the \"cell\" (same as attention_decoder in legacy_seq2seq). \r\ni was thinking that this is kind of Not in the OOP because we had the embeddingWrapper for RNN cells.\r\n\r\ni'll make the pull request about next month. and i'm closing this issue for now will reopen when i'm ready to submit the PR."]}, {"number": 9714, "title": "Tensorflow causes SIGSEGV in numpy", "body": "x_train Examples Loaded = (55000, 784)\r\ny_train Examples Loaded = (55000, 10)\r\n\r\n[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\r\n[New Thread 0x7ffff2211700 (LWP 6011)]\r\n[New Thread 0x7ffff2a12700 (LWP 6012)]\r\n[New Thread 0x7fffd33a4700 (LWP 6013)]\r\n[Thread 0x7fffd33a4700 (LWP 6013) exited]\r\n[Thread 0x7ffff2a12700 (LWP 6012) exited]\r\n[Thread 0x7ffff2211700 (LWP 6011) exited]\r\n\r\nProgram received signal SIGSEGV, Segmentation fault.\r\n0x00007ffff716602f in _int_free (av=0x7ffff74a8760 <main_arena>, \r\n    p=<optimized out>, have_lock=0) at malloc.c:3996\r\n3996\tmalloc.c: No such file or directory.\r\n\r\ngdb backtrace report\r\n\r\n(gdb) bt\r\n#0  0x00007ffff716602f in _int_free (av=0x7ffff74a8760 <main_arena>, \r\n    p=<optimized out>, have_lock=0) at malloc.c:3996\r\n#1  0x00007ffff2ad9ef0 in ?? ()\r\n   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0\r\n#2  0x00007ffff2ad4770 in ?? ()\r\n   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0\r\n#3  0x00007ffff2ad486a in ?? ()\r\n   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0\r\n#4  0x00007ffff2a28e09 in ?? ()\r\n   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0\r\n#5  0x00007ffff2a26e7f in ?? ()\r\n   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..0\r\n#6  0x00000000009649e0 in ?? ()\r\n#7  0x00007ffff521f1c8 in __frame_dummy_init_array_entry ()\r\n   from /home/shreeranga/PP/Exp/venvs/lib/python2.7/site-packages/numpy/core/..o\r\n#8  0x00007fffffffdc00 in ?? ()\r\n#9  0x00007ffff2ae7cd1 in ?? ()\r\n", "comments": ["It's a SEGV, which looks like numpy. Could you answer the following:\r\n\r\n* What tensorflow version are you using? Which OS?\r\n* Can you build from source with debug symbols and try again?\r\n* Were you loading ops or using something in `tf.contrib`?\r\n* Any small repro?", "I'm using tensorflow 1.1.0 and my OS is Ubuntu 14.04\r\nI couldn't build from the source. However, I uninstalled tensorflow and pip installed again. Everything seems to be working now.", "OK, please reopen if it happens again and you have a way to repro."]}, {"number": 9713, "title": "Exception during running the graph: Unable to get element from the feed as bytes.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\ngcloud ml-engine local predict \\\r\n    --model-dir=$MODEL_DIR \\\r\n    --json-instances=\"$DATA_DIR/test2.json\" \\\r\n\r\n### Describe the problem\r\nI'm using a beam pipeline to preprocess my text to integers bag of words, similar to this example https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/reddit_tft/reddit.py\r\n\r\n      words = tft.map(tf.string_split, inputs[name])\r\n      result[name + '_bow'] = tft.string_to_int(\r\n          words, frequency_threshold=frequency_threshold)\r\n\r\nPreprocessing and training seem to work fine. I train a simple linear model and point to the transform function and run an experiment. \r\n\r\nWhen I try to run a prediction I get an error, no idea what I'm doing wrong.\r\n\r\n### Source code / logs\r\nWARNING:root:MetaGraph has multiple signatures 2. Support for multiple signatures is\r\n limited. By default we select named signatures.\r\nERROR:root:Exception during running the graph: Unable to get element from the feed a\r\ns bytes.\r\nTraceback (most recent call last):\r\n  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 136, in <mo\r\ndule>\r\n    main()\r\n  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 131, in mai\r\nn\r\n    instances=instances)\r\n  File \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\r\ne_sdk/prediction/prediction_lib.py\", line 656, in local_predict\r\n    _, predictions = model.predict(instances)\r\n  File \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\r\ne_sdk/prediction/prediction_lib.py\", line 553, in predict\r\n    outputs = self._client.predict(columns, stats)\r\n  File \"/Users/xyz/Downloads/google-cloud-sdk/lib/third_party/cloud_ml_engin\r\ne_sdk/prediction/prediction_lib.py\", line 382, in predict\r\n    \"Exception during running the graph: \" + str(e))\r\nprediction_lib.PredictionError: (4, 'Exception during running the graph: Unable to g\r\net element from the feed as bytes.')\r\n\r\n    def feature_columns(vocab_size=100000):\r\n        result = []\r\n        for key in TEXT_COLUMNS:\r\n            column = tf.contrib.layers.sparse_column_with_integerized_feature(key, vocab_size, combiner='sum')\r\n        result.append(column)\r\n    return result\r\n\r\n    model_fn = tf.contrib.learn.LinearClassifier(\r\n          feature_columns=feature_columns(),\r\n          n_classes=15,\r\n          model_dir=output_dir\r\n        )  \r\n\r\n    def get_transformed_reader_input_fn(transformed_metadata,\r\n                                        transformed_data_paths,\r\n                                        batch_size,\r\n                                        mode):\r\n      \"\"\"Wrap the get input features function to provide the runtime arguments.\"\"\"\r\n      return input_fn_maker.build_training_input_fn(\r\n          metadata=transformed_metadata,\r\n          file_pattern=(\r\n              transformed_data_paths[0] if len(transformed_data_paths) == 1\r\n              else transformed_data_paths),\r\n          training_batch_size=batch_size,\r\n          label_keys=[LABEL_COLUMN],\r\n          reader=gzip_reader_fn,\r\n          key_feature_name='key',\r\n          reader_num_threads=4,\r\n          queue_capacity=batch_size * 2,\r\n          randomize_input=(mode != tf.contrib.learn.ModeKeys.EVAL),\r\n          num_epochs=(1 if mode == tf.contrib.learn.ModeKeys.EVAL else None))\r\n\r\n\r\n    transformed_metadata = metadata_io.read_metadata(\r\n        args.transformed_metadata_path)\r\n    raw_metadata = metadata_io.read_metadata(args.raw_metadata_path)\r\n\r\n    train_input_fn = get_transformed_reader_input_fn(\r\n        transformed_metadata, args.train_data_paths, args.batch_size,\r\n        tf.contrib.learn.ModeKeys.TRAIN)\r\n\r\n    eval_input_fn = get_transformed_reader_input_fn(\r\n        transformed_metadata, args.eval_data_paths, args.batch_size,\r\n        tf.contrib.learn.ModeKeys.EVAL)\r\n\r\n    serving_input_fn = input_fn_maker.build_parsing_transforming_serving_input_fn(\r\n            raw_metadata,\r\n            args.transform_savedmodel,\r\n            raw_label_keys=[],\r\n            raw_feature_keys=model.TEXT_COLUMNS)\r\n\r\n    export_strategy = tf.contrib.learn.utils.make_export_strategy(\r\n        serving_input_fn,\r\n        default_output_alternative_key=None,\r\n        exports_to_keep=5,\r\n        as_text=True)\r\n\r\n    return Experiment(\r\n        estimator=model_fn,\r\n        train_input_fn=train_input_fn,\r\n        eval_input_fn=eval_input_fn,\r\n        export_strategies=export_strategy,\r\n        eval_metrics=model.get_eval_metrics(),\r\n        train_monitors=[],\r\n        train_steps=args.train_steps,\r\n        eval_steps=args.eval_steps,\r\n        min_eval_frequency=1\r\n    )\r\n\r\n", "comments": ["@rhaertel80 might have some additional insight.\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9711, "title": "eval() error about variable_scope()", "body": "I run the code from official tutorial:\r\nhttps://www.tensorflow.org/programmers_guide/variable_scope\r\n\r\n```\r\nwith tf.variable_scope(\"foo\", initializer=tf.constant_initializer(0.4)):\r\n    v = tf.get_variable(\"v\", [1])\r\n    assert v.eval() == 0.4  # Default initializer as set above.\r\n    w = tf.get_variable(\"w\", [1], initializer=tf.constant_initializer(0.3))\r\n    assert w.eval() == 0.3  # Specific initializer overrides the default.\r\n    with tf.variable_scope(\"bar\"):\r\n        v = tf.get_variable(\"v\", [1])\r\n        assert v.eval() == 0.4  # Inherited default initializer.\r\n    with tf.variable_scope(\"baz\", initializer=tf.constant_initializer(0.2)):\r\n        v = tf.get_variable(\"v\", [1])\r\n        assert v.eval() == 0.2  # Changed default initializer\r\n```\r\n**the problem as below**:\r\nCannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`\r\n\r\n```\r\nwith tf.Session():\r\n    with tf.variable_scope(\"foo\", initializer=tf.constant_initializer(0.4)):\r\n        v = tf.get_variable(\"v\", [1])\r\n        assert v.eval() == 0.4  # Default initializer as set above.\r\n        w = tf.get_variable(\"w\", [1], initializer=tf.constant_initializer(0.3))\r\n        assert w.eval() == 0.3  # Specific initializer overrides the default.\r\n        with tf.variable_scope(\"bar\"):\r\n            v = tf.get_variable(\"v\", [1])\r\n            assert v.eval() == 0.4  # Inherited default initializer.\r\n        with tf.variable_scope(\"baz\", initializer=tf.constant_initializer(0.2)):\r\n            v = tf.get_variable(\"v\", [1])\r\n            assert v.eval() == 0.2  # Changed default initializer\r\n```\r\n\r\n**when i try with session the problem changed as below:**\r\n\r\nAttempting to use uninitialized value foo/v\r\n\r\n\t [[Node: foo/v/_0 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_4_foo/v\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](foo/v)]]\r\n\r\nthis problem in both windows and linux\r\nthe platform is :\r\nwindows10 , python3.5 , tf.__version__ 0.12.1\r\nubuntu, python2.7  , tf.__version__  1.1.0-rc2\r\n\r\nhow can I solve this problem?\r\n", "comments": ["You're on the right tarck!\r\n\r\nI tend to use: `with tf.Session() as sess` rather than a default session. After that, you need to use `sess.run(tf.global_variables_initializer())` and it should work.\r\n\r\nThis question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 9710, "title": "Error restoring checkpoints using tf.estimator.Estimator or tf.contrib.learn.Estimator, If moved since training", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Titan X\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\noutput_dir = /location1\r\nestimator = learn.Estimator(model_fn=model_fn, model_dir=output_dir,\r\n                                params=params, config = config)\r\nestimator.fit(input_fn=train_input_fn, steps = 1000)\r\n```\r\n```bash\r\n$ cp -r /location1 /location2\r\n$ rm -rf /location1\r\n```\r\n```python\r\noutput_dir = /location2\r\nestimator = learn.Estimator(model_fn=model_fn, model_dir=output_dir,\r\n                                params=params, config = config)\r\nestimator.fit(input_fn=train_input_fn, steps = 1000)\r\n```\r\n```bash\r\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on /location1\r\n```\r\n### Describe the problem\r\nI had a need to copy my checkpoints to continue training on another machine, without access to the original location. \r\nThe root cause seems to be in tensorflow/python/training/session_manager.py  at line 177 where \r\n`ckpt = saver_mode.get_checkpoint_state(checkpoint_dir)` is used to find the checkpoint path used at training, and then line 188\r\n`saver.restore(sess, ckpt.model_checkpoint_path)` is used to restore, instead of using, for example, the latest checkpoint available provided by the model_dir argument to estimator.fit, which is the behavior I expected. \r\nIn any case, it would be nice to be able to point an estimator.fit() or .train() (v1.1) call to continue at a specific checkpoint file.\r\n\r\n", "comments": ["Right. You have to open the checkpoint file. It contains absolute path references.\r\n\r\nI'm going to close this, since this is expected behavior, even unpleasant.", "Is that necessary to save paths as absolute paths? Why not using relative paths?"]}, {"number": 9709, "title": "Missing image in https://www.tensorflow.org/api_docs/python/tf/gather", "body": "### System information\r\nn/a\r\n\r\n### Describe the problem\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/gather includes an image, https://www.tensorflow.org/api_docs/images/Gather.png, but this does not exist.  Gather.png doesn't seem to exist anywhere in the TF repository.", "comments": ["I think this has already be fixed by this commit: https://github.com/tensorflow/tensorflow/commit/7c3ad6569611aac36fd2527030bf98fb8756919f\r\nSeems that the tensorflow.org does not update.", "Hi, the website still appears not to have been updated, and so the issue I reported still manifests.\r\n\r\nCan we leave this open until the website is updated?  Or if there's a bug for the fact that we don't update the website frequently enough, I'd be OK dup'ing this to that one.", "Sure.\r\n\r\n@wolffg @martinwicke on tensorflow.org refresh rate.", "Website has been updated. Closing this."]}, {"number": 9708, "title": "tf.random_crop exception after upgrading to tf1.1 from tf1.0", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.4\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:v1.1.0-rc0-61-g1ec6ed5 1.1.0\r\n\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Tesla m40 / 12 gb\r\n- **Exact command to reproduce**: \r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThe tf.random_crop gives exception even when it receives input of valid size. assertion failed: [Need value.shape >= size, got ] [224 224 3] [224 224 3]\r\n\r\n### Source code / logs\r\n  File \"../foo.py\", line 1356, in _pp_augment\r\n    aug = tf.random_crop(aug, [crop_size[0], crop_size[1], aug_dim3])\r\n  File \"/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/ops/random_ops.py\", line 303, in random_crop\r\n    [\"Need value.shape >= size, got \", shape, size])\r\n  File \"/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 121, in Assert\r\n    condition, data, summarize, name=\"Assert\")\r\n  File \"/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py\", line 39, in _assert\r\n    summarize=summarize, name=name)\r\n  File \"/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/anaconda3/envs/tf1.1/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): assertion failed: [Need value.shape >= size, got ] [224 224 3] [224 224 3]\r\n         [[Node: image_filters/train_tower_0/random_crop_1/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](image_filters/train_tower_0/random_crop_1/All/_29, image_filters/train_tower_0/random_crop_1/Assert/Assert/data_0, image_filters/train_tower_0/random_crop_1/Shape/_31, image_filters/train_tower_0/random_crop_1/size/_33)]]\r\n         [[Node: image_filters/train_tower_0/DecodeRaw_1/_93 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_232_image_filters/train_tower_0/DecodeRaw_1\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n", "comments": ["@girving the error message does look strange.", "Looking now.  The code looks fine so far, which is confusing.", "Ah, the problem is that `Assert` truncates its arguments at 3 entries by default.  The shapes are actually bigger, and presumably `shape < size` someone in the invisible bit.  I'll fix the error message.", "@saurabh203 Can you confirm that your tensors have rank > 3?", "@girving The tensors have the rank 3 only. But the issue seems like an OS related issue. The earlier run was erroneously reported as Ubuntu but was SLES 12. My apologies for the mistake. The code seems to run fine on the ubuntu machine that I tested today.  I do have to make it work on the SLES too. Is there any other reason apart from Assert truncation which might cause this issue?", "@saurabh203 No idea why it would fail on SLES 12 only.  Please let us know if you figure out the cause, but I don't think we'll be able to debug it on our end without more information."]}, {"number": 9707, "title": "ar", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Looks like this was submitted in error."]}, {"number": 9706, "title": "Branch 155249446", "body": "", "comments": []}, {"number": 9704, "title": "Making sure GLSTMCell is visible through tf.contrib.rnn.GLSTMCell", "body": "This is a followup to #9606. Just adding it to \r\n\r\n>  contrib/rnn/\\_\\_init\\_\\_.py.\r\n\r\nOne question, though. Am I supposed  to add something to:\r\n> /contrib/rnn/python/tools/checkpoint_convert.py \r\n\r\nIf yes, what exactly? The challenge here is that GLSTM has different number of variables/groups - hence number of \"kernels\" is determined at runtime.", "comments": ["Can one of the admins verify this patch?", "LGTM\n\nOn May 5, 2017 3:16 PM, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n> @vrv <https://github.com/vrv> requested your review on:\n> tensorflow/tensorflow#9704\n> <https://github.com/tensorflow/tensorflow/pull/9704> Making sure\n> GLSTMCell is visible through tf.contrib.rnn.GLSTMCell.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9704#event-1071321111>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3MEXfa7GZp2fE85l8K24yiqcPLlks5r25_MgaJpZM4NSdm5>\n> .\n>\n", "@tensorflow-jenkins test this please", "also, fixed an issue in 08f91565e97067041a4d2f12093e533b51344cbe \r\nVerified this works on models/tutorials/ptb and on https://github.com/google/seq2seq  (older version didn't work on seq2seq).", "@tensorflow-jenkins test this please", "No need to use checkpoint converter because no one has created checkpoints\nwith your cell yet, and we plan to change the banking system very soon so\nthey won't have a chance to create them before we start using the new\nvariable names.\n\nOn May 5, 2017 6:32 PM, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n> Merged #9704 <https://github.com/tensorflow/tensorflow/pull/9704>.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/9704#event-1071426868>, or mute\n> the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6BU187Ah4wSRTruXCr0gnNoWVN7ks5r282ygaJpZM4NSdm5>\n> .\n>\n"]}]