[{"number": 30910, "title": "[TF 2.0 keras] Conv2D looses shapes with dilation rate other than 1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen I use Conv2D layer with dilation rate other than 1 output looses shapes\r\n**Describe the expected behavior**\r\nI expect to see calculated shape with None only in first dimension\r\n**Code to reproduce the issue**\r\n```python\r\nfrom tensorflow.keras import Input\r\nfrom tensorflow.keras.layers import Conv2D\r\n\r\ntensor = Input(shape=(512,512,3))\r\n\r\ny_1 = Conv2D(filters=256, kernel_size=1, dilation_rate=1, name='dilation_1')(tensor)\r\ny_6 = Conv2D(filters=256, kernel_size=1, dilation_rate=6, name='dilation_6')(tensor)\r\n\r\nprint(y_1)\r\nprint(y_6)\r\n```\r\n\r\n**Other info / logs**\r\nOutput is:\r\n> Tensor(\"dilation_1/Identity:0\", shape=(None, 512, 512, 256), dtype=float32)\r\nTensor(\"dilation_6/Identity:0\", shape=(None, None, None, 256), dtype=float32)\r\n", "comments": ["I could able to reproduce the issue with Tensorflow 2.0.0.beta1 on Colab. Please take a look at gist of [Colab](https://colab.research.google.com/drive/1swIU2bAUBTdgeEcriw_SAJiQtiXtMBLh).", "@lytkarinskiy This was resolved in tf-nightly-builds. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/8d0b507a3f1d1972012542f6c440c110/tf_30910_dilation.ipynb). \r\n\r\nI am closing the issue. Feel free to open if the issue persists. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30910\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30910\">No</a>\n"]}, {"number": 30909, "title": "TF 2.0 nightly: tf.keras.estimator.model_to_estimator -> got an unexpected keyword argument 'use_v2_estimator'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YEs\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.5 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): No \r\n- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190721\r\n- Python version: Python 3.6.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n```\r\n>>> import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\r\nv1.12.1-6727-g97b7aa03b7 2.0.0-dev20190721\r\n```\r\n\r\n**Describe the current behavior**\r\nThe following code was working with some earlier release but now it is crahsing\r\n```\r\nestimator = tf.keras.estimator.model_to_estimator(\r\n    keras_model = model,\r\n    config=training_config\r\n)\r\n```\r\n\r\n**Describe the expected behavior**\r\nShould work out of the box as before\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nfrom absl import logging\r\n\r\nlogging.set_verbosity(logging.INFO)\r\n# Define the estimator's input_fn\r\nSTEPS_PER_EPOCH = 5\r\nBATCH_SIZE = 64\r\nNUM_EPOCHS = 5\r\n\r\n\r\ndef input_fn():\r\n    datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n    mnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\n    BUFFER_SIZE = 10000\r\n    BATCH_SIZE = 64\r\n\r\n    def scale(image, label):\r\n        image = tf.cast(image, tf.float32)\r\n        image /= 255\r\n    \r\n        return image, label[..., tf.newaxis]\r\n\r\n    train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n    return train_data.repeat()\r\n\r\n# Define train & eval specs\r\ntrain_spec = tf.estimator.TrainSpec(input_fn=input_fn,\r\n                                    max_steps=STEPS_PER_EPOCH * NUM_EPOCHS)\r\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn,\r\n                                  steps=STEPS_PER_EPOCH)\r\n\r\ndef make_model():\r\n    return tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32, 3, activation='relu',\r\n                               kernel_regularizer=tf.keras.regularizers.l2(0.02),\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dropout(0.1),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.BatchNormalization(),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n\r\nmodel = make_model()\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\n#####\r\n#strategy=None \r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\n# config tf.estimator to use a give strategy\r\ntraining_config = tf.estimator.RunConfig(train_distribute=strategy)\r\n#####\r\n\r\nestimator = tf.keras.estimator.model_to_estimator(\r\n    keras_model = model,\r\n    config=training_config\r\n)\r\n\r\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0721 17:02:51.036228 4662060480 cross_device_ops.py:1207] There is non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\r\nI0721 17:02:51.037434 4662060480 run_config.py:558] Initializing RunConfig with distribution strategies.\r\nI0721 17:02:51.038416 4662060480 estimator_training.py:167] Not using Distribute Coordinator.\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-1-490d351436b4> in <module>\r\n     64 estimator = tf.keras.estimator.model_to_estimator(\r\n     65     keras_model = model,\r\n---> 66     config=training_config\r\n     67 )\r\n     68 \r\n\r\n~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/keras/estimator/__init__.py in model_to_estimator_v2(keras_model, keras_model_path, custom_objects, model_dir, config, checkpoint_format)\r\n    164       config=config,\r\n    165       checkpoint_format=checkpoint_format,\r\n--> 166       use_v2_estimator=True)\r\n    167 # LINT.ThenChange(//tensorflow_estimator/python/estimator/keras.py)\r\n\r\nTypeError: model_to_estimator() got an unexpected keyword argument 'use_v2_estimator'\r\n\r\n\r\n```\r\n", "comments": ["It seems the issue was fixed in tf-nightly-2.0-preview==2.0.0.dev20190722 but only work with strategy=None  and not strategy = tf.distribute.MirroredStrategy(). I will open a separet ticket. Closing\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30909\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30909\">No</a>\n"]}, {"number": 30908, "title": "model.fit output is badly formatted in TF2 on Windows (distributed training example)", "body": "**System information**\r\n- Custom code: No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): Tensorflow-GPU 2 beta 1\r\n- Python version: 3.7.3 Anaconda\r\n- CUDA/cuDNN version: 10.0 / 7.6.1\r\n- GPU model and memory: Occurs with RTX 2080 Ti and RTX 2080 Max Q on 2 separate machines\r\n\r\n**Describe the current behavior**\r\nI downloaded the notebook from https://www.tensorflow.org/beta/tutorials/distribute/keras to test running on my local machines and also some EC2 instances.  The EC2 instances have Ubuntu 18.04 and either 1 or 4 Tesla V100 GPUs.  I installed both local and EC2 instances from scratch, used Anaconda on all of them, and installed the tensorflow 2 beta 1 binary as per instructions on the website.  The notebook runs faultlessly on the EC2 instances.  However, this is the output of model.fit(train_dataset, epochs=12, callbacks=callbacks) on my Windows machines.  As you can see it does successfully traini but the output is not formatted properly and chucks out a whole lot of text:\r\n\r\nTrain on None steps\r\nEpoch 1/12\r\n    346/Unknown - 6s 34ms/step - loss: 0.5130 - accuracy: 0.85 - 6s 34ms/step - loss: 0.5108 - accuracy: 0.85 - 6s 34ms/step - loss: 0.5089 - accuracy: 0.85 - 6s 34ms/step - loss: 0.5072 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5064 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5057 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5048 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5034 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5018 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5001 - accuracy: 0.85 - 6s 33ms/step - loss: 0.5001 - accuracy: 0.85 - 6s 32ms/step - loss: 0.4988 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4969 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4956 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4944 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4931 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4926 - accuracy: 0.86 - 6s 32ms/step - loss: 0.4913 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4897 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4882 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4867 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4858 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4846 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4844 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4836 - accuracy: 0.86 - 6s 31ms/step - loss: 0.4830 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4820 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4808 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4804 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4800 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4786 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4775 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4771 - accuracy: 0.86 - 6s 30ms/step - loss: 0.4764 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4762 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4753 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4736 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4722 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4706 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4692 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4683 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4671 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4670 - accuracy: 0.86 - 6s 29ms/step - loss: 0.4657 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4646 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4635 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4622 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4611 - accuracy: 0.86 - 6s 28ms/step - loss: 0.4598 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4584 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4570 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4566 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4555 - accuracy: 0.87 - 6s 28ms/step - loss: 0.4549 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4540 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4532 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4523 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4508 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4495 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4481 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4483 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4471 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4457 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4447 - accuracy: 0.87 - 6s 27ms/step - loss: 0.4439 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4428 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4420 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4414 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4404 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4401 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4391 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4378 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4371 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4361 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4347 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4349 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4336 - accuracy: 0.87 - 6s 26ms/step - loss: 0.4330 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4321 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4316 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4308 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4294 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4291 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4280 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4270 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4262 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4251 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4238 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4231 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4220 - accuracy: 0.87 - 6s 25ms/step - loss: 0.4209 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4199 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4187 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4175 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4164 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4156 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4151 - accuracy: 0.88 - 6s 24ms/step - loss: 0.4140 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4139 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4142 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4133 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4125 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4118 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4116 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4110 - accuracy: 0.88 - 7s 24ms/step - loss: 0.4100 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4103 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4101 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4097 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4096 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4089 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4080 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4073 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4063 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4060 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4055 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4049 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4041 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4034 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4031 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4019 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4015 - accuracy: 0.88 - 7s 23ms/step - loss: 0.4006 - accuracy: 0.88 - 7s 22ms/step - loss: 0.4001 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3996 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3993 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3989 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3979 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3972 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3964 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3960 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3956 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3952 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3949 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3941 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3931 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3924 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3920 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3916 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3908 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3898 - accuracy: 0.88 - 7s 22ms/step - loss: 0.3892 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3891 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3885 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3882 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3878 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3871 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3870 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3868 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3862 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3853 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3851 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3849 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3850 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3844 - accuracy: 0.88 - 7s 21ms/step - loss: 0.3837 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3832 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3825 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3821 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3815 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3812 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3806 - accuracy: 0.89 - 7s 21ms/step - loss: 0.3800 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3798 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3792 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3785 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3779 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3773 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3774 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3767 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3764 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3761 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3755 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3747 - accuracy: 0.8925\r\n    520/Unknown - 7s 20ms/step - loss: 0.3742 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3738 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3739 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3737 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3733 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3731 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3727 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3722 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3713 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3708 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3707 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3700 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3696 - accuracy: 0.89 - 7s 20ms/step - loss: 0.3692 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3687 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3679 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3674 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3668 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3665 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3660 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3653 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3655 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3653 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3652 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3647 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3644 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3637 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3634 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3632 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3625 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3617 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3613 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3608 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3603 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3597 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3594 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3590 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3591 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3584 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3579 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3573 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3567 - accuracy: 0.89 - 7s 19ms/step - loss: 0.3562 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3555 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3549 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3543 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3540 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3536 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3530 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3524 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3520 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3516 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3512 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3505 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3498 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3494 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3491 - accuracy: 0.89 - 7s 18ms/step - loss: 0.3485 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3480 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3473 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3473 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3467 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3461 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3460 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3455 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3448 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3444 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3440 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3433 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3432 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3432 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3430 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3425 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3425 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3419 - accuracy: 0.90 - 7s 18ms/step - loss: 0.3419 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3414 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3409 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3406 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3400 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3395 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3394 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3391 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3386 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3383 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3380 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3375 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3371 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3367 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3363 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3358 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3354 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3352 - accuracy: 0.90 - 7s 17ms/step - loss: 0.3347 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3343 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3338 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3333 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3332 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3328 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3322 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3320 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3318 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3313 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3312 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3306 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3301 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3299 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3295 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3293 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3291 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3285 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3283 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3282 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3278 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3271 - accuracy: 0.90 - 8s 17ms/step - loss: 0.3268 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3264 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3262 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3260 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3254 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3251 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3246 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3241 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3238 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3235 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3233 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3230 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3224 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3220 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3215 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3211 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3205 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3202 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3200 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3196 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3192 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3193 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3191 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3185 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3182 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3183 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3182 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3180 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3177 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3173 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3171 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3167 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3163 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3161 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3157 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3154 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3149 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3147 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3143 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3141 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3136 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3133 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3132 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3129 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3125 - accuracy: 0.90 - 8s 16ms/step - loss: 0.3122 - accuracy: 0.91 - 8s 16ms/step - loss: 0.3120 - accuracy: 0.91 - 8s 16ms/step - loss: 0.3118 - accuracy: 0.91 - 8s 16ms/step - loss: 0.3116 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3114 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3110 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3106 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3103 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3099 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3095 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3095 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3093 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3089 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3086 - accuracy: 0.9111\r\n    694/Unknown - 8s 15ms/step - loss: 0.3081 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3082 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3079 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3074 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3072 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3069 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3065 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3063 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3060 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3058 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3054 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3053 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3051 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3049 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3049 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3045 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3040 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3038 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3037 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3032 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3030 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3026 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3025 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3021 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3017 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3015 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3013 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3009 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3005 - accuracy: 0.91 - 8s 15ms/step - loss: 0.3002 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2998 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2997 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2993 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2991 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2988 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2985 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2981 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2978 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2975 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2972 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2969 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2968 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2967 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2965 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2961 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2958 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2956 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2953 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2950 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2947 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2943 - accuracy: 0.91 - 8s 15ms/step - loss: 0.2940 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2936 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2933 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2931 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2927 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2923 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2920 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2916 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2913 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2909 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2905 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2902 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2898 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2895 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2892 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2888 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2887 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2884 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2881 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2879 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2875 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2872 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2868 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2866 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2865 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2862 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2860 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2859 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2858 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2859 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2856 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2853 - accuracy: 0.91 - 8s 14ms/step - loss: 0.2852 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2850 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2846 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2843 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2840 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2838 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2839 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2836 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2832 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2829 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2826 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2823 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2819 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2817 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2814 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2811 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2808 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2805 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2803 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2800 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2797 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2795 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2793 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2791 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2788 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2785 - accuracy: 0.91 - 9s 14ms/step - loss: 0.2781 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2779 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2777 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2775 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2771 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2769 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2767 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2764 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2761 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2758 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2755 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2752 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2751 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2748 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2746 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2748 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2745 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2743 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2740 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2737 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2736 - accuracy: 0.92 - 9s 14ms/step - loss: 0.2732 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2731 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2728 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2727 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2723 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2721 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2717 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2714 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2712 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2708 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2706 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2704 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2702 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2698 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2695 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2694 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2691 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2690 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2688 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2685 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2683 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2682 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2679 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2676 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2673 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2670 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2668 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2665 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2662 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2659 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2657 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2656 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2653 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2652 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2649 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2647 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2644 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2642 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2642 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2640 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2638 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2636 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2633 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2632 - accuracy: 0.9241\r\n    864/Unknown - 9s 13ms/step - loss: 0.2629 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2627 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2625 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2622 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2619 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2615 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2612 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2610 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2607 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2605 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2603 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2601 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2599 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2597 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2594 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2591 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2589 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2587 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2585 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2584 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2583 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2581 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2579 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2578 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2580 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2578 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2578 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2577 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2576 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2573 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2571 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2569 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2567 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2565 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2564 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2562 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2560 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2558 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2556 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2554 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2552 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2551 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2549 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2548 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2546 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2545 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2543 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2541 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2539 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2537 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2537 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2534 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2533 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2530 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2529 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2526 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2525 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2522 - accuracy: 0.92 - 9s 13ms/step - loss: 0.2520 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2517 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2515 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2513 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2512 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2510 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2508 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2506 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2505 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2503 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2500 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2498 - accuracy: 0.92 - 9s 12ms/step - loss: 0.2495 - accuracy: 0.92 - 10s 12ms/step - loss: 0.2492 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2491 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2488 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2486 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2485 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2483 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2481 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2480 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2478 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2477 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2476 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2474 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2471 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2470 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2468 - accuracy: 0.928 - 10s 12ms/step - loss: 0.2466 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2464 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2461 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2460 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2458 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2456 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2454 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2452 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2450 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2449 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2447 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2445 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2443 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2441 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2439 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2438 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2436 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2434 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2432 - accuracy: 0.929 - 10s 12ms/step - loss: 0.2429 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2427 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2425 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2422 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2421 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2418 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2416 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2414 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2412 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2410 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2407 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2406 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2403 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2402 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2400 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2399 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2397 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2395 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2394 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2392 - accuracy: 0.930 - 10s 12ms/step - loss: 0.2390 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2388 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2386 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2385 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2384 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2382 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2380 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2379 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2377 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2378 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2376 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2374 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2372 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2369 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2368 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2367 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2365 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2364 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2363 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2361 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2359 - accuracy: 0.931 - 10s 12ms/step - loss: 0.2357 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2356 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2356 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2355 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2353 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2351 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2349 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2348 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2347 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2345 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2343 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2342 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2341 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2340 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2338 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2336 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2335 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2333 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2331 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2330 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2329 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2328 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2326 - accuracy: 0.932 - 10s 12ms/step - loss: 0.2325 - accuracy: 0.9329\r\n    938/Unknown - 10s 12ms/step - loss: 0.2323 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2322 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2320 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2318 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2316 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2314 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2312 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2310 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2309 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2307 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2306 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2304 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2302 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2301 - accuracy: 0.933 - 10s 12ms/step - loss: 0.2298 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2297 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2295 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2293 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2294 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2293 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2291 - accuracy: 0.933 - 10s 11ms/step - loss: 0.2289 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2288 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2286 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2285 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2283 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2281 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2280 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2278 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2277 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2275 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2273 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2272 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2271 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2269 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2267 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2266 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2266 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2265 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2263 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2261 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2261 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2260 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2258 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2257 - accuracy: 0.934 - 10s 11ms/step - loss: 0.2255 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2253 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2251 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2250 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2248 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2247 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2246 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2245 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2243 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2241 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2239 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2239 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2238 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2237 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2235 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2234 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2233 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2232 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2230 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2228 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2226 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2225 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2223 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2221 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2220 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2219 - accuracy: 0.935 - 10s 11ms/step - loss: 0.2218 - accuracy: 0.936 - 10s 11ms/step - loss: 0.2216 - accuracy: 0.936 - 10s 11ms/step - loss: 0.2215 - accuracy: 0.9360\r\nLearning rate for epoch 1 is 0.0010000000474974513\r\n938/938 [==============================] - 11s 11ms/step - loss: 0.2215 - accuracy: 0.9360\r\nEpoch 2/12\r\n915/938 [============================>.] - ETA: 9:19 - loss: 0.0448 - accuracy: 1.00 - ETA: 40s - loss: 0.1246 - accuracy: 0.9646 - ETA: 22s - loss: 0.1092 - accuracy: 0.966 - ETA: 16s - loss: 0.1017 - accuracy: 0.970 - ETA: 12s - loss: 0.1019 - accuracy: 0.969 - ETA: 10s - loss: 0.1007 - accuracy: 0.969 - ETA: 9s - loss: 0.1063 - accuracy: 0.968 - ETA: 8s - loss: 0.0995 - accuracy: 0.97 - ETA: 7s - loss: 0.0975 - accuracy: 0.97 - ETA: 7s - loss: 0.0970 - accuracy: 0.97 - ETA: 6s - loss: 0.0969 - accuracy: 0.97 - ETA: 6s - loss: 0.0949 - accuracy: 0.97 - ETA: 5s - loss: 0.0936 - accuracy: 0.97 - ETA: 5s - loss: 0.0933 - accuracy: 0.97 - ETA: 5s - loss: 0.0931 - accuracy: 0.97 - ETA: 5s - loss: 0.0908 - accuracy: 0.97 - ETA: 4s - loss: 0.0888 - accuracy: 0.97 - ETA: 4s - loss: 0.0874 - accuracy: 0.97 - ETA: 4s - loss: 0.0878 - accuracy: 0.97 - ETA: 4s - loss: 0.0873 - accuracy: 0.97 - ETA: 4s - loss: 0.0864 - accuracy: 0.97 - ETA: 4s - loss: 0.0860 - accuracy: 0.97 - ETA: 3s - loss: 0.0851 - accuracy: 0.97 - ETA: 3s - loss: 0.0848 - accuracy: 0.97 - ETA: 3s - loss: 0.0841 - accuracy: 0.97 - ETA: 3s - loss: 0.0829 - accuracy: 0.97 - ETA: 3s - loss: 0.0832 - accuracy: 0.97 - ETA: 3s - loss: 0.0827 - accuracy: 0.97 - ETA: 3s - loss: 0.0825 - accuracy: 0.97 - ETA: 3s - loss: 0.0827 - accuracy: 0.97 - ETA: 3s - loss: 0.0824 - accuracy: 0.97 - ETA: 2s - loss: 0.0817 - accuracy: 0.97 - ETA: 2s - loss: 0.0815 - accuracy: 0.97 - ETA: 2s - loss: 0.0822 - accuracy: 0.97 - ETA: 2s - loss: 0.0814 - accuracy: 0.97 - ETA: 2s - loss: 0.0806 - accuracy: 0.97 - ETA: 2s - loss: 0.0798 - accuracy: 0.97 - ETA: 2s - loss: 0.0791 - accuracy: 0.97 - ETA: 2s - loss: 0.0790 - accuracy: 0.97 - ETA: 2s - loss: 0.0794 - accuracy: 0.97 - ETA: 2s - loss: 0.0790 - accuracy: 0.97 - ETA: 2s - loss: 0.0791 - accuracy: 0.97 - ETA: 2s - loss: 0.0789 - accuracy: 0.97 - ETA: 2s - loss: 0.0788 - accuracy: 0.97 - ETA: 1s - loss: 0.0786 - accuracy: 0.97 - ETA: 1s - loss: 0.0779 - accuracy: 0.97 - ETA: 1s - loss: 0.0775 - accuracy: 0.97 - ETA: 1s - loss: 0.0773 - accuracy: 0.97 - ETA: 1s - loss: 0.0771 - accuracy: 0.97 - ETA: 1s - loss: 0.0772 - accuracy: 0.97 - ETA: 1s - loss: 0.0768 - accuracy: 0.97 - ETA: 1s - loss: 0.0768 - accuracy: 0.97 - ETA: 1s - loss: 0.0767 - accuracy: 0.97 - ETA: 1s - loss: 0.0763 - accuracy: 0.97 - ETA: 1s - loss: 0.0761 - accuracy: 0.97 - ETA: 1s - loss: 0.0759 - accuracy: 0.97 - ETA: 1s - loss: 0.0754 - accuracy: 0.97 - ETA: 1s - loss: 0.0747 - accuracy: 0.97 - ETA: 0s - loss: 0.0743 - accuracy: 0.97 - ETA: 0s - loss: 0.0744 - accuracy: 0.97 - ETA: 0s - loss: 0.0737 - accuracy: 0.97 - ETA: 0s - loss: 0.0734 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0732 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0728 - accuracy: 0.97 - ETA: 0s - loss: 0.0725 - accuracy: 0.97 - ETA: 0s - loss: 0.0725 - accuracy: 0.9789\r\nLearning rate for epoch 2 is 0.0010000000474974513\r\n938/938 [==============================] - 4s 4ms/step - loss: 0.0719 - accuracy: 0.9791\r\nEpoch 3/12\r\n926/938 [============================>.] - ETA: 10:02 - loss: 0.0245 - accuracy: 1.000 - ETA: 49s - loss: 0.0628 - accuracy: 0.9868  - ETA: 27s - loss: 0.0623 - accuracy: 0.986 - ETA: 19s - loss: 0.0527 - accuracy: 0.988 - ETA: 15s - loss: 0.0555 - accuracy: 0.987 - ETA: 13s - loss: 0.0552 - accuracy: 0.985 - ETA: 11s - loss: 0.0553 - accuracy: 0.986 - ETA: 10s - loss: 0.0551 - accuracy: 0.985 - ETA: 9s - loss: 0.0535 - accuracy: 0.986 - ETA: 8s - loss: 0.0554 - accuracy: 0.98 - ETA: 7s - loss: 0.0536 - accuracy: 0.98 - ETA: 7s - loss: 0.0537 - accuracy: 0.98 - ETA: 6s - loss: 0.0546 - accuracy: 0.98 - ETA: 6s - loss: 0.0556 - accuracy: 0.98 - ETA: 6s - loss: 0.0553 - accuracy: 0.98 - ETA: 5s - loss: 0.0565 - accuracy: 0.98 - ETA: 5s - loss: 0.0558 - accuracy: 0.98 - ETA: 5s - loss: 0.0575 - accuracy: 0.98 - ETA: 5s - loss: 0.0573 - accuracy: 0.98 - ETA: 5s - loss: 0.0570 - accuracy: 0.98 - ETA: 4s - loss: 0.0558 - accuracy: 0.98 - ETA: 4s - loss: 0.0553 - accuracy: 0.98 - ETA: 4s - loss: 0.0549 - accuracy: 0.98 - ETA: 4s - loss: 0.0552 - accuracy: 0.98 - ETA: 4s - loss: 0.0554 - accuracy: 0.98 - ETA: 4s - loss: 0.0551 - accuracy: 0.98 - ETA: 3s - loss: 0.0545 - accuracy: 0.98 - ETA: 3s - loss: 0.0540 - accuracy: 0.98 - ETA: 3s - loss: 0.0534 - accuracy: 0.98 - ETA: 3s - loss: 0.0531 - accuracy: 0.98 - ETA: 3s - loss: 0.0527 - accuracy: 0.98 - ETA: 3s - loss: 0.0523 - accuracy: 0.98 - ETA: 3s - loss: 0.0526 - accuracy: 0.98 - ETA: 3s - loss: 0.0524 - accuracy: 0.98 - ETA: 3s - loss: 0.0522 - accuracy: 0.98 - ETA: 2s - loss: 0.0523 - accuracy: 0.98 - ETA: 2s - loss: 0.0521 - accuracy: 0.98 - ETA: 2s - loss: 0.0524 - accuracy: 0.98 - ETA: 2s - loss: 0.0520 - accuracy: 0.98 - ETA: 2s - loss: 0.0519 - accuracy: 0.98 - ETA: 2s - loss: 0.0517 - accuracy: 0.98 - ETA: 2s - loss: 0.0519 - accuracy: 0.98 - ETA: 2s - loss: 0.0518 - accuracy: 0.98 - ETA: 2s - loss: 0.0514 - accuracy: 0.98 - ETA: 2s - loss: 0.0511 - accuracy: 0.98 - ETA: 2s - loss: 0.0510 - accuracy: 0.98 - ETA: 1s - loss: 0.0507 - accuracy: 0.98 - ETA: 1s - loss: 0.0505 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0505 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0501 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 1s - loss: 0.0499 - accuracy: 0.98 - ETA: 1s - loss: 0.0500 - accuracy: 0.98 - ETA: 1s - loss: 0.0496 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0493 - accuracy: 0.98 - ETA: 1s - loss: 0.0492 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0488 - accuracy: 0.98 - ETA: 0s - loss: 0.0487 - accuracy: 0.98 - ETA: 0s - loss: 0.0486 - accuracy: 0.98 - ETA: 0s - loss: 0.0484 - accuracy: 0.98 - ETA: 0s - loss: 0.0489 - accuracy: 0.98 - ETA: 0s - loss: 0.0491 - accuracy: 0.98 - ETA: 0s - loss: 0.0495 - accuracy: 0.98 - ETA: 0s - loss: 0.0492 - accuracy: 0.98 - ETA: 0s - loss: 0.0490 - accuracy: 0.9850\r\nLearning rate for epoch 3 is 0.0010000000474974513\r\n938/938 [==============================] - 4s 5ms/step - loss: 0.0490 - accuracy: 0.9850\r\nEpoch 4/12\r\n930/938 [============================>.] - ETA: 9:29 - loss: 0.0240 - accuracy: 1.00 - ETA: 43s - loss: 0.0553 - accuracy: 0.9821 - ETA: 23s - loss: 0.0465 - accuracy: 0.983 - ETA: 16s - loss: 0.0397 - accuracy: 0.986 - ETA: 13s - loss: 0.0417 - accuracy: 0.986 - ETA: 11s - loss: 0.0395 - accuracy: 0.987 - ETA: 9s - loss: 0.0392 - accuracy: 0.988 - ETA: 8s - loss: 0.0367 - accuracy: 0.98 - ETA: 7s - loss: 0.0364 - accuracy: 0.98 - ETA: 7s - loss: 0.0353 - accuracy: 0.98 - ETA: 6s - loss: 0.0360 - accuracy: 0.98 - ETA: 6s - loss: 0.0358 - accuracy: 0.98 - ETA: 5s - loss: 0.0353 - accuracy: 0.98 - ETA: 5s - loss: 0.0350 - accuracy: 0.98 - ETA: 5s - loss: 0.0344 - accuracy: 0.98 - ETA: 5s - loss: 0.0351 - accuracy: 0.98 - ETA: 4s - loss: 0.0349 - accuracy: 0.98 - ETA: 4s - loss: 0.0344 - accuracy: 0.98 - ETA: 4s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0332 - accuracy: 0.99 - ETA: 4s - loss: 0.0337 - accuracy: 0.99 - ETA: 4s - loss: 0.0341 - accuracy: 0.99 - ETA: 3s - loss: 0.0342 - accuracy: 0.98 - ETA: 3s - loss: 0.0344 - accuracy: 0.98 - ETA: 3s - loss: 0.0343 - accuracy: 0.98 - ETA: 3s - loss: 0.0341 - accuracy: 0.98 - ETA: 3s - loss: 0.0337 - accuracy: 0.99 - ETA: 3s - loss: 0.0334 - accuracy: 0.99 - ETA: 3s - loss: 0.0330 - accuracy: 0.99 - ETA: 3s - loss: 0.0327 - accuracy: 0.99 - ETA: 2s - loss: 0.0325 - accuracy: 0.99 - ETA: 2s - loss: 0.0328 - accuracy: 0.99 - ETA: 2s - loss: 0.0324 - accuracy: 0.99 - ETA: 2s - loss: 0.0322 - accuracy: 0.99 - ETA: 2s - loss: 0.0317 - accuracy: 0.99 - ETA: 2s - loss: 0.0316 - accuracy: 0.99 - ETA: 2s - loss: 0.0320 - accuracy: 0.99 - ETA: 2s - loss: 0.0319 - accuracy: 0.99 - ETA: 2s - loss: 0.0316 - accuracy: 0.99 - ETA: 2s - loss: 0.0314 - accuracy: 0.99 - ETA: 2s - loss: 0.0312 - accuracy: 0.99 - ETA: 2s - loss: 0.0312 - accuracy: 0.99 - ETA: 1s - loss: 0.0311 - accuracy: 0.99 - ETA: 1s - loss: 0.0309 - accuracy: 0.99 - ETA: 1s - loss: 0.0307 - accuracy: 0.99 - ETA: 1s - loss: 0.0305 - accuracy: 0.99 - ETA: 1s - loss: 0.0303 - accuracy: 0.99 - ETA: 1s - loss: 0.0303 - accuracy: 0.99 - ETA: 1s - loss: 0.0300 - accuracy: 0.99 - ETA: 1s - loss: 0.0302 - accuracy: 0.99 - ETA: 1s - loss: 0.0300 - accuracy: 0.99 - ETA: 1s - loss: 0.0298 - accuracy: 0.99 - ETA: 1s - loss: 0.0296 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 1s - loss: 0.0298 - accuracy: 0.99 - ETA: 1s - loss: 0.0298 - accuracy: 0.99 - ETA: 1s - loss: 0.0297 - accuracy: 0.99 - ETA: 0s - loss: 0.0296 - accuracy: 0.99 - ETA: 0s - loss: 0.0297 - accuracy: 0.99 - ETA: 0s - loss: 0.0295 - accuracy: 0.99 - ETA: 0s - loss: 0.0293 - accuracy: 0.99 - ETA: 0s - loss: 0.0293 - accuracy: 0.99 - ETA: 0s - loss: 0.0292 - accuracy: 0.99 - ETA: 0s - loss: 0.0289 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0286 - accuracy: 0.99 - ETA: 0s - loss: 0.0287 - accuracy: 0.99 - ETA: 0s - loss: 0.0284 - accuracy: 0.99 - ETA: 0s - loss: 0.0282 - accuracy: 0.99 - ETA: 0s - loss: 0.0281 - accuracy: 0.9924\r\nLearning rate for epoch 4 is 9.999999747378752e-05\r\n938/938 [==============================] - 4s 5ms/step - loss: 0.0279 - accuracy: 0.9925\r\nEpoch 5/12\r\n934/938 [============================>.] - ETA: 11:27 - loss: 0.0173 - accuracy: 1.000 - ETA: 1:21 - loss: 0.0281 - accuracy: 0.991 - ETA: 45s - loss: 0.0214 - accuracy: 0.9936 - ETA: 30s - loss: 0.0197 - accuracy: 0.993 - ETA: 22s - loss: 0.0249 - accuracy: 0.992 - ETA: 18s - loss: 0.0270 - accuracy: 0.991 - ETA: 15s - loss: 0.0246 - accuracy: 0.993 - ETA: 13s - loss: 0.0232 - accuracy: 0.993 - ETA: 12s - loss: 0.0234 - accuracy: 0.993 - ETA: 11s - loss: 0.0223 - accuracy: 0.993 - ETA: 10s - loss: 0.0232 - accuracy: 0.993 - ETA: 9s - loss: 0.0230 - accuracy: 0.992 - ETA: 9s - loss: 0.0236 - accuracy: 0.99 - ETA: 8s - loss: 0.0247 - accuracy: 0.99 - ETA: 7s - loss: 0.0248 - accuracy: 0.99 - ETA: 7s - loss: 0.0258 - accuracy: 0.99 - ETA: 7s - loss: 0.0266 - accuracy: 0.99 - ETA: 6s - loss: 0.0272 - accuracy: 0.99 - ETA: 6s - loss: 0.0277 - accuracy: 0.99 - ETA: 6s - loss: 0.0285 - accuracy: 0.99 - ETA: 5s - loss: 0.0279 - accuracy: 0.99 - ETA: 5s - loss: 0.0280 - accuracy: 0.99 - ETA: 5s - loss: 0.0275 - accuracy: 0.99 - ETA: 5s - loss: 0.0278 - accuracy: 0.99 - ETA: 4s - loss: 0.0278 - accuracy: 0.99 - ETA: 4s - loss: 0.0277 - accuracy: 0.99 - ETA: 4s - loss: 0.0276 - accuracy: 0.99 - ETA: 4s - loss: 0.0275 - accuracy: 0.99 - ETA: 4s - loss: 0.0273 - accuracy: 0.99 - ETA: 4s - loss: 0.0270 - accuracy: 0.99 - ETA: 4s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 3s - loss: 0.0275 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0278 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 2s - loss: 0.0275 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0271 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0274 - accuracy: 0.99 - ETA: 2s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 1s - loss: 0.0274 - accuracy: 0.99 - ETA: 1s - loss: 0.0272 - accuracy: 0.99 - ETA: 1s - loss: 0.0271 - accuracy: 0.99 - ETA: 1s - loss: 0.0269 - accuracy: 0.99 - ETA: 1s - loss: 0.0270 - accuracy: 0.99 - ETA: 1s - loss: 0.0267 - accuracy: 0.99 - ETA: 1s - loss: 0.0266 - accuracy: 0.99 - ETA: 1s - loss: 0.0264 - accuracy: 0.99 - ETA: 1s - loss: 0.0261 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 1s - loss: 0.0257 - accuracy: 0.99 - ETA: 1s - loss: 0.0259 - accuracy: 0.99 - ETA: 0s - loss: 0.0258 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0257 - accuracy: 0.99 - ETA: 0s - loss: 0.0256 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0255 - accuracy: 0.99 - ETA: 0s - loss: 0.0254 - accuracy: 0.99 - ETA: 0s - loss: 0.0252 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0251 - accuracy: 0.99 - ETA: 0s - loss: 0.0250 - accuracy: 0.9932\r\nLearning rate for epoch 5 is 9.999999747378752e-05\r\n938/938 [==============================] - 5s 5ms/step - loss: 0.0250 - accuracy: 0.9933\r\nEpoch 6/12\r\n921/938 [============================>.] - ETA: 11:48 - loss: 0.0304 - accuracy: 0.984 - ETA: 57s - loss: 0.0276 - accuracy: 0.9916  - ETA: 34s - loss: 0.0299 - accuracy: 0.990 - ETA: 26s - loss: 0.0301 - accuracy: 0.990 - ETA: 21s - loss: 0.0314 - accuracy: 0.990 - ETA: 17s - loss: 0.0290 - accuracy: 0.991 - ETA: 14s - loss: 0.0269 - accuracy: 0.992 - ETA: 12s - loss: 0.0255 - accuracy: 0.992 - ETA: 11s - loss: 0.0240 - accuracy: 0.992 - ETA: 10s - loss: 0.0235 - accuracy: 0.993 - ETA: 9s - loss: 0.0232 - accuracy: 0.993 - ETA: 8s - loss: 0.0229 - accuracy: 0.99 - ETA: 8s - loss: 0.0236 - accuracy: 0.99 - ETA: 7s - loss: 0.0236 - accuracy: 0.99 - ETA: 7s - loss: 0.0237 - accuracy: 0.99 - ETA: 6s - loss: 0.0237 - accuracy: 0.99 - ETA: 6s - loss: 0.0249 - accuracy: 0.99 - ETA: 6s - loss: 0.0245 - accuracy: 0.99 - ETA: 6s - loss: 0.0243 - accuracy: 0.99 - ETA: 5s - loss: 0.0245 - accuracy: 0.99 - ETA: 5s - loss: 0.0250 - accuracy: 0.99 - ETA: 5s - loss: 0.0250 - accuracy: 0.99 - ETA: 5s - loss: 0.0249 - accuracy: 0.99 - ETA: 5s - loss: 0.0252 - accuracy: 0.99 - ETA: 4s - loss: 0.0255 - accuracy: 0.99 - ETA: 4s - loss: 0.0254 - accuracy: 0.99 - ETA: 4s - loss: 0.0255 - accuracy: 0.99 - ETA: 4s - loss: 0.0253 - accuracy: 0.99 - ETA: 4s - loss: 0.0251 - accuracy: 0.99 - ETA: 4s - loss: 0.0252 - accuracy: 0.99 - ETA: 3s - loss: 0.0247 - accuracy: 0.99 - ETA: 3s - loss: 0.0244 - accuracy: 0.99 - ETA: 3s - loss: 0.0248 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0249 - accuracy: 0.99 - ETA: 3s - loss: 0.0247 - accuracy: 0.99 - ETA: 3s - loss: 0.0246 - accuracy: 0.99 - ETA: 3s - loss: 0.0245 - accuracy: 0.99 - ETA: 3s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0248 - accuracy: 0.99 - ETA: 2s - loss: 0.0248 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0246 - accuracy: 0.99 - ETA: 2s - loss: 0.0245 - accuracy: 0.99 - ETA: 2s - loss: 0.0243 - accuracy: 0.99 - ETA: 2s - loss: 0.0242 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0241 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 2s - loss: 0.0240 - accuracy: 0.99 - ETA: 1s - loss: 0.0238 - accuracy: 0.99 - ETA: 1s - loss: 0.0239 - accuracy: 0.99 - ETA: 1s - loss: 0.0239 - accuracy: 0.99 - ETA: 1s - loss: 0.0240 - accuracy: 0.99 - ETA: 1s - loss: 0.0238 - accuracy: 0.99 - ETA: 1s - loss: 0.0238 - accuracy: 0.99 - ETA: 1s - loss: 0.0237 - accuracy: 0.99 - ETA: 1s - loss: 0.0236 - accuracy: 0.99 - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 1s - loss: 0.0234 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0233 - accuracy: 0.99 - ETA: 1s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0236 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0234 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0235 - accuracy: 0.99 - ETA: 0s - loss: 0.0233 - accuracy: 0.99 - ETA: 0s - loss: 0.0231 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0232 - accuracy: 0.99 - ETA: 0s - loss: 0.0230 - accuracy: 0.9937\r\nLearning rate for epoch 6 is 9.999999747378752e-05\r\n938/938 [==============================] - 5s 5ms/step - loss: 0.0231 - accuracy: 0.9937\r\nEpoch 7/12\r\n929/938 [============================>.] - ETA: 11:36 - loss: 0.0087 - accuracy: 1.000 - ETA: 1:13 - loss: 0.0184 - accuracy: 0.996 - ETA: 41s - loss: 0.0189 - accuracy: 0.9967 - ETA: 27s - loss: 0.0227 - accuracy: 0.994 - ETA: 21s - loss: 0.0234 - accuracy: 0.994 - ETA: 17s - loss: 0.0263 - accuracy: 0.993 - ETA: 14s - loss: 0.0245 - accuracy: 0.994 - ETA: 12s - loss: 0.0232 - accuracy: 0.994 - ETA: 11s - loss: 0.0230 - accuracy: 0.994 - ETA: 10s - loss: 0.0220 - accuracy: 0.994 - ETA: 9s - loss: 0.0213 - accuracy: 0.994 - ETA: 8s - loss: 0.0215 - accuracy: 0.99 - ETA: 8s - loss: 0.0219 - accuracy: 0.99 - ETA: 7s - loss: 0.0219 - accuracy: 0.99 - ETA: 7s - loss: 0.0228 - accuracy: 0.99 - ETA: 6s - loss: 0.0239 - accuracy: 0.99 - ETA: 6s - loss: 0.0235 - accuracy: 0.99 - ETA: 6s - loss: 0.0237 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 5s - loss: 0.0242 - accuracy: 0.99 - ETA: 5s - loss: 0.0246 - accuracy: 0.99 - ETA: 5s - loss: 0.0244 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 4s - loss: 0.0239 - accuracy: 0.99 - ETA: 4s - loss: 0.0239 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 4s - loss: 0.0244 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 4s - loss: 0.0242 - accuracy: 0.99 - ETA: 3s - loss: 0.0241 - accuracy: 0.99 - ETA: 3s - loss: 0.0240 - accuracy: 0.99 - ETA: 3s - loss: 0.0238 - accuracy: 0.99 - ETA: 3s - loss: 0.0237 - accuracy: 0.99 - ETA: 3s - loss: 0.0234 - accuracy: 0.99 - ETA: 3s - loss: 0.0231 - accuracy: 0.99 - ETA: 3s - loss: 0.0229 - accuracy: 0.99 - ETA: 3s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0228 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0234 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0234 - accuracy: 0.99 - ETA: 2s - loss: 0.0234 - accuracy: 0.99 - ETA: 2s - loss: 0.0233 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0231 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 2s - loss: 0.0229 - accuracy: 0.99 - ETA: 1s - loss: 0.0227 - accuracy: 0.99 - ETA: 1s - loss: 0.0225 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0226 - accuracy: 0.99 - ETA: 1s - loss: 0.0225 - accuracy: 0.99 - ETA: 1s - loss: 0.0223 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0222 - accuracy: 0.99 - ETA: 1s - loss: 0.0221 - accuracy: 0.99 - ETA: 1s - loss: 0.0220 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 1s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0220 - accuracy: 0.99 - ETA: 0s - loss: 0.0218 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0219 - accuracy: 0.99 - ETA: 0s - loss: 0.0216 - accuracy: 0.99 - ETA: 0s - loss: 0.0217 - accuracy: 0.99 - ETA: 0s - loss: 0.0216 - accuracy: 0.9944\r\nLearning rate for epoch 7 is 9.999999747378752e-05\r\n938/938 [==============================] - 5s 5ms/step - loss: 0.0215 - accuracy: 0.9944\r\nEpoch 8/12\r\n922/938 [============================>.] - ETA: 11:03 - loss: 0.0745 - accuracy: 0.953 - ETA: 50s - loss: 0.0263 - accuracy: 0.9922  - ETA: 28s - loss: 0.0237 - accuracy: 0.992 - ETA: 20s - loss: 0.0197 - accuracy: 0.994 - ETA: 16s - loss: 0.0179 - accuracy: 0.994 - ETA: 13s - loss: 0.0199 - accuracy: 0.994 - ETA: 11s - loss: 0.0184 - accuracy: 0.994 - ETA: 10s - loss: 0.0184 - accuracy: 0.995 - ETA: 9s - loss: 0.0198 - accuracy: 0.994 - ETA: 8s - loss: 0.0205 - accuracy: 0.99 - ETA: 8s - loss: 0.0202 - accuracy: 0.99 - ETA: 7s - loss: 0.0196 - accuracy: 0.99 - ETA: 7s - loss: 0.0198 - accuracy: 0.99 - ETA: 6s - loss: 0.0201 - accuracy: 0.99 - ETA: 6s - loss: 0.0196 - accuracy: 0.99 - ETA: 6s - loss: 0.0201 - accuracy: 0.99 - ETA: 5s - loss: 0.0206 - accuracy: 0.99 - ETA: 5s - loss: 0.0202 - accuracy: 0.99 - ETA: 5s - loss: 0.0200 - accuracy: 0.99 - ETA: 5s - loss: 0.0208 - accuracy: 0.99 - ETA: 4s - loss: 0.0203 - accuracy: 0.99 - ETA: 4s - loss: 0.0204 - accuracy: 0.99 - ETA: 4s - loss: 0.0207 - accuracy: 0.99 - ETA: 4s - loss: 0.0208 - accuracy: 0.99 - ETA: 4s - loss: 0.0210 - accuracy: 0.99 - ETA: 4s - loss: 0.0211 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0208 - accuracy: 0.99 - ETA: 3s - loss: 0.0210 - accuracy: 0.99 - ETA: 3s - loss: 0.0209 - accuracy: 0.99 - ETA: 2s - loss: 0.0207 - accuracy: 0.99 - ETA: 2s - loss: 0.0207 - accuracy: 0.99 - ETA: 2s - loss: 0.0205 - accuracy: 0.99 - ETA: 2s - loss: 0.0205 - accuracy: 0.99 - ETA: 2s - loss: 0.0202 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0196 - accuracy: 0.99 - ETA: 1s - loss: 0.0196 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0196 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0196 - accuracy: 0.99 - ETA: 0s - loss: 0.0196 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0193 - accuracy: 0.99 - ETA: 0s - loss: 0.0192 - accuracy: 0.99 - ETA: 0s - loss: 0.0191 - accuracy: 0.99 - ETA: 0s - loss: 0.0190 - accuracy: 0.99 - ETA: 0s - loss: 0.0189 - accuracy: 0.99 - ETA: 0s - loss: 0.0190 - accuracy: 0.9954\r\nLearning rate for epoch 8 is 9.999999747378752e-06\r\n938/938 [==============================] - 4s 5ms/step - loss: 0.0189 - accuracy: 0.9954\r\nEpoch 9/12\r\n919/938 [============================>.] - ETA: 11:06 - loss: 0.0661 - accuracy: 0.984 - ETA: 58s - loss: 0.0357 - accuracy: 0.9922  - ETA: 30s - loss: 0.0301 - accuracy: 0.992 - ETA: 22s - loss: 0.0310 - accuracy: 0.991 - ETA: 17s - loss: 0.0337 - accuracy: 0.991 - ETA: 14s - loss: 0.0304 - accuracy: 0.991 - ETA: 12s - loss: 0.0306 - accuracy: 0.991 - ETA: 11s - loss: 0.0282 - accuracy: 0.992 - ETA: 9s - loss: 0.0276 - accuracy: 0.992 - ETA: 9s - loss: 0.0269 - accuracy: 0.99 - ETA: 8s - loss: 0.0255 - accuracy: 0.99 - ETA: 7s - loss: 0.0245 - accuracy: 0.99 - ETA: 7s - loss: 0.0240 - accuracy: 0.99 - ETA: 6s - loss: 0.0249 - accuracy: 0.99 - ETA: 6s - loss: 0.0247 - accuracy: 0.99 - ETA: 6s - loss: 0.0243 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.99 - ETA: 5s - loss: 0.0237 - accuracy: 0.99 - ETA: 5s - loss: 0.0241 - accuracy: 0.99 - ETA: 5s - loss: 0.0234 - accuracy: 0.99 - ETA: 4s - loss: 0.0230 - accuracy: 0.99 - ETA: 4s - loss: 0.0229 - accuracy: 0.99 - ETA: 4s - loss: 0.0227 - accuracy: 0.99 - ETA: 4s - loss: 0.0222 - accuracy: 0.99 - ETA: 4s - loss: 0.0217 - accuracy: 0.99 - ETA: 4s - loss: 0.0215 - accuracy: 0.99 - ETA: 4s - loss: 0.0212 - accuracy: 0.99 - ETA: 3s - loss: 0.0212 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0203 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0201 - accuracy: 0.99 - ETA: 3s - loss: 0.0200 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0198 - accuracy: 0.99 - ETA: 1s - loss: 0.0198 - accuracy: 0.99 - ETA: 1s - loss: 0.0198 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0195 - accuracy: 0.99 - ETA: 0s - loss: 0.0193 - accuracy: 0.99 - ETA: 0s - loss: 0.0193 - accuracy: 0.99 - ETA: 0s - loss: 0.0192 - accuracy: 0.99 - ETA: 0s - loss: 0.0190 - accuracy: 0.99 - ETA: 0s - loss: 0.0189 - accuracy: 0.99 - ETA: 0s - loss: 0.0188 - accuracy: 0.99 - ETA: 0s - loss: 0.0186 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0186 - accuracy: 0.9956\r\nLearning rate for epoch 9 is 9.999999747378752e-06\r\n938/938 [==============================] - 4s 5ms/step - loss: 0.0186 - accuracy: 0.9956\r\nEpoch 10/12\r\n919/938 [============================>.] - ETA: 9:17 - loss: 0.0493 - accuracy: 0.98 - ETA: 39s - loss: 0.0212 - accuracy: 0.9937 - ETA: 21s - loss: 0.0178 - accuracy: 0.994 - ETA: 15s - loss: 0.0171 - accuracy: 0.994 - ETA: 12s - loss: 0.0164 - accuracy: 0.995 - ETA: 10s - loss: 0.0156 - accuracy: 0.995 - ETA: 9s - loss: 0.0169 - accuracy: 0.995 - ETA: 8s - loss: 0.0171 - accuracy: 0.99 - ETA: 7s - loss: 0.0177 - accuracy: 0.99 - ETA: 6s - loss: 0.0178 - accuracy: 0.99 - ETA: 6s - loss: 0.0186 - accuracy: 0.99 - ETA: 5s - loss: 0.0193 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0186 - accuracy: 0.99 - ETA: 4s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0191 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 4s - loss: 0.0194 - accuracy: 0.99 - ETA: 4s - loss: 0.0194 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0190 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0193 - accuracy: 0.99 - ETA: 2s - loss: 0.0193 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0189 - accuracy: 0.99 - ETA: 1s - loss: 0.0188 - accuracy: 0.99 - ETA: 1s - loss: 0.0186 - accuracy: 0.99 - ETA: 1s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0184 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0184 - accuracy: 0.99 - ETA: 0s - loss: 0.0184 - accuracy: 0.9957\r\nLearning rate for epoch 10 is 9.999999747378752e-06\r\n938/938 [==============================] - 4s 4ms/step - loss: 0.0184 - accuracy: 0.9957\r\nEpoch 11/12\r\n926/938 [============================>.] - ETA: 10:17 - loss: 0.0177 - accuracy: 1.000 - ETA: 50s - loss: 0.0256 - accuracy: 0.9952  - ETA: 26s - loss: 0.0294 - accuracy: 0.992 - ETA: 18s - loss: 0.0294 - accuracy: 0.992 - ETA: 14s - loss: 0.0263 - accuracy: 0.992 - ETA: 12s - loss: 0.0261 - accuracy: 0.993 - ETA: 10s - loss: 0.0245 - accuracy: 0.993 - ETA: 9s - loss: 0.0231 - accuracy: 0.994 - ETA: 8s - loss: 0.0218 - accuracy: 0.99 - ETA: 8s - loss: 0.0207 - accuracy: 0.99 - ETA: 7s - loss: 0.0215 - accuracy: 0.99 - ETA: 7s - loss: 0.0215 - accuracy: 0.99 - ETA: 6s - loss: 0.0212 - accuracy: 0.99 - ETA: 6s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0207 - accuracy: 0.99 - ETA: 5s - loss: 0.0203 - accuracy: 0.99 - ETA: 5s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0198 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0191 - accuracy: 0.99 - ETA: 3s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0203 - accuracy: 0.99 - ETA: 3s - loss: 0.0201 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0201 - accuracy: 0.99 - ETA: 2s - loss: 0.0199 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0200 - accuracy: 0.99 - ETA: 2s - loss: 0.0201 - accuracy: 0.99 - ETA: 2s - loss: 0.0199 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 1s - loss: 0.0195 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0194 - accuracy: 0.99 - ETA: 1s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0188 - accuracy: 0.99 - ETA: 1s - loss: 0.0187 - accuracy: 0.99 - ETA: 1s - loss: 0.0186 - accuracy: 0.99 - ETA: 1s - loss: 0.0184 - accuracy: 0.99 - ETA: 1s - loss: 0.0184 - accuracy: 0.99 - ETA: 1s - loss: 0.0183 - accuracy: 0.99 - ETA: 1s - loss: 0.0186 - accuracy: 0.99 - ETA: 1s - loss: 0.0188 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0188 - accuracy: 0.99 - ETA: 0s - loss: 0.0189 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0186 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0184 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0182 - accuracy: 0.99 - ETA: 0s - loss: 0.0181 - accuracy: 0.99 - ETA: 0s - loss: 0.0182 - accuracy: 0.9957\r\nLearning rate for epoch 11 is 9.999999747378752e-06\r\n938/938 [==============================] - 4s 5ms/step - loss: 0.0182 - accuracy: 0.9957\r\nEpoch 12/12\r\n920/938 [============================>.] - ETA: 11:13 - loss: 0.0046 - accuracy: 1.000 - ETA: 54s - loss: 0.0102 - accuracy: 0.9988  - ETA: 31s - loss: 0.0183 - accuracy: 0.996 - ETA: 21s - loss: 0.0226 - accuracy: 0.995 - ETA: 17s - loss: 0.0211 - accuracy: 0.995 - ETA: 14s - loss: 0.0203 - accuracy: 0.995 - ETA: 12s - loss: 0.0203 - accuracy: 0.995 - ETA: 10s - loss: 0.0209 - accuracy: 0.995 - ETA: 10s - loss: 0.0213 - accuracy: 0.994 - ETA: 9s - loss: 0.0208 - accuracy: 0.994 - ETA: 8s - loss: 0.0199 - accuracy: 0.99 - ETA: 7s - loss: 0.0202 - accuracy: 0.99 - ETA: 7s - loss: 0.0202 - accuracy: 0.99 - ETA: 7s - loss: 0.0200 - accuracy: 0.99 - ETA: 6s - loss: 0.0195 - accuracy: 0.99 - ETA: 6s - loss: 0.0196 - accuracy: 0.99 - ETA: 6s - loss: 0.0192 - accuracy: 0.99 - ETA: 5s - loss: 0.0198 - accuracy: 0.99 - ETA: 5s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0198 - accuracy: 0.99 - ETA: 5s - loss: 0.0205 - accuracy: 0.99 - ETA: 5s - loss: 0.0202 - accuracy: 0.99 - ETA: 4s - loss: 0.0199 - accuracy: 0.99 - ETA: 4s - loss: 0.0196 - accuracy: 0.99 - ETA: 4s - loss: 0.0192 - accuracy: 0.99 - ETA: 4s - loss: 0.0195 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 4s - loss: 0.0201 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0196 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0199 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 3s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0192 - accuracy: 0.99 - ETA: 2s - loss: 0.0191 - accuracy: 0.99 - ETA: 2s - loss: 0.0191 - accuracy: 0.99 - ETA: 2s - loss: 0.0194 - accuracy: 0.99 - ETA: 2s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0198 - accuracy: 0.99 - ETA: 2s - loss: 0.0197 - accuracy: 0.99 - ETA: 2s - loss: 0.0196 - accuracy: 0.99 - ETA: 2s - loss: 0.0195 - accuracy: 0.99 - ETA: 2s - loss: 0.0193 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0192 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0191 - accuracy: 0.99 - ETA: 1s - loss: 0.0190 - accuracy: 0.99 - ETA: 1s - loss: 0.0189 - accuracy: 0.99 - ETA: 1s - loss: 0.0189 - accuracy: 0.99 - ETA: 1s - loss: 0.0187 - accuracy: 0.99 - ETA: 1s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0186 - accuracy: 0.99 - ETA: 0s - loss: 0.0187 - accuracy: 0.99 - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0183 - accuracy: 0.99 - ETA: 0s - loss: 0.0181 - accuracy: 0.99 - ETA: 0s - loss: 0.0181 - accuracy: 0.99 - ETA: 0s - loss: 0.0180 - accuracy: 0.99 - ETA: 0s - loss: 0.0180 - accuracy: 0.9958\r\nLearning rate for epoch 12 is 9.999999747378752e-06\r\n938/938 [==============================] - 5s 5ms/step - loss: 0.0180 - accuracy: 0.9958\r\n<tensorflow.python.keras.callbacks.History at 0x1715158c048>\r\n\r\n**Describe the expected behavior**\r\nYou get a neat concise overview of training on the linux EC2 installations.  Just as it should be.\r\n\r\n**Code to reproduce the issue**\r\nYes, cleanly install Windows 10, update to the latest 19.03 release with Windows updates, install the latest Anaconda, install the Tensorflow 2 beta 1 binary, and run this notebook locally.\r\n\r\n**Other info / logs**\r\nNil", "comments": ["@dbonner Did you ran this with multi-gpu hardware or no-gpu hardware? I can reproduce the issue but the log is slightly different. I think it is using 8  threads in my haraware. See the log below.\r\n\r\n```\r\nmodel.fit(train_dataset, epochs=12, callbacks=callbacks)\r\nTrain on None steps\r\nEpoch 1/12\r\n    938/Unknown - 31s 33ms/step - loss: 0.2033 - accuracy: 0.9408   4/Unknown - 5s 1s/step - loss: 2.2097 - accuracy: 0.1719     36/Unknown - 6s 176ms/step - loss: 1.1959 - accuracy: 0.6701 - 8s 88ms/step - loss: 0.7144 - accuracy: 0.8002    101/Unknown - 8s 81ms/step - loss: 0.6679 - accuracy: 0.8117 - 9s 74ms/step - loss: 0.6143 - accuracy: 0.8265 - 11s 57ms/step - loss: 0.4730 - accuracy: 0.8649 - 11s 56ms/step - loss: 0.4682 - accuracy: 0.8662 - 16s 43ms/step - loss: 0.3399 - accuracy: 0.9015 - 17s 42ms/step - loss: 0.3304 - accuracy: 0.9040 - 18s 40ms/step - loss: 0.3070 - accuracy: 0.9112    480/Unknown - 19s 40ms/step - loss: 0.2978 - accuracy: 0.9138 - 19s 39ms/step - loss: 0.2914 - accuracy: 0.9156    544/Unknown - 21s 38ms/step - loss: 0.2757 - accuracy: 0.9200    575/Unknown - 22s 38ms/step - loss: 0.2676 - accuracy: 0.9224    656/Unknown - 24s 37ms/step - loss: 0.2478 - accuracy: 0.9279    741/Unknown - 27s 36ms/step - loss: 0.2328 - accuracy: 0.9324    788/Unknown - 28s 36ms/step - loss: 0.2242 - accuracy: 0.9346 - 30s 35ms/step - loss: 0.2134 - accuracy: 0.9379    885/Unknown - 30s 34ms/step - loss: 0.2103 - accuracy: 0.9387    901/Unknown - 31s 34ms/step - loss: 0.2080 - accuracy: 0.9394    917/Unknown - 31s 34ms/step - loss: 0.2061 - accuracy: 0.9400 - 31s 34ms/step - loss: 0.2047 - accuracy: 0.9404\r\nLearning rate for epoch 1 is 0.0010000000474974513\r\n938/938 [==============================] - 32s 34ms/step - loss: 0.2033 - accuracy: 0.9408\r\nEpoch 2/12\r\n936/938 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9801    - ETA: 54s - loss: 0.0989 - accuracy: 0.9711 - ETA: 17s - loss: 0.0747 - accuracy: 0.9781865/938 [==========================>...] - ETA: 2s - loss: 0.0679 - accuracy: 0.9799\r\nLearning rate for epoch 2 is 0.0010000000474974513\r\n938/938 [==============================] - 27s 28ms/step - loss: 0.0675 - accuracy: 0.9801\r\nEpoch 3/12\r\n935/938 [============================>.] - ETA: 0s - loss: 0.0473 - accuracy: 0.98601   - ETA: 53s - loss: 0.0598 - accuracy: 0.9825 - ETA: 51s - loss: 0.0573 - accuracy: 0.9834 - ETA: 20s - loss: 0.0520 - accuracy: 0.9847 - ETA: 18s - loss: 0.0512 - accuracy: 0.9848407/938 [============>.................] - ETA: 16s - loss: 0.0502 - accuracy: 0.9850607/938 [==================>...........] - ETA: 9s - loss: 0.0500 - accuracy: 0.9850 745/938 [======================>.......] - ETA: 5s - loss: 0.0488 - accuracy: 0.9858 - ETA: 3s - loss: 0.0481 - accuracy: 0.9859\r\nLearning rate for epoch 3 is 0.0010000000474974513\r\n938/938 [==============================] - 26s 28ms/step - loss: 0.0472 - accuracy: 0.9860\r\nEpoch 4/12\r\n936/938 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9931    - ETA: 2:29 - loss: 0.0424 - accuracy: 0.9888376/938 [===========>..................] - ETA: 17s - loss: 0.0331 - accuracy: 0.9901 - ETA: 12s - loss: 0.0309 - accuracy: 0.9909721/938 [======================>.......] - ETA: 6s - loss: 0.0284 - accuracy: 0.9921921/938 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9931\r\nLearning rate for epoch 4 is 9.999999747378752e-05\r\n938/938 [==============================] - 25s 26ms/step - loss: 0.0263 - accuracy: 0.9931\r\nEpoch 5/12\r\n935/938 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9939   302/938 [========>.....................] - ETA: 20s - loss: 0.0276 - accuracy: 0.9919 - ETA: 16s - loss: 0.0256 - accuracy: 0.9926605/938 [==================>...........] - ETA: 9s - loss: 0.0245 - accuracy: 0.9932\r\nLearning rate for epoch 5 is 9.999999747378752e-05\r\n938/938 [==============================] - 25s 27ms/step - loss: 0.0228 - accuracy: 0.9939\r\nEpoch 6/12\r\n937/938 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9947    - ETA: 47s - loss: 0.0221 - accuracy: 0.9940228/938 [======>.......................] - ETA: 24s - loss: 0.0234 - accuracy: 0.9934 - ETA: 16s - loss: 0.0232 - accuracy: 0.9936467/938 [=============>................] - ETA: 14s - loss: 0.0238 - accuracy: 0.9935 - ETA: 11s - loss: 0.0228 - accuracy: 0.9938568/938 [=================>............] - ETA: 10s - loss: 0.0226 - accuracy: 0.9939634/938 [===================>..........] - ETA: 8s - loss: 0.0222 - accuracy: 0.9941 - ETA: 7s - loss: 0.0219 - accuracy: 0.9942\r\nLearning rate for epoch 6 is 9.999999747378752e-05\r\n938/938 [==============================] - 25s 27ms/step - loss: 0.0209 - accuracy: 0.9947\r\nEpoch 7/12\r\n937/938 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9951    - ETA: 1:19 - loss: 0.0206 - accuracy: 0.9937 - ETA: 41s - loss: 0.0256 - accuracy: 0.9927128/938 [===>..........................] - ETA: 35s - loss: 0.0238 - accuracy: 0.9934 - ETA: 18s - loss: 0.0211 - accuracy: 0.9943 - ETA: 16s - loss: 0.0207 - accuracy: 0.9946459/938 [=============>................] - ETA: 16s - loss: 0.0207 - accuracy: 0.9946 - ETA: 11s - loss: 0.0203 - accuracy: 0.9946691/938 [=====================>........] - ETA: 9s - loss: 0.0201 - accuracy: 0.9947 - ETA: 8s - loss: 0.0200 - accuracy: 0.9947713/938 [=====================>........] - ETA: 8s - loss: 0.0199 - accuracy: 0.9947 - ETA: 0s - loss: 0.0196 - accuracy: 0.9951\r\nLearning rate for epoch 7 is 9.999999747378752e-05\r\n938/938 [==============================] - 34s 36ms/step - loss: 0.0193 - accuracy: 0.9951\r\nEpoch 8/12\r\n937/938 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9962    - ETA: 11:25 - loss: 0.0237 - accuracy: 0.9969 59/938 [>.............................] - ETA: 1:29 - loss: 0.0253 - accuracy: 0.9947 85/938 [=>............................] - ETA: 1:10 - loss: 0.0241 - accuracy: 0.9952128/938 [===>..........................] - ETA: 55s - loss: 0.0196 - accuracy: 0.9961 - ETA: 48s - loss: 0.0205 - accuracy: 0.9951 - ETA: 33s - loss: 0.0199 - accuracy: 0.9949 - ETA: 22s - loss: 0.0183 - accuracy: 0.9955 - ETA: 15s - loss: 0.0177 - accuracy: 0.9959642/938 [===================>..........] - ETA: 14s - loss: 0.0178 - accuracy: 0.9958798/938 [========================>.....] - ETA: 6s - loss: 0.0173 - accuracy: 0.9959837/938 [=========================>....] - ETA: 4s - loss: 0.0171 - accuracy: 0.9960\r\nLearning rate for epoch 8 is 9.999999747378752e-06\r\n938/938 [==============================] - 37s 40ms/step - loss: 0.0168 - accuracy: 0.9962\r\nEpoch 9/12\r\n937/938 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9965   243/938 [======>.......................] - ETA: 23s - loss: 0.0188 - accuracy: 0.9959304/938 [========>.....................] - ETA: 20s - loss: 0.0187 - accuracy: 0.9959358/938 [==========>...................] - ETA: 18s - loss: 0.0188 - accuracy: 0.9959 - ETA: 16s - loss: 0.0186 - accuracy: 0.9959\r\nLearning rate for epoch 9 is 9.999999747378752e-06\r\n938/938 [==============================] - 25s 26ms/step - loss: 0.0165 - accuracy: 0.9965\r\nEpoch 10/12\r\n936/938 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9964    - ETA: 38s - loss: 0.0197 - accuracy: 0.9949273/938 [=======>......................] - ETA: 21s - loss: 0.0185 - accuracy: 0.9957282/938 [========>.....................] - ETA: 20s - loss: 0.0187 - accuracy: 0.9957335/938 [=========>....................] - ETA: 18s - loss: 0.0182 - accuracy: 0.9958383/938 [===========>..................] - ETA: 16s - loss: 0.0185 - accuracy: 0.9956576/938 [=================>............] - ETA: 10s - loss: 0.0176 - accuracy: 0.9959607/938 [==================>...........] - ETA: 9s - loss: 0.0174 - accuracy: 0.9960 - ETA: 1s - loss: 0.0165 - accuracy: 0.9964\r\nLearning rate for epoch 10 is 9.999999747378752e-06\r\n938/938 [==============================] - 24s 26ms/step - loss: 0.0163 - accuracy: 0.9965\r\nEpoch 11/12\r\n937/938 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9965    - ETA: 36s - loss: 0.0197 - accuracy: 0.9956180/938 [====>.........................] - ETA: 28s - loss: 0.0180 - accuracy: 0.9960211/938 [=====>........................] - ETA: 26s - loss: 0.0177 - accuracy: 0.9959262/938 [=======>......................] - ETA: 22s - loss: 0.0175 - accuracy: 0.9958 - ETA: 21s - loss: 0.0173 - accuracy: 0.9958368/938 [==========>...................] - ETA: 17s - loss: 0.0173 - accuracy: 0.9958 - ETA: 17s - loss: 0.0176 - accuracy: 0.9958\r\nLearning rate for epoch 11 is 9.999999747378752e-06\r\n938/938 [==============================] - 25s 27ms/step - loss: 0.0161 - accuracy: 0.9965\r\nEpoch 12/12\r\n937/938 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9966    - ETA: 36s - loss: 0.0186 - accuracy: 0.9965 - ETA: 27s - loss: 0.0211 - accuracy: 0.9954300/938 [========>.....................] - ETA: 19s - loss: 0.0188 - accuracy: 0.9958354/938 [==========>...................] - ETA: 17s - loss: 0.0184 - accuracy: 0.9959 - ETA: 16s - loss: 0.0183 - accuracy: 0.9959423/938 [============>.................] - ETA: 15s - loss: 0.0180 - accuracy: 0.9961 - ETA: 14s - loss: 0.0180 - accuracy: 0.9961\r\nLearning rate for epoch 12 is 9.999999747378752e-06\r\n938/938 [==============================] - 25s 26ms/step - loss: 0.0159 - accuracy: 0.9966\r\n```\r\n\r\n", "I used a single GPU, an RTX 2080 to.  The badly formatted output occured in\nJupyter notebook.\n\nOn Fri., 9 Aug. 2019, 12:58 am Vishnuvardhan Janapati, <\nnotifications@github.com> wrote:\n\n> @dbonner <https://github.com/dbonner> Did you ran this with multi-gpu\n> hardware or no-gpu hardware? I can reproduce the issue but the log is\n> slightly different. I think it is using 8 threads in my haraware. See the\n> log below.\n>\n> model.fit(train_dataset, epochs=12, callbacks=callbacks)\n> Train on None steps\n> Epoch 1/12\n>     938/Unknown - 31s 33ms/step - loss: 0.2033 - accuracy: 0.9408   4/Unknown - 5s 1s/step - loss: 2.2097 - accuracy: 0.1719     36/Unknown - 6s 176ms/step - loss: 1.1959 - accuracy: 0.6701 - 8s 88ms/step - loss: 0.7144 - accuracy: 0.8002    101/Unknown - 8s 81ms/step - loss: 0.6679 - accuracy: 0.8117 - 9s 74ms/step - loss: 0.6143 - accuracy: 0.8265 - 11s 57ms/step - loss: 0.4730 - accuracy: 0.8649 - 11s 56ms/step - loss: 0.4682 - accuracy: 0.8662 - 16s 43ms/step - loss: 0.3399 - accuracy: 0.9015 - 17s 42ms/step - loss: 0.3304 - accuracy: 0.9040 - 18s 40ms/step - loss: 0.3070 - accuracy: 0.9112    480/Unknown - 19s 40ms/step - loss: 0.2978 - accuracy: 0.9138 - 19s 39ms/step - loss: 0.2914 - accuracy: 0.9156    544/Unknown - 21s 38ms/step - loss: 0.2757 - accuracy: 0.9200    575/Unknown - 22s 38ms/step - loss: 0.2676 - accuracy: 0.9224    656/Unknown - 24s 37ms/step - loss: 0.2478 - accuracy: 0.9279    741/Unknown - 27s 36ms/step - loss: 0.2328 - accuracy: 0.9324    788/Unknown - 28s 36ms/step - loss: 0.2242 - accuracy: 0.9346 - 30s 35ms/step - loss: 0.2134 - accuracy: 0.9379    885/Unknown - 30s 34ms/step - loss: 0.2103 - accuracy: 0.9387    901/Unknown - 31s 34ms/step - loss: 0.2080 - accuracy: 0.9394    917/Unknown - 31s 34ms/step - loss: 0.2061 - accuracy: 0.9400 - 31s 34ms/step - loss: 0.2047 - accuracy: 0.9404\n> Learning rate for epoch 1 is 0.0010000000474974513\n> 938/938 [==============================] - 32s 34ms/step - loss: 0.2033 - accuracy: 0.9408\n> Epoch 2/12\n> 936/938 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9801    - ETA: 54s - loss: 0.0989 - accuracy: 0.9711 - ETA: 17s - loss: 0.0747 - accuracy: 0.9781865/938 [==========================>...] - ETA: 2s - loss: 0.0679 - accuracy: 0.9799\n> Learning rate for epoch 2 is 0.0010000000474974513\n> 938/938 [==============================] - 27s 28ms/step - loss: 0.0675 - accuracy: 0.9801\n> Epoch 3/12\n> 935/938 [============================>.] - ETA: 0s - loss: 0.0473 - accuracy: 0.98601   - ETA: 53s - loss: 0.0598 - accuracy: 0.9825 - ETA: 51s - loss: 0.0573 - accuracy: 0.9834 - ETA: 20s - loss: 0.0520 - accuracy: 0.9847 - ETA: 18s - loss: 0.0512 - accuracy: 0.9848407/938 [============>.................] - ETA: 16s - loss: 0.0502 - accuracy: 0.9850607/938 [==================>...........] - ETA: 9s - loss: 0.0500 - accuracy: 0.9850 745/938 [======================>.......] - ETA: 5s - loss: 0.0488 - accuracy: 0.9858 - ETA: 3s - loss: 0.0481 - accuracy: 0.9859\n> Learning rate for epoch 3 is 0.0010000000474974513\n> 938/938 [==============================] - 26s 28ms/step - loss: 0.0472 - accuracy: 0.9860\n> Epoch 4/12\n> 936/938 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9931    - ETA: 2:29 - loss: 0.0424 - accuracy: 0.9888376/938 [===========>..................] - ETA: 17s - loss: 0.0331 - accuracy: 0.9901 - ETA: 12s - loss: 0.0309 - accuracy: 0.9909721/938 [======================>.......] - ETA: 6s - loss: 0.0284 - accuracy: 0.9921921/938 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9931\n> Learning rate for epoch 4 is 9.999999747378752e-05\n> 938/938 [==============================] - 25s 26ms/step - loss: 0.0263 - accuracy: 0.9931\n> Epoch 5/12\n> 935/938 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9939   302/938 [========>.....................] - ETA: 20s - loss: 0.0276 - accuracy: 0.9919 - ETA: 16s - loss: 0.0256 - accuracy: 0.9926605/938 [==================>...........] - ETA: 9s - loss: 0.0245 - accuracy: 0.9932\n> Learning rate for epoch 5 is 9.999999747378752e-05\n> 938/938 [==============================] - 25s 27ms/step - loss: 0.0228 - accuracy: 0.9939\n> Epoch 6/12\n> 937/938 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9947    - ETA: 47s - loss: 0.0221 - accuracy: 0.9940228/938 [======>.......................] - ETA: 24s - loss: 0.0234 - accuracy: 0.9934 - ETA: 16s - loss: 0.0232 - accuracy: 0.9936467/938 [=============>................] - ETA: 14s - loss: 0.0238 - accuracy: 0.9935 - ETA: 11s - loss: 0.0228 - accuracy: 0.9938568/938 [=================>............] - ETA: 10s - loss: 0.0226 - accuracy: 0.9939634/938 [===================>..........] - ETA: 8s - loss: 0.0222 - accuracy: 0.9941 - ETA: 7s - loss: 0.0219 - accuracy: 0.9942\n> Learning rate for epoch 6 is 9.999999747378752e-05\n> 938/938 [==============================] - 25s 27ms/step - loss: 0.0209 - accuracy: 0.9947\n> Epoch 7/12\n> 937/938 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9951    - ETA: 1:19 - loss: 0.0206 - accuracy: 0.9937 - ETA: 41s - loss: 0.0256 - accuracy: 0.9927128/938 [===>..........................] - ETA: 35s - loss: 0.0238 - accuracy: 0.9934 - ETA: 18s - loss: 0.0211 - accuracy: 0.9943 - ETA: 16s - loss: 0.0207 - accuracy: 0.9946459/938 [=============>................] - ETA: 16s - loss: 0.0207 - accuracy: 0.9946 - ETA: 11s - loss: 0.0203 - accuracy: 0.9946691/938 [=====================>........] - ETA: 9s - loss: 0.0201 - accuracy: 0.9947 - ETA: 8s - loss: 0.0200 - accuracy: 0.9947713/938 [=====================>........] - ETA: 8s - loss: 0.0199 - accuracy: 0.9947 - ETA: 0s - loss: 0.0196 - accuracy: 0.9951\n> Learning rate for epoch 7 is 9.999999747378752e-05\n> 938/938 [==============================] - 34s 36ms/step - loss: 0.0193 - accuracy: 0.9951\n> Epoch 8/12\n> 937/938 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9962    - ETA: 11:25 - loss: 0.0237 - accuracy: 0.9969 59/938 [>.............................] - ETA: 1:29 - loss: 0.0253 - accuracy: 0.9947 85/938 [=>............................] - ETA: 1:10 - loss: 0.0241 - accuracy: 0.9952128/938 [===>..........................] - ETA: 55s - loss: 0.0196 - accuracy: 0.9961 - ETA: 48s - loss: 0.0205 - accuracy: 0.9951 - ETA: 33s - loss: 0.0199 - accuracy: 0.9949 - ETA: 22s - loss: 0.0183 - accuracy: 0.9955 - ETA: 15s - loss: 0.0177 - accuracy: 0.9959642/938 [===================>..........] - ETA: 14s - loss: 0.0178 - accuracy: 0.9958798/938 [========================>.....] - ETA: 6s - loss: 0.0173 - accuracy: 0.9959837/938 [=========================>....] - ETA: 4s - loss: 0.0171 - accuracy: 0.9960\n> Learning rate for epoch 8 is 9.999999747378752e-06\n> 938/938 [==============================] - 37s 40ms/step - loss: 0.0168 - accuracy: 0.9962\n> Epoch 9/12\n> 937/938 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9965   243/938 [======>.......................] - ETA: 23s - loss: 0.0188 - accuracy: 0.9959304/938 [========>.....................] - ETA: 20s - loss: 0.0187 - accuracy: 0.9959358/938 [==========>...................] - ETA: 18s - loss: 0.0188 - accuracy: 0.9959 - ETA: 16s - loss: 0.0186 - accuracy: 0.9959\n> Learning rate for epoch 9 is 9.999999747378752e-06\n> 938/938 [==============================] - 25s 26ms/step - loss: 0.0165 - accuracy: 0.9965\n> Epoch 10/12\n> 936/938 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9964    - ETA: 38s - loss: 0.0197 - accuracy: 0.9949273/938 [=======>......................] - ETA: 21s - loss: 0.0185 - accuracy: 0.9957282/938 [========>.....................] - ETA: 20s - loss: 0.0187 - accuracy: 0.9957335/938 [=========>....................] - ETA: 18s - loss: 0.0182 - accuracy: 0.9958383/938 [===========>..................] - ETA: 16s - loss: 0.0185 - accuracy: 0.9956576/938 [=================>............] - ETA: 10s - loss: 0.0176 - accuracy: 0.9959607/938 [==================>...........] - ETA: 9s - loss: 0.0174 - accuracy: 0.9960 - ETA: 1s - loss: 0.0165 - accuracy: 0.9964\n> Learning rate for epoch 10 is 9.999999747378752e-06\n> 938/938 [==============================] - 24s 26ms/step - loss: 0.0163 - accuracy: 0.9965\n> Epoch 11/12\n> 937/938 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9965    - ETA: 36s - loss: 0.0197 - accuracy: 0.9956180/938 [====>.........................] - ETA: 28s - loss: 0.0180 - accuracy: 0.9960211/938 [=====>........................] - ETA: 26s - loss: 0.0177 - accuracy: 0.9959262/938 [=======>......................] - ETA: 22s - loss: 0.0175 - accuracy: 0.9958 - ETA: 21s - loss: 0.0173 - accuracy: 0.9958368/938 [==========>...................] - ETA: 17s - loss: 0.0173 - accuracy: 0.9958 - ETA: 17s - loss: 0.0176 - accuracy: 0.9958\n> Learning rate for epoch 11 is 9.999999747378752e-06\n> 938/938 [==============================] - 25s 27ms/step - loss: 0.0161 - accuracy: 0.9965\n> Epoch 12/12\n> 937/938 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9966    - ETA: 36s - loss: 0.0186 - accuracy: 0.9965 - ETA: 27s - loss: 0.0211 - accuracy: 0.9954300/938 [========>.....................] - ETA: 19s - loss: 0.0188 - accuracy: 0.9958354/938 [==========>...................] - ETA: 17s - loss: 0.0184 - accuracy: 0.9959 - ETA: 16s - loss: 0.0183 - accuracy: 0.9959423/938 [============>.................] - ETA: 15s - loss: 0.0180 - accuracy: 0.9961 - ETA: 14s - loss: 0.0180 - accuracy: 0.9961\n> Learning rate for epoch 12 is 9.999999747378752e-06\n> 938/938 [==============================] - 25s 26ms/step - loss: 0.0159 - accuracy: 0.9966\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30908?email_source=notifications&email_token=AAB26QXNNXM7OA7KNVJSLCLQDQYA5A5CNFSM4IFSMUSKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD333WCQ#issuecomment-519551754>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAB26QXXU6XE5DZ7ELJSUH3QDQYA5ANCNFSM4IFSMUSA>\n> .\n>\n", "@dbonner I think it is more related to Jupyter notebook. I have checked with Python IDE, python using Spyder, and Google colab. None of them show the log like Jupyter notebook. You need to check some settings there. I have noticed couple of other issues (raised by Jupyter notebook users) related to logging (error/warning) like you. Please check Jupyter community which might have better solution. Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!"]}, {"number": 30907, "title": "tf.contrib.factorization.WALSMatrixFactorization( AttributeError: 'module' object has no attribute 'WALSMatrixFactorization')", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI just run walsmodel through gcloud ml-engine jobs\r\nI run it through my mac terminal, I didn't change any code over here https://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/machine_learning/deepdive/10_recommend/walsmodel\r\n\r\nBut it always has the following error:\r\nexperiment_fn tf.contrib.factorization.WALSMatrixFactorization( AttributeError: 'module' object has no attribute 'WALSMatrixFactorization'\r\n\r\n`gcloud ml-engine jobs submit training wals_190721_012226 --region=us-east1 --module-name=walsmodel.task --runtime-version 1.14 --python-version 3.5 --package-path=/home/mabodx/anguis/news_recommendation/10_recommend/walsmodel --job-dir=gs://buzzbreak/news_recommendation_2019_07_21T01_18_02Z/ --staging-bucket=gs://buzzbreak --scale-tier=BASIC_GPU --runtime-version= -- --output_dir=gs://buzzbreak/news_recommendation_2019_07_21T01_18_02Z/model_trained_190721_012212 --input_path=gs://buzzbreak/news_recommendation_2019_07_21T01_18_02Z/news/data --num_epochs=500 --nitems=39681 --nusers=38781 --topk=1000`\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nhttps://github.com/GoogleCloudPlatform/training-data-analyst/tree/master/courses/machine_learning/deepdive/10_recommend/walsmodel\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@mabodx Will it be possible to provide the minimal code snippet to reproduce the issue. Thanks!\r\n", "@gadagashwini  I just use code over here \r\nhttps://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/10_recommend/walsmodel/model.py#L25\r\n\r\nNever change anything", "@mabodx I tried executing the code given in the file model.py but i didn't see any error. Let us know what is the expected behavior. Thanks! ", "@gadagashwini \r\nI just try to run this \r\n```\r\nRun on Cloud\r\nIn [ ]:\r\n%%bash\r\ngsutil -m cp data/* gs://${BUCKET}/wals/data\r\nIn [ ]:\r\n%%bash\r\nOUTDIR=gs://${BUCKET}/wals/model_trained\r\nJOBNAME=wals_$(date -u +%y%m%d_%H%M%S)\r\necho $OUTDIR $REGION $JOBNAME\r\ngsutil -m rm -rf $OUTDIR\r\ngcloud ml-engine jobs submit training $JOBNAME \\\r\n    --region=$REGION \\\r\n    --module-name=walsmodel.task \\\r\n    --package-path=${PWD}/walsmodel \\\r\n    --job-dir=$OUTDIR \\\r\n    --staging-bucket=gs://$BUCKET \\\r\n    --scale-tier=BASIC_GPU \\\r\n    --runtime-version=$TFVERSION \\\r\n    -- \\\r\n    --output_dir=$OUTDIR \\\r\n    --input_path=gs://${BUCKET}/wals/data \\\r\n    --num_epochs=10 --nitems=5668 --nusers=82802\r\n\r\n```\r\nI get this code from this notebook in before `In[36]`\r\nhttps://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive/10_recommend/wals.ipynb\r\n\r\n![image](https://user-images.githubusercontent.com/4045116/61628733-b65b3700-acb5-11e9-8105-ccd29a2e9615.png)\r\n\r\n\r\nIf you run this from any terminal through command,  you will get the above error (Don't run it through Notebook, because notebook works fine). \r\nI suppose this is because version error with google cloud sdk version if you run it through terminal (which is different from google notebook)", "Apologies for the delay in response. \r\n`tf.contrib` module is deprecated and has moved out of TensorFlow starting 2.X\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30907\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30907\">No</a>\n"]}, {"number": 30906, "title": "tf.scatter_nd_update missing", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.0.0-beta1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n Tensorflow 2.0 does not have the function tf.scatter_nd_update. Is this normal ? \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\ntf.scatter_nd_update\r\nAttributeError: module 'tensorflow' has no attribute 'scatter_nd_update'\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["did you ever solve this?"]}, {"number": 30905, "title": "Failed to install tensorflow-estimator", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10.0.18362.0\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip / binary\r\n- TensorFlow version:\r\n1.14 (?)\r\n- Python version:\r\nPython 3.7.4 (installed from Windows Store)\r\n- Installed using virtualenv? pip? conda?:\r\npip 19.1.1\r\n- Bazel version (if compiling from source):\r\nn/a\r\n- GCC/Compiler version (if compiling from source):\r\nn/a\r\n- CUDA/cuDNN version:\r\nn/a\r\n- GPU model and memory:\r\nNvidia GeForce GTX 1080; 16 GB\r\n\r\n**Describe the problem**\r\n\r\nWhen trying to install Tensorflow, I get an error while installing the Tensorflow-Estimator dependency. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n* Open `cmd.exe` as administrator\r\n\r\n```\r\npip install --upgrade tensorflow-estimator\r\n\r\nERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\movgp\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python37\\\\site-packages\\\\tensorflow_estimator\\\\python\\\\estimator\\\\canned\\\\linear_optimizer\\\\python\\\\utils\\\\__pycache__\\\\sharded_mutable_dense_hashtable.cpython-37.pyc'\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\n Directory of C:\\Users\\movgp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\linear_optimizer\\python\\utils\\__pycache__\r\n\r\n2019-07-21  09:46    <DIR>          .\r\n2019-07-21  09:46    <DIR>          ..\r\n2019-07-21  09:46            21,504 sdca_ops.cpython-37.pyc\r\n               1 File(s)         21,504 bytes\r\n```\r\n", "comments": ["@MovGP0 \r\nJust to verify did you get chance to follow instructions from [TensorFlow](https://www.tensorflow.org/install/pip) website .Please, let us know. \r\nAlso, please go through the below link and see if it helps you.Thanks!\r\nhttps://stackoverflow.com/questions/55739873/tf-estimator-package-not-installed\r\n", "I'm having the same problem, I tried both links and neither worked. I also tried the solution to this link, but it didn't work either.\r\nhttps://github.com/googleapis/google-cloud-python/issues/6647", "did you try rerunning the command?\r\nOn windows, sometimes we see spurious failures.", "@gunan Sure. First thing I've tried. ", "I definitely think this is a pip issue, but not sure where to look for a solution.\r\nMaybe the path is too long and pip has problems?", "I had the exact same problem. Here is what I did to make the installation work:\r\n\r\n- Downloaded python from here : https://www.python.org/downloads/release/python-374/\r\n\r\n- Ran the installer and installed for all users (I did this because the path length is shorter, who knows if that was the difference). Additionally I also installed pip through this installer as well.\r\n\r\n- At the end of the installation I clicked on the button to allow PATH lengths to exceed 260 characters. \r\n\r\nAfter doing this and re-running \"pip install tensorflow-gpu\" it passed the error shown above. My guess would be that python installed from the windows store will, by-default, install to a path which is too long or fail to bypass the 260 path-length restriction. Previously I was using the version of Python that came with my Windows 10 installation. ", "On Windows 10, you could disable the path length restriction\r\n\r\n```powershell\r\n# run as admin\r\n$path = 'HKLM:\\SYSTEM\\CurrentControlSet\\Control\\FileSystem';\r\n$propertyName = 'LongPathsEnabled';\r\nSet-ItemProperty -Path $path -Name $propertyName -Value 1;\r\n```\r\n\r\nCreate a symbolic link or hard link:\r\n\r\n```powershell\r\n# run as admin\r\n$target = (ls ~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7* | Select -First 1).FullName;\r\n$targetPath = Join-Path -Path $target -ChildPath '\\LocalCache\\local-packages\\Python37';\r\n$path = 'C:\\Python37';\r\n\r\nNew-Item -ItemType Junction -Path $path -Target $targetPath;\r\n# New-Item -ItemType HardLink -Path $path -Target $targetPath;\r\n```\r\n\r\nUpdate the environment variables. \r\n\r\n```powershell\r\n$newPath = $Env:Path.Replace($targetPath, 'C:\\Python37');\r\n[System.Environment]::SetEnvironmentVariable('Path', $newPath);\r\n```\r\n\r\nRestart the computer\r\n\r\n```powershell\r\nRestart-Computer\r\n```\r\n\r\nThis way we could have a store-managed version of python that supports the longer paths. \r\n\r\n### Testing the configuration\r\n```cmd\r\npip install --upgrade tensorflow-estimator\r\n\r\nCollecting tensorflow-estimator\r\n  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\r\nInstalling collected packages: tensorflow-estimator\r\nSuccessfully installed tensorflow-estimator-1.14.0\r\n```", "@gunan can you update the documentation? thank you! ", "@lamberta It may be useful to mention disabling path length restriction may help resolve pip installation issues on windows.\r\n\r\n@MovGP0 did disabling tpath length restriction solve the problem for you?\r\nShould we close the issue?", "Thanks, but I think we already have this in the docs: https://www.tensorflow.org/install/pip\r\nSelect Windows tab ...\r\n*Make sure [long paths are enabled](https://superuser.com/questions/1119883/windows-10-enable-ntfs-long-paths-policy-option-missing) on Windows.*\r\n\r\nIs something else needed? (That page [is here in GitHub](https://github.com/tensorflow/docs/blob/master/site/en/install/pip.html))"]}, {"number": 30904, "title": "Supporting dynamic shape tensor in sparse_image_warp", "body": "Hello,\r\nThe current version of `sparse_image_warp.py` only support static shaped tensors. (The Issue regarding this problem has been already in Issues board(#28431))\r\nThis PR is for upgrading [`sparse_image_warp.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/image/python/ops/sparse_image_warp.py) to support computations of tensor with dynamic shape. \r\nInspired by dynamical support of dense_image_warp function in #23394, I started with changes in `get_shape()`; however, this requires more operational changes including `math_ops` and `array_ops`.\r\n\r\nThe functional test is also done with time_warping operation in SpecAugment which requires `sparse_image_warp`. ", "comments": ["Hi @comc35!\r\n\r\nThank you for taking a shot at fixing this.\r\n\r\nBut the TensorFlow 2.0 release is just around the corner.\r\n\r\n`tf.contrib` is not included in TensorFlow 2.0. It will be deleted.\r\n\r\nThis functionality now lives in the [tensorflow/addons repository](https://github.com/tensorflow/addons/tree/master/tensorflow_addons/image)  please resubmit your PR there. \r\n"]}, {"number": 30903, "title": "Duplication indices in the results of hessian_vector_product()", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code.\r\n- OS Platform and Distribution: Ubuntu 18.04.\r\n- TensorFlow installed from pip\r\n- TensorFlow version 1.14 with GPU support\r\n- Python version: 3.7.3\r\n- CUDA/cuDNN version: 10\r\n\r\n**Describe the current behavior**\r\n\r\nHi, when I am using the hessian_vector_product() to computer the hessian_vector_product of my model, it returns results organized as IndexedSliceValue. This is OK because the model include data structure like embeddings. \r\n\r\nHowever, when I do the assert on the indices of the returned IndexedSliceValue. I notice that there are duplicated indices with different corresponding values. May I know how to deal with these conflict <indices, values>? Thanks in advance.\r\n", "comments": ["@ZHTLoveYC ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 30902, "title": "Format tf.function's error message when input and signature does not match", "body": "This fix tries to address the issue raised in #30576 where the error message is hard to interpret:\r\n```\r\nValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'random_normal:0' shape=(1, 123, 1) dtype=float32>, <tf.Tensor 'random_normal_1:0' shape=(1, 123, 2) dtype=float32>, <tf.Tensor 'random_normal_2:0' shape=(1, 123, 3) dtype=float32>, <tf.Tensor 'random_normal_3:0' shape=(1, 123, 4) dtype=float32>, <tf.Tensor 'random_normal_4:0' shape=(1, 123, 5) dtype=float32>, <tf.Tensor 'random_normal_5:0' shape=(1, 123, 6) dtype=float32>, <tf.Tensor 'random_normal_6:0' shape=(1, 123, 7) dtype=float32>, <tf.Tensor 'random_normal_7:0' shape=(1, 123, 8) dtype=float32>, <tf.Tensor 'random_normal_8:0' shape=(1, 123, 1) dtype=float32>)), input_signature ((TensorSpec(shape=(?, ?, 1), dtype=tf.float32, name=None), TensorSpec(shape=(?, ?, 2), dtype=tf.float32, name=None), TensorSpec(shape=(?, ?, 3), dtype=tf.float32, name=None), TensorSpec(shape=(?, ?, 4), dtype=tf.float32, name=None), TensorSpec(shape=(?, ?, 5), dtype=tf.float32, name=None), TensorSpec(shape=(?, ?, 6), dtype=tf.float32, name=None), TensorSpec(shape=(?, ?, 7), dtype=tf.float32, name=None), TensorSpec(shape=(?, ?, 8), dtype=tf.float32, name=None), TensorSpec(shape=(?, ?, 9), dtype=tf.float32, name=None)))\r\n```\r\n\r\nThis fix formats the error message:\r\n```\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    Tensor(\"random_normal:0\", shape=(1, 123, 1), dtype=float32),\r\n    Tensor(\"random_normal_1:0\", shape=(1, 123, 2), dtype=float32),\r\n    Tensor(\"random_normal_2:0\", shape=(1, 123, 3), dtype=float32),\r\n    Tensor(\"random_normal_3:0\", shape=(1, 123, 4), dtype=float32),\r\n    Tensor(\"random_normal_4:0\", shape=(1, 123, 5), dtype=float32),\r\n    Tensor(\"random_normal_5:0\", shape=(1, 123, 6), dtype=float32),\r\n    Tensor(\"random_normal_6:0\", shape=(1, 123, 7), dtype=float32),\r\n    Tensor(\"random_normal_7:0\", shape=(1, 123, 8), dtype=float32),\r\n    Tensor(\"random_normal_8:0\", shape=(1, 123, 1), dtype=float32))\r\n  input_signature: (\r\n    TensorSpec(shape=(?, ?, 1), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(?, ?, 2), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(?, ?, 3), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(?, ?, 4), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(?, ?, 5), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(?, ?, 6), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(?, ?, 7), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(?, ?, 8), dtype=tf.float32, name=None),\r\n    TensorSpec(shape=(?, ?, 9), dtype=tf.float32, name=None))\r\n```\r\n\r\nThis fix fixes #30576.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": []}, {"number": 30901, "title": "Issue with AttentionWrapper and non unique key dictionary in feed_dict", "body": "Hello,\r\n\r\nI would like to use Bahdanau Attention in a decoder, using tf.dynamic_rnn\r\n\r\nFor my decoder, I am feeding the previous state of AttentionWrapperState at each time step.\r\n\r\n~~~\r\n    attn_meca = tf.contrib.seq2seq.BahdanauAttention(config.num_lstm_units, multimodal_feature)   \r\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(lstm_cell_expl, attn_meca, output_attention=False)\r\n    \r\n...\r\n\r\n        lstm_outputs, final_state = tf.nn.dynamic_rnn(cell=attn_cell,\r\n                                                    inputs=seq_embedding,\r\n                                                    sequence_length=sequence_length,\r\n                                                    initial_state=initial_state,\r\n                                                    dtype=tf.float32,\r\n                                                    scope=lstm_scope)\r\n~~~\r\n\r\nI am feeding the state as follows \r\n~~~\r\n feed_dict={model['input_seqs']: current_pred, \r\n                   model['initial_state']: state, \r\n                   model['input_mask']: mask, \r\n                   model['dropout_input']: 1.0,\r\n                   model['keep_prob']: keep_prob}\r\n\r\ncurrent_pred, state = sess.run([model['preds'], model['final_state']], feed_dict=feed_dict)\r\n~~~\r\n\r\nAnd I get this error :\r\n\r\n`ValueError: Could not flatten dictionary: key Tensor(\"lstm/lstm/attention_wrapper/Softmax:0\", shape=(?, ?), dtype=float32) is not unique.`\r\n\r\nI printed the state that I'm feeding:\r\n`AttentionWrapperState(cell_state=LSTMStateTuple(c=<tf.Tensor 'lstm/lstm/attention_wrapper/lstm_cell/add_1:0' shape=(?, 512) dtype=float32>, h=<tf.Tensor 'lstm/lstm/attention_wrapper/lstm_cell/mul_2:0' shape=(?, 512) dtype=float32>), attention=<tf.Tensor 'lstm/lstm/attention_wrapper/concat_1:0' shape=(?, 512) dtype=float32>, time=<tf.Tensor 'lstm/lstm/attention_wrapper/add:0' shape=() dtype=int32>, alignments=<tf.Tensor 'lstm/lstm/attention_wrapper/Softmax:0' shape=(?, ?) dtype=float32>, alignment_history=(), attention_state=<tf.Tensor 'lstm/lstm/attention_wrapper/Softmax:0' shape=(?, ?) dtype=float32>)`\r\n\r\nSo I see that there are two `'lstm/lstm/attention_wrapper/Softmax:0'`\r\n\r\nHow can I change that when I define my attention cell?\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapperState\r\n\r\n\r\nThanks in advance.\r\n\r\n", "comments": ["@virginie-do In order to expedite the trouble-shooting process, please provide a complete code snippet to reproduce the issue reported here. Thanks!\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 30900, "title": "ERROR: saved_model_cli", "body": "working on ubuntu 18.4\r\n\r\ntensorflow version 2.0.0b1\r\n\r\nI am trying to find `output_node_names` with the help of command `saved_model_cli show --dir DIR_PATH --all`\r\n\r\ngot this ERROR `OSError: SavedModel file does not exist at: DIR_PATH`\r\n\r\nthis is my SAVED MODEL\r\n\r\n![Screenshot from 2019-07-20 21-06-51](https://user-images.githubusercontent.com/32290207/61580704-5b4d0700-ab32-11e9-8fac-7fc38ab4b00b.png)\r\n\r\n", "comments": ["Please somebody help me to find \u201coutput_node_names\u201d, I have to create \u201cfreeze_graph.pb\u201d, @ymodak @anush-o", "I don't  see ```saved_model``` file in your directory. Your model tree should contain ```saved_model``` file.  I made a toy example to illustrate,\r\n```ssd_mobilenet_v1_coco_2017_11_17\r\n\u251c\u2500\u2500 checkpoint\r\n\u251c\u2500\u2500 frozen_inference_graph.pb\r\n\u251c\u2500\u2500 model.ckpt.data-00000-of-00001\r\n\u251c\u2500\u2500 model.ckpt.index\r\n\u251c\u2500\u2500 model.ckpt.meta\r\n\u2514\u2500\u2500 saved_model\r\n    \u251c\u2500\u2500 saved_model.pb\r\n    \u2514\u2500\u2500 variables\r\n```\r\nsaved_model_cli works in above case.", "can you please tell me how to create `saved_model` folder, if not then can you please tell me a code or something to get `output_node_names` (graph is too big so manually do it is not possible ) @ymodak ", "See https://www.tensorflow.org/guide/saved_model#build_and_load_a_savedmodel to build saved model\r\nand then https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel", "@ymodak problem is the model is pretrained one, i didn't make it. I just want to know the `output_node_names` to convert it into `freeze_graph.pb`", "Perhaps referring to this similar post on stackoverflow can help.\r\nhttps://stackoverflow.com/questions/38958662/tensorflow-what-are-the-output-node-names-for-freeze-graph-py-in-the-model-wi/38959056\r\nand\r\nhttps://www.youtube.com/watch?v=QW6532LtiTc\r\nThanks!"]}, {"number": 30899, "title": "failed to \u201csudo bazel build -c opt --config=cuda --verbose_failures //tensorflow:libtensorflow.so\u201d", "body": "I want to compile tensorflow\u2018 c library version\r\nsystem info\uff1a\r\n  ubuntu 16.04\r\n  bazel 0.19.2\r\n  tensorflow 1.13\r\n  gpu RTX 2080 Ti\r\n  cuda10.0\r\n  cudnn 7.4.2\r\n\r\nsteps\uff1a\r\n1.Install bazel by sh file\r\n2.git clone --recursive https://github.com/tensorflow/tensorflow\r\n3.git checkout r1.3\r\n4.sudo ./configure\r\n5.sudo bazel build -c opt --config=cuda --verbose_failures //tensorflow:libtensorflow.so\r\n\r\nerror:\r\n Starting local Bazel server and connecting to it...\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nDEBUG: /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nERROR: /home/liupeng/tensorflow/tensorflow/core/kernels/BUILD:4841:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/icu/release-62-1.tar.gz: Checksum was dd16ace2fbfa98271e266fb6ab4fce056a8baf8274059687fd1b9a1159a44de9 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/core/kernels:unicode_ops'\r\nERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/icu/release-62-1.tar.gz: Checksum was dd16ace2fbfa98271e266fb6ab4fce056a8baf8274059687fd1b9a1159a44de9 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761\r\nINFO: Elapsed time: 780.835s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (119 packages loaded, 8680 targets configured)\r\n", "comments": ["Just to verify did you get chance to follow instructions from [TensorFlow](https://www.tensorflow.org/install/source) website .Please, let us know. Thanks!", "> I want to compile tensorflow\u2018 c library version\r\n> system info\uff1a\r\n> ubuntu 16.04\r\n> bazel 0.19.2\r\n> tensorflow 1.13\r\n> gpu RTX 2080 Ti\r\n> cuda10.0\r\n> cudnn 7.4.2\r\n> \r\n> steps\uff1a\r\n> 1.Install bazel by sh file\r\n> 2.git clone --recursive https://github.com/tensorflow/tensorflow\r\n> 3.git checkout r1.3\r\n> 4.sudo ./configure\r\n> 5.sudo bazel build -c opt --config=cuda --verbose_failures //tensorflow:libtensorflow.so\r\n> \r\n> error:\r\n> Starting local Bazel server and connecting to it...\r\n> WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\n> DEBUG: /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:\r\n> Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n> ERROR: /home/liupeng/tensorflow/tensorflow/core/kernels/BUILD:4841:1: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/icu/release-62-1.tar.gz: Checksum was dd16ace2fbfa98271e266fb6ab4fce056a8baf8274059687fd1b9a1159a44de9 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761 and referenced by '//tensorflow/core/kernels:unicode_ops'\r\n> ERROR: Analysis of target '//tensorflow:libtensorflow.so' failed; build aborted: no such package '@icu//': java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/unicode-org/icu/archive/release-62-1.tar.gz, https://github.com/unicode-org/icu/archive/release-62-1.tar.gz] to /home/liupeng/.cache/bazel/_bazel_root/f585ecc291b1d46eb648d7609ae1ce83/external/icu/release-62-1.tar.gz: Checksum was dd16ace2fbfa98271e266fb6ab4fce056a8baf8274059687fd1b9a1159a44de9 but wanted e15ffd84606323cbad5515bf9ecdf8061cc3bf80fb883b9e6aa162e485aa9761\r\n> INFO: Elapsed time: 780.835s\r\n> INFO: 0 processes.\r\n> FAILED: Build did NOT complete successfully (119 packages loaded, 8680 targets configured)\r\n\r\nwhy i see people using -c opt instead of --config=opt? @ravikyram ", "The error message says it cannot download one of the TF dependencies. You need to check your internet connection.\r\nEspecially users in china have reported similar issues. Is it possible you are in China?", "@liupengkd \r\nCan you please confirm if the issue still persists.Thanks!", "@ravikyram I am very sorry, I have been busy recently, there is no reply. I switched to 1.9.0 and compiled successfully. Did not continue to try 1.13", "@gunan Yes, I am in China. I also think so. But when I changed to 1.9.0, I compiled successfully.", "@liupengkd \r\nThanks for the update. I am closing this issue since the query is been resolved.Thanks!"]}, {"number": 30898, "title": "Makefile build does not create contrib ops", "body": "**System information**\r\n- OS Platform and Distribution: MacOS\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: c6352706d6280d7ed34964969503b104471a9335\r\n- Bazel version (if compiling from source): not used\r\n- GCC/Compiler version (if compiling from source):\r\n```\r\nConfigured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.14.sdk/usr/include/c++/4.2.1\r\nApple LLVM version 10.0.1 (clang-1001.0.46.4)\r\nTarget: x86_64-apple-darwin18.6.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n```\r\n\r\n**Describe the problem**\r\n\r\nWhen compiling with `tensorflow/contrib/makefile/build_all_linux.sh`, the following inclusion does not work:\r\n```\r\n#include <tensorflow/cc/ops/standard_ops.h>\r\n```\r\nbecause the header `#includes` something that's not there:\r\n```\r\n<tfdir>/tensorflow/tensorflow/cc/ops/standard_ops.h:19:10: fatal error: 'tensorflow/cc/ops/array_ops.h' file not found\r\n#include \"tensorflow/cc/ops/array_ops.h\"\r\n         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n1 error generated.\r\n```\r\n\r\nSearching for `array_ops` in the repository e.g. reveals a CMake macro named `GENERATE_CONTRIB_OP_LIBRARY` defined in in `tensorflow/contrib/cmake/tf_core_ops.cmake`.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/6c7c3bb0f3563e8a147935597617cf3d052c7852/tensorflow/contrib/cmake/tf_core_ops.cmake#L76\r\n\r\nObviously, the Makefile does not use this. Is it intentional that these ops are not being built with the makefile? Even if I remove the header, a program does not work at runtime due to missing ops.\r\n\r\nI can provide a minimal C++ file and build scripts if desired, but would like to know first if it's supposed to be like this.\r\n", "comments": ["The same problem occurs with the CMake build (which I have fixed many problems in, but running into this as well). ", "May I know the reason why you don't use bazel?", "Because it's generally a pain to install the correct version, and none of my knowledge transfers to it. I find it lamentable that such an important project does not have a build process which works easily on all systems. ", "Hi, this issue seems not to be Intel specific, but about compiling of TF. I would suggest other experts to share ideas.", "This came now to my eyes.\r\n\r\nTwo things: makefile build is no longer supported (since at least months ago). Also, recently we removed contrib as it won't be available in 2.0.\r\n\r\nI think it would be best to try using Bazel. `./configure.py` has the versioning information at the top:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/250a5d47828aa229857266d59b32314bda79bcb3/configure.py#L52-L53\r\n\r\nOnce you start using it, makefile knowledge will transfer in time. I'm saying this from experience, I was using makefile's everywhere but then I switch to Bazel for TF and I like this model slightly better for extremely large cross-platform codebases.", "@themightyoarfish Is this still an issue? Please close the issue if this was resolved already. Thanks!", "I have no idea to be honest, i have not touched Tensorflow since raising this issue. I suppose it can be closed for now. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30898\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30898\">No</a>\n"]}, {"number": 30897, "title": "Inconsistency between using keras and tf.keras", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: Tensorflow 2 \r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\nThere is inconsistency when using keras or tf.keras to fit the model to the same dataset. As everyone can see, everything is similar in both code except calling layers and other objects from tf.keras or pure Keras. The codes are attached. \r\n\r\nI'm absolutely confused! I tried to double-check the code line by line, and everything is the same in both codes, except calling modules from either pure keras or tf.keras.\r\n\r\nIt is absolutely disappointing to me! Why they would be totally different.\r\nIn fact, the codes are the same except that layers call from either keras or tf.keras; why these models should be such completely different behaviors.\r\n\r\n`   \r\n\r\n# The modules/layers can be called from either:\r\n\r\n    from tensorflow.python.keras.layers import (Input, Dense, BatchNormalization, GaussianNoise, \r\n    GaussianDropout)\r\n    from tensorflow.python.keras.models import Model\r\n    from tensorflow.python.keras.utils import np_utils\r\n    from tensorflow.python.keras.callbacks import CSVLogger, History\r\n\r\nOr \r\n\r\n    from keras.layers import Input, Dense, BatchNormalization, GaussianNoise, GaussianDropout\r\n    from keras.models import Model\r\n    from keras.utils import np_utils\r\n    from keras.callbacks import CSVLogger, History\r\n\r\n# Creating Models: Everything in the models' description is the same.\r\n    \r\n   \r\n    inputs = Input(shape=(19671,), name=\"inputs\")\r\n\r\n    inputs_0 = BatchNormalization(name=\"inputs_0\")(inputs)\r\n\r\n    inputs_1 = Dense(1024, activation=\"linear\", name=\"inputs_1\")(inputs_0)\r\n\r\n    inputs_2 = BatchNormalization(name=\"inputs_2\")(inputs_1)\r\n\r\n    inputs_3 = Dense(128, activation=\"softplus\", name=\"inputs_3\")(inputs_2)\r\n\r\n    inputs_4 = BatchNormalization(name=\"inputs_4\")(inputs_3)\r\n\r\n    encoded = Dense(units=12, activation='relu', name='encoded')(inputs_4)\r\n\r\n    inputs_5 = Dense(512, activation=\"softplus\", name=\"inputs_5\")(encoded)\r\n\r\n    decoded_tcga = Dense(units=19671, activation='linear', name=\"m_rna\")(inputs_5)\r\n\r\n    decoded_micro_rna = Dense(units=2588, activation='linear', name=\"mi_rna\")(inputs_5)\r\n\r\n    cl_0 = Dense(units=27, activation=\"softmax\", name=\"cl_tissue\")(encoded)\r\n\r\n    cl_2 = Dense(units=33, activation=\"softmax\", name=\"cl_disease\")(encoded)\r\n\r\n    scae = Model(inputs=inputs, outputs=[decoded_tcga, decoded_micro_rna, cl_0, cl_2])\r\n\r\n    scae.compile(optimizer='nadam',\r\n                 loss=[\"mse\", \"mse\", \"cosine_proximity\", \"cosine_proximity\"],\r\n                 loss_weights=[0.001, 0.001, 0.5, 0.5],\r\n                 metrics={\"m_rna\": [\"mse\", \"mae\"],\r\n                          \"mi_rna\": [\"mse\", \"mae\"], \"cl_tissue\": \"acc\", \"cl_disease\": \"acc\"})\r\n\r\n    scae\r\n`\r\n\r\n` \r\n\r\n# Loading Datasets (The datasets are too large, about 5.5 GB).\r\n    local_dataset_folder = \r\n    \"/home/ermia/MEGA/PycharmProjects_VSC/TCGA_Full_Version/TCGA/Dataset/\"\r\n    local_results_folder = \r\n    \"/home/ermia/MEGA/PycharmProjects_VSC/TCGA_Full_Version/TCGA/Results/\"\r\n    \r\n    from sklearn.preprocessing import LabelEncoder, normalize\r\n    import pandas as pd\r\n\r\n    seed = 2019\r\n    np.random.seed(seed=seed)\r\n    dataset_folder = local_dataset_folder\r\n    df_m_rna_address = dataset_folder + \"fpkm.csv\"\r\n    df_mi_rna_address = dataset_folder + \"miRNA.csv\"\r\n    df_tissue_address = dataset_folder + \"tissue.csv\"\r\n    df_disease_address = dataset_folder + \"disease.csv\"\r\n\r\n    df_m_rna = np.loadtxt(df_m_rna_address, delimiter=\",\")\r\n    df_mi_rna = np.loadtxt(df_mi_rna_address, delimiter=\",\")\r\n    df_tissue = np.ravel(pd.DataFrame.as_matrix(pd.read_csv(df_tissue_address, delimiter=\",\", \r\n    header=None)))\r\n    df_disease = np.ravel(pd.DataFrame.as_matrix(pd.read_csv(df_disease_address, delimiter=\",\", \r\n    header=None)))\r\n\r\n    df_m_rna = normalize(X=df_m_rna, axis=0, norm=\"max\")\r\n    df_mi_rna = normalize(X=df_mi_rna, axis=0, norm=\"max\")\r\n\r\n    label_encoder_tissue = LabelEncoder()\r\n    label_encoder_tissue.fit(df_tissue)\r\n    encoded_tissue = label_encoder_tissue.transform(df_tissue)\r\n\r\n    label_encoder_disease = LabelEncoder()\r\n    label_encoder_disease.fit(df_disease)\r\n    encoded_disease = label_encoder_disease.transform(df_disease)\r\n\r\n    categorical_tissue = np_utils.to_categorical(encoded_tissue)\r\n    categorical_disease = np_utils.to_categorical(encoded_disease)\r\n    m_rna = df_m_rna\r\n    mi_rna = df_mi_rna\r\n\r\n    indices = np.arange(m_rna.shape[0])\r\n    indices = indices[0:10750]\r\n    np.random.shuffle(indices)\r\n\r\n    m_rna = m_rna[indices]\r\n    mi_rna = mi_rna[indices]\r\n\r\n    categorical_tissue = categorical_tissue[indices]\r\n    categorical_disease = categorical_disease[indices]\r\n\r\n    m_rna_train = m_rna[0:9750, ]\r\n    m_rna_test = m_rna[9750:10750, ]\r\n\r\n    mi_rna_train = mi_rna[0:9750, ]\r\n    mi_rna_test = mi_rna[9750:10750, ]\r\n\r\n    categorical_tissue_train = categorical_tissue[0:9750, ]\r\n    categorical_tissue_test = categorical_tissue[9750:10750, ]\r\n\r\n    categorical_disease_train = categorical_disease[0:9750, ]\r\n    categorical_disease_test = categorical_disease[9750: 10750, ]\r\n\r\n    print(\"data loading has just been finished\")\r\n    print(m_rna.shape, mi_rna.shape, categorical_tissue.shape, categorical_disease.shape)\r\n\r\n     batch_size = 64\r\n     nb_epochs = 200\r\n\r\n`\r\n\r\n`\r\n\r\n# Fitting Models.\r\n    scae.fit(m_rna_train, [m_rna_train, mi_rna_train, categorical_tissue_train, categorical_disease_train], \r\n    batch_size=batch_size, epochs=nb_epochs,\r\n          callbacks=[csv_logger, history],\r\n          validation_data=(m_rna_test, [m_rna_test, mi_rna_test, categorical_tissue_test, \r\n    categorical_disease_test]), verbose=2)\r\n\r\n\r\n`\r\nThe first model is not convergent at all! and the accuracy of subproblems are about 0.\r\n\r\n@fchollet ", "comments": [" #15831 is the same issue!", "Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!", "> Please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?template=10-build-installation-issue.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide as many details as possible to find the root cause of the issue. Thanks!\r\n\r\nHi, Update the template.", "@ErmiaAzarkhalili Can you provide a simple standalone code to reproduce the issue? You could use public datasets like mnist, cifar..... Thanks!", "hey guys, I had a similar issue but with native TF and Keras. Are these connected?[https://github.com/tensorflow/tensorflow/issues/30944](url)", "> @ErmiaAzarkhalili Can you provide a simple standalone code to reproduce the issue? You could use public datasets like mnist, cifar..... Thanks!\r\n\r\nI think its due to using of tf.keras.metrics.Accuracy() instead of tf.keras.metrics.CategoricalAccuracy(). Hence, everything is fine now and m,y model converges like a charm.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30897\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30897\">No</a>\n"]}, {"number": 30896, "title": "Standard way of sharing architectures?", "body": "**System information**\r\n- TensorFlow version (you are using):\r\n1.14, and sometimes the latest v2 pre-release\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFound a number of interesting architectures on GitHub. Starting making PRs to them, to support Python 2.7\u20133.6, and soon for proper setuptools integration.\r\n\r\nThe [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) API, and [tfds in general](https://www.tensorflow.org/datasets/api_docs/python/tfds) are both useful approaches for preparing data, and exposing it for others to consume.\r\n\r\nHowever, AFAIK, there is no standard way of sharing, say: Keras models. Internally have moved to [`gin`](https://github.com/google/gin-config), but not sure if that's the best approach for all.\r\n\r\n**Will this change the current api? How?**\r\nIntroduce a new module, similar to how tfds was introduced for preparing and packaging datasets.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone. Especially those that want to combine multiple architectures from different sources\u2014or in my scenario\u2014to experiment with different architectures.\r\n\r\n**Any Other info.**\r\nConcept:\r\n```python\r\nimport some_broke_arch\r\nimport other_neat_arch\r\nimport horrible_v_arch\r\n\r\nmodel   = some_broke_arch.get_arch(   **standard_arch_params  )\r\nmetrics = other_neat_arch.get_metrics(**standard_metric_params)\r\nloss    = horrible_v_arch.get_loss(   **standard_loss_params  )\r\n\r\nmodel.compile(loss=loss, optimizer=keras.optimizers.RMSprop, metrics=metrics)\r\nprint(model.summary())\r\n# &etc.\r\n````", "comments": ["@SamuelMarks,\r\nSorry for the delayed response. Can you please let us know if the **`Pre-Trained Models`** mentioned in [tf.keras.applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications) is what you are looking for? Thanks!", "@rmothukuru No, what I suggested two years ago is a general interface/concept that is to be implemented by each of architecture, metric, loss, &etc.; and also for the model. I am aware of `tf.keras.applications`; having used it for years (back when there were multiple backends for Keras). `include_top`, `weights`, `pooling`, `classes`, `classifier_activation` are common configuration parameters.\r\n\r\nWhen you read a research paper / blog post / Jupyter Notebook they include the exact combination of parameters\u2014and other building-blocks\u2014required to replicate their results. However, if one wants to build an ensemble of 10 of the best networks for solving <problem>, one would have to manually glue them all together.\r\n\r\nIf one wanted to contribute to every academic open-source repository, include setuptools, dependencies, and expose these as proper Python modules (rather than the common Jupyter format), such that a discoverable set of packages can be exposed (e.g., under a custom classifier on [PyPI](https://pypi.org)\u2026 or on some new zoo.tensorflow.org type domain), and that these could then be easily swapped with your provided transfer learning models (out of`tf.keras.application`), and combined into ensembles, and all-in-all compared against each other to improve ones own experiments.\r\n\r\nEnough intricacies need to be exposed so you can replace the optimizer|loss|hyperparameter|etc. of a known model to see if your new configuration gives a better result on your dataset.\r\n\r\nI don't want to define my own format, as then I'll be fighting others for the best API. I would rather TensorFlow put up a guide and some official interfaces/types that must be implemented by each model in the zoo, such that I can go on my way with these large number of contributions to each open-source\u2014and potentially useful\u2014TensorFlow model.\r\n\r\nWhat do you think?", "@SamuelMarks,\r\nThank you for the detailed  explanation. Leaving it open. ", "@SamuelMarks I suggest to start a thread in [our forum](https://discuss.tensorflow.org/)", "@SamuelMarks following up from todays ML Community Day, did you want to author an RFC instead of filing an issue: https://github.com/tensorflow/community/blob/master/governance/TF-RFCs.md", "I second @bhack on creating a forum thread about this proposal to find an RFC sponsor.  ", "Good idea. I spoke with @fchollet a couple of days ago about this\u2026 Fran\u00e7ois Chollet: do you want to be my RFC sponsor?", "@SamuelMarks Are you still interested in contributing? If yes, please feel free to open a PR in  [keras-team/keras repo.](https://github.com/keras-team/keras/issues) repository.\r\n\r\nPlease note that Keras development moved to keras-team/keras repository to focus on only keras. Thanks!\r\n\r\nPlease let me know if you are busy, i can create this PR in keras-team/keras (with your content). Thanks", "@jvishnuvardhan Yes: I am still interested in contributing. Okay, I'll cross-post there [and to the [RFC repo](https://github.com/tensorflow/community/tree/master/rfcs)?].\r\n\r\nEDIT: Posted https://github.com/keras-team/keras/issues/15762", "I suggest you to post this idea to the forum before opening a new RFC PR as you need to try to collect interested sponsors for the PR", "With the forum I  mean https://discuss.tensorflow.org", "@bhack Yep I had posted on that keras-team org before I saw your reply to post on the forum. The forum is moderated so the topic only just got through the gates:\r\nhttps://discuss.tensorflow.org/t/standard-way-of-sharing-architectures-e-g-to-pypi-org/6390\r\n\r\nUpvote if you please", "@SamuelMarks If you have already posted this issue in [TensorFlow Forum](https://discuss.tensorflow.org/t/standard-way-of-sharing-architectures-e-g-to-pypi-org/6390/3) please move this to close status ? Thanks", "@SamuelMarks We will track the progress in keras-team/keras repo. I am closing this issue here.Thanks"]}, {"number": 30895, "title": "[TF 2.0] autograph issue", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  \r\nYes \r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n Windows 10 Pro 64-bit (10.0, Build 18362) & Ubuntu 18.04.2 LTS \r\n\r\n- TensorFlow installed from (source or binary):  PyPI \r\n\r\n- TensorFlow version (use command below):  2.0.0-dev20190718 \r\n\r\n- Python version: 3.6.7 \r\n\r\n- CUDA/cuDNN version: cudatoolkit-10.0.130-0/cudnn-7.6.0-cuda10.0_0 \r\n\r\n- GPU model and memory: \r\n(/device:GPU:0 with 1387 MB memory) -> physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\n\r\n**Describe the current behavior**\r\ntf.function raised a warning log (only on Windows Python Shell, even IPython works fine)\r\nand tf.autograph.to_code did not work... (Both Windows and Colab)\r\n```\r\n>>> @tf.function\r\n... def calc(x):\r\n...     return 0 if x<=0.5 else 1\r\n...\r\n>>> calc(12)\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0720 14:49:32.090106 25088 ag_logging.py:146] Entity <function calc at 0x00000280F0429C80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calc at 0x00000280F0429C80>: ValueError: Unable to locate the source code of <function calc at 0x00000280F0429C80>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\nWARNING: Entity <function calc at 0x00000280F0429C80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function calc at 0x00000280F0429C80>: ValueError: Unable to locate the source code of <function calc at 0x00000280F0429C80>. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code\r\n<tf.Tensor: id=16, shape=(), dtype=int32, numpy=1>\r\n```\r\nand\r\n```\r\n>>> print(tf.autograph.to_code(calc))\r\nTraceback (most recent call last):\r\n  File \"C:\\tools\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 614, in to_graph\r\n    return conversion.convert(entity, program_ctx)\r\n  File \"C:\\tools\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\conversion.py\", line 310, in convert\r\n    free_nonglobal_var_names = entity.__code__.co_freevars\r\nAttributeError: 'Function' object has no attribute '__code__'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\tools\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 754, in to_code\r\n    experimental_optional_features=experimental_optional_features))\r\n  File \"C:\\tools\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\", line 618, in to_graph\r\n    entity, e.__class__.__name__, str(e)))\r\ntensorflow.python.autograph.impl.api.ConversionError: converting <tensorflow.python.eager.def_function.Function object at 0x00000280F0426DD8>: AttributeError: 'Function' object has no attribute '__code__'\r\n```\r\n**Describe the expected behavior**\r\nautograph works\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n@tf.function\r\ndef calc(x):\r\n    return 0 if x <= 0.5 else 1\r\n\r\nprint(calc(2.5))\r\nprint(tf.autograph.to_code(calc))\r\n```\r\n\r\nPlease take a look, Thank you.\r\n", "comments": ["Was able to reproduce the issue with the Tensorflow vesrion 2.0.0-dev20190718 on Colab. Find the gist [here](https://colab.research.google.com/drive/1VcnK8pmrTIe9lWXi5eVqugNRJ2L99r0a). ", "Unfortunately, in the Python REPL, functions don't expose source code, which is required for autograph. Note that other shells, like Jupyter don't expose this limitation.\r\n\r\nAdmittedly, the explanation of the error is tucked inside the warning message, and it would be nice to make it more obvious: \r\n\r\n```\r\nNote that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file.\r\n```", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30895\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30895\">No</a>\n", "Hi @mdanatg ,\r\n\r\nThanks for your help, the first issue is easy as the log gives the hint (and it is the log that asked me to report\r\nBut what confused me is the second issue: `ConversionError: converting <tensorflow.python.eager.def_function.Function object at 0x7facb0169d30>: AttributeError: 'Function' object has no attribute '__code__'` raised by calling `tf.autograph.to_code(calc)`. I think this one has little connection between Python Shell as it can be reproduced with Python Shell, IPython in my PC or Colab based on tf-nightly-gpu-2.0-preview-2.0.0.dev20190723 and 2.0.0-dev20190718.\r\n\r\nThis API is ok during ML GDE Testing in Jan, and I find that https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/impl/api.py#L499-L504 gives a check of `target_entity` if it has attribute name `__code__` but it does not work, maybe a bug?\r\n\r\nAttached is the full log:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py in to_graph(entity, recursive, experimental_optional_features)\r\n    619         autograph_module=tf_inspect.getmodule(to_graph))\r\n--> 620     return conversion.convert(entity, program_ctx)\r\n    621   except (ValueError, AttributeError, KeyError, NameError, AssertionError) as e:\r\n\r\n3 frames\r\nAttributeError: 'Function' object has no attribute '__code__'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConversionError                           Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py in to_graph(entity, recursive, experimental_optional_features)\r\n    622     logging.error(1, 'Error converting %s', entity, exc_info=True)\r\n    623     raise ConversionError('converting {}: {}: {}'.format(\r\n--> 624         entity, e.__class__.__name__, str(e)))\r\n    625 \r\n    626 \r\n\r\nConversionError: converting <tensorflow.python.eager.def_function.Function object at 0x7facb0169d30>: AttributeError: 'Function' object has no attribute '__code__'\r\n```\r\nThanks in advance.", "@kkimdev FYI\r\n\r\nAh, I completely missed the second part, sorry about that. I think this would be a useful bug to fix. In essence, `to_code` requires the original function, but `@tf.function` returns a wrapper object over the original function. We should make `to_code` aware of this wrapper so that users don't see this surprising result.\r\n\r\nIn the mean time, the following workaround should work (subject to the above limitation):\r\n\r\n```\r\nprint(tf.autograph.to_code(calc.python_function))  # python_function gives us the actual function\r\n```\r\n\r\n", "@mdanatg thanks for your responding\r\nI'm not sure but both autograph decorator and `to_code` can work fine together in r1.14 as I just tested, is the autograph reconstructed in r2.0?\r\n\r\nhere comes my code and its output:\r\n```python\r\nimport tensorflow as tf\r\n@tf.contrib.autograph.convert()\r\ndef calc(x):\r\n    return 1 if x>0.5 else 0\r\n\r\nprint(tf.contrib.autograph.to_code(calc))\r\n```\r\n```\r\ndef tf__calc(*args, **kwargs):\r\n  \"\"\"Wrapper that calls the converted version of f.\"\"\"\r\n  do_return = False\r\n  retval_ = ag__.UndefinedReturnValue()\r\n  try:\r\n    do_return = True\r\n    retval_ = ag__.converted_call(converted_call, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (f, None, ag__.converted_call('ConversionOptions', converter, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (), {'recursive': recursive, 'force_conversion': True, 'optional_features': optional_features}), args, kwargs), None)\r\n  except Exception as e:\r\n    cond = ag__.converted_call(hasattr, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (e, 'ag_error_metadata'), None)\r\n\r\n    def get_state():\r\n      return ()\r\n\r\n    def set_state(_):\r\n      pass\r\n\r\n    def if_true():\r\n      raise ag__.converted_call('to_exception', e.ag_error_metadata, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (ag__.converted_call(type, None, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), (e,), None),), None)\r\n      return ag__.match_staging_level(1, cond)\r\n\r\n    def if_false():\r\n      raise\r\n      return ag__.match_staging_level(1, cond)\r\n    ag__.if_stmt(cond, if_true, if_false, get_state, set_state)\r\n  cond_1 = ag__.is_undefined_return(retval_)\r\n\r\n  def get_state_1():\r\n    return ()\r\n\r\n  def set_state_1(_):\r\n    pass\r\n\r\n  def if_true_1():\r\n    retval_ = None\r\n    return retval_\r\n\r\n  def if_false_1():\r\n    return retval_\r\n  retval_ = ag__.if_stmt(cond_1, if_true_1, if_false_1, get_state_1, set_state_1)\r\n  return retval_\r\n```\r\n\r\nThank you", "@kuri-leo indeed, they don't work together, because `@convert` replaces the function with a wrapper, and  `to_code` converts that instead. It's a shortcoming in the API that we never fixed because we're deprecated the `@convert` decorator: instead we recommend using `@tf.function` instead of `@tf.contrib.autograph.convert`, both in 2.0 and 1.14. Sorry for not adding a deprecation warning.\r\n\r\nI should add that the official autograph API has moved out of `contrib`, which is removed in 2.0. Instead, please use the API in `tf.autograph`. See the [2.0 API docs](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph) and [1.14 API docs](https://www.tensorflow.org/api_docs/python/tf/autograph).", "@mdanatg @kkimdev I'm interested in working on this. Please correct me if I'm wrong. Is the solution is to check whether the `function.autograph == true` on [to_code source code here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/impl/api.py)? If it's true, then we get the source from `entity.python_function` instead of only `entity`?\r\n\r\n\r\n", "@ilhamfp It would be something similar, yes, but we need a more robust way to test that the object is a TF function. Performing an `isinstance` check for [Function](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/def_function.py#L241) objects would not be ideal because that would create a circular dependency (tf.function already depends on autograph).\r\n\r\nSo a better method would be to add a utility in [tensor_util.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/tensor_util.py), similar to `is_tensor` -- say `is_tf_function`, which can perform that check instead.\r\n@alextp What would be the best method to check for a Function object, short of verifying the type?", "Sorry, a lot of messages to catch up on.\r\n\r\nThe issue seems to be that you want to_code to be transparent on code already wrapped with tf.function. I think this is too much special-casing. Maybe we can add a tf.function().to_code method for people who are curious, but I'd rather not break the layering of the stack.", "I think that's a good point. Perhaps a better alternative would be to add a hint to the error message. Right now we attempt to get the function's __code__ and fail. Instead, it might make sense to do something like this:\r\n\r\n```\r\nif not hasattr(type(entity), '__code__'):\r\n  raise ValueError('Cannot apply autograph to a function that doesn't expose a __code__ object. If this is a @tf.function, please use to_code(f.python_function) instead.')\r\n```\r\n\r\nThoughts?", "SGTM\n\nOn Thu, Aug 1, 2019 at 11:50 AM Dan Moldovan <notifications@github.com>\nwrote:\n\n> I think that's a good point. Perhaps a better alternative would be to add\n> a hint to the error message. Right now we attempt to get the function's\n> *code* and fail. Instead, it might make sense to do something like this:\n>\n> if not hasattr(type(entity), '__code__'):\n>   raise ValueError('Cannot apply autograph to a function that doesn't expose a __code__ object. If this is a @tf.function, please use to_code(f.python_function) instead.')\n>\n> Thoughts?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30895?email_source=notifications&email_token=AAABHRNIH5XVPBWXOT6ZZ33QCMWBHA5CNFSM4IFN2ND2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3LQ2RQ#issuecomment-517410118>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHROG4QRYSUQW3LOEXLDQCMWBHANCNFSM4IFN2NDQ>\n> .\n>\n\n\n-- \n - Alex\n", "Ummm, Sorry for my late reply as I'm trapped in some other works...\r\n\r\nActually, I do this just because I was preparing slide for my GDG and TFUG events and wanted to show how the autograph works. So this is not an in-production issue but just a presentation issue.\r\n\r\nThank you all for your kind help :-)", "Hi,\r\nI am a beginner and would like to contribite to this issue. This would be my first contribution. Any guidence would be very helpful.\r\nThanks in advance", "@Soniyanayak51 tf.function is defined in tensorflow/python/eager/def_function.py. That's where the Function object is defined where the to_code attribute could be added.", "Thanks for the reply. Working on it\r\n", "Wait, I thought my #31268 PR has already fixed this issue?  \r\nPlease correct me if I'm wrong. I'm worried that we're fixing the same issue twice.", "Oh, if so we should close this issue.\n\nOn Thu, Aug 8, 2019 at 12:47 PM Ilham F Putra <notifications@github.com>\nwrote:\n\n> Wait, I thought my #31268\n> <https://github.com/tensorflow/tensorflow/pull/31268> PR has already\n> fixed this issue?\n> Please correct me if I'm wrong. I'm worried that we're fixing the same\n> issue twice.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30895?email_source=notifications&email_token=AAABHRPUUABUWEYT2Z6TVS3QDRZ4VA5CNFSM4IFN2ND2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD34V7JA#issuecomment-519659428>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRLA4AEDW7BMW2KRJL3QDRZ4VANCNFSM4IFN2NDQ>\n> .\n>\n\n\n-- \n - Alex\n", "@ilhamfp I like your fix.", "Thank you! All credits to @mdanatg ", "so this issue is closed or opened till\r\nas it's showing open", "@KeshavJB5 Currently PR was approved. This will be closed when the PR merges. Thanks!", " #31268 has been merged. I think we can close this?", "Yep, looks like this can be closed. Thanks everyone!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30895\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30895\">No</a>\n"]}, {"number": 30894, "title": "tf.TensorArray with Defun", "body": "when using Defun under tf 1.x, the tf.TensorArray input will degrade to a Tensor.\r\n\r\n```\r\nxxxx output_tags : <dtype: 'float32'>\r\nTensor(\"output_tas:0\", dtype=float32, device=/device:CPU:0)\r\n[  FAILED  ] SpeechOpsFeatTest.test_splice\r\n======================================================================\r\nERROR: test_splice (__main__.SpeechOpsFeatTest)\r\ntest_splice (__main__.SpeechOpsFeatTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"speech_ops_test.py\", line 202, in test_splice\r\n    out = tffeat.splice(feat, left_context=2, right_context=2)\r\n  File \"/delta/delta/data/feat/speech_ops.py\", line 376, in splice\r\n    swap_memory=False)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3501, in while_loop\r\n    return_same_structure)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 3012, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 2937, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\", line 535, in __call__\r\n    self.add_to_graph(ops.get_default_graph())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\", line 516, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\", line 348, in _create_definition_if_needed                                                                                                                             \r\n    self._create_definition_if_needed_impl()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\", line 379, in _create_definition_if_needed_impl                                                                                                                        \r\n    capture_resource_var_by_value=self._capture_resource_var_by_value)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\", line 915, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"/delta/delta/data/feat/speech_ops.py\", line 357, in _loop_body\r\n    new_output_tas = output_tas.write(time,  new_feat)\r\nAttributeError: 'Tensor' object has no attribute 'write'\r\n```\r\n\r\nwithout Defun decorator,  everything is ok\r\n\r\n```\r\nxxxx output_tags : <dtype: 'float32'>\r\n<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7f2528260908>\r\nI0720 03:20:01.964524 139798580987712 speech_ops_test.py:203] splice : Tensor(\"TensorArrayStack_1/TensorArrayGatherV3:0\", shape=(?, ?, 1, ?), dtype=float32)\r\n[       OK ] SpeechOpsFeatTest.test_splice\r\n```\r\n\r\n\r\n\r\n```\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/fields.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/parsing.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/period.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/frequencies.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/timestamps.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/resolution.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/missing.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/interval.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/properties.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/ops.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/sparse.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/indexing.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/internals.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /usr/lib/python3.6/lib-dynload/mmap.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/matplotlib/ft2font.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/kiwisolver.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/matplotlib/_path.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/window.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/skiplist.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/reduction.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/parsers.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/json.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/writers.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/util/_move.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/io/msgpack/_packer.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/io/msgpack/_unpacker.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling init: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/testing.cpython-36m-x86_64-linux-gnu.so\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/bin/python3 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /lib/x86_64-linux-gnu/libutil.so.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /lib/x86_64-linux-gnu/libexpat.so.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /lib/x86_64-linux-gnu/libz.so.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_opcode.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/x86_64-linux-gnu/libffi.so.6 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_umath.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/_multiarray_tests.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/linalg/lapack_lite.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/linalg/_umath_linalg.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_bz2.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /lib/x86_64-linux-gnu/libbz2.so.1.0 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_lzma.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /lib/x86_64-linux-gnu/liblzma.so.5 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_decimal.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/x86_64-linux-gnu/libmpdec.so.2 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/fft/fftpack_lite.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/random/mtrand.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_hashlib.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/../libtensorflow_framework.so.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:\r\n      8583:\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_csv.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/termios.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/fast_tensor_util.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /lib/x86_64-linux-gnu/libuuid.so.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/wrapt/_wrappers.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_json.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/_ssl.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/x86_64-linux-gnu/libssl.so.1.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_errors.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_conv.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5r.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_objects.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/defs.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5t.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/utils.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5z.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5a.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5s.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5p.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5ac.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/_proxy.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5d.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5ds.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5f.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5g.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5i.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5fd.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5o.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/h5l.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5_hl-db841637.so.100.1.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/libhdf5-5773eb11.so.103.0.0 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libz-a147dcb0.so.1.2.3 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libsz-1c7dd0cf.so.2.0.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/h5py/.libs/./libaec-2147abcd.so.0.0.4 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /lib/x86_64-linux-gnu/librt.so.1 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /lib/x86_64-linux-gnu/libdl.so.2 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/_lib/_ccallback_c.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/_sparsetools.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/_csparsetools.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_shortest_path.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_tools.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_traversal.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_min_spanning_tree.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/sparse/csgraph/_reordering.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/ndimage/_nd_image.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/ndimage/_ni_label.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_fblas.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_flapack.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_flinalg.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_solve_toeplitz.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/_decomp_update.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/cython_blas.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/scipy/linalg/cython_lapack.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../.libs/libopenblasp-r0-2ecf47d5.3.7.dev.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/local/lib/python3.6/dist-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0 [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/conversion.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/c_timestamp.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/nattype.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/np_datetime.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/timezones.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/tzconversion.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/timedeltas.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/offsets.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/ccalendar.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/strptime.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/fields.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/parsing.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/period.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/frequencies.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/timestamps.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslibs/resolution.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/hashtable.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/missing.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/lib.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/tslib.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/algos.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/interval.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/properties.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/hashing.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/ops.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/index.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/join.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/sparse.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/indexing.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/internals.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /usr/lib/python3.6/lib-dynload/mmap.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/reshape.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/matplotlib/ft2font.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/kiwisolver.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/matplotlib/_path.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/window.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/skiplist.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/groupby.cpython-36m-x86_64-linux-gnu.so [0]\r\n      8583:\r\n      8583:\r\n      8583:     calling fini: /home/delta/.local/lib/python3.6/site-packages/pandas/_libs/reduction.cpython-36m-x86_64-linux-gnu.so [0]\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n== check python ===================================================\r\npython version: 3.6.8\r\npython branch:\r\npython build version: ('default', 'Jan 14 2019 11:02:34')\r\npython compiler version: GCC 8.0.1 20180414 (experimental) [trunk revision 259383\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Linux\r\nos kernel version: #1 SMP Fri Sep 7 08:20:28 UTC 2018\r\nos release version: 4.9.125-linuxkit\r\nos platform: Linux-4.9.125-linuxkit-x86_64-with-Ubuntu-18.04-bionic\r\nlinux distribution: ('Ubuntu', '18.04', 'bionic')\r\nlinux os distribution: ('Ubuntu', '18.04', 'bionic')\r\nmac version: ('', ('', '', ''), '')\r\nuname: uname_result(system='Linux', node='39917737c69c', release='4.9.125-linuxkit', version='#1 SMP Fri Sep 7 08:20:28 UTC 2018', machine='x86_64', processor='x86_64')                                                                                       \r\narchitecture: ('64bit', 'ELF')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 4.8.5-4ubuntu8) 4.8.5\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== check pips ===================================================\r\nnumpy                         1.16.4   \r\nprotobuf                      3.8.0    \r\ntensorflow                    1.14.0   \r\ntensorflow-estimator          1.14.0   \r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 1.14.0\r\ntf.version.GIT_VERSION = v1.14.0-rc1-22-gaf24dc91b5\r\ntf.version.COMPILER_VERSION = 4.8.5\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 1.14.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: /usr/local/lib/python3.6/dist-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(2, 7, 15, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n ", "comments": ["@zh794390558 ,\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "@oanush \r\nI'm having this issue, too.\r\nThis is a minimal example to reproduce it:\r\n\r\n`import tensorflow as tf\r\nfrom tensorflow.python.framework import function\r\n\r\narr = tf.TensorArray(tf.int32, 1, dynamic_size=True)\r\n\r\n@function.Defun(tf.int32, tf.int32, func_name=\"func\", out_names=[\"ret\"])\r\ndef f(i, arr):\r\n    arr = arr.write(i, 1)\r\n    return arr\r\n\r\nx = f(1, arr)\r\n\r\nsess = tf.Session()\r\nsess.run(x)\r\n`"]}, {"number": 30893, "title": "Add NMSv3 GPU op", "body": "This PR adds GPU version of NMSv3 Op and extends NMS kernel to support legacy networks as well as modern networks, addressing the issue mentioned in original NMS GPU op commit.", "comments": ["@aaroey, can you take a look. This is pretty much the same of NMSv2 except that the NMS op is limited to boxes with scores greater than the threshold.", "@samikama  hi, do you have an expectation of the speed-up from CUDA? I've been trying on some synthetic data of 100,000 boxes, and get a CPU time that is much better than your implementation...?", "@samikama could you address the comment by @fferroni above and provide a simple repro of the performance benefits?", "Can one of the admins verify this patch?", "I'm seeing a reasonable speed improvement of R-CNN training after upgrading from 1.13 to master (which I assume is due to the GPU NMS kernel) ", "@fferroni Could you post your test case? In my tests it was on par with CPU implementation. Nevertheless, I did improve the performance with a new commit. Here are some numbers from my tests\r\n```\r\npython TestNMSv3.py --num_iter=1000 -num_boxes=4096 --max_output=2000\r\nGPUv3     times=  1.196 +-   0.040 msec \r\nGPUv2     times=  1.170 +-   0.468 msec \r\nCPU         times= 77.669 +-  17.420 msec\r\nHCPU      times= 75.239 +-  17.725 msec \r\n```\r\nTest randomly generates 4096 boxes and scores that are fed into NMSv2 and NMSv3 ops on GPU and NMSV3 op on CPU, max_output_size is set to 2000 in all ops, data is placed in variables so no host to device copy needed. CPU is fed with data on GPU, HCPU is fed with data on CPU. All arguments of the ops are identical whenever possible. 1000 tests are run and average and stddev is reported. There can be further improvements that will show up when op is executed on graph by changing op to an async op but considering the improvement, I don't think it is necessary.\r\n", "@aaroey do you have any more concerns?", "This PR breaks an internal test and was partially rolled back in https://github.com/tensorflow/tensorflow/commit/009dcac2213e737f22db69e84615b66f77bd6ddb, and the problem is tracked in #32446."]}, {"number": 30892, "title": "tf.keras.models.load_model fails on Sequential model", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): `tf-nightly-gpu-2.0-preview==2.0.0-dev20190718`\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nAttempt to run `tf.keras.models.load_model` on a `tf.keras.Sequential` with lazily generated inputs produces an error.\r\n\r\n**Describe the expected behavior**\r\nNo error is thrown when running the script below. `model2` is a clone of `model`.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tempfile\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef main():\r\n    batch_size = 3\r\n\r\n    image_shape = (32, 32, 3)\r\n    inputs = tf.random.uniform((batch_size, *image_shape))\r\n\r\n    model = tf.keras.Sequential((\r\n        tf.keras.layers.Conv2D(\r\n            filters=16,\r\n            kernel_size=3,\r\n            strides=2,\r\n            padding='SAME',\r\n            activation='linear'),\r\n    ))\r\n\r\n    _ = model(inputs)\r\n\r\n    with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\r\n        tf.keras.models.save_model(model, fd.name, overwrite=True)\r\n        model2 = tf.keras.models.load_model(fd.name, compile=False)\r\n\r\n    print(model2.summary())\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\nRunning the script above produces the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/kristian/conda/envs/softlearning-2/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/home/kristian/conda/envs/softlearning-2/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/kristian/github/hartikainen/softlearning-2/tests/test_sequential_serialize.py\", line 31, in <module>\r\n    main()\r\n  File \"/home/kristian/github/hartikainen/softlearning-2/tests/test_sequential_serialize.py\", line 25, in main\r\n    model2 = tf.keras.models.load_model(fd.name, compile=False)\r\n  File \"/home/kristian/conda/envs/softlearning-2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\", line 138, in load_model\r\n    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)\r\n  File \"/home/kristian/conda/envs/softlearning-2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 165, in load_model_from_hdf5\r\n    load_weights_from_hdf5_group(f['model_weights'], model.layers)\r\n  File \"/home/kristian/conda/envs/softlearning-2/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 671, in load_weights_from_hdf5_group\r\n    ' layers.')\r\nValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.\r\nException ignored in: <function _TensorCacheDeleter.__del__ at 0x7f9fabdde158>\r\nTraceback (most recent call last):\r\n  File \"/home/kristian/conda/envs/softlearning-2/lib/python3.7/site-packages/tensorflow_core/python/eager/context.py\", line 316, in __del__\r\nTypeError: argument of type 'NoneType' is not iterable\r\n```", "comments": ["Related to https://github.com/tensorflow/tensorflow/issues/28668. cc @ymodak.", "I was able to reproduce the issue on Colab with Tensorflow-gpu 2.0.0-dev20190718 version. ", "from __future__ import absolute_import, division, print_function, unicode_literals\r\nimport tensorflow as tf\r\n\r\n\r\nif __name__==\"__main__\":\r\n  tf.config.set_soft_device_placement(True)\r\n  tf.debugging.set_log_device_placement(True)\r\n\r\n  # Creates some tensors\r\n  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\r\n  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\r\n  c = tf.matmul(a, b)\r\n\r\n  print(c)\r\n\r\nI met the same problem,  with Tensorflow-gpu 2.0.0-dev20190718.\r\nNo such error if I ran it in the python shell", "```\r\n[ 2019-08-19 20:59:05,685 INFO   asr_solver.py:187   3739  ]  eval: load model from: exp/asr-ctc/ckpt/best_model.h5\r\nTraceback (most recent call last):\r\n  File \"/home/luban/zhanghui/nlu-ml/egs/mini_an4/asr/v1/../../../../delta/main.py\", line 111, in <module>\r\n    app.run(main)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/home/luban/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/luban/zhanghui/nlu-ml/egs/mini_an4/asr/v1/../../../../delta/main.py\", line 81, in main\r\n    solver.eval()\r\n  File \"/home/luban/zhanghui/nlu-ml/delta/utils/solver/asr_solver.py\", line 364, in eval\r\n    self.model_fn(mode=utils.EVAL)\r\n  File \"/home/luban/zhanghui/nlu-ml/delta/utils/solver/asr_solver.py\", line 189, in model_fn\r\n    self.model.load_weights(str(model_path))\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\", line 162, in load_weights\r\n    return super(Model, self).load_weights(filepath, by_name)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1426, in load_weights\r\n    saving.load_weights_from_hdf5_group(f, self.layers)\r\n  File \"/home/luban/zhanghui/env/py3.6_tf1.14pip/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 737, in load_weights_from_hdf5_group\r\n    ' layers.')\r\nValueError: You are trying to load a weight file containing 1 layers into a model with 4 layers.\r\n```\r\n\r\nsame problem using tf1.14 , model is this https://github.com/didi/delta/blob/master/delta/models/asr_model.py#L75\r\n", "Same problem with \r\n\r\n```\r\nfrom tensorflow import keras\r\nkeras.__version__\r\nOut[27]: \r\n'2.2.4-tf'\r\nimport tensorflow\r\ntensorflow.__version__\r\nOut[29]: \r\n'2.0.0-beta1'\r\n```\r\n\r\nMWE:\r\n```\r\nfrom tensorflow.python.keras.layers import Conv2D, GlobalAveragePooling2D, Input\r\nfrom tensorflow.python.keras.models import Sequential, load_model\r\n\r\nmodel = Sequential([\r\n    Input((224, 224, 3)),\r\n    Conv2D(10, (3, 3)),\r\n    GlobalAveragePooling2D(),\r\n])\r\nmodel.summary()\r\n\r\nmodel.save('test.hdf5')\r\n\r\nmodel_2 = load_model('test.hdf5')\r\n# ValueError: You are trying to load a weight file containing 1 layers into a model with 0 layers.\r\n\r\n```", "@hartikainen \r\n\r\n> Model cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).\r\n\r\nWhen I add `input_shape` (input_shape=(image_shape) to the first layer, there is no error as you described. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/19028b5d44b1dcb7b8593a1d932654a6/untitled559.ipynb). Thanks!\r\n\r\nI am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30892\">No</a>\n", "Hey @jvishnuvardhan. I don't think your comment resolves the issue. I'm aware that explicitly specifying the `input_shape` makes the example work, but my issue was specific to the case where we feed in an input to the `Sequential` model only after it's initialization, and thus the input shape is inferred later on.\r\n\r\nIf you decide that this is a TensorFlow feature and not a bug, then I'm fine with closing the issue. Still, personally I think it's a bug and tensorflow should support my example's way of model saving/loading since it has all the information needed to do that.\r\n\r\nUnfortunately, I'm not able to reopen the issue, so in case you think this is still a bug, could you do it?\r\n\r\nAlso, @tensorflow-bot's links don't work (note that I haven't answered to the link in this issue yet):\r\n![image](https://user-images.githubusercontent.com/2308543/66641417-a0a14200-ec12-11e9-8528-e95a9d4a865b.png)", "@hartikainen I think it is difficult to infer shape without manually specifying or running .fit or .predict atleast once. May be I am wrong. I will reopen this issue. Thanks!", "@jvishnuvardhan I think it should be possible once the first forward pass is done, i.e. the line `_ = model(inputs)` in my example is important. After that, all the inputs and weights should be fully defined. Thanks for reopening!", "I've the same problem here. My model was created, trained and saved using tf.keras and I'm not able to tf.keras.models.load_model it. I saved the model using HDF5 entire model option as explained in this link: https://www.tensorflow.org/tutorials/keras/save_and_load ; the load_model function fails with a weird error:\r\n\r\nIndexError Traceback (most recent call last)\r\n\r\n(...)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in from_config(cls, config, custom_objects)\r\n1150 assert layer_name in created_layers\r\n1151 layer = created_layers[layer_name]\r\n-> 1152 layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\r\n1153 output_tensors.append(nest.flatten(layer_output_tensors)[tensor_index])\r\n1154\r\n\r\nIndexError: list index out of range", "@andmax It looks like different error. Can you please open a new issue with your details, error trace and standalone code to reproduce the issue? I think it will help community who has similar issue as you. Thanks!", "Thanks @jvishnuvardhan I followed your suggestion and created a specific issue:\r\nhttps://github.com/tensorflow/tensorflow/issues/33357", "@hartikainen Looks like this was resolved in recent `tf-nightly`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/0612e4ebc2548bc5a06727336bf5d541/30892.ipynb). Thanks!\r\n\r\nPlease feel free to close the issue if this was resolved for you. Thanks!", "Neat, this looks good in the colab, so I'll close it. Thanks a lot @jvishnuvardhan!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30892\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30892\">No</a>\n"]}, {"number": 30891, "title": "ffmpeg not found when build in Raspberry Pi 3 B+ Raspbian 10", "body": "**System information**\r\n- OS : Raspbian 10\r\n- Device : Rasberry Pi 3 B+\r\n- TensorFlow installed from source\r\n- TensorFlow version: 1.9\r\n- Python version: 3\r\n- Installed using : I followed the instruction from https://www.tensorflow.org/install/source_rpi \r\n\r\nI followed the instruction from https://www.tensorflow.org/install/source_rpi   The commands are copy from the link as follows:\r\n\r\n`git clone https://github.com/tensorflow/tensorflow.git`\r\n\r\n`cd tensorflow`\r\n\r\n`git checkout r1.9`\r\n\r\n`CI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4\" `\r\n`    tensorflow/tools/ci_build/ci_build.sh PI-PYTHON3 \\`\r\n`    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh`\r\n\r\nI got the following error.\r\n\r\n> Package ffmpeg is not available, but is referred to by another package.\r\n> This may mean that the package is missing, has been obsoleted, or\r\n> is only available from another source\r\n> \r\n> E: Package 'ffmpeg' has no installation candidate\r\n> The command '/bin/sh -c /install/install_deb_packages.sh' returned a non-zero code: 100\r\n> ERROR: docker build failed. Dockerfile is at /home/pi/tensorflow/tensorflow/tools/ci_build/Dockerfile.pi-python3\r\n", "comments": ["I am getting the same error. I have tried it on Stretch and Buster on both version 3 and version 4 PIs without success.", "Can you please test with TF 2.3 branch since it was last tested with TF 2.3.0-rc2?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30891\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30891\">No</a>\n"]}, {"number": 30890, "title": "Fix for unit test failure", "body": "[INTEL MKL] Fix for failing unit testcase in : //tensorflow/core:graph_mkl_layout_pass.\r\nIt was failing due to wrong output check. The testcase was added for FusedBatchNormV3 support with Intel MKL DN PR: #30386 \r\n", "comments": ["@jojivk73 thank you for your contribution , can you please update description with relevant information.", "\r\n-Done"]}, {"number": 30889, "title": "Set the padded IO for CuDNN RNN only when necessary", "body": "Previously, we always execute the cudnnSetRNNPaddingMode(), which might cause the waste of workspace allocation.\r\n\r\nThis PR executes the cudnnSetRNNPaddingMode() only when the new cuDNNRNN***Ex is called and (sequence_lengths has different values or time_major = False).\r\n\r\nFor example of this RNN mode:\r\n```\r\nseqLength  = 20\r\nnumLayers  = 2\r\nhiddenSize = 512\r\ninputSize  = 512\r\nminiBatch  = 64\r\ndropout    = 0.000000\r\ndirection  = CUDNN_UNIDIRECTIONAL\r\nmode       = CUDNN_RNN_RELU\r\nalgo       = CUDNN_RNN_ALGO_STANDARD\r\nprecision  = CUDNN_DATA_FLOAT\r\n```\r\nCUDNN_RNN_PADDED_IO_ENABLED is unset: the workspace is 9961472 bytes.\r\nCUDNN_RNN_PADDED_IO_ENABLED is set: the workspace is 21496576 bytes (~2.1x more memory).\r\n\r\nfyi. @nluehr ", "comments": ["It seems the lstm_v2 and gru_v2 tests in \"Linux GPU\" run into \"CUDNN_STATUS_EXECUTION_FAILED\". But I can successfully run them in my local GPU. Could you provide some advice? @aaroey ", "@houtoms not sure if you can see this log:\r\n```\r\n2019-07-22 19:44:23.602236: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_EXECUTION_FAILED\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1779): 'cudnnRNNForwardTrainingEx( cudnn.handle(), rnn_desc.handle(), input_desc.data_handle(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.data_handle(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n2019-07-22 19:44:23.602294: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1523 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 4, 2, 1, 3, 2, 0]\r\n```\r\n\r\nSeems to be relevant but not sure how.", "@houtoms also I think the tests are run in P100, maybe you can try with that?", "@aaroey I can see those logs. I tried the gru_v2_test.py again on V100 and P100 and no failures occur.\r\n\r\nAlso, in this test, no cudnnRNNForwardTrainingEx() has been called.", "I also ran the lstm_v2_test.py under keras/layers and didn't see any errors.", "@aaroey Can you help with the failed tests?", "@houtoms sorry I'm OOO this week, will check when I'm back.", "> @houtoms not sure if you can see this log:\r\n> \r\n> ```\r\n> 2019-07-22 19:44:23.602236: E tensorflow/stream_executor/dnn.cc:588] CUDNN_STATUS_EXECUTION_FAILED\r\n> in tensorflow/stream_executor/cuda/cuda_dnn.cc(1779): 'cudnnRNNForwardTrainingEx( cudnn.handle(), rnn_desc.handle(), input_desc.data_handle(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.data_handle(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'\r\n> 2019-07-22 19:44:23.602294: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1523 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 4, 2, 1, 3, 2, 0]\r\n> ```\r\n> \r\n> Seems to be relevant but not sure how.\r\n\r\n@aaroey @kaixih Interesting, it seems to be very close to the issue we are investigating in https://github.com/mozilla/DeepSpeech/issues/3088\r\n\r\nI'm continuing to investigate and will file a new issue, but I'm curious to see that even on this PR there was dubious behavior."]}, {"number": 30888, "title": "Added operation `tf.math.reciprocal_no_nan()`", "body": "Hello, \r\n\r\nThis is a PR following feature request #30620. As discussed there, for now I have only added the op in the Python API. If required I'd be glad to add it to the C++ kernels and implement the gradient too.  ", "comments": ["Yes, it's those two commands\n\nOn Mon, Jul 22, 2019 at 12:01 PM Karthik Muthuraman <\nnotifications@github.com> wrote:\n\n> *@kmh4321* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/math_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/30888#discussion_r305981272>\n> :\n>\n> > @@ -4003,3 +4003,37 @@ def polyval(coeffs, x, name=None):\n>      for c in coeffs[1:]:\n>        p = c + p * x\n>      return p\n> +\n> +@tf_export(\"math.reciprocal_no_nan\", \"reciprocal_no_nan\")\n>\n> I found these two commands:\n>\n>     $ bazel build tensorflow/tools/api/tests:api_compatibility_test\n>     $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\n>           --update_goldens True\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30888?email_source=notifications&email_token=AAABHROAZWJUDHXOIU22JULQAX7ZTA5CNFSM4IFL5ZM2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOB7FW5AI#discussion_r305981272>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJ5JYZWHZXBKSO522TQAX7ZTANCNFSM4IFL5ZMQ>\n> .\n>\n\n\n-- \n - Alex\n", "I have regenerated the files. ", "Yes, let's raise a separate issue to improve the exception in div_no_nan in\neager mode.\n\nOn Fri, Jul 26, 2019 at 1:36 PM Karthik Muthuraman <notifications@github.com>\nwrote:\n\n> *@kmh4321* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/ops/math_ops.py\n> <https://github.com/tensorflow/tensorflow/pull/30888#discussion_r307899274>\n> :\n>\n> > +       `complex64` or `complex128`.\n> +    name: A name for the operation (optional).\n> +\n> +  Returns:\n> +    A `Tensor` of same shape and type as `x`.\n> +\n> +  Raises:\n> +    TypeError: x must be of a valid dtype.\n> +\n> +  \"\"\"\n> +  allowed_dtypes = [dtypes.float16, dtypes.float32, dtypes.float64,\n> +                    dtypes.complex64, dtypes.complex128]\n> +  with ops.name_scope(name, \"reciprocal_no_nan\", [x]) as scope:\n> +    x = ops.convert_to_tensor(x, name=\"x\")\n> +    if x.dtype.base_dtype not in allowed_dtypes:\n> +      raise TypeError(\"x has incorrect data type: {} \\n \"\n>\n> @alextp <https://github.com/alextp> so I'll remove exception handling\n> here. But in eager mode, div_no_nan() doesn't give a valid/useful\n> exception. How should we handle that? I can also raise a separate issue to\n> discuss this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30888?email_source=notifications&email_token=AAABHRKGDVZWSXDAVSMSRWLQBNN6PA5CNFSM4IFL5ZM2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOB7YA4NY#discussion_r307899274>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRNBPWRZCKOCNQAD45LQBNN6PANCNFSM4IFL5ZMQ>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp one of the test cases was still failing as it was performing exception handling in both eager and regular modes. I did not add the decorator `@testutil.run_in_graph_and_eager_modes` to that particular test and ran the tests locally and they worked but for some reason, the CI on Github failed the test for Ubuntu Python2 so I removed that test case for now."]}, {"number": 30887, "title": "TF 2.0: tf.distribute example not working (NCCL all reduce issue)", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.0b1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10/7.4\r\n- GPU model and memory: Titan Xp\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nWhen I try to run one of the distributed examples, the model fails using the default NCCL all reduce cross devices ops:\r\n\r\n<details>\r\n        <summary> NCCL All Reduce Error </summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-15-df42e8590c26> in <module>\r\n----> 1 model.fit(train_dataset, epochs=50, callbacks=callbacks)\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    641         max_queue_size=max_queue_size,\r\n    642         workers=workers,\r\n--> 643         use_multiprocessing=use_multiprocessing)\r\n    644 \r\n    645   def evaluate(self,\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_distributed.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\r\n    679           validation_steps=validation_steps,\r\n    680           validation_freq=validation_freq,\r\n--> 681           steps_name='steps_per_epoch')\r\n    682 \r\n    683   def evaluate(self,\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\r\n    292           else:\r\n    293             actual_inputs = ins()\r\n--> 294           batch_outs = f(actual_inputs)\r\n    295         except errors.OutOfRangeError:\r\n    296           if is_dataset:\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in execution_function(input_fn)\r\n    811     def execution_function(input_fn):\r\n    812       # `numpy` translates Tensors to values in Eager mode.\r\n--> 813       return [out.numpy() for out in distributed_function(input_fn)]\r\n    814     return execution_function\r\n    815 \r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\r\n    426         # Lifting succeeded, so variables are initialized and we can run the\r\n    427         # stateless function.\r\n--> 428         return self._stateless_fn(*args, **kwds)\r\n    429     else:\r\n    430       canon_args, canon_kwds = \\\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   1333     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   1334     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 1335     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   1336 \r\n   1337   @property\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)\r\n    587     \"\"\"\r\n    588     return self._call_flat(\r\n--> 589         (t for t in nest.flatten((args, kwargs), expand_composites=True)\r\n    590          if isinstance(t, (ops.Tensor,\r\n    591                            resource_variable_ops.ResourceVariable))))\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)\r\n    669     # Only need to override the gradient in graph mode and when we have outputs.\r\n    670     if context.executing_eagerly() or not self.outputs:\r\n--> 671       outputs = self._inference_function.call(ctx, args)\r\n    672     else:\r\n    673       self._register_gradient()\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)\r\n    443             attrs=(\"executor_type\", executor_type,\r\n    444                    \"config_proto\", config),\r\n--> 445             ctx=ctx)\r\n    446       # Replace empty list with None\r\n    447       outputs = outputs or None\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):\r\n\r\n~/.virtualenvs/physics3/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nInternalError: 2 root error(s) found.\r\n  (0) Internal:  internal error\r\n\t [[node Adam/NcclAllReduce_8 (defined at <ipython-input-15-df42e8590c26>:1) ]]\r\n  (1) Internal:  internal error\r\n\t [[node Adam/NcclAllReduce_8 (defined at <ipython-input-15-df42e8590c26>:1) ]]\r\n\t [[GroupCrossDeviceControlEdges_4/Adam/Adam/update_9/Const/_679]]\r\n0 successful operations.\r\n9 derived errors ignored. [Op:__inference_distributed_function_9447]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\n\r\n</details>\r\n\r\nSwitching to hierarchical all reduce in the scope allows the code to run, but with a performance penalty and more errrors:\r\n\r\n<details>\r\n        <summary> Hierarchical All Reduce Error </summary>\r\n\r\n```\r\n2019-07-19 15:06:47.632060: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-19 15:06:47.947109: W tensorflow/core/framework/model.cc:475] Failed to find a tunable parameter that would decrease the output time. This means that the autotuning optimization got stuck in a local maximum. The optimization attempt will be aborted.\r\n2019-07-19 15:06:48.925479: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-19 15:07:02.595311: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.\r\n2019-07-19 15:07:02.610888: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcupti.so.10.0\r\n2019-07-19 15:07:07.254401: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 1045 kernel records, 173 memcpy records.\r\n2019-07-19 15:07:12.732921: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_9/Const/_879]]\r\n2019-07-19 15:07:12.732922: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[replica_4/metrics/accuracy/AssignAddVariableOp_1/_375]]\r\n2019-07-19 15:07:12.732962: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[Adam/Adam/group_deps/_951]]\r\n2019-07-19 15:07:12.733034: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[replica_9/metrics/accuracy/AssignAddVariableOp_1/_203]]\r\n2019-07-19 15:07:12.732989: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_9/Const/_631]]\r\n2019-07-19 15:07:12.732981: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_359]]\r\n2019-07-19 15:07:12.733029: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[metrics/accuracy/div_no_nan/ReadVariableOp_16/_450]]\r\n2019-07-19 15:07:12.732962: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_9}}]]\r\n2019-07-19 15:07:12.733232: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[GroupCrossDeviceControlEdges_2/Adam/Adam/update_9/Const/_727]]\r\n2019-07-19 15:07:12.733380: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n\t [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_9/Const/_763]]\r\n2019-07-19 15:07:12.735619: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_8}}]]\r\n[I 15:08:10.336 LabApp] Saving file at /k\r\n```\r\n\r\n</details>\r\n\r\nFurther investigation has revealed that this only occurs when I add the 10th GPU on the node to the strategy... Using the *first* 9 GPUs is fine, but including the 10th GPU with any other combination of GPUs leads to this behavior. Confirmed on multiple independent systems after reinstalling cuda/drivers and restarting nodes. I still get an out-of-range error, but the model trains. The model also trains when *only* using the 10th gpu.\r\n\r\n<details>\r\n\r\n<summary> out of range error with nccl all reduce </summary>\r\n\r\n```\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1c:00.0\r\n2019-07-20 12:10:09.708295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1d:00.0\r\n2019-07-20 12:10:09.710603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1e:00.0\r\n2019-07-20 12:10:09.712948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3d:00.0\r\n2019-07-20 12:10:09.715631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3e:00.0\r\n2019-07-20 12:10:09.717939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3f:00.0\r\n2019-07-20 12:10:09.720316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:40:00.0\r\n2019-07-20 12:10:09.720644: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-20 12:10:09.722035: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-20 12:10:09.723341: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-07-20 12:10:09.723662: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-07-20 12:10:09.725225: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-07-20 12:10:09.726450: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-07-20 12:10:09.730176: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-20 12:10:09.761821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6\r\n2019-07-20 12:10:09.762583: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2019-07-20 12:10:11.263993: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3c306c0 executing computations on platform CUDA. Devices:\r\n2019-07-20 12:10:11.264051: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1\r\n2019-07-20 12:10:11.264071: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): TITAN Xp, Compute Capability 6.1\r\n2019-07-20 12:10:11.264088: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): TITAN Xp, Compute Capability 6.1\r\n2019-07-20 12:10:11.264104: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): TITAN Xp, Compute Capability 6.1\r\n2019-07-20 12:10:11.264119: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (4): TITAN Xp, Compute Capability 6.1\r\n2019-07-20 12:10:11.264135: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (5): TITAN Xp, Compute Capability 6.1\r\n2019-07-20 12:10:11.264151: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (6): TITAN Xp, Compute Capability 6.1\r\n2019-07-20 12:10:11.273304: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300010000 Hz\r\n2019-07-20 12:10:11.275776: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x45d7fb0 executing computations on platform Host. Devices:\r\n2019-07-20 12:10:11.275796: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-07-20 12:10:11.308790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1c:00.0\r\n2019-07-20 12:10:11.310775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1d:00.0\r\n2019-07-20 12:10:11.312785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1e:00.0\r\n2019-07-20 12:10:11.314726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3d:00.0\r\n2019-07-20 12:10:11.316719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3e:00.0\r\n2019-07-20 12:10:11.318675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3f:00.0\r\n2019-07-20 12:10:11.320680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:40:00.0\r\n2019-07-20 12:10:11.320722: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-20 12:10:11.320732: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-20 12:10:11.320741: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-07-20 12:10:11.320767: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-07-20 12:10:11.320778: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-07-20 12:10:11.320787: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-07-20 12:10:11.320797: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-20 12:10:11.347344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6\r\n2019-07-20 12:10:11.347438: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-20 12:10:11.362271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-20 12:10:11.362286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6\r\n2019-07-20 12:10:11.362292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y\r\n2019-07-20 12:10:11.362296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y\r\n2019-07-20 12:10:11.362300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y\r\n2019-07-20 12:10:11.362304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y\r\n2019-07-20 12:10:11.362309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y\r\n2019-07-20 12:10:11.362313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y\r\n2019-07-20 12:10:11.362317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N\r\n2019-07-20 12:10:11.378594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11424 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:1c:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.380969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11424 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:1d:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.385188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 11424 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:1e:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.389861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 11424 MB memory) -> physical GPU (device: 3, name: TITAN Xp, pci bus id: 0000:3d:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.393361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:4 with 11424 MB memory) -> physical GPU (device: 4, name: TITAN Xp, pci bus id: 0000:3e:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.397087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:5 with 11424 MB memory) -> physical GPU (device: 5, name: TITAN Xp, pci bus id: 0000:3f:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.400702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:6 with 11424 MB memory) -> physical GPU (device: 6, name: TITAN Xp, pci bus id: 0000:40:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.579203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1c:00.0\r\n2019-07-20 12:10:11.581543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1d:00.0\r\n2019-07-20 12:10:11.583872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1e:00.0\r\n2019-07-20 12:10:11.585869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3d:00.0\r\n2019-07-20 12:10:11.587873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 4 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3e:00.0\r\n2019-07-20 12:10:11.589889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 5 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:3f:00.0\r\n2019-07-20 12:10:11.591870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 6 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:40:00.0\r\n2019-07-20 12:10:11.591903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-07-20 12:10:11.591913: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-20 12:10:11.591921: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-07-20 12:10:11.591931: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-07-20 12:10:11.591957: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-07-20 12:10:11.591967: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-07-20 12:10:11.591978: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-20 12:10:11.618747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0, 1, 2, 3, 4, 5, 6\r\n2019-07-20 12:10:11.619236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-07-20 12:10:11.619246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 4 5 6\r\n2019-07-20 12:10:11.619255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y Y Y Y\r\n2019-07-20 12:10:11.619261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y Y Y Y\r\n2019-07-20 12:10:11.619292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y Y Y Y\r\n2019-07-20 12:10:11.619297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N Y Y Y\r\n2019-07-20 12:10:11.619303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 4:   Y Y Y Y N Y Y\r\n2019-07-20 12:10:11.619320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 5:   Y Y Y Y Y N Y\r\n2019-07-20 12:10:11.619326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 6:   Y Y Y Y Y Y N\r\n2019-07-20 12:10:11.635672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 11424 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:1c:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.637653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:1 with 11424 MB memory) -> physical GPU (device: 1, name: TITAN Xp, pci bus id: 0000:1d:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.639716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:2 with 11424 MB memory) -> physical GPU (device: 2, name: TITAN Xp, pci bus id: 0000:1e:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.641688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:3 with 11424 MB memory) -> physical GPU (device: 3, name: TITAN Xp, pci bus id: 0000:3d:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.643693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:4 with 11424 MB memory) -> physical GPU (device: 4, name: TITAN Xp, pci bus id: 0000:3e:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.645687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:5 with 11424 MB memory) -> physical GPU (device: 5, name: TITAN Xp, pci bus id: 0000:3f:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:11.647730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:6 with 11424 MB memory) -> physical GPU (device: 6, name: TITAN Xp, pci bus id: 0000:40:00.0, compute capability: 6.1)\r\n2019-07-20 12:10:26.189075: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1483] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-07-20 12:10:26.273130: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-07-20 12:10:28.489688: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-07-20 12:10:36.757915: I tensorflow/core/profiler/lib/profiler_session.cc:174] Profiler session started.\r\n2019-07-20 12:10:36.759100: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcupti.so.10.0\r\n2019-07-20 12:10:40.328991: I tensorflow/core/platform/default/device_tracer.cc:641] Collecting 722 kernel records, 110 memcpy records.\r\n2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_2}}]]\r\n\t [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_6/Const/_297]]\r\n2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_2}}]]\r\n\t [[replica_3/metrics/accuracy/AssignAddVariableOp_1/_187]]\r\n2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_2}}]]\r\n\t [[GroupCrossDeviceControlEdges_5/Adam/Adam/update_5_1/Const/_365]]\r\n2019-07-20 12:10:45.501340: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_2}}]]\r\n\t [[PermConstNCHWToNHWC-LayoutOptimizer/_16]]\r\n2019-07-20 12:10:45.501348: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_2}}]]\r\n\t [[GroupCrossDeviceControlEdges_2/Adam/Adam/update_6/Const/_421]]\r\n2019-07-20 12:10:45.501348: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_2}}]]\r\n\t [[GroupCrossDeviceControlEdges_2/Identity_1/_523]]\r\n2019-07-20 12:10:45.501354: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_2}}]]\r\n\t [[replica_5/loss/mul/_260]]\r\n2019-07-20 12:10:45.502198: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_2}}]]\r\n```\r\n</details>\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\n\r\nhttps://www.tensorflow.org/beta/tutorials/distribute/keras\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @mjlbach, thanks for filing the issue.  Please confirm a couple of things:\r\n1. Are you using `tf.distribute.MirroredStrategy` on a single machine with multiple GPUs?\r\n2. Can you run NCCL tests on the machine without any TensorFlow code? https://github.com/NVIDIA/nccl-tests", "1. Yes a single node\r\n2. It appears to be an nccl issue; I opened an issue with  NVIDIA/nccl#241\r\n\r\n```\r\nmjlbach@node05:~/nccl-tests$  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 8\r\n# nThread 1 nGpus 8 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1\r\n#\r\n# Using devices\r\n#   Rank  0 Pid 226099 on node05-ccncluster device  0 [0x1a] TITAN Xp\r\n#   Rank  1 Pid 226099 on node05-ccncluster device  1 [0x1b] TITAN Xp\r\n#   Rank  2 Pid 226099 on node05-ccncluster device  2 [0x1c] TITAN Xp\r\n#   Rank  3 Pid 226099 on node05-ccncluster device  3 [0x1d] TITAN Xp\r\n#   Rank  4 Pid 226099 on node05-ccncluster device  4 [0x1e] TITAN Xp\r\n#   Rank  5 Pid 226099 on node05-ccncluster device  5 [0x3d] TITAN Xp\r\n#   Rank  6 Pid 226099 on node05-ccncluster device  6 [0x3e] TITAN Xp\r\n#   Rank  7 Pid 226099 on node05-ccncluster device  7 [0x3f] TITAN Xp\r\n#\r\n#                                                     out-of-place                       in-place\r\n#       size         count    type   redop     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)                     (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)\r\n           8             2   float     sum    42.86    0.00    0.00  1e-07    42.51    0.00    0.00  1e-07\r\n          16             4   float     sum    42.46    0.00    0.00  1e-07    43.06    0.00    0.00  1e-07\r\n          32             8   float     sum    42.90    0.00    0.00  6e-08    42.75    0.00    0.00  6e-08\r\n          64            16   float     sum    42.81    0.00    0.00  6e-08    43.06    0.00    0.00  6e-08\r\n         128            32   float     sum    42.81    0.00    0.01  6e-08    42.92    0.00    0.01  6e-08\r\n         256            64   float     sum    43.05    0.01    0.01  3e-08    43.34    0.01    0.01  3e-08\r\n         512           128   float     sum    42.79    0.01    0.02  3e-08    42.65    0.01    0.02  3e-08\r\n        1024           256   float     sum    42.91    0.02    0.04  1e-07    43.00    0.02    0.04  1e-07\r\n        2048           512   float     sum    43.35    0.05    0.08  2e-07    43.25    0.05    0.08  2e-07\r\n        4096          1024   float     sum    43.46    0.09    0.16  2e-07    43.40    0.09    0.17  2e-07\r\n        8192          2048   float     sum    44.38    0.18    0.32  2e-07    43.88    0.19    0.33  2e-07\r\n       16384          4096   float     sum    49.15    0.33    0.58  2e-07    48.86    0.34    0.59  2e-07\r\n       32768          8192   float     sum    72.44    0.45    0.79  2e-07    71.88    0.46    0.80  2e-07\r\n       65536         16384   float     sum    120.5    0.54    0.95  2e-07    121.7    0.54    0.94  2e-07\r\n      131072         32768   float     sum    129.5    1.01    1.77  2e-07    129.5    1.01    1.77  2e-07\r\n      262144         65536   float     sum    157.1    1.67    2.92  2e-07    157.0    1.67    2.92  2e-07\r\n      524288        131072   float     sum    205.4    2.55    4.47  2e-07    205.3    2.55    4.47  2e-07\r\n     1048576        262144   float     sum    305.1    3.44    6.01  2e-07    305.0    3.44    6.02  2e-07\r\n     2097152        524288   float     sum    647.4    3.24    5.67  2e-07    495.1    4.24    7.41  2e-07\r\n     4194304       1048576   float     sum    900.7    4.66    8.15  2e-07    898.9    4.67    8.17  2e-07\r\n     8388608       2097152   float     sum   1735.0    4.83    8.46  2e-07   1718.9    4.88    8.54  2e-07\r\n    16777216       4194304   float     sum   3425.8    4.90    8.57  2e-07   3406.6    4.92    8.62  2e-07\r\n    33554432       8388608   float     sum   6793.3    4.94    8.64  2e-07   6792.5    4.94    8.64  2e-07\r\n    67108864      16777216   float     sum    13579    4.94    8.65  2e-07    13574    4.94    8.65  2e-07\r\n   134217728      33554432   float     sum    27135    4.95    8.66  2e-07    27134    4.95    8.66  2e-07\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 3.0361\r\n#\r\nmjlbach@node05:~/nccl-tests$  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 10\r\n# nThread 1 nGpus 10 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1\r\n#\r\n# Using devices\r\n#   Rank  0 Pid 226138 on node05-ccncluster device  0 [0x1a] TITAN Xp\r\n#   Rank  1 Pid 226138 on node05-ccncluster device  1 [0x1b] TITAN Xp\r\n#   Rank  2 Pid 226138 on node05-ccncluster device  2 [0x1c] TITAN Xp\r\n#   Rank  3 Pid 226138 on node05-ccncluster device  3 [0x1d] TITAN Xp\r\n#   Rank  4 Pid 226138 on node05-ccncluster device  4 [0x1e] TITAN Xp\r\n#   Rank  5 Pid 226138 on node05-ccncluster device  5 [0x3d] TITAN Xp\r\n#   Rank  6 Pid 226138 on node05-ccncluster device  6 [0x3e] TITAN Xp\r\n#   Rank  7 Pid 226138 on node05-ccncluster device  7 [0x3f] TITAN Xp\r\n#   Rank  8 Pid 226138 on node05-ccncluster device  8 [0x40] TITAN Xp\r\n#   Rank  9 Pid 226138 on node05-ccncluster device  9 [0x41] TITAN Xp\r\nSegmentation fault (core dumped)\r\nmjlbach@node05:~/nccl-tests$\r\n```", "Thanks for opening the NCCL issue.  A potential work around in TensorFlow is to use a different `cross_device_ops` object with `MirroredStrategy`.  The default is [`NcclAllReduce`](https://github.com/tensorflow/tensorflow/blob/f22a98fcf8855cb252658437f248874bd7602082/tensorflow/python/distribute/cross_device_ops.py#L793) but you can specify a different cross device ops object in the [strategy constructor](https://github.com/tensorflow/tensorflow/blob/d1305cf106fc461aff05f7a08f1ed365f9ade4f6/tensorflow/python/distribute/mirrored_strategy.py#L354); perhaps you can try [`HierarchicalCopyAllReduce`](https://github.com/tensorflow/tensorflow/blob/d1305cf106fc461aff05f7a08f1ed365f9ade4f6/tensorflow/python/distribute/cross_device_ops.py#L819).  Note that you may see lower performance with this workaround.\r\n\r\nI'm going to close this because it seems like a platform/NCCL issue, but please re-open if there's anything else.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30887\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30887\">No</a>\n", "To follow-up; the issue was an X server was running on the GPUs.", "Edit: This issue is not closed, even after fixing NCCL the problem persists.", "Do you mean the out of range error?", "No, even though NCCL works now, the example still fails with the same NCCL error. NCCL tests compiled against system (2.4.7) NCCL yields:\r\n\r\n```\r\n(physics3)mjlbach@node05-ccncluster:~/nccl-tests$ ./build/all_reduce_perf -b 8 -e 128M -f\r\n2 -g 10\r\n# nThread 1 nGpus 10 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 validation: 1\r\n#\r\n# Using devices\r\n#   Rank  0 Pid 204463 on node05-ccncluster device  0 [0x1a] TITAN Xp\r\n#   Rank  1 Pid 204463 on node05-ccncluster device  1 [0x1b] TITAN Xp\r\n#   Rank  2 Pid 204463 on node05-ccncluster device  2 [0x1c] TITAN Xp\r\n#   Rank  3 Pid 204463 on node05-ccncluster device  3 [0x1d] TITAN Xp\r\n#   Rank  4 Pid 204463 on node05-ccncluster device  4 [0x1e] TITAN Xp\r\n#   Rank  5 Pid 204463 on node05-ccncluster device  5 [0x3d] TITAN Xp\r\n#   Rank  6 Pid 204463 on node05-ccncluster device  6 [0x3e] TITAN Xp\r\n#   Rank  7 Pid 204463 on node05-ccncluster device  7 [0x3f] TITAN Xp\r\n#   Rank  8 Pid 204463 on node05-ccncluster device  8 [0x40] TITAN Xp\r\n#   Rank  9 Pid 204463 on node05-ccncluster device  9 [0x41] TITAN Xp\r\n#\r\n#                                                     out-of-place                       in-place\r\n#       size         count    type   redop     time   algbw   busbw  error     time   algbw   busbw  error\r\n#        (B)    (elements)                     (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)\r\n           8             2   float     sum    98.43    0.00    0.00  1e-07    110.5    0.00    0.00  1e-07\r\n          16             4   float     sum    109.4    0.00    0.00  1e-07    108.9    0.00    0.00  1e-07\r\n          32             8   float     sum    82.26    0.00    0.00  6e-08    57.92    0.00    0.00  6e-08\r\n          64            16   float     sum    57.85    0.00    0.00  6e-08    58.01    0.00    0.00  6e-08\r\n         128            32   float     sum    57.95    0.00    0.00  6e-08    57.64    0.00    0.00  6e-08\r\n         256            64   float     sum    57.86    0.00    0.01  3e-08    58.32    0.00    0.01  3e-08\r\n         512           128   float     sum    57.67    0.01    0.02  3e-08    58.17    0.01    0.02  3e-08\r\n        1024           256   float     sum    58.26    0.02    0.03  2e-07    58.24    0.02    0.03  2e-07\r\n        2048           512   float     sum    58.88    0.03    0.06  2e-07    58.21    0.04    0.06  2e-07\r\n        4096          1024   float     sum    60.01    0.07    0.12  2e-07    59.67    0.07    0.12  2e-07\r\n        8192          2048   float     sum    59.74    0.14    0.25  2e-07    59.66    0.14    0.25  2e-07\r\n       16384          4096   float     sum    61.11    0.27    0.48  2e-07    60.57    0.27    0.49  2e-07\r\n       32768          8192   float     sum    82.05    0.40    0.72  2e-07    81.89    0.40    0.72  2e-07\r\n       65536         16384   float     sum    140.4    0.47    0.84  2e-07    142.4    0.46    0.83  2e-07\r\n      131072         32768   float     sum    154.1    0.85    1.53  2e-07    154.1    0.85    1.53  2e-07\r\n      262144         65536   float     sum    191.3    1.37    2.47  2e-07    193.2    1.36    2.44  2e-07\r\n      524288        131072   float     sum    246.7    2.12    3.82  2e-07    249.2    2.10    3.79  2e-07\r\n     1048576        262144   float     sum    352.6    2.97    5.35  2e-07    353.3    2.97    5.34  2e-07\r\n     2097152        524288   float     sum    566.8    3.70    6.66  2e-07    564.5    3.72    6.69  2e-07\r\n     4194304       1048576   float     sum    982.8    4.27    7.68  2e-07    981.2    4.27    7.69  2e-07\r\n     8388608       2097152   float     sum   1847.0    4.54    8.18  2e-07   1839.7    4.56    8.21  2e-07\r\n    16777216       4194304   float     sum   3566.8    4.70    8.47  2e-07   3567.5    4.70    8.46  2e-07\r\n    33554432       8388608   float     sum   6890.0    4.87    8.77  2e-07   6893.3    4.87    8.76  2e-07\r\n    67108864      16777216   float     sum    13753    4.88    8.78  2e-07    13756    4.88    8.78  2e-07\r\n   134217728      33554432   float     sum    27445    4.89    8.80  2e-07    27432    4.89    8.81  2e-07\r\n# Out of bounds values : 0 OK\r\n# Avg bus bandwidth    : 3.00048\r\n#\r\n```\r\n\r\nBut when I run the example keras script yields the following python error:\r\n```\r\nInternalError: 2 root error(s) found.\r\n  (0) Internal:  internal error\r\n\t [[node Adam/NcclAllReduce_8 (defined at <ipython-input-14-b87e02e8ae0a>:1) ]]\r\n  (1) Internal:  internal error\r\n\t [[node Adam/NcclAllReduce_8 (defined at <ipython-input-14-b87e02e8ae0a>:1) ]]\r\n\t [[Adam/Adam/Identity_2/ReadVariableOp/_963]]\r\n0 successful operations.\r\n9 derived errors ignored. [Op:__inference_distributed_function_219447]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function\r\n```\r\n```\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[Adam/Adam/Identity_2/ReadVariableOp/_963]]\r\n2019-07-24 21:47:55.279011: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[Adam/Adam/group_deps/_919]]\r\n2019-07-24 21:47:55.279054: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_8/Const/_795]]\r\n2019-07-24 21:47:55.279094: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[GroupCrossDeviceControlEdges_2/Adam/Adam/update_9/Const/_779]]\r\n2019-07-24 21:47:55.279132: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_9/Const/_639]]\r\n2019-07-24 21:47:55.279178: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[GroupCrossDeviceControlEdges_8/Adam/Adam/update_8/Const/_831]]\r\n2019-07-24 21:47:55.279252: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[GroupCrossDeviceControlEdges_7/Adam/Adam/update_9/Const/_563]]\r\n2019-07-24 21:47:55.279296: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[GroupCrossDeviceControlEdges_0/Adam/Adam/update_9/Const/_647]]\r\n2019-07-24 21:47:55.279336: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[GroupCrossDeviceControlEdges_1/Adam/Adam/update_9/Const/_695]]\r\n2019-07-24 21:47:55.279780: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n         [[Identity_1/_1008]]\r\n2019-07-24 21:47:55.280420: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: internal error\r\n         [[{{node Adam/NcclAllReduce_8}}]]\r\n2019-07-24 21:47:55.280487: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n2019-07-24 21:47:55.280545: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n2019-07-24 21:47:55.280591: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n2019-07-24 21:47:55.280660: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n2019-07-24 21:47:55.280738: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n2019-07-24 21:47:55.280799: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n2019-07-24 21:47:55.280856: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n2019-07-24 21:47:55.280916: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n2019-07-24 21:47:55.280965: W tensorflow/core/framework/op_kernel.cc:1546] OP_REQUIRES failed at nccl_ops.cc:100 : Internal: internal error\r\n```", "Okay, can you run with `NCCL_DEBUG=INFO` and post the logs?", "This seems to be the cause... I had this issue running NCCL-tests when X was enabled, but now NCCL-tests runs fine.\r\n\r\n```\r\n2019-07-25 09:33:34.093469: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 0 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76d46da2c0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.105255: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 1 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76cc74aba0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.115187: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 2 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76b071ae60 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.123710: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 3 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76c870a380 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.134640: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 4 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76c47226a0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.145270: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 5 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76c0740bc0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.149014: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 6 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76d072a8e0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.151168: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 7 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76bc731660 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.151761: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 8 and 9, status: Internal: failed to enable\r\npeer access from 0x7f76b87398c0 to 0x7f76b4713830: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.151981: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 0, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76d46da2c0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.152197: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 1, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76cc74aba0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.152413: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 2, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76b071ae60: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.152628: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 3, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76c870a380: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.152846: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 4, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76c47226a0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.153051: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 5, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76c0740bc0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.153254: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 6, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76d072a8e0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.153459: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 7, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76bc731660: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.153664: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1597] Unable to enable peer access between device ordinals 9 and 8, status: Internal: failed to enable\r\npeer access from 0x7f76b4713830 to 0x7f76b87398c0: CUDA_ERROR_TOO_MANY_PEERS: peer mapping resources exhausted\r\n2019-07-25 09:33:34.156023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:1a:00.0\r\n```", "I think you may be running into [this issue mentioned in NVIDIA docs](https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__PEER__ACCESS.html#group__CUDA__PEER__ACCESS_1g0889ec6728e61c05ed359551d67b3f5a) and also discussed in another [TensorFlow issue thread](https://github.com/tensorflow/benchmarks/issues/143).  Can you follow-up with NVIDIA on the NCCL issue and confirm that more than 8 GPUs is actually a well-supported configuration?", "In the linked issue in NVIDIA/nccl#241, @sjeaugey believes it's an issue on the TF side", "Hi @sjeaugey, can you help us understand what are the possible causes of `CUDA_ERROR_TOO_MANY_PEERS`?", "There is indeed a limitation to 8 peers as described in the documentation, hence if you enable P2P between all GPUs it will fail with 10 GPUs or more per system. NCCL only uses 2 connections at the moment so it does work in that case (provided Xorg didn't allocate all peer connections already).\r\n\r\nWhether you want to support that case in TF is up to you. Maybe you don't need to enable p2p between all GPUs if you are using NCCL for intra-node communication.\r\n\r\nAnd while this 10x TitanXp configuration is probably not supported, there could be similar configurations with e.g. T4 which you might want to support.", "got some problem on Windows. is it a limitation and we can't do anything with this?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30887\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30887\">No</a>\n"]}, {"number": 30886, "title": "Refactor AssertNextDatasetOp", "body": "This PR refactors `AssertNextDatasetOp` and adds the tests.\r\n\r\ncc: @jsimsa ", "comments": []}, {"number": 30885, "title": "Possible GPU Memory Leak in Non-Distributed Use of `tf.estimator.train_and_evaluate`", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 1.10.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: K80 12GB\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running `tf.estimator.train_and_evaluate` it seems the graph or session is somehow not properly destroyed when `max_steps` is `None` or less than the number of steps in the `input_fn`.  This can result in OOM errors when running prediction afterward on GPU.\r\n\r\n**Describe the expected behavior**\r\n\r\nUsing `tf.estimator.train_and_evaluate` should destroy the graph/session when either the max steps has been reached, OR the input_fn raises an `OutOfRangeError`\r\n\r\n**Code to reproduce the issue**\r\n\r\n`python min.py bad`\r\n\r\nFor expected behavior:\r\n\r\n`python min.py good`\r\n\r\nNOTE: Your running the bad code may or may not crash depending on your GPU memory.  To force crashing, you can increase the array size: `np.random.rand(xx, yy)`\r\n\r\n```python\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom hooks import InitHook\r\n\r\ndef input_fn():\r\n  dataset = tf.data.Dataset.range(100)\r\n  # Make sequence data\r\n  dataset = dataset.map(lambda x: {'x': [x]})\r\n  dataset = dataset.repeat(3)\r\n  return dataset\r\n\r\ndef model_fn(features, labels, mode, params):\r\n  seq = features['x']\r\n  with tf.device('/gpu:0'):\r\n    arr = np.random.rand(3000000, 400)\r\n    var = tf.get_variable('big', arr.shape, trainable=False)\r\n    emb = tf.nn.embedding_lookup(var, seq)\r\n  logits = tf.layers.dense(emb, 1000)\r\n  predictions = tf.greater(logits, 0.0)\r\n  # Don't care about loss but have to provide something\r\n  loss = tf.reduce_mean(logits)\r\n  trainable_vars = tf.trainable_variables()\r\n  global_step = tf.train.get_or_create_global_step()\r\n  saveable_vars = trainable_vars + [global_step]\r\n  def init_fn(scaffold, sess):\r\n    sess.run(var.initializer, {var.initial_value: arr})\r\n  saver = tf.train.Saver(var_list=saveable_vars)\r\n  if mode == tf.estimator.ModeKeys.TRAIN:\r\n    scaffold = tf.train.Scaffold(init_fn=init_fn, saver=saver)\r\n    optimizer = tf.train.GradientDescentOptimizer(0.1)\r\n    train_op = optimizer.minimize(loss, global_step=global_step)\r\n    output_spec = tf.estimator.EstimatorSpec(\r\n      mode=mode,\r\n      loss=loss,\r\n      scaffold=scaffold,\r\n      train_op=train_op)\r\n  elif mode == tf.estimator.ModeKeys.EVAL:\r\n    hooks = [InitHook(var.initializer, {var.initial_value: arr})]\r\n    ready_for_local_init_op = tf.constant([], dtype=tf.string)\r\n    ready_op = tf.constant([], dtype=tf.string)\r\n    scaffold = tf.train.Scaffold(init_fn=init_fn, saver=saver, ready_for_local_init_op=ready_for_local_init_op, ready_op=ready_op)\r\n    output_spec = tf.estimator.EstimatorSpec(\r\n      scaffold=scaffold,\r\n      mode=mode,\r\n      evaluation_hooks=hooks,\r\n      loss=loss)\r\n  elif mode == tf.estimator.ModeKeys.PREDICT:\r\n    ready_for_local_init_op = tf.constant([], dtype=tf.string)\r\n    ready_op = tf.constant([], dtype=tf.string)\r\n    scaffold = tf.train.Scaffold(ready_for_local_init_op=ready_for_local_init_op, ready_op=ready_op, saver=saver)\r\n    hooks = [InitHook(var.initializer, {var.initial_value: arr})]\r\n    predictions = {\r\n      'predictions': predictions\r\n    }\r\n    output_spec = tf.estimator.EstimatorSpec(\r\n      mode=mode,\r\n      predictions=predictions,\r\n      scaffold=scaffold,\r\n      prediction_hooks=hooks)\r\n  return output_spec\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\nestimator = tf.estimator.Estimator(model_fn=model_fn, config=tf.estimator.RunConfig())\r\neval_spec = tf.estimator.EvalSpec(input_fn=input_fn, start_delay_secs=0, throttle_secs=3)\r\nif sys.argv[1] == 'bad':\r\n  train_spec = tf.estimator.TrainSpec(input_fn=input_fn)\r\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\r\nelif sys.argv[1] == 'good':\r\n  train_spec = tf.estimator.TrainSpec(input_fn=input_fn, max_steps=100)\r\n  estimator.evaluate(input_fn=input_fn)\r\nresults = estimator.predict(input_fn=input_fn)\r\nfor result in results:\r\n  pass\r\n```\r\n\r\nEdit:\r\nIn the same directory is `hooks.py` containing the following:\r\n```python\r\nclass InitHook(training.SessionRunHook):\r\n  def __init__(self, op, feed_dict):\r\n    self.op = op\r\n    self.feed_dict = feed_dict\r\n\r\n  def after_create_session(self, session, coord):  # pylint: disable=unused-argument\r\n    session.run(self.op, feed_dict=self.feed_dict)\r\n```", "comments": ["Limiting gpu memory can help resolve OOM error. \r\nSee https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth", "> Limiting gpu memory can help resolve OOM error.\r\n> See https://www.tensorflow.org/guide/using_gpu#allowing_gpu_memory_growth\r\n\r\nI didn't get a GPU out of memory error, it was my RAM that eventually exceeded 70GB", "**TF Version : tensorflow-gpu==2.0.0-rc0**\r\n\r\nWhen ever I use `tf.data`, I always experience a lag on my Dell G7 laptop (32GB RAM, Nividia 1060 6GB, 12cores i7). With TFRecords in place, the very reason of switching to Dataset APIs goes for waste. After each epoch the RAM memory foot print increases linearly till OS kicks in and kills the application.\r\n\r\nWith Tensorflow 2, I had to do quite a bit of work to port my existing code out of `contrib` and `tf.slim` APIs, post that I am stuck with this dataset OOM issue :(  \r\n\r\nLooking for the community help to resume my work ASAP. Thanks!\r\n\r\nFollowing code snippet is what I mostly use:\r\n\r\n ```python\r\n def memory_usage_psutil():\r\n        # return the memory usage in MB\r\n        import psutil\r\n        process = psutil.Process(os.getpid())\r\n        mem = process.memory_info()[0] / float(2 ** 20)\r\n        print(\"\\n\\n\\n\\n\\n\\n\")\r\n        print_info(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\r\n        print(f\"Memory used is {mem}\")\r\n        print_info(\"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\r\n        print(\"\\n\\n\\n\\n\\n\\n\")\r\n        return mem\r\n\r\n#======================================================================\r\n\r\n    def _get_train_dataset(self):\r\n        \"\"\"\r\n        Reads TFRecords, decode and batches them\r\n        :return: dataset\r\n        \"\"\"\r\n        print_info(\"_get_train_dataset\")\r\n        memory_usage_psutil()\r\n        path = os.path.join(self._train_out_dir, \"*.tfrecords\")\r\n        path = path.replace(\"//\", \"/\")\r\n\r\n        files = tf.data.Dataset.list_files(path)\r\n\r\n        # assert len(files) > 0\r\n\r\n        # print_info(f\"Number of TFRecords : {len(files)}\")\r\n\r\n        # TF dataset APIs\r\n        # dataset = tf.data.TFRecordDataset(files, num_parallel_reads=self._num_cores)\r\n        dataset = files.interleave(\r\n            tf.data.TFRecordDataset, cycle_length=self._num_cores,\r\n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        dataset = dataset.shuffle(self._batch_size*2, 42)\r\n        # Map the generator output as features as a dict and labels\r\n        dataset = dataset.map(map_func=self.decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n        dataset = dataset.batch(batch_size=self._batch_size, drop_remainder=False)\r\n        dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n        # dataset = dataset.repeat()\r\n        # dataset = dataset.cache(filename=os.path.join(self.iterator_dir, \"train_data_cache\"))\r\n        print_info(\"Dataset output sizes are: \")\r\n        print_info(dataset)\r\n        memory_usage_psutil()\r\n\r\n        return dataset\r\n\r\n#======================================================================\r\n\r\n    def _get_train_spec(self, max_steps=None):\r\n        # Estimators expect an input_fn to take no arguments.\r\n        # To work around this restriction, we use lambda to capture the arguments and provide the expected interface.\r\n        return tf.estimator.TrainSpec(\r\n            input_fn=lambda: self.dataset.train_set(),\r\n            max_steps=max_steps,\r\n            hooks=self._train_hooks)\r\n\r\n    def _get_eval_spec(self, steps):\r\n        return tf.estimator.EvalSpec(\r\n            input_fn=lambda: self.dataset.validation_set(),\r\n            steps=steps,\r\n            hooks=self._eval_hooks)\r\n\r\n#======================================================================\r\n\r\nself._estimator = tf.estimator.Estimator(model_fn=self._model, config=config, params=None)\r\n\r\n#======================================================================\r\n\r\n    def train_and_evaluate(self, max_train_steps=None, eval_steps=None):\r\n        \"\"\"\r\n        Trains and evaluates the model. See\r\n        :tf_main:`tf.estimator.train_and_evaluate\r\n        <estimator/train_and_evaluate>` for more details.\r\n\r\n        Args:\r\n            max_train_steps (int, optional): Total number of steps for which\r\n                to train model. If `None`, train forever or until the train\r\n                data generates the OutOfRange exception. If OutOfRange occurs\r\n                in the middle, training stops before :attr:`max_steps` steps.\r\n            eval_steps (int, optional): Number of steps for which to evaluate\r\n                model. If `None`, evaluates until the eval data raises an\r\n                OutOfRange exception.\r\n        \"\"\"\r\n        train_spec = self._get_train_spec(max_steps=max_train_steps)\r\n        eval_spec = self._get_eval_spec(steps=eval_steps)\r\n        tf.estimator.train_and_evaluate(self._estimator, train_spec, eval_spec)\r\n\r\n#======================================================================\r\n\r\n```", "@corynezin  could you please update the issue description stating RAM OOM ? ", "Update:  \r\nBy disabling TF 2 behavior, the memory build up got slowed down to MBs from GBs.\r\n\r\n```python\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```", "@Mageswaran1989 Can you please post a new issue to describe your problem? Its better to keep one issue thread for each problem. Thanks!", "@ymodak A new issue is raised @ https://github.com/tensorflow/tensorflow/issues/32052", "@corynezin  I met this error, have you solve it ? ", "@ymodak  I have met the same issue, can you help us ?", "> @corynezin I met this error, have you solve it ?\r\n\r\nI have not been able to solve it, for now I am running `predict` in a separate process using the checkpoint produced during training.", "@corynezin Is this still an issue with latest tf version. Thanks!", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30885\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30885\">No</a>\n"]}, {"number": 30884, "title": "[XLA:GPU][ROCm] Refactor and extract platform dependent logic from cudnn_conv_algorithm_picker", "body": "This PR creates a base class `gpu_conv_algorithm_picker` merges both `cudnn_conv_algorithm_picker` and `rocm_conv_algorithm_picker`. `PickBestAlgorithmNoCache()` function becomes platform dependent.\r\n\r\n------\r\n\r\nTest performed: //tensorflow/compiler/xla/tests:convolution_test all passed\r\n@whchung @timshen91 ", "comments": ["The check results failures are unrelated with this PR. All tests are passing.\r\n\r\nAccording to invocation log, the failure is due to \r\n\r\n> ERROR: /tmpfs/src/github/tensorflow/tensorflow/tools/api/golden/BUILD:3:1: invalid label '//tensorflow:tensorflow_py:__subpackages__' in element 0 of 'package' argument: invalid target name 'tensorflow_py:__subpackages__': target names may not contain ':'\r\nERROR: package contains errors: tensorflow/tools/api/golden\r\n", "@jerryyin , how much are the ROCM vs cuDNN implementations differ? The only difference I've seen is that ROCM doesn't have redzone checking / content checking yet. Otherwise, the portability should be already provided by AlgorithmDesc, ProfileResult, and RunCudnnConv (with a different name, of course). If we add a flag to turn off the checking, would that work for ROCM?", "@timshen91 Thanks for your review. You are right about redzone & comparator difference. Beside this, all difference comes from the implementation of the `PickBestAlgorithmNoCache()` function. Because it is coupled into the implementation from stream_executor's implementation. Assuming we want `ROCm` and `Cuda` into one class:\r\n\r\n- We call the current implementation of convolution as \"find mode\". Regarding the find mode, we do not loop through and profile the convolution algorithm candidates. This means the implementation has to diverge between `ROCm` and `Cuda`.\r\n- The on-going implementation of convolution is \"immediate mode\". Regarding the immediate mode, we are still experimenting between profiling/non-profiling versions. For the profiling version, the biggest difference come from the on going discussion from divergence of `GetAlgorithms()` divergence, from @deven-amd review [here](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/524/files#r298292150). Feel free to provide feedback on the new interface.\r\n- Other than stuff mentioned above, we should be able to work through the platform related utility functions, like `CetCudnnVersion()`, `GetComputeCapability()`.\r\n\r\nAs a summary, the best chance to achieve unified implementation is dependent on:\r\n\r\n- `ROCm` side implement and adopt Redzone allocator (This is not a prioritized feature, but on our list). It is probably not enough to add a flag to turn off Redzone checking from your side.\r\n- The immediate mode choose to profile algorithms, as cuda do (dependent on @deven-amd's feature )\r\n- The immediate mode does not use the new `GetAlgorithms()` inteface (dependent on [PR#524](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/pull/524/files))\r\n\r\nMy suggestion is to keep the branching child class between `ROCm` and `Cuda` for now, because of the above reasons and revisit this topic when Deven submit his PR for master branch. Let me know your thoughts @timshen91 @whchung \r\n", "> @timshen91 Thanks for your review. You are right about redzone & comparator difference. Beside this, all difference comes from the implementation of the `PickBestAlgorithmNoCache()` function. Because it is coupled into the implementation from stream_executor's implementation.\r\n\r\nThanks for the details. I'd suggest below:\r\n* Keep the ROCM and Cuda code in the same class / file.\r\n* Write `PickBestCudaAlgorithmNoCache` and `PickBestRocmAlgorithmNoCache` separately. Since `GetAlgorithms` only appears in `PickBestCudaAlgorithmNoCache`, and you feel free do whatever you want in `PickBestRocmAlgorithmNoCache`.\r\n\r\nThis way we allow (1) freedom for ROCM implementation, (2) reuse the caching code likewise, (3) and also keep the boilerplate down.\r\n\r\nA while back I suggested the same approach for `core/kernel/conv_ops.cc` in https://github.com/tensorflow/tensorflow/pull/26018#issuecomment-476378148.\r\n\r\nWhat do you think?", "@timshen91 I think that's reasonable to do, using inheritance can bring coupling problems that hard to resovle later. Let me make sure we are on the same page. Are you suggesting the following?\r\n\r\n```\r\nGpuConvAlgorithmPicker::PickBestCudaAlgorithmNoCache(){...}\r\nGpuConvAlgorithmPicker::PickBestROCmAlgorithmNoCache(){...}\r\nGpuConvAlgorithmPicker::PickBestAlgorithm(){\r\n...\r\n#if GOOGLE_CUDA\r\n   PickBestCudaAlgorithmNoCache()\r\n#elif TENSORFLOW_USE_ROCM\r\n   PickBestROCmAlgorithmNoCache()\r\n#endif\r\n\r\n...\r\n}\r\n```\r\nDo you agree with:\r\n\r\n- Renaming `CudaConvAlgorithmPicker` to `GpuConvAlgorithmPicker`\r\n- Use macros to branch between different implementations", "> @timshen91 I think that's reasonable to do, using inheritance can bring coupling problems that hard to resovle later. Let me make sure we are on the same page. Are you suggesting the following?\r\n> \r\n> ```\r\n> GpuConvAlgorithmPicker::PickBestCudaAlgorithmNoCache(){...}\r\n> GpuConvAlgorithmPicker::PickBestROCmAlgorithmNoCache(){...}\r\n> GpuConvAlgorithmPicker::PickBestAlgorithm(){\r\n> ...\r\n> #if GOOGLE_CUDA\r\n>    PickBestCudaAlgorithmNoCache()\r\n> #elif TENSORFLOW_USE_ROCM\r\n>    PickBestROCmAlgorithmNoCache()\r\n> #endif\r\n> \r\n> ...\r\n> }\r\n> ```\r\n\r\nYes, except for the `#if`.\r\n\r\n> \r\n> Do you agree with:\r\n> \r\n> * Renaming `CudaConvAlgorithmPicker` to `GpuConvAlgorithmPicker`\r\n\r\nYes.\r\n\r\n> * Use macros to branch between different implementations\r\n\r\nNo. I think `if (StreamExecutor::platform() == ...)` is a better branch, as it is a runtime branch and it allows both CUDA and ROCM logic to coexist - so that we don't recompile XLA for CUDA vs ROCM.", "@timshen91 Sounds good, will do\r\n\r\n-------\r\n\r\nI tried my best to make commit history non-confusing.\r\n\r\n- First commit: No code change, rename cudnn_conv_algorithm_picker to gpu_conv_algorithm_picker\r\n- Second commit: Refactor `Cuda` side implementation of `pickBestAlgorithmNoCache()`\r\n- Third commit: Refactor `ROCm` side implementation of `pickBestAlgorithmNoCache()`", "@timshen91 Addressed feedback in the newest commit. Thanks for your patience as this PR grows larger and larger. Brief explanation of the major changes:\r\n\r\n- Moved `tensorflow/stream_executor/cuda/redzone_allocator.h` up one level, to make the header shared across `Cuda` and `ROCm`.\r\n- Implement the boilerplate code for `ROCm` redzone_allocator.\r\n- Moved PtxCompilationOptions from `tensorflow/stream_executor/cuda/ptxas_utils.h` to a independent header `tensorflow/stream_executor/gpu_asm_opts.h` and make the header shared across `Cuda` and `ROCm`.\r\n- Manged to remove all `#if` macros from `gpu_conv_algorithm_picker`.\r\n- and corresponding naming/dependency changes.\r\n\r\n\r\n", "@timshen91 Appreciate the effort in reviewing this large PR. It looks like there are some CI failures. I will make sure to get those fixed in the new commit. I will ping you once I'm done with it.\r\n\r\n-----\r\n\r\nLooks like bazel is enforcing some new rules, i.e. It freaked out when the rule is written like\r\n`//tensorflow/core:platform/default/build_config_root.bzl`, and insisted to be written like `//tensorflow/core/platform:default/build_config_root.bzl`. Though I my local compilation, it complained on importing from `//tensorflow/core/platform` as well.\r\n\r\n-----\r\n\r\nThe second commit should be address all existing issues. Feel free to review ahead.", "@timshen91 Would you mind take a second look? Since this PR touches a wide range of files, it is running into conflict often, and requires frequent rebase. Is there a way to have it merged quickly? ", "> @timshen91 Would you mind take a second look? Since this PR touches a wide range of files, it is running into conflict often, and requires frequent rebase. Is there a way to have it merged quickly?\r\n\r\nI'm trying to get it checked in. In general / next time, you can split one PR into multiple ones just for easy check-in. For example, this PR has a lot of renamings and non-functional refactoring that can be separated out.", "@timshen91 Thank you! Will remember to split it next time. ", "Also, as this PR is growing larger, it's easier to split it into multiple PRs and check in separately. That way you don't have to rebase the whole PR all the time.", "@timshen91 Sounds good. Starting from next review, I will split it into two: One on XLA side gpu_conv_runner, another on stream executor side redzone_allocator. Will ping you when it is split into two.", "Can one of the admins verify this patch?", "Should this PR be closed if it's being split?", "Closing, since the PR was split. ", "As an update. Both phase 1 #31908 (merged) and phase 2 #32364 (for review) of this PR are submitted."]}, {"number": 30883, "title": "How to define a ReLU with TensorFlow custom_gradient?", "body": "I'm practicing using TensorFlow's custom_gradient decorator and I tried to define a simple ReLU. One would think it would be as simple as defining the gradient to be 1 when x > 0 and 0 otherwise. However, the following code does not yield the same gradients as a ReLU:\r\n\r\n```\r\n@tf.custom_gradient\r\ndef relu(x):\r\n    def grad(dy):\r\n        return tf.cond(tf.reshape(x, []) > 0,\r\n                       lambda: tf.cast(tf.reshape(1, dy.shape), tf.float32),\r\n                       lambda: tf.cast(tf.reshape(0, dy.shape), tf.float32))\r\n    return tf.nn.relu(x), grad\r\n```\r\n\r\nCan someone explain to me why this standard definition of ReLU's gradient does not yield the same performance as:\r\n\r\n```\r\n@tf.custom_gradient\r\ndef relu(x):\r\n    def grad(dy):\r\n        return dy\r\n    return tf.nn.relu(x), grad\r\n```", "comments": ["@slerman12 Please provide TensorFlow version.Thanks!\r\n", "1.12.0", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n", "Hi, no one could answer this question on StackOverflow. As far as I know, this is a bug \u2014 Why is ReLU\u2019s gradient not 1 when x > 0 and 0 otherwise? That\u2019s the standard definition. Please help, I need to be able to use this custom_gradient function for my research, but it\u2019s not working as expected in this simple example test case. "]}, {"number": 30882, "title": "Dense does not flatten inputs with rank >2 and behaves exactly like TimeDistributed(Dense)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6\r\n- TensorFlow installed from (source or binary): from pip install\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7 2.0.0-beta1\r\n- Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14\r\n\r\n**Describe the current behavior**\r\nA note in `Dense` [documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) says that \r\n> Note: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel.\r\n\r\nI don't see this happening in real life. Instead, `Dense` behaves on a 3-rank tensor as it would behave if it was wrapped in a `TimeDistributed` layer, making me question the utility of `TimeDistributed` at all.\r\n\r\n**Describe the expected behavior**\r\n`Dense` should flatten its input like the documentation says. In the first example bellow, the shape of the kernel weights of `dense` should be `(5 * 3, 2)` = `(15, 2)` instead of `(3, 2)`, which is the shape of `dense2` (as expected in the case of `dense2`).\r\n\r\n**Code to reproduce the issue**\r\n\r\nFirst example:\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\ntf.random.set_seed(12)\r\nnp.random.seed(12)\r\n\r\ninit = tf.keras.initializers.GlorotUniform(seed=12)\r\n\r\ninp = tf.constant(np.random.normal(0, 1, (1, 5, 6)))\r\ninp = tf.cast(inp, dtype=tf.float32)\r\n\r\ngru = tf.keras.layers.GRU(3, return_sequences=True)(inp)\r\nprint(gru.shape)\r\n#(1, 5, 3)\r\n\r\ndense = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init)\r\nprint(dense(gru))\r\n#tf.Tensor(\r\n#[[[ 1.5456871  -0.5280464 ]\r\n#  [ 0.11647969 -0.20553198]\r\n#  [ 0.58126366 -0.16031623]\r\n#  [-0.22882831 -0.22649539]\r\n#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)\r\n\r\nfor w in dense.weights:\r\n    print(w.shape)\r\n#(3, 2) instead of (5 * 3, 2) if Dense indeed flattened its input\r\n#(2,)\r\n\r\ntddense = tf.keras.layers.TimeDistributed(dense)\r\nprint(tddense(gru))\r\n#tf.Tensor(\r\n#[[[ 1.5456871  -0.5280464 ]\r\n#  [ 0.11647969 -0.20553198]\r\n#  [ 0.58126366 -0.16031623]\r\n#  [-0.22882831 -0.22649539]\r\n#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)\r\n# if Dense kernel had shape (15, 2), this should result in the following error:\r\n# InvalidArgumentError: Matrix size-incompatible: In[0]: [5,3], In[1]: [15,2] [Op:MatMul]\r\n# but instead what we get is the same output\r\n# than without TimeDistributed, without error\r\n\r\ndense2 = tf.keras.layers.Dense(2, kernel_initializer=init, bias_initializer=init)\r\ntddense = tf.keras.layers.TimeDistributed(dense2)\r\nprint(tddense(gru))\r\n#tf.Tensor(\r\n#[[[ 1.5456871  -0.5280464 ]\r\n#  [ 0.11647969 -0.20553198]\r\n#  [ 0.58126366 -0.16031623]\r\n#  [-0.22882831 -0.22649539]\r\n#  [ 0.62777793 -0.32470667]]], shape=(1, 5, 2), dtype=float32)\r\n\r\nfor w in dense2.weights:\r\n    print(w.shape)\r\n#(3, 2) as expected\r\n#(2,)\r\n```\r\n\r\nSecond example, with a rank even larger than 3:\r\n```\r\nimport tensorflow as tf\r\n\r\nprint('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))\r\n\r\ninp = tf.keras.Input(shape=(10, 25, 25, 3))\r\ndense_layer1 = tf.keras.layers.Dense(78)\r\nx = dense_layer1(inp)\r\nprint('Output shape without TimeDistributed:')\r\nprint(x.shape)\r\n\r\ndense_layer2 = tf.keras.layers.Dense(78)\r\ny=tf.keras.layers.TimeDistributed(dense_layer2)(inp)\r\nprint('Output shape with TimeDistributed:')\r\nprint(y.shape)\r\n\r\nprint('Weight shapes without TimeDistributed:')\r\nfor weight in dense_layer1.trainable_weights:\r\n    if len(weight.shape) == 2:\r\n        print('    kernel shape:')\r\n    else:\r\n        print('    bias shape:')\r\n    print(weight.shape)\r\n    \r\nprint('Weight shapes with TimeDistributed:')\r\nfor weight in dense_layer2.trainable_weights:\r\n    if len(weight.shape) == 2:\r\n        print('    kernel shape:')\r\n    else:\r\n        print('    bias shape:')\r\n    print(weight.shape)\r\n```\r\n\r\nwhich outputs is:\r\n```\r\nUsing Tensorflow version 2.0.0-beta1 (git version v2.0.0-beta0-16-g1d91213fe7)\r\nOutput shape without TimeDistributed:\r\n(None, 10, 25, 25, 78)\r\nOutput shape with TimeDistributed:\r\n(None, 10, 25, 25, 78)\r\nWeight shapes without TimeDistributed:\r\n    kernel shape:\r\n(3, 78)\r\n    bias shape:\r\n(78,)\r\nWeight shapes with TimeDistributed:\r\n    kernel shape:\r\n(3, 78)\r\n    bias shape:\r\n(78,)\r\n```\r\nWe see, in this example, that `Dense` and `TimeDistributed(Dense)` behave the same in that they only touch to the last dimension of the input.\r\n\r\n\r\n", "comments": ["Issue replicating with TF version-2.0.0beta1, please find the gist of [collab](https://colab.sandbox.google.com/gist/oanush/864dcb1fde8c517b6f7beacc5a968178/30882.ipynb). Thanks!", "@anush-o  It says access denied. Please change your settings to enable it.", "The issue is still here in tf2.0.0.", "The note is incorrect, we will update the note to reflect the code behavior.", "Commit https://github.com/tensorflow/tensorflow/commit/2e6a3c58e4b96cac864f244e4886ef00b3184986#diff-5fb1fa5fa46d0ec9a01d5a60b7d8acc8", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30882\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30882\">No</a>\n", "I appreciate how the new note is comprehensive @pavithrasv .\r\n\r\nBut now I don't get the purpose of `TimeDistributed` anymore, as I don't see the difference between `Dense` and `TimeDistributed(Dense)`.", "They are the same, `TimeDistributed`  doesn't just apply to Dense layers but i see that the main example in the TimeDistributed docs is using Dense layer. i'll update that.", "The change to `TimeDistributed` docs have also been submitted. Thank you!"]}, {"number": 30881, "title": "Inference on TFLite object detection model fails on run : \"(num_classes_with_background - num_classes <= 1) was not true\"", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, however greatly adapted from scripts\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Lenovo PB2-690M Android 6.0.1 API 23\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA toolkit 10.0.130 / cuDNN 7.3.1\r\n- GPU model and memory: Intel i7-6700 3.7GHz 16Go\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe script is in two parts :\r\nFirst few pictures are taken on phone to re-train a model with overfiting (ssd_resnet_50_fpn_coco) on the server, which produces a .tflite model, then sends it back to the device. \r\n\r\nProblem emerges when I try to run the inference with a newly taken image and the .tflite model as in the associated TFLite object detection example script : \r\nCaused by: java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/detection_postprocess.cc:688 (num_classes_with_background - num_classes <= 1) was not true.Node number 228 (TFLite_Detection_PostProcess) failed to invoke. \r\nat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:149)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)\r\n        at tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:164)\r\n\r\n**Describe the expected behavior**\r\nAs you can guess, it would be expected to run without failure. Note that using the .tflite model given by the TFLite object detection example script produces no error with the same picture (although it wouldn't yield any pertinent result). \r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nServer Python Part, used to convert the exported graph into .tflite file\r\n\r\n  ```\r\n    graph_def_file=os.path.join(BOX_PATH, \"tflite_graph.pb\")\r\n    input_arrays=[\"normalized_input_image_tensor\"]\r\n    output_arrays=['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']\r\n    input_tensor={\"normalized_input_image_tensor\":[1, 640, 640, 3]}\r\n    converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_tensor)\r\n    converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.allow_custom_ops = True;\r\n    tflite_quant_model = converter.convert()\r\n    open(os.path.join(BOX_PATH, \"frozen_inference_graph.tflite\", \"wb\").write(tflite_quant_model)\r\n\r\n```\r\nHere is the principal change in script for the inference run (Java code part):\r\n[TFLiteObjectDetectionAPIModel.txt](https://github.com/tensorflow/tensorflow/files/3411835/TFLiteObjectDetectionAPIModel.txt)\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nInference script taken from https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android\r\nModel tested with following script prior to .tflite conversion\r\nhttps://github.com/tiangolo/tensorflow-models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n\r\nHere it is, thanks for your reply and help !\r\n", "comments": ["Solution ?\r\nI removed that line\r\n `converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`\r\n used during the conversion of the .pb to the .tflite model graph file as the documentation showed this flag as experimental, and the error disappeared. \r\nAlthough the model now runs, it has really low precision (4%~) even when the inference is run on the training images (on which it has 99% accuracy since it is over-fitting) which indicates an error during conversion since the example models runs clearly with the code.\r\n\r\nHowever I am not sure of why adding target operations raised an issue, moreover since almost every code sample available seems to add this target_ops flag without any problem.\r\n\r\nAn Idea of why the problem could have emerged because of this ?\r\n", "Is there any progress on this?", "Well I was forced to not use a TFLite model since conversion broke it every time, so instead I used the frozen_graph.pb given by the export_inference_graph method, and the example application using a similar frozen graph to obtain a working app, but I still couldn't put my head around TFLite conversion.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30881\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30881\">No</a>\n"]}]