[{"number": 1450, "title": " GPU sync failed", "body": "```\n`I` tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:107] Allocating 3.51GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:118] GPU 0 memory begins at 0x503dc0000 extends to 0x5e45e3000\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1099] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\n```\n\nhi!\nwhat's wrong with this, how i can solve this.I'm using cuda7.5 cudnn7.0 and all they are ok running on CPU. but when run GPU ,it occur wrong.\nAnd I can local the operation which can't run on GPU\n\n```\nwith tf.device(\"/cpu:0\"):\n optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n```\n\nwhen i remvoe \"tf.device(\"/cpu:0\"),it ocure the bug reported above\n", "comments": ["It might be because GTX970 has some memory issues if you are allocating more than 3.5Gb (see http://wccftech.com/nvidia-geforce-gtx-970-memory-issue-fully-explained/), you can try to allocate less than 3.5gb memory and check if it corrects the issue:\n\n```\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n```\n", "Yikes.  Good to know @aymericdamien, thanks!\n", "it' make no sense.....\nwhen i change to this\n\n```\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n     sess.run(init)\n     step=1\n```\n\nit had the same error:\n\n```\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 970\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.1775\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.60GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512B\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:53] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:107] Allocating 2.52GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:118] GPU 0 memory begins at 0x503dc0000 extends to 0x5a54564cc\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1099] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\n```\n", "Are you building from source, or did you install the pip package?  What's your environment?  E.g., all the information the template had but you removed :).  If you built from source, what command line did you use? \n", "I build from source, and I build a whl, install it by pip. and i have test ok on  \n\n> https://github.com/Duum/TensorFlow-Examples/blob/master/examples/3%20-%20Neural%20Networks/convolutional_network.py\n\non GPU. but it's not ok work on my code.\nmy computer is ubuntu15.10 gcc version is 5.2.1 , my cuda is 7.5 ,and so i comment the error of gcc version error in cuda code\nmy cudnn verison is 7.0\n\n```\n sudo cp lib64/* /usr/local/cuda/lib64/\n sudo cp include/cudnn.h /usr/local/cuda/include/\n```\n\nmy command are:\n\n```\nPlease specify the location of python. [Default is /usr/bin/python]: \nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\nPlease specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify the Cudnn version you want to use. [Leave empty to use system default]: 7.0\nPlease specify the location where cuDNN 7.0 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size.\n[Default is: \"3.5,5.2\"]: \nSetting up Cuda include\nSetting up Cuda lib64\nSetting up Cuda bin\nSetting up Cuda nvvm\nConfiguration finished\n\n```\n\nAnd i live in china mainland, so I change some code in the WORKSPACE file:\n\n```\n\n git_repository(\n     name = \"grpc\",\n     commit = \"403cd6c\", init_submodules = True,\n    remote = \"https://github.com/melody-rain/grpc.git\",    )    \n```\n\nand i also  change .gitmudles file:\n\n```\n   [submodule \"google/protobuf\"]\n      path = google/protobuf\n       url = https://github.com/google/protobuf.git\n   [submodule \"third_party/boringssl\"]\n       path = third_party/boringssl\n      url = https://github.com/doubler/boringssl.git \n```\n\nmy build command are:\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\nbazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\n pip install /tmp/tensorflow_pkg/tensorflow-0.7.1-py2-none-linux_x86_64.whl\n\n\n```\n", "@zffchen78: Can you take a look?  Is there any relationship between this and  #2471?\n", "@vrv: Reassigning to you per @zffchen78's request.\n", "Pretty sure this is going to be hard for us to debug without being able to reproduce this.  \n\nI would suggest:\n1) Upgrading your nvidia drivers \n2a) Updating cuda to 7.5 and cudnn v4 and installing TensorFlow r0.9 or\n2b) Updating cuda to 8.0 and cudnn v5 and installing TensorFlow from sources\n\nand then try again.\n", "Automatically closing because there was no response. Please reopen if it is still an issue.\n", "I am getting the same error when I create a simple custom operator that operates on a list of input tensors of type int32. My input tensor is 5 elements, so this is clearly not a memory limitation issue. \n\nSpecifics:\nubuntu 14.04\nGeForce GTX TITAN driver version 367.44\ncuda 7.5, cudnn v4\nbinary pip install tensrflow gpu version 0.10.0rc0\npython 2.7\n\nBuild and run the attached source code:\n$ python cuda_op_unittest.py \nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 5.94GiB\nFree memory: 5.45GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\nI tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\n\nint32: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] int32: /job:localhost/replica:0/task:0/gpu:0\nint32/input_0: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] int32/input_0: /job:localhost/replica:0/task:0/gpu:0\n**\\* running on GPU ***\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)\n\nKey info:\nIf I run this with tf.device('/cpu:0') it works. \nIf I make my inputs and outputs a single tensor instead of a list of 1 tensor it also works. (This took a while to figure out!). ie instead of .Input(\"input: in_types\") in the REGISTER_OP use .Input(\"input: int32\")\n\nNotes: \nbased on the response to https://github.com/tensorflow/tensorflow/issues/4387, some research led me here: http://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op. It seems that tensorflow does not really support GPU operators on integer tensors and adding that support is difficult. In the interim though, better documentation on integer tensor support and a meaningful error message would be preferable to a core dump :). \n\n[issue1450.zip](https://github.com/tensorflow/tensorflow/files/475304/issue1450.zip)\n", "Does it work if you define the input type  as int64? \n", "I can't build a custom operator with type int64:\n\nkarenbre@karenZ820:~/workspace/issue1450$ ./build.sh\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nIn file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/type_traits.h:22:0,\n                 from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:25,\n                 from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:22,\n                 from cuda_op_kernel.cc:17:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/types.h: In instantiation of \u2018struct tensorflow::DataTypeToEnum<long int>\u2019:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:587:46:   required from \u2018typename tensorflow::TTypes<T, NDIMS>::ConstTensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) const [with T = long int; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const long int, 1, 1, long int>, 16>]\u2019\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:354:40:   required from \u2018typename tensorflow::TTypes<T>::ConstFlat tensorflow::Tensor::flat() const [with T = long int; typename tensorflow::TTypes<T>::ConstFlat = Eigen::TensorMap<Eigen::Tensor<const long int, 1, 1, long int>, 16>]\u2019\ncuda_op_kernel.cc:64:45:   required from \u2018void AddOneOp<Device>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice]\u2019\ncuda_op_kernel.cc:80:80:   required from here\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:136:3: error: static assertion failed: Specified Data Type not supported\n   static_assert(IsValidDataType<T>::value, \"Specified Data Type not supported\");\n   ^\nIn file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:23:0,\n                 from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:25,\n                 from cuda_op_kernel.cc:17:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h: In instantiation of \u2018typename tensorflow::TTypes<T, NDIMS>::ConstTensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) const [with T = long int; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const long int, 1, 1, long int>, 16>]\u2019:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:354:40:   required from \u2018typename tensorflow::TTypes<T>::ConstFlat tensorflow::Tensor::flat() const [with T = long int; typename tensorflow::TTypes<T>::ConstFlat = Eigen::TensorMap<Eigen::Tensor<const long int, 1, 1, long int>, 16>]\u2019\ncuda_op_kernel.cc:64:45:   required from \u2018void AddOneOp<Device>::Compute(tensorflow::OpKernelContext_) [with Device = Eigen::ThreadPoolDevice]\u2019\ncuda_op_kernel.cc:80:80:   required from here\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:587:46: error: \u2018v\u2019 is not a member of \u2018tensorflow::DataTypeToEnum<long int>\u2019\n   CheckTypeAndIsAligned(DataTypeToEnum<T>::v());\n                                              ^\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h: In instantiation of \u2018typename tensorflow::TTypes<T, NDIMS>::Tensor tensorflow::Tensor::shaped(tensorflow::gtl::ArraySlice<long long int>) [with T = long int; long unsigned int NDIMS = 1ul; typename tensorflow::TTypes<T, NDIMS>::Tensor = Eigen::TensorMap<Eigen::Tensor<long int, 1, 1, long int>, 16>]\u2019:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:284:40:   required from \u2018typename tensorflow::TTypes<T>::Flat tensorflow::Tensor::flat() [with T = long int; typename tensorflow::TTypes<T>::Flat = Eigen::TensorMap<Eigen::Tensor<long int, 1, 1, long int>, 16>]\u2019\ncuda_op_kernel.cc:70:57:   required from \u2018void AddOneOp<Device>::Compute(tensorflow::OpKernelContext*) [with Device = Eigen::ThreadPoolDevice]\u2019\ncuda_op_kernel.cc:80:80:   required from here\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:546:46: error: \u2018v\u2019 is not a member of \u2018tensorflow::DataTypeToEnum<long int>\u2019\n   CheckTypeAndIsAligned(DataTypeToEnum<T>::v());\n                                              ^\n", "Note, it does work with int16. \n", "I just met the same problem whether I installed tensorflow from source or official binary(the installation procedure was of no problems).\nGPU: gtx titan x(12G)\ncuda: 7.5 + cudnnv5.1\n`E tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available`\n`F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed`\n`Aborted (core dumped)`\n", "also saw this last night; after 2hrs running at 80% GPU util\n\n```\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\n```\n\nTITAN X (Pascal)\n\n```\n$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Sep 15 19:53 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 15 19:53 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 15 19:53 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Sep 15 19:53 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Sep 15 19:53 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 79337624 Sep 15 20:08 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 79337624 Sep 15 20:08 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 79337624 Sep 15 20:08 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n\n$ cd ~/dev/tensorflow/\n$ git rev-parse HEAD\n503a202761877250f1b268041a5bab14dad2b2ca\n\n$ bazel version\n.\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n```\n", "I am getting something similar during back-propagation. Bottleneck generation works fine.\n\n```\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)\n```\n", "@matpalm: Has it happened consistently since? These kind of one off failures can happen if there's some GPU hardware issues.  @yash0307 same question: does it happen immediately or only after a while?\n\n@kbrems can you include the int64 code? int64 should definitely compile, and I can't figure out the error from the compiler output alone.\n", "I've not seen it again & have been running similar jobs (i.e. in terms of GPU util & mem load) almost every night since\n", "Here is the example with int64. I just pulled the latest source from tensorflow master this morning and tried again and it still does not compile. \n[issue1450int64.zip](https://github.com/tensorflow/tensorflow/files/528293/issue1450int64.zip)\n", "Your in_types looks to be int16, not int64, not sure if this is the only problem though.  Other than that, this does seem like something we do all the time in other kernels, so I'm not sure why it's not compiling.\n", "My search/replace failed to catch that. I changed the in_types to int64, but it still does not compile. \n", "Even though we typedef int64 to int64_t, I think you need to use int64, not int64_t.  The following simpler code (which doesn't add one, but for illustration) compiled for me:\n\n```\nREGISTER_OP(\"AddOne\")\n    .Input(\"input: int64\")\n    .Output(\"output: int64\")\n    .Doc(R\"doc(\nAdds 1 to all elements of the tensor.\n\noutput: A Tensor.\n  output = input + 1\n)doc\");\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <typename Device>\nclass AddOneOp : public OpKernel {\n public:\n  explicit AddOneOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat<int64>();\n\n    // Create an output tensor\n    Tensor* output_tensor = NULL;\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\n                                                     &output_tensor));\n    auto output = output_tensor->template flat<int64>();\n    output = input;\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"AddOne\").Device(DEVICE_CPU), AddOneOp<CPUDevice>);\n```\n", "It seems that somewhere deep within Eigen, int64 is defined as a long long int, but on 64 bit ubuntu, int64_t is defined as a long int in stdint.h, so the 2 are not compatible.  I can work around that in this simple example, but it means that all our custom cuda kernels would then have to depend on Eigen types instead of the standard types for linux. . \n\nOn the plus side, my original issue with in32_t generating the GPU sync error and core dump seems to have gone away with release 0.11.0rc0 (built from latest source). Although I have also upgraded to CUDA 8.0 since the original problem, so perhaps that fixed something. \n", "I have a similar problem:\r\nCuda 8, cudnn v5.1 on a titan X\r\nusing keras with tensorflow-gpu==1.1.0\r\n\r\n```\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1067] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\n2017-06-19 17:12:19.722285: F tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\r\nAborted (core dumped)\r\n```\r\nIt occurs intermittently on training (usually after a few epochs)", "Same problem\r\n`2017-12-04 03:27:19.316336: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\nTraceback (most recent call last):\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"C:\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.InternalError: GPU sync failed`\r\nCode:\r\n```\r\n\r\noriginal =  tf.Variable(img, dtype = tf.float32)\r\nx =         tf.Variable(img, dtype = tf.float32)\r\nclique = x*x # tf.multiply(x,x) # tf.square(x,x)\r\n\r\noptimizer = tf.train.GradientDescentOptimizer(1e-2)\r\ntrain = optimizer.minimize(clique)\r\ninit = tf.global_variables_initializer()#tf.initialize_all_variables()\r\noptimize()\r\n```\r\n\r\nIt works perfectly on CPU (when cuda visible devices = -1). It is strange: when i use tf.add() it works on GPU, but tf.multiply(), tf.square() (not tested on another math functions) gives an error.\r\n\r\n**CUDA and CuDNN 8, Win10, 1050Ti, tensorflow 1.4 pip install.**", "I met same problem ,and I final solve it by decreasing the size of batch.It's so strange that I can run this program with bigger batch size before", "with tf.device(\"/cpu:0\"):\r\n optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n\r\nQ. What should be the learning_rate?", "I meet the same issue when I running codes with Keras with GPU. I solved it after release the memory. It is highly probably that you have no enough memory to be used. That  is also why someone said to reduce the batch size will also work. Good luck.  ", "I had the same problem. My batch-size is 64, and I changed it for 32. It had run."]}, {"number": 1449, "title": "Need better doc about the difference between name scope and variable scope in tensorflow and What's the context manager?(Soved))", "body": "Hi,\nAlthough stack overflow have a question about this, http://stackoverflow.com/questions/34215746/difference-between-variable-scope-and-name-scope-in-tensorflow. But right now the answers are not so clear and the doc also not tell the difference too, but I believe understanding the difference between name scope and variable scope and context manager is beneficial to programming for newbie using tensorflow.\n", "comments": ["https://gist.github.com/alexgorban/fa5755c7f7fa49b47961, This may be helpful for anyone confused by these functions.\n", "there're few [more questions](http://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow) regarding this issue - and it's not me asking :)\n\nand even after reading answers and @lfwin 's gist, I would rather not say that I understand the purpose of this scope diversity\n\ncould anyone give any practical recipe/code snippet illustrating those scopes' usage examples? (or just something like: \"most of the time if you think you need a scope, use tf.variable_scope()\")\n", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow.\r\n\r\nDocumentation continues to change and I hope improve.  I do not want to suggest your issue was solved without giving you a link.  ", "To understand `name_scope` and why it's useful, I suggest reading the \"Name scoping and nodes\" section on this page:\r\nhttps://www.tensorflow.org/get_started/graph_viz#name_scoping_and_nodes\r\n\r\nTo understand `variable_scope` and why it's useful, I suggest reading the \"Sharing Variables\" section on this page:\r\nhttps://www.tensorflow.org/programmers_guide/variables#sharing_variables\r\n\r\nI stumbled upon this issue after some Googling because:\r\n\r\n- I was reading the beginner tutorial, [Deep MNIST for Experts](https://www.tensorflow.org/get_started/mnist/pros), and in the beginning it said \"...you can download the fully implemented deep net from [mnist_deep.py](https://www.github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/mnist/mnist_deep.py)\"\r\n- [mnist_deep.py](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/mnist/mnist_deep.py) had lots of `tf.name_scope()` calls, but there was no mention of `name_scope` on the [tutorial itself](https://www.tensorflow.org/get_started/mnist/pros).\r\n\r\nI believe some beginners going through the same tutorial will find this a bit confusing."]}, {"number": 1448, "title": "bazel build failed ", "body": "Hi,\nI'm running on ubuntu 14.04 with cuda 7.0, cudnn4 in system. \nI followed the instruction on  https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#optional-install-cuda-gpus-on-linux. But when build with bazel (0.2.0), I always get:\n\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\nINFO: Found 1 target...\nERROR: missing input file '//third_party/gpus/cuda:include/thrust/detail/config/exec_check_disable.h':/home/wenchen/frameworks/tensorflow/third_party/gpus/cuda/include/thrust/detail/config/exec_check_disable.h (Permission denied).\nERROR: /home/wenchen/frameworks/tensorflow/tensorflow/stream_executor/BUILD:5:1: //tensorflow/stream_executor:stream_executor: missing input file '//third_party/gpus/cuda:include/thrust/detail/config/exec_check_disable.h'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nERROR: /home/wenchen/frameworks/tensorflow/tensorflow/stream_executor/BUILD:5:1 1 input file(s) do not exist.\nINFO: Elapsed time: 0.814s, Critical Path: 0.00s\n\nplease help. \n\nThanks,\nWenchen\n", "comments": ["The permission denied bit might mean that the permissions for that file are weird.\n", "ok, can you define weird?\n", "Unfortunately no, it's a guess without knowledge of details.  What happens if you do `ls -ld` on that file and all the directories above it?\n", "I figured it out thanks to `ls -ld`, my exec_check_disable.h denies access for others.\nJust `sudo chmod o=r /usr/local/cuda/include/thrust/detail/config/exec_check_disable.h` and it builds.\n\nThanks for pointing out girving.\n"]}, {"number": 1447, "title": "ci_build: fixes", "body": "- do not require $USER variable\n- do not contact github in print_build_info.sh\n  - there is a easier and faster way to get the clone url\n", "comments": ["Merged.\n"]}, {"number": 1446, "title": "Tensorflow is using the wrong cuda toolkit", "body": "I recently installed tensorflow on an Ubuntu 12.04 machine that has both cuda 6.5 and 7.0 installed. However, when I import tensorflow I get the following error:\n\n```\n    import tensorflow as tf\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\n    from tensorflow import contrib\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/__init__.py\", line 23, in <module>\n    from tensorflow.contrib import layers\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/__init__.py\", line 68, in <module>\n    from tensorflow.contrib.layers.python.layers import *\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/python/layers/__init__.py\", line 22, in <module>\n    from tensorflow.contrib.layers.python.layers.initializers import *\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/contrib/layers/python/layers/initializers.py\", line 24, in <module>\n    from tensorflow.python.ops import random_ops\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/random_ops.py\", line 23, in <module>\n    from tensorflow.python.framework import ops\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/ops.py\", line 39, in <module>\n    from tensorflow.python.framework import versions\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/framework/versions.py\", line 22, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n  File \"/usr/lib/python3.4/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\nImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory\n```\n\nAm I doing something wrong? Can I use cuda 6.5 or 7.0?\n\nEdit: installation was via pip(3).\n", "comments": ["Yes, you have to build from sources and you can specify the toolkits you want to use via ./configure\n", "So the pip installation only supports cuda 7.5?\n", "Yes, I believe so.\n", "Perhaps that should be documented somewhere?\n", "It is documented here: https://github.com/tensorflow/tensorflow/releases/tag/v0.7.1\nThe official website is not updated, though.\n", "Why cuda version is hard-coded?\n", "Closing since the pip packages are built against specific CUDA versions.  @vrv: We can reopen if we have plans to make pip packages that work everywhere, but I'm not sure if that's practical.\n"]}, {"number": 1445, "title": "Adding skflow to tensorflow/contrib", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "Hi @terrytangyuan @makseq @dansbecker @elqursh @dgboy2000 @cbonnett @ivallesp @frol @liyongsea,\n\nWe decided to move skflow into tensorflow - for better convergence of APIs and generally better support.\nCan you please comment in this thread with \"I signed it! message for CLA to propagate for you to tensorflow project (this will make you contributor of tensorflow and keep your stats).\n\nThanks!\n", "I signed it!\n", "I signed it!\n", "I signed it!\n", "I signed it!\n", "I signed it!\n\nOn Wed, Mar 9, 2016, 6:42 PM Illia Polosukhin notifications@github.com\nwrote:\n\n> Hi @terrytangyuan https://github.com/terrytangyuan @makseq\n> https://github.com/makseq @dansbecker https://github.com/dansbecker\n> @elqursh https://github.com/elqursh @dgboy2000\n> https://github.com/dgboy2000 @cbonnett https://github.com/cbonnett\n> @ivallesp https://github.com/ivallesp @frol https://github.com/frol\n> @liyongsea https://github.com/liyongsea,\n> \n> We decided to move skflow into tensorflow - for better convergence of APIs\n> and generally better support.\n> Can you please comment in this thread with \"I signed it! message for CLA\n> to propagate for you to tensorflow project (this will make you contributor\n> of tensorflow and keep your stats).\n> \n> Thanks!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1445#issuecomment-194569028\n> .\n", "I signed it!\n\nOn Wed, Mar 9, 2016 at 4:43 PM, elqursh notifications@github.com wrote:\n\n> I signed it!\n> \n> On Wed, Mar 9, 2016, 6:42 PM Illia Polosukhin notifications@github.com\n> wrote:\n> \n> > Hi @terrytangyuan https://github.com/terrytangyuan @makseq\n> > https://github.com/makseq @dansbecker https://github.com/dansbecker\n> > @elqursh https://github.com/elqursh @dgboy2000\n> > https://github.com/dgboy2000 @cbonnett https://github.com/cbonnett\n> > @ivallesp https://github.com/ivallesp @frol https://github.com/frol\n> > @liyongsea https://github.com/liyongsea,\n> > \n> > We decided to move skflow into tensorflow - for better convergence of\n> > APIs\n> > and generally better support.\n> > Can you please comment in this thread with \"I signed it! message for CLA\n> > to propagate for you to tensorflow project (this will make you\n> > contributor\n> > of tensorflow and keep your stats).\n> > \n> > Thanks!\n> > \n> > \u2014\n> > Reply to this email directly or view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/pull/1445#issuecomment-194569028>\n> > .\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1445#issuecomment-194590287\n> .\n", "I signed it!\n", "I signed it!\n", "Jenkins, test this please\n", "Jenkins, test this please\n", "Jenkins, test this please\n", "Jenkins, test this please\n", "Sorry about the change in suggestions, but PR 1472, which is already merged after discussion, ended up including all the deb and pip packages required for the builds and tests here. As such, please remove all changes to the following files: \ntensorflow/tools/ci_build/Dockerfile.cpu\ntensorflow/tools/ci_build/Dockerfile.gpu\ntensorflow/tools/ci_build/install/install_deb_packages.sh\ntensorflow/tools/ci_build/builds/test_installation.sh\ntensorflow/tools/ci_build/builds/pip.sh\nThanks.\n", "@ilblackdragon How about the `\"cla: no\"` thing? Can we still merge it which actually makes sense since they signed `cla` from skflow already? \n", "Everybody has signed the Google CLA when contributing to skflow.\n"]}, {"number": 1444, "title": "SummaryWriter writes constants with the graph definition", "body": "It looks like SummaryWriter includes the values of constants when writing the graph definition. I'm currently loading word embeddings using python and using the numpy array to initialize a variable. This results in the ~300M of embeddings to be dumped to disk during summarization.  I understand why a constant would be part of the model graph if the goal was to save and restore the graph, but I don't believe this is the goal of the SummaryWriter. Is it possible to strip these out to save disk space?\n\nHere's a trivial example. If the declaration of `random_b` is removed, the resulting summary is ~4.6K, but with it, it's about 7.6M.\n\n```\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nsession = tf.Session()\n\nlogdir = \"/tmp/tflogs\"\n\nrandom_a = tf.Variable(tf.random_normal([1000000]))\nrandom_b = tf.Variable(np.random.rand(1000000))\ntf.histogram_summary(\"random_var\", random_a)\nos.makedirs(logdir)\nwriter = tf.train.SummaryWriter(logdir, session.graph_def)\ninit = tf.initialize_all_variables()\nmerged_summary_op = tf.merge_all_summaries()\nsession.run(init)\n\nsummary = session.run(merged_summary_op)\nwriter.add_summary(summary, 0)\n```\n", "comments": ["I'd also like to point out that this makes it impossible to visualize the graph in tensorboard, since it just hangs parsing the constants in the graph def.\n", "Constants are part of the `GraphDef`, so unfortunately this is intended behavior.  To keep them out of the `GraphDef`, change from\n\n```\nrandom_a = tf.Variable(tf.random_normal([1000000]))\n```\n\nto\n\n```\nrandom_a = tf.Variable(shape=[1000000])\nrandom_a.assign(np.random.randn(*random_a.shape)).eval()\n```\n\nUg, actually that'll implicitly create a constant too.  @petewarden: What's the right solution here?\n", "The same question came up on StackOverflow today: http://stackoverflow.com/a/35904439/3574081\n\n(FWIW: `random_a = tf.Variable(tf.random_normal([1000000]))` won't add a large constant to the graph, but `random_a = tf.Variable(np.random.normal(size=[1000000]))` will.)\n", "To answer what I think is the original question, it's definitely possible to strip out Constant ops from a graph def. We do the opposite in the freeze_graph script, so we could create a 'strip_graph' script or similar to slim down files by replacing large constants with something smaller (though we'd have to do some resize tricks to do it properly).\n\nI think the bigger question is 'Why are there these big weights in my graph def file?'. We're working on answering that here by looking at different ways that we can save external weights, without using the Variable/Restore checkpoints since those are fairly specialized for training. We don't have a good design for that yet though.\n\nDoes that help at all? \n", "@mrry: That solves the code shown, but I believe the original code loads a big numpy array from a file.\n", "@mrry's suggestion works perfectly as a work-around for my large numpy array (I'm responsible for the stackoverflow question as well) and I'm able to use Tensorboard's graph visualization again.\n\n@petewarden - FWIW, I never use the Saver to persist the entire session. I'm always cherrypicking variables so that I can deploy just the model to other machines for inference or train on a different dataset. So any help with managing training state vs. learned model state would be great.\n", "@vgatto  I'd also like to point out that this makes it impossible to visualize the graph in tensorboard, since it just hangs parsing the constants in the graph def. Can you explain why and how can I display the graph? \n", "Closing this as intended behavior.  @toma5692 and others: If you want to keep graphs small, use `tf.Variable` instead of `tf.constant`.\n"]}, {"number": 1443, "title": "Hard-coded gcc path", "body": "When compiling with GPU support, the compiler path is set in the file\n//third_party/gpus/crosstool/crosstool_wrapper_is_not_gcc on line 47 to 50:\n\n```\nCPU_COMPILER = ('/usr/bin/gcc')\netc\n```\n\nwhich can result in using an old version on e.g. clusters where several compilers might be installed under a path such as\n`/appl/gcc/5.3.0/bin/gcc`\n\nIt would be nice to be able to configure the compiler path.\n\nEdit: This overlaps with the hard coded paths in other parts of Bazel, see for example bazelbuild/bazel#760\n", "comments": ["It'd be good to have this as part of `configure`, but I don't know how awkward it is.  @ebrevdo: Care to comment?\n", "https://github.com/tensorflow/tensorflow/pull/1220 might be relevant\n", "@damienmg any thoughts?\n", "I agree, for now it should probably be part of the ./configure script. With next Bazel realease the new auto-configuration mechanism could replace it.\n", "+1 if we can configure via bazelrc.  Alternatively it would be great if we could set up a symlink or something similar to the way we do python\n", "So what's the status of this.  Should we accept #1220 since that's going the ./configure route, or wait for bazel auto config?\n", "I think you should accept #1220, bazel auto config is going to be in next bazel release but you probably don't want to stick to the latest bazel (and also have it a bit more tested before it goes live).\n", "@vrv: This is fixed by #1220, right?\n", "Yes.\n"]}, {"number": 1442, "title": "Same model behaves different when restored in Linux vs OSX", "body": "Using the recent TensorFlow pip install 0.7.0 I have encountered some frustrating behavior. It might be something I'm doing wrong on my end, but I've ruled out everything I can.\n\nHere's what's happening:\nI've trained a model on a linux system using a GPU. I then scp the checkpoint file to my mac, and restored the checkpoint file into exactly the same code-base. However, the results were much poorer. Thinking I might have a bug or a bad saved model, I went back to the linux and loaded the saved model there, reran the exact same evaluation procedure using identical data, code, etc. and the results were much better once again. I don't know anything about the purpose of the ckpt.meta file, but copying this into the same directory didn't help.\n\nDoes this sound familiar at all? Again, I can't rule out a bug on my end, but everything is identical as far as I can tell, apart from the running environment (Linux+GPU vs Mac). Any ideas?\n", "comments": ["It's impossible for us to debug this without more information.  First, though, a wild guess: what happens if you restore and test on Linux but using only CPU?  That might at least isolate it to a GPU bug.  If it does turn out to be CPU vs. GPU, can you isolate which op is problematic?\n", "I did try disabling the GPU and that didn't seem to help. Good suggestion on trying to isolate which op... let me spend some time doing that and I'll get back to you.\n", "OK, I think I've worked it out. I still need to do a few more tests to be sure, but it looks like it is actually another 3rd-party package I'm using that is behaving differently on the two systems. Closing this issue since the problem is in another package and Tensorflow is working great.\n", "To satisfy my curiosity: what was the other package?\n", "An organic chemistry toolkit called Indigo :)\nhttp://lifescience.opensource.epam.com/indigo/\n", "Hi, I have the same problem with loading trained model on GPU host and loaded on CPU host (both linux Ubuntu 14.04 LTS). There is example:\non GPU - Model accuracy: 0.79\non CPU - Model accuracy: 0.16\nThe code and model definition is same in Python.\nUnfortunately on both machines Indigo package not installed... and model loads on both host without errors.\n", "@shoc2005: As with the other issue, we're going to need more information.  What version of tensorflow are you using?  Can you localize the differences to a particular op?\n", "@girving: some minutes ago I achieved the same result as in GPU on CPU host - no difference. But now I retraining my model on GPU host with last code modifications. I think this problem may be caused by pyc (python file) files...\n"]}, {"number": 1441, "title": "valueerror when install 0.7.1 in centos 7", "body": "[martin@localhost ~]$ sudo pip install ./tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n[sudo] password for martin: \nException:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/pip/basecommand.py\", line 139, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/site-packages/pip/commands/install.py\", line 235, in run\n    InstallRequirement.from_line(name, None))\n  File \"/usr/lib/python2.7/site-packages/pip/req.py\", line 118, in from_line\n    return cls(req, comes_from, url=url)\n  File \"/usr/lib/python2.7/site-packages/pip/req.py\", line 43, in **init**\n    req = pkg_resources.Requirement.parse(req)\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 2914, in parse\n    reqs = list(parse_requirements(s))\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 2839, in parse_requirements\n    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),\"version spec\")\n  File \"/usr/lib/python2.7/site-packages/pkg_resources.py\", line 2807, in scan_list\n    raise ValueError(\"Expected \"+item_name+\" in\",line,\"at\",line[p:])\nValueError: ('Expected version spec in', './tensorflow-0.7.1-cp27-none-linux_x86_64.whl', 'at', '/tensorflow-0.7.1-cp27-none-linux_x86_64.whl')\n\nStoring complete log in /root/.pip/pip.log\n[martin@localhost ~]$ \n[martin@localhost ~]$ python -V\nPython 2.7.5\n[martin@localhost ~]$ uname -a\nLinux localhost.localdomain 3.10.0-327.10.1.el7.x86_64 #1 SMP Tue Feb 16 17:03:50 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n", "comments": ["@martinwicke: Looks like the wonderful world of wheel naming conventions? \n", "Can you try with the 0.9 wheel? I have no idea what the problem can be and the message is useless, but maybe we get lucky.\n", "Closing due to lack of activity. Please re-open if this is still a problem.\n"]}, {"number": 1440, "title": "Cuda 3.0 not working", "body": "When installing from pip tensorflow GPU version (Linux), minimum cuda version is 3.5.\nInstall instructions have been changed to:\n\"TensorFlow GPU support requires having a GPU card with NVidia Compute Capability >= 3.0\"\nHowever, gpu_device.cc is still requiring cuda 3.5:\n\n```\n// \"configure\" uses the specific name to substitute the following string.\n// If you change it, make sure you modify \"configure\" as well.\nstd::vector<CudaVersion> supported_cuda_compute_capabilities = {\n    CudaVersion(\"3.5\"), CudaVersion(\"5.2\")};\n\n}  // namespace\n```\n", "comments": ["@zheng-xq: Should this be changed to 3.0?\n", "@alex-pardo did you `./configure` with `TF_UNOFFICIAL_SETTINGS=1` and specified `3.0` at the cuda compute capability step ?\n"]}, {"number": 1439, "title": "why tensorflow run slow in loop", "body": "I just run many times of sess.run in loop like this:\n\n```\ninput1 = tf.placeholder(tf.float32)\ninput2 = tf.placeholder(tf.float32)\nsess = tf.Session()\nfor image_num in range(128):\n  image_result_table=np.zeros((96,32),dtype=np.float32)\n  for sub_num in range(96):\n      for centroids_num in range(32):\n       vector_a = local3_value[image_num,sub_num*4:(sub_num+1)*4]\n       vector_b = centroids[sub_num,centroids_num]\n       mat_result = tf.mul(input1, input2)\n           reduce_sum = tf.reduce_sum(mat_result)\n       result =sess.run(reduce_sum,feed_dict={input1:vector_a, input2:vector_b})\n           print(result)\n```\n\nat first ,it run fast. howerver, it become slower and slower. It takes almost an hour to get the whole result. \n", "comments": ["For future issues, please use Markdown to format the code.  For this one, I edited it to indent by four spaces to preserve code formatting.\n", "Your code snippet is incomplete, since you haven't shown us the definition of `local3_value` and `centroids`.  There are also some strange bits, such as allocating `image_result_table` but never using it.\n", "@girving: the problem here is that the program is constantly adding more nodes to the graph because it is mixing graph construction with graph execution.\n", "Ah, right, forgot about our quadratic time behavior.  @MisayaZ: Move the `mat_result` and `reduce_sum` definitions outside the loops and you should be fine. \n", "Thanks\n", "Hi @girving , when I run this code, the result shows that it just need 0.0273 second in the first epoch while 0.1528 second in the 200th epoch, and I don't know what is wrong with my code\n\n```\nimport tensorflow as tf\nimport numpy as np\nimport time\n\ndelta = np.random.uniform(size=(500, 500))\nweight = tf.Variable(tf.truncated_normal([500, 500]))\n\nx = tf.placeholder('float')\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\nfor epoch in range(200):\n    t = time.time()\n    for batch in range(10):\n        update = weight.assign_add(x)\n        sess.run(update, feed_dict={x: delta})\n    print 'epoch %s done, time used %s' % (epoch, time.time()-t)\n```\n\nSorry for my bad english :D\n", "@hit-suit   \"the program is constantly adding more nodes to the graph\". I think this is the problem.\n", "@MisayaZ @girving I have similar problem, the network is running slower and slower, and take up all the memory of my CPU, not GPU. So I was wondering if there is some convenient tool to check the graph, so I can know which nodes I am constantly adding to the graph? Thanks.\n", "Hi Tensorflowers!\r\n\r\nI am experiencing the same slow down issue by looping over sessions. In each loop I attempt to set a different seed for kernel initializer in `tf.layers.dense` , and I believe that \"the program is constantly adding more nodes to the graph\" causes this problem. In the following snipped I do my best to show how my program is structured. I set a global seed, and then generate a set of seeds in order to initialize my kernels differently in each run (or chain). The program is a feed forward neural net with two hidden layers with relu on the hidden layer units, and mean squared error as the cost function. \r\n\r\nThe reason I am doing this is because I want to have independent trainings, and reproducible results. I was wondering if there is any way around to set the seed for the kernel initilizers without creating multiple nodes? At this point, I really appreciate any form of comments or help.\r\n\r\n\r\n**UPDATE**: I figured I could reset the graph by adding `tf.reset_default_graph()` after sess.close()\r\n```\r\nglobal_seed = 1234\r\nnp.random.seed(global_seed)\r\nseeds = np.random.randint(0, 4294967295, size=nchain)\r\nfor ii in range(nchain): # loop over different runs/chains\r\n            tf.set_random_seed(seeds[ii]) # set the seed\r\n            # set up the model x [input] -> y [output] with two hidden layers\r\n            x   = tf.placeholder(tf.float32, [None, nfeature])\r\n            # nfeature, Units, scale are number of features, number of units on each hidden layer \r\n            # and regularization scale, respectively\r\n            kernel_init0 = tf.random_normal_initializer(stddev=np.sqrt(1./(nfeature+1)), seed=seeds[ii])\r\n            kernel_init1 = tf.random_normal_initializer(stddev=np.sqrt(2./(Units[0]+1)), seed=seeds[ii]) \r\n            kernel_init  = tf.random_normal_initializer(stddev=np.sqrt(2./(Units[1]+1)), seed=seeds[ii]) \r\n            y0 = tf.layers.dense(x,  units=Units[0], activation=tf.nn.relu, kernel_initializer=kernel_init0,\r\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\r\n            y1 = tf.layers.dense(y0, units=Units[1], activation=tf.nn.relu, kernel_initializer=kernel_init1,\r\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\r\n            y  = tf.layers.dense(y1, units=1,   activation=None,  kernel_initializer=kernel_init,\r\n                                    kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=scale))\r\n            # placeholders for the input errorbar and label\r\n            y_  = tf.placeholder(tf.float32, [None, 1])\r\n            w   = tf.placeholder(tf.float32, [None, 1])\r\n\r\n            #\r\n            # objective function plus regularization term\r\n            mse = tf.losses.mean_squared_error(y_, y, weights=w)\r\n            l2_loss = tf.losses.get_regularization_loss()\r\n            mse += l2_loss\r\n            #\r\n            global_step = tf.Variable(0, name='global_step', trainable=False)\r\n            optimizer   = tf.train.AdamOptimizer(learning_rate)\r\n            train_step  = optimizer.minimize(mse, global_step=global_step)            \r\n            # \r\n            # initialize the NN\r\n            sess = tf.InteractiveSession()\r\n            tf.global_variables_initializer().run()            \r\n            for epoch in range(number_of_epoch): # loop on training epochs\r\n                # do training ...\r\n            # evaluate something on test data    \r\n            sess.close()\r\n            # UPDATE July 22, 2018\r\n            tf.reset_default_graph()\r\n```"]}, {"number": 1438, "title": "Tensorflow missing symbol in compilation", "body": "Running on ArchLinux and compiling with the 2 suggested changes in [Issue 1346](https://github.com/tensorflow/tensorflow/issues/1346) -  GCC4.9 and -D_FORCE_INLINES.\n\n_Env_\nGCC 4.9.3\nBazel version states \"from-head\" - build taken from https://github.com/bazelbuild/bazel/archive/0.2.0.tar.gz\nBuilding with Cuda and cudnn\n\nI'm running into an issue where the linking step for the tutorials_example_trainer, where the CheckOpMessageBuilder::NewString() symbol is missing:\n\n```\nERROR: /mnt/work/work/tensorflow/tensorflow/cc/BUILD:61:1: Linking of rule '//tensorflow/cc:tutorials_example_trainer' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer ... (remaining 627 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*)':\ntmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIiiEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIiiEEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<int, unsigned long>(int const&, unsigned long const&, char const*)':\ntmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIimEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIimEEPSsRKT_RKT0_PKc]+0x41): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<tensorflow::DataType, tensorflow::DataType>(tensorflow::DataType const&, tensorflow::DataType const&, char const*)':\ntmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringINS_8DataTypeES2_EEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringINS_8DataTypeES2_EEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<long long, int>(long long const&, int const&, char const*)':\ntmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIxiEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIxiEEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\n```\n\nI have followed the tutorial thus far and am unsure why\n", "comments": []}, {"number": 1437, "title": "Tensorflow missing symbol in compilation", "body": "Running on ArchLinux and compiling with the 2 suggested changes in [Issue 1346](https://github.com/tensorflow/tensorflow/issues/1346) -  GCC4.9 and -D_FORCE_INLINES.\n\n_Env_\nGCC 4.9.3\nBazel version states \"from-head\" - build taken from https://github.com/bazelbuild/bazel/archive/0.2.0.tar.gz\nBuilding with Cuda and cudnn\n\nI'm running into an issue where the linking step for the tutorials_example_trainer, where the CheckOpMessageBuilder::NewString() symbol is missing:\n\n```\nERROR: /mnt/work/work/tensorflow/tensorflow/cc/BUILD:61:1: Linking of rule '//tensorflow/cc:tutorials_example_trainer' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/local_linux-opt/bin/tensorflow/cc/tutorials_example_trainer ... (remaining 627 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nbazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*)':\ntmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIiiEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIiiEEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<int, unsigned long>(int const&, unsigned long const&, char const*)':\ntmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIimEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIimEEPSsRKT_RKT0_PKc]+0x41): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<tensorflow::DataType, tensorflow::DataType>(tensorflow::DataType const&, tensorflow::DataType const&, char const*)':\ntmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringINS_8DataTypeES2_EEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringINS_8DataTypeES2_EEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\nbazel-out/local_linux-opt/bin/tensorflow/core/kernels/libtranspose_functor_gpu.lo(transpose_functor_gpu.cu.o): In function `std::string* tensorflow::internal::MakeCheckOpString<long long, int>(long long const&, int const&, char const*)':\ntmpxft_000060ce_00000000-7_transpose_functor_gpu.cu.compute_35.cudafe1.cpp:(.text._ZN10tensorflow8internal17MakeCheckOpStringIxiEEPSsRKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIxiEEPSsRKT_RKT0_PKc]+0x40): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString()'\n```\n\nI have followed the tutorial thus far and am unsure why\n", "comments": ["@keveman: Do you any ideas here?  Feel free to comment and reassign.\n", "No ideas. Passing the baton to @zheng-xq \n", "Okay, got it compiled.\n\nFor anyone following along from Arch Linux and wanting to compile this the following diff should help:\n\n```\ndiff --git a/third_party/gpus/crosstool/CROSSTOOL b/third_party/gpus/crosstool/CROSSTOOL\nindex dfde7cd..f84955b 100644\n--- a/third_party/gpus/crosstool/CROSSTOOL\n+++ b/third_party/gpus/crosstool/CROSSTOOL\n@@ -45,6 +45,7 @@ toolchain {\n   tool_path { name: \"gcc\" path: \"clang/bin/crosstool_wrapper_driver_is_not_gcc\" }\n   # Use \"-std=c++11\" for nvcc. For consistency, force both the host compiler\n   # and the device compiler to use \"-std=c++11\".\n+  cxx_flag: \"-D_FORCE_INLINES\"\n   cxx_flag: \"-std=c++11\"\n   linker_flag: \"-lstdc++\"\n   linker_flag: \"-B/usr/bin/\"\ndiff --git a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\nindex a67b039..f284677 100755\n--- a/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n+++ b/third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n@@ -44,10 +44,11 @@ import sys\n import pipes\n\n CURRENT_DIR = os.path.dirname(sys.argv[0])\n-CPU_COMPILER = ('/usr/bin/gcc')\n+CPU_COMPILER = ('/usr/bin/gcc-4.9')\n NVCC_PATH = CURRENT_DIR + '/../../../cuda/bin/nvcc'\n-GCC_HOST_COMPILER_PATH = ('/usr/bin/gcc')\n-LLVM_HOST_COMPILER_PATH = ('/usr/bin/gcc')\n+#GCC_HOST_COMPILER_PATH = ('/usr/bin/gcc')\n+GCC_HOST_COMPILER_PATH = ('/usr/bin/gcc-4.9')\n+LLVM_HOST_COMPILER_PATH = ('/usr/bin/gcc-4.9')\n PREFIX_DIR = os.path.dirname(GCC_HOST_COMPILER_PATH)\n```\n", "To clarify: was the `LLVM_HOST_COMPILER_PATH` bit the part that fixed the `NewString` error?\n", "Yes, suspect it was either `LLVM_HOST_COMPILER_PATH` or `CPU_COMPILER` as I changed both.\n"]}, {"number": 1436, "title": "Open vocabulary file in binary mode", "body": "The seq2seq tutorial breaks for python3 as GFile does not specify encoding. This fix explicitly opens the file in binary for both python 3 and python 2.7\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Looks good!  Can you sign the contributor agreement (see above)?\n", "Jenkins, test this please.\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Jenkins, test this please.\n", "@martinwicke: Did I invoke Jenkins correctly?  I don't see anything happening, but maybe that's just a lack of patience?\n", "Hmm... Jenkins, test this please?\n", "Yours seemed to work better.\n", "Yup. There was a problem. Fixed it. Should work for you as well now.\n", "@OlavHN: Can you rebase this on top of master to get rid of the merge commit?  I'll merge after that.  Thank you for the fix! \n", "@girving After training the model and attempting to do translations I had the same issues. Added a patch that solved my issues.\n\nI added it to the same PR as it's related to the same issue.\n", "Jenkins, test this please.\n", "@tensorflow-jenkins: Test this please\n", "Looks good -- just one comment and then can you squash?  We'll merge right after.\n", "@tensorflow-jenkins: test this please\n"]}, {"number": 1435, "title": "Fixing Broken Link", "body": "Fixed broked links\n", "comments": ["Can one of the admins verify this patch?\n", "I fixed C++ documentation links\n", "@martinwicke: Are these files autogenerated? \n", "@girving @martinwicke I think those files are autogenerated and we both made changes which should have fixed the links.  Do you need to run something internally and push to production?  This also references the comment @girving made on #1303 \n", "I need to redeploy the website. Otherwise this should be fixed.\n"]}, {"number": 1434, "title": "tf.nn.xw_plus_b not in API docs ", "body": "The method tensorflow.nn.xw_plus_b() exists but is not documented in the [online API documentation](https://www.tensorflow.org/versions/master/api_docs/python/nn.html). \n", "comments": ["I think this is intentional: it's better to use `tf.matmul(x, w) + b` directly.  We are working to lock down our symbol exports so that only documented symbols are exported, but we're a bit lax at the moment.\n", "`tf.nn.xw_plus_b()`  has option to name the operation while`tf.matmul(x, w) + b` has not. Is there any way around that?\n", "@dParadiz you can just use tf.add(tf.matmul(x, w), b, name='<name>')"]}, {"number": 1433, "title": "reduce mean for gpu is not registered", "body": "The reduce_mean [GPU implementations](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_gpu.cu.cc#L44-L60) are not registered [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_mean.cc), so that the reduce_mean is still placed on CPU.\n\nWondering if it's a negligence or intended. If it's intended, please close this.\n", "comments": ["\"accidental\" may be a better word than \"negligence\". :)\n\n@benoitsteiner: Want to take a look at this?\n", "That was indeed a mistake on my part. Somehow I didn't checkin the kernel registration part. We should merge pull request #1457 that fixes my oversight. \n"]}, {"number": 1432, "title": "seq2seq tutorial example not working for python 3.4", "body": "### Environment info\n\nOperating System:\nUbuntu 14.04\n1. Which pip package you installed.\n   sudo pip3 install --upgrade /tmp/pip/tensorflow-*.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.7.1\n   (Built from commit 263d00d271077 with `TF_UNOFFICIAL_SETTING=1 ./configure` )\n### Steps to reproduce\n1. Apply https://github.com/tensorflow/tensorflow/commit/cdd0e2b7c542a59322c054aa1a52b2753c1cf69e \n2. Run `python3 ../tensorflow/tensorflow/models/rnn/translate/translate.py --data_dir .`\n3. Fail with trying to decode training data as ascii (see log at the end)\n### What have you tried?\n\nModified https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/default/_gfile.py#L63 to `self._fp = open(name, mode, encoding=\"latin-1\")`\n\nThis seems to fix this issue for python 3.4.\n### Logs or other output that would be helpful\n\nCreating vocabulary ./vocab40000.fr from data ./giga-fren.release2.fr\nTraceback (most recent call last):\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 276, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 273, in main\n    train()\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 137, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 268, in prepare_wmt_data\n    create_vocabulary(fr_vocab_path, train_path + \".fr\", fr_vocabulary_size)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 136, in create_vocabulary\n    for line in f:\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_gfile.py\", line 176, in __next__\n    return next(self._fp)\n  File \"/usr/lib/python3.4/encodings/ascii.py\", line 26, in decode\n    return codecs.ascii_decode(input, self.errors)[0]\n", "comments": ["Can you confirm that you're using python 3?  You say `pip3` and `python` above; is just `python` Python 3?  If so, what version?\n", "Sorry for being unclear. The command was left over from the github issue template.\n\n`python3 --version`\nPython 3.4.3\n`python3 -c \"import tensorflow; print(tensorflow.__version__)\"`\n0.7.1\n`python --version`\nPython 2.7.6\n`python -c \"import tensorflow; print(tensorflow.__version__)\"`\nImportError: No module named tensorflow\n", "Next question: what exception are you seeing?  It may just be my eyes glazing over, but it looks like the initial comment has a traceback but not the actual exception.\n", "The important line was missing, yes.\n\n`python3 ../../../tensorflow/tensorflow/models/rnn/translate/translate.py --data_dir .`\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nPreparing WMT data in .\nExtracting tar file ./training-giga-fren.tar\nUnpacking ./giga-fren.release2.fr.gz to ./giga-fren.release2.fr\nUnpacking ./giga-fren.release2.en.gz to ./giga-fren.release2.en\nDownloading http://www.statmt.org/wmt15/dev-v2.tgz to ./dev-v2.tgz\nSuccesfully downloaded dev-v2.tgz 21393583 bytes\nExtracting tgz file ./dev-v2.tgz\nCreating vocabulary ./vocab40000.fr from data ./giga-fren.release2.fr\nTraceback (most recent call last):\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 276, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 273, in main\n    train()\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 137, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 268, in prepare_wmt_data\n    create_vocabulary(fr_vocab_path, train_path + \".fr\", fr_vocabulary_size)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 136, in create_vocabulary\n    for line in f:\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_gfile.py\", line 177, in __next__\n    return next(self._fp)\n  File \"/usr/lib/python3.4/encodings/ascii.py\", line 26, in decode\n    return codecs.ascii_decode(input, self.errors)[0]\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 14: ordinal not in range(128)\n", "Does changing `with gfile.GFile(data_path, mode=\"r\") as f:` in `models/rnn/translate/data_utils.py` to have `mode=\"rb\"` fix the problem?  If so, that's a better fix than teaching `gfile.py` about latin.\n", "Line 134 to `with gfile.GFile(data_path, mode=\"rb\") as f:`\n\ntogether with\n\nLine 140 to `tokens = tokenizer(line.decode('latin1')) if tokenizer else basic_tokenizer(line.decode('latin1'))`\n\nalso seems to solve the issue for python 3.\n", "What error do you get if you only make the line 134 change?  The reason I ask is that it should work the same way in Python 2 and Python 3, and Python 2 doesn't automatically use unicode.\n", "`python3 ../../../tensorflow/tensorflow/models/rnn/translate/translate.py --data_dir .`\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nPreparing WMT data in .\nCreating vocabulary ./vocab40000.fr from data ./giga-fren.release2.fr\nTraceback (most recent call last):\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 276, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 273, in main\n    train()\n  File \"../../../tensorflow/tensorflow/models/rnn/translate/translate.py\", line 137, in train\n    FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 268, in prepare_wmt_data\n    create_vocabulary(fr_vocab_path, train_path + \".fr\", fr_vocabulary_size)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 140, in create_vocabulary\n    tokens = tokenizer(line) if tokenizer else basic_tokenizer(line)\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/models/rnn/translate/data_utils.py\", line 109, in basic_tokenizer\n    words.extend(re.split(_WORD_SPLIT, space_separated_fragment))\n  File \"/usr/lib/python3.4/re.py\", line 200, in split\n    return _compile(pattern, flags).split(string, maxsplit)\nTypeError: can't use a string pattern on a bytes-like object\n", "So the right fix for that is to put `b` in front of the regular expression definitions to get `bytes`-compatible regular expressions.  A few more fixes along those lines may be required.\n", "If you want to keep trying to fix it I'm happy to keep helping, but I could also try to find someone to reproduce on our end.\n", "I've posted an initial attempt to https://github.com/tensorflow/tensorflow/pull/1436\n\nI've not tested this on 2.7 and would also need pointers.\n", "I believe this was fixed by #1436.\n"]}, {"number": 1431, "title": "Install Error\uff1aSSLError: hostname 'storage.googleapis.com' doesn't match either of 'accounts.google.com', '*.partner.android.com'", "body": "When I use command :\n\"sudo pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl\" \nto install Iget this error, please tell me how can I solve?\n", "comments": ["@martinwicke: Whose the right person for pip issues? \n", "@amirkhango Is this still an issue with 0.7.1? Or are you deliberately trying to install 0.5.0?\n", "@martinwicke  you mean I just change 0.5.0 to 0.7.1 in that command? Though this problem is still ! Can u tell me how do you find this solution? \n", "@amirkhango have you solved this problem?\n", "@amirkhango \ni got it, in this site: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#sslerror-ssl_verify_failed\n\nyou can download(http://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_CONTAINER_TYPE=CPU,TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-0.7.1-cp27-none-linux_x86_64.whl) and \nuse `pip install xxx.whl`\n"]}, {"number": 1430, "title": "node placement algorithm in the source code?", "body": "I have read the whitepaper and felt very excited about the **node placement** algorithm decribed in **section 3.2**, so I tried to figure out the details from the source code. However, I cannot find the files that implement this algorithm. \nI have read the tensorflow/core/common_runtime/**Simple_placer**.h (& Simple_placer.cc) and it seems it's too \"simple\" to cover the algorithm descibed in the whitepaper. Other files like tensorflow/core/graph/**Graph_partition**.h (& Graph_partition.cc) didn't have that algorithm neither. And the costmodel implemented in tensorflow/core/graph/**Costmodel**.h (& Costmodel.cc), I cannot find their caller in other place.\nSomeone please help me?\n", "comments": ["Please ask non-issue questions on stackexchange or the discussion list, not as Github issues.\n"]}, {"number": 1429, "title": "update protobuf to 3.0.0b2.post2", "body": "", "comments": ["tests don't look good. Is that a bazel 0.2 issue?\n", "It is! But I was looking at it puzzled before reading your comment. It would be really good to have https://github.com/bazelbuild/bazel/issues/1014.\n", "@tensorflow-jenkins test this please.\n", "Jenkins, test this please.\n", "(Can you remind me what we need to upgrade this for?)\n", "I suspect this is going to be updated for ios support in a separate PR.\n"]}, {"number": 1428, "title": "Fix Error using tf.image.random._ : 'numpy.ndarray' object has no attribute 'get_shape' #1399", "body": "Fix Error using tf.image.random._ : 'numpy.ndarray' object has no attribute 'get_shape' #1399 by fix flip_left_to_right to call convert_to_tensor\nFixes #1399 \n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Looks good!  @martinwicke: Can you launch the tests? \n", "You can, too!\n\nJenkins, test this please.\n", "@martinwicke: How does one look at the test output?  http://ci.tensorflow.org/job/tensorflow-pull-requests-mac-python3/99/console shows which tests fail, but I don't know how to see how they fail.\n", "http://ci.tensorflow.org/job/tensorflow-pull-requests-mac-python3/99/consoleFull shows the full log. Also, theres's a little button on top of the log to show the full output.\n\nThis looks fine, these three python3 tests are unrelated.\n", "If I have the power to run tests, does that also mean I have the power to merge?  What's the process for that these days?\n", "@girving @martinwicke So does this mean it is OK to merge this? :smiley: \n", "Merged!\n", "https://github.com/tensorflow/tensorflow/commit/b4b2575ec4bf7e6da2686505f61b5f16cb9273ab\n"]}, {"number": 1427, "title": "Mandlebrot example fails on docker image", "body": "I'm running through the [Mandlebrot Set tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/mandelbrot/index.html) and hit a snag.\n\nI am running tensor flow from Docker -- the `b.gcr.io/tensorflow/tensorflow` image\n\nWhen I try to initialize all variables, I get this error:\n\n```\nW tensorflow/core/kernels/cast_op.cc:125] Unimplemented: Cast complex64 to float is not supported\nE tensorflow/core/common_runtime/executor.cc:275] Executor failed to create kernel. Unimplemented: Cast complex64 to float is not supported\n```\n\nFull error after copy/pasting tutorial code to python file:\n\n```\nW tensor [[Node: zeros_like/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_COMPLEX64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](zeros_like/ZerosLike)]]\nTraceback (most recent call last):xecutor.cc:275] Executor failed to create kernel.\n  File \"mb.py\", line 36, in <module>oat is not supported\n    tf.i [[Node: zeros_like/Cast = Cast[DstT=DT_FLOAT, SrcT=DT_COMPLEX64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](zeros_like/ZerosLike)]]n/framework/ops.py\",\nCaused by op u'zeros_like/Cast', defined at:\n  File \"mb.py\", line 34, in <module> feed_dict, self.graph, session)\n    ns = tf.Variable(tf.zeros_like(xs, tf.float32))orflow/python/framework/ops.py\",\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py\", line 632, in zeros_liken, feed_dict)\n    ret = gen_math_ops.cast(ret, dtype)ackages/tensorflow/python/client/session.py\"\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 193, in castNone, fetches, feed_dict)\n    return _op_def_lib.apply_op(\"Cast\", x=x, DstT=DstT, name=name)lient/session.py\"\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)/lib/python2.7/dist-packages/tensorflow/python/client/session.py\"\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2040, in create_op\n    original_op=self._default_original_op, op_def=op_def)/python/client/session.py\"\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1087, in __init__\n    self._traceback = _extract_stack()mplementedError: Cast complex64 to float is n\not supported\n```\n\nIs the docker image out of date?\n", "comments": ["This is caused by changes in the behavior of zeros_like (See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L641). \n\nAs a temporary workaround, please replace\n`ns = tf.Variable(tf.zeros_like(xs, tf.float32))`\nwith\n`ns = tf.Variable(tf.zeros(Z.shape))`\n\nWe will provide a more permanent solution to this soon.\n", "That did the trick! Thanks.\n", "Fix in review.\n"]}, {"number": 1426, "title": "Is CentOS 6.5 supported?", "body": "I have try each method in installation guide to install GPU-tensorflow in my centos 6.5 server?\nHowever, the error below is still exists.\n`ImportError: /lib64/libc.so.6: version GLIBC_2.14 not found (required by /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so)`\nIs there anyone who've successfully installed tensor flow in centos 6.5?\n", "comments": ["You should build from the source code.\nThe pre-built binary may not work for older glibc. CentOS 6.5 has glibc 2.12 which IS too old for the pre-built binary.\n", "@thinxer Thanks for you reply.\nI also try to build from source, but it failed again, it seems that it could be related to brazel, \ncould you please elaborate the steps to build tensor flow including the version of brazel.\n", "I have no experience building TF on CentOS 6. I guess you need a compiler capable of C++11 features to begin with, such as GCC 4.8+.\n", "You'll need to provide more information if you want help.  If the build failed, what was the error message, and which compiler are you using?  You'll definitely need a C++11 capable compiler.\n", "@girving Below is the complete information\n `$ python\nPython 2.7.11 (default, Mar  5 2016, 20:08:04)\n[GCC 4.9.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n> > >     from tensorflow.python import *\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/**init**.py\", line 50, in <module>\n> > >     from tensorflow.python.framework.framework_lib import *\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/framework_lib.py\", line 62, in <module>\n> > >     from tensorflow.python.framework.ops import Graph\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 40, in <module>\n> > >     from tensorflow.python.framework import versions\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/versions.py\", line 24, in <module>\n> > >     from tensorflow.python import pywrap_tensorflow\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n> > >     _pywrap_tensorflow = swig_import_helper()\n> > >   File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n> > >     _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n> > > ImportError: /usr/local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: symbol memcpy, version GLIBC_2.14 not defined in file libc.so.6 with link time reference`\n", "@thinxer I am using gcc 4.9.2.\n", "So you have compiled from source successfully but cannot load it from Python?\n\nIt seems that your Python is compiled from a lower version of GCC which uses GLIBC_2.12, but the built tensorflow binary uses GLIBC 2.14.\n\nYou may try LD_PRELOAD glibc 2.14, which should be found within your GCC 4.9 installation.\n", "See https://github.com/bazelbuild/bazel/issues/760. Right now it requires customizing CROSSTOOL, but someone on the bazel team is trying to come up with a fix so that custom gcc envs are recognized and handled automatically. \n", "Also the LD_PRELOAD option isn't so great because it'll mess with other things. For example I use IPython, and using a different glibc leads to a memory leak with quick infinite consumption. \n", "I'm going to close this since it appears to be a bazel issue.  Please reopen if you think there's a fix on the TensorFlow side.\n", "I've compiled from source python 3.5 glibc 2.14 and gcc 6.2. Installed the glibc and gcc in my home directory with ./configure --prefix=~/libs. For python I did an altinstall, so now I have python 2.6 for centos to use and python 3.5 at the same time with different binaries i.e. python and python3.5 commands.\nNow I have a script that puts my glibc and libstdc++ in the LD_LIBRARY_PATH before runing python3.5\nDownloaded libcudnn 5.1 for cuda 7.5 from nvidia developers page and added to the ~/libs folder\nI've installed cuda 7.5 from the cuda repo, hence I have nvidia driver version 352.99, cuda toolkit and everything related to cuda taken care by yum.\n\nBut when I run any tensorflow example or simple session to add 10+32 I get stuck at:\n\n`I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0)`\n\nwith 0% of GPU utilization and python3.5 using almost all GPU memory\n\nI'm using a GTX970\nuname -a gives\nLinux jigsaw 2.6.32-504.23.4.el6.x86_64 #1 SMP Tue Jun 9 20:57:37 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\n\nAnything you guys can do for me?\n", "I have succeeded in compiling a GPU, Python 3.5 version of TensorFlow 0.10.0 on a CentOS 6 Docker, and it ran well on our university's CentOS 6 cluster. Check https://github.com/leelabcnbc/DevOps/tree/master/Docker. Basically, it's replacing some hardcoded lines in CROSSTOOL-related items, and adding `-lm` to everything to prevent errors like #2291. I think Google can make compiling TensorFlow on CentOS less frustrating, if they make some hardcoded stuff link to correct locations.\n", "I [was able](https://github.com/tensorflow/tensorflow/issues/110#issuecomment-265431453) to build tensorflow 0.12rc0 on CentOS6.5 without having root privileges, and it seem to work OK with glibc 2.12, without any `LD_LIBRARY_PATH` or `LD_PRELOAD` tricks. It looks like the key is to statically link necessary libraries, e.g. with `-lm`, like it was [just mentioned](https://github.com/tensorflow/tensorflow/issues/1426#issuecomment-250024253) by @zym1010  (I've also added `-lrt`). ", "@i3v and @zym1010   When you compile the tensorflow, which version of bazel you installed in your box?\r\nVery appreciate your answer", "@jimht011\r\nI've used bazel-0.4.1, just because it was the latest at that time. And bazel-0.4.0 few days before that (for just the same reason). "]}, {"number": 1425, "title": "Add complex128 dtype", "body": "I've started implementing support for `complex128` in order to fix #1420.\nThis commit contains all changes necessary to create and run `complex128` variables.\n\nIf this is okay so far, I'll add tests, change the documentation and enable `complex128` for all ops that are already using `complex64` (and add the corresponding tests).\n\nThe only change I wasn't sure about is whether it was okay to rename the `TF_COMPLEX` enum entry to `TF_COMPLEX64` at https://github.com/tensorflow/tensorflow/blob/97f585d506cccc57dc98f234f4d5fcd824dd3c03/tensorflow/core/public/tensor_c_api.h#L81, as this is not backwards-compatible with external source code that uses the C API.\nShould I change the entry back to `TF_COMPLEX`?\n", "comments": ["Can one of the admins verify this patch?\n", "@ibab: Taking a look now.  As for `TF_COMPLEX`, let's have both `TF_COMPLEX` and `TF_COMPLEX64` with the same enum value (https://stackoverflow.com/questions/11412516/enum-why-can-two-different-enumeration-constants-have-the-same-integer-value).\n", "Looks great, thank you for the contribution!  Just those couple style comments on the Python code.  This is by far the least number of comments I've wanted to make on a TensorFlow PR so far. :)\n\nIt's good to keep the commits separate while you're working on it, and once everything is in place we can squash them into a single test-passing commit.\n", "Okay, thanks for the comments!\nI'll implement them and proceed with converting the existing kernels.\n", "@ibab: Actually, let's hold the kernel updates to a separate PR.  Adding just the dtype and associated framework support is a nice standalone step. \n", "Okay, I've added the style changes.\nI've fixed the problem with `scomplex` and `dcomplex` earlier today in https://github.com/tensorflow/tensorflow/commit/03422935b98145f5b1060198e1f7274b665ddd87 \n", "Awesome, looks good to me.  @martinwicke: Can we run this through the tests?  Is it better to smash into a single commit before or after testing? \n", "Jenkins, test this please.\n", "@josh11b: Looks like this PR requires the backwards compatibility test to run to make it work (unsurprising since there's a new numeric type).  Is the update script in open source yet?\n", "@girving: I can run `./bazel-out/local_linux-py3-opt/bin/tensorflow/core/ops/compat/update_ops`.\nI thought that I didn't need to do it, as I hadn't changed any ops, but some of them use all numeric types.\nIt didn't show up in my tests, as I had used it after I modified some kernels. Sorry for that. I'll push the remaining files in a second.\n", "Can someone ask travis to re-run the tests, to see if they now pass? Thanks!\n", "Jenkins, test this please.\n", "The backwards compatibility test has recently been moved to open-source, along with the update_ops script.\n", "@josh11b: Thanks! If the script hasn't changed, then my updated `ops.pbtxt` should be fine, though, right?\n\nIt seems that @yaroslavvb is not a `tensorflow member` and Jenkins won't listen to him :disappointed: \n", "@tensorflow-jenkins: test this please.\n", "The Python 3 test failures seem to be pre-existing ones not caused by this PR.\nMaybe @girving would be interested in merging this now?\n", "Excellent!  I'll take another quick look and then merge.\n", "@ibab: Unfortunately the backwards compatibility test files have been changed since your update, so the test fails again for me when I try to merge.  Can you merge, regenerate those files, and then squash into one commit?  There's a bit of an unfortunate race condition here.\n\nIncidentally, in a future commit we should add a note to `RELEASE.md` saying TensorFlow has complex64 support now. :)\n", "I'll relaunch tests ASAP when you do that and try to get it in as quickly as I can.  Large refactoring changes can be tricky to sneak into an atomic system.\n", "Okay, I'll remove the updated ops history file, rebase on master, rebuild the ops file and squash all commits.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Okay, I've updated the PR. Tests are passing on my side.\n", "Trying merge now...\n", "Jenkins, test this please.\n", "Jenkins, test this please.  (The first time failed for some infrastructure reason)\n", " @martinwicke: The infrastructure failure persists.  Ideas?\n\n@ibab: Do you mind if I pull this in and commit it myself (crediting you explicitly of course)?  Looks like our Jenkins setup is experiencing some issues, and I'd like to get this in before the compatibility test bit rots again. \n", "Yeah, that's fine with me. It would be nice if you could keep the author field of the original commit.\n", "Excellent, looks like the infrastructure failure has passed.  Back to the normal merging process.\n", "Cheers :tada: \n", "Thank for the contribution!\n"]}, {"number": 1424, "title": "Logsoftmax kernel", "body": "for issue #443\n", "comments": ["Can one of the admins verify this patch?\n", "@girving how do you run tests when you develop locally? Do you simply do:\n\n```\nbazel test  //tensorflow/core/kernels:matm...\n```\n\netc.? Thanks\n", "@kashif: Yep, though the names of tests unfortunately don't correspond exactly to the directory structure.  Usually it's easiest to write op tests in Python, so I'd either put something `python/kernel_tests/softmax_op_test.py` or make `log_softmax_op_test.py`.  You can run these as\n\n```\nblaze test python/softmax_op_test python/log_softmax_op_test\n```\n\nI.e., the `kernel_tests` part goes away.  You can also run all the tests by doing `blaze test ...`. \n", "@girving thank you, can you kindly review now?\n", "Please squash this into one commit.  It's hard to see the differences from master as is, since you introduced a bunch of code in one commit and then removed it in another.\n", "We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n\n<!-- need_author_consent -->\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@girving squashed \n", "cool thanks! I'll get that done and squash it back to a single commit\n", "@tensorflow-jenkins test this please\n", "@jendap the builds are failing due to some other tests not related to my changes... is there a way to fix that from my side?\n", "@kashif: Actually the backwards compatibility test is failing, which does require an update on your end.  Please do this:\n\n```\nbazel run core/ops/compat/update_ops\n```\n\nIt updates some files that keep track of the history of op changes to ensure that everything is backwards compatible.  In this case, `update_ops` will add a new entry for `LogSoftmax`.\n", "Note that you can reproduce this problem locally by running `bazel test ...`.  You should see the same failure.\n", "@girving thanks! will do... wow I learned so much in the pull request thanks for that!!\n", "Apologies for not responding to this for a few days.  What's the status?  It looks like the squash hasn't happened yet.\n", "@girving no worries! squashed\n", "I still see two commits.  Did you squash?\n", "opps missed 1 squashing...\n", "Looks good!  Jenkins: Test this please.\n", "Tests all look good.  Unfortunately Github recently changed the \"Merge pull request\" button to not work if you're out of date.  Can you rebase?  Apologies for the trouble; we haven't yet figured out to hack around this unfortunate behavior change.\n", "@tensorflow-jenkins: test this please\n", "Does this implementation use a safe way to compute LogSumExp, preventing the underflow, using the \"log sum exp trick\"? I could not see it in code. More info on preventing the underflow here: http://math.stackexchange.com/questions/648514/preventing-underflow-log-sum-exp-trick", "It seems to me that this should have been applied at some point: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.reduce_logsumexp.md"]}, {"number": 1423, "title": "Need force_gpu_if_available for tests", "body": "For GPU tests, we have the `force_gpu` and `use_gpu` flags. `force_gpu=True` is good for manual testing during development, but if an op cannot be placed on a gpu because there isn't a GPU or because TensorFlow was built without GPU support, it will make the test fail. \n\nFor tests, we really need a way to say \"fail if a GPU is available, but it's not used\". Could be as simple as providing the equivalent of `force_gpu=tf.compiled_with_GPU_support() and tf.get_GPU_devices()`. Both functions don't exist, but really should.\n", "comments": ["`tf.test.is_built_with_cuda` may be useful.\n", "We never merged the PR -- some test was still failing. So this issue is still not fixed.\n", "@martinwicke I'm not sure I understand.  You suggested that it might be as simple as `force_gpu=tf.test.is_built_with_cuda`, which already exists.  What is missing?\n", "Do we have a function telling you (easily) whether there is a GPU?\nOn Thu, Jun 9, 2016 at 08:11 Geoffrey Irving notifications@github.com\nwrote:\n\n> @martinwicke https://github.com/martinwicke I'm not sure I understand.\n> You suggested that it might be as simple as\n> force_gpu=tf.test.is_built_with_cuda, which already exists. What is\n> missing?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1423#issuecomment-224926091,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAjO_SS8rfbapwl-IneHVNEJFYljhnSKks5qKCz_gaJpZM4HrGFM\n> .\n", "Ah, right.  Apologies for poor reading comprehension.  That other function doesn't exist as far as I know.\n", "You can do this:\n\nfrom tensorflow.python.client import device_lib\ndevice_names = [d.name for d in device_lib.list_local_devices()]\nself.assertTrue(\"/gpu:0\" in device_names)\n\nOn Thu, Jun 9, 2016 at 8:20 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> Ah, right. Apologies for poor reading comprehension. That other function\n> doesn't exist as far as I know.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1423#issuecomment-224929150,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AABaHJAiGz1-6lTPlgwVqEoBBZcSy6tZks5qKC8lgaJpZM4HrGFM\n> .\n", "If someone wants to make a PR, wrapping Yaroslav's code up as `tf.test.is_gpu_available` seems good.\n", "is_gpu_available is now ready.\r\nForce gpu if available has been in the plans, but getting delayed deprioritized so far.", "@martinwicke says close."]}, {"number": 1422, "title": "Tensorflow just not working", "body": "I stopped using tensorflow for a while. Now i want to get back to it, but after installing, when I try to import tensorflow module on python i get this message:\n\n```\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there\n```\n\nHave tried uninstalling all previous existing versions and exiting any tensorflow folder, i'm running python from root folder, but I still get the error. Also if I download tensorflow from github and run from tensorflow version 6.0 I get same error\nAny idea?\nThanks\n", "comments": ["- Can you include the stack of the entire error message?\n- Have you tried installing in a virtualenv from scratch , using version 0.7.1 ?\n", "Check this out, i had this exact same trouble this weekend\n\nhttps://github.com/tensorflow/tensorflow/issues/1402#event-580456750\n", "thanks @ushnish, but shouldn't it work only by following the steps explained on the installation guide?\n@vrv I get the same error if I install using virtualenv from scratch.\nHere's the commands I do installing with pip and the entire error message:\n`sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl`\n\n```\nCollecting tensorflow==0.7.1 from https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n  Downloading https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl (11.6MB)\n...\nInstalling collected packages: tensorflow\nSuccessfully installed tensorflow-0.7.1\n```\n\n...\n\n```\n>>> import tensorflow as tf\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 41, in <module>\n    raise ImportError(msg)\nImportError: Traceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 35, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 6, in <module>\n    from google.protobuf import descriptor as _descriptor\nImportError: No module named protobuf\n\n\nError importing tensorflow.  Unless you are using bazel,\nyou should not try to import tensorflow from its source directory;\nplease exit the tensorflow source tree, and relaunch your python interpreter\nfrom there.\n\n```\n", "https://www.tensorflow.org/versions/master/get_started/os_setup.html#pip-installation-issues might help :)\n", "Right! Didn't see I also had to uninstall protubuf. Thanks!\n", "Sounds from the discussion like this issue can be closed - please re-open if you're still experiencing problems.\n", "Please help me to solve this problem i am trying from several days. i get this error when i import tensorflow after installation , i am using windows OS\r\n\r\n>>> import tensorflow as tf\r\nRuntimeError: module compiled against API version 0xc but this version of numpy is 0xa\r\nImportError: numpy.core.multiarray failed to import\r\nImportError: numpy.core.umath failed to import\r\nImportError: numpy.core.umath failed to import\r\n2018-10-18 18:24:15.573678: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr"]}, {"number": 1421, "title": "TensorBoard showing nothing! ", "body": "I am trying to run the example in my Linux machine (Ubuntu 14.04): \n**tensorflow/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py**\n\nFirst, I type \"`python mnist_with_summaries.py`\" in command line. Everything looks good.\nThen following the tutorial, I type \"`tensorboard --logdir =/tmp/mnist_logs`\". Following message appeared.\n\nWARNING:tensorflow:Unable to read TensorBoard tag\nStarting TensorBoard  on port 6006\n(You can navigate to http://0.0.0.0:6006)\n\nThen I visit \"http://0.0.0.0:6006\" on Chrome,  nothing appears except:\n\n\" **No scalar summary tags were found.\nMaybe data hasn't loaded yet, or maybe you need to add some tf.scalar_summary ops to your graph, and serialize them using the tf.training.summary_io.SummaryWriter**. \"\n\nIn the terminal, following messages showed up:\n\n\"127.0.0.1 - - [07/Mar/2016 11:45:06] \"GET / HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /lib/css/global.css HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/lodash/lodash.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/plottable/plottable.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/d3/d3.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/plottable/plottable.css HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/graphlib/dist/graphlib.core.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/polymer/polymer.html HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/webcomponentsjs/webcomponents-lite.min.js HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/iron-ajax/iron-ajax.html HTTP/1.1\" 200 -\n127.0.0.1 - - [07/Mar/2016 11:45:07] \"GET /external/dagre/dist/dagre.core.min.js HTTP/1.1\n... ... \"\n\nI checked **/tmp/mnist_log/**, there is an event file named \"**events.out.tfevents.1457371474.rdii-Alienware-X51**\". Why nothing shows up on my TensorBoard ?  Please help me! \n", "comments": ["I just ran this locally on my machine and it worked. Could you upload the events file so I can see if I can reproduce it? Also, how large is the events file? it should be around 1MB.\n", "This is the event file (1.1MB).\n[events.out.tfevents.1457371474.rdii-Alienware-X51.tar.gz](https://github.com/tensorflow/tensorflow/files/162093/events.out.tfevents.1457371474.rdii-Alienware-X51.tar.gz)\n\nFollowing is all the information appears in my command  window : \n\n`yi@rdii-Alienware-X51:~$ tensorboard --logdir =/tmp/mnist_logs\n\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/dist-\n\npackages/tensorflow/tensorboard/TAG' on path /usr/local/lib/python2.7/dist-\n\npackages/tensorflow/tensorboard/TAG\n\nWARNING:tensorflow:Unable to read TensorBoard tag\n\nStarting TensorBoard  on port 6006\n\n(You can navigate to http://0.0.0.0:6006)\n\n127.0.0.1 - - [07/Mar/2016 15:09:25] \"GET / HTTP/1.1\" 200 -\n... GET requests elided...\n127.0.0.1 - - [07/Mar/2016 15:09:26] \"GET /data/runs HTTP/1.1\" 200 -`\n", "OK, I've looked into the event file you've provided - it's perfectly fine, which means the issue is in TensorBoard. Specifically, I think TensorBoard is never detecting that the file exists, and so isn't loading anything. However, I can't reproduce this behavior on my Mac or Linux.\n\nCan you please help me explore this issue by doing the following: \n1. Let's just verify that the bug still reproduces if you build from source. \n- Clone the TensorFlow repository\n- Checkout the branch r0.7\n- run ./configure (no need to put in GPU support)\n- bazel build `//tensorflow/tensorboard:tensorboard`\n- `./bazel-bin/tensorflow/tensorboard/tensorboard --logdir=/tmp/mnist_logs`\n  Note: make sure the logdir is an absolute path, not a relative path. I think there is an issue with relative paths, let's disambiguate between them.\n\nIf TensorBoard is still broken, let's add some debugging statements into `tensorflow/python/platform/default/_gfile.py`, which is the file that contains the interface between TensorBoard and the filesystem. We can test my hypothesis that TensorBoard isn't loading the right paths by adding some print statements in the following functions:\n\nListDirectory (line 353): print the directory, and the files\nExists (line 250): print the path, and whether it exists\n\nIsDirectory (line 255): print the path, and whether it is a directory\n\nLet's also check the subdirectories generated by TensorBoard:\ngo to line 194 of tensorflow/tensorboard/tensorboard.py and print out the subdirectories\n\nOnce you've added these statements, rebuild tensorboard (as above) and rerun it.\n\nCan you please try this and let me know what gets printed? \n\nAlso, feel free to ping me via Google Hangouts at danmane@google.com or danmane@gmail.com and I'll help debug this synchronously.\n\nThanks for your help! Let's fix TensorBoard :)\n", "I'm also having this problem. I'm on Ubuntu 15.10 and Python 3.4.\n\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG' on path /usr/local/lib/python3.4/dist-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\nStarting TensorBoard  on port 6006\n(You can navigate to http://0.0.0.0:6006)\n\nNothing shows up on Tensorboard pages.\n", "Experiencing the same problems, setup:\nUbuntu 14.04.4 \nTensorflow version 0.7.1\n\nI believe my events file is okay, made sure to flush(), running in chrome, tried uinstalling tf and reinstalling, messed with aliases and stuff in the path to events directory. Tried all these things but getting same errors and lack of anything in tensorboard.\n\nIf there is anything I can do to help you guys figure it out, let me know.\n", "Previously, I used **pip to install TensorFlow**. The TensorBoard shows nothing.\nToday I try to **build from the source**. Then TensorBoard works!\n@danmane  \nI am trying to pip install on another Linux machine to make sure that \"pip install\" can reproduce the problem. \n\nI will let you know if I can reproduce that problem with pip installation.\n", "Well, I tried pip installation on another Linux machine, seems everything magically worked!  \n", "Mine was a pip installation and I am not seeing anything on tensorboard. \n", "I pip installed the nightly binary and tensorboard looks to be working now for me. Maybe this is something that has been fixed? \n", "Some more data:\n\nI tried repro'ing this issue with the given events file.\nBuilding tensorflow from scratch, tensorboard was working as expected.\nDoing a pip install on my Ubuntu 14.04 machine, tf 0.7.1, I was able to see the issue (no graphs showing up in tensorboard).\nWhen I pip uninstalled and reinstalled tensorflow, tensorboard started working as expected.\n", "I had this problem, but was specifying my path as \"~/path/to/logs\" -- once I swapped out \"~\" for the absolute path to HOME, it worked fine.\n\nOne other thing that might be useful to others having this issue: It seems you must start tensorboard _after_ the file has been created.\n", "I am having this same issue on debian8.\nStandard python3 install, sudo apt-get install pip3, sudo pip3 install <tensorflow cpu 64bit>\n\nRan the mnist with summaries, did \ntensorboard --logdir=/tmp/mnist_logs --debug\n\nAnd its correctly finding the 1.1MB events file\n\nDEBUG:tensorflow:No more events in /tmp/mnist_logs/events.out.tfevents.1460670145.Moko\nINFO:tensorflow:No path found after /tmp/mnist_logs/events.out.tfevents.1460670145.Moko\nDEBUG:tensorflow:No more events in /tmp/mnist_logs/events.out.tfevents.1460670145.Moko\nINFO:tensorflow:No path found after /tmp/mnist_logs/events.out.tfevents.1460670145.Moko\nINFO:tensorflow:Multiplexer done loading. Load took 0.6 secs\n\nNavigating to localhost:6006 gives me the tensorboard dashboard, but thats it. no graphs or anything.\n", "I should add my 2 cents here. I am experiencing pretty much the same issue, although my error message is a little different.\n\n`\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/site-packages/tensorflow/tensorboard/TAG' on path /usr/local/lib/python2.7/site-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag\n`\n\nIn fact, that TAG directory/file is indeed not present on my filesystem.\n", "I have the same TAG error, but as far as I understand that is just used for debugging which is why its only a warning not an error.\n", "@DM-  - So does the tool actually work for you with this error?\n\nI was able to find this issue which seems related - https://github.com/tensorflow/tensorflow/issues/530\nBased on the recommendations in that issue I was able to run tensorboard from\nthe bazel-bin directory without receiving the TAG warning, however no events are showing up in the dashboard. My event files do not appear to be empty:\n\n```\nsiakhnin@mac-korbel21:~/tools/tensorflow$ ls -lah /tmp/mnist_logs/train/\ntotal 38824\ndrwxr-xr-x  3 siakhnin  wheel   102B 15 Apr 11:59 ./\ndrwxr-xr-x  4 siakhnin  wheel   136B 15 Apr 11:59 ../\n-rw-r--r--  1 siakhnin  wheel    19M 15 Apr 12:00 events.out.tfevents.1460714373.mac-korbel21.wlan.embl.de\nsiakhnin@mac-korbel21:~/tools/tensorflow$ ls -lah /tmp/mnist_logs/test\ntotal 5328\ndrwxr-xr-x  3 siakhnin  wheel   102B 15 Apr 11:59 ./\ndrwxr-xr-x  4 siakhnin  wheel   136B 15 Apr 11:59 ../\n-rw-r--r--  1 siakhnin  wheel   2.6M 15 Apr 12:00 events.out.tfevents.1460714374.mac-korbel21.wlan.embl.de\n```\n\nWhen I look at the debug output it appears that all the HTTP requests being made in the background are successful (return 200). For instance the request for\n\n/data/graph?run=train&limit_attr_size=1024&large_attrs_key=_too_large_attrs\n\ndoes, in fact, appear to return the valid JSON for the graph if I issue it directly from my browser. However the dashboard shows nothing. Tried in both Chrome and Firefox.\n", "@llevar - I'm surprised that you are getting a valid response for the graph if you issue it from the browser, but the dashboard is showing nothing. (Just to be clear, this is also the \"graph\" dashboard that is showing nothing, i.e. if you click \"graph\" in the top right of the screen.)\n\nCan you try with TensorBoard from rc0.8 and see if this still happens? Also, are you getting any 404s on the console in the TensorBoard frontend? \n", "Re: the missing TAG file - that is not particularly important, and should be fixed in 0.8.\n", "@danmane - All the tabs are empty. Looking at the page with Dev Tools in Chrome it looks like the CSS stylesheet is missing from the deployment directory for some reason. And indeed the GET call for it results in a 404 \n\n```\nWARNING:tensorflow:IOError [Errno 2] No such file or directory: '/usr/local/lib/python2.7/site-packages/tensorflow/tensorboard/lib/css/global.css' on path /usr/local/lib/python2.7/site-packages/tensorflow/tensorboard/lib/css/global.css\n127.0.0.1 - - [15/Apr/2016 21:01:43] code 404, message Not Found\n127.0.0.1 - - [15/Apr/2016 21:01:43] \"GET /lib/css/global.css HTTP/1.1\" 404 -\n```\n\nI get this message not matter whether I run tensorboard from the pip package installation dir or from bazel-bin. After the 404 are a bunch of javascript errors but I'm assuming they are there because of the missing stylesheet.\n\n<img width=\"774\" alt=\"screen shot 2016-04-15 at 9 10 30 pm\" src=\"https://cloud.githubusercontent.com/assets/5853959/14572334/9452eca2-034e-11e6-90a1-b29501e914bd.png\">\n", "OK, this was a known issue in the original 0.8 release candidate, but it was fixed by #1926 two days ago. Is it possible that your code is 2 days out of date? \n", "Ok, so I rebuilt from the 0.8 rc0 tag and can confirm that the the change in #1926 is in my tensorboard/BUILD file. However when the wheel file gets generated there is still no css included in the tensorboard/lib directory:\n\n![image](https://cloud.githubusercontent.com/assets/5853959/14580104/58e1ed10-03c3-11e6-9bf1-7a3c9cde7c6d.png)\n", "@llevar Thank you very much for that report, that's super important that we don't release a broken python package with r0.8. I'll make sure this gets fixed shortly.\n", "@vrv @martinwicke Can you guys shed any light on why this file would be missing from the pip package? I'm not very familiar with that part of the codebase. \nIt looks like the pip_package BUILD file just references tensorflow/tensorboard as a directory, so I'm confused how it would skip the css file. \n", "globs only go down to the directory that contains the next BUILD file, and\nthen they stop descending into the tree. You can blame me, because I added\nmany more BUILD files to the opensource tree, including one in lib/ which\nnow prevents the frontend rule to include anything in lib/ via its glob.\nIt's still a bit surprising to me because lib/BUILD does have all_files,\nbut it may be that you have to depend on tensorboard/lib as well now that\nit has its own BUILD file.\n\nOn Sat, Apr 16, 2016 at 2:23 AM Daniel W Mane notifications@github.com\nwrote:\n\n> @vrv https://github.com/vrv @martinwicke\n> https://github.com/martinwicke Can you guys shed any light on why this\n> file would be missing from the pip package? I'm not very familiar with that\n> part of the codebase.\n> It looks like the pip_package BUILD file just references\n> tensorflow/tensorboard as a directory, so I'm confused how it would skip\n> the css file.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1421#issuecomment-210780370\n", "Any plans for a fix? I'd love to be able to use tensorboard.\n", "@llevar, what worked for me is copying the global.css file from this git repository to the location listed in the error log in your earlier comment.  That is, I copied [global.css](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/lib/css/global.css) to where tensorboard was looking for it on my system: `/usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard/lib/css/`.\n\nFrom your earlier comment, it looks like you might want to copy that global.css file to the following location on your system: `/usr/local/lib/python2.7/site-packages/tensorflow/tensorboard/lib/css/`.  Also, you will probably need to create that `css` directory first.\n", "Thanks @romanows! That makes sense. This should be part of the official installation instructions until they fix the bug. :-)\n", "The bug should now be fixed at r0.8 head. This will move to master as well\nsoon.\nOn Tue, Apr 19, 2016 at 05:14 Sergei Iakhnin notifications@github.com\nwrote:\n\n> Thanks @romanows https://github.com/romanows! That makes sense. This\n> should be part of the official installation instructions until they fix the\n> bug. :-)\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1421#issuecomment-211888781\n", "@llevar: On current head I've built & used pip packages on both MacOSX and linux and found that the css file was included properly. \n\nGoing to close this issue, please re-open it if you continue to have issues (or open a new issue, this one has become very cluttered)\n", "Confirmed working for me on r0.8 head. Thanks for looking into this.\n", "Reinstalling from master did not solve the problem for me (OSX),   downloading and adding  `/css` and `/js` from [source](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tensorboard/lib) to   `/usr/local/lib/python2.7/site-packages/tensorflow/tensorboard/lib/` did though as previously mentioned\n\nedit: Omg getting teary eyed.. Tensorboard is awesome\n", "Yep, @ragulpr's fix worked for me on Mac.\n", "Upgrading from TF 0.7 to TF 0.9 on my Mac solved my problem. Despite tensorboard still shows the TAG warning, there is no problem with visualization.\n", "It looks like it's looking for this file: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/TAG\nwhich (in my case) was actually not present in the /tensorboard directory.\n\nAfter **creating a file called \"TAG\" with the content \"22\"** (which was the content in the master branch when I looked it up) everything worked fine.\n", "I also had the same TAG issue: \n`WARNING:tensorflow:IOError [Errno 2] No such file or directory: '/Users/Qihong/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG' on path /Users/Qihong/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG\nWARNING:tensorflow:Unable to read TensorBoard tag`\n\n**michaelosthege** said that this can be resolved by downloading that TAG file, but can someone explain what's the purpose of that TAG file? What is the number in the file? \n", "The tag file has no purpose other than displaying a version number at\nstartup. The missing tag file can be ignored.\nOn Fri, Aug 5, 2016 at 9:25 AM QiHong Lu notifications@github.com wrote:\n\n> I also had the same TAG issue:\n> WARNING:tensorflow:IOError [Errno 2] No such file or directory:\n> '/Users/Qihong/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG'\n> on path\n> /Users/Qihong/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/tensorboard/TAG\n> \n> WARNING:tensorflow:Unable to read TensorBoard tag\n> \n> _michaelosthege_ said that this can be resolved by downloading that TAG\n> file, but can someone explain what's the purpose of that TAG file? What is\n> the number in the file?\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1421#issuecomment-237895871,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABVc12kyCTdMXQvEnmKZmc3LD55dv9EQks5qc2PjgaJpZM4HrAd2\n> .\n", "I see... Thanks! \n", "I faced the same issue on Ubuntu with both TF 0.9 and 0.10. Firefox did not show anything on Tensorboard. With Chromium, it worked like a charm.\n", "I'm running MNIST sample https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/  on FT 1.0 GPU on Windows 10.  All the TF events  are getting properly  recorded at C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries .\r\n\r\n When I ran  \r\n\r\n tensorboard --logdir=C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries --debug \r\n\r\n I got\r\n\r\n INFO:tensorflow:TensorBoard path_to_run is: {'C:\\\\tmp\\\\tensorflow\\\\mnist\\\\logs\\\\mnist_with_summaries': 'C'}\r\nINFO:tensorflow:Event Multiplexer initializing.\r\nINFO:tensorflow:Event Multiplexer done initializing\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries\r\nINFO:tensorflow:Adding events from directory C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries\\test\r\nINFO:tensorflow:Constructing EventAccumulator for C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries\\test\r\nINFO:tensorflow:Adding events from directory C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries\\train\r\nINFO:tensorflow:Constructing EventAccumulator for C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries\\train\r\nINFO:tensorflow:Done with AddRunsFromDirectory: C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\nINFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nDEBUG:tensorflow:Opening a record reader pointing at C:\\tmp\\tensorflow\\mnist\\logs\\mnist_with_summaries\\test\\events.out.tfevents.1487636143.DESKTOP-F60FRVO\r\nINFO:tensorflow:TensorBoard is tag: b'41'\r\nStarting TensorBoard b'41' on port 6006\r\n(You can navigate to http://192.168.0.9:6006)\r\n....\r\n\r\nNothing shows up on Tensorboard pages at http://192.168.0.9:6006\r\n", "@Alex6381 - If you have a linux/mac machine available, could you try porting the tfevents files onto linux/mac and turning on a linux/mac TensorBoard? That way we can see if the issue is with the data itself, or with Windows TensorBoard.", "Unfortunately, I don't have linux/mac machine available.", "@Alex6381 try the Windows subsystem for Linux!\r\nHere's a manual: http://blog.mosthege.net/2016/05/11/running-tensorflow-with-native-linux-binaries-in-the-windows-subsystem-for-linux/\r\nRemember to substitute the pip package urls with the most recent version.", "The issue got fixed by properly specifying path to logs\r\n tensorboard --logdir=/**tmp/tensorflow/mnist/logs** --debug\r\nThanks!", "# question: I can run tensorboard,but Browser page is blank\r\nI can access the 127.0.0.1:6006, but the page is blank\r\n# run tensorboard\r\n[lhy@localhost logs]$ tensorboard --logdir=/tmp/logs --debugINFO:tensorflow:TensorBoard is in debug mode.\r\nINFO:tensorflow:Starting TensorBoard in directory /tmp/logs\r\nINFO:tensorflow:TensorBoard path_to_run is: {'/tmp/logs': None}\r\nINFO:tensorflow:Event Multiplexer initializing.\r\nINFO:tensorflow:Event Multiplexer done initializing\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: /tmp/logs\r\nINFO:tensorflow:Adding events from directory /tmp/logs\r\nINFO:tensorflow:Constructing EventAccumulator for /tmp/logs\r\nINFO:tensorflow:Done with AddRunsFromDirectory: /tmp/logs\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\nINFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nDEBUG:tensorflow:Opening a record reader pointing at /tmp/logs/events.out.tfevents.1488352194.localhost.localdomain\r\nINFO:tensorflow:TensorBoard is tag: 39\r\nStarting TensorBoard 39 on port 6006\r\n(You can navigate to http://127.0.0.1:6006)\r\nDEBUG:tensorflow:No more events in /tmp/logs/events.out.tfevents.1488352194.localhost.localdomain\r\nINFO:tensorflow:No path found after /tmp/logs/events.out.tfevents.1488352194.localhost.localdomain\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.130 secs\r\nINFO:tensorflow:path ../external/web_animations_js/web-animations-next-lite.min.js.map not found, sending 404\r\nINFO:tensorflow:returning 404 to 127.0.0.1 for /web-animations-js/web-animations-next-lite.min.js.map\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: /tmp/logs\r\nINFO:tensorflow:Adding events from directory /tmp/logs\r\nINFO:tensorflow:Done with AddRunsFromDirectory: /tmp/logs\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\nINFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nDEBUG:tensorflow:No more events in /tmp/logs/events.out.tfevents.1488352194.localhost.localdomain\r\nINFO:tensorflow:No path found after /tmp/logs/events.out.tfevents.1488352194.localhost.localdomain\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.003 secs\r\nINFO:tensorflow:TensorBoard reload process beginning\r\nINFO:tensorflow:Starting AddRunsFromDirectory: /tmp/logs\r\nINFO:tensorflow:Adding events from directory /tmp/logs\r\nINFO:tensorflow:Done with AddRunsFromDirectory: /tmp/logs\r\nINFO:tensorflow:TensorBoard reload process: Reload the whole Multiplexer\r\nINFO:tensorflow:Beginning EventMultiplexer.Reload()\r\nDEBUG:tensorflow:No more events in /tmp/logs/events.out.tfevents.1488352194.localhost.localdomain\r\nINFO:tensorflow:No path found after /tmp/logs/events.out.tfevents.1488352194.localhost.localdomain\r\nINFO:tensorflow:Finished with EventMultiplexer.Reload()\r\nINFO:tensorflow:TensorBoard done reloading. Load took 0.001 secs\r\n\r\n\r\n# chrome logs\r\n## (1)\r\nweb-animations-next-lite.min.js.map\t   not find\r\n## (2)\r\n\r\n`No support for OES_texture_float extension.\r\nUncaught TypeError: Cannot read property 'getShaderPrecisionFormat' of null :6006/weblas_weblas_js/file/weblas.js:22\r\nThis file is deprecated. Please use `iron-flex-layout/iron-flex-layout-classes.html`, and one of the specific dom-modules instead\r\nThis file is deprecated. Please use `iron-flex-layout/iron-flex-layout-classes.html`, and one of the specific dom-modules instead\r\nUncaught SyntaxError: Unexpected identifier data:text/javascript;charset=utf-8,%0A%20%20Polymer(%7B%0A%20%20%20%20is%3A\u2026%3Dhttp%3A%2F%2F127.0.0.1%3A6006%2Fdist%2Ftf-tensorboard.html-23.js%0A:160\r\nUncaught SyntaxError: Unexpected token > data:text/javascript;charset=utf-8,%0A%20%20Polymer(%7B%0A%20%20%20%20is%3A\u2026L%3Dhttp%3A%2F%2F127.0.0.1%3A6006%2Fdist%2Ftf-tensorboard.html-24.js%0A:17\r\nUncaught SyntaxError: Unexpected token ( data:text/javascript;charset=utf-8,%0A(function()%20%7B%0APolymer(%7B%0A%20\u2026RL%3Dhttp%3A%2F%2F127.0.0.1%3A6006%2Fdist%2Ftf-tensorboard.html-70.js%0A:9\r\nUncaught SyntaxError: Unexpected token ILLEGAL data:text/javascript;charset=utf-8,%0A%20%20%20%20Polymer(%7B%0A%20%20%20%2\u2026L%3Dhttp%3A%2F%2F127.0.0.1%3A6006%2Fdist%2Ftf-tensorboard.html-72.js%0A:77\r\n`", "The Graph tab is working fine but the embedding visualizer isn't. All that it says is that the tensorboard is still loading points and dimensions.\r\nThe terminal issues the following \"WARNING:tensorflow:path ../external/favicon.ico not found, sending 404\". I am trying to use the visualizer for word embeddings trained using gensim", "**Ad Blocker can cause \"No data found\" problem**\r\nI just want to add, that I had the problem that the TensorBoard wasn't showing any data although the log_dir was set correctly. The problem seemed to be the AdBlock plugin, that blocked specific content from being transfered. I add localhost to the AdBlock whitelist. Now its showing everything correctly.", "@phisad, that's fascinating that an ad blocking plugin broke TensorBoard. We may want to add that information to the TensorBoard README.\r\n\r\nAnyone else, if you're experiencing TensorBoard data issues, please create an issue on the TensorBoard repo (github/tensorflow/tensorboard).", "I have an other strange behavior with Tensorbord with the mnist example. (windows 10 Anaconda used)\r\n(worked from out a jupyternotebook) and delivered files at\r\nd:/tmp/tensorflow/mnist/logs/mnist_with_summaries\r\ncontaining two subdirectorie test and train\r\nthen using out of a BAT file:\r\nD:/Anaconda3/Scripts/tensorboard.exe --logdir=\"d:/tmp/tensorflow/mnist/logs/mnist_with_summaries\" --port=6007\r\nTHIS works as suspected at:  localhost:6007\r\nD:/Anaconda3/Scripts/tensorboard.exe --logdir=\"d:/tmp/tensorflow/mnist/logs/mnist_with_summaries\" --port=6006\r\nDoes not show anything  using localhost:6006\r\n\r\n????", "1. When I run the command: tensorboard --logdir=20171124-174639/ --debug\r\n```\r\nStarting TensorBoard b'54' at http://sai:6006\r\n(Press CTRL+C to quit)\r\nWARNING:tensorflow:path ../external/weblas_weblas_js/file/weblas.map.json not found, sending 404\r\nWARNING:tensorflow:path ../external/web_animations_js/web-animations-next-lite.min.js.map not found, sending 404\r\n```\r\n2. I open the chrome and it show me a blank page\r\n\r\n3. So, I open the console in the chrome and found that in the picture:\r\n![error in the chrome](https://user-images.githubusercontent.com/4996787/33205914-89d84bcc-d143-11e7-8de4-0b29ccadcaa1.png)\r\n\r\n4. **But I can see the graph in firefox....**", "> I am having this same issue on debian8.\r\n> Standard python3 install, sudo apt-get install pip3, sudo pip3 install <tensorflow cpu 64bit>\r\n> \r\n> Ran the mnist with summaries, did\r\n> tensorboard --logdir=/tmp/mnist_logs --debug\r\n> \r\n> And its correctly finding the 1.1MB events file\r\n> \r\n> DEBUG:tensorflow:No more events in /tmp/mnist_logs/events.out.tfevents.1460670145.Moko\r\n> INFO:tensorflow:No path found after /tmp/mnist_logs/events.out.tfevents.1460670145.Moko\r\n> DEBUG:tensorflow:No more events in /tmp/mnist_logs/events.out.tfevents.1460670145.Moko\r\n> INFO:tensorflow:No path found after /tmp/mnist_logs/events.out.tfevents.1460670145.Moko\r\n> INFO:tensorflow:Multiplexer done loading. Load took 0.6 secs\r\n> \r\n> Navigating to localhost:6006 gives me the tensorboard dashboard, but thats it. no graphs or anything.\r\n\r\nI have encountered the same issue, it remains unsolved after trying many solutions, but when I tried in safari, the graph just comes out!!!"]}]