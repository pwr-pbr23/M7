[{"number": 666, "title": "\"Error computing ByteSize (possible overflow?)\" when trying to save tensorflow model", "body": "[libprotobuf ERROR google/protobuf/src/google/protobuf/message_lite.cc:293] Error computing ByteSize (possible overflow?).\n\nThis happens for my mlp model, hidden size > 60, \ntf.train.Saver().save(self.session, model_path + '/model.ckpt', global_step = epoch)\n\nHow to avoid this?\n", "comments": ["Do you have a link to a model that reproduces this problem?\n", "I met this problem too, it seems that the variable is too big. In my model, when the size of variable is [10000000, 192],  it reports \"[libprotobuf ERROR google/protobuf/src/google/protobuf/message_lite.cc:293]Error computing ByteSize (possible overflow?).\", when the size of variable is [20000000, 192], it reports \"[libprotobuf FATAL google/protobuf/src/google/protobuf/message_lite.cc:72] This shouldn't be called if all the sizes are equal.\"\n", "I get this error when trying to save a text classification model that uses pre-trained word embeddings.  I was able to reproduce it with the minimal snippet below. It definitely looks like a variable size issue since the example completes successfully once you switch from 3,000,000 rows to 300,000 rows.\n\nThe issue looks important since since these models are common in NLP and not being able to save them means you can't really use them in production.\n\n```\n# Reproducing the error:\nimport tensorflow as tf\nimport numpy as np\n\nwith tf.device('/cpu:0'):\n    rows = 3000000\n    cols = 300\n    input_W = tf.placeholder(tf.float32, [rows, cols])\n    W = tf.Variable(input_W)\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables(), feed_dict={input_W: np.random.rand(rows, cols)})\n    saver = tf.train.Saver(tf.all_variables())\n    path = saver.save(sess, \"dump.bin\",  write_meta_graph=False)\n    print \"Done.\"\n\n```\n", "+1. same problem here.\n", "If you don't use the meta_graph feature, please add \"write_meta_graph=False\" option to saver.save().\n\nSherry\n", "Sherry, I added `write_meta_graph=False` to the snippet from my previous comment (see modified snippet above, using TensorFlow 0.8), but I'm getting the same error. this doesn't seem to help.\n", "Same error here.\n", "The error is due to current size limit on protocol buffers in the implementation of message lite. Can you shard the Variable so it's not so large? \n\nbool MessageLite::SerializePartialToCodedStream(\n    io::CodedOutputStream\\* output) const {\n  const int size = ByteSize();  // Force size to be cached.\n  if (size < 0) {\n    // Messages >2G cannot be serialized due to overflow computing ByteSize.\n    GOOGLE_LOG(ERROR) << \"Error computing ByteSize (possible overflow?).\";\n    return false;\n  }\n", "The variables are the embeddings of word2vec with a large vocabulary. It\nseems not easy to shard them without changing the NegTrainOp and python\ncaller code a lot?\n\nThanks,\n-B\n\nOn Sun, May 15, 2016 at 11:49 AM, Sherry Moore notifications@github.com\nwrote:\n\n> The error is due to current size limit on protocol buffers in the\n> implementation of message lite. Can you shard the Variable so it's not so\n> large?\n> \n> bool MessageLite::SerializePartialToCodedStream(\n> io::CodedOutputStream\\* output) const {\n> const int size = ByteSize(); // Force size to be cached.\n> if (size < 0) {\n> // Messages >2G cannot be serialized due to overflow computing ByteSize.\n> GOOGLE_LOG(ERROR) << \"Error computing ByteSize (possible overflow?).\";\n> return false;\n> }\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/666#issuecomment-219303234\n", "Closing since this issue seems to have been diagnosed.  Please direct further questions about sharding variables to StackOverflow.\n"]}, {"number": 665, "title": "CUDA_ERROR_ILLEGAL_INSTRUCTION occurs while running MNIST on Cuda 3.0 device", "body": "Thanks for your working on Cuda compute capability 3.0 devices.\nI've installed tensorflow from source on ubuntu14.04+Cuda Toolkit 7.0+CUDNN Toolkit 6.5 with NVIDIA driver 352.63.\nThe MNIST program on tutorial always goes well at first but crashes after a few seconds with CUDA_ERROR_ILLEGAL_ADDRESS or CUDA_ERROR_ILLEGAL_INSTRUCTION like below:\n...\nstep 1300, training accuracy 0.94\nstep 1400, training accuracy 0.96\nstep 1500, training accuracy 0.98\nstep 1600, training accuracy 0.94\nstep 1700, training accuracy 0.96\nstep 1800, training accuracy 0.96\nstep 1900, training accuracy 1\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:123] Unexpected Event status: 1\nAborted (core dumped)\n\nor\n...\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:123] Unexpected Event status: 1\nAborted (core dumped)\n\nHere's my dmesg information:\n[  436.027718] nvidia_uvm: Loaded the UVM driver, major device number 248\n[  569.125603] NVRM: GPU at PCI:0000:01:00: GPU-4e535941-7a6a-dcbe-2873-5faabb6099e7\n[  569.125625] NVRM: Xid (PCI:0000:01:00): 13, Graphics SM Warp Exception on (GPC 0, TPC 0): Out Of Range Address\n[  569.125640] NVRM: Xid (PCI:0000:01:00): 13, Graphics SM Global Exception on (GPC 0, TPC 0): Physical Multiple Warp Errors\n[  569.125650] NVRM: Xid (PCI:0000:01:00): 13, Graphics Exception: ESR 0x504648=0x20000e 0x504650=0x24 0x504644=0x13eff2 0x50464c=0x7f\n[  569.125750] NVRM: Xid (PCI:0000:01:00): 13, Graphics Exception: ChID 0020, Class 0000a0c0, Offset 00001b0c, Data 00000000\n\nDid anyone meet the problem above?\n", "comments": ["@liujunyi-sjtu, I cannot reproduce the problem on my side. Could you provide some more information? How did you bulid and run the binary?\n\nIf we still cannot reproduce this problem, then you probably have to run cuda-memcheck on your side, and post the relevant results. Thanks. \n", "Closing due to lack of information.  I'm happy to reopen if more information arises.\n", "Firstly, thanks for tensorflow!  \n\nI am experiencing a similar issue. I installed tensorflow on Fedora 23 via **pip**.  My CUDA installation seems to be working fine, as I can use the R package **gputools** with spectacular results. However the MNIST example fails with a misaligned memory error. My graphics card info from **lspci** is:\n\n01:00.0 VGA compatible controller: NVIDIA Corporation GM107 [GeForce GTX 750 Ti](rev a2)\n\nThe attached file shows the script I'm trying to run, and the results when running with **cuda-memcheck**.\n\nAnd here is some output from **dmesg**. Do you need more information? And thanks again for this wonderful software!\n\n[30573.966313] NVRM: Xid (PCI:0000:01:00): 13, Graphics SM Warp Exception on (GPC 0, TPC 4): Misaligned Address\n[30573.966336] NVRM: Xid (PCI:0000:01:00): 13, Graphics Exception: ESR 0x506648=0x3000f 0x506650=0x0 0x506644=0xd3eff2 0x50664c=0x7f\n[30573.966358] NVRM: Xid (PCI:0000:01:00): 13, Graphics Exception: ChID 0028, Class 0000b0c0, Offset 00001b0c, Data 00000000\n[30829.981713] NVRM: Xid (PCI:0000:01:00): 43, Ch 00000028, engmask 00000101\n\n[tensorflow-error.txt](https://github.com/tensorflow/tensorflow/files/310171/tensorflow-error.txt)\n", "@johnfrombluff Why do you think it is a related error?  Unrelated errors should be filed as separate issues.\n", "What a fool I am. Sorry to waste your time.\n"]}, {"number": 664, "title": "Enable building with CUDA support on Mac OS X", "body": "- building with CUDA support on OS X requires GNU coreutils due to the OS X\n  native readlink command behaving differently from the GNU version - you can\n  install it using homebrew: \"brew install coreutils\"\n- OS X CUDA builds against CUDA toolkit v. 7.5 to overcome host compiler\n  incompatibility - the toolkit versions (CUDA & cuDNN) are now controlled by\n  variables set in the configure script\n- cuda/platform.bzl file is dynamically generated by the configure script to\n  overcome bazel limitations for using \"select\" to set the platform\n  specific names and paths of CUDA libraries\n- SE_STATIC_THREAD_LOCAL_POD now uses __thread instead of thread_local which\n  is not yet supported by the version of clang shipped by Apple. __thread\n  supports only primitive types, but is more performant\n- Updates Eigen to a newer version that fixes a (clang) compilation error\n", "comments": ["Nice! I think this is clearly and improvement over the existing one. I tried on my macbook with cuda and it works. I also published a [small tutorial](https://medium.com/@fabmilo/how-to-compile-tensorflow-with-cuda-support-on-osx-fd27108e27e1#.kaq068rjh) on how to use this patch]. Hope it can be merged soon. @vrv @mrry thoughts ?\n", "Thanks for trying this out @Mistobaan and writing a tutorial!  I'd add a step for installing GNU coreutils (\"brew install coreutils\") to the tutorial - most people probably don't have it installed.\n", "This is very nice!  Thanks for this contribution -- we'll have @zheng-xq take a look at this soon.\n", "@ville-k good point. Updated :)\n", "@leary-google, could you review the stream-execuctor portion of this change? \n", "@Mistobaan I followed your instructions, but \"brew cask install cuda\" defaults to CUDA 7.0, and @ville-k patch is using 7.5 by default.  I tried both.  With CUDA 7.0, I get a compile error as such:\n\n```\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_sin.cu.cc:\nnvcc fatal   : The version ('70002') of the host compiler ('Apple clang') is not supported\nERROR: /Users/fpmc/git/tensorflow/tensorflow/core/BUILD:339:1: error while parsing .d file: /private/var/tmp/_bazel_fpmc/b41e6b4c9df9b99106d3673ec4f590dc/tensorflow/bazel-out/local_darwin-opt/bin/tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/cwise_op_gpu_sin.cu.d (No such file or directory).\n```\n\nBy manually downloading CUDA 7.5 and installing it, it compiles.\n", "@ville-k In testing your change I find that the newly added ALT_PATH doesn't really work.  If you have `libcudnn.6.5.dylib` located inside `/usr/local/cuda/` and not `/usr/local/cuda/lib/`, the symlink command at the end of your `cuda_config.sh` will end up silently creating a bad symlink like this\n\n```\nlibcudnn.6.5.dylib@ -> /usr/local/cuda/lib/libcudnn.6.5.dylib/lib/libcudnn.6.5.dylib\n```\n\nI think having the ALT_PATH stuff is hard to get right.\n", "@fpmchu Thanks for testing the ALT_PATH build scenario for cuDNN!  The fix for the problem you discovered turned out to be pretty simple - I have it on a separate branch for now: \nhttps://github.com/ville-k/tensorflow/commit/1232b37d3596a154a74e162db33c888111eba17d\n\n@vrv What's the project's policy for issues found and fixed during PR review?  New commits on the existing PR or open a separate PR?\n", "New commits on existing PR seems fine -- we'll just ask to squash the commits prior to validation and merging.\n", "@ville-k Cool.  While the fix looks ok, I'm still not understanding the purpose of adding ALT_PATH.  Is it just to allow users to put them in /usr/local/lib?  What's wrong with using /usr/local/cuda/lib?\n", "@fpmchu ALT_PATH is there to support existing lib search path functionality for both linux and mac. The original configure and cuda_config.sh scripts look for cudnn.so.6.5 under both \"/usr/local/cuda\" and \"/usr/local/cuda/lib64\". Depending on the platform, these locations will now be searched if the user inputs \"/usr/local/cuda\" as the cudnn install dir:\n**Linux**\n- /usr/local/cuda/lib64/cudnn.so.6.5 \n- /usr/local/cuda/cudnn.so.6.5  (ALT_PATH)\n\n**Mac**\n- /usr/local/cuda/lib/cudnn.6.5.dylib\n- /usr/local/cuda/cudnn.6.5.dylib  (ALT_PATH)\n", "Thanks @fpmchu to try the tutorial out. I think it installed 7.0 because you have an old cuda formula. I updated the tutorial to suggest to update homebrew first and check for the cuda version. \n", "Thanks @Mistobaan.  I actually think you mean `brew update` though.  I did try \"upgrade\" before and that didn't work.  I didn't know that \"update\" is the thing to do to \"upgrade brew\" :-)\n", "Because of peculiarities in our internal build process, we won't be able to merge this right now. I'll leave this open since it may be useful for people. When we find someone to resolve the internal problems, we may be able to absorb it at a later time.\n\nI'm sorry about that -- I would love to have this in.\n", "Can one of the admins verify this patch?\n", "Thanks for the update @martinwicke ! Is the main issue causing problems with your internal build process the automatic generation of the \"platform.bzl\" file? \n", "That, and reuse of shared code elsewhere. The code elsewhere can be updated\n(internally) which is why we need someone here to fix that up. The problem\nwith stream_executor is that it almost has to be treated like generated\ncode. In sorry this bit you after you've done all this work, I'm still\nhoping we can absorb it somehow, and we'll make the restrictions on\nstream_executor clearer for the future.\nOn Fri, Jan 8, 2016 at 07:25 Ville Kallioniemi notifications@github.com\nwrote:\n\n> Thanks for the update @martinwicke https://github.com/martinwicke ! Is\n> the main issue causing problems with your internal build process the\n> automatic generation of the \"platform.bzl\" file?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/664#issuecomment-170030821\n> .\n", "I get a segfault when the  cuda libs are not setup properly due to `getenv(\"LD_LIBRARY_PATH\")` returning null:\n\n```\n(lldb) bt\n* thread #1: tid = 0x30ae49, 0x00007fff8ed8e752 libsystem_c.dylib`strlen + 18, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x0)\n  * frame #0: 0x00007fff8ed8e752 libsystem_c.dylib`strlen + 18\n    frame #1: 0x0000000105f0d7a4 _pywrap_tensorflow.so`std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::operator<<<std::__1::char_traits<char> >(std::__1::basic_ostream<char, std::__1::char_traits<char> >&, char const*) [inlined] std::__1::char_traits<char>::length(__s=0x0000000000000000) + 52 at string:651\n    frame #2: 0x0000000105f0d790 _pywrap_tensorflow.so`std::__1::basic_ostream<char, std::__1::char_traits<char> >& std::__1::operator<<<std::__1::char_traits<char> >(__os=0x00007fff5fbfbb10, __str=0x0000000000000000) + 32 at ostream:882\n    frame #3: 0x000000010895c96a _pywrap_tensorflow.so`perftools::gputools::internal::DsoLoader::GetDsoHandle(tensorflow::StringPiece, void**, perftools::gputools::internal::DsoLoader::LoadKind) [inlined] std::__1::enable_if<(__os=0x00007fff5fbfbb10, __x=0x00007fff5fbfbb08)) && (is_base_of<std::__1::ios_base, tensorflow::internal::LogMessage>::value), tensorflow::internal::LogMessage&&>::type std::__1::operator<<<tensorflow::internal::LogMessage, char*>(tensorflow::internal::LogMessage&&, char* const&) + 19 at ostream:1057\n<trimmed>\n(lldb) frame select 4\nframe #4: 0x000000010895c957 _pywrap_tensorflow.so`perftools::gputools::internal::DsoLoader::GetDsoHandle(path=(data_ = \"libcuda.dylib\", size_ = 13), dso_handle=0x00007fff5fbfc138, load_kind=kLocal) + 887 at dso_loader.cc:99\n   96     string path_string = path.ToString();\n   97     *dso_handle = dlopen(path_string.c_str(), dynload_flags);\n   98     if (*dso_handle == nullptr) {\n-> 99       LOG(INFO) << \"Couldn't open CUDA library \" << path\n   100                << \". LD_LIBRARY_PATH: \" << getenv(\"LD_LIBRARY_PATH\");\n   101      // TODO(b/22689637): Eliminate unnecessary ToString once StrCat has been\n   102      // moved to the open-sourceable version.\n```\n", "I eventually did get this working but the version of Eigen referenced in here is very broken. Eigen HEAD (fd9611fa2d9c) does work aside from a nvcc build break in TensorIntDiv.h, `DividerHelper<64, T>::computeMultiplier` is missing a cast... but it does at least seem to work.\n\nPrevious failure looked like this:\n\n> libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: mutex lock failed: Invalid argument\n\nAfter a bit of hunting around it turns out that the mutex instances had already been destructed after someone called `exit(1)` :hushed: \n\n```\n(lldb) bt\n* thread #2: tid = 0x4649e, 0x00007fff98e3f738 libsystem_c.dylib`exit, stop reason = breakpoint 1.1\n  * frame #0: 0x00007fff98e3f738 libsystem_c.dylib`exit\n    frame #1: 0x00000001066e9f0e _pywrap_tensorflow.so`void Eigen::internal::EigenMetaKernel_Vectorizable<Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_difference_op<float>, true>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, int>, 16> const> const> const, Eigen::GpuDevice>, int>(Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_difference_op<float>, true>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, int>, 16> const> const> const, Eigen::GpuDevice>, int) + 14\n    frame #2: 0x00000001066e6ec1 _pywrap_tensorflow.so`tensorflow::functor::BinaryFunctor<Eigen::GpuDevice, tensorflow::functor::sub<float>, 1>::Right(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::TensorFixedSize<float const, Eigen::Sizes<>, 1, long>, 16>) + 257\n```\n\nAnd it turns out that all the `EigenMetaKernel_Vectorizable` specializations don't work as intended:\n\n```\n(lldb) disassemble\n_pywrap_tensorflow.so`void Eigen::internal::EigenMetaKernel_Vectorizable<Eigen::TensorEvaluator<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, int>, 16>, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_right<float, float, Eigen::internal::scalar_difference_op<float>, true>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, int>, 16> const> const> const, Eigen::GpuDevice>, int>:\n    0x1066e9f00 <+0>:  pushq  %rbp\n    0x1066e9f01 <+1>:  movq   %rsp, %rbp\n    0x1066e9f04 <+4>:  movl   $0x1, %edi\n    0x1066e9f09 <+9>:  callq  0x1069fe13e               ; symbol stub for: exit\n    0x1066e9f0e <+14>: nop\n```\n", "I'm not able to reproduce the issue with LD_LIBRARY_PATH you mentioned @NathanHowell Which version of OSX and Python are you using?\nI reported the missing cast issue to Eigen before this PR and they fixed it for the version that I'm using (their fix was to add a constructor, not cast the argument). Sounds like they might've had some regression if it's broken in their HEAD. I'll push an update to this PR today with latest from master and try to see if I can find a rev of Eigen that is never and does not have have regression.\n", "@ville-k the segfault might have been from an older version of Xcode, I upgraded to 7.2 trying to track down the other issue and I think it's been fixed... but it should use `DYLD_LIBRARY_PATH` on osx rather than `LD_LIBRARY_PATH` right?\n", "@NathanHowell I was surprised about LD_LIBRARY_PATH working too when I stumbled into it working by accident. Apple added this at some point for UNIX conformance - they're checking both env vars nowadays:\nhttp://www.opensource.apple.com/source/dyld/dyld-360.18/src/dyld.cpp\n", "Worked for me as well with @Mistobaan 's instructions.  Note that `coreutils` has to be installed from brew even if you think it's already installed, because the installer calls `greadlink`, so the brew package is a hard dependency on OS X.  In addition, there should be a better way of handling the `LD_LIBRARY_PATH` issue.  Setting any of those environment variables manually can wreak all sorts of havok on OS X.  Neither theano nor torch needs it set explicitly. \n", "@ville-k would you mind terribly rebasing?  On linux, updates to bazel have created a number of installation issues where fixes were rolled into the git in the last few weeks.\n", "@elbamos I have a rebased/hacked up branch here: https://github.com/NathanHowell/tensorflow/tree/cuda_osx2 (EDIT: cuda_osx3 is broken)\n\nIt also needs a patch to Eigen, not sure if it's 100% correct but it at least builds and runs:\n\n```\n--- ./bazel-tensorflow/external/eigen_archive/eigen-eigen-c8e5d094f3a9/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h    2016-01-25 15:51:44.000000000 -0800\n+++ ./bazel-tensorflow/external/eigen_archive/eigen-eigen-c8e5d094f3a9/unsupported/Eigen/CXX11/src/Tensor/TensorIntDiv.h    2016-01-25 16:04:11.000000000 -0800\n@@ -98,7 +98,7 @@\n       return static_cast<uint64_t>((static_cast<__uint128_t>(1) << (64+log_div)) / static_cast<__uint128_t>(divider) - (static_cast<__uint128_t>(1) << 64) + 1);\n #else\n       const uint64_t shift = 1ULL << log_div;\n-      TensorUInt128<uint64_t, uint64_t> result = (TensorUInt128<uint64_t, static_val<0> >(shift, 0) / TensorUInt128<static_val<0>, uint64_t>(divider) - TensorUInt128<static_val<1>, static_val<0> >(1, 0) + TensorUInt128<static_val<0>, static_val<1> >(1));\n+      TensorUInt128<uint64_t, uint64_t> result = (TensorUInt128<uint64_t, static_val<0> >(shift, 0) / TensorUInt128<static_val<0>, uint64_t>(static_cast<uint64_t>(divider)) - TensorUInt128<static_val<1>, static_val<0> >(1, 0) + TensorUInt128<static_val<0>, static_val<1> >(1));\n       return static_cast<uint64_t>(result);\n #endif\n     }\n```\n", "@elbamos I started a rebase over the weekend, but ran into the Eigen issue also reported here: https://github.com/tensorflow/tensorflow/issues/883\nI'll give it another try tomorrow.\n", "@elbamos I just rebased and pointed this PR to temporarily use my fork of Eigen that builds on OSX (Eigen PR is pending)\n", "@ville-k but I thought it was working on OS X as it was?\n", "Anyway here's what I get now with gcc 5.2:\n\n```\nINFO: From Compiling tensorflow/core/kernels/xent_op_gpu.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n/usr/lib/gcc/x86_64-linux-gnu/5/include/mwaitxintrin.h(36): error: identifier \"__builtin_ia32_monitorx\" is undefined\n\n/usr/lib/gcc/x86_64-linux-gnu/5/include/mwaitxintrin.h(42): error: identifier \"__builtin_ia32_mwaitx\" is undefined\n\n2 errors detected in the compilation of \"/tmp/tmpxft_000041a1_00000000-10_xent_op_gpu.cu.compute_52.cpp1.ii\".\nERROR: /mnt/hfsshare/LIBRARIES/tensorflow/tensorflow/core/BUILD:339:1: output 'tensorflow/core/_objs/gpu_kernels/tensorflow/core/kernels/xent_op_gpu.cu.o' was not created.\nERROR: /mnt/hfsshare/LIBRARIES/tensorflow/tensorflow/core/BUILD:339:1: not all outputs were created.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 34.791s, Critical Path: 34.04s\n```\n\nAnd with 4.9:\n\n```\nINFO: Found 1 target...\nERROR: /mnt/hfsshare/LIBRARIES/tensorflow/tensorflow/stream_executor/BUILD:7:1: undeclared inclusion(s) in rule '//tensorflow/stream_executor:stream_executor':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/stream_executor/cuda/cuda_rng.cc':\n  '/usr/local/cuda-7.5/include/cuda_runtime.h'\n  '/usr/local/cuda-7.5/include/host_config.h'\n  '/usr/local/cuda-7.5/include/builtin_types.h'\n  '/usr/local/cuda-7.5/include/device_types.h'\n  '/usr/local/cuda-7.5/include/driver_types.h'\n  '/usr/local/cuda-7.5/include/surface_types.h'\n  '/usr/local/cuda-7.5/include/texture_types.h'\n  '/usr/local/cuda-7.5/include/vector_types.h'\n  '/usr/local/cuda-7.5/include/channel_descriptor.h'\n  '/usr/local/cuda-7.5/include/cuda_runtime_api.h'\n  '/usr/local/cuda-7.5/include/host_defines.h'\n  '/usr/local/cuda-7.5/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-7.5/include/driver_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.hpp'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 9.779s, Critical Path: 9.27s\n```\n", "I have this error on mac os x\n\n```\ntensorflow/stream_executor/cuda/cuda_dnn.cc:843:48: error: expected body of lambda expression\n  auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n                                               ^\n./tensorflow/core/platform/default/thread_annotations.h:83:3: note: expanded from macro 'SHARED_LOCKS_REQUIRED'\n  THREAD_ANNOTATION_ATTRIBUTE__(shared_locks_required(__VA_ARGS__))\n  ^\n./tensorflow/core/platform/default/thread_annotations.h:42:42: note: expanded from macro 'THREAD_ANNOTATION_ATTRIBUTE__'\n#define THREAD_ANNOTATION_ATTRIBUTE__(x) __attribute__((x))\n```\n", "This is due to an old version of Clang/XCode. You probably just need to\nupgrade to XCode 7.x.\nOn Feb 3, 2016 07:12, \"Geka000\" notifications@github.com wrote:\n\n> I have this error on mac os x\n> \n> tensorflow/stream_executor/cuda/cuda_dnn.cc:843:48: error: expected body of lambda expression\n>   auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n>                                                ^\n> ./tensorflow/core/platform/default/thread_annotations.h:83:3: note: expanded from macro 'SHARED_LOCKS_REQUIRED'\n>   THREAD_ANNOTATION_ATTRIBUTE__(shared_locks_required(**VA_ARGS**))\n>   ^\n> ./tensorflow/core/platform/default/thread_annotations.h:42:42: note: expanded from macro 'THREAD_ANNOTATION_ATTRIBUTE__'\n> #define THREAD_ANNOTATION_ATTRIBUTE__(x) **attribute**((x))\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/664#issuecomment-179285763\n> .\n", "Can one of the admins verify this patch?\n", "@martinwicke I changed this PR to use a checked in platform.bzl file rather than autogenerating it in the configure script. Hopefully this will no longer break your internal build system and will allow this PR to be merged.  I also [worked with](https://bitbucket.org/eigen/eigen/pull-requests/158/add-constructor-for-long-types/diff) the Eigen project to fix a compilation error that was preventing Eigen from building under Cuda 7.5 - this PR branch points to the latest Eigen with that fix. Here's a quick summary of how building under OSX with/without Cuda support now works:\n- when TF_UNOFFICIAL_SETTING=1 env variable is set while running the configure script, the configure script replaces the platform name, CUDA version and cuDNN version constants inside platform.bzl to match user selections.  For OSX, you need to select Cuda version 7.5.\n- unmodified platform.bzl file is used when configure is run in regular mode (i.e. not using unofficial settings) - OSX Cuda support will not work in this case.\n", "Hey @martinwicke let me know if there is there are any more changes needed to merge this PR. I'll be happy to address any concerns.\n", "I built the latest version of this PR successfully. Thanks @ville-k!\n\nUsing @Mistobaan's gist as a starting point, I wrote updated/expanded build instructions for those less familiar with building tensorflow:\n\nhttps://gist.github.com/ageitgey/819a51afa4613649bd18\n\nHope that helps out someone who is looking to try this.\n", "Hey @vrv or @martinwicke could you guys give some feedback based on the changes I made and summarized in my previous comment. \n", "I think the biggest problem mentioned in https://github.com/tensorflow/tensorflow/pull/664#issuecomment-170044135 is the stream executor bits -- we don't own that code and it gets upstreamed a lot, so even if we accepted the change, it would likely get overwritten by our next upstreaming :(\n\nWe'll try to figure out a better story for stream_executor soon -- the same problem exists for Windows too.\n", "@ageitgey your [instructions](https://gist.github.com/ageitgey/819a51afa4613649bd18) worked great! I'm now using my GeForce GT 750M with 2GB of RAM to train with. :-) (Did this on OS X Yosemite - 10.10.5 - MacBook Pro Retina 15-inch Mid 2014)\n", "I have also followed the instruction above and all the building activities works fine but when I start python and try to import tensorflow I get this error:\n\n```\nMatsMacBookPro2:~ mats$ python\nPython 2.7.10 (default, Oct 23 2015, 18:05:06) \n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import tensorflow\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\n    from tensorflow.python import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 37, in <module>\n    from tensorflow.core.framework.graph_pb2 import *\n  File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 10, in <module>\n    from google.protobuf import descriptor_pb2\n  File \"/Library/Python/2.7/site-packages/google/protobuf/descriptor_pb2.py\", line 1533, in <module>\n    __module__ = 'google.protobuf.descriptor_pb2'\n  File \"/Library/Python/2.7/site-packages/google/protobuf/reflection.py\", line 123, in __new__\n    new_class = superclass.__new__(cls, name, bases, dictionary)\nTypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases\n```\n\nI have a MacBook Pro Retina, 15-inch, Late 2013, GeForce GT 750M, 16GB RAM, running OS X 10.11.3\n", "Check the installation instructions on the website for this one -- it's a\ncommon issue, probably reinstalling protobuf will fix it.\n\nOn Thu, Feb 25, 2016 at 1:52 PM Mats Berggrund notifications@github.com\nwrote:\n\n> I have also followed the instruction above and all the building activities\n> works fine but when I start python and try to import tensorflow I get this\n> error:\n> \n> MatsMacBookPro2:~ mats$ python\n> Python 2.7.10 (default, Oct 23 2015, 18:05:06)\n> [GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\n> Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n> \n> > > > import tensorflow\n> > > > Traceback (most recent call last):\n> > > >   File \"<stdin>\", line 1, in <module>\n> > > >   File \"/Library/Python/2.7/site-packages/tensorflow/**init**.py\", line 23, in <module>\n> > > >     from tensorflow.python import *\n> > > >   File \"/Library/Python/2.7/site-packages/tensorflow/python/**init**.py\", line 37, in <module>\n> > > >     from tensorflow.core.framework.graph_pb2 import *\n> > > >   File \"/Library/Python/2.7/site-packages/tensorflow/core/framework/graph_pb2.py\", line 10, in <module>\n> > > >     from google.protobuf import descriptor_pb2\n> > > >   File \"/Library/Python/2.7/site-packages/google/protobuf/descriptor_pb2.py\", line 1533, in <module>\n> > > >     **module** = 'google.protobuf.descriptor_pb2'\n> > > >   File \"/Library/Python/2.7/site-packages/google/protobuf/reflection.py\", line 123, in **new**\n> > > >     new_class = superclass.**new**(cls, name, bases, dictionary)\n> > > > TypeError: metaclass conflict: the metaclass of a derived class must be a (non-strict) subclass of the metaclasses of all its bases\n> \n> I have a MacBook Pro Retina, 15-inch, Late 2013, GeForce GT 750M, 16GB\n> RAM, running OS X 10.11.3\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/664#issuecomment-189002621\n> .\n", "unfortunately I have tried all that. From what I can see I have the correct protobuf installed:\n\n> > > import google.protobuf\n> > > print google.protobuf.**version**\n> > > 3.0.0a3\n", "@matspetter Can you try again after updating protobuf again (to b2)? This may be resolved now.\n", "Well, I spent about 10 hours trying to get this to work before finding this thread. I had to make a 50 changes and realized that you had actually made virtually all the same ones, plus another 50!\n\nI followed your instructions (except brew cask install cuda because I had already installed that earlier and tested it with the CUDA toolkit and it was working). Unfortunately, even though I got way further than I ever had before, it failed again, with the first command bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer.\n\nIt failed with 15 errors generated, almost all of them in tensorflow/stream_executor/cuda/cuda_dnn.cc\n\nThis is so frustrating. I don't understand why it's so hard to get this to work for macs.\n", "Is there any progress on checks for this? I notice that it has been over 90 days since the original post.\n\nAlso, is there any reason that bazel should not be compiling with cuda_dnn v5?\n", "For cudnn v5 see #1786.\n", "actually what I did was to make sure which versions of python I had installed. I had a whole mess of different versions from macports, osx itself (?), homebrew etc. I removed it all and reinstalled all from homebrew. Then I got it working\n", "I have a MBP 2015, a Vidock + with external Nvidia 680 and running El Capitan (10.11.3), and the branch work fine. Really good contribution, thank you ville-k !!!\n", "@ville-k: any interest in reviving this CL against master?  I think we'd like to merge these changes and then we'll try to keep it working as best we can.\n", "@vrv Absolutely!  I'll start rebasing tonight.\n", "Fantastic!!!\n\nSent from my iPhone\n\nOn Apr 18, 2016, at 9:35 AM, Ville Kallioniemi <notifications@github.com<mailto:notifications@github.com>> wrote:\n\n@vrvhttps://github.com/vrv Absolutely! I'll start rebasing tonight.\n\n## \n\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHubhttps://github.com/tensorflow/tensorflow/pull/664#issuecomment-211380353\n", "@vrv I rebased my branch and removed some redundant functionality. The latest version of apple clang does [not appear to work](https://devtalk.nvidia.com/default/topic/879431/nvcc-amp-clang-7-no-typo-here-/?offset=20) with Cuda 7.5 and you'll have to downgrade to version 7.0.2. I'm using these configure options and bazel incantations to successfully build on OSX:\n1. `./configure` with these options\n   Host compiler: /usr/bin/clang\n   Cuda SDK version: 7.5\n   Cuda SDK location: /Developer/NVIDIA/CUDA-7.5\n   CuDnn version: 6.5\n   CuDnn location: /Developer/NVIDIA/CUDA-6.5\n2. `bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`\n3. `./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\n4. `pip install /tmp/tensorflow_pkg/tensorflow-0.8.0rc0-py3-none-any.whl`\n", "Nice! I tested on my machine with cudnn v5.0 and it works. [Updated the tutorial to help installing the previous version of Xcode 7.2 to make it compile](https://medium.com/@fabmilo/how-to-compile-tensorflow-with-cuda-support-on-osx-fd27108e27e1#.lqbb05men)\n", "Doh, sorry for not getting to this -- looks like there are a few more conflicts.  I left some comments but this generally looks great -- nice refactoring!\n\ncc @zheng-xq \n", "@vrv Rebased and addressed code review comments.\n", "Okay, this generally looks good!  Let's weed out any test failures: test this please, @tensorflow-jenkins \n", "Good work on the update.\n\nThere may still be an issue with cudnn v5 when packaged/installed with pip.\n\nFor both cudnn v4 and v5, the compile completes successfully. Invoking an example trainer,\nbazel-bin/tensorflow/cc/tutorials_example_trainer, runs successfully with BOTH libraries:\n\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library **libcudnn.4.dylib or ibcudnn.5.dylib** locally\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.7.5.dylib locally\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.dylib locally\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.7.5.dylib locally\n> > > I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:883] OS X does not support NUMA - returning NUMA node zero\n> > > I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\n> > > name: GeForce GTX 980 Ti\n> > > major: 5 minor: 2 memoryClockRate (GHz) 1.076\n> > > ...\n> > > 000001/000009 lambda = 2.000000 x = [0.894427 -0.447214] y = [1.788854 -0.894427]\n\nHowever, when packaged and installed with pip, a compile with cudnn 4 works as expected but cudnn 5 will fail on load of the cudnn.5.dylib library:\n\n> > > import tensorflow as tf\n> > > I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.7.5.dylib locally\n> > > Segmentation fault: 11\n\nThis is the point at which libcudnn.5.dylib loads. Since cudnn 5 works with the example trainer, the packaging must be breaking something, perhaps a misplaced non-versioned symlink? Odd that cudnn 4 works though. Perhaps a misplaced non-versioned symlink? \n", "@markb729: right now we don't package one binary that works with both cudnn4 and 5, because the APIs are different.\n\nAt some point I think it would be conceivable to implement the stream executor in such a way that different cudnn versions are different implementations of the same interface, and we dispatch exactly once during initialization to the right one.\n", "@tensorflow-jenkins test this please \n", "One more try!   test this please\n", "Woohoo!!  thank you so much for this contribution.  We'll try our best to keep it working, though without OS X / GPU test machines, we can't promise too much.\n", "@vrv Awesome! I really appreciate your thoughtful feedback and help in figuring out the build issues!\n", "Finally! Thanks for all the hard work! I will get an external GPU so we can test this and make sure it continues to work.\n", "Nice ! \ud83d\udc4d   \n", "Hi guys, I'm new here. I was wondering if there shouldn't be a \"MacOS **G**PU Tests\" in the lists of tests being run at http://ci.tensorflow.org to make sure the new CUDA/GPU functionality keeps working?\n", "Yes. We're installing hardware for that over the weekend. A test should\ncome some time next week.\nOn Fri, May 13, 2016 at 17:09 Christian Hansen notifications@github.com\nwrote:\n\n> Hi guys, I'm new here. I was wondering if there shouldn't be a \"MacOS _G_PU\n> Tests\" in the lists of tests being run at http://ci.tensorflow.org to\n> make sure the new CUDA/GPU functionality keeps working?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/664#issuecomment-219187149\n", "I'm trying to build with CUDA 8.0, CuDNN 5.0, and clang-703.0.31 (CUDA test projects seem to build just fine).\n\nI get the following error:\n\nINFO: Found 1 target...\nINFO: From Executing genrule //third_party/gpus/cuda:cuda_config_check [for host]:\n/bin/bash: greadlink: command not found\nERROR: /Projects/tensorflow/third_party/gpus/cuda/BUILD:204:1: declared output 'third_party/gpus/cuda/cuda.config' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).\nERROR: /Projects/tensorflow/third_party/gpus/cuda/BUILD:204:1: not all outputs were created.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nINFO: Elapsed time: 120.460s, Critical Path: 0.23s\n\nAny ideas?\n\nEdit: Oops! Never mind, I forget to install coreutils. I'm running into more trouble with CUDA 8.0, but it looks like these are known issues not related to this ticket.\n", "Since Apple does not sell any computers with Nvidia GPUs,  could you tell us what hardware you are using this with ? Is is some sort of Thunderbolt-attached GPU enclosure with an Nvidia card in it ?\n", "Once-upon-a-time Apple did provide computers with NVIDIA chips. I have a 2012 MacBook Pro with a 650M. \n", "Yes. I have the same the same MacBook pro with 650m card.\n", "I have a 2014 MacBook Pro with a 750m card which is still acceptably recent.\nI hope Apple is going to offer NVIDIA as an option in some future Macs, if not this year maybe next year.\nAnd of course there's also [iBuildMacs.com](https://ibuildmacs.com/) and [create.pro](https://create.pro/)\n", "@martin-gorner Apple sells refurbished laptops with NVidia, ie, http://www.apple.com/shop/product/FE294LL/A/refurbished-154-inch-macbook-pro-23ghz-quad-core-intel-i7-with-retina-display\n", "Has anyone tried this with GPU in a Thunderbolt enclosure ? This video seems to imply that CUDA works well in that situation: https://youtu.be/Bsf9lHM8qLk\n", "Our (new) Mac GPU tests run in just such a setup: Mac Pro + Quadro M4000 in\na Bizon2.\nOn Thu, Jul 28, 2016 at 01:56 martin-gorner notifications@github.com\nwrote:\n\n> Has anyone tried this with GPU in a Thunderbolt enclosure ? This video\n> seems to imply that CUDA works well in that situation:\n> https://youtu.be/Bsf9lHM8qLk\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/664#issuecomment-235838384,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_Y_Kae-uJER4i9bXzTIJg8y5tqBdks5qaG6ggaJpZM4G9X8_\n> .\n", "> Once-upon-a-time Apple did provide computers with NVIDIA chips. I have a 2012 MacBook Pro with a 650M.\r\n\r\ni have the same mac pro. do you have it using gpu?i had been trying for 3 days trying to find good tutorials to make it work but nothing."]}, {"number": 663, "title": "Cross-compile Android demo for CUDA", "body": "I was wondering how can I build the android demo with GPU support. I have a device with CUDA capability (Tegra K1 device). What I did was to pass a `--config=cuda` when building the demo. Is it supposed to be enough to enable gpu support by only passing  `--config=cuda`?\n\nI ./configured the workspace given cuda and cudnn path and also specified proper `compute_capability=3.2`\nNow that the app is running on my device, how can I make sure if it has gpu support? I did not see a noticeable speed up by doing this. So, What is the proper way to run and validate it?\n\nP.S.\nI am using CUDA 6.0. \nI modified few lines to ignore CUDA 7.5 and use 6.0 instead.\n", "comments": ["If you want to make sure you are placing ops on the GPU, you can find a test that can use the GPU (such as the [RoundTest](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops_test.py#L45)), and modify it so the session is created with `force_gpu=True`:\n\n```\nwith self.test_session(use_gpu=True, force_gpu=True):\n```\n\nIf the test then fails, that's a strong indicator you're not running a GPU. The RoundTest is a good one because it has only ops that can actually run on a GPU, other tests have a mixture, and `force_gpu` is pretty remorseless.\n", "Thanks @martinwicke. Actually, the code you mentioned (RoundTest) is written in Python. What I need is a C++ code that I can test it on my android device. So, I need a piece of code that can be compiled with GPU=on and has the capability to be run under my android device. \nI can not test it on my PC, as it does not come with a GPU device. \n\nDo you know how can I tell bazel to use gpu code rather than scalar version. And does that work for the android demo? I know I can pass --config=cuda for the running `//tensorflow/cc:tutorials_example_trainer`. But it doesn't seem to be enough for the android demo.\n", "`--config=cuda` will compile for GPU, but that doesn't mean that all ops are necessarily run on GPU. If you have a `SessionOptions` object, you can `set_log_device_placement(true)`, which will log the placement of ops, and you can check that they actually run on GPU.\n", "Thank you @martinwicke . I will test that option to make sure and will post the result here.\nBut according to the unchanged performance of the process I guess it is not gpu enabled. \n(I mistakenly close the issue and reopened it)\n", "I tried to log the used device in native jni part. However, setting `log_device_placement` is not as straight forward as in Python. \nAs a desperate try I did the following change into the Android demo:\n\n```\n    tensorflow::GraphDef tensorflow_graph;\n    LOG(INFO) << \"Graph created.\";\n    graph::SetDefaultDevice(\"/gpu:0\", &tensorflow_graph);\n```\n\nStill not sure if it uses CUDA device or not.\n", "One more update:\n`config.device_count_size()` returns 0 at run-time??\n", "You ran configure with TF_UNOFFICIAL_SETTING, right?\n\nOn Thu, Jan 7, 2016 at 10:36 PM Hamid Bazargani notifications@github.com\nwrote:\n\n> One more update:\n> config.device_count_size() returns 0 at run-time??\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/663#issuecomment-169910128\n> .\n", "That's correct.\n", "The set_log_device_placement is a c++ function on SessionOptions -- nothing in c++ is as straightforward as in python, but it shouldn't be hard to do if you're running everything from c++ anyway.\n", "http://cudamusing.blogspot.com/2015/11/building-tensorflow-for-jetson-tk1.html might be a relevant post from shortly after our release -- lots has changed but there might be some helpful hints there.\n", "@martinwicke I don't find (grep) `set_log_device_placement`. There is no such a method. Can you provide more details.\n@vrv I have seen it. I got some of my modifications from that post. However, it is for Jetson device with Linux_for_Tegra that makes the story different.\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/config.proto#L92\n\nWhich is a structure within https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/public/session_options.h#L58\n\nwhich you can pass to the Session() constructor\n", "Sorry I'm not familiar with Proto message format.\nI set the `log_device_placement` to true. And it seems everything is running on cpu0.\nAt least I know on which device the process is being executed.\nNow the main question is still left open. How to make it run on gpu?\n", "BTW, \nI am able to build other examples like example_trainer with CUDA. \nMy problem is the android demo.\n", "Duh. Sorry, I should have noticed that. It's quite possible that the build rules for android are not properly set up for GPU support at all. Assigning someone with better knowledge about mobile issues.\n", "The Android TF build targets do not currently support CUDA.\n\nA major reason for this is that Tensorflow currently requires CUDA 7.0 and the NVIDIA Codeworks support for 7.0+ is very limited -- currently only the Shield Android TV is listed as 7.0 capable (see http://docs.nvidia.com/gameworks/index.html#developertools/mobile/codeworks_android/codeworks_android_release_notes.htm).\n\nThis could change in the future if support improves for CUDA on Android -- though if you get something working on your own please let us know!\n", "Is there any plan to use renderscript on android devices?\n", "Thanks @andrewharp. \nI was aware about the point. however, I like to use an unofficial approach. \nI was able to solve all compatibility issues. \nAssuming a CUDA 7.0 device, how we can tell bazel to build the android demo with GPU support?\nPassing --config=cuda does not seem to work for the Adnroid demo, but it worked for other native examples on x86.\nSome useful effort on bringing TF to Cuda < 7.0 (Jetson TK1) at\nhttp://cudamusing.blogspot.ca/2015/11/building-tensorflow-for-jetson-tk1.html\n", "@hamidb This is something that possibly may come in the future, but we can't support at this time.\n\nIf you wish to proceed on your own the best pointer I can give you is to try the NVIDIA Codeworks combined with a custom Bazel build rule such as [this](https://gist.github.com/andrewharp/e221ca9ae629561442e3) (you would need to modify it obviously), which would let you compile the kernels into libraries that you could then link in as dependencies. I can't speak to any further issues you would encounter, though.\n", "@bhack We have been working with the Renderscript team (they've recently introduced some intrinsics for key operations we need like GEMMs) but we don't have an estimate for when and how that will be exposed.\n", "@petewarden Yes I asked because I've noted in the API level 24.\n", "@petewarden I am very in interested in using the GPU on Android with tensorflow. Does Tensorflow  have mobile GPU support with Renderscript or OpenCL currently ? \n", "TensorFlow doesn't have Renderscript support in yet, and OpenCL is not in there either.\n", "For  OpenCL you can follow https://github.com/tensorflow/tensorflow/issues/22\n", "I believe the original issue was addressed, closing due to lack of recent activity.\n", "I just wanted to see what the plans were for Renderscript or CUDA support for TF on Android. Can we have any updates on future releases or ways of using the GPU for the Android demo ? Thanks :)\n", "I'm here to bump my earlier question.", "Someone from qualcomm just presented at TF dev summit saying 8x speed-up of image recognition on qualcomm chips using TensorFlow with Hexagon library optimization", "Just want to ask what's the stage of GPU support for Android platform? I ask this because it's a very important feature for any deep learning framework, and now there are already some related research papers published. If tensorflow supports GPU on Android platform, I guess, there be many more followers :)", "Hi @llhe \u2014\u00a0please don't use TensorFlow GitHub issues for promotional purposes, it's off-topic. We are glad that there are many competing open source frameworks, but let's stay on-topic for TensorFlow assistance within the TensorFlow Github organization. That will help us all use our time most efficiently to help users.\r\n"]}, {"number": 662, "title": "optimize slice_input_producer", "body": "When shuffle=False, input_slice_producer can be implemented directly with FIFOQueue.\nThe original implementation using range and array indexing is slower.\n\nI could pass the unit test `input_test.py`. But please correct me if I could use the name and `set_shape` in a better way (i'm not totally sure).\n\nI simply run it on my old laptop on CPU and I saw `input_slice_producer + batch` operations (which is a no-op) introduces no overhead, while with the original implementation I see obvious overhead.\n", "comments": ["One issue with this change is that it makes multiple copies of the input data (by default 32), which could have an unfortunate effect on memory usage. Perhaps a cleaner optimization would be to produce intervals (rather than ranges), which could be used to `tf.slice()` the appropriate elements from the input tensor(s), rather than `tf.gather()` them?\n", "Thanks. I'm not sure what do you mean. What's the difference between **interval** and **range**?\n\nAnd also, why do you think a slice operation would be faster than gather? It looks like they are doing the same thing since we only need to produce one element at a time.\n", "Can one of the admins verify this patch?\n", "Sorry for the delay in getting back to you! Looking at your PR more closely, it seems like I misunderstood the nature of your change (and the nature of `slice_input_producer()`, which I had mistakenly assumed produces batches rather than single elements...). I'm still a bit unclear about why your change makes things more efficient: as far as I can tell, the change will end up copying more data out of `input_tensor` on each iteration, which is potentially wasteful. Do you have details of a benchmark that shows the improvement?\n\nLooking at the implementation, it also looks like the `array_ops.gather()` is the only thing that your change avoids, but at the expense of copying the input tensor into the queue repeatedly, which could potentially lead to a performance regression. It seems like you could get almost all of the benefit by replacing the `array_ops.gather()` with an `array_ops.slice()`, which in many cases could avoid the copy altogether.\n\nDoes that make sense?\n", "``` python\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport sys\nimport cv2\n\ndef get_output(img):\n    img = tf.train.slice_input_producer([img], shuffle=False, capacity=32)\n    img = tf.train.batch(img, 128)\n\n    img = tf.reshape(img, [-1, 28 * 28 * 3])\n    W = tf.get_variable('W', [28 * 28 * 3, 10],\n                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n    out = tf.matmul(img, W)\n    return tf.sigmoid(out)\n\nG = tf.Graph()\nwith G.as_default():\n    FAKE = np.random.rand(28, 28, 3)\n    img = tf.constant(FAKE, dtype=tf.float32, shape=[28, 28, 3])\n    img = tf.train.batch([img], 128)\n    output = get_output(img)\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n\n    coord = tf.train.Coordinator()\n    tf.train.start_queue_runners(\n        sess=sess, coord=coord, daemon=True, start=True)\n\n    start = time.time()\n    for k in range(5):\n        sess.run(output)\n        print k\n    print time.time() - start, \" seconds\"\n    coord.request_stop()\n```\n\nI made a script for testing. On my laptop without GPU, I see the following numbers:\n- Run with 0.6.0 release: 16s.\n- With my patch: 0.25s\n- Without `slice + batch` (delete the first two lines in `get_output`): 0.14s\n\nAnd it surprises me that a `slice + batch` can slow down the run time so much. Last time I tested (with real data and models) the difference wasn't that much.\nI'm also interested in why that happens. Maybe there are ways to optimize this script that I didn't notice.\n\nAlso, I noticed that if I passed a constant variable of shape 128x28x28x3 to `get_output` instead, there is no performance issue anymore.\n", "I think that there's a better way to solve your problem:\n\n``` python\ndef get_output(img):\n    img = tf.train.limit_epochs([img], num_epochs=None)  # Produce infinitely many times.\n    img = tf.train.batch(img, 128, enqueue_many=True)  # Enqueues a batch at a time.\n\n    img = tf.reshape(img, [-1, 28 * 28 * 3])\n    W = tf.get_variable('W', [28 * 28 * 3, 10],\n                        initializer=tf.truncated_normal_initializer(stddev=0.1))\n    out = tf.matmul(img, W)\n    return tf.sigmoid(out)\n```\n\nOn my workstation, this modified version of your benchmark takes between 0.05 and 0.08 seconds.\n", "In this case of course, this script is just made to test the speed of `input_slice_producer.`\nIf for example I want to apply some image processing functions (random brightness, etc) on each image in a batch, then I may still need to use slice producer to take single image from a batch.\n", "Can one of the admins verify this patch?\n", "What's the status of this?\n", "Closing due to inactivity.\n"]}, {"number": 661, "title": "The picture in tf.gather(params, indices, name=None) made mistake", "body": "The picture in tf.gather is wrong. It is P_5 in the `params` that should point to P_5 in `indices`. \n", "comments": ["Duplicate of #448\n"]}, {"number": 660, "title": "Remove all {#anchor} tags in non-API Markdown docs", "body": "This commit does two things:\n1. Removes all '{#anchor}' syntax from Markdown header lines in the codebase\n2. Replaces all links that used those #anchors with links conforming to\n   Github/Tensorflow website auto-generated anchors\n\nThe work is a continuation on #648.\n\nThis commit does not change the API docs, as those Markdown files are\nauto-generated.\n\nGenerated using Python script available here:\nhttps://github.com/samjabrahams/tensorflow_util/blob/master/py/change_header_anchor_links.py\n\nCross-page links changed by hand using this script to find non-api\nanchor links:\nhttps://github.com/samjabrahams/tensorflow_util/blob/master/py/get_anchor_references.py\n", "comments": ["Messed up apostrophes are taken care of.\n", "I'll test internally to make sure this matches exactly the pipeline other\ntools have. I was pretty sure that's true, but the trailing '-' scared me a\nlittle. Worst case, we'll have to include only anchors with a trailing '-',\nstill a big improvement.\n\nOn Sun, Jan 3, 2016 at 7:56 PM Vijay Vasudevan notifications@github.com\nwrote:\n\n> Assigned #660 https://github.com/tensorflow/tensorflow/pull/660 to\n> @martinwicke https://github.com/martinwicke.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/660#event-504391321.\n", "From what I've seen, you only end up with a trailing anchor when you do something odd like leaving space between the final punctuation words. Something like \"This header trails off ...\". The last \"create_anchor_from_header\" function I posted works for the tests I played around with [here](https://github.com/samjabrahams/tensorflow_util/blob/master/md/github_header_test.md)\n\nI'm going to reapply the latest function to double check that there aren't any edge cases in the changed files.\n", "I looked, there aren't, so don't worry. I'll merge once my tests are done.\n\nOn Mon, Jan 4, 2016 at 12:08 PM Sam Abrahams notifications@github.com\nwrote:\n\n> From what I've seen, you only end up with a trailing anchor when you do\n> something odd like leaving space between the final punctuation words.\n> Something like \"This header trails off ...\". The last\n> \"create_anchor_from_header\" function I posted works for the tests I played\n> around with here\n> https://github.com/samjabrahams/tensorflow_util/blob/master/md/github_header_test.md\n> \n> I'm going to reapply the latest function to double check that there aren't\n> any edge cases in the changed files.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/660#issuecomment-168789604\n> .\n", "I'm good to merge this -- but this only does os_setup.md, correct? I'll change the title appropriately, or alternative, you could run this on all the files.\n", "This should change all non-API docs that have {#anchor} notation, and every link that points to those anchors both internally and from other files. See the 7 changed files.\n\nIt looks like most of the documentation stopped using the {#anchor} notation aside from a few files and the auto-generated API docs\n", "Great. Thanks a lot for this!\n"]}, {"number": 659, "title": "Fix typo", "body": "Fix typo\n", "comments": []}, {"number": 658, "title": "zombie process resulting from using many GPUs", "body": "I run the example cifar10_multi_gpu_train.py on a PC equipped with 4 GPUs. Somehow the program occupies 4 GPUs, but only the first GPU is doing computation.\n\nWhen I hit ctrl + c, the python scripts did not terminate, and I found zombie process using htop (the zombie process is associated with command python). After that I have to reboot using \"reboot -nf\" to gain access to GPUs. Tensorflow was installed from source (https://github.com/tensorflow/tensorflow.git).\n\n```\n  File \"cifar10_multi_gpu_train.py\", line 270, in <module>\n    tf.app.run()\n  File \"/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"cifar10_multi_gpu_train.py\", line 266, in main\n    train()\n  File \"cifar10_multi_gpu_train.py\", line 236, in train\n    _, loss_value = sess.run([train_op, loss])\n  File \"/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 373, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/home/pc/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 433, in _do_run\n    target_list)\nKeyboardInterrupt\n```\n\nHowever, if I do export CUDA_VISIBLE_DEVICES=0 before running the example to limit it to one GPU, I am able to terminate the python scripts normally using ctrl + c.\n\nnvidia-smi\n![screenshot 2015-12-31 10 58 57](https://cloud.githubusercontent.com/assets/12500045/12065926/a9860536-afad-11e5-8b5d-12fdb343327d.png)\n\nhtop\n![screenshot 2015-12-31 11 01 15](https://cloud.githubusercontent.com/assets/12500045/12065944/da739de8-afad-11e5-937b-dcfe1fc3591d.png)\n", "comments": ["I noticed a similar bug as well, it also is a function of when you do ctrl-c, quite often I've had TensorFlow crash the driver. I presume there must be a NVIDIA driver bug as well, since a user process should never be able to crash a kernel driver...\n", "@wchan I was able to do multi-GPU training with tensorflow 0.5, but not with tensorflow 0.6. Did you find this problem? When you use multiple GPUs, do all GPUs work normally? When you do ctrl+c, does it crash the driver every time?\n\nI was using the official example cifar10_multi_gpu_train.py to test multi-GPU training\n", "I was running in to the same issue. Specifically, I could replicate it by\n1. starting a python prompt and running\n   \n   ```\n   import tensorflow as tf\n   sess = tf.InteractiveSession()\n   ```\n2. killing the python process\n\nThe python process would not die and any other code that used the GPU driver (including simply running nvidia-smi) would hang on a read to /dev/nvidiactl. Setting CUDA_VISIBLE_DEVICES prevented the problem.\n\nAfter turning on persistence mode (as below) I no longer observed this problem.\n\n```\n$ nvidia-smi -pm 1\n```\n", "@mcogswell what does persistence mode do?  Does this solve multi-GPU training case?\n", "[Persistence mode](http://docs.nvidia.com/deploy/driver-persistence/index.html#persistence-mode) keeps the driver loaded even when not used. I'm not sure about the cifar10 example script, but I see no reason why it should be different.\n", "Same problem here, we are using Titan Z which has dual GPU. The python process would not die and any other code that used the GPU driver (including simply running nvidia-smi) would hang. export CUDA_VISIBLE_DEVICES=0 solved the probelm\n", "This could be related to this problem:  \n\nhttps://groups.google.com/forum/#!msg/torch7/kLusyLEj4oc/uwbK1GiQDwAJ\n\nThis is an issue associated with kernel version and cuda drivers when multiple GPUs are being used.   \n\nUsing Nicholas Leonard's instructions at the end solved this issue for me, without having to use CUDA_VISIBLE_DEVICES.\n", "Closing because yamins81 provided a solution.  Feel free to reopen if this is still a problem!\n", "I wish this issue is reopened since @yamins81's solution still doesn't solve the problem of not being able to use multiple GPUs.\n"]}, {"number": 657, "title": "MemoryError - Convolutional.py ", "body": "I followed the pip install of TensorFlow and when trying the \"Test your installation\" I recevied the following output with a MemoryError.\n\n```\nExtracting data/train-images-idx3-ubyte.gz\nTraceback (most recent call last):\n  File \"convolutional.py\", line 290, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"convolutional.py\", line 127, in main\n    train_data = extract_data(train_data_filename, 60000)\n  File \"convolutional.py\", line 75, in extract_data\n    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\nMemoryError\n```\n\nAny suggestions would be much appreciated. \n", "comments": ["This failure occurs before TensorFlow even begins executing. It looks the system you're running on has very little RAM available, and can't handle loading the whole MNIST training set into memory and preprocessing it in python. There might be ways to reduce the total amount of memory used at that stage, but this seems like a particularly pathological situation.\n", "@vincentvanhoucke thank you. I accidentally installed on my 512 MB server rather than my 2 GB ram server. Thanks again.\n"]}, {"number": 656, "title": "Tiny fix to API docs", "body": "One piece of example code in train.md is incorrect.\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Withdrawing; will submit CL internally.\n"]}, {"number": 655, "title": "Add bitcast op", "body": "`bitcast` would be the same as numpy's unwisely named `view`: it would turn a tensor of one POD dtype into any different POD type such that the last dimension's size is compatible, without touching the bits in any way.  Specifically,\n\n```\ntf.bitcast(x, dtype).shape = x.shape[:-1] + [x.shape[-1] * sizeof(x.dtype) / sizeof(dtype)]\n```\n\nwith an error if the division isn't an exact integer.  The most useful case of bitcast is anything to `uint8` or back.\n", "comments": ["Sounds like this may supercede DecodeRaw?\n\nOn Wed, Dec 30, 2015 at 1:32 PM, Geoffrey Irving notifications@github.com\nwrote:\n\n> bitcast would the same as numpy's unwisely named view: it would turn a\n> tensor of one POD dtype into any different POD type such that the last\n> dimension's size is compatible, without touching the bits in any way.\n> Specifically,\n> \n> tf.bitcast(x, dtype).shape = x.shape[:-1] + [x.shape[-1] \\* sizeof(x.dtype) / sizeof(dtype)]\n> \n> with an error if the division isn't an exact integer. The most useful cast\n> of bitcast is anything to uint8 or back.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/655.\n", "Is DecodeRaw currently a thing?\n", "Note: `bitcast` doesn't need to do any copying.\n", "We could optionally have a flag for whether a dimension should be added (if the new type is smaller) or removed (if the new type is bigger).  Adding / removing dimensions is more natural in most cases.  In fact, maybe it should just be the only way?\n", "(After the code search:) `DecodeRaw` is pretty different, currently, since it takes a tensor of strings which are separately allocated.  `bitcast` would explicitly not work on string tensors, and would do O(1) work similar to `reshape`.\n"]}, {"number": 654, "title": "Beam Search 4 Translate.py", "body": "Hi, \nI'm wondering which steps are necessary to move from the greedy decoder currently implemented to an actual beam search decoder. Is this enhancement already in someone's roadmap? If not, could anyone tell me which is the right point in the code where to add this functionality?  \nThanks a lot! \nMarcello \n", "comments": ["I would like to contribute too.\n\n@vrv @lukaszkaiser knowing where the proper place to add it will be very useful.\n", "When writing the seq2seq module, the idea was that the loop_function argument (e.g. of attention_decoder) could be used to provide various forms of decoding. It works for the greedy case, but we have not implemented a beam-search yet. I think it should be possible using loop_function and the top-k op from tensorflow. But we'll see - if it's too hard to do it inside the graph, then we can change the design and go with a python-side decoder. Having a decoder in the graph has advantages though, esp. when building more complex models, so I'd like to try that first. All ideas, comments, remarks and code are welcome of course!\n", "@lukaszkaiser \nHere's a self contained example demonstrating a possible beam search implementation:\n\n``` python\nfrom __future__ import division\nimport tensorflow as tf\n\nwith tf.Graph().as_default():\n    beam_size = 3 # Number of hypotheses in beam.\n    num_symbols = 5 # Output vocabulary size.\n    embedding_size = 10\n    num_steps = 3\n    embedding = tf.zeros([num_symbols, embedding_size])\n    output_projection = None\n\n    # log_beam_probs: list of [beam_size, 1] Tensors\n    #  Ordered log probabilities of the `beam_size` best hypotheses\n    #  found in each beam step (highest probability first).\n    # beam_symbols: list of [beam_size] Tensors \n    #  The ordered `beam_size` words / symbols extracted by the beam\n    #  step, which will be appended to their corresponding hypotheses\n    #  (corresponding hypotheses found in `beam_path`).\n    # beam_path: list of [beam_size] Tensor\n    #  The ordered `beam_size` parent indices. Their values range\n    #  from [0, `beam_size`), and they denote which previous\n    #  hypothesis each word should be appended to.\n    log_beam_probs, beam_symbols, beam_path  = [], [], []\n    def beam_search(prev, i):\n        if output_projection is not None:\n            prev = tf.nn.xw_plus_b(\n                prev, output_projection[0], output_projection[1])\n\n        # Compute \n        #  log P(next_word, hypothesis) = \n        #  log P(next_word | hypothesis)*P(hypothesis) =\n        #  log P(next_word | hypothesis) + log P(hypothesis)\n        # for each hypothesis separately, then join them together \n        # on the same tensor dimension to form the example's \n        # beam probability distribution:\n        # [P(word1, hypothesis1), P(word2, hypothesis1), ...,\n        #  P(word1, hypothesis2), P(word2, hypothesis2), ...]\n\n        # If TF had a log_sum_exp operator, then it would be \n        # more numerically stable to use: \n        #   probs = prev - tf.log_sum_exp(prev, reduction_dims=[1])\n        probs = tf.log(tf.nn.softmax(prev))\n        # i == 1 corresponds to the input being \"<GO>\", with\n        # uniform prior probability and only the empty hypothesis\n        # (each row is a separate example).\n        if i > 1:\n            probs = tf.reshape(probs + log_beam_probs[-1], \n                               [-1, beam_size * num_symbols])\n\n        # Get the top `beam_size` candidates and reshape them such\n        # that the number of rows = batch_size * beam_size, which\n        # allows us to process each hypothesis independently.\n        best_probs, indices = tf.nn.top_k(probs, beam_size)\n        indices = tf.stop_gradient(tf.squeeze(tf.reshape(indices, [-1, 1])))\n        best_probs = tf.stop_gradient(tf.reshape(best_probs, [-1, 1]))\n\n        symbols = indices % num_symbols # Which word in vocabulary.\n        beam_parent = indices // num_symbols # Which hypothesis it came from.\n\n        beam_symbols.append(symbols)\n        beam_path.append(beam_parent)\n        log_beam_probs.append(best_probs)\n        return tf.nn.embedding_lookup(embedding, symbols)\n\n    # Setting up graph.\n    inputs = [tf.placeholder(tf.float32, shape=[None, num_symbols])\n              for i in range(num_steps)]\n    for i in range(num_steps):\n        beam_search(inputs[i], i + 1)\n\n    # Running the graph.\n    input_vals = [0, 0, 0]\n    l = np.log\n    eps = -10 # exp(-10) ~= 0\n\n    # These values mimic the distribution of vocabulary words\n    # from each hypothesis independently (in log scale since\n    # they will be put through exp() in softmax).\n    input_vals[0] = np.array([[0, eps, l(2), eps, l(3)]])\n    # Step 1 beam hypotheses =\n    # (1) Path: [4], prob = log(1 / 2)\n    # (2) Path: [2], prob = log(1 / 3)\n    # (3) Path: [0], prob = log(1 / 6)\n\n    input_vals[1] = np.array([[l(1.2), 0, 0, l(1.1), 0], # Path [4] \n                              [0,   eps, eps, eps, eps], # Path [2]\n                              [0,  0,   0,   0,   0]])   # Path [0]\n    # Step 2 beam hypotheses =\n    # (1) Path: [2, 0], prob = log(1 / 3) + log(1)\n    # (2) Path: [4, 0], prob = log(1 / 2) + log(1.2 / 5.3)\n    # (3) Path: [4, 3], prob = log(1 / 2) + log(1.1 / 5.3)\n\n    input_vals[2] = np.array([[0,  l(1.1), 0,   0,   0], # Path [2, 0]\n                              [eps, 0,   eps, eps, eps], # Path [4, 0]\n                              [eps, eps, eps, eps, 0]])  # Path [4, 3]\n    # Step 3 beam hypotheses =\n    # (1) Path: [4, 0, 1], prob = log(1 / 2) + log(1.2 / 5.3) + log(1)\n    # (2) Path: [4, 3, 4], prob = log(1 / 2) + log(1.1 / 5.3) + log(1)\n    # (3) Path: [2, 0, 1], prob = log(1 / 3) + log(1) + log(1.1 / 5.1)\n\n    input_feed = {inputs[i]: input_vals[i][:beam_size, :] \n                  for i in xrange(num_steps)} \n    output_feed = beam_symbols + beam_path + log_beam_probs\n    session = tf.InteractiveSession()\n    outputs = session.run(output_feed, feed_dict=input_feed)\n\n    expected_beam_symbols = [[4, 2, 0],\n                             [0, 0, 3],\n                             [1, 4, 1]]\n    expected_beam_path = [[0, 0, 0],\n                          [1, 0, 0],\n                          [1, 2, 0]]\n\n    print(\"predicted beam_symbols vs. expected beam_symbols\")\n    for ind, predicted in enumerate(outputs[:num_steps]):\n        print(list(predicted), expected_beam_symbols[ind])\n    print(\"\\npredicted beam_path vs. expected beam_path\")\n    for ind, predicted in enumerate(outputs[num_steps:num_steps * 2]):\n        print(list(predicted), expected_beam_path[ind])\n    print(\"\\nlog beam probs\")\n    for log_probs in outputs[2 * num_steps:]:\n        print(log_probs)\n```\n\nOutput:\n\n```\n\npredicted beam_symbols vs. expected beam_symbols\n([4, 2, 0], [4, 2, 0])\n([0, 0, 3], [0, 0, 3])\n([1, 4, 1], [1, 4, 1])\n\npredicted beam_path vs. expected beam_path\n([0, 0, 0], [0, 0, 0])\n([1, 0, 0], [1, 0, 0])\n([1, 2, 0], [1, 2, 0])\n\nlog beam probs\n[[-0.6931622 ]\n [-1.09862733]\n [-1.79177451]]\n[[-1.098809  ]\n [-2.17854738]\n [-2.26555872]]\n[[-2.17872906]\n [-2.26574016]\n [-2.63273931]]\n```\n\nA simple function very similar to the decoding step of Viterbi is needed to extract the best hypothesis. Furthermore, there will have to be logic outside the function that extracts the correct recurrent state for each hypothesis. This can be done with an embedding lookup of prev_state using beam_parent multiplied by each example's index in batch \\* beam_size.\n\nOne downside to this implementation is that it does not pull off hypotheses that have reached the <EOS> token from the beam. It might be possible to do that, though I think it would require significantly more complicated logic. I'm interested in hearing your thoughts!\n", "FYI, I have a variation of a beam search for TF here:\nhttps://github.com/wchan/tensorflow/blob/master/speech4/models/las_decoder.py\n", "I wouldn't mind helping out either. I currently use sampling with temperature to generate different outputs given the same input. \n", "I like the first code a lot, I think it's advantegous when the beam-search is done in the graph, so we can just feed the graph once and get the whole sequence. I think the only thing missing was pulling out hypotheses, right? But we have the top-k op in TensorFlow, woudn't that suffice?\n", "Hi, \nlet me first say that my knowledge of tensorflow is still quite limited, hence forgive me if I'll write something wrong. From what I read I understood that (1) it is not good getting back and forth between the computations on the graph (on GPU) and computations in python (on CPU), and (2) it is advisable \nto also exploit as much as possibly parallel computations on the graph (on GPU), which means in our case computing expansions of alternative hypotheses in parallel.  \n\nI have put down the following figure to explain how the beam search could work. \n\n![nmt-search](https://cloud.githubusercontent.com/assets/1135354/12116947/9e9898d2-b3bd-11e5-9f23-b2c8ba03ab52.jpg)\n\nThe figure focuses on the decoding step and shows on the top outputs of decoding steps that can be computed in parallel, and the best K=2 output words for each step. As no recombination is possible with RNN, because each output word depends on its whole history, we have to select the output words (with possible repetitions) that result in the top B=3 cumulative scores.  (Notice that K should ideally be equal to B to have beams search with beam B ) These B words become the input for the following step. To allow backtracking of the best translation we only need to store the provenance of each input with respect to the input of the previous step (see dotted arrows). A numeric index corresponding to a position index should work, too. Once we have finished with all computations, we can start backtracking the \ninput trellis (on the bottom) starting from the input word <\\/s> with the best global score. This could be performed with a linear pass over all columns, from right to left. \n", "@lukaszkaiser In the Seq2Seq paper they say, \"As soon as the `<EOS>`\nsymbol is appended to a hypothesis, it is removed from the beam and is added to the set of complete\nhypotheses\". The problem with the implementation I did above is that once an EOS token is appended, the hypothesis remains on the beam. This means that the effective beam size is reduced by one. The easy way to avoid this is to extract 2k hypothesis, which guarantees that we will follow the Seq2Seq approach. Of course, it has the downside of doubling computation.\n\n@mfederico In TF, Python is just a front end language -- no operations are actually run in Python. Whether an operation runs on GPU depends on whether the operation has a GPU kernel (e.g. `.cu` file) and other scheduling heuristics. Computing hypotheses in parallel is equivalent to decreasing the batch size for each model replica (actually faster because data transfer between GPUs doesn't have to happen).\n\nI like the idea of computing top k on each hypothesis, then computing top k on the combination of remaining words. I think this would be faster if the top k operation is computed in parallel for each row (`O(n + k^2) vs. O(nk)`, where `n = vocab size, k = beam size, k << n`). @lukaszkaiser would this be faster?\n", "@PrajitR, i really like the approach as well ... question, how would you stop the graph early? i.e., if u know all future partial hypothesis will be worse than the best completed partial hypothesis.\n", "> I like the idea of computing top k on each hypothesis, then computing top k on the combination of remaining words.\n\nI like this idea too, but isn't computing top k on the combination of remaining words expensive?\n", "If the k top-k lists are sorted, they will remain sorted if we add the cumulative score of the input hypotheses they were generated from.  The finding the global top-k takes O(k log k) in the worst case with k space (see algorithm  http://stackoverflow.com/a/21051271).\n", "@PrajitR, I'm trying to implement your suggestion into my experiments, using the Seq2seq interfaces. I think your code is very clever and make lots of sense. Nevertheless, I'm still confused by one thing you mentioned:\n\n\"(...) extracts the correct recurrent state for each hypothesis. This can be done with an embedding lookup of prev_state using beam_parent multiplied by each example's index in batch \\* beam_size.\"\n\nIf I'm using a batch of 1 (i.e., one sentence at the time - am I right with this assertion?), wouldn't  just the lookup of prev_state suffice? If not, I think I didn't get that.\n\nIn addition, I think that if we combine the predicted symbols with their parents right after producing them, we could keep a list of complete hypotheses to add those which reach the EOS symbol and we could try to remove them from the beam_path. This would not stop the graph earlier but I think would solve the problem of reducing the effectiveness of the beam size.\n", "Just as a comment: there is some experimental support for session.partial_run() in TensorFlow in the 0.7 release. Since partial_run does not deallocate the tensors, it should make step-by-step decoding from seq2seq models much easier. And since we can have a few of them in parallel, it could also greatly simplify beam_search. But it's experimental for now, so beware - I'm just testing it.\n", "Finally, I'm interested to know if there is a version of `translate.py` which support beam search in decoding?\n", "I learning translation currently, and found many papers that describe beam seach for Statistical Machine Translation. The main idea is that for the translations that cover the same number of source sentence words, select at most k best candidates, which may combined with Estimating Future Cost method. Are there any papers  describe beam search for neural machine translation (NMT)? At each step,  is selecting k best candidates that have same length is enough? thanks!\n", "@giancds Is it possible to get the code of your implementation?\n", "Hi tilneyyang,\n\nI have declined of implementing the solution suggested in here. I am currently trying some new options on the decoder and have found out that this particular solution does not give me the flexibility I need. \n\nHowever, I am still doing the beam search but running one step at the time, so part of the beam search is done outside the graph (at the cost of decoding speed to get that freedom). \n\nMy code is largely based on the code of @kyunghyuncho (you can find his code [here](https://github.com/nyu-dl/dl4mt-tutorial)), but he uses Theano in the implementation. \n\nMine you can find [here](https://github.com/giancds/tsf_nmt) under the nmt_models.py file.\n", "I'd like to share my in-graph beam search implementation that ~~uses the loop_function approach~~\n\nhttps://gist.github.com/nikitakit/6ab61a73b86c50ad88d409bac3c3d09f\n\nI believe it correctly implements length-bucketed beam search. So far I've only tested that the outputs look reasonable (as opposed to comparing with a known-good beam search implementation).\n\n~~The loop_function API is really nice, but I'm not sure if bending over backwards to avoid writing a custom op was the best decision here.~~\n\nEDIT 2: After further thought, I've discovered a serious flaw in the original beam search implementation I posted. The gist is now updated with a fix. In the process, I had to abandon the loop_function API, since it was not sufficient to correctly implement beam search.\n", "For those interested in a dynamic_rnn() based beam search, here is an implementation that is working well for us - https://github.com/sdlg/nlc/commit/b5088d102752a9c2aa4a923abb17abfb35026d82\n", "@avati It appears that your implementation only works on a single example at a time `batch_size=1`. Is this by accident or by design?\n\nFor example, compare [your code](https://github.com/sdlg/nlc/blob/b5088d102752a9c2aa4a923abb17abfb35026d82/nlc_model.py#L202) with [mine](https://gist.github.com/nikitakit/6ab61a73b86c50ad88d409bac3c3d09f#file-tf_beam_decoder-py-L121). Note that your call to `top_k` is on a flat vector (so if there is a batch size >1, the examples compete with each other), while mine is on a 2D matrix. My code has hacks for allowing `batch_size>1`, though I have now discovered that it's flawed in a different way.\n", "As of now `batch_size=1` is by design. I suppose it is possible to support `batch_size>1` with `dynamic_rnn()`, but the code got unreadable quickly the first time i tried. `dynamic_rnn()` based code is in general less readable already compared to the time-step unfolded code (which enjoys use of native python loops etc). I plan to revisit `batch_size>1` support sometime in the future.\n", "FYI my [gist](https://gist.github.com/nikitakit/6ab61a73b86c50ad88d409bac3c3d09f) is now updated with fixes and a new API.\n\nI'd love to get feedback from others on whether this API would suit their needs!\n\nI think I've mostly converged on something that works well with typical uses of tensorflow. I'm curious if there is interest in getting something like this added to `tf.contrib` (maybe @lukaszkaiser can comment?) If that is the case, I'm willing to add in some more optimizations (e.g. dynamic early-stopping like the code by @avati) and do general profiling/cleanup on the code.\n", "Here is the link for extension of  tensorflow seq2seq model for conversation models and has the option of beam search and setting beam size .\n\nhttps://github.com/pbhatia243/Neural_Conversation_Models\n", "@pbhatia243 Hi\nI have read your code [https://github.com/pbhatia243/Neural_Conversation_Models/blob/master/my_seq2seq.py#L729](url) for beam_search, and I got several questions maybe you can shed some light on that\n1)line 119\n`emb_prev  = tf.reshape(emb_prev,[beam_size,embedding_size])`, it seams that `emb_prev` has a shape of [batch_size \\* beam_size, embedding_size] before reshape, which i thought might lead to a reshape failure. But if it works , would you explain that for me ?\n\n2) line 100\n`probs = tf.reshape(probs + log_beam_probs[-1],\n                               [-1, beam_size * num_symbols])`  \n\n As i considered, only when the probs matrix is in beam-major order could this reshape work well, \nlike:\nbatch0_beam0_symbol_vec\nbatch0_beam1_symbol_vec\nbatch0_beam2_symbol_vec\n....\nBut i found that in line  668 - 670\n\n```\nfor kk in range(beam_size):\n      states.append(state)\nstate = tf.reshape(tf.concat(0, states), [-1, state_size])\n```\n\nthe  `state` is constructed in a seemly batch-major manner, which can't be align to the probs (which come from last cell output that takes the concatenation of state and x as input) \nis there anything i missed?\n", "the problem are soluved, the batch_size is 1, so all the reshape works well, all clear\n", "I see that the `raw_rnn` API has been updated a few weeks ago to allow hidden state reordering, as required by beam search. This means that using it will likely yield the cleanest and most computationally efficient way to integrate beam search with tensorflow.\n", "@nikitakit could you elaborate what you mean with \"hidden state reordering\" and how you believe this can solve the requirements by beam search?\n\nThanks\n", "Jumping in on this -- has beam search been added to the seq2seq models, or has someone implemented a neat way of incorporating it into existing models?", "Now that my research has once again touched on beam search, I've taken another look at the latest comments here. It seems that my previous remarks about `raw_rnn` were greeted with some confusion, though I thought the meaning was sufficiently clear if you understand the math behind beam search. I hope that in the meantime everyone found a working beam search solution among those posted in this thread.\r\n\r\nI've now also updated my [gist](https://gist.github.com/nikitakit/6ab61a73b86c50ad88d409bac3c3d09f) to take advantage of the latest changes to `raw_rnn`. The API it provides has changed slightly, and the code passes all of the test cases I have (though fair warning: I have yet to apply it to my own full-scale models).\r\n\r\nThis new version is more efficient in that it can stop early if the highest-scoring solution has already been found. I've also now figured out how to transparently handle all cells (including those with attention) without requiring a custom wrapper class, and have updated the API to take advantage of that. The new API is just a single `beam_decoder` function.\r\n\r\nIt should be straightforward to slot in anywhere greedy search is currently used. (If anyone has code that can adapt it to the seq2seq primitives, I think that may be helpful for others. I myself tend to use the base rnn functions instead of anything higher-level).\r\n\r\nLast time my beam search implementation was suggesting for merging into `tf.contrib`, it was determined that the old API was not ideal. I'd be happy to try again with this new API if others are interested.\r\n\r\nTo conclude, a note on back-propagation: a few people have asked me whether it's possible to use my code during training, and back-prop through the beam search. I've come to conclude that it is fine to just run the beam search to find the top-scoring sequences, and then do a second forced decode under those sequences (where the second decode is back-propagated through). Beam search has to consider a very large number of possible candidates, so duplicating this work shouldn't be too much of an issue. On the other hand, the way the beam search couples multiple candidate sequence means that any naive application of automatic differentiation may instantiate backwards-pass tensors for all of the candidates, rather than just the sequence you care about. That would be an even bigger performance hit than duplicating a small fraction of the forwards pass computations.", "Can you send me the Pull Request with the changes", "@pbhatia243 Could you clarify what you meant by your last comment?\r\n\r\nTo be clear, my implementation is self-contained and independent of what's in your repo, so surely you were referring to something else? (Feel free to use my code if you need it, though)\r\n\r\nOne thing I want to point out, having taken a look at your Neural Conversion Models repo, is that it uses a loop function approach. I therefore suspect that the implementation is actually subtly broken -- I believe it contains the same mistake that I made when I first tried to implement in-graph beam search. (It's really hard to notice without directly looking for it or having good tests.) If you look at the code [here](https://github.com/pbhatia243/Neural_Conversation_Models/blob/master/my_seq2seq.py#L215), you'll see that the cell produces both outputs and hidden states. The outputs are eventually [reordered](https://github.com/pbhatia243/Neural_Conversation_Models/blob/master/my_seq2seq.py#L103) in a top-k step. But when the outputs are re-ordered, the states need to be re-ordered as well! Yet, I have not found a re-ordering step anywhere (i.e. states are never passed to a function like tf.gather). This is the reason why I had to abandon the original loop_function based approach and waited for hidden state reordering to be added to `raw_rnn` before I could take advantage of it.", "Can't we use the newly added `dynamic_rnn_decoder`?\r\nAccording to the documentation, we can use `context_state` parameter of `decoder_fn` to implement beam search, however I am not sure how to use that.\r\nI think I could see if we can implement beam search using the function in a couple of weeks. Is there anyone interested on this?\r\n", "@jihunchoi Thanks for bringing this to my attention. As far as I can tell `dynamic_rnn_decoder` is just a thin wrapper around `raw_rnn`. That is, the raw_rnn based implementation I have would be functionally identical to a dynamic_rnn_decoder one, so no point in modifying something that already works.", "The `dynamic_rnn_decoder` was built to, eventually, support beam search.", "In the original [Google's SmartReply paper](https://arxiv.org/abs/1606.04870):\r\n> Our search is conducted as follows. First, the elements of R (the set of all possible responses) are organized into a trie. Then, we conduct a left-to-right beam search, but only retain hypotheses that appear in the trie. This search process has complexity O(bl) for beam size b and maximum response length l. Both b and l are typically in the range of 10-30, so this method dramatically reduces the time to find the top responses and is a critical element of making this system deployable.\r\n\r\nI want to implement something similar. Some questions:\r\n\r\n 1. How to represent the set of all possible responses (R) in a trie data structure in TF?\r\n 2. Does the current implementation of beam search support retaining only hypotheses that appear in the trie?", "Hi, @pbhatia243 \r\n\r\nThank you for your repo. It's a good extension.\r\nBut as @nikitakit says, it seems that your code in Neural Conversion Models repo is not the complete Beam Search. It may be a solution between the argmax greedy method and Beam Search. **And worse still, it will always get wrong responses.**\r\n```python\r\n    symbols = indices % num_symbols # Which word in vocabulary.\r\n    beam_parent = indices // num_symbols # Which hypothesis it came from.\r\n```\r\nWhen the beam_parent is changed with the previous time stamp, we must reset the state of RNN in this beam path. Then using this state and this input to inference the output. \r\nIf we don't reset the state, the decoding processing will be in the different beam paths. Do you think so?\r\n\r\nAs the normal Beam Search method, the state of the decoder is pruned step by step. It need to reset the state of RNN during decoding.\r\n\r\nCould you add a step to reset the state by the beam_parent in your code?", "@Syndrome777 : I am about to give a second pass on beam search. Could you elaborate on reseting the state. Do you mean reorder ?", "@nikitakit : How has the performance been on your large dataset. Using my approach, even though its an approximation, I do get realistic answers and lower perplexity. I am about to give a second pass on beam search. Could you elaborate on reseting the state. Do you mean reorder ?", "@pbhatia243 I think you must have figured this out already, but, I am leaving this comment, hoping to help people reading this thread. I believe what Syndrome777 and nikitakit meant to point out was \"reordering\" the states after you reorder the outputs sequences. \r\n\r\nIf you do not re-order the states along with the outputs(beams), then you could end up trying to generate next token of sequence A with a state used to generate sequences B in the previous step, which most likely would not know of the context of sequence A. ", "Agree with junskang, I have used nikitakit's doing in graph beam search and face this problem. I you use attention you also need to re-order attention. Also for someone trying to use beam search I suggest refering to https://github.com/google/seq2seq/tree/master/seq2seq which has a dynamic implementaion of ingraph beam search. For out graph beam search you can refer to  models/im2txt", "@chenghuige Thanks! Would give us some example of the google implementation? Does it cover mini-batches? If we can only process a sequence at a time, does time matter?", "I'd recommend using the google/seq2seq github repo. It's a very good, complete and maintained seq2seq implementation. Beam search is here: https://github.com/google/seq2seq/blob/master/seq2seq/inference/beam_search.py", "@lukaszkaiser any plans on adding beam search to tensorflow from google/seq2seq?\r\nThe google/seq2seq repo seems very good for experimentation but it does a lot of things different than TF (e.g., conf files to define classes). ", "I think we always welcome contributions and stable things moving to tf.contrib, but I'll not be working on it personally.", "thanks @lukaszkaiser. If I end up implementing it i'll send a pull request.\r\n", "Great! I think tensorflow/contrib/seq2seq/ would be a good place for it.", "Hi All,\r\nI have problem with NMT all out put is unknown. my parallel data is small. is this the problem and how can i solve it ", "Hello, I am currently trying to use nikitakit's beam search code in conjunction with an attention mechanism. @nikitakit mentioned in a previous post that the code now works with attention but later @chenghuige said that it doesn't reorder attention -- does this mean it is not possible to currently use nikitakit's code with an attention mechanism? Thanks!\r\n\r\n\r\n\r\n", "My code works fine with attention. All of my own models that i used it for relied on attention. However, you do need to make sure that cell_transform is set correctly (instructions in the docstring).\r\n\r\nThat said, the code still uses the tensor flow 0.12 API and I haven't updated it for tensorflow 1.0", "Great, thanks! Out of curiosity, when you've integrated attention have you been using AttentionCellWrapper, or some other means? Also, does the code in general support custom attention cell wrappers?", "@classicCoder16 I used custom cells, but AttentionCellWrapper will work too. Anything that implements the RNNCell interface is compatible -- the interface specifies how the cells are supposed to store their state, and the beam search has access to that for state reordering.", "Hi, @nikitakit , I am currently working on an application in which I would like to incorporate attention with your implementation of beam search. So far I have been struggling to implement my own custom attention wrapper and I wanted to ask if you would be willing to share your code for the custom attention cells which you referenced in your most recent post? Thanks for the help in advance.", "Hi, @nikitakit, Does your implementation works in tensorflow version >=1.0?  For example, if I use TrainingHelper during the training? how should I use your implementation during the testing? Thanks.", "@PrajitR I have extended your version and tested it in my seq2seq model. Hope it helps. \r\n\r\nhttps://stackoverflow.com/a/50304227/3552975\r\n\r\nAny suggestions are highly appreciated. Thanks. "]}, {"number": 653, "title": "translate.py - train path error", "body": "```\npython translate.py --decode --en_vocab_size=20000 --fr_vocab_size=20000 \n--train_dir /Users/Translated/Downloads/train \n--data /Users/Translated/Downloads/data --size=256 \n--num_layers=3 --steps_per_checkpoint=50\n```\n\nWill not load the correct model\n\ntranslate.py\n\n``` python\nif ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n```\n\nwill fail on my Mac OSX El Capitain using VirtualEnv and cause to load an empty model.\nI did not test on other systems.\n\nVariable dump in pbb debugger:\n\n```\n(Pdb) pp FLAGS.train_dir\n'/Users/Translated/Downloads/train' (Correct) \n(Pdb) pp ckpt.model_checkpoint_path\nu'train/translate.ckpt-15350' (Wrong)\n```\n\nI was hoping it was related to #600 but it is not.  \n", "comments": ["@marcotrombetti, @mrry: Looks like this fell through the cracks.  Is it still an issue?\n", "Closing this issue due to lack of activity.\n"]}, {"number": 652, "title": "--output_layer=pool_3 throws error", "body": "Also posted this on [stackoverflow](http://stackoverflow.com/questions/34466356/error-running-label-image-with-output-layer-pool-3) but this could be a bug so I'm posting here:\n\nThe tensorflow exercise in the image recognition tutorial suggests running the c++ example with  `--output_layer=pool_3`. I have tried running this and am getting an error:\n\n```\n$ bazel-bin/tensorflow/examples/label_image/label_image --output_layer=pool_3\n\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4\nW tensorflow/core/common_runtime/executor.cc:1076] 0x558ae6a5d210 Compute status: Invalid argument: input must be 2-dimensional\n     [[Node: top_k = TopK[T=DT_FLOAT, k=5, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const/_0)]]\nE tensorflow/examples/label_image/main.cc:311] Running print failed: Invalid argument: input must be 2-dimensional\n     [[Node: top_k = TopK[T=DT_FLOAT, k=5, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Const/_0)]]\n```\n", "comments": ["Resolved in stackoverflow solution. Perhaps some comments from the solution should be added to the official readme for clarity?\n"]}, {"number": 651, "title": "How ti make dataset?", "body": "I have found to make my own dataset.\nThe example dataset is already bytestream.\n\nCan you explain how can I add my image set?\n", "comments": ["Title is wrong spell. \nHow can I make dataset as bytestream?\n", "This should be a stackoverflow question rather than a github issue.  Github issues are for bugs or feature requests that should be fixed on the tensorflow side.\n"]}, {"number": 650, "title": "TensorBoard's graph tab doesn't render in Firefox", "body": "TensorBoard's graph tab works fine in Chrome and Safari, but does not render in Firefox.\n\n[A few other people have reported this issue](https://groups.google.com/a/tensorflow.org/forum/?utm_medium=email&utm_source=footer#!topic/discuss/Lc7Y_6UEFOM).\n", "comments": ["I also had an issue of producing a graph in Firefox. I hope they support the Firefox in Linux (I use Ubuntu) in near future.\n", "Most prople has this problem with firefox or safari. However, I am using **chromium 47.0**...\n![image](https://cloud.githubusercontent.com/assets/6175880/12097756/72574918-b358-11e5-9ee7-285a6d05deb5.png)\n", "Fixed with 1c579361cd1e088dd5e05a394b1561a73e3667ba. The commit is very large since it bundled many changes, but the important change is in minimap.ts. Feel free to reopen if you run into this issue again. You won't see this fix before the next release unless you rebuild TensorBoard yourself following the instructions in  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md\n", "Great. It's working over here.\n", "@dsmilkov I built the latest version, however, the problem still exists. There's no error messages, only a warning in chrome console: \n`/deep/ combinator is deprecated. See https://www.chromestatus.com/features/6750456638341120 for more details.`.\nBtw, I adjusted a bit code in `google/protobuf/src/google/protobuf/io/coded_stream.h : 611`:\n\n```\nstatic const int kDefaultTotalBytesLimit = 256 << 20;  // 64MB\n```\n\nin order to load my large tensorflow log file. But I don't think this will cause problem.\nI use Chrome 47.0. \n![image](https://cloud.githubusercontent.com/assets/6175880/12357264/e1120726-bbe1-11e5-8f6e-ad8e9df921c5.png)\n", "@frankyjuang The issue you are having might be unrelated to this issue, since you are already using Chrome and not Firefox. Can you share your log directory with us? You can gzip it and ideally attach it here to this issue, or email it to dsmilkov@gmail.com. \n", "@dsmilkov Here is it. [Large log file](https://drive.google.com/file/d/0B7ldj0upUtbdNmNIcU81YUpBbVk/view?usp=sharing)\nIt is actually the image model inception v3. All  I have done is write the graph to this file. Thanks\n", "@dsmilkov Could you please explain where this directory (tensorflow/tensorboard) that the instructions in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md instructs you to do 3 installations? Is it in /usr/local/lib/python2.7/dist-packages/tensorflow/tensorboard, or the source (i.e. git clone) that you downloaded to your home directory?\n", "You should only need to do those installations if you want to rebuild the frontend from source, in which case you would do this in the source (ie git clone) directory. \nFor just running TensorBoard as a user, none of those commands are necessary.\n"]}, {"number": 649, "title": "translate.py - empty _buckets sets will crash", "body": "If the dev_set does not contain all the string related to all the _buckets \nthe script will crash.\nEg. If the dev set does not contain a string between 20 and 25 words, the script will crash.\n\ntranslate.py, around line 178\n\n``` python\nencoder_inputs, decoder_inputs, target_weights = model.get_batch(\n              dev_set, bucket_id)\n...\n```\n\nI think that before that line you should had add an if to avoid that\n\n``` python\n     if len(dev_set[bucket_id])>0:\n           encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n              dev_set, bucket_id)\n           _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                       target_weights, bucket_id, True)\n           eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n           print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_px))\n```\n", "comments": ["marcotrombetti@, please do send a PR with a fix.\n", "encounter same issue as @marcotrombetti , I just send a PR with fix @keveman #1496 \n", "looks like the PR fixed this!"]}, {"number": 648, "title": "Fixes anchor links in installation instructions", "body": "The header anchor links in the docs (inside the directory g3doc) is using a {#anchor} format that github's Markdown does not understand.  This PR primarily fixes the anchor text in the installation instructions, which I assume is fairly popular.\n", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "the internal and OSS versions have diverged for a few days now. Also, they're different, kept in sync with a bunch of script. You can't copy any content from the third_party version into github, it won't work. You'll have to redo the change (it shoudl be possible to apply the patch to the public version you checked out from github, but no guarantees on that either).\n", "I apologize.  Let me retry this.\n", "I signed it!\n\n2015-12-29 17:18 GMT-05:00 googlebot notifications@github.com:\n\n> Thanks for your pull request. It looks like this may be your first\n> contribution to a Google open source project. Before we can look at your\n> pull request, you'll need to sign a Contributor License Agreement (CLA).\n> \n> [image: :memo:] _Please visit https://cla.developers.google.com/\n> https://cla.developers.google.com/ to sign._\n> \n> Once you've signed, please reply here (e.g. I signed it!) and we'll\n> \n> ## verify. Thanks.\n> - If you've already signed a CLA, it's possible we don't have your\n>   GitHub username or you're using a different email address. Check your\n>   existing CLA data https://cla.developers.google.com/clas and verify\n>   that your email is set on your git commits\n>   https://help.github.com/articles/setting-your-email-in-git/.\n> - If you signed the CLA as a corporation, please let us know the\n>   company's name.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/648#issuecomment-167889357\n> .\n", "Finally got my commits squashed properly, snagging all of these {#anchors}. See #660.\n", "Awesome. I didn't know about the -N uniquification. Do we have a case like\nthat? I'd like to check to make sure that our website behaves identically.\n\nOn Thu, Dec 31, 2015 at 1:55 PM Sam Abrahams notifications@github.com\nwrote:\n\n> Finally got my commits squashed properly, snagging all of these\n> {#anchors}. See #660 https://github.com/tensorflow/tensorflow/pull/660.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/648#issuecomment-168253516\n> .\n", "I haven't seen a case where it comes up in the TensorFlow docs- I just know that Github performs that action.\n\nAlso, I noticed that the code doesn't handle apostrophes or other mid-word non-alphanumeric characters correctly. Whoops! Luckily it only messed up three links, so I'll get that patched in.\n\nI believe the change needed to make the python code work is to remove non-alphanumeric/- characters instead of replacing them with a space.\n", "Those three goofed links in #660 have been fixed. Also, I modified the anchor-creating Python function back in the line comments to work properly. Just had to change the first lambda to `lambda x: \"\"`\n", "After realizing I missed some details for the GitHub-ification of header anchors, [I played around with more edge cases](https://github.com/samjabrahams/tensorflow_util/blob/master/md/github_header_test.md) to hone the algorithm. I had to make three small changes to what I had above:\n1. GitHub does not remove trailing hyphens\n2. It replaces groups of whitespace with hyphens _before_ removing strange characters. \n   - This can lead to trailing hyphens. e.g. \"This header ends with ellipses ...\" -> \"#this-header-ends-with-ellipses-\"\n3. Make sure that the regex substitution is flagged for UTF-8 encoding\n\nGitHub handles UTF-8 encoding in its header anchors, so make sure that any settings necessary to enable UTF-8 are switched on.\n\nI changed the code in the line notes above, but here is the fixed Python code again so going back up to read it isn't necessary. To test out Unicode letters in Python, make sure to explicitly make the string unicode with `u\"This is my h\u00e9ader string!\"` syntax:\n\n``` python\ndef create_anchor_from_header(header, existingAnchors=None):\n\n    # Strip white space on the left/right and make lower case\n    out = header.strip().lower()\n\n    # Replace groups of white space with hyphens\n    out = re.sub(ur'\\s+', lambda x: \"-\", out, flags=re.UNICODE)\n\n    # Remove non-alphanumeric characters, hyphens, and spaces\n    out = re.sub(ur'[^\\w\\- ]+', lambda x: \"\", out, flags=re.UNICODE)\n\n    if existingAnchors:\n        if out in existingAnchors:\n            i = 1\n            while (out + \"-\" + str(i)) in existingAnchors and i < 100:\n                i = i + 1\n            out += \"-\" + str(i)\n    return out\n```\n", "This PR is superceded by #660, I'll close it.\n", "I'm late to this.  Thanks @samjabrahams for taking this and making it much better :-)\nI wrote a small one to fix up some issues I found:\n  https://github.com/tensorflow/tensorflow/pull/683\nNot sure if there are other like this.\n"]}, {"number": 647, "title": "Cannot import tensorflow under python", "body": "Hi all, I first successfully installed the tensorflow following the instructions of \"Pip Installation\". However, I couldn't import tensorflow in Python. Thanks a lot!!\n\nThe error comes like this:\n\nzhuotun@sunformoon:~$ python\nPython 2.7.11 |Anaconda 2.4.1 (64-bit)| (default, Dec  6 2015, 18:08:32) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nAnaconda is brought to you by Continuum Analytics.\nPlease check out: http://continuum.io/thanks and https://anaconda.org\n\n> > > import tensorflow as tf\n> > > Traceback (most recent call last):\n> > >   File \"<stdin>\", line 1, in <module>\n> > > ImportError: No module named tensorflow\n\nI double check that I have installed successfully the tensorflow by re-installing the tensorflow as following:\n\nzhuotun@sunformoon:~$ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n[sudo] password for zhuotun: \nDownloading/unpacking https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl\n  Downloading tensorflow-0.6.0-cp27-none-linux_x86_64.whl (11.4MB): 11.4MB downloaded\nRequirement already up-to-date: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.6.0)\nRequirement already up-to-date: protobuf==3.0.0a3 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.6.0)\nRequirement already up-to-date: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.6.0)\nRequirement already up-to-date: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==0.6.0)\nRequirement already up-to-date: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf==3.0.0a3->tensorflow==0.6.0)\nInstalling collected packages: tensorflow\nSuccessfully installed tensorflow\n", "comments": ["It looks (from your reported output) like this is an Anaconda issue. From what I can tell, the recommended way to install using Anaconda is to do the following:\n\n```\n$ pip install virtualenv\n$ conda create --name=tensorflow_env python=2.7\n$ source activate tensorenv\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp34-none-linux_x86_64.whl\n```\n", "Hi Derek,\n\nThanks a lot for your reply.\n\nBased on your suggestion, I successfully installed a virtualenv of tensorflow using the following:\n\n$ conda install virtualenv # suggested by the output of terminal not using pip install\n$ conda create --name=tensorflow_env python=2.7\n$ source activate tensorflow_env\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.5.0-cp27-none-linux_x86_64.whl   # the code you provided is for the python 3\n\nThen, I tested the installation using  the mnist demo. It worked! Cool!\n", "Glad to hear it worked! One minor suggestion: sorry for the wrong link, but you should probably install from the following link to get the latest (0.6.0) version of TensorFlow:\n\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl \n", "Just to add to this issue: The original problem existed, because a `sudo` was used before `pip`, with you having a local anaconda installation. So the `sudo`'s `pip` led to the system wide default `pip`, which installed tensorflow not into your local anaconda `site-packages` and hence it couldn't be found.\n", "Hey osdf,\n\n```\n           Your comment proved to be a life saver, I was actually committing the mistake you just mentioned and was trying to access tensorflow from inside the environment. Removed sudo and it worked like magic. Thanks a lot for this comment.\n```\n\nBest,\nParth\n", "Hi all,\n\nThe above given suggestion is fr installing tensorflow in linux under conda environment.  I am trying to install it on windows environment. Kindly help\n\nI tried \nconda install --channel https://anaconda.org/jjhelmus/tensorflow tensorflow\n", "I had the same problem as sunformoon and mrry's comment solved it.\n\nI followed the online tutorial for TensorFlow ( https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#test-the-tensorflow-installation ) on installing tensorflow on a virtualenv when I got the problem\n", "@mrry\n\nFor Ubuntu 64 bit CPU Only :\nhttps://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl\n\n$ pip install virtualenv\n$ conda create --name=tensorflow_env python=2.7\n$ source activate tensorenv\n$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0rc1-cp27-none-linux_x86_64.whl\n\nFor more and different binary versions :\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md\n", "To me \r\ni am using python3, but in terminal i was writing like python\r\nthat'why it is giving error[ImportError: No module named tensorflow] to me\r\n", "I have installed tensorflow on my Mac using simple pip command (e.g. pip install tensorflow). Now I am trying to test the setup by invoking python from command line and getting the same error:\r\n\r\n```\r\n$ python\r\nPython 2.7.10 (default, Oct 23 2015, 19:19:21) \r\n[GCC 4.2.1 Compatible Apple LLVM 7.0.0 (clang-700.0.59.5)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n```\r\n\r\nPlease help, what might be wrong here?\r\n\r\n", "I just tried doing the same thing by invoking python3 from command line instead of python and now the import tensorflow worked smoothly. What does that mean? Tensorflow has no compatibility with python 2.7 or 2.8??", "$ cd /path/to/tensorflow/bin\r\n$ source activate tensorflow\r\n$ python\r\n```\r\n>>> import tensorflow as tf\r\n>>> print(tf.__version__)\r\n1.0.1\r\n```\r\ndone\uff01", "how can i use tensorflow in pypy?  help!!!", "@AhmetTavli \r\nfollowed your procedure\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: No module named tensorflow\r\n", "In case of you've validated tensorflow was corrected installed with terminal, while this error triggered in Ipython notebook, their might be a difference between your local python and ipython version, try this:\r\n```\r\n# in terminal\r\nimport sys\r\nprint (sys.version)\r\n# in my case output is 2.7.9\r\n# in notebook\r\nimport sys\r\nprint (sys.version)\r\n# in my case output is 2.7.10\r\n```\r\nTo solve, look at [this](https://stackoverflow.com/questions/9386048/ipython-reads-wrong-python-version).\r\n\r\n", "I solve the problem by following steps (in Windows) this fix the problem in pycharm:\r\n1- Run Anaconda prompt\r\n2- Activate root (>activate root)\r\n3- pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.2.1-cp35-cp35m-win_amd64.whl\r\nFinish!", "I am using mac,\r\nI have tensorflow being shown at `pip3 show tensorflow` but when I try to compile a .py file it says \r\n\r\n> `ModuleNotFoundError: No module named 'tensorflow'`\r\n\r\nI used conda install as an alternate to solve the problem.", "I updated tensorflow from 1.1.0 to 1.3.0 using the wheel file: tensorflow-1.3.0-cp35-cp35m-win_amd64.whl and in the line:\r\n\r\n`import tensorflow as tf`\r\n\r\nI got the following error:\r\n\r\n```\r\nfrom numpy.core.multiarray import c_einsum\r\nImportError: cannot import name 'c_einsum'\r\n```\r\n\r\nAny help?", "Always activate the tensorflow environment (activate tensorflow) in the Anaconda prompt window before using tensorflow.", "I have the same issue on my windows box. like osdf commented on Feb 2, 2016, I run anaconda prompt as administrator, then system can find the tensorflow I installed. \r\nBTW, I was using Console2.", "first step is to ACTIVATE tensorflow\r\nit can be done by  firing these commands  \r\n\"cd tensorflow/bin\"          \r\n\"source activate tensorflow\"\r\n\r\nand that should do allow you to import tensorflow.\r\n\r\nparticularly in my case,\r\nnext few steps are...\r\n\r\nnow go to the folder where premade_estimator resides\r\n\"cd models/samples/core/get_started/\"\r\nnow run premade_estimator.py file by typing this command,\r\n\"python premade_estimator.py\".\r\n\r\nWORKED!!\r\n\r\n\r\n\r\n\r\n"]}, {"number": 646, "title": "Can't call shuffle_batch with a boolean tensor", "body": "`tf.bool` is not a valid dtype for tensors passed to `tf.train.shuffle_batch` (or most queues, I think)\n\nI'm porting a data input pipeline to tensorflow, but doing it a little differently than the seq2seq tutorials. I have variable-length sequential data that I pad in pre-processing, while also generating a boolean mask for the valid entries. However, I can't store the mask in a `tf.bool` tensor because that breaks the queue system.\n", "comments": ["Thanks for pointing this out! There's no technical reason why this should be, just an oversight on our part. I'll prepare a patch.\n", "Should be fixed at HEAD now.\n"]}, {"number": 645, "title": "Casting int64 to int8 is not supported", "body": "Nor `int32`->`int8` or `int16`->`int8`.\n\nGranted, these aren't the safest of operations, but it's still a surprise that they throw an exception.\n", "comments": ["What version are you using?  I believe @martinwicke fixed this, and it might already be in 0.6.0.\n", "I'm using 0.6.0 for Mac via pip\n", "I'll wait for @martinwicke's reply, but that may mean it's only in git and will be released with the next version.\n", "Yeah, that change was after 0.6.0. If you update to master (which means\nbuilding yourself), you should have universal casts.\n\nOn Tue, Dec 29, 2015 at 2:16 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> I'll wait for @martinwicke https://github.com/martinwicke's reply, but\n> that may mean it's only in git and will be released with the next version.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/645#issuecomment-167889234\n> .\n", "Closing since it's fixed in git.\n"]}, {"number": 644, "title": "Add Adadelta optimizer ", "body": "This is my attempt at #516 . Looking for some early feedback as is my first attempt. \n", "comments": ["missing the GPU impl, did you forget to git add the file? cause the code references it but the file is missing ; )\n\nalso, I think a few lines are over 80 chars. but otherwise, LGTM.\n", "@zffchen78: when you get a chance can you take a look at this?\n", "Can one of the admins verify this patch?\n", "LGTM\n", "@vrv ok I will add the cuda part and do the edits !\n", "Two suggestions:\n- could you add a learning rate parameter `lr`? The original publication does not have it, but flexibility here is a good thing.\n- could you rename `decay_rate` to `rho`, following the paper and the rmsprop impl in the same file?\n\nI think there is a mistake:\nhttps://github.com/Mistobaan/tensorflow/blob/7a262ee6467c909cae723e0de5fb87a2a7e9a664/tensorflow/core/kernels/training_ops.cc#L53\nThis line either should have `accum = ...` (instead of `+=`) or follow the respective line in the rmsprop implementation further down (https://github.com/Mistobaan/tensorflow/blob/7a262ee6467c909cae723e0de5fb87a2a7e9a664/tensorflow/core/kernels/training_ops.cc#L133).\n", "FYI, there's a couple of bugs, namely the += shoulda been =\n\nmy impl is here (GPU included + separate lr included + sparse):\nhttps://github.com/wchan/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc\n", "Hi, mistobaan, please let us know how you plan to proceed on this? I can see the following work items:\n1. address the accum issue raised by osdf and wchan. I do not have background to judge whether it's a bug or its your design choice. I can only review the implementation matches the specification, where your specification says accum +=; whether that matches the intended algorithm. It's up to you three to judge;\n2. whether to add lr option. It's also up to you and others to discuss and agree on something. \n\n1 & 2 must be settled before we can add this op since it affects the op's interface / specification and will be hard to fix later.\n\n3) gpu support and test;\n4) sparse support and test;\n\nYou can get 1&2 done and merged first; and collaborate w/ others to get 3&4 done or you can incrementally get them done later.\n", "Hi!\nI am working on a patch that has all the above suggestions and the gpu code\nfrom william chen\n\nOn Saturday, January 16, 2016, zffchen78 notifications@github.com wrote:\n\n> Hi, mistobaan, please let us know how you plan to proceed on this? I can\n> see the following work items:\n> 1. address the accum issue raised by osdf and wchan. I do not have\n> background to judge whether it's a bug or its your design choice. I can\n> only review the implementation matches the specification, where your\n> specification says accum +=; whether that matches the intended algorithm.\n> It's up to you three to judge;\n> 2. whether to add lr option. It's also up to you and others to discuss and\n> agree on something.\n> \n> 1 & 2 must be settled before we can add this op since it affects the op's\n> interface / specification and will be hard to fix later.\n> 1. gpu support and test;\n> 2. sparse support and test;\n> \n> You can get 1&2 done and merged first; and collaborate w/ others to get\n> 3&4 done or you can incrementally get them done later.\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/644#issuecomment-172246805\n> .\n\n## \n\nLinkedIn: http://linkedin.com/in/fmilo\nTwitter: @fabmilo\n\n## Github: http://github.com/Mistobaan/\n\nSimplicity, consistency, and repetition - that's how you get through. (Jack\nWelch)\nPerfection must be reached by degrees; she requires the slow hand of time\n(Voltaire)\nThe best way to predict the future is to invent it (Alan Kay)\n", "@Mistobaan: any updates?\n", "@vrv I think the operation themselves are up to date and correct. I was not able to create a solid testing for GPU as I don't have a supported GPU environment at the moment.  I just rebased the patch. \n", "Ok, we'll probably soon have GPU testing, right @martinwicke ? :).  Then we can try pulling this in.\n", "@tensorflow-jenkins: test this please\n", "Can one of the admins verify this patch?\n", "tensorflow/core/kernels/training_ops.cc:338:8: error: template-id 'operator()<>' for 'void tensorflow::functor::ApplyAdadelta<Eigen::GpuDevice, double>::operator()(const GPUDevice&, tensorflow::TTypes<double, 1, long int>::Flat, tensorflow::TTypes<double, 1, long int>::Flat, tensorflow::TTypes<double, 1, long int>::Flat, tensorflow::TTypes<double, 1, long int>::ConstScalar, tensorflow::TTypes<double, 1, long int>::ConstScalar, tensorflow::TTypes<double, 1, long int>::ConstScalar, tensorflow::TTypes<double, 1, long int>::ConstFlat)' does not match any template declaration\n   void ApplyAdadelta<GPUDevice, T>::operator()(                           \\\n        ^\ntensorflow/core/kernels/training_ops.cc:348:1: note: in expansion of macro 'DECLARE_GPU_SPEC'\n DECLARE_GPU_SPEC(double);\n ^\ntensorflow/core/kernels/training_ops.cc:345:41: note: saw 1 'template<>', need 2 for specializing a member function template\n       typename TTypes<T>::ConstFlat grad);                                \\\n                                         ^\ntensorflow/core/kernels/training_ops.cc:348:1: note: in expansion of macro 'DECLARE_GPU_SPEC'\n DECLARE_GPU_SPEC(double);\n ^\ntensorflow/core/kernels/training_ops.cc:352:17: error: expected constructor, destructor, or type conversion before '(' token\n REGISTER_KERNELS(GPU, float);\n                 ^\ntensorflow/core/kernels/training_ops.cc:353:17: error: expected constructor, destructor, or type conversion before '(' token\n REGISTER_KERNELS(GPU, double);\n", "(ping this thread when this is ready -- it looks like you added a commit but don't know if it's ready)\n", "@Mistobaan, any update on this? I'm also really interested in trying AdaDelta. Thanks!\n", "@tensorflow-jenkins: test this please\n", "@Mistobaan: I think this looks good pending the GPU tests finishing -- the only other thing I think we need is for you to run\n\n bazel-bin/tensorflow/core/ops/compat/update_ops tensorflow/core/ops\n\nand add that updated file to your commit for tracking backwards compatibility.\n", "@vrv I run and added the modified files as you requested. \n", "@tensorflow-jenkins: test this please\n", "Looks like nothing built -- can you double check and verify that your commit compiles and passes in at least one config?\n", "@vrv let's try again I ran the updated command on an old branch and force pushed \n", "@tensorflow-jenkins: test this please\n"]}, {"number": 643, "title": "Eliminate TileGrad in favor of reshape followed by reduce_sum", "body": "`TileGrad` is exactly equivalent to a reshape which splits each dimension into two pieces, an input dimension piece and a tile dimension piece, then does a reduce_sum over all the tile dimensions to get back the input shape.  There is no need for a separate op.\n\nFor example, if `x.shape = (2, 3)` and we do `tf.tile(x, [5, 7])`, the tiled tensor has shape `(2 * 5, 3 * 7)`.  Given a gradient `y` w.r.t. that output with shape `(2 * 5, 3 * 7)`, we can recover the input gradient as `tf.reduce_sum(tf.reshape(y, [5, 2, 7, 3]), reduction_indices=[0, 2])`.\n\nNote that this works even in the case where `multiples` has zeros: in that case we'll do a `reduce_sum` where some of the dimensions we sum over will be zero.  This is well defined even if the result after summation is nonempty: in that case the result is zero.  If sum doesn't already work in that case, it should.\n", "comments": ["@benoitsteiner: Is there a different reason why a separate `TileGrad` op is required?\n", "I fixed this a while ago.\n"]}, {"number": 642, "title": "cifar10-train: tensorflow/core/common_runtime/executor.cc:1076] 0x3455350 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights", "body": "Hi,\n\nI'm trying to run this model on a system with GeForce GTX 960M:\npython /usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_train.py\nand get the following output:\n\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: \nname: GeForce GTX 960M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.69GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 1.50GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0x501a40000 extends to 0x5616a0000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity/_79 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_344_Identity\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Variable/initial_value/_81 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1185_Variable/initial_value\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv1/biases/Assign/_67 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_304_conv1/biases/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/biases/Assign/_66)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_2/_63 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_293_Identity_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_3/_55 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_275_Identity_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv2/biases/Assign/_51 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_273_conv2/biases/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv2/biases/Assign/_50)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_4/_47 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_262_Identity_4\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_5/_39 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_233_Identity_5\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_6/_31 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_220_Identity_6\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local3/biases/Assign/_35 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_231_local3/biases/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local3/biases/Assign/_34)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_7/_23 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_191_Identity_7\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local4/biases/Assign/_19 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_189_local4/biases/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local4/biases/Assign/_18)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_8/_15 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_178_Identity_8\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_9/_7 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_160_Identity_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: softmax_linear/biases/Assign/_3 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_158_softmax_linear/biases/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax_linear/biases/Assign/_2)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Identity_1/_71 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_306_Identity_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv1/weights/Assign/_75 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_342_conv1/weights/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights/Assign/_74)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: softmax_linear/weights/Assign/_11 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_176_softmax_linear/weights/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax_linear/weights/Assign/_10)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv1/biases/_64 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_303_conv1/biases\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/biases)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv2/biases/_48 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_272_conv2/biases\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv2/biases)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local3/biases/_32 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_230_local3/biases\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local3/biases)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local4/biases/_16 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_188_local4/biases\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local4/biases)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: softmax_linear/biases/_0 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_157_softmax_linear/biases\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax_linear/biases)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: init/NoOp_1/_88 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_1192_init/NoOp_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Variable/Assign/_84 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_1186_Variable/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv1/weights/Assign/_76 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_342_conv1/weights/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv1/biases/Assign/_68 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_304_conv1/biases/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv2/weights/Assign/_60 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_291_conv2/weights/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv2/biases/Assign/_52 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_273_conv2/biases/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local3/weights/Assign/_44 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_260_local3/weights/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local3/biases/Assign/_36 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_231_local3/biases/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local4/weights/Assign/_28 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_218_local4/weights/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local4/biases/Assign/_20 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_189_local4/biases/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: softmax_linear/weights/Assign/_12 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_176_softmax_linear/weights/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: softmax_linear/biases/Assign/_4 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_158_softmax_linear/biases/Assign\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: conv2/weights/Assign/_59 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_291_conv2/weights/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv2/weights/Assign/_58)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local4/weights/Assign/_27 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_218_local4/weights/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local4/weights/Assign/_26)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x322d680 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: local3/weights/Assign/_43 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_260_local3/weights/Assign\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](local3/weights/Assign/_42)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x32983e0 Compute status: Failed precondition: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n     [[Node: Variable/initial_value/_80 = _HostSend[T=DT_INT32, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1185_Variable/initial_value\", _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Variable/initial_value)]]\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_train.py\", line 138, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_train.py\", line 134, in main\n    train()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/models/image/cifar10/cifar10_train.py\", line 94, in train\n    sess.run(init)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.FailedPreconditionError: Attempting to use uninitialized value conv1/weights\n     [[Node: conv1/weights/_72 = _Send[T=DT_FLOAT, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_341_conv1/weights\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](conv1/weights)]]\n\nCan somebody tell me what's wrong?\n\nThe mnist model: tensorflow/models/image/mnist/convolutional.py\nseems to work well: \n\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcublas.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcudnn.so.6.5 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcufft.so.7.0 locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:101] successfully opened CUDA library libcurand.so.7.0 locally\nI tensorflow/core/common_runtime/local_device.cc:40] Local device intra op parallelism threads: 4\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:909] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:103] Found device 0 with properties: \nname: GeForce GTX 960M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\npciBusID 0000:01:00.0\nTotal memory: 2.00GiB\nFree memory: 1.71GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:127] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:137] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:702] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Allocating 1.51GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:52] GPU 0 memory begins at 0x501a40000 extends to 0x562600000\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.0KiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 4.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 8.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 16.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 32.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 64.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 128.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 256.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 512.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 1.00GiB\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:66] Creating bin of max chunk size 2.00GiB\nI tensorflow/core/common_runtime/direct_session.cc:58] Direct session inter op parallelism threads: 4\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nInitialized!\nEpoch 0.00\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nEpoch 0.12\nMinibatch loss: 3.297, learning rate: 0.010000\nMinibatch error: 6.2%\nValidation error: 7.5%\n", "comments": ["I'm having the same issue, it doesn't seem to make sense...\n\nBut this might clear things up:\n\nhttp://stackoverflow.com/questions/34213543/new-error-running-cifar-10-demo-with-tf-0-6-0\n", "Duplicate of https://github.com/tensorflow/tensorflow/issues/481\n", "Is there a commit we should be looking for with the fixes for the CIFAR example (is this a bug with the tutorial / example or with TF?) If it's just a tutorial change, I'd love to test it out.\n", "I think it was a bug somewhere in TF -- if you build from sources you'll get the fix, or in the next release that we cut.\n"]}, {"number": 641, "title": "Mac install inside virtualenv cannot import examples", "body": "I tried the tutorial on my mac, after using virtualenv to install (the normal pip installation failed for me).\n\nI ran\n\n``` bash\n$ cd $PATH_TO_VIRTUALENV_OF_TENSORFLOW\n$ source ./bin/activate\n(tensorflow)$ python\n>>> import tensorflow as tf  # This succeeds\n>>> from tensorflow.examples.tutorials.mnist import input_data\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named examples.tutorials.mnist\n```\n\nInspecting the module tensorflow shows there is no submodule named \"examples\".\n\nI think this is a Mac only issue, not sure if it's only virtualenv too.  When I tried the above on Linux it worked for me.\n", "comments": ["I just tried this with a fresh virtualenv (on a Mac), and it worked. Which version of Tensorflow is this? This should work if `tf.__version__` is 0.6.0.\n\nI'll close this for now.\n", "Just got it to work by having this be my first 3 lines of code: \n\n```\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n```\n"]}, {"number": 640, "title": "Attention modifications", "body": "In the `attention_decoder` of `seq2seq.py`, attention is computed by the statements \n\n``` python\ncell_output, new_state = cell(x, states[-1])\nattns = attention(new_state)\nwith vs.variable_scope(\"AttnOutputProjection\"):\n  output = rnn_cell.linear([cell_output] + attns, output_size, True)\n```\n\nHowever, in Grammar as a Foreign Language, attention is computed using the last hidden vector of the multi-layer LSTM (e.g. `cell_output`). Computing attention with `new_state` involves using every layer's state, including their cell state (which should probably be only used inside the LSTM layer). To match GaaFL, the attention call should be\n\n``` python\nattns = attention(cell_output)\n```\n\nOf course, if the `cell` is wrapped by an `OutputProjectionWrapper` as done in `embedding_attention_seq2seq`, then this call is computationally expensive. To prevent this, `embedding_attention_seq2seq` can be modified as such:\n\n``` python\n# Starting from line 606.\noutput_size = None\nif output_projection is None:\n  output_size = num_decoder_symbols # Projection wrapper removed.\n```\n\nNow, `cell_output` will be the last hidden vector of the multi-layered LSTM. The reason this works is because `output` is already being projected for the softmax, so `cell_output` does not need to be projected to a length of `num_decoder_symbols`. If as before`cell` is wrapped by `OutputProjectionWrapper`, the `cell_output` will have size `num_decoder_symbols`. Computing the `output` will then involve a `(num_decoder_symbols + num_heads * attn_size) x num_decoder_symbols` matrix, which can be huge. However, if we make the change, then the matrix is only of size `(cell.output_size + num_heads * attn_size) x num_decoder_symbols`.\n\nThoughts? Have I missed something important? If this sounds good, I can make a PR. Thanks!\n", "comments": ["Thanks for spotting the bug - you're right that the code uses the whole multi-layer state to compute the attention query while in the paper it was only the output! It should be corrected, but I'm not sure if your solution will work well if the cell is already wrapped in an OutputProjectionWrapper. I mean - it will work technically, but it will basically first apply the cell to the end (to get a vector of size cell.output_size) and then it'll project it back to just output_size (we should change these names too, I'm sorry for the poor choice of variable names in this function).\n\nI see a few possible solutions. The cleanest might be to forbid the use of attention_decoder without output_projection - in fact most use-cases already specify the output projection separately. But, since this is a breaking change, maybe we should start as you suggest. I'll be happy to review a code if you want to write it, of course!\n", "You're totally right about the case when the cell is already wrapped in an OutputProjectionWrapper. However, in this case also, we should remove the line where the cell is again wrapped in OutputProjectionWrapper -- the hidden state would be projected to cell.output_size (which probably equals num_decoder_symbols), and then reprojected to num_decoder_symbols.\n\nSince forbidding attention_decoder without output_projection would be a breaking change, one thing we can do is to check if `isinstance(cell, OutputProjectionWrapper)` and print a warning telling the user to specify output_projection instead. Then in future versions of TF, a `ValueError` can be returned in this case and output_projection will not have a default value.\n", "This issue probably contributes to the bad translation results people have reported from running the translation demo code (see https://github.com/tensorflow/tensorflow/issues/550). @PrajitR, I understand it may not be straightforward to provide a fix for the general seq2seq case, but what might the fix specifically for the _translate_ demo implementation look like?\n", "I vote we get rid of OutputProjectionWrapper completely.  This is the kind\nof problem that it contributes to.  It's also much slower than doing a\nprojection on the whole output.\n\nOn Thu, Dec 31, 2015 at 3:27 AM, Lukasz Kaiser notifications@github.com\nwrote:\n\n> Thanks for spotting the bug - you're right that the code uses the whole\n> multi-layer state to compute the attention query while in the paper it was\n> only the output! It should be corrected, but I'm not sure if your solution\n> will work well if the cell is already wrapped in an\n> OutputProjectionWrapper. I mean - it will work technically, but it will\n> basically first apply the cell to the end (to get a vector of size\n> cell.output_size) and then it'll project it back to just output_size (we\n> should change these names too, I'm sorry for the poor choice of variable\n> names in this function).\n> \n> I see a few possible solutions. The cleanest might be to forbid the use of\n> attention_decoder without output_projection - in fact most use-cases\n> already specify the output projection separately. But, since this is a\n> breaking change, maybe we should start as you suggest. I'll be happy to\n> review a code if you want to write it, of course!\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/640#issuecomment-168177312\n> .\n", "I'm not so sure about 2 points made above. I agree with the general idea though, but before we accept the statements as facts we need to think about them.\n\n(1) I'm not sure that this issue actually makes the models worse. I'll run experiments next week to check it -- it might well be. But it's not clear - does anyone have numbers to support it? I'll try to get some numbers next week.\n\n(2) I'm not sure using OutputProjectionWrapper is much slower. In fact I ran a few simple tests and it was at most marginally slower or the same. Eugene - do you have numbers for that?\n\nStill - we should think about making the attention and output projection situation cleaner. And I think this will require breaking the current API -- this part was maybe just not designed well enough. Let's think how to make it right.\n", "> (1) I'm not sure that this issue actually makes the models worse. I'll run experiments next week to check it -- it might well be. But it's not clear - does anyone have numbers to support it? I'll try to get some numbers next week.\n\nI don't have numbers for translation, but I will say that I have used this seq2seq example pretty heavily with the attention on all hidden states from each layer. I would say with 300k steps plus I get pretty acceptable results.\n\nAlso, maybe I'm missing something here, but can't we just slice the hidden state before we input it into the attention mechanism? If we are only interested in the hidden state output of the last layer, why don't we just slice that part from the total hidden state?\n\nJust as an update I did:\n\n``` python\ncell_output, new_state = cell(x, states[-1])\n\n#I need to do some thinking on begin and size args\ninput_state_attn = tf.slice(new_state, [0, cell.output_size*(num_dec_layers-1)], [-1, cell.output_size])\n\nattns = attention(input_state_attn)\n\nwith vs.variable_scope(\"AttnOutputProjection\"):\n  output = rnn_cell.linear([cell_output] + attns, output_size, True)\n```\n\nHowever, when I run it, the model blows up and I get inf perplexity after 200 steps, so clearly I'm doing something wrong. I turned down the learning rate by 10x and it still exhibits the same behavior. \n\nI looked into the output projection wrapper and it doesn't seem to modify the hidden state argument that is passed into it at all. Therefore, I think you are okay passing a slice of the hidden state into attention, regardless if you're using an output projection or not. \n\nIf anyone can explain the cause of the blowup in this approach, would be very much appreciated.\n", "I've been curious about slicing the output state as well, as mentioned by LeavesBreathe.\n", "Any update on a plan to resolve this?\n", "@lukaszkaiser @ebrevdo Can one of you modify the title, close, mark as contributions welcome, or similar?  \"Attention modifications\" isn't very informative as a title.\n", "I think this has been resolved (in the sense that the current setup works) a while ago, closing.\n", "Is it resolved?  so the code in seq2seq.py has been modified to just make outprojection of the last layer?\nwhere can i see the latest version?\n"]}, {"number": 639, "title": "Using cuda_kernel_helpers in average pooling.", "body": "This removes some code duplication.\n", "comments": ["@tensorflow-jenkins, test this please.\n", "This CL passes my local testing. \n", "LGTM\n"]}, {"number": 638, "title": "could we have some branch to tag the release version. ", "body": "Sometimes the code just have some small issue, if people fetch the master.\n", "comments": ["There's a 0.6.0 tag, but you're right, there should also be a branch so\nsome more parts of github's UI pick it up.\nOn Mon, Dec 28, 2015 at 07:17 kevin yang notifications@github.com wrote:\n\n> Sometimes the code could not work, if people fetch the master\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/638.\n", "I made a 0.6.0 branch. Feel free to submit pull requests against it if you want to backport bugfixes.\n"]}, {"number": 637, "title": "[Website] Download of Docker Images not possible", "body": "This is not related to the source code but to the website.\nI'm not able to download any of the four Docker Images. \n\nb.gcr.io/tensorflow/tensorflow: TensorFlow CPU binary image.\nb.gcr.io/tensorflow/tensorflow:latest-devel: CPU Binary image plus source code.\nb.gcr.io/tensorflow/tensorflow:latest-gpu: TensorFlow GPU binary image.\nb.gcr.io/tensorflow/tensorflow:latest-devel-gpu: GPU Binary image plus source code.\n\nError Message:\n\"404. That\u2019s an error.\nThe requested URL /tensorflow/tensorflow was not found on this server. That\u2019s all we know.\"\n", "comments": ["Sorry, a browser extension made links out of the urls, so I thought I had to download them this way...\n"]}]