[{"number": 35884, "title": "Benchmark test failed on windows", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):tf-nightly\r\n- Python version:3.7.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nFile \"\\\\?\\C:\\Users\\RUNNER~1\\AppData\\Local\\Temp\\Bazel.runfiles_e847arzc\\runfiles\\__main__\\tensorflow_addons\\activations\\rrelu_test.py\", line 88, in benchmarkRreluOp\r\n    self.run_op_benchmark(sess, result.op, min_iters=25)\r\n  File \"C:\\hostedtoolcache\\windows\\Python\\3.7.5\\x64\\lib\\site-packages\\tensorflow_core\\python\\platform\\benchmark.py\", line 389, in run_op_benchmark\r\n    \"throughput\": mbs / median_delta\r\nZeroDivisionError: float division by zero\r\n\r\n**Describe the expected behavior**\r\nSame behavior as ubuntu\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n```python \r\nself.run_op_benchmark(sess, result.op, min_iters=25)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nhttps://github.com/tensorflow/addons/issues/839\r\nIt seems [```time.time()```](https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/platform/benchmark.py#L294) cause the problem and ```timeit.default_timer()``` could fix the problem", "comments": ["Thanks for the analysis.\r\nAs long as we do not add a new external dependency, I am ok with the proposed solution.", "@fsx950223  It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you try using Latest Version 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35884\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35884\">No</a>\n"]}, {"number": 35883, "title": "AlphaDropout & mixed_float16 - Op has type float32 that does not match type float16", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 31\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): v2.1.0-1-ga9af83a149 2.1.0\r\n- Python version: 3.7.5\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): 8.3.1\r\n- CUDA/cuDNN version: 10.2.89 / 7.6.5.33\r\n- GPU model and memory: Nvidia GeForce GTX 1070 TI 8GB\r\n\r\n**Describe the current behavior**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 468, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1290, in convert_to_tensor\r\n    (dtype.name, value.dtype.name, value))\r\nValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: <tf.Tensor 'Cast:0' shape=(None, 1) dtype=float32>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/tmp/test.py\", line 6, in <module>\r\n    dropout = tf.keras.layers.AlphaDropout(0.5)(input)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\", line 773, in __call__\r\n    outputs = call_fn(cast_inputs, *args, **kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/noise.py\", line 202, in call\r\n    return K.in_train_phase(dropped_inputs, inputs, training=training)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 4303, in in_train_phase\r\n    x = switch(training, x, alt)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\", line 4236, in switch\r\n    x = control_flow_ops.cond(condition, then_expression_fn, else_expression_fn)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 1174, in cond\r\n    return cond_v2.cond_v2(pred, true_fn, false_fn, name)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/cond_v2.py\", line 83, in cond_v2\r\n    op_return_value=pred)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/noise.py\", line 197, in dropped_inputs\r\n    x = inputs * kept_idx + alpha_p * (1 - kept_idx)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\", line 902, in binary_op_wrapper\r\n    return func(x, y, name=name)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py\", line 1201, in _mul_dispatch\r\n    return gen_math_ops.mul(x, y, name=name)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\", line 6125, in mul\r\n    \"Mul\", x=x, y=y, name=name)\r\n  File \"/home/phemmer/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 504, in _apply_op_helper\r\n    inferred_from[input_arg.type_attr]))\r\nTypeError: Input 'y' of 'Mul' Op has type float32 that does not match type float16 of argument 'x'.\r\n```\r\n**Describe the expected behavior**\r\nNo exception\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\nmixed_precision.set_policy(mixed_precision.Policy('mixed_float16'))\r\n\r\ninput = tf.keras.Input(shape=(1))\r\ndropout = tf.keras.layers.AlphaDropout(0.5)(input)\r\n```", "comments": ["@phemmer,\r\nCould you please try using float32 instead of mixed_float16. It works for me, please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/aa47f7d48407e665500f9c24c16fb1a8/35883.ipynb).\r\nFor more information refer to [this](https://github.com/tensorflow/tensorflow/issues/34782#issuecomment-562350888) issue.", "Yes, it works, but I'm not quite sure what the intent of that test is. Tensorflow has worked with float32 for ever.\r\nI'm also not seeing any relation to the linked issue. That issue is about compiling the model, which isn't being done in this issue (compiling will happen eventually, but error occurs before we get to that point), and the documentation already covers the need for the final layer in a model to be float32.\r\nLastly other layers, like Dropout and GaussianDropout, work fine with `mixed_float16`. It's only AlphaDropout which blows up.\r\n...Unless the linked issue is just to refer to the immaturity of support for `mixed_float16` in general, and not specifically about compilation, or loss precision.\r\n\r\nApologies if I'm misunderstanding something though.", "You're not misunderstanding anything, as any layer which does not work with `mixed_float16` is a serious bug. Thanks for the report and short example to reproduce! I will address.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35883\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35883\">No</a>\n"]}, {"number": 35882, "title": "[SEGFAULT]: root_node nullptr check", "body": "Accessing root_node after it may possibly still be nullptr.", "comments": ["@gaurav1086 there are some sanity checks failing , can you please take a look.\r\nHere is the link:https://source.cloud.google.com/results/invocations/b15d98de-a35a-4020-8fdf-3d3978f2c401/log", "@rthadur , thanks a lot for the review. \r\nThe two tests that are failing are: \r\ndo_pip_smoke_test: Pip Smoke Test: Checking py_test dependencies exist in pip package\r\ndo_bazel_nobuild: bazel nobuild\r\nThey do not seem to be related to my change. I will run the tests again to see if they prevail.\r\n\r\n", "The sanity failures are due to https://support.sonatype.com/hc/en-us/articles/360041287334 \r\n\r\nThis is a bazel failure, hoping it will be fixed soon", "@mihaimaruseac thanks for the clarification. ", "Yes. You are right. In file: tensorflow/core/platform/default/logging.cc:237, it calls abort(), when the LOG_LEVEL is FATAL,\r\n\r\n if (severity_ == FATAL) {\r\n    abort();\r\n}\r\n\r\nHowever, I think having this change wouldn't hurt as being good coding practice.\r\n", "> However, I think having this change wouldn't hurt as being good coding practice.\r\n\r\nCan you share why you think this is good coding practice?", "@sanjoy , If someone in the future by mistake removes the \"LOG(FATAL) << msg\" statement or changes the log level from FATAL to something else, it might crash the code by accessing the root_node(nullptr). So good to have these additional checks (without undermining performance). However it's obviously a choice. ", "> @sanjoy , If someone in the future by mistake removes the \"LOG(FATAL) << msg\" statement or changes the log level from FATAL to something else, it might crash the code by accessing the root_node(nullptr). So good to have these additional checks (without undermining performance). However it's obviously a choice.\r\n\r\nBut there are cons to adding the `return false;` as well.  For instance if someone does not know `LOG(FATAL)` crashes , they might assume that `ParseInstructionList` legitimately returns false if `root_node` is `nullptr`.\r\n\r\nIMO the right fix here is to add a clear comment that `LOG(FATAL)` crashes so the immediately following deref of `root_node` is OK.  What do you think?", "@sanjoy , that makes sense. Imho we should both write the comment as well as add 'return false' statement.", "> @sanjoy , that makes sense. Imho we should both write the comment as well as add 'return false' statement.\r\n\r\nI don't think we should also add the `return false;` (which will be dead code).\r\n\r\nI'm not convinced by your argument that if someone hypothetically removes / changes `LOG(FATAL)` in the future *then* the `return false;` will help us avoid a crash.  I don't think code needs to be hardened against arbitrary mutations around it :), it will be the responsibility for whoever changed the `LOG(FATAL)` to also make sure we do the right thing for a null `root_node`.  For instance, in the future someone could remove the entire `if (root_node == nullptr) {` clause, but I don't think it is productive to modify the rest of the code so that it keeps working with that mutation.", "Done."]}, {"number": 35881, "title": "[ROCm][XLA] Adding address space cast to generate correct llvm", "body": "Background: Kernel variables in Nvdia/OpenCL side are represented as address space 0, whereas in AMDGPU side is represented as address space 5.\r\n\r\nThis PR replaced `CreateBitCast()` to `CreatePointerBitCastOrAddrSpaceCast()`, aiming to fix AMDGPU address space. Without this change, the generated llvm IR will have a mismatch in address space, causing widespread failures in unit tests. E,g:\r\n\r\n> Invalid bitcast\r\n   %add.typed = bitcast float addrspace(5)* %add.raw to float*\r\n\r\n@whchung @cheshire \r\n\r\n", "comments": []}, {"number": 35880, "title": "Unable to use tensorflow gpu ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 Home v1903\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version:2.1.0\r\n- Python version: 3.5.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 7.1.0\r\n- GPU model and memory: RTX 2080 Super\r\n\r\n\r\n\r\n**Describe the problem**\r\nimport tensorflow as tf not working\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nimport tensorflow as tf\r\n\r\n\r\n**Any other info / logs**\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The file cannot be accessed by the system.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\soumi\\.conda\\envs\\soumil\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The file cannot be accessed by the system.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Hey @soumilkanwal80, this seems similar to #35618 which was solved, did you test the proposed solution ? ", "@soumilkanwal80 As mentioned in the above comment this issue has been solved. You can refer to this comment [here](https://github.com/tensorflow/tensorflow/issues/35618#issuecomment-573740837) for more details. \r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35880\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35880\">No</a>\n"]}, {"number": 35879, "title": "Running fit on an tf.keras.Sequential with a Keras Sequence generator causes deadlock with use_multiprocessing", "body": "> \r\n> == check python ===================================================\r\n> python version: 3.7.4\r\n> python branch: \r\n> python build version: ('default', 'Sep  7 2019 18:27:02')\r\n> python compiler version: Clang 10.0.1 (clang-1001.0.46.4)\r\n> python implementation: CPython\r\n> \r\n> \r\n> == check os platform ===============================================\r\n> os: Darwin\r\n> os kernel version: Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.26~2/RELEASE_X86_64\r\n> os release version: 19.0.0\r\n> os platform: Darwin-19.0.0-x86_64-i386-64bit\r\n> linux distribution: ('', '', '')\r\n> linux os distribution: ('', '', '')\r\n> mac version: ('10.15', ('', '', ''), 'x86_64')\r\n> uname: uname_result(system='Darwin', node='Andrews-MacBook.local', release='19.0.0', version='Darwin Kernel Version 19.0.0: Wed Sep 25 20:18:50 PDT 2019; root:xnu-6153.11.26~2/RELEASE_X86_64', machine='x86_64', processor='i386')\r\n> architecture: ('64bit', '')\r\n> machine: x86_64\r\n> \r\n> \r\n> == are we in docker =============================================\r\n> No\r\n> \r\n> == compiler =====================================================\r\n> xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun\r\n> \r\n> == check pips ===================================================\r\n> numpy                1.17.4   \r\n> protobuf             3.10.0   \r\n> tensorflow           2.1.0    \r\n> tensorflow-estimator 2.0.1    \r\n> \r\n> == check for virtualenv =========================================\r\n> True\r\n> \r\n> == tensorflow import ============================================\r\n> tf.version.VERSION = 2.1.0\r\n> tf.version.GIT_VERSION = v2.0.0-rc2-26-g64c3d382ca\r\n> tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.11.45.5)\r\n> \r\n> == env ==========================================================\r\n> LD_LIBRARY_PATH is unset\r\n> DYLD_LIBRARY_PATH is unset\r\n> \r\n> == nvidia-smi ===================================================\r\n> ./tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n> \r\n> == cuda libs  ===================================================\r\n> \r\n> == tensorflow installed from info ==================\r\n> Name: tensorflow\r\n> Version: 2.1.0\r\n> Summary: TensorFlow is an open source machine learning framework for everyone.\r\n> Home-page: https://www.tensorflow.org/\r\n> Author-email: packages@tensorflow.org\r\n> License: Apache 2.0\r\n> Location: /Users/andrew/.local/share/virtualenvs/lib_andrew_scratch--yvJ8pLH/lib/python3.7/site-packages\r\n> Required-by: \r\n> \r\n> == python version  ==============================================\r\n> (major, minor, micro, releaselevel, serial)\r\n> (3, 7, 4, 'final', 0)\r\n> \r\n> == bazel version  ===============================================\r\n> \r\n\r\n**Describe the current behavior**\r\nIt seems all threads are waiting in a deadlock behaviour\r\n\r\n**Describe the expected behavior**\r\nTo train the model\r\n\r\n**Code to reproduce the issue**\r\n\r\n\r\n\r\n```\r\nfrom tensorflow.keras.utils import Sequence\r\nimport tensorflow as tf\r\nimport numpy as  np\r\n\r\nclass DataGenerator(Sequence):\r\n    def __init__(self):\r\n        self.batch_size = 32\r\n        self.output_shape = (6, 12)\r\n\r\n    def __len__(self):\r\n        return 128\r\n\r\n    def __getitem__(self, index):\r\n        X = np.random.uniform(-1, 1, (self.batch_size, *self.output_shape))\r\n        y = np.random.uniform(-1, 1, (self.batch_size, 1))\r\n        return (X, y)\r\n\r\ndef build_model():\r\n    model = tf.keras.Sequential(name='hello')\r\n    model.add(tf.keras.layers.Flatten(input_shape=(6, 12)))\r\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\r\n                  loss=tf.keras.losses.MeanSquaredError())\r\n    return model\r\n\r\nif __name__== \"__main__\":\r\n\r\n    gen = DataGenerator()\r\n    val_gen = DataGenerator()\r\n\r\n    model = build_model()\r\n    model.fit(x = gen, \r\n        validation_data = val_gen,\r\n        epochs = 8,\r\n        workers = 4,\r\n        use_multiprocessing = True,\r\n        shuffle=True)\r\n\r\n```\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n", "comments": ["\r\n[example.py.zip](https://github.com/tensorflow/tensorflow/files/4061814/example.py.zip)\r\n", "I have found removing the line `validation_data = val_gen,` stops the deadlock", "@andrewstanfordjason, \r\nYou can pass a Dataset instance as the validation_data argument in fit.\r\nFor more read [here](https://www.tensorflow.org/guide/keras/train_and_evaluate#using_a_validation_dataset). Thanks! ", "This is indeed my error for not fully reading the docs. It would be nice if the validation data took a generator as the x input does. I guess I can work around it by using evaluate between epochs. Thank you", "I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks!", "\r\nAre there any plans to support `tf.keras.Sequence` instances as arguments for the `validation_data` argument in the future? It is kind of counterintuitive that it works for the `x` and `y` arguments but not for validation_data.", "I have worked around this by using evaluate(). It's not ideal but as my data is generated it works.", "Yes, thanks. I saw your workaround and I am doing actually the same now.", "What is going on here? I wrote a lot of code using Sequence and it is not even deprecated with time given to port, it just stops working with no warning.  My code won't run at all now, and there is no guide to porting a Sequence to a DataSet (a Sequence is not a generator so it is not that easy).  This should not be closed... I am dropping back to 2.0.1.", "This should not be closed. It is counterintuitive to use your `x` data as `tf.keras.Sequence` and your `validation_data` as a `tuple` or `tf.data.Dataset`. And, as jostheim said, there is no guide to porting `tf.keras.Sequence` to a `tf.data.Dataset`, and it does not seem to be easy enough to justify the lack of a guide about how to do. If `tf.keras.Sequence` is not deprecated, why `validation_data` argument in `tf.keras.fit()` cannot support it?", "Is anyone working on this or the ```keras.Sequence``` is not going to be working anymore?", "For others who have issues porting Sequence to Dataset as others in this thread, `keras.Sequence` implements the `__iter__` method.  So this can be easily passed to [`tf.data.Dataset.from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) to create a new Dataset that contains all the elements of that Sequence. Dirty solution, that is imho not a good one, but it gets the job done. "]}, {"number": 35878, "title": "MultiWorkerMirroredStrategy Keras Example Hangs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3.7.3\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla K80, 1 GPU per worker, 2 workers\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I run the distributed training example for the MultiWorkerMirroredStrategy with Keras documented [here](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras), approximately half the time the training will be successful and otherwise the workers will hang after the first epoch.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe training should successfully complete every time with no hanging.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport os, json\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({'cluster': {'worker': ['X.X.X.X:2000', 'X.X.X.X:2000']}, 'task': {'type': 'worker', 'index': 0}})\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\ntf.config.optimizer.set_jit(True)\r\ntfds.disable_progress_bar()\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\nNUM_WORKERS = 2\r\n\r\ndef make_datasets_unbatched():\r\n  # Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  datasets, info = tfds.load(name='mnist',\r\n                            with_info=True,\r\n                            as_supervised=True)\r\n  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n\r\ntrain_datasets = make_datasets_unbatched().batch(BATCH_SIZE)\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n    tf.keras.layers.MaxPooling2D(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n    metrics=['accuracy'])\r\n  return model\r\nGLOBAL_BATCH_SIZE = 64 * NUM_WORKERS\r\nwith strategy.scope():\r\n  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n  multi_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\n2020-01-14 20:53:41.329726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-14 20:53:41.393549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.394307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\n2020-01-14 20:53:41.394552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-01-14 20:53:41.396280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-01-14 20:53:41.397495: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-01-14 20:53:41.397794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-01-14 20:53:41.399430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-01-14 20:53:41.400687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-01-14 20:53:41.404610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-14 20:53:41.404722: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.405478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.406161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-01-14 20:53:41.406769: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-01-14 20:53:41.414319: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300070000 Hz\r\n2020-01-14 20:53:41.414709: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56414dcdbd50 executing computations on platform Host. Devices:\r\n2020-01-14 20:53:41.414747: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\r\n2020-01-14 20:53:41.722508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.723344: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56414dcf9870 executing computations on platform CUDA. Devices:\r\n2020-01-14 20:53:41.723406: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\r\n2020-01-14 20:53:41.723661: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.724362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\n2020-01-14 20:53:41.724432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-01-14 20:53:41.724476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-01-14 20:53:41.724515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-01-14 20:53:41.724558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-01-14 20:53:41.724594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-01-14 20:53:41.724633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-01-14 20:53:41.724670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-14 20:53:41.724757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.725490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.726167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-01-14 20:53:41.726224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-01-14 20:53:41.727650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-14 20:53:41.727681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2020-01-14 20:53:41.727699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2020-01-14 20:53:41.728328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.729081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.729792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n2020-01-14 20:53:41.731085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.731843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\n2020-01-14 20:53:41.731906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\r\n2020-01-14 20:53:41.731950: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-01-14 20:53:41.731993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\r\n2020-01-14 20:53:41.732036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\r\n2020-01-14 20:53:41.732078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\r\n2020-01-14 20:53:41.732119: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\r\n2020-01-14 20:53:41.732161: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-14 20:53:41.732267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.733006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.733681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\r\n2020-01-14 20:53:41.733719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-14 20:53:41.733746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0\r\n2020-01-14 20:53:41.733768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N\r\n2020-01-14 20:53:41.734378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.735121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 20:53:41.735829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\nD0114 20:53:41.736038696   13433 log.cc:95]                  Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:41.736065073   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:41.736085632   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:41.736249487   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:41.736271507   13433 is_epollexclusive_available.cc:86] epoll_ctl with EPOLLEXCLUSIVE | EPOLLONESHOT succeeded. This is evidence of no EPOLLEXCLUSIVE support. Not using epollex polling engine.\r\nI0114 20:53:41.736291557   13433 ev_epollex_linux.cc:1633]   Skipping epollex because it is not supported.\r\nI0114 20:53:41.736308984   13433 ev_epoll1_linux.cc:116]     grpc epoll fd: 22\r\nD0114 20:53:41.736329223   13433 ev_posix.cc:170]            Using polling engine: epoll1\r\nD0114 20:53:41.736356694   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:41.736392859   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:41.736408436   13433 dns_resolver.cc:334]        Using native dns resolver\r\nD0114 20:53:41.736428850   13433 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nE0114 20:53:41.737641232   13433 socket_utils_common_posix.cc:198] check for SO_REUSEPORT: {\"created\":\"@1579035221.737631945\",\"description\":\"SO_REUSEPORT unavailable on compiling system\",\"file\":\"external/grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":166}\r\n2020-01-14 20:53:41.737822: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:258] Initialize GrpcChannelCache for job worker -> {0 -> X.X.X.X:2000, 1 -> localhost:2000}\r\n2020-01-14 20:53:41.738932: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:365] Started server with target: grpc://localhost:2000\r\nNumber of devices: 2\r\nD0114 20:53:43.049644334   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:43.049683824   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:43.049689625   13552 env_linux.cc:71]            Warning: insecure environment read function 'getenv' used\r\nD0114 20:53:43.049770214   13552 dns_resolver.cc:275]        Start resolving.\r\nI0114 20:53:43.050377745   13518 subchannel.cc:1025]         New connected subchannel at 0x7f2a04006d60 for subchannel 0x7f2a18008070\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nTrain for 5 steps\r\nEpoch 1/3\r\n2020-01-14 20:53:46.911991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\r\n2020-01-14 20:53:48.937723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n```\r\nThe worker hangs indefinitely after `Successfully opened dynamic library libcudnn.so.7`\r\n", "comments": ["Encounters an error at the same point of execution in TF 2.1 as TF 2.0 (not just a TF2.0 problem)\r\n```\r\n2020-01-15 19:13:45.016826: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2000\r\nNumber of devices: 2\r\nDownloading and preparing dataset mnist (11.06 MiB) to /root/tensorflow_datasets/mnist/1.0.0...\r\nWARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\r\nlocal data directory. If you'd instead prefer to read directly from our public\r\nGCS bucket (recommended if you're running on GCP), you can instead set\r\ndata_dir=gs://tfds-data/datasets.\r\n\r\nDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/1.0.0. Subsequent calls will reuse this data.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nTrain for 5 steps\r\nEpoch 1/3\r\n2020-01-15 19:14:02.726512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-01-15 19:14:04.253929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-15 19:14:05.262732: I tensorflow/compiler/jit/xla_compilation_cache.cc:242] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n2020-01-15 19:14:05.733056: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:253 : Internal: unhandled system error\r\n2020-01-15 19:14:05.733145: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: unhandled system error\r\n\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\r\n\t [[GroupCrossDeviceControlEdges_0/Identity_3/_11]]\r\n2020-01-15 19:14:05.733273: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: unhandled system error\r\n\t [[{{node scoped_allocator_1_1_CollectiveReduce}}]]\r\nSegmentation fault (core dumped)\r\n```", "Changed `train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)` to `train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE).repeat()` with no success on TF 2.1", "I'm not sure but I think this could be originating from https://github.com/tensorflow/tensorflow/blob/4c9f77709aa612058434d91984e0b7b7e3ef5774/tensorflow/core/nccl/nccl_manager.cc#L51.\r\n\r\nCan you rerun with the [`NCCL_DEBUG`](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html#nccl-debug) environment variable set to `INFO`?", "@dubey with TF 2.1 I now get the following for the following code.\r\n\r\n```python\r\nimport os, json\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n  'cluster': {\r\n    'worker': ['X.X.X.X:2000', 'X.X.X.X:2000'],\r\n  },\r\n  'task': {\r\n    'type': 'worker',\r\n    'index': 0\r\n  }\r\n})\r\nos.environ['NCCL_DEBUG'] = 'INFO'\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING\r\n# strategy = tf.distribute.MirroredStrategy() # NCCL vs RING\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\ndef make_datasets_unbatched():\r\n  # Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  datasets, info = tfds.load(name='mnist',\r\n                            with_info=True,\r\n                            as_supervised=True)\r\n  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\ntrain_datasets = make_datasets_unbatched().batch(BATCH_SIZE)\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n    tf.keras.layers.MaxPooling2D(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n    metrics=['accuracy'])\r\n  return model\r\nGLOBAL_BATCH_SIZE = 64 * 2\r\nwith strategy.scope():\r\n  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n  multi_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)\r\n```\r\n\r\nLogs:\r\n\r\n```\r\n2020-02-18 22:43:03.345917: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2000, 1 -> X.X.X.X:2000}\r\n2020-02-18 22:43:03.347371: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2000\r\nNumber of devices: 8\r\n0117-190446-scram251-10-95-224-133:7890:8187 [0] NCCL INFO NET/Socket : Using [0]eth0:10.95.224.133<0>\r\n0117-190446-scram251-10-95-224-133:7890:8187 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n0117-190446-scram251-10-95-224-133:7890:8187 [0] external/nccl_archive/src/misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nTrain for 5 steps\r\nEpoch 1/3\r\n2020-02-18 22:43:10.121251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-18 22:43:10.477178: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n2020-02-18 22:43:10.808782: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext_3}}]]\r\n\t [[SGD/Cast/ReadVariableOp/_2]]\r\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.\r\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.\r\n0/5 [..............................] - ETA: 0sTraceback (most recent call last):\r\n  File \"x.py\", line 47, in <module>\r\n    multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 187, in run_one_epoch\r\n    aggregator.finalize()\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 144, in finalize\r\n    raise ValueError('Empty training data.')\r\nValueError: Empty training data.\r\n2020-02-18 22:43:11.074620: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```", "cc @ckkuang ", "I suspect the issue is because of automatic data sharding. The program uses `tfds` to create datasets. `tfds` is usually backed up by files in GCS. The mnist dataset probably has only one file, and TF tries to [shard](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions#auto_shard_policy) the file to two workers, so one worker ends up getting no input.\r\n\r\nCan you try disabling the automatic sharding by following instructions [here](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#dataset_sharding_and_batch_size)? Alternatively you can also try setting the `auto_shard_policy` to DATA.", "Hi @ckkuang, I have tried your suggestion but unfortunately I receive the same error. Here is the change I made:\r\n\r\n```python\r\nimport os, json\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n  'cluster': {\r\n    'worker': ['X.X.X.X:2000', 'X.X.X.X:2000'],\r\n  },\r\n  'task': {\r\n    'type': 'worker',\r\n    'index': 0\r\n  }\r\n})\r\nos.environ['NCCL_DEBUG'] = 'INFO'\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING\r\n# strategy = tf.distribute.MirroredStrategy() # NCCL vs RING\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\ndef make_datasets_unbatched():\r\n  # Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  datasets, info = tfds.load(name='mnist',\r\n                            with_info=True,\r\n                            as_supervised=True)\r\n  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n\r\ntrain_datasets = make_datasets_unbatched().batch(BATCH_SIZE)\r\n\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\r\ntrain_datasets = train_datasets.with_options(options)\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n    tf.keras.layers.MaxPooling2D(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n    metrics=['accuracy'])\r\n  return model\r\nGLOBAL_BATCH_SIZE = 64 * 2\r\nwith strategy.scope():\r\n  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\n  multi_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)\r\n```\r\n\r\nlogs:\r\n```\r\nTrain for 5 steps\r\nEpoch 1/3\r\n2020-02-19 00:40:30.369268: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-19 00:40:30.720959: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\n\t [[metrics/accuracy/div_no_nan/allreduce_1/CollectiveReduce/_10]]\r\n2020-02-19 00:40:30.721068: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence\r\n\t [[{{node IteratorGetNext}}]]\r\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.\r\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 15 batches). You may need to use the repeat() function when building your dataset.\r\n0/5 [..............................] - ETA: 0sTraceback (most recent call last):\r\n  File \"x.py\", line 54, in <module>\r\n    multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 187, in run_one_epoch\r\n    aggregator.finalize()\r\n  File \"/databricks/conda/envs/databricks-ml-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\", line 144, in finalize\r\n    raise ValueError('Empty training data.')\r\nValueError: Empty training data.\r\n2020-02-19 00:40:30.966412: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```", "I also tried turning off autosharding before calling `train_datasets.batch(BATCH_SIZE)`", "Ah it works now - I added the fix in the wrong place, thanks! I am now receiving a different error but the training makes progress through all the epochs.\r\n\r\n```python\r\nimport os, json\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n  'cluster': {\r\n    'worker': ['X.X.X.X:2000', 'X.X.X.X:2000'],\r\n  },\r\n  'task': {\r\n    'type': 'worker',\r\n    'index': 0\r\n  }\r\n})\r\nos.environ['NCCL_DEBUG'] = 'INFO'\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING\r\n# strategy = tf.distribute.MirroredStrategy() # NCCL vs RING\r\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\r\nBUFFER_SIZE = 10000\r\nBATCH_SIZE = 64\r\ndef make_datasets_unbatched():\r\n  # Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image /= 255\r\n    return image, label\r\n  datasets, info = tfds.load(name='mnist',\r\n                            with_info=True,\r\n                            as_supervised=True)\r\n  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\r\n    tf.keras.layers.MaxPooling2D(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(64, activation='relu'),\r\n    tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n    loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n    metrics=['accuracy'])\r\n  return model\r\nGLOBAL_BATCH_SIZE = 64 * 2\r\nwith strategy.scope():\r\n  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE).repeat()\r\n  options = tf.data.Options()\r\n  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n  train_datasets = train_datasets.with_options(options)\r\n  multi_worker_model = build_and_compile_cnn_model()\r\nmulti_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)\r\n```\r\n\r\nlogs:\r\n\r\n```\r\nTrain for 5 steps\r\nEpoch 1/3\r\n2020-02-19 00:50:39.229227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-02-19 00:50:40.274897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-02-19 00:50:42.146923: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found\r\nRelying on driver to perform ptx compilation. This message will be only logged once.\r\n0117-190446-scram251-10-95-238-76:12426:12816 [0] NCCL INFO NET/Socket : Using [0]eth0:10.95.238.76<0>\r\n0117-190446-scram251-10-95-238-76:12426:12816 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\r\n\r\n0117-190446-scram251-10-95-238-76:12426:12816 [0] external/nccl_archive/src/misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -> 2\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO CUDA Dev 0[0], Socket NIC distance :  SYS\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -> 2\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Ring 00 : 0 -> 1 [receive] via NET/Socket/0\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -> 2\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Ring 00 : 1 -> 0 [send] via NET/Socket/0\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -> 2\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Ring 01 : 0 -> 1 [receive] via NET/Socket/0\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Could not find real path of /sys/class/net/eth0/device\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO bazel-out/k8-py2-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/net.h:24 -> 2\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO Ring 01 : 1 -> 0 [send] via NET/Socket/0\r\n0117-190446-scram251-10-95-238-76:12426:12818 [0] NCCL INFO comm 0x7f9c9800d180 rank 1 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE\r\n5/5 [==============================] - 5s 1s/step - loss: 2.3133 - accuracy: 0.0969\r\nEpoch 2/3\r\n5/5 [==============================] - 0s 5ms/step - loss: 2.3054 - accuracy: 0.0984\r\nEpoch 3/3\r\n5/5 [==============================] - 0s 5ms/step - loss: 2.3137 - accuracy: 0.0984\r\n2020-02-19 00:50:42.330314: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n2020-02-19 00:50:42.341750: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled\r\n2020-02-19 00:50:42.560553: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n```", "Looks like `train_datasets` is overwritten at the next line of `with strategy.scope():`. Can you try setting the option after that? You can also set the option inside `make_datasets_unbatched()` before returning the dataset.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35878\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35878\">No</a>\n", "Thanks for confirmation! The new logs are only showing warnings about caching more input data than needed. Closing this issue now. Feel free to reopen if you encounter new errors.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35878\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35878\">No</a>\n", "Gotcha, thanks @ckkuang !"]}, {"number": 35877, "title": "Bazel build fail: ModuleNotFoundError: No module named 'google.protobuf'", "body": "\r\n\r\n**System: MacOS Mojave version 10.14.4**\r\n\r\n**Description:** \r\ncompiled from master branch and got error `ModuleNotFoundError: No module named 'google.protobuf'`\r\n\r\n**Commands followed:** \r\n```shell\r\npip install -U --user pip six numpy wheel setuptools mock 'future>=0.17.1'\r\npip install -U --user keras_applications --no-deps\r\npip install -U --user keras_preprocessing --no-deps\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow # defaults to master branch -> compiled from master branch. \r\n./configure \r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n**Error:** \r\n```shell \r\nERROR: /Users/johnkarasev/PycharmProjects/untitled/tensorflow/tensorflow/BUILD:866:1: Executing genrule //tensorflow:tf_python_api_gen_v2 failed (Exit 1)\r\nTraceback (most recent call last):\r\n  File \"/private/var/tmp/_bazel_johnkarasev/ecc035b543493d4fee11271827f64626/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.ru\r\nnfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py\", line 27, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"/private/var/tmp/_bazel_johnkarasev/ecc035b543493d4fee11271827f64626/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.ru\r\nnfiles/org_tensorflow/tensorflow/python/__init__.py\", line 64, in <module>\r\n    from tensorflow.core.framework.graph_pb2 import *\r\n  File \"/private/var/tmp/_bazel_johnkarasev/ecc035b543493d4fee11271827f64626/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.ru\r\nnfiles/org_tensorflow/tensorflow/core/framework/graph_pb2.py\", line 7, in <module>\r\n    from google.protobuf import descriptor as _descriptor\r\nModuleNotFoundError: No module named 'google.protobuf'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /Users/johnkarasev/PycharmProjects/untitled/tensorflow/tensorflow/tools/pip_package/BUILD:41:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen failed (Exit\r\n 1)\r\nINFO: Elapsed time: 6.212s, Critical Path: 1.12s\r\nINFO: 12 processes: 12 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n", "comments": ["**graph_pb2.py**\r\n```python\r\n# -*- coding: utf-8 -*-\r\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\r\n# source: tensorflow/core/framework/graph.proto\r\n\r\nimport sys\r\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))\r\nfrom google.protobuf import descriptor as _descriptor # <- ERROR HERE\r\nfrom google.protobuf import message as _message \r\nfrom google.protobuf import reflection as _reflection\r\nfrom google.protobuf import symbol_database as _symbol_database\r\n# @@protoc_insertion_point(imports)\r\n\r\n```\r\n", "`pip3 install protobuf` duh lol", "\ud83d\udc46 that fixed it"]}, {"number": 35876, "title": "Keras `predict` with sigmoid output returns probabilities instead of labels", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yee\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.10\r\n- TensorFlow installed from (source or binary): PyPI wheel\r\n- TensorFlow version: v2.1.0-rc2-17-ge5bf8de\r\n- Python version: 3.7.5\r\n\r\n**Describe the current behavior**\r\n\r\nThe `predict` method of a Keras model with a sigmoid activiation function for the output returns probabilities.\r\n\r\n**Describe the expected behavior**\r\n\r\n`predict` should return class indices or class labels, as in the case of softmax activation. The current behavior also seems to be redundant with the method `predict_proba`, which in turn doesn't seem to be documented, at least on https://www.tensorflow.org/api_docs/python/tf/keras/Model\r\n\r\n#7287 is perhaps related.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Activation\r\n\r\nmodel = Sequential([\r\n    Dense(10, activation = \"relu\"),\r\n    Dense(1, activation = \"sigmoid\")])\r\nmodel.compile(\r\n    optimizer = \"rmsprop\",\r\n    loss = \"binary_crossentropy\")\r\nmodel.fit(\r\n    [[1, 2], [1, 3], [1, 1], [2, 2], [2, 3]],\r\n    [True, False, False, True, True])\r\n\r\nprint(model.predict([[1, 2], [1, 3], [1, 1]]))\r\n```", "comments": ["There is no `predict_proba` method in the keras API, contrary to the scikit-learn one.\r\n\r\nThus, `predict` always returns the predicted probabilities, which you can easily transform into labels if you wish, either using `tf.argmax(prediction, axis=-1)` (for softmax activation) or, in your example case, `tf.greater(prediction, .5)` (provided you want to use a .5 threshold, which you could also adjust depending on your modelling context).", "> There is no `predict_proba` method in the keras API, contrary to the scikit-learn one.\r\n\r\n`model.predict_proba` exists, has a docstring, seems to do the same thing as `predict` (at least for sigmoid activation), and doesn't seem to have a deprecation warning or anything.\r\n\r\n> Thus, `predict` always returns the predicted probabilities\r\n\r\nNot for softmax activation.", "> `model.predict_proba` exists, has a docstring, seems to do the same thing as `predict` (at least for sigmoid activation), and doesn't seem to have a deprecation warning or anything.\r\n\r\nSorry, it appears that `Sequential` indeed provides with a `predict_proba` method; it is, however, not inherited from the base `Model` class, and thus not always available (hence my ignoring its existence - sorry). As `predict` comes from this base class, which does not have a `predict_proba`, it returns predicted probabilities (it it acted differently, it would be breaking the basic keras API).\r\n\r\nNow, I do not understand why there is a `predict_proba` implemented as `Sequential` level... I will check the source code.\r\n\r\n> Not for softmax activation.\r\n\r\nI must insist that softmax does yield predicted probabilities.\r\n\r\n```python\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Activation\r\n\r\nmodel = Sequential([\r\n    Dense(10, activation = \"relu\"),\r\n    Dense(2, activation = \"softmax\")])\r\nmodel.compile(\r\n    optimizer = \"rmsprop\",\r\n    loss = \"sparse_categorical_crossentropy\")\r\n\r\nprint(model.predict([[1, 2], [1, 3], [1, 1]]))\r\n```\r\n\r\nOutput (in my case, which obviously will not yield the same values due to weights' randomness):\r\n```\r\n[[0.43271175 0.5672882 ]\r\n [0.3681818  0.63181823]\r\n [0.5206317  0.4793683 ]]\r\n```", "Okay, so [source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/sequential.py) indicates that `Sequential` implements both the `predict_proba` method (that merely calls `predict` and raises an exception if there is any value outside of ]0, 1[) and a `predict_classes` one that basically does what I first indicated (using argmax or a .5 threshold) based on, once again, the outputs of `predict`.", "> I must insist that softmax does yield predicted probabilities.\r\n\r\nI was getting all 1s when I called `model.predict` with softmax, but I had only 1 output node rather than 2 (`Dense(1, activation = \"softmax\")`), which in hindsight, you're probably not supposed to do, unlike for sigmoid activation. Perhaps there should be an error message for this. Thanks for showing me how to do it right.", "Perhaps `predict_proba` and `predict_classes` should be added to `Model` or removed from `Sequential` for consistency.", "> I was getting all 1s when I called model.predict with softmax, but I had only 1 output node\r\n\r\nWell, since softmax balances out a vector of values so that they sum to 1, having a single node will make its output probability jump to 1 in any case, which is indeed an undesirable behavior but could understandably cause your confusion :)\r\n\r\n> Perhaps `predict_proba` and `predict_classes` should be added to Model or removed from Sequential for consistency.\r\n\r\nLet us wait for a tensorflower to pick up the issue and give his/her point of view on that question.\r\nImho, the issue with porting it everywhere is that the implemented methods are targeted at single output tensors, while `Model` allows for much more general behaviors. But having these methods in `Sequential` can indeed be confusing.", "@Kodiologist ,\r\nAny update on the issue ?Thanks!", "@oanush I don't understand. What sort of update are you hoping to receive from me?", "@Kodiologist Thanks for your issue. Perhaps  you may want to raise a feature request for adding `predict_proba` and `predict_classes` methods  for functional or subclassed models.\r\nClosing this issue since `model.predict` method is behaving as intended. Thanks!", "@ymodak It's just as well that you didn't actually close the issue, since you might as well use this issue for the API and documentation complaints I mentioned earlier instead of opening a new issue for them.", "Perhaps now the requested features are available: https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/wrappers/scikit_learn.py#L225\r\n", "Closing this issue since it's resolved. Thanks"]}, {"number": 35875, "title": "Colab crashes due to tcmalloc large allocation", "body": "tcmalloc: large alloc 6645350400 bytes == 0x695ff4000 @ 0x7f6d5f840b6b 0x7f6d5f860379 0x7f6d2b49ec27 0x7f6d2b291a7f 0x7f6d2b15d3cb 0x7f6d2b123526 0x7f6d2b1243b3 0x7f6d2b124583 0x7f6d2f9ad7b5 0x7f6d3012d949 0x7f6d3012e3fe 0x7f6d3010ab82 0x7f6d3010b5bf 0x7f6d3010f44c 0x7f6d301110aa 0x7f6d2df5db25 0x7f6d2def4e1b 0x50a8af 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a067e 0x50d966\r\n\r\nThis perhaps happens only I load High-quality images....and ofc it needs more RAM but not more than the RAM offered by colab.Any solutions?\r\n\r\nWARNING: tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 6645350400 exceeds 10% of system memory.\r\n\r\n", "comments": ["What version of TF are you using? Also can you terminate your colab session and start again ? \r\nMany times if you are executing same runtime over and over again colab runtime ram consumption is accumulated.", "I use TF 2.0 for this and I have tried different versions of TensorFlow.Still, the issue is not resolved.\r\nI have tried restarting the session also but nothing fixed.", "@DineshRajanT, Can you share the colab gist to analyze the issue. Thanks", "\r\nTimestamp | Level | Message\r\n-- | -- | --\r\nJan 17, 2020, 1:25:05 PM | WARNING | WARNING:root:kernel fa12775d-feb3-4b3f-8cce-813369a6303d restarted\r\nJan 17, 2020, 1:25:05 PM | INFO | KernelRestarter: restarting kernel (1/5), keep random ports\r\nJan 17, 2020, 1:24:59 PM | WARNING | tcmalloc: large alloc 6645350400 bytes == 0x4e6616000 @ 0x7fa458f66b6b 0x7fa458f86379 0x7fa424bdec27 0x7fa4249d1a7f 0x7fa42489d3cb 0x7fa424863526 0x7fa4248643b3 0x7fa424864583 0x7fa4290ed7b5 0x7fa42986d949 0x7fa42986e3fe 0x7fa42984ab82 0x7fa42984b5bf 0x7fa42984f44c 0x7fa4298510aa 0x7fa42769db25 0x7fa427634e1b 0x50a8af 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a067e 0x50d966\r\nJan 17, 2020, 1:24:59 PM | WARNING | 2020-01-17 07:54:59.920376: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 6645350400 exceeds 10% of system memory.\r\nJan 17, 2020, 1:24:58 PM | WARNING | tcmalloc: large alloc 6645350400 bytes == 0x35a496000 @ 0x7fa458f66b6b 0x7fa458f86379 0x7fa424bdec27 0x7fa4249d1a7f 0x7fa42489d3cb 0x7fa424863526 0x7fa4248643b3 0x7fa424864583 0x7fa42a7f2445 0x7fa42986d949 0x7fa42986e3fe 0x7fa42984ab82 0x7fa42984b5bf 0x7fa42984f44c 0x7fa4298510aa 0x7fa42769db25 0x7fa427634e1b 0x50a8af 0x50c5b9 0x508245 0x5096b7 0x595311 0x5a522c 0x557f7e 0x50ece0 0x509455 0x595311 0x5a067e 0x50d966 0x508245 0x509642\r\nJan 17, 2020, 1:24:58 PM | WARNING | 2020-01-17 07:54:58.468326: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 6645350400 exceeds 10% of system memory.\r\nJan 17, 2020, 1:24:57 PM | WARNING | tcmalloc: large alloc 6645350400 bytes == 0x418ec000 @ 0x7fa458f66b6b 0x7fa458f86379 0x7fa424bdec27 0x7fa4249d1a7f 0x7fa42489d3cb 0x7fa424863526 0x7fa4248643b3 0x7fa424864583 0x7fa4290ed7b5 0x7fa42986d949 0x7fa42986e3fe 0x7fa42984ab82 0x7fa42984b5bf 0x7fa42984f44c 0x7fa4298510aa 0x7fa42769db25 0x7fa427634e1b 0x50a8af 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509455 0x595311 0x5a067e 0x50d966\r\nJan 17, 2020, 1:24:57 PM | WARNING | 2020-01-17 07:54:57.756308: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 6645350400 exceeds 10% of system memory.\r\nJan 17, 2020, 1:24:55 PM | WARNING | tcmalloc: large alloc 6645350400 bytes == 0x1ce316000 @ 0x7fa458f66b6b 0x7fa458f86379 0x7fa424bdec27 0x7fa4249d1a7f 0x7fa42489d3cb 0x7fa424863526 0x7fa4248643b3 0x7fa424864583 0x7fa428dbff76 0x7fa428e568c6 0x7fa42986d949 0x7fa42986e3fe 0x7fa42984ab82 0x7fa42984b5bf 0x7fa42984f44c 0x7fa4298510aa 0x7fa42769db25 0x7fa427634e1b 0x50a8af 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50d390 0x508245 0x50a080 0x50aa7d 0x50d390 0x509455 0x595311 0x5a067e\r\nJan 17, 2020, 1:24:55 PM | WARNING | 2020-01-17 07:54:55.587820: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 6645350400 exceeds 10% of system memory.\r\n\r\n", "potential duplicate https://github.com/tensorflow/tensorflow/issues/18736", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 35874, "title": "Executing XLA compiled function inside tf.GradientTape context leads to extraneous GPU kernels and D2D copies", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v1.12.1-21967-gd80fda0 2.1.0-dev20200109\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA Version 10.1.243 / cuDNN 7.6.4.38-1\r\n- GPU model and memory: TITAN V, 12GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am trying to speed my code using tf.function decoration, with `experimental_compile=True`. I observe that my code indeed runs faster using this decoration, but executing the function within a `tf.GradientTape()` context seems to generate two GPU kernels instead of one, even though I am not computing any gradients. Moreover, executing the function inside of the `tf.GradientTape()` context produces two D2D copies, which are not present when executing outside of `tf.GradientTape()`. The following illustrates my point (the first kernel is running outside `tf.GradientTape()`and the second is running inside `tf.GradientTape()`:\r\n\r\n![Screen Shot 2020-01-14 at 12 40 12 PM](https://user-images.githubusercontent.com/10225518/72367919-0cd88980-36cb-11ea-9d0c-2fc0937a5db6.png)\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nI am not sure if this behavior is expected or not. If it is not expected, I would certainly like it if there were less GPU kernel calls, leading to much faster code.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python import eager\r\n\r\ndef main():\r\n\r\n    assert tf.executing_eagerly()\r\n\r\n    gpus_p = tf.config.experimental.list_physical_devices('GPU')\r\n    if gpus_p:\r\n        try:\r\n            # Currently, memory growth needs to be the same across GPUs\r\n            for gpu in gpus_p:\r\n                tf.config.experimental.set_memory_growth(gpu, True)\r\n        except RuntimeError as e:\r\n            # Memory growth must be set before GPUs have been initialized\r\n            print(e)\r\n\r\n\r\n    def tf_forward(x,xmax):\r\n        return tf.quantization.fake_quant_with_min_max_vars(inputs=x,\r\n                                                            min=-xmax,\r\n                                                            max=xmax,\r\n                                                            num_bits=8)\r\n\r\n\r\n    tf_forward_tf_function = tf.function(func=tf_forward,\r\n                                        experimental_compile=True)\r\n\r\n    \r\n\r\n    x = tf.random.uniform(shape=[3, 3, 96, 1])\r\n    xmax = tf.Variable(0.5)\r\n\r\n    tf_forward_tf_function(x,xmax)\r\n\r\n    eager.profiler.start()\r\n    for n in range(1000):\r\n        x = tf_forward_tf_function(x,xmax)\r\n        with tf.GradientTape() as tape:\r\n            tape.watch(x)\r\n            x = tf_forward_tf_function(x,xmax)\r\n    \r\n    profiler_result = eager.profiler.stop()\r\n    eager.profiler.save('/test/log', profiler_result)\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Thanks for such a detailed bug report! I have a fix ready which removes the two extra D2D copies.", "I'll mark the bug as fixed based on removing those extra copies. From what I understand, the fact that the kernel for gradient computation is launched regardless of whether derivative is used is just a property of eager runtime, and can not really be avoided (if you don't need the gradients => don't launch under the gradient tape).", "Thank you! I apologize for the silly question, but how do I get the fix for the D2D copies?", "You would have to wait for it to land after it passes internal testing.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35874\">No</a>\n", "Actually apologies, this was the incorrect commit, the proper fix has still not landed yet.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35874\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35874\">No</a>\n"]}, {"number": 35873, "title": "having print statements in dataset iterator makes it much slower", "body": "I was trying to write the model training by using gradient Tape. Somehow, it is slower than Keras.fit() method. On debugging lot of time. I came to know that the reason was having some other code while iterating through the dataset, I was storing few things in a list and a print statements.\r\n\r\nIt is what slowing the program. here is the code that is required. I've run it multiple times.\r\n![image](https://user-images.githubusercontent.com/14745146/72365972-55ba2d80-371f-11ea-9524-757d4fb9197d.png)\r\n\r\nAnd after adding a print statement, \r\n![image](https://user-images.githubusercontent.com/14745146/72366050-7b473700-371f-11ea-88d5-ba93b33d5da5.png)\r\n\r\nThe time got reduced if I reduce the print statements. \r\n![image](https://user-images.githubusercontent.com/14745146/72366092-974ad880-371f-11ea-9822-6f84c3190cc4.png)\r\n\r\nCan anyone explain this?\r\n", "comments": ["here is the code that I've used for train_step..\r\n\r\n```python\r\n@tf.function\r\ndef train_step__(model, batch, labels, loss_fn, optimizer, acc_fn):\r\n    with tf.GradientTape() as tape:\r\n        # recored all the values on a tape in forward pass\r\n        out = model(batch)\r\n        loss = loss_fn(y_true=labels, y_pred=out)\r\n        acc = acc_fn(y_true=labels, y_pred=out)\r\n    # backward pass : grad of parameters w.r.t Loss\r\n    gradients = tape.gradient(loss, model.trainable_variables)\r\n    optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\r\n    # return the loss before applying the gradients\r\n    return loss, acc\r\n```\r\n\r\nIf I wish to add some verbose statements while training using Gradient Tape, It seems it is slowing down the training process. \r\n\r\nis there any alternative to print verbose statements like these to output while training without slowing down the training process like here?", "Not a TF issue:\r\n\r\n```python\r\nimport timeit\r\nimport sys\r\n\r\ndef no_print(n=100):\r\n  s = 0\r\n  for x in range(n):\r\n    s += x\r\n\r\ndef do_print(n=100):\r\n  s = 0\r\n  for x in range(n):\r\n    s += x\r\n    print(s)\r\n\r\n\r\nn=10\r\nprint(timeit.timeit(stmt=lambda:no_print(n)), file=sys.stderr)\r\nprint(timeit.timeit(stmt=lambda:do_print(n)), file=sys.stderr)\r\n```\r\n\r\nAs the following results show, printing in a loop is supposed to slow you down.\r\n\r\n```\r\n(gh35873) mihaimaruseac@ankh:/tmp/gh35873$ python test.py >/dev/null\r\n0.6009224290028214\r\n4.1080495161004364\r\n```\r\n\r\nMight I suggest looking into `tf.summary`?", "Ohkkk. Thanks !! \r\n\r\nI will look into the tf.summary.\r\n\r\nThank you once again."]}, {"number": 35872, "title": "cudart64_101.dll and nvcuda.dll not found", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:2\r\n- Python version:3.7\r\n- Installed using :pip\r\n- CUDA/cuDNN version: none\r\n- GPU model and memory:1696\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI m not able to install Tensorflow due to a problem with my Graphic Card\r\nhere's the error message : \r\n![CG](https://user-images.githubusercontent.com/48018890/72365775-2a244c80-36f9-11ea-9fe4-5c5d8e97c031.jpg)\r\n![pb CG](https://user-images.githubusercontent.com/48018890/72365807-3c9e8600-36f9-11ea-8df3-bf0c4d5b6055.jpg)\r\ntensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.", "comments": ["It's a warning message stating gpu setup is incorrect. TF is successfully installed on your machine however you won't be able to use GPU for computations and operations will be placed on CPU. \r\n", "so I can use Tensorflow anyway ? if it's so, how please ? thank you ", "As it says in the log `Ignore above cudart dlerror if you dont have gpu setup`.\r\nYou can safely ignore the logs and can use tf functionality.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35872\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35872\">No</a>\n", "if you have a gpu, and it is giving that error, what should you do", "If you have a GPU and still you are seeing this error it can occur due to many reasons. Most common reason is incompatible TF and Cuda version. Note TF version 2.0 supports cuda 10.0\r\nFurther also make sure you update appropriate cuda environment paths.\r\nSee  https://www.tensorflow.org/install/gpu#software_requirements"]}, {"number": 35871, "title": "Add usage example for MaxPool2D", "body": "Add usage example for MaxPool2D", "comments": ["Hi, @rthadur, It has been a couple of days since this has been last looked at. Is there anything I need to do and is there any way I can expedite the review of this PR as I am doing it as part of Google CodeIn 2019.", "@dynamicwebpaige gentle ping for review.\r\ncc @mihaimaruseac ", "@mihaimaruseac I've updated it so that it prints the shape of the output instead of the actual output. Is this ok? If not, how should I be ensuring that the output is always the same?", "@rthadur, Mihai Maruseac mentioned that I would need to print an output for the doctest. Do you know whether the current commit has an acceptable output for the doctest?", "@rthadur as mentioned, I have until the 23rd to finish this pr so if it is possible could you give a little guidance on what needs to be done here. Thanks.", "@Joseph-Rance sorry for the delay, we had a 3-days long weekend over here so I minimized amount of time spent with work related things.\r\n\r\nYou should try to ensure that the output always stays the same. See guidelines at https://www.tensorflow.org/community/contribute/docs_ref", "Hi, thanks for the reply. I'll try to get the output correct based on that documentation. Do you know how I can remove the random element of the .fit() function, though?", "I couldn't find a way to get the same output every time with the .fit() function, so I removed the conv2d() layer from the model (as they are not necessary for the usage example) to remove the need for .fit(). Now the code should make the same prediction each time.", "Hi, I just wanted to say thank you for all your help (and putting up with my @ ing over the weekend). Joseph"]}, {"number": 35870, "title": "Install Tensorflow C++ API", "body": "**System information**\r\n- OS Platform and Distribution :  Linux Ubuntu 18.04\r\n- TensorFlow installed from : Anaconda\r\n- TensorFlow version: 1.15\r\n- Python version:3.7(anaconda)\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source):1.2.1\r\n- GCC/Compiler version (if compiling from source):7.4.0\r\n- CUDA/cuDNN version: cuda10.1 cudnn 7.6\r\n- GPU model and memory:  NVIDIA-SMI 435.21       Driver Version: 435.21 \r\n\r\n\r\n\r\n**Describe the problem**\r\nI want to install tensorflow c api and start training my models in c as in python \r\ni followed the instruction mentioned here [https://www.tensorflow.org/install/source](url)\r\nbut after the making configuration by bazel i cannot get my code running \r\nmy code \r\n`#include \"tensorflow/cc/ops/const_op.h\"\r\n#include \"tensorflow/cc/ops/image_ops.h\"\r\n#include \"tensorflow/cc/ops/standard_ops.h\"\r\n#include \"tensorflow/core/framework/graph.pb.h\"\r\n#include \"tensorflow/core/framework/tensor.h\"\r\n#include \"tensorflow/cc/framework/gradients.h\"\r\n#include \"tensorflow/core/graph/default_device.h\"\r\n#include \"tensorflow/core/graph/graph_def_builder.h\"\r\n#include \"tensorflow/core/lib/core/errors.h\"\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n#include \"tensorflow/core/lib/core/threadpool.h\"\r\n#include \"tensorflow/core/lib/io/path.h\"\r\n#include \"tensorflow/core/lib/strings/stringprintf.h\"\r\n#include \"tensorflow/core/platform/init_main.h\"\r\n#include \"tensorflow/core/platform/logging.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/util/command_line_flags.h\"\r\n#include \"tensorflow/core/platform/load_library.h\"`\r\nit gives this dependency errors due to missing files (.h files)\r\nI have two questions\r\n1- where should be the header files placed ?\r\n2- how can i get the missing files which is not here [https://github.com/tensorflow/tensorflow/tree/master/tensorflow](url) ?", "comments": ["@AbduElrahmanRezk,\r\nCould you please go through [this](https://www.tensorflow.org/install/lang_c) link and check if it works. Thanks!", "Any updates regarding this issue? Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35870\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35870\">No</a>\n"]}, {"number": 35869, "title": "TensorFlow Lite: Makefile fix", "body": "With recent versions, TensorFlow Lite fails to compile using the provided Makefile: missing file for FFT2D, resulting in missing functions, and extra files for ABSL and Ruy, resulting in main() functions included in the generated library.\r\n\r\nThis small fix makes compilation work again.", "comments": ["I've tried to reproduce this but I can't see this on master branch. Did you have any change on tensorflow/lite/tools/make/download_dependencies.sh ? Which command did you use to build TFLite?", "The fix was actually incomplete, I've modified my commit accordingly.\r\n\r\nThe problem doesn't show directly because the provided Makefile produces a static library. When the executables are created using that library, the linker only picks used object files and everything goes fine.\r\n\r\nHowever I work in an environment where it's preferred to have dynamic libraries. So I built one with the following command:\r\n`g++ -shared -o tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so -Wl,--whole-archive tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a -Wl,--no-whole-archive,--as-needed -lstdc++ -lpthread -lm -lz -ldl`\r\n(assuming I compiled the static library with `make -f tensorflow/lite/tools/make/Makefile BUILD_WITH_NNAPI=false TARGET=linux TARGET_ARCH=x86_64`)\r\n\r\nWhen using the previous command, you can see the multiple main() definition problem:\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a(print_hash_of.o): In function `main':\r\n> print_hash_of.cc:(.text.startup+0x0): multiple definition of `main'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.a(tune_tool.o):tune_tool.cc:(.text.startup+0x0): first defined here\r\n> collect2: error: ld returned 1 exit status\r\n\r\nAs for the rest, they can be triggered by trying to compile the example minimal executable using the dynamic library instead of the static one. This results in the following errors:\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `tflite::resource::ResourceVariable::AssignFrom(TfLiteTensor const*)'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `absl::Mutex::Unlock()'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `absl::GetStackTrace(void**, int, int)'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `absl::Mutex::ReaderLock()'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `absl::Mutex::ReaderUnlock()'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `absl::Mutex::~Mutex()'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `rdft2d'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `absl::Mutex::Lock()'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `tflite::resource::GetResourceVariable(std::unordered_map<int, std::unique_ptr<tflite::resource::ResourceBase, std::default_delete<tflite::resource::ResourceBase> >, std::hash<int>, std::equal_to<int>, std::allocator<std::pair<int const, std::unique_ptr<tflite::resource::ResourceBase, std::default_delete<tflite::resource::ResourceBase> > > > >*, int)'\r\n> tensorflow/lite/tools/make/gen/linux_x86_64/lib/libtensorflow-lite.so: undefined reference to `tflite::resource::CreateResourceVariableIfNotAvailable(std::unordered_map<int, std::unique_ptr<tflite::resource::ResourceBase, std::default_delete<tflite::resource::ResourceBase> >, std::hash<int>, std::equal_to<int>, std::allocator<std::pair<int const, std::unique_ptr<tflite::resource::ResourceBase, std::default_delete<tflite::resource::ResourceBase> > > > >*, int)'\r\n> collect2: error: ld returned 1 exit status\r\n\r\nWhen applying the proposed fix, it compiles fine.\r\n\r\nAs for the tensorflow/lite/tools/make/download_dependencies.sh, I didn't make any change to it. There used to be a wrong URL referenced in it (still present in the 2.1.0 release) but this has already been fixed with commit 0438fe6.", "> I see these fft2d, absl changes are needed. But I can't find any fix related to Ruy. Could you confirm?\r\n\r\nThe Ruy-related fix is the following line (line 131):\r\n> `$(wildcard tensorflow/lite/*/*/*tool.cc) \\`\r\n\r\nIt is implemented in a more 'generic' way to avoid potential future similar problems with tools being compiled into the library.", "There are some changes landed to fix Makefile build recently.\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/83d0eb1daa31b621d46319326b572d0807e562fb#diff-83ecfeac89b493b8d443b0e83d24ebe0\r\nhttps://github.com/tensorflow/tensorflow/commit/ccbbc5846c8fadd8dfda27d1b4b47eeb496b0a78#diff-83ecfeac89b493b8d443b0e83d24ebe0\r\n\r\nDo you still need your patch? Could you sync master and test again?\r\n\r\n", "@bxgaillard Can you please resolve conflicts? Thanks!", "@terryheo wrote:\r\n> There are some changes landed to fix Makefile build recently.\r\n> \r\n> [83d0eb1#diff-83ecfeac89b493b8d443b0e83d24ebe0](https://github.com/tensorflow/tensorflow/commit/83d0eb1daa31b621d46319326b572d0807e562fb#diff-83ecfeac89b493b8d443b0e83d24ebe0)\r\n> [ccbbc58#diff-83ecfeac89b493b8d443b0e83d24ebe0](https://github.com/tensorflow/tensorflow/commit/ccbbc5846c8fadd8dfda27d1b4b47eeb496b0a78#diff-83ecfeac89b493b8d443b0e83d24ebe0)\r\n> \r\n> Do you still need your patch? Could you sync master and test again?\r\n\r\nThese patches fix some (most?) of the problems my patch is addressing. I will test to see if they are enough on their own.\r\n\r\n@gbaned wrote:\r\n> @bxgaillard Can you please resolve conflicts? Thanks!\r\n\r\nSure!\r\n\r\nI'll do my tests and get back to you ASAP.", "> I'll do my tests and get back to you ASAP.\r\n\r\nSo as I said, the aforementioned patches fix part of the problem. In doing the modifications I ran into another problem introduced by recent code which prevents compilation of the benchmark binaries: tensorflow/lite/profiling/profile_summary_formatter.cc is missing from PROFILE_SUMMARIZER_SRCS.\r\n\r\nAnyway I ammended my commit and pushed the changes for your reviewing pleasure.", "There is a PR for the PROFILE_SUMMARIZER_SRCS fix.\r\nhttps://github.com/tensorflow/tensorflow/pull/36904", "> There is a PR for the PROFILE_SUMMARIZER_SRCS fix.\r\n> #36904\r\n\r\nOh sorry, I didn't see that. Would you like me to remove that part from my commit?"]}, {"number": 35868, "title": "Optimization for Tnullptr", "body": "Added Missing Space.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F35868) for more info**.\n\n<!-- need_sender_cla -->", "We will not be encouraging one liner grammatical changes as this is expensive process, thank you "]}, {"number": 35867, "title": "Linking error when building Tensorflow 2.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentos 7 (Docker image)\r\n- TensorFlow installed from (source or binary):\r\nSource\r\n- TensorFlow version:\r\nr2.1\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\n0.29.1 (installed using bazelisk)\r\n- GCC/Compiler version (if compiling from source):\r\n7.3.1 (installed via devtoolset-7)\r\n\r\n**Describe the problem**\r\nWhen building Tensorflow 2.1, the following linking error appears:\r\n\r\n```\r\nERROR: /tmp/bazel/external/swig/BUILD.bazel:5:1: Linking of rule \r\n'@swig//:swig' failed (Exit 1)\r\nbazel-out/host/bin/external/swig/_objs/swig/allocate.o:allocate.cxx:\r\nfunction Allocate::~Allocate(): \r\nerror: undefined reference to 'operator delete(void*, unsigned long)'\r\n```\r\n\r\nSeems like an incompatibility issue when building some third party components caused when looking for that symbol.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`$ docker run centos:7`\r\n\r\nThen inside the container:\r\n```\r\n# yum-config-manager --enable rhel-server-rhscl-7-rpms -y && yum install devtoolset-7 -y\r\n# export PATH=/opt/rh/devtoolset-7/root/usr/bin${PATH:+:${PATH}}\r\n# export PCP_DIR=/opt/rh/devtoolset-7/root\r\n# export PERL5LIB=/opt/rh/devtoolset-7/root//usr/lib64/perl5/vendor_perl:/opt/rh/devtoolset-7/root/usr/lib/perl5:/opt/rh/devtoolset-7/root//usr/share/perl5/vendor_perl${PERL5LIB:+:${PERL5LIB}}\r\n# rpmlibdir=$(rpm --eval \"%{_libdir}\")\r\n# if [ \"$rpmlibdir\" != \"${rpmlibdir/lib64/}\" ]; then\r\n#       rpmlibdir32=\":/opt/rh/devtoolset-7/root${rpmlibdir/lib64/lib}\"\r\n# fi\r\n# export LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root$rpmlibdir$rpmlibdir32${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n# export LD_LIBRARY_PATH=/opt/rh/devtoolset-7/root$rpmlibdir$rpmlibdir32:/opt/rh/devtoolset-7/root$rpmlibdir/dyninst$rpmlibdir32/dyninst${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\r\n# pythonvers=2.7\r\n# export PYTHONPATH=/opt/rh/devtoolset-7/root/usr/lib64/python$pythonvers/site-packages:/opt/rh/devtoolset-7/root/usr/lib/python$pythonvers/site-packages${PYTHONPATH:+:${PYTHONPATH}}\r\n# export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.222.b10-1.el7_7.x86_64\r\n# git clone https://github.com/tensorflow/ && cd tensorflow && git checkout r2.1\r\n# export OPTM=3\r\n# export PYTHON_BIN_PATH=/usr/bin/python\r\n# export USE_DEFAULT_PYTHON_LIB_PATH=1\r\n# export CC_OPT_FLAGS=\"-march=${ARCH} -mtune=$TUNE\"\r\n# export TF_NEED_JEMALLOC=1\r\n# export TF_NEED_KAFKA=0\r\n# export TF_NEED_OPENCL_SYCL=0\r\n# export TF_NEED_GCP=0\r\n# export TF_NEED_HDFS=0\r\n# export TF_NEED_S3=0\r\n# export TF_ENABLE_XLA=1\r\n# export TF_NEED_GDR=0\r\n# export TF_NEED_VERBS=0\r\n# export TF_NEED_OPENCL=0\r\n# export TF_NEED_MPI=0\r\n# export TF_NEED_TENSORRT=0\r\n# export TF_SET_ANDROID_WORKSPACE=0\r\n# export TF_DOWNLOAD_CLANG=0\r\n# export TF_NEED_CUDA=0\r\n# ./configure && bazel build --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\nNote this error message appears when building different components. Here are some examples:\r\n\r\n```\r\nERROR: /tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:713:1: Linking of rule '//tensorflow/compiler/mlir/tensorflow:derived_attr_populator_gen' failed (Exit 1)\r\nbazel-out/host/bin/external/llvm/_objs/tablegen/Main.o:Main.cpp:function llvm::cl::list<std::string, bool, llvm::cl::parser<std::string> >::~list(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\n```\r\n\r\n```\r\nERROR: /tmp/bazel/external/com_google_protobuf/BUILD:406:1: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1)\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/code_generator.o:code_generator.cc:function google::protobuf::compiler::ParseGeneratorParameter(std::string const&, std::vector<std::pair<std::string, std::string>, std::allocator<std::pair<std::string, std::string> > >*): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'\r\nbazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function google::protobuf::io::StringOutputStream::~StringOutputStream(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\n```\r\n", "comments": ["Update: \r\nI was able to build TF 2.1 using Bazel 0.26.1 with no issues. I'm not sure what differences v0.27.1 introduces, but at least for my configuration, 0.26.1 works. I haven't found the root cause, though.", "I was able to replicate this on my local as well.. are others seeing this as well?", "I met a similar problem on CentOS 7 with Bazel 1.2.1 and GCC 8.3.1. Here is the error message:\r\n```\r\nERROR: /home/xxx/.cache/bazel/_bazel_xxx/26d8964cd405c0a736925fad9b93a973/external/swig/BUILD.bazel:5:1: Linking of rule '@swig//:swig' failed (Exit 1)\r\nbazel-out/host/bin/external/swig/_objs/swig/allocate.o:allocate.cxx:function Allocate::~Allocate(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/swig/_objs/swig/contract.o:contract.cxx:function Contracts::~Contracts(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/swig/_objs/swig/lang.o:lang.cxx:function Language::~Language(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\nbazel-out/host/bin/external/swig/_objs/swig/module.o:module.cxx:function Swig_register_module(char const*, Language* (*)()) [clone .cold.0]: error: undefined reference to 'operator delete(void*, unsigned long)'\r\n```", "I also had this problem when building r2.1 and v2.1.0 in the https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/tools/ci_build/Dockerfile.rbe.ubuntu16.04-manylinux2010 environment using both /dt8 and /dt7:\r\n\r\n`ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/flatbuffers/BUILD.bazel:99:1: Linking of rule '@flatbuffers//:flatc' failed (Exit 1)\r\nbazel-out/host/bin/external/flatbuffers/_objs/flatc/cpp_generator.o:cpp_generator.cc:function std::basic_stringbuf<char, std::char_traits<char>, std::allocator<char> >::~basic_stringbuf(): error: undefined reference to 'operator delete(void*, unsigned long)'\r\n...`\r\n\r\nI've had to hack the configure.py file to allow bazel==0.26.1 in order to build v2.1.0 whls with MKL with devtools-7 and 8.", "The solution from @wdirons works for me : `BAZEL_LINKLIBS=-l%:libstdc++.a bazel build -c opt //tensorflow/tools/pip_package:build_pip_package `", "Hi @feihugis, thanks for the workaround. Any clue why it doesn't get solved by adding \"--linkopt=-lstdc++\"? I tried that, but seems like failing. I will try your suggestion.", "> Hi @feihugis, thanks for the workaround. Any clue why it doesn't get solved by adding \"--linkopt=-lstdc++\"? I tried that, but seems like failing. I will try your suggestion.\r\n\r\nI am not very clear about this. @wdirons Do you have any ideas?", "Before compiling TensorFlow, I had to compile Bazel and it was failing with the same error:\r\n\r\nI found these related to the issue:\r\nhttps://github.com/bazelbuild/bazel/issues/8652\r\nhttps://github.com/bazelbuild/bazel/pull/8660\r\nhttps://github.com/bazelbuild/bazel/issues/10327\r\n\r\nThey say to use `BAZEL_LINKOPTS=-static-libstdc++ BAZEL_LINKLIBS=-l%:libstdc++.a`, but I found just using BAZEL_LINKLIBS is enough. \r\n", "> I tried that, but seems like failing. I will try your suggestion.\r\n\r\n@DnPlas,\r\nIs this still an issue?\r\n\r\nCould you please update TensorFlow to the latest stable version v2.4.1 and check if you are facing the same error. Thanks!", "I have not tried with 2.4, but seems like the problem is more related to bazel than it is to Tensorflow. Using @wdirons workaround worked for me, and some other folks that referenced this issue.", "@DnPlas,\r\nThank you for the update. Can we please close this issue as it is resolved?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35867\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35867\">No</a>\n", "I'm posting here in case someone else encounters similar issue through google with centos 7 and Bazel 3.7.2 followed by when loading datasets:\r\n```\r\n\"/home/external/tf_python_python3/lib64/python3.6/site-packages/numpy/lib/npyio.py\", line 255, in __getitem__\r\n    bytes = self.zip.open(key)\r\n  File \"/usr/lib64/python3.6/zipfile.py\", line 1448, in open\r\n    return ZipExtFile(zef_file, mode, zinfo, zd, True)\r\n  File \"/usr/lib64/python3.6/zipfile.py\", line 781, in __init__\r\n    self._decompressor = _get_decompressor(self._compress_type)\r\n  File \"/usr/lib64/python3.6/zipfile.py\", line 701, in _get_decompressor\r\n    return zlib.decompressobj(-15)\r\nValueError: Invalid initialization option\r\n```\r\n\r\nsetting following fixed both of them:\r\n```\r\nTF_SYSTEM_LIBS=zlib,png\r\nBAZEL_LINKLIBS=-lstdc++\r\n```\r\n"]}, {"number": 35866, "title": "TF 2.1 breaks GPU discovery", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 16.04:\r\n- TensorFlow installed with `pip install tensorflow_gpu`:\r\n- TensorFlow version: `2.1`\r\n- Python version: `3.6.8`\r\n- Installed using virtualenv? `pip`\r\n- CUDA/cuDNN version: `10.2`\r\n- GPU model and memory: `8x V100-SXM2 @ 16GB on GCP`\r\n\r\n**Describe the problem**\r\nI have been using `tf 2.0` since some time with no issues on a multi-gpu system (8 GPUs). The newest version of the package I am using ([OpenNMT-tf 2.4](https://github.com/OpenNMT/OpenNMT-tf)) required `tf 2.1` so I had to upgrade. After that I am getting errors related to loading certain CUDA libraries. \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nThe error is thrown when I run: `devices = tf.config.list_logical_devices(device_type=\"GPU\")`. Works fine in ` tf2.0`, fails in `tf 2.1`. When importing `tensorflow` in 2.1 I do get some warnings that are definitely connected.\r\n\r\n**Logs when using tf.2.0**\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n2.0.0\r\n>>> devices = tf.config.experimental.list_physical_devices(device_type=\"GPU\")\r\n>>> devices\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:4', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:5', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:6', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:7', device_type='GPU')]\r\n```\r\n\r\nso everything looks great. But here is the newer version:\r\n```\r\n>>> import tensorflow as tf\r\n2020-01-14 14:34:47.468372: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\r\n2020-01-14 14:34:47.468469: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\r\n2020-01-14 14:34:47.468485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n\r\n>>> # Let's ignore all these warnings even though they are relevant.\r\n>>> tf.__version__\r\n'2.0.1'\r\n>>> # Observe that my package changed the call from experimental to just config\r\n>>> devices = tf.config.list_logical_devices(device_type=\"GPU\")\r\n2020-01-14 14:35:43.622258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-01-14 14:37:12.937392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:12.938977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-01-14 14:37:12.939108: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:12.940655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 1 with properties: \r\npciBusID: 0000:00:05.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-01-14 14:37:12.940720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:12.942288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 2 with properties: \r\npciBusID: 0000:00:06.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-01-14 14:37:12.942358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:12.943868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 3 with properties: \r\npciBusID: 0000:00:07.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-01-14 14:37:12.943923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:12.945474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 4 with properties: \r\npciBusID: 0000:00:08.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-01-14 14:37:12.945535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:12.947104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 5 with properties: \r\npciBusID: 0000:00:09.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-01-14 14:37:12.947174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:12.948703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 6 with properties: \r\npciBusID: 0000:00:0a.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-01-14 14:37:12.948757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:12.950260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 7 with properties: \r\npciBusID: 0000:00:0b.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\r\ncoreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\r\n2020-01-14 14:37:12.953172: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-01-14 14:37:12.953284: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-01-14 14:37:12.953360: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2020-01-14 14:37:12.953427: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2020-01-14 14:37:12.953492: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2020-01-14 14:37:12.953556: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\r\n2020-01-14 14:37:12.957955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-01-14 14:37:12.958006: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1592] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-01-14 14:37:12.958522: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\r\n2020-01-14 14:37:12.967598: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\r\n2020-01-14 14:37:12.970122: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x439c380 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-01-14 14:37:12.970151: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-01-14 14:37:13.851388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:13.901617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:13.993165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:14.010794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:14.037557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:14.069363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:14.110305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:14.158077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-01-14 14:37:14.160296: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x44233a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-01-14 14:37:14.160327: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-01-14 14:37:14.160335: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-01-14 14:37:14.160340: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-01-14 14:37:14.160346: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-01-14 14:37:14.160352: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-01-14 14:37:14.160358: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-01-14 14:37:14.160372: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-01-14 14:37:14.160379: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): Tesla V100-SXM2-16GB, Compute Capability 7.0\r\n2020-01-14 14:37:14.163651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-01-14 14:37:14.163680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] \r\n\r\n>>> devices\r\n[]\r\n>>> # Even if we try the old call to experimental, we don't get the logs but still fails\r\n>>>  tf.config.experimental.list_physical_devices(device_type=\"GPU\")\r\n[]\r\n>>>\r\n```\r\n\r\nSeems like there is an issue with cuda libraries. Here is the `nvidia-smi` output in case its relevant:\r\n\r\n```\r\nTue Jan 14 14:51:03 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 430.26       Driver Version: 430.26       CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\r\n| N/A   37C    P0    46W / 300W |     11MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla V100-SXM2...  Off  | 00000000:00:05.0 Off |                    0 |\r\n| N/A   38C    P0    44W / 300W |     11MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla V100-SXM2...  Off  | 00000000:00:06.0 Off |                    0 |\r\n| N/A   37C    P0    43W / 300W |     11MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla V100-SXM2...  Off  | 00000000:00:07.0 Off |                    0 |\r\n| N/A   36C    P0    42W / 300W |     11MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   4  Tesla V100-SXM2...  Off  | 00000000:00:08.0 Off |                    0 |\r\n| N/A   35C    P0    41W / 300W |     11MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   5  Tesla V100-SXM2...  Off  | 00000000:00:09.0 Off |                    0 |\r\n| N/A   40C    P0    44W / 300W |     11MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   6  Tesla V100-SXM2...  Off  | 00000000:00:0A.0 Off |                    0 |\r\n| N/A   35C    P0    45W / 300W |     11MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   7  Tesla V100-SXM2...  Off  | 00000000:00:0B.0 Off |                    0 |\r\n| N/A   39C    P0    43W / 300W |     11MiB / 16160MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n", "comments": ["Hi,\r\n\r\nTensorFlow 2.1 requires CUDA 10.1 but it seems you have CUDA 10.2 installed. See the requirements here: https://www.tensorflow.org/install/gpu#software_requirements", "That was it, thanks. Closing this."]}, {"number": 35865, "title": "ImportError: DLL load failed: The specified procedure could not be found. appears after an hour of usage", "body": "- OS Platform: Windows 10 Enterprise (10.0.17134)\r\n- TensorFlow installed with: pip install tensorflow-cpu==2.1.0\r\n- CPU: Intel(R) Core(TM) i5-6500\r\n- GPU: Intel(R) HD Graphics 530 (i do not want to use GPU, this is just for completeness)\r\n- Python version: 3.6\r\n- Code used in PyCharm CommunityEdition 2019.3.1\r\n- Anaconda, version 4.7.12\r\n\r\nI had the Problem, that after upgrading to tensorflow 2.1.0 the `import tensorflow` statement would not work\r\nAfter enough search i found, that i needed the Visual C++ Redistributable for 2015, 2017 and 2019, which i now have installed. After that, i was able to run `import tensorflow`, but after about an hour later, the statement failed again. \r\nThe only change to my anaconda-environment was, that i installed opencv via `conda install opencv`, which should not change tensorflow\r\n\r\nThe Traceback:\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\*\\.conda\\envs\\sirenai\\lib\\site-packages\\tensorflow\\__init__.py\", line 101, in <module>\r\n            from tensorflow_core import *\r\n  File \"C:\\Users\\*\\.conda\\envs\\sirenai\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in \r\n            <module>\r\n            from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\*\\.conda\\envs\\sirenai\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n            module = self._load()\r\n  File \"C:\\Users\\*\\.conda\\envs\\sirenai\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n            module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\*\\.conda\\envs\\sirenai\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n            return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\*\\.conda\\envs\\sirenai\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 64, \r\n            in <module>\r\n            from tensorflow.core.framework.graph_pb2 import *\r\n  File \"C:\\Users\\*\\.conda\\envs\\sirenai\\lib\\site-packages\\tensorflow_core\\core\\framework\r\n            \\graph_pb2.py\", line 7, in <module>\r\n            from google.protobuf import descriptor as _descriptor\r\n  File \"C:\\Users\\*\\.conda\\envs\\sirenai\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 47, in \r\n            <module>\r\n            from google.protobuf.pyext import _message\r\nImportError: DLL load failed: The specified procedure could not be found.", "comments": ["@Hofmann-Max,\r\nOpen ...\\Lib\\site-packages\\tensorflow\\python\\_pywrap_tensorflow_internal.pyd use Dependency Walker, it will show you the \r\nDLL dependency tree, you will find which DLL cause the problem. You can then install the needed package version add some path to system variable.\r\nReference Issue : https://github.com/tensorflow/tensorflow/issues/28848\r\nDependency Walker link : http://www.dependencywalker.com/", "@gadagashwini \r\nThank you for your response\r\nThis file does not exist in this folder. It only contains the Folder __pycache__ and the File __init__.py\r\n__pycache__ itself only contains __init__.cpython-36.pyc", "@Hofmann-Max,\r\nTry to uninstall the existing Tensorflow and Python and install freshly by creating virtual environment.\r\nBelow are the instructions to install Tensorflow 2.1 using PIP\r\n#Install tensorflow using pip virtual env \r\n$pip install virtualenv\r\n$virtualenv tf_2.1.0   # tf_2.1.0 is virtual env name\r\n$source tf_2.1.0/bin/activate\r\ntf_2.1.0 $ pip install tensorflow==2.1.0\r\ntf_2.1.0 $ python\r\n>>import tensorflow as tf\r\n>>tf.__version__\r\n2.1.0\r\nLet us know if it helps. Thanks!", "I managed to work around the problem by upgrading Python from 3.6 to 3.7\r\nI am not sure at all why this error occurs or why upgrading fixes the problem\r\n\r\nNow with Python 3.7 it works fine", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35865\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35865\">No</a>\n"]}, {"number": 35864, "title": "Failed to invoke the interpreter with error: Provided data count 602112 must match the required count 270000.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): iOS 12.0.1\r\n- TensorFlow installed from (source or binary): CocoaPods\r\n- TensorFlow version (or github SHA if from source): \r\n  - TensorFlowLiteC (2.1.0)\r\n  - TensorFlowLiteSwift (2.1.0):\r\n    - TensorFlowLiteC (= 2.1.0)\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nimport os, cv2, re, random\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.preprocessing.image import img_to_array, load_img\r\nfrom keras import layers, models, optimizers\r\nfrom keras import backend as K\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow.python.keras import models, layers\r\n\r\nimg_width = 150\r\nimg_height = 150\r\nTRAIN_DIR = './input/train/'\r\nTEST_DIR = './input/test/'\r\ntrain_images_dogs_cats = [TRAIN_DIR+i for i in os.listdir(TRAIN_DIR)] # use this for full dataset\r\ntest_images_dogs_cats = [TEST_DIR+i for i in os.listdir(TEST_DIR)]\r\n\r\ndef atoi(text):\r\n\treturn int(text) if text.isdigit() else text\r\n\r\ndef natural_keys(text):\r\n\treturn [ atoi(c) for c in re.split('(\\d+)', text) ]\r\n\t\r\ntrain_images_dogs_cats.sort(key=natural_keys)\r\ntrain_images_dogs_cats = train_images_dogs_cats[0:1300] + train_images_dogs_cats[12500:12500 + 1300]\r\n\r\ntest_images_dogs_cats.sort(key=natural_keys)\r\n\r\ndef prepare_data(list_of_images):\r\n\t\"\"\"\r\n\tReturns two arrays: \r\n\t\tx is an array of resized images\r\n\t\ty is an array of labels\r\n\t\"\"\"\r\n\tx = [] # images as arrays\r\n\ty = [] # labels\r\n\t\r\n\tfor image in list_of_images:\r\n\t\tx.append(cv2.resize(cv2.imread(image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))\r\n\t\r\n\tfor i in list_of_images:\r\n\t\tif 'dog' in i:\r\n\t\t\ty.append(1)\r\n\t\telif 'cat' in i:\r\n\t\t\ty.append(0)\r\n\t\t#else:\r\n\t\t\t#print('neither cat nor dog name present in images')\r\n\t\t\t\r\n\treturn x, y\r\n\t\r\nX, Y = prepare_data(train_images_dogs_cats)\r\nprint(K.image_data_format())\r\n\r\n# First split the data in two sets, 80% for training, 20% for Val/Test)\r\nX_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=0.2, random_state=1)\r\n\r\nnb_train_samples = len(X_train)\r\nnb_validation_samples = len(X_val)\r\nbatch_size = 16\r\n\r\nmodel = models.Sequential()\r\n\r\nmodel.add(layers.Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3)))\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(layers.Conv2D(32, (3, 3)))\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(layers.Conv2D(64, (3, 3)))\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(64))\r\nmodel.add(layers.Activation('relu'))\r\nmodel.add(layers.Dropout(0.5))\r\nmodel.add(layers.Dense(1))\r\nmodel.add(layers.Activation('sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n\t\t\t\t\t\t\toptimizer='rmsprop',\r\n\t\t\t\t\t\t\tmetrics=['accuracy'])\r\n\r\n\r\n\r\nmodel.summary()\r\n\r\ntrain_datagen = ImageDataGenerator(\r\n\trescale=1. / 255,\r\n\tshear_range=0.2,\r\n\tzoom_range=0.2,\r\n\thorizontal_flip=True)\r\n\r\nval_datagen = ImageDataGenerator(\r\n\trescale=1. / 255,\r\n\tshear_range=0.2,\r\n\tzoom_range=0.2,\r\n\thorizontal_flip=True)\r\n\t\r\ntrain_generator = train_datagen.flow(np.array(X_train), Y_train, batch_size=batch_size)\r\nvalidation_generator = val_datagen.flow(np.array(X_val), Y_val, batch_size=batch_size)\r\n\r\nhistory = model.fit_generator(\r\n\ttrain_generator, \r\n\tsteps_per_epoch=nb_train_samples // batch_size,\r\n\tepochs=30,\r\n\tvalidation_data=validation_generator,\r\n\tvalidation_steps=nb_validation_samples // batch_size\r\n)\r\n\r\nmodel.save_weights('model_wieghts.h5')\r\nmodel.save('model_keras.h5')\r\n\r\nX_test, Y_test = prepare_data(test_images_dogs_cats) #Y_test in this case will be []\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\r\n\r\ntest_generator = val_datagen.flow(np.array(X_test), batch_size=batch_size)\r\nprediction_probabilities = model.predict_generator(test_generator, verbose=1)\r\n\r\ncounter = range(1, len(test_images_dogs_cats) + 1)\r\nsolution = pd.DataFrame({\"id\": counter, \"label\":list(prediction_probabilities)})\r\ncols = ['label']\r\n\r\nfor col in cols:\r\n\tsolution[col] = solution[col].map(lambda x: str(x).lstrip('[').rstrip(']')).astype(float)\r\n\r\nsolution.to_csv(\"dogsVScats.csv\", index = False)\r\n\r\n# Convert the model.\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\nopen(\"./model_from_keras.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n**The output from the converter invocation**\r\nLog from console:\r\n```\r\nFailed to invoke the interpreter with error: Provided data count 602112 must match the required count 270000.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttps://gofile.io/?c=nOBHAC\r\n```\r\n\r\n**Failure details**\r\nI took an example for iOS.\r\nThen I generated a model using this instruction (the modified script is above):\r\nhttps://www.kaggle.com/sarvajna/dogs-vs-cats-keras-solution\r\nThen I replaced the old .tflite file with the new one.\r\nResult - the error with data size. Where and how to fix it\r\n\r\nP.S. It seems I have the same/similar issue but for iOS:\r\nhttps://github.com/tensorflow/tensorflow/issues/21602", "comments": ["I'm using this same tutorial and save the model with\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_new)\r\ntflite_model = converter.convert()\r\nopen(\"category_model.tflite\", \"wb\").write(tflite_model)\r\n```\r\nand getting the same error\r\n```\r\nFailed to invoke the interpreter with error: Provided data count 602112 must match the required count 270000.\r\n```", "It seems the problem is with input buffer size. In `ModelDataHandler` I changed my \r\n```\r\n// MARK: - Model Parameters\r\n\r\n  let batchSize = 1\r\n  let inputChannels = 3\r\n  let inputWidth = 224\r\n  let inputHeight = 224\r\n```\r\nto\r\n```\r\n// MARK: - Model Parameters\r\n\r\n  let batchSize = 1\r\n  let inputChannels = 3\r\n  let inputWidth = 150 // this should match your training data\r\n  let inputHeight = 150 // this should match your training data\r\n```\r\nand now working perfectly.\r\n\r\n[https://github.com/tensorflow/tensorflow/issues/21602#issuecomment-414412800](url)", "Looks like you found and fixed the issue :-).\r\nPlease re-open if still applies.\r\n\r\nThanks!", "I'm trying to run\u00a0(SSD ResNet50 V1 FPN 640x640)\r\n[http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz](url) on iOS. I've successfully converted the model to tflite; it's 208.2 MB.\r\nI did edit ModelDataHandler.swift based on the model parameters:\r\n```\r\n// MARK: Model parameters\r\n  let batchSize = 1\r\n  let inputChannels = 3\r\n  let inputWidth = 640\r\n  let inputHeight = 640\r\n\r\n// image mean and std for floating model, should be consistent with parameters used in model training (I didn't change any here)\r\n  let imageMean: Float = 127.5\r\n  let imageStd:  Float = 127.5\r\n```\r\n\r\nOnce I try to run the model on iOS by following \r\n[https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/ios](url)\r\n\r\nIt keeps printing the bellow error message nonstop:\r\n\r\n**Failed to invoke the interpreter with error: Provided data count 1228800 must match the required count 3.**\r\n\r\nWould you please help?", "@AzinPoshtyar There seems to be a mis-match between the data you provide, and what the model expects. Its weird that the model expects only 3 bytes in the input; something seems to be off. As a sanity check, could you look at the `Test .tflite model` section [here](https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tflite.ipynb), and try to use it with your model in a Python script (on desktop)? You can also use some tool like Netron to check if the model is correct.", "My issue is a bit like this. Can anyone tell me what I'm doing wrong? \r\nhttps://github.com/tensorflow/tensorflow/issues/46199\r\n"]}, {"number": 35863, "title": "Validation and Evaluation Metric Issues in TensorFlow when Transfer Learning", "body": "I have come across some odd behaviours when training CNNs with Tensorflow 2.0 and would appreciate any help in solving them. I am doing transfer learning (just training the classification head) using the pre-trained networks available in 'tensorflow.keras.applications' and have noticed the following:\r\n\r\n 1. For the first epoch, validation metrics are always zero, no matter what I do.  \r\n 2. When training after the first epoch, the training metrics improve as you would expect, but the validation metrics essentially are random guesses, even when the EXACT same dataset is used as a training and a validation dataset. It is like it isn't using the model being trained to do its evaluation.\r\n\r\nI have tried, VGG16, MobileNetV2, and ResNet50V2, and they all exhibit the same behaviours. \r\n\r\nThe configurations I am able to reproduce this on are:\r\n - Ubuntu 18.04LTS, Nvidia RTX2080ti with driver version 430.50, CUDA10.0, TensorFlow-gpu==2.0.0 \r\n - MacBook Pro, TensorFlow==2.0.0 (cpu)\r\n\r\nBoth are running in Conda environments and I have installed TensorFlow with pip. I have put some sample code to show the essence of my workflow down below just in case I am doing anything obviously stupid. Any help would be very appreciated as I am at a loss as to how to fix it.\r\n\r\n`\r\ndef parse_function(example_proto):\r\n    image_feature_description = {\r\n        'label': tf.io.FixedLenFeature([], tf.int64),\r\n        'image_raw': tf.io.FixedLenFeature([], tf.string)\r\n    }\r\n    parsed_example = tf.io.parse_single_example(example_proto, image_feature_description)\r\n    image = tf.io.decode_image(\r\n                            parsed_example['image_raw'], \r\n                            channels = 3, \r\n                            dtype = tf.float32, \r\n                            expand_animations = False\r\n                            )\r\n    image = tf.image.per_image_standardization(image)\r\n    label = tf.one_hot(parsed_example['label'], 24, dtype=tf.float32) \r\n    return (image, label)\r\n\r\ndef load_dataset(TFRecord_dir, record_name):\r\n    record_files = tf.io.matching_files(os.path.join(TFRecord_dir, record_name + '.tfrecords-????'))\r\n    shards = tf.data.TFRecordDataset(record_files)\r\n    shards = shards.shuffle(tf.cast(tf.shape(record_files)[0], tf.int64))\r\n    dataset = shards.map(map_func=parse_function)\r\n    dataset = dataset.batch(batch_size=16, drop_remainder = True)\r\n    dataset = dataset.prefetch(16)\r\n    return dataset\r\n\r\n\r\nbase_model = tf.keras.applications.ResNet50V2(\r\n                                            input_shape=(224,224,3),\r\n                                            weights='imagenet',\r\n                                            include_top = False\r\n                                            )\r\nbase_model.trainable = False\r\n\r\nmodel = tf.keras.Sequential([\r\n        base_model,\r\n        tf.keras.layers.GlobalAveragePooling2D(),\r\n        tf.keras.layers.Dropout(0.5),\r\n        tf.keras.layers.Dense(24, activation = 'softmax')\r\n        ])\r\n\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.Adam(),\r\n    loss=tf.keras.losses.CategoricalCrossentropy(),\r\n    metrics=[ \r\n            tf.keras.metrics.CategoricalAccuracy(),\r\n            tf.keras.metrics.TopKCategoricalAccuracy(),\r\n            tf.keras.metrics.Precision(),\r\n            tf.keras.metrics.Recall()\r\n            ])\r\n\r\ntrain_dataset = load_dataset(train_dir, 'train')\r\n\r\nmodel.fit(train_dataset,\r\n                verbose = 1,\r\n                epochs= 5,\r\n                validation_data = train_dataset)\r\nmodel.evaluate(train_dataset)\r\n`\r\n\r\n\r\n\r\n\r\n", "comments": ["@Jon573 ,\r\nLooks like the given code is incomplete,can you please share complete code?Thanks!", "Hi, thanks for your reply. All I have left out is:\r\n`import tensorflow as tf` and `import os`, and `train_dir` is a folder on my local computer that contains TFRecords. I know the dataset is fine as I can use it with the Object Detection API and it works. I am using `train_dataset` as the training, validation and testing datasets as a debugging stage to rule out the model over fitting to the training dataset. ", "@Jon573 ,\r\nJust provide us the complete code with link of data-set used so that we can reproduce the issue.Thanks!", "@oanush \r\nI am unable to provide the dataset for privacy reasons, but I no longer have the problem since I moved to using the latest docker image provided over the weekend. There must have been something wrong with the install on both machines but I don't know what. \r\n\r\nI also found, during debugging, that normalising the images using `image = (image/127.5) -1 ` as done in the transfer learning with trained CNN tutorial (https://www.tensorflow.org/tutorials/images/transfer_learning) would exhibit the same behaviour, even in the docker container, i.e. would training metrics would improve, but the validation metrics would remain random on the same dataset used to train.\r\n"]}, {"number": 35862, "title": "Inherit CMSIS CCFLAGS when generating mbed project", "body": "In\r\ntensorflow/lite/experimental/micro/tools/make/ext_libs/cmsis.inc\r\nthe flags LOOP_UNROLL and ARM_DSP aren't inherited when creating an mbed project.\r\n\r\nExample: make -f tensorflow/lite/micro/tools/make/Makefile generate_hello_world_mbed_project TARGET_ARCH=cortex-m7 TAGS=cmsis-nn\r\n", "comments": ["@petewarden I think the LOOP_UNROLL won't be inherited without this change. @mansnils, will u please check? That flag will be removed soon though, so it won't matter.\r\n\r\nHowever, I'm thinking this change may be needed for other flags that are set. Now, or for the future. For example, if parsing tflite file to flag out float ops.\r\n\r\nLet me know what you think. ", "@freddan80 Correct, it won't be inherited without this change"]}, {"number": 35861, "title": "Errors with tf.data.Dataset shape ", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: no GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior** \"Error when checking input: expected input1 to have 4 dimensions, but got array with shape (96, 96, 1)\" \r\n\r\n**Describe the expected behavior** tf.data.dataset size its [[90, 96, 96, 1], [90, 6]], Input shape of CNN Model is (None,96,96,1) - so this should not give error\r\n\r\n**Code to reproduce the issue**\r\nhttps://www.kaggle.com/anirbank/bengali-graphemes-starter-eda-basic-cnn-model\r\n\r\n**Other info / logs**\r\nLogs given in code: Input shape:\r\n(None, 96, 96, 1)\r\ntf.data.Dataset shape : [[90, 96, 96, 1], [90, 6]]\r\n\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n Thanks!\r\n", "That is not correct, this has been raised as a bug earlier. It is not\nsomething that I dnt know how to use. I have printed the tensor shapes,\neverything is fine, but still I get the error msg.\n\nOn Wed, Jan 15, 2020 at 11:17 AM Yasir Modak <notifications@github.com>\nwrote:\n\n> This question is better asked on StackOverflow\n> <http://stackoverflow.com/questions/tagged/tensorflow> since it is not a\n> bug or feature request. There is also a larger community that reads\n> questions there.\n> Thanks!\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/35861?email_source=notifications&email_token=ADIW7N2LU2M47MQ7FNDSW3LQ52PPFA5CNFSM4KGRHV2KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEI7ELQI#issuecomment-574506433>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ADIW7N4NOAK3PLOVN3BGOF3Q52PPFANCNFSM4KGRHV2A>\n> .\n>\n"]}, {"number": 35860, "title": "TensorBoard callback without profile_batch setting cause Errors: CUPTI_ERROR_INSUFFICIENT_PRIVILEGES and CUPTI_ERROR_INVALID_PARAMETER", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Stateless LSTM from Keras tutorial using tf backend\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: MX150 10GB\r\n\r\n**Describe the current behavior**\r\nWhen using tf.keras.callbacks.TensorBoard() without the profile_batch setting, it gives out errors of CUPTI_ERROR_INSUFFICIENT_PRIVILEGES and CUPTI_ERROR_INVALID_PARAMETER from tensorflow/core/profiler/internal/gpu/cupti_tracer.cc.\r\n\r\n**Describe the expected behavior**\r\nWith profile_batch = 0, these two errors are gone. \r\nBut comes back when profile_batch = 1, or other non-zero values.\r\n\r\n**Code to reproduce the issue**\r\n```\r\n\r\nfrom __future__ import print_function\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, LSTM\r\n\r\n\r\ninput_len = 1000\r\ntsteps = 2\r\nlahead = 1\r\nbatch_size = 1\r\nepochs = 5\r\n\r\nprint(\"*\" * 33)\r\nif lahead >= tsteps:\r\n    print(\"STATELESS LSTM WILL ALSO CONVERGE\")\r\nelse:\r\n    print(\"STATELESS LSTM WILL NOT CONVERGE\")\r\nprint(\"*\" * 33)\r\n\r\nnp.random.seed(1986)\r\n\r\nprint('Generating Data...')\r\n\r\n\r\ndef gen_uniform_amp(amp=1, xn=10000):\r\n\r\n    data_input = np.random.uniform(-1 * amp, +1 * amp, xn)\r\n    data_input = pd.DataFrame(data_input)\r\n    return data_input\r\n\r\n\r\nto_drop = max(tsteps - 1, lahead - 1)\r\ndata_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop)\r\n\r\nexpected_output = data_input.rolling(window=tsteps, center=False).mean()\r\n\r\nif lahead > 1:\r\n    data_input = np.repeat(data_input.values, repeats=lahead, axis=1)\r\n    data_input = pd.DataFrame(data_input)\r\n    for i, c in enumerate(data_input.columns):\r\n        data_input[c] = data_input[c].shift(i)\r\n\r\nexpected_output = expected_output[to_drop:]\r\ndata_input = data_input[to_drop:]\r\n\r\n\r\ndef create_model(stateful):\r\n    model = Sequential()\r\n    model.add(LSTM(20,\r\n              input_shape=(lahead, 1),\r\n              batch_size=batch_size,\r\n              stateful=stateful))\r\n    model.add(Dense(1))\r\n    model.compile(loss='mse', optimizer='adam')\r\n    return model\r\n\r\nprint('Creating Stateful Model...')\r\nmodel_stateful = create_model(stateful=True)\r\n\r\n\r\ndef split_data(x, y, ratio=0.8):\r\n    to_train = int(input_len * ratio)\r\n    to_train -= to_train % batch_size\r\n    x_train = x[:to_train]\r\n    y_train = y[:to_train]\r\n    x_test = x[to_train:]\r\n    y_test = y[to_train:]\r\n\r\n    # tweak to match with batch_size\r\n    to_drop = x.shape[0] % batch_size\r\n    if to_drop > 0:\r\n        x_test = x_test[:-1 * to_drop]\r\n        y_test = y_test[:-1 * to_drop]\r\n\r\n    # some reshaping\r\n    reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1))\r\n    x_train = reshape_3(x_train)\r\n    x_test = reshape_3(x_test)\r\n\r\n    reshape_2 = lambda x: x.values.reshape((x.shape[0], 1))\r\n    y_train = reshape_2(y_train)\r\n    y_test = reshape_2(y_test)\r\n\r\n    return (x_train, y_train), (x_test, y_test)\r\n\r\n\r\n(x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output)\r\nprint('x_train.shape: ', x_train.shape)\r\nprint('y_train.shape: ', y_train.shape)\r\nprint('x_test.shape: ', x_test.shape)\r\nprint('y_test.shape: ', y_test.shape)\r\n\r\nprint('Creating Stateless Model...')\r\nmodel_stateless = create_model(stateful=False)\r\n\r\nimport os\r\nimport datetime\r\nROOT_DIR = os.getcwd()\r\nlog_dir = os.path.join('callback_tests')\r\nif not os.path.exists(log_dir):\r\n    os.makedirs(log_dir)\r\nprint(log_dir)\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n                                       \r\nprint('Training')\r\nhistory = model_stateless.fit(x_train,\r\n                    y_train,\r\n                    batch_size=batch_size,\r\n                    epochs=epochs,\r\n                    verbose=1,\r\n                    validation_data=(x_test, y_test),\r\n                    shuffle=False,\r\n                    callbacks=[tensorboard_callback]\r\n                    )\r\n\r\n\r\n```\r\n\r\n**Other info / logs**\r\nTrain on 800 samples, validate on 200 samples\r\n2020-01-14 21:30:27.591905: I tensorflow/core/profiler/lib/profiler_session.cc:225] Profiler session started.\r\n2020-01-14 21:30:27.594743: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1259] Profiler found 1 GPUs\r\n2020-01-14 21:30:27.599172: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cupti64_101.dll\r\n2020-01-14 21:30:27.704083: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-01-14 21:30:27.716790: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\nEpoch 1/5\r\n2020-01-14 21:30:28.370429: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020-01-14 21:30:28.651767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020-01-14 21:30:29.662864: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\r\n2020-01-14 21:30:29.670282: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.\r\n800/800 [==============================] - 5s 6ms/sample - loss: 0.0011 - val_loss: 0.0011\r\nEpoch 2/5\r\n800/800 [==============================] - 3s 4ms/sample - loss: 8.5921e-04 - val_loss: 0.0010\r\nEpoch 3/5\r\n800/800 [==============================] - 3s 3ms/sample - loss: 8.5613e-04 - val_loss: 0.0010\r\nEpoch 4/5\r\n800/800 [==============================] - 3s 4ms/sample - loss: 8.5458e-04 - val_loss: 9.9713e-04\r\nEpoch 5/5\r\n800/800 [==============================] - 3s 4ms/sample - loss: 8.5345e-04 - val_loss: 9.8825e-04\r\n", "comments": ["Facing the same problem.", "Same issue. Different code sample.\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 2.1.0\r\nPython version: 3.7.6\r\nCUDA/cuDNN version: 10.1\r\nGPU model and memory: RX 2080 Ti", "Same (INSUFFICIENT PRIVILEGES):\r\n\r\n```\r\n2020-01-15 20:28:38.181667: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-01-15 20:28:38.183369: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n```\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\nTensorFlow installed from (source or binary): binary\r\n\r\nCUDA/cuDNN version: 10.1\r\n\r\nHost: ttmagpie_d99d3f105d0a\r\nPython: 3.6.9 (default, Nov  7 2019, 10:44:02) \r\n[GCC 8.3.0]\r\nTensorflow: 2.1.0\r\nGPU: available\r\nGPU 0: GeForce GTX 960M (UUID: GPU-c604cc5b-50da-5483-bde7-f562fb1c3420)\r\nGPU Memory:4GB\r\nKeras: 2.2.4-tf\r\nHub: 0.7.0\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 960M    Off  | 00000000:02:00.0 Off |                  N/A |\r\n| N/A   46C    P5    N/A /  N/A |   3869MiB /  4046MiB |     26%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n```\r\n", "@gawain-git-code ,\r\nI tried running the code in colab and I was able to run it successfully. please find the [gist](https://colab.sandbox.google.com/gist/oanush/65e1bdec0247583699dc2eab13c69864/35860.ipynb) for reference.Thanks!", "@airMeng @kevin-hartman @eduardofv Are you guys satisfy with the answer from @oanush ?\r\n\r\nI personally would not because that was just telling me I can only use profile_batch settings safely in google colab instead of on my own machine setup. \r\n\r\nThis was not answering the root cause of the problem of why cupti_tracer is signalling the errors. But thank you @oanush for spending the time to help out.\r\n", "I am still receiving the error but have moved to another environment. It may have to do with driver updates on Ubuntu. Will try to check again and get back to you.\r\n", "The error states that\r\n\r\n> User doesn't have sufficient privileges which are required to start the profiling session. One possible reason for this may be that the NVIDIA driver or your system administrator may have restricted access to the NVIDIA GPU performance counters. \r\n\r\nMay be its an error with the configuration itself.\r\n\r\n\r\n", "Facing same issue when capturing profile data by tensorboard through gRPC: Tried following [solution](https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti) by nvidia (enable non privileged access to profile counters) - to no avail. my training runs as root inside a container (which was other solution suggested by nvidia).\r\nTF2.1.0, NVIDIA Driver Version: 418.56 CUDA Version: 10.1", "I have the same issue when I use the official TensorFlow docker image(tensorflow/tensorflow:2.1.0-gpu-py3).\r\nI used 'tf.keras.callbacks.TensorBoard' with model.fit() method, but during the first epoch following errors came out.\r\n\r\nE tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n\r\nE tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\r\n\r\nSo I go back to the 'tensorflow/tensorflow:2.0.0-gpu-py3' image.", "@dartlune Did going back to the 'tensorflow/tensorflow:2.0.0-gpu-py3' image helped?\r\n\r\nI cannot save the model in both versions of the docker image! The weird thing is that when running the model in jupyter notebook, it saves the model each iteration! But not in python3!\r\n\r\nAny suggestions?", "@tamaramiteva Actually I had some error with tensorflow/tensorflow:2.0.0-gpu-py3'.\r\nIt was a little different error with the above things.\r\nSo I did a little search and found a solution that adds some paths.\r\n\r\nLD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nLD_INCLUDE_PATH=:/usr/local/cuda/include:/usr/local/cuda/extras/CUPTI/include", "Any update about this problem?", "This is due to NVIDIA CUPTI libary API change:  https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\r\nIn order for GPU profiling to work you need to run your job as sudo.\r\nOn Kubernetes you'll need a allowPrivilegeEscalation: true, you can see example here:\r\nhttps://github.com/vlasenkoalexey/criteo/blob/05e2aa4c5a15b9e437a364295b7f1e5e2653a22b/scripts/template.yaml.jinja#L136\r\nIt is not convenient and won't work for all use-cases, I hope there is a better solution.\r\n\r\nAlso note that tensorboard profiler plugin got broken by Chrome 80 update, see https://github.com/tensorflow/tensorboard/issues/3209\r\n\r\nSuggested workaround works - run Chrome with --enable-blink-features=ShadowDOMV0,CustomElementsV0,HTMLImports flags \r\n\r\nlike:\r\n/usr/bin/google-chrome-stable --enable-blink-features=ShadowDOMV0,CustomElementsV0,HTMLImports\r\n", "Adding `options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` to /etc/modprobe.d/nvidia-kernel-common.conf\r\nand reboot should resolve the permision issue.", "@tamaramiteva \r\nI resolved the problem with docker run option '--privileged=true'.\r\nThere is no more errors such as 'CUPTI_ERROR_INSUFFICIENT_PRIVILEGES'.", "> @tamaramiteva\r\n> I resolved the problem with docker run option '--privileged=true'.\r\n> There is no more errors such as 'CUPTI_ERROR_INSUFFICIENT_PRIVILEGES'.\r\n\r\nGOOD!!! it worksl. Thank you", "> Adding `options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` to /etc/modprobe.d/nvidia-kernel-common.conf\r\n> and reboot should resolve the permision issue.\r\n\r\nI had the same issue and this works perfectly for me. No more errors about libcupti.", "Anyone with the same issue on Windows 10? The two offered solutions only work for Linux.", "> Anyone with the same issue on Windows 10? The two offered solutions only work for Linux.\r\n\r\nThis solved the issue for me:\r\nRight-click on your desktop desktop for quick access to the NVIDIA Control Panel\r\nWindows Step 1: Open the NVIDIA Control Panel, select 'Desktop', and ensure 'Enable Developer Settings' is checked.\r\nWindows Step 2: Under 'Developer' > 'Manage GPU Performance Counters', select 'Allow access to the GPU performance counter to all users' to enable unrestricted profiling[1]", "The current proposed solution didn't work for me.\r\nMy error still reads:\r\n```\r\n2020-03-31 17:35:59.512872: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-03-31 17:35:59.514268: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n2020-03-31 17:36:00.187507: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\r\n2020-03-31 17:36:00.187591: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.\r\n```\r\n\r\nI also tried simply running `modprobe nvidia NVreg_RestrictProfilingToAdminUsers=0`, but it didn't work either.\r\nStarting the application in `sudo` mode did solve the problem, but of course it's not ideal...", "> @tamaramiteva\r\n> I resolved the problem with docker run option '--privileged=true'.\r\n> There is no more errors such as 'CUPTI_ERROR_INSUFFICIENT_PRIVILEGES'.\r\n\r\nThis worked for me and now the profile page of tensorboard is finally showing some output! Thanks @dartlune ", "> > Anyone with the same issue on Windows 10? The two offered solutions only work for Linux.\r\n> \r\n> This solved the issue for me:\r\n> Right-click on your desktop desktop for quick access to the NVIDIA Control Panel\r\n> Windows Step 1: Open the NVIDIA Control Panel, select 'Desktop', and ensure 'Enable Developer Settings' is checked.\r\n> Windows Step 2: Under 'Developer' > 'Manage GPU Performance Counters', select 'Allow access to the GPU performance counter to all users' to enable unrestricted profiling[1]\r\n\r\nThis seems to be a global setting, so it works with monkey-patched Anaconda environments.", "> The current proposed solution didn't work for me.\r\n> My error still reads:\r\n> \r\n> ```\r\n> 2020-03-31 17:35:59.512872: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1307] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n> 2020-03-31 17:35:59.514268: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1346] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\r\n> 2020-03-31 17:36:00.187507: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1329] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER\r\n> 2020-03-31 17:36:00.187591: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:88]  GpuTracer has collected 0 callback api events and 0 activity events.\r\n> ```\r\n> \r\n> I also tried simply running `modprobe nvidia NVreg_RestrictProfilingToAdminUsers=0`, but it didn't work either.\r\n> Starting the application in `sudo` mode did solve the problem, but of course it's not ideal...\r\n\r\nI got the same problem as yours, have you found an ideal way?", "@Student-HXJ unfortunately no, I was just running my script or jupyter notebook with sudo.", "> > Anyone with the same issue on Windows 10? The two offered solutions only work for Linux.\r\n> \r\n> This solved the issue for me:\r\n> Right-click on your desktop desktop for quick access to the NVIDIA Control Panel\r\n> Windows Step 1: Open the NVIDIA Control Panel, select 'Desktop', and ensure 'Enable Developer Settings' is checked.\r\n> Windows Step 2: Under 'Developer' > 'Manage GPU Performance Counters', select 'Allow access to the GPU performance counter to all users' to enable unrestricted profiling[1]\r\n\r\nAlso just for a reminder, dont forget to press 'Apply' button on the right bottom corner.", "> Adding `options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` to /etc/modprobe.d/nvidia-kernel-common.conf\r\n> and reboot should resolve the permision issue.\r\n\r\nIn my case, this solution doesn't work for anaconda, but works for normal python. Thanks a lot!", "For people using Docker on Linux:\r\nInstead of running the container with `--privileged=true` just pass `--cap-add=CAP_SYS_ADMIN`.", "I'm facing same error.\r\nI use ubuntu18.04 , CUDA10.1 and tensorflow2.2.\r\nPlease tell me if you can solve it in the same environment.\r\n", "If you have this problem on windows, Open cmd in \"Administrator Mode\"", "@uchar thanks for your reply.\r\nI had error like [this](https://github.com/tensorflow/tensorflow/issues/35860#issue-549483874)\r\nBut refer to this, I added `\"NVreg_RestrictProfilingToAdminUsers=0\"` to `/etc/modprobe.d/nvidia-kernel-common.conf` and `export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.1/extras/CUPTI/lib64` to `~/.bashrc`. Then I could  solve it!\u3000\r\n> Adding `options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` to /etc/modprobe.d/nvidia-kernel-common.conf\r\n> and reboot should resolve the permision issue.\r\n\r\n", "Can confirm that the following fixed the problem for me. It's possible that only a subset is strictly necessary.\r\n\r\n(note: Docker config, using official 2.2.0 image)\r\n* updating host machine to Ubuntu 20.04\r\n* adding `options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` to `/etc/modprobe.d/nvidia-kernel-common.conf` and running `update-initramfs -u`\r\n* adding `export CUDA_VERSION=\"10.1\"`, `export LD_LIBRARY_PATH=\"/usr/local/cuda-${CUDA_VERSION}/lib64:/usr/local/cuda-${CUDA_VERSION}/extras/CUPTI/lib64` and `export LD_INCLUDE_PATH=\"/usr/local/cuda-${CUDA_VERSION}/include:/usr/local/cuda-${CUDA_VERSION}/extras/CUPTI/include\"` to the host machine's `.zshrc`\r\n* adding `ENV LD_INCLUDE_PATH=\"/usr/local/cuda/include:/usr/local/cuda/extras/CUPTI/include:$LD_INCLUDE_PATH` to the Dockerfile\r\n* running the Docker container with `--privileged`", "> For people using Docker on Linux:\r\n> Instead of running the container with `--privileged=true` just pass `--cap-add=CAP_SYS_ADMIN`.\r\n\r\nThis is the solution for multiple-GPU training. Thanks!", "> LD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\n> LD_INCLUDE_PATH=:/usr/local/cuda/include:/usr/local/cuda/extras/CUPTI/include\r\n\r\nThanks , It worked for me.", "I have had the test on the above-mentioned solutions. It seems that there is no quick way to go out of the dilemma of the error \"CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\" \r\n\r\n**1. The ad-hoc solution** \r\n\r\nEven though Nvidia gives the temporal solution \"CAP_SYS_ADMIN\", it is a ad hoc solution. It sometimes works and does not work in the rest of the time. \r\n\r\n$ python abc.py --cap-add=CAP_SYS_ADMIN\r\n\r\n**2. LD_LIBRARY_PATH is not reliable**\r\n\r\nThe following solutions sometimes work ans does not work in the rest of the time. \r\n\r\nLD_LIBRARY_PATH=:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\r\nLD_INCLUDE_PATH=:/usr/local/cuda/include:/usr/local/cuda/extras/CUPTI/include\r\n\r\nor \r\n\r\nLD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.1/extras/CUPTI/lib64\r\n\r\n**3. No path of \"/etc/modprobe.d/nvidia-kernel-common.conf\"**\r\n\r\nThe modprobe.d does not include the path iof \"/etc/modprobe.d/nvidia-kernel-common.conf\". So I could not add \"NVreg_RestrictProfilingToAdminUsers=0\" to /etc/modprobe.d/nvidia-kernel-common.conf \r\n\r\nNvidia gives quite value explanation on the error. So it is quite strange. \r\n\r\n", "> Adding `options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` to /etc/modprobe.d/nvidia-kernel-common.conf\r\n> and reboot should resolve the permision issue.\r\n\r\nThanks , It worked for me.", "None of the solutions offered here nor  anywhere else has worked for me. Perhaps it may work if one upgrades from Ubuntu 16.04 to Ubuntu 18.04. But since I'm on a shared server, it may take some time to do the upgrade. I have not tried docker yet.\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 2.3.0 (installed with pip install -U tensorflow)\r\nPython version: 3.8.5\r\nCUDA/cuDNN version: 10.1\r\nGPU model and memory: GTX 1080 Ti  11 GB\r\n", "I am having the same error in anaconda environment. None of the solutions posted above work for me. Does anyone have any ideas what can be done?\r\nAlso what does this error actually mean, if someone is kind enough to explain it to a noob ?", "Tensorflow use NVIDIA provided libcupti for GPU tracing support. However since CUDA 10, that functionality requires CAP_SYS_ADMIN privilege, or you should change the /etc/modprobe.d/nvidia-kernel-common.conf (which also require sudo, but only once).\r\nMore information can be found at (which include how to do it on windows)\r\nhttps://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\r\n\r\nI believe that NVIDIA enforce this restriction because of some research papers said you can steal user secrets by probing performance counters. ", "Hey @trisolaran thanks for brief intro. The thing is i do not have /etc/modprobe.d/nvidia-kernel-common.conf such file. I am using a conda environment.", "@SarfarazHabib Hi I am using a conda enviroment too and I solved this problem by adding  \r\n`options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` to `/etc/modprobe.d/nvidia-kernel-common.conf`. I had not had the file too , so you should make the file.", "@kunihik0 Thanks alot for the help. The error is now gone but my training stucks after random epochs. I am using tensorflow 2.3 for now on ubuntu 18.04. Can anyone guide me in any direction with respect to this new problem ??", "> > Anyone with the same issue on Windows 10? The two offered solutions only work for Linux.\r\n> \r\n> This solved the issue for me:\r\n> Right-click on your desktop desktop for quick access to the NVIDIA Control Panel\r\n> Windows Step 1: Open the NVIDIA Control Panel, select 'Desktop', and ensure 'Enable Developer Settings' is checked.\r\n> Windows Step 2: Under 'Developer' > 'Manage GPU Performance Counters', select 'Allow access to the GPU performance counter to all users' to enable unrestricted profiling[1]\r\n\r\nThis solution works great for Windows 10 systems.  But what about Windows Server 2019?  It seems that now Microsoft requires you to get NVIDIA Control Panel from the Microsoft Store, but that is not available on Windows Server 2019.  Is there an alternative way to allow these permissions on Windows Server 2019?", "> Adding `options nvidia \"NVreg_RestrictProfilingToAdminUsers=0\"` to /etc/modprobe.d/nvidia-kernel-common.conf\r\n> and reboot should resolve the permision issue.\r\n\r\nThis works for me after switching from conda to virtualenv and I also need to use `sudo` inside my virtualenv with the python under `/venv/bin/python`. Other dependencies are:\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): 2.3.1\r\nPython version: 3.7.6\r\nCUDA/cuDNN version: 11.0\r\nGPU model: GeForce GTX TITAN X\r\n\r\nNow I can profile with --profile_steps=1000, 1005, for example, 5 steps, but if I increase it to 10, there is this non-deterministic segfault appearing. Not sure whether this happened to anyone else?", "> Now I can profile with --profile_steps=1000, 1005, for example, 5 steps, but if I increase it to 10, there is this non-deterministic segfault appearing. Not sure whether this happened to anyone else?\r\n\r\nYes, I get that segfault too \u2013 I think it's because the overhead of profiling, on top of regular GPU computations, causes GPU memory overflow.\r\n", "In order to run docker:\r\n`nvidia-docker run '--privileged=true' -d -it --name retina_net -v /home/readib/Experiments/:/home -p 8000:8888 -v /tmp/.X11-unix/:/tmp/.X11-unix -e DISPLAY=$DISPLAY retina_net:latest /bin/bash`", "@vlasenkoalexey Do  you mean the  version  of   NVIDIA CUPTI libary that  changes  API result in the error?  Will the old version that API  doesn't change  run normally? ", "CUPTI library is part of CUDA, before CUDA 10.x profiling didn't require admin privileges. See nvidia doc for details: https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti", "@vlasenkoalexey But  CUDA 10.x  is also troubled with the privileges problem in my local , so do many people under this issue. My local configuration:  ubuntu 18.04   python3.7   cuda10.1/ cuda10.2 (two machines)", "Was able to reproduce the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/10817d7a9423b0ac02cc77ef7ac9097d/untitled123.ipynb)..Thanks !", "I also met this problem.My OS is centos7, adding the conf file under /etc/modprobe.d/ and then rebuilding the inital RAM disk by `sudo dracut  --force` and reboot solve the problem.", "Hi @gawain-git-code  , Could you look at this[ thread ](https://forums.developer.nvidia.com/t/cupti-has-no-sufficient-privilege/144565/2)for answer ? ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35860\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35860\">No</a>\n", "how about cpu train?", "I'm running a tensorflow application in a Docker Container on a Windows Machine with WSL2. I get the following errors:\r\n\r\n` tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\r\nI tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs\r\nI tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1\r\nE tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1408] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_NOT_INITIALIZED\r\nE tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1447] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI_ERROR_NOT_INITIALIZED\r\nE tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1430] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI_ERROR_INVALID_PARAMETER`\r\n\r\nI changed the  /etc/modprobe.d/nvidia-kernel-common.conf file as suggested and run the docker container as root-user. \r\nDoes anyone have an idea what to try next?"]}, {"number": 35859, "title": "Fix typo in code example in Attention()", "body": "cut and paste error means `value_embeddings` was being calculated with `query_input` instead of `value_input`", "comments": ["Please open against master branch, not against a release branch. Once final release is done, we only update the branches in case of a patch release and patch releases only contain security updates mostly."]}, {"number": 35858, "title": "Memory leak in LSTM with dropout when executed in eager mode", "body": "**System information**\r\n- Have I written custom code: Simple LSTM layer with dropouts in eager loop\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: v2.0.0 and v2.1.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\nAn LSTM layer with dropout enabled leaks memory in each eager execution. This is likely due to the dropout mask in the RNN layer not being properly released (related to #19671). \r\n\r\nIf the dropout arguments are set to 0 then the memory remains constant.\r\n\r\n**Describe the expected behavior**\r\nThe memory usage should remain constant among batches with dropouts enabled.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\nlayer = tf.keras.layers.LSTM(64, return_sequences=True, \r\n                             dropout=0.4, \r\n                             recurrent_dropout=0.2)\r\n\r\nwhile True:\r\n    X = tf.random.uniform((500, 100, 64))\r\n    pred = layer(X, training=True)\r\n```\r\n\r\n**Other info / logs**\r\nMemory usage when dropouts are enabled taken with memory_profiler\r\n![memory_leak](https://user-images.githubusercontent.com/5730052/72333667-70a78600-36bc-11ea-9ac7-7bc71ec2468d.png)\r\n\r\nMemory usage when dropouts are disabled\r\n![no_memory_leak](https://user-images.githubusercontent.com/5730052/72333733-95036280-36bc-11ea-982c-4621ec5776da.png)\r\n\r\n**Workaround**\r\nA working workaround is to set `tf.random.set_seed(seed)` in each iteration, which seems to clear the cached masks, but it can seriously affect the experiments.", "comments": ["@vsimkus,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue and to visualize the memory usage. Thanks!", "Just run the script above with [`memory_profiler`](https://pypi.org/project/memory-profiler/#time-based-memory-usage). \r\n```bash\r\nmprof run python memory_leak_example.py # Run the script with memory_profiler\r\nmprof plot # Visualise memory usage\r\n```", "With tf-nightly, the memory footprint becomes stable after 5 epochs and doesn't increase afterwards.\r\n\r\nCould you confirm this is still the case for you for nightly version?", "I think the issue is fixed recently the adding the kernel cache size limit. Closing this bug for now.", "Thanks. With `tf-nightly` the memory usage indeed does not blow up, although it is quite irregular and does not seem to stabilise.\r\n![memory_leak2](https://user-images.githubusercontent.com/5730052/72621388-7530a080-3941-11ea-9b75-dceb446d862d.png)\r\nIt would be good to understand the behaviour.", "Again, confirm. This is a tricky bug... whenever **implicit dropout of lstm** is activated. Using a separate tf.keras.layers.Dropout is fine.\r\nBtw, **in eager mode** is tricky too, there are many memory leakages since tf2.0. e.g. tf.keras.layers.TimeDistributed, Model.fit_generator... Probably due to the introduction of autograph and implicit eager execution in tf2.0."]}, {"number": 35857, "title": "How to do Early stopping with monitoring weighted metric of multi-output model?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3\r\n\r\n**Describe the current behavior**\r\nIs there any way to do early stopping with monitoring weighted metric of multi-output model?\r\nI want to have a \"global\" metric like weighted accuracy to control my training process.\r\nBut I cant find any example or solution on the internet.  \r\n\r\n**Code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1mcqYo0OFs91uxo3mN82UpLCqCymSKTAI#scrollTo=vqTq1JH5U7Fj\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\nimage_input = keras.Input(shape=(32, 32, 3), name='img_input')\r\ntimeseries_input = keras.Input(shape=(None, 10), name='ts_input')\r\n\r\nx1 = layers.Conv2D(3, 3)(image_input)\r\nx1 = layers.GlobalMaxPooling2D()(x1)\r\n\r\nx2 = layers.Conv1D(3, 3)(timeseries_input)\r\nx2 = layers.GlobalMaxPooling1D()(x2)\r\n\r\nx = layers.concatenate([x1, x2])\r\n\r\nscore_output = layers.Dense(1, name='score_output')(x)\r\nclass_output = layers.Dense(5, activation='softmax', name='class_output')(x)\r\n\r\nmodel = keras.Model(inputs=[image_input, timeseries_input],\r\n                    outputs=[score_output, class_output])\r\nmodel.compile(\r\n    optimizer=keras.optimizers.RMSprop(1e-3),\r\n    loss={'score_output': keras.losses.MeanSquaredError(),\r\n          'class_output': keras.losses.CategoricalCrossentropy()},\r\n    metrics={'score_output': [keras.metrics.CategoricalAccuracy()],\r\n             'class_output': [keras.metrics.CategoricalAccuracy()]},\r\n    loss_weights={'score_output': 2., 'class_output': 1.})\r\ncallbacks = [\r\n    # keras.callbacks.EarlyStopping(\"score_output_categorical_accuracy\", patience=2, restore_best_weights=True, mode=\"auto\")\r\n    keras.callbacks.EarlyStopping(\"weighted_categorical_accuracy\", patience=2, restore_best_weights=True, mode=\"auto\")\r\n]\r\nimport numpy as np\r\n# Generate dummy Numpy data\r\nimg_data = np.random.random_sample(size=(100, 32, 32, 3))\r\nts_data = np.random.random_sample(size=(100, 20, 10))\r\nscore_targets = np.random.random_sample(size=(100, 1))\r\nclass_targets = np.random.random_sample(size=(100, 5))\r\n\r\n# Fit on lists\r\nmodel.fit([img_data, ts_data], [score_targets, class_targets],\r\n          callbacks=callbacks,\r\n          batch_size=32,\r\n          epochs=10)\r\n```\r\n\r\n**Other info / logs**\r\n`WARNING:tensorflow:Early stopping conditioned on metric `weighted_categorical_accuracy` which is not available.`\r\n", "comments": ["@Liu-Da,\r\nI do not have the permission to view the colab link you have provided. Please try saving the Gist using the following method 'File' -> 'Save a copy as Github Gist', and share the link of the new window.\r\n\r\nAlso, I tried to reproduce the issue and did not get any warnings in the code. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/15f7010d7bde816ddd1d5e7c33f6edc2/35857.ipynb). Thanks! ", "> @Liu-Da,\r\n> I do not have the permission to view the colab link you have provided. Please try saving the Gist using the following method 'File' -> 'Save a copy as Github Gist', and share the link of the new window.\r\n> \r\n> Also, I tried to reproduce the issue and did not get any warnings in the code. Please find the Gist [here](https://colab.sandbox.google.com/gist/amahendrakar/15f7010d7bde816ddd1d5e7c33f6edc2/35857.ipynb). Thanks!\r\n\r\n@amahendrakar \r\nActually, there is warining in your [Gist](https://colab.research.google.com/gist/amahendrakar/15f7010d7bde816ddd1d5e7c33f6edc2/35857.ipynb).\r\n![image](https://user-images.githubusercontent.com/24472521/72412560-97db8180-37a8-11ea-8e5b-4655ed966850.png)\r\nThis warning indicate that the early stopping callback cant find **weighted_categorical_accuracy** metric.", "@Liu-Da did you define any `custom_loss function`? As there is no loss/custom_loss function with name `\"weighted_categorical_accuracy\"` in your code, it was throwing that `warning`. Please define a custom_loss function with that name then there will be no issue.\r\n\r\nThere are several ways to write a custom loss function. Here is one way of writing it.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Input\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.losses import Loss\r\nimport numpy as np\r\n\r\ninp = Input(shape=(1,), dtype=tf.float32)\r\ntargets = Input(shape=(1,), dtype=tf.float32)\r\nw = Input(shape=(1,), dtype=tf.float32)\r\nout = Dense(1)(inp)\r\nm = Model(inputs=[inp, w, targets], outputs=out)\r\n\r\n\r\n\r\ndef weighted_loss(y_true, y_pred, weights):\r\n      loss = tf.math.squared_difference(y_pred, y_true)\r\n      w_loss = tf.multiply(weights, loss)\r\n      return tf.reduce_mean(tf.reduce_sum(w_loss, axis=-1))\r\n  \r\nm.add_loss(weighted_loss(targets, out, w))\r\n\r\nm.compile(loss=None, optimizer='adam')\r\nx = np.ones((512,1))\r\nm.fit([x, x, x], epochs=10)\r\nm.save('test.h5')      \r\nm2 = tf.keras.models.load_model('test.h5')\r\nm2.fit([x, x, x], epochs=10)\r\n```\r\n\r\nI am closing this issue as it was resolved. Feel free to post further questions in Stackoverflow as the focus of GitHub repo is mainly for Bugs and performance related issue. Thanks!", "> @Liu-Da did you define any `custom_loss function`? As there is no loss/custom_loss function with name `\"weighted_categorical_accuracy\"` in your code, it was throwing that `warning`. Please define a custom_loss function with that name then there will be no issue.\r\n> \r\n> There are several ways to write a custom loss function. Here is one way of writing it.\r\n> \r\n> ```\r\n> import tensorflow as tf\r\n> from tensorflow.keras.layers import Dense, Input\r\n> from tensorflow.keras.models import Model\r\n> from tensorflow.keras.losses import Loss\r\n> import numpy as np\r\n> \r\n> inp = Input(shape=(1,), dtype=tf.float32)\r\n> targets = Input(shape=(1,), dtype=tf.float32)\r\n> w = Input(shape=(1,), dtype=tf.float32)\r\n> out = Dense(1)(inp)\r\n> m = Model(inputs=[inp, w, targets], outputs=out)\r\n> \r\n> \r\n> \r\n> def weighted_loss(y_true, y_pred, weights):\r\n>       loss = tf.math.squared_difference(y_pred, y_true)\r\n>       w_loss = tf.multiply(weights, loss)\r\n>       return tf.reduce_mean(tf.reduce_sum(w_loss, axis=-1))\r\n>   \r\n> m.add_loss(weighted_loss(targets, out, w))\r\n> \r\n> m.compile(loss=None, optimizer='adam')\r\n> x = np.ones((512,1))\r\n> m.fit([x, x, x], epochs=10)\r\n> m.save('test.h5')      \r\n> m2 = tf.keras.models.load_model('test.h5')\r\n> m2.fit([x, x, x], epochs=10)\r\n> ```\r\n> \r\n> I am closing this issue as it was resolved. Feel free to post further questions in Stackoverflow as the focus of GitHub repo is mainly for Bugs and performance related issue. Thanks!\r\n\r\n@jvishnuvardhan \r\nIt seems that I did not describe my problem well. Let me explain in more detail. When I define a model with multiple outputs, we can define a built-in metric or a custom metric for each output to describe the performance of the model. But my question is how can I define a weighted metric on multiple output nodes.\r\n\r\nfor example. If my model has three output `out1` `out2` `out3`, their metrics are `accuracy`, and correspondingly we can get `acc1` `acc2` `acc3`. My question is if I can define a new metric based on three acc (`weighted_acc = (acc1 + acc2 + acc3) / 3`) and use this metric as an indicator for earlystopping monitoring. This indicator does not correspond to an output node, but describes the performance of the entire model.\r\n\r\nIn my understanding, the `weighted_loss` mentioned in your code is defined for a single output. I couldn't find any information on how to define a metric for multiple outputs. \r\n\r\nI would be very grateful if you could give me any suggestions on this.", "@jvishnuvardhan \r\nI uploaded an image to further describe my problem.\r\n![361579404857_ pic_hd](https://user-images.githubusercontent.com/24472521/72674290-eee69c80-3aaf-11ea-9501-03b4da7933f1.jpg)\r\n", "@jvishnuvardhan \r\nI found a similar issue in the keras project, but there is no feasible solution. https://github.com/keras-team/keras/issues/10287", "> > @Liu-Da did you define any `custom_loss function`? As there is no loss/custom_loss function with name `\"weighted_categorical_accuracy\"` in your code, it was throwing that `warning`. Please define a custom_loss function with that name then there will be no issue.\r\n> > There are several ways to write a custom loss function. Here is one way of writing it.\r\n> > ```\r\n> > import tensorflow as tf\r\n> > from tensorflow.keras.layers import Dense, Input\r\n> > from tensorflow.keras.models import Model\r\n> > from tensorflow.keras.losses import Loss\r\n> > import numpy as np\r\n> > \r\n> > inp = Input(shape=(1,), dtype=tf.float32)\r\n> > targets = Input(shape=(1,), dtype=tf.float32)\r\n> > w = Input(shape=(1,), dtype=tf.float32)\r\n> > out = Dense(1)(inp)\r\n> > m = Model(inputs=[inp, w, targets], outputs=out)\r\n> > \r\n> > \r\n> > \r\n> > def weighted_loss(y_true, y_pred, weights):\r\n> >       loss = tf.math.squared_difference(y_pred, y_true)\r\n> >       w_loss = tf.multiply(weights, loss)\r\n> >       return tf.reduce_mean(tf.reduce_sum(w_loss, axis=-1))\r\n> >   \r\n> > m.add_loss(weighted_loss(targets, out, w))\r\n> > \r\n> > m.compile(loss=None, optimizer='adam')\r\n> > x = np.ones((512,1))\r\n> > m.fit([x, x, x], epochs=10)\r\n> > m.save('test.h5')      \r\n> > m2 = tf.keras.models.load_model('test.h5')\r\n> > m2.fit([x, x, x], epochs=10)\r\n> > ```\r\n> > \r\n> > \r\n> > I am closing this issue as it was resolved. Feel free to post further questions in Stackoverflow as the focus of GitHub repo is mainly for Bugs and performance related issue. Thanks!\r\n> \r\n> @jvishnuvardhan\r\n> It seems that I did not describe my problem well. Let me explain in more detail. When I define a model with multiple outputs, we can define a built-in metric or a custom metric for each output to describe the performance of the model. But my question is how can I define a weighted metric on multiple output nodes.\r\n> \r\n> for example. If my model has three output `out1` `out2` `out3`, their metrics are `accuracy`, and correspondingly we can get `acc1` `acc2` `acc3`. My question is if I can define a new metric based on three acc (`weighted_acc = (acc1 + acc2 + acc3) / 3`) and use this metric as an indicator for earlystopping monitoring. This indicator does not correspond to an output node, but describes the performance of the entire model.\r\n> \r\n> In my understanding, the `weighted_loss` mentioned in your code is defined for a single output. I couldn't find any information on how to define a metric for multiple outputs.\r\n> \r\n> I would be very grateful if you could give me any suggestions on this.\r\n\r\nhttps://github.com/keras-team/keras/issues/10287#issuecomment-636499210"]}, {"number": 35856, "title": "Adding AdaMod Optimizer in Tensorflow 2.x", "body": "**System information**\r\n\r\nTensorFlow version (you are using): 2.0\r\nAre you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI am looking to add one new optimizer called \"Adamod\" . Adamod was  proposed by Jianbang Ding in the paper \"An Adaptive and Momental Bound Method for Stochastic Learning\"\r\nAdaMod uses a new parameter called \u201cBeta3\u201d. The results are improved convergence, no need for warmup, and less sensitivity to the actual learning rate chosen. Beta3 parameter controls the degree of memory.\r\n\r\n**Will this change the current api? How?**\r\nI 've edited `optimizers.py` and created a new file adamod.py which is compatible with Tensorflow 2.x.\r\nUsage :\r\n\r\n```\r\nmodel.compile(optimizer='adamod',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n```\r\n", "comments": ["Thanks for the PR!\r\nHave you checked with developers@tensorflow.org about having a new optimizer? Usually if it's not widely used we would recommend to put it under tensorflow addons first.", "@monk1337 Can you please check reviewer comments and keep us posted. Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!\r\n"]}, {"number": 35855, "title": "build failed on Centos 6.7 with Error: no such instruction: `mulxq %r9,%r8,%r9'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS release 6.7 (Final)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12.0\r\n- Python version: 2.7.9\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 4.8.2\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the problem**\r\nERROR: /home/xiaoju/.cache/bazel/_bazel_xiaoju/be98a53b27ec7c052c1f167e0f93692d/external/boringssl/BUILD:130:1: C++ compilation of rule '@boringssl//:crypto' failed (Exit 1)\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S: Assembler messages:\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:758: Error: no such instruction: `mulxq %r9,%r8,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:759: Error: no such instruction: `mulxq %r10,%rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:760: Error: no such instruction: `mulxq %r11,%rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:762: Error: no such instruction: `mulxq %r12,%rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:764: Error: no such instruction: `mulxq %r15,%rdx,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:771: Error: no such instruction: `mulxq 0+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:772: Error: no such instruction: `adcxq %rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:773: Error: no such instruction: `adoxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:775: Error: no such instruction: `mulxq 8+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:776: Error: no such instruction: `adcxq %rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:777: Error: no such instruction: `adoxq %rbp,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:779: Error: no such instruction: `mulxq 16+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:780: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:781: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:783: Error: no such instruction: `mulxq 24+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:785: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:786: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:787: Error: no such instruction: `adcxq %r8,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:788: Error: no such instruction: `adoxq %r8,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:792: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:793: Error: no such instruction: `adcxq %rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:794: Error: no such instruction: `adoxq %rbp,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:796: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:797: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:798: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:800: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:801: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:802: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:804: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:806: Error: no such instruction: `mulxq %r15,%rdx,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:807: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:808: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:810: Error: no such instruction: `adcxq %r8,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:811: Error: no such instruction: `adoxq %r8,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:815: Error: no such instruction: `mulxq 0+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:816: Error: no such instruction: `adcxq %rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:817: Error: no such instruction: `adoxq %rbp,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:819: Error: no such instruction: `mulxq 8+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:820: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:821: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:823: Error: no such instruction: `mulxq 16+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:824: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:825: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:827: Error: no such instruction: `mulxq 24+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:829: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:830: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:831: Error: no such instruction: `adcxq %r9,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:832: Error: no such instruction: `adoxq %r9,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:836: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:837: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:838: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:840: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:841: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:842: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:844: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:845: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:846: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:848: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:850: Error: no such instruction: `mulxq %r15,%rdx,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:851: Error: no such instruction: `adcxq %rcx,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:852: Error: no such instruction: `adoxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:854: Error: no such instruction: `adcxq %r9,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:855: Error: no such instruction: `adoxq %r9,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:859: Error: no such instruction: `mulxq 0+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:860: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:861: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:863: Error: no such instruction: `mulxq 8+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:864: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:865: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:867: Error: no such instruction: `mulxq 16+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:868: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:869: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:871: Error: no such instruction: `mulxq 24+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:873: Error: no such instruction: `adcxq %rcx,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:874: Error: no such instruction: `adoxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:875: Error: no such instruction: `adcxq %r10,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:876: Error: no such instruction: `adoxq %r10,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:880: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:881: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:882: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:884: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:885: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:886: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:888: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:889: Error: no such instruction: `adcxq %rcx,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:890: Error: no such instruction: `adoxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:892: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:894: Error: no such instruction: `mulxq %r15,%rdx,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:895: Error: no such instruction: `adcxq %rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:896: Error: no such instruction: `adoxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:898: Error: no such instruction: `adcxq %r10,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:899: Error: no such instruction: `adoxq %r10,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:903: Error: no such instruction: `mulxq 0+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:904: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:905: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:907: Error: no such instruction: `mulxq 8+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:908: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:909: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:911: Error: no such instruction: `mulxq 16+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:912: Error: no such instruction: `adcxq %rcx,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:913: Error: no such instruction: `adoxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:915: Error: no such instruction: `mulxq 24+128(%r14),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:918: Error: no such instruction: `adcxq %rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:919: Error: no such instruction: `adoxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:921: Error: no such instruction: `adcxq %r11,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:922: Error: no such instruction: `adoxq %r11,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:999: Error: no such instruction: `mulxq %r14,%r9,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1000: Error: no such instruction: `mulxq %r15,%rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1003: Error: no such instruction: `mulxq %r8,%rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1011: Error: no such instruction: `mulxq %r15,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1012: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1013: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1015: Error: no such instruction: `mulxq %r8,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1017: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1018: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1021: Error: no such instruction: `mulxq %r8,%rcx,%r14'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1025: Error: no such instruction: `adcxq %r9,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1026: Error: no such instruction: `adoxq %rcx,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1027: Error: no such instruction: `adcxq %r10,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1028: Error: no such instruction: `adoxq %r15,%r14'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1031: Error: no such instruction: `mulxq %rdx,%r8,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1033: Error: no such instruction: `adcxq %r11,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1034: Error: no such instruction: `adoxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1035: Error: no such instruction: `adcxq %r12,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1036: Error: no such instruction: `mulxq %rdx,%rcx,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1038: Error: no such instruction: `adcxq %r13,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1039: Error: no such instruction: `adoxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1040: Error: no such instruction: `adcxq %r14,%r14'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1041: Error: no such instruction: `mulxq %rdx,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1044: Error: no such instruction: `adoxq %rax,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1045: Error: no such instruction: `adcxq %r15,%r15'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1046: Error: no such instruction: `adoxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1047: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1048: Error: no such instruction: `mulxq %rdx,%rcx,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1049: Error: no such instruction: `adoxq %rcx,%r14'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1050: Error: no such instruction: `adoxq %rax,%r15'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1054: Error: no such instruction: `mulxq 32(%rsi),%rdx,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1057: Error: no such instruction: `mulxq 0(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1058: Error: no such instruction: `adcxq %rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1059: Error: no such instruction: `adoxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1060: Error: no such instruction: `mulxq 8(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1061: Error: no such instruction: `adcxq %rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1062: Error: no such instruction: `adoxq %rbp,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1063: Error: no such instruction: `mulxq 16(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1064: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1065: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1066: Error: no such instruction: `mulxq 24(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1067: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1068: Error: no such instruction: `adoxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1069: Error: no such instruction: `adcxq %rax,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1073: Error: no such instruction: `mulxq 32(%rsi),%rdx,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1075: Error: no such instruction: `mulxq 0(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1076: Error: no such instruction: `adoxq %rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1077: Error: no such instruction: `adcxq %rbp,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1078: Error: no such instruction: `mulxq 8(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1079: Error: no such instruction: `adoxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1080: Error: no such instruction: `adcxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1081: Error: no such instruction: `mulxq 16(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1082: Error: no such instruction: `adoxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1083: Error: no such instruction: `adcxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1084: Error: no such instruction: `mulxq 24(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1085: Error: no such instruction: `adoxq %rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1086: Error: no such instruction: `adcxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1087: Error: no such instruction: `adoxq %rax,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1091: Error: no such instruction: `mulxq 32(%rsi),%rdx,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1093: Error: no such instruction: `mulxq 0(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1094: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1095: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1096: Error: no such instruction: `mulxq 8(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1097: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1098: Error: no such instruction: `adoxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1099: Error: no such instruction: `mulxq 16(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1100: Error: no such instruction: `adcxq %rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1101: Error: no such instruction: `adoxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1102: Error: no such instruction: `mulxq 24(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1103: Error: no such instruction: `adcxq %rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1104: Error: no such instruction: `adoxq %rbp,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1105: Error: no such instruction: `adcxq %rax,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1109: Error: no such instruction: `mulxq 32(%rsi),%rdx,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1111: Error: no such instruction: `mulxq 0(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1112: Error: no such instruction: `adoxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1113: Error: no such instruction: `adcxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1114: Error: no such instruction: `mulxq 8(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1115: Error: no such instruction: `adoxq %rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1116: Error: no such instruction: `adcxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1117: Error: no such instruction: `mulxq 16(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1118: Error: no such instruction: `adoxq %rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1119: Error: no such instruction: `adcxq %rbp,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1120: Error: no such instruction: `mulxq 24(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1121: Error: no such instruction: `adoxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1122: Error: no such instruction: `adcxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1123: Error: no such instruction: `adoxq %rax,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1711: Error: no such instruction: `mulxq %r9,%r8,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1712: Error: no such instruction: `mulxq %r10,%rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1715: Error: no such instruction: `mulxq %r11,%rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1718: Error: no such instruction: `mulxq %r12,%rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1721: Error: no such instruction: `shlxq %r14,%r8,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1723: Error: no such instruction: `shrxq %r14,%r8,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1731: Error: no such instruction: `mulxq %r15,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1740: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1741: Error: no such instruction: `adcxq %rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1742: Error: no such instruction: `adoxq %rbp,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1744: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1745: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1746: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1748: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1749: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1750: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1752: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1754: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1755: Error: no such instruction: `shlxq %r14,%r9,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1756: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1757: Error: no such instruction: `shrxq %r14,%r9,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1759: Error: no such instruction: `adcxq %r8,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1760: Error: no such instruction: `adoxq %r8,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1768: Error: no such instruction: `mulxq %r15,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1777: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1778: Error: no such instruction: `adcxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1779: Error: no such instruction: `adoxq %rbp,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1781: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1782: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1783: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1785: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1786: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1787: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1789: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1791: Error: no such instruction: `adcxq %rcx,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1792: Error: no such instruction: `shlxq %r14,%r10,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1793: Error: no such instruction: `adoxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1794: Error: no such instruction: `shrxq %r14,%r10,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1796: Error: no such instruction: `adcxq %r9,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1797: Error: no such instruction: `adoxq %r9,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1805: Error: no such instruction: `mulxq %r15,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1814: Error: no such instruction: `mulxq 0+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1815: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1816: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1818: Error: no such instruction: `mulxq 8+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1819: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1820: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1822: Error: no such instruction: `mulxq 16+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1823: Error: no such instruction: `adcxq %rcx,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1824: Error: no such instruction: `adoxq %rbp,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1826: Error: no such instruction: `mulxq 24+128(%rsi),%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1828: Error: no such instruction: `adcxq %rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1829: Error: no such instruction: `shlxq %r14,%r11,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1830: Error: no such instruction: `adoxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1831: Error: no such instruction: `shrxq %r14,%r11,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1833: Error: no such instruction: `adcxq %r10,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1834: Error: no such instruction: `adoxq %r10,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1842: Error: no such instruction: `mulxq %r15,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1876: Error: no such instruction: `mulxq %r14,%r9,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1877: Error: no such instruction: `mulxq %r15,%rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1880: Error: no such instruction: `mulxq %r8,%rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1887: Error: no such instruction: `mulxq %r15,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1888: Error: no such instruction: `adcxq %rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1889: Error: no such instruction: `adoxq %rbp,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1891: Error: no such instruction: `mulxq %r8,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1893: Error: no such instruction: `adcxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1894: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1898: Error: no such instruction: `mulxq %r8,%rcx,%r14'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1901: Error: no such instruction: `adcxq %r9,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1902: Error: no such instruction: `adoxq %rcx,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1903: Error: no such instruction: `adcxq %r10,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1904: Error: no such instruction: `adoxq %r15,%r14'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1906: Error: no such instruction: `mulxq %rdx,%r8,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1908: Error: no such instruction: `adcxq %r11,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1909: Error: no such instruction: `adoxq %rbp,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1910: Error: no such instruction: `adcxq %r12,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1911: Error: no such instruction: `mulxq %rdx,%rcx,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1913: Error: no such instruction: `adcxq %r13,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1914: Error: no such instruction: `adoxq %rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1915: Error: no such instruction: `adcxq %r14,%r14'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1917: Error: no such instruction: `mulxq %rdx,%rcx,%rbp'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1919: Error: no such instruction: `adoxq %rax,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1920: Error: no such instruction: `adcxq %r15,%r15'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1921: Error: no such instruction: `adoxq %rcx,%r12'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1923: Error: no such instruction: `adoxq %rbp,%r13'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1925: Error: no such instruction: `mulxq %rdx,%rcx,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1927: Error: no such instruction: `adoxq %rcx,%r14'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1928: Error: no such instruction: `shlxq %rsi,%r8,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1929: Error: no such instruction: `adoxq %rax,%r15'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1930: Error: no such instruction: `shrxq %rsi,%r8,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1937: Error: no such instruction: `mulxq %r8,%rcx,%r8'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1939: Error: no such instruction: `shlxq %rsi,%r9,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1941: Error: no such instruction: `shrxq %rsi,%r9,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1947: Error: no such instruction: `mulxq %r9,%rcx,%r9'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1949: Error: no such instruction: `shlxq %rsi,%r10,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1951: Error: no such instruction: `shrxq %rsi,%r10,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1957: Error: no such instruction: `mulxq %r10,%rcx,%r10'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1959: Error: no such instruction: `shlxq %rsi,%r11,%rcx'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1961: Error: no such instruction: `shrxq %rsi,%r11,%rax'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:1967: Error: no such instruction: `mulxq %r11,%rcx,%r11'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2127: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2128: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2129: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2135: Error: no such instruction: `vpermd %ymm1,%ymm2,%ymm1'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2148: Error: suffix or operands invalid for `vpcmpeqd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2149: Error: suffix or operands invalid for `vpcmpeqd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2151: Error: suffix or operands invalid for `vpaddd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2152: Error: suffix or operands invalid for `vpaddd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2155: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2156: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2157: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2158: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2159: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2160: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2162: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2163: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2164: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2165: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2166: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2167: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2191: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2192: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2199: Error: no such instruction: `vpermd %ymm1,%ymm2,%ymm1'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2214: Error: suffix or operands invalid for `vpcmpeqd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2215: Error: suffix or operands invalid for `vpcmpeqd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2216: Error: suffix or operands invalid for `vpcmpeqd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2218: Error: suffix or operands invalid for `vpaddd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2219: Error: suffix or operands invalid for `vpaddd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2220: Error: suffix or operands invalid for `vpaddd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2223: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2224: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2225: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2226: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2227: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2228: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2230: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2231: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2232: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2233: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2234: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2235: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2244: Error: suffix or operands invalid for `vpcmpeqd'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2246: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2247: Error: suffix or operands invalid for `vpand'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2249: Error: suffix or operands invalid for `vpxor'\r\nexternal/boringssl/linux-x86_64/crypto/fipsmodule/p256-x86_64-asm.S:2250: Error: suffix or operands invalid for `vpxor'\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build -c opt //tensorflow:libtensorflow_cc.so\r\n", "comments": ["@striveth \r\nJust to verify did you get chance to follow instructions from\u00a0[TensorFlow website\u00a0](https://www.tensorflow.org/install/source).Please, let us know how it progress.Thanks!\r\n\r\n\r\n\r\n", "> @striveth\r\n> Just to verify did you get chance to follow instructions from\u00a0[TensorFlow website\u00a0](https://www.tensorflow.org/install/source).Please, let us know how it progress.Thanks!\r\n\r\ni just download the source from release website(https://github.com/tensorflow/tensorflow/releases/tag/v1.12.0), and then execute the command: bazel build -c opt //tensorflow:libtensorflow_cc.so", "I resolved it by updating binutils version to 2.31", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35855\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35855\">No</a>\n"]}]