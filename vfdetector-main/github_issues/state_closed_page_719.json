[{"number": 32006, "title": "Very unhelpful error msg building keras models with while loops", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v1.12.1-9365-gff401a6 1.15.0-dev20190821\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / ??\r\n- GPU model and memory: Quadro 2gb\r\n\r\n**Describe the current behavior**\r\n`_create_keras_history_helper` is making building networks much more pleasant in general by forgoing the need to wrap everything in `Lambda` layers. It's failing for while loops without `Lambda` wrapping, giving a very unhelpful error message. `tensorflow.python.framework.errors_impl.InvalidArgumentError: A cross-device loop must have a pivot predicate: while/while_context`\r\n\r\n**Describe the expected behavior**\r\nIndicate the source of the problem/possible resolution.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef cond(i, x):\r\n    return tf.reduce_all(x < 10)\r\n\r\ndef body(i, x):\r\n    return i + 1, x + i\r\n\r\nx = tf.keras.layers.Input(shape=(), dtype=tf.float32)\r\ninc = tf.while_loop(cond, body, [tf.constant(0, dtype=tf.float32), x])\r\n# the following fixes things\r\n# inc = tf.keras.layers.Lambda(lambda x: tf.while_loop(\r\n#     cond, body, [tf.constant(0, dtype=tf.float32), x]))(x)\r\n\r\nmodel = tf.keras.Model(inputs=x, outputs=inc)  # <- error occurs here\r\n```\r\n\r\n**Other info / logs**\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"loop.py\", line 23, in <module>\r\n    model = tf.keras.Model(inputs=x, outputs=inc)  # <- error occurs here\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\", line 147, in __init__\r\n    super(Model, self).__init__(*args, **kwargs)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 164, in __init__\r\n    self._init_graph_network(*args, **kwargs)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\", line 457, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\", line 267, in _init_graph_network\r\n    base_layer_utils.create_keras_history(self._nested_outputs)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 184, in create_keras_history\r\n    _, created_layers = _create_keras_history_helper(tensors, set(), [])\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 231, in _create_keras_history_helper\r\n    layer_inputs, processed_ops, created_layers)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 231, in _create_keras_history_helper\r\n    layer_inputs, processed_ops, created_layers)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 229, in _create_keras_history_helper\r\n    constants[i] = backend.function([], op_input)([])\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 3473, in __call__\r\n    self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\", line 3410, in _make_callable\r\n    callable_fn = session._make_callable_from_options(callable_opts)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1505, in _make_callable_from_options\r\n    return BaseSession._Callable(self, callable_options)\r\n  File \".../.anaconda2/envs/tf-nightly-gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\", line 1460, in __init__\r\n    session._session, options_ptr)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: A cross-device loop must have a pivot predicate: while/while_context\r\n```", "comments": ["@jackd ,\r\nWhen tried executing the given code I got error `InvalidArgumentError: You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?] [[{{node input_1}}]]` Thanks!", "@oanush I've just re-run and my output hasn't changed, though if you are getting that output then I'm even more bermused. That sounds like an error associated with running a session, but there's nothing in the code trying to do that.", "@jackd,\r\nThank you, I was able to replicate the issue with TF-nightly-gpu. Kindly find the [Gist](https://colab.research.google.com/drive/1sNlOEfOWiUZEE9KH044VfmMcPbMIWIuO) of colab.", "Can you wrap the while loop in a lambda layer?\r\n\r\n```\r\ninc = tf.keras.layers.Lambda(lambda i: tf.while_loop(cond, body, [tf.constant(0, dtype=tf.float32), i]))(x)\r\n```\r\n\r\nBy default if TF ops are used in a tf.keras model, without having been wrapped in a tf.keras layer, we try to wrap them ourselves. This will work only for use cases where we can backtrack to the inputs of the model. If your custom op has a control dependency for example, this automatic wrapping will not work. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32006\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32006\">No</a>\n", "@pavithrasv yes, wrapping it in a keras layer resolves things. I'm starting to accept that `_create_keras_history_helper` isn't the panacea I thought it once was - and trying to write code that feels more like non-keras tf without wrapping things in `Lambda`s is generally just a bad idea.\r\n\r\nMy main issue is the lack of informative error message. `A cross-device loop must have a pivot predicate: while/while_context` doesn't exactly shout \"you're missing a `Lambda` layer.\""]}, {"number": 32005, "title": "Fix build", "body": "Added missing entries forkernels/internal/ruy/*.cc", "comments": ["@mattn Could you please resolve the conflicts? Thanks!", "@gbaned done"]}, {"number": 32004, "title": "Python/C++ API interpreter example for hybrid models", "body": "I have a hybrid tflite model, e.g. was converted with the option\r\n\r\n`converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]`\r\n\r\nSo it contains both tflite ops and normal ops. I test a lot, so this is quicker than implementing the missing parts myself. When trying to load the model with an interpreter, either with the Python or the C++ API, I get errors:\r\n\r\nPython\r\n```\r\nRuntimeError: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.Node number 4 (Flex) failed to prepare.\r\n```\r\nC++\r\n```\r\nINFO: Initialized TensorFlow Lite runtime.\r\nERROR: Regular TensorFlow ops are not supported by this interpreter. Make sure you invoke the Flex delegate before inference.\r\nERROR: Node number 4 (FlexSoftplus) failed to prepare.\r\n```\r\n\r\nIt doesn't seem there are docs that cover how to treat this error and load such hybrid models correctly. If I have missed any docs by any chance, please share the link!", "comments": ["You probably need to build the C++ code with bazel, using  tensorflow/lite/delegates/flex:delegate as dependency:\r\nhttps://www.tensorflow.org/lite/guide/ops_select#c\r\n\r\nFor Python, it is stated that it's under active development. +1 for that feature. It would simplify writing tests. If there's any documentation or idea how to TFLite with select ops interference working in python, I'd be very grateful :)", "Any idea how to make that work on python? It's been a long time under active development now", "+1 for that feature", "Haven't tried it myself, but there's now a build flag to enable the delegate:\r\ntflite_pip_with_flex\r\n\r\nInstructions can be found in: tensorflow/lite/tools/pip_package/README.md\r\n\r\nNote that is only a few days old (see commit 865127af8a), so you'll need to build from master.", "@yaysummeriscoming thanks for the hint! Looks promising. At least we know that some progress is being made \ud83d\udc4d ", "See #40157 for updates on this thread", "With tf-nightly, now you can use TF ops without any issue.", "> You probably need to build the C++ code with bazel, using tensorflow/lite/delegates/flex:delegate as dependency:\r\n> https://www.tensorflow.org/lite/guide/ops_select#c\r\n> \r\n> For Python, it is stated that it's under active development. +1 for that feature. It would simplify writing tests. If there's any documentation or idea how to TFLite with select ops interference working in python, I'd be very grateful :)\r\n\r\nHello @tpeet \r\nCould you please provide the detailed instructions on how to build TFLite C++ API , using tensorflow/lite/delegates/flex:delegate as dependency?\r\n\r\nFrom the link you shared I don't know where to specify the build dependencies.\r\n\r\nThanks."]}, {"number": 32003, "title": "TF Serving version 1.10.0", "body": "Hi all,\r\n\r\nCan some one give me steps to build TF Serving version 1.10.0  on centos7 image", "comments": ["@amrithadevadiga \r\nThis issue is more suitable for TensorFlow Serving repo. Please post it on Serving repo from [here.](https://github.com/tensorflow/serving/issues) Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32003\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32003\">No</a>\n"]}, {"number": 32002, "title": "Tensorflow Lite compute library for hardware acceleration ", "body": "Does Tensorflow lite has their own compute library for hardware(arm cpu, gpu, fpga, etc.) acceleration? If they have it, where is it located at? which directory? Thanks!", "comments": ["See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/gpu for using TFLite on GPU", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32002\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32002\">No</a>\n"]}, {"number": 32001, "title": "The title is the shared_embeddings module\uff0cbut the document introduces shared_embedding_columns module", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://tensorflow.google.cn/versions/r2.0/api_docs/python/tf/feature_column/shared_embeddings\r\n\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe description in the document is a way to use the shared_embedding_columns module, but the title is the shared_embeddings module, and the shared_embedding_columns module has been removed in tensorflow2.0.\r\n", "comments": ["This is fixed with TF 2.4.1 api docs. Thanks!\r\nhttps://www.tensorflow.org/api_docs/python/tf/feature_column/shared_embeddings"]}, {"number": 32000, "title": "[tflite] Fix for the test interpreter_test.py with Python3", "body": "The test interpreter_test.py is failing when it is run using Python3.\r\nThere are several issues due to difference between Python2 and Python3:\r\n\r\nThe string in Python2 is stored as bytes by default, but as unicode in Python3 by default.\r\nIn Python3 if the string is in bytes, it has the prefix 'b'.\r\nWe convert all strings to be the same.\r\n\r\nThere is an exception chaining in Python3. As result, the message thrown from the exception is different.\r\n\r\nThere is a segmentation fault on Python3, when all tests in the file interpreter_test are run. The bug is that the global variable stores the pointer to the function, which goes away from the scope.\r\n\r\nThe extract from the sample failure log, when running //tensorflow/lite/python:interpreter_test test:\r\n\r\n> INFO: From Testing //tensorflow/lite/python:interpreter_test:\r\n> 20:08:53  ==================== Test output for //tensorflow/lite/python:interpreter_test:\r\n> 20:08:53  Running tests under Python 3.6.8: /usr/local/bin/python\r\n> 20:08:53  [ RUN      ] InterpreterDelegateTest.testDelegate\r\n> 20:08:53  [       OK ] InterpreterDelegateTest.testDelegate\r\n> 20:08:53  [ RUN      ] InterpreterDelegateTest.testDestructionOrder\r\n> 20:08:53  [  FAILED  ] InterpreterDelegateTest.testDestructionOrder\r\n> 20:08:53  [ RUN      ] InterpreterDelegateTest.testFail\r\n> 20:08:53  Traceback (most recent call last):\r\n> 20:08:53    File \"_ctypes/callbacks.c\", line 234, in 'calling callback function'\r\n> 20:08:53    File \"/.../execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/python/interpreter_test.runfiles/org_tensorflow/tensorflow/lite/python/interpreter.py\", line 111, in report\r\n> 20:08:53      self.message += x\r\n> 20:08:53  TypeError: must be str, not bytes\r\n> 20:08:53  [  FAILED  ] InterpreterDelegateTest.testFail\r\n> 20:08:53  [ RUN      ] InterpreterDelegateTest.testMultipleInterpreters\r\n> 20:08:53  [       OK ] InterpreterDelegateTest.testMultipleInterpreters\r\n> 20:08:53  [ RUN      ] InterpreterDelegateTest.testOptions\r\n> 20:08:53  [       OK ] InterpreterDelegateTest.testOptions\r\n> 20:08:53  [ RUN      ] InterpreterDelegateTest.test_session\r\n> 20:08:53  W0821 19:08:46.430220 139904211531584 deprecation.py:323] From /usr/lib/python3.6/contextlib.py:60: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\n> 20:08:53  Instructions for updating:\r\n> 20:08:53  Use `self.session()` or `self.cached_session()` instead.\r\n> 20:08:53  [       OK ] InterpreterDelegateTest.test_session\r\n> 20:08:53  [ RUN      ] InterpreterTensorAccessorTest.testBase\r\n> 20:08:53  [       OK ] InterpreterTensorAccessorTest.testBase\r\n> 20:08:53  [ RUN      ] InterpreterTensorAccessorTest.testBaseProtectsFunctions\r\n> 20:08:53  [       OK ] InterpreterTensorAccessorTest.testBaseProtectsFunctions\r\n> 20:08:53  [ RUN      ] InterpreterTensorAccessorTest.testGetTensorAccessor\r\n> 20:08:53  [       OK ] InterpreterTensorAccessorTest.testGetTensorAccessor\r\n> 20:08:53  [ RUN      ] InterpreterTensorAccessorTest.testTensorAccessor\r\n> 20:08:53  [       OK ] InterpreterTensorAccessorTest.testTensorAccessor\r\n> 20:08:53  [ RUN      ] InterpreterTensorAccessorTest.test_session\r\n> 20:08:53  [       OK ] InterpreterTensorAccessorTest.test_session\r\n> 20:08:53  [ RUN      ] InterpreterTest.testFloat\r\n> 20:08:53  [       OK ] InterpreterTest.testFloat\r\n> 20:08:53  [ RUN      ] InterpreterTest.testString\r\n> 20:08:53  [       OK ] InterpreterTest.testString\r\n> 20:08:53  [ RUN      ] InterpreterTest.testUint8\r\n> 20:08:53  [       OK ] InterpreterTest.testUint8\r\n> 20:08:53  [ RUN      ] InterpreterTest.test_session\r\n> 20:08:53  [  SKIPPED ] InterpreterTest.test_session\r\n> 20:08:53  [ RUN      ] InterpreterTestErrorPropagation.testInvalidIndex\r\n> 20:08:53  [       OK ] InterpreterTestErrorPropagation.testInvalidIndex\r\n> 20:08:53  [ RUN      ] InterpreterTestErrorPropagation.testInvalidModelContent\r\n> 20:08:53  [       OK ] InterpreterTestErrorPropagation.testInvalidModelContent\r\n> 20:08:53  [ RUN      ] InterpreterTestErrorPropagation.testInvalidModelFile\r\n> 20:08:53  [       OK ] InterpreterTestErrorPropagation.testInvalidModelFile\r\n> 20:08:53  [ RUN      ] InterpreterTestErrorPropagation.testInvalidModelFileContent\r\n> 20:08:53  [       OK ] InterpreterTestErrorPropagation.testInvalidModelFileContent\r\n> 20:08:53  [ RUN      ] InterpreterTestErrorPropagation.testInvokeBeforeReady\r\n> 20:08:53  [       OK ] InterpreterTestErrorPropagation.testInvokeBeforeReady\r\n> 20:08:53  [ RUN      ] InterpreterTestErrorPropagation.test_session\r\n> 20:08:53  [  SKIPPED ] InterpreterTestErrorPropagation.test_session\r\n> 20:08:53  ======================================================================\r\n> 20:08:53  FAIL: testDestructionOrder (__main__.InterpreterDelegateTest)\r\n> 20:08:53  testDestructionOrder (__main__.InterpreterDelegateTest)\r\n> 20:08:53  Make sure internal _interpreter object is destroyed before delegate.\r\n> 20:08:53  ----------------------------------------------------------------------\r\n> 20:08:53  Traceback (most recent call last):\r\n> 20:08:53    File \"/.../execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/python/interpreter_test.runfiles/org_tensorflow/tensorflow/lite/python/interpreter_test.py\", line 330, in testDestructionOrder\r\n> 20:08:53      self.assertEqual(destructions, ['interpreter', 'test_delegate'])\r\n> 20:08:53  AssertionError: Lists differ: ['interpreter', b'test_delegate'] != ['interpreter', 'test_delegate']\r\n> 20:08:53  \r\n> 20:08:53  First differing element 1:\r\n> 20:08:53  b'test_delegate'\r\n> 20:08:53  'test_delegate'\r\n> 20:08:53  \r\n> 20:08:53  - ['interpreter', b'test_delegate']\r\n> 20:08:53  ?                 -\r\n> 20:08:53  \r\n> 20:08:53  + ['interpreter', 'test_delegate']\r\n> 20:08:53  \r\n> 20:08:53  ======================================================================\r\n> 20:08:53  FAIL: testFail (__main__.InterpreterDelegateTest)\r\n> 20:08:53  testFail (__main__.InterpreterDelegateTest)\r\n> 20:08:53  ----------------------------------------------------------------------\r\n> 20:08:53  Traceback (most recent call last):\r\n> 20:08:53    File \"/.../execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/python/interpreter_test.runfiles/org_tensorflow/tensorflow/lite/python/interpreter.py\", line 165, in load_delegate\r\n> 20:08:53      delegate = Delegate(library, options)\r\n> 20:08:53  ValueError\r\n> 20:08:53  \r\n> 20:08:53  During handling of the above exception, another exception occurred:\r\n> 20:08:53  \r\n> 20:08:53  ValueError: Failed to load delegate from /.../execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/python/interpreter_test.runfiles/org_tensorflow/tensorflow/lite/python/testdata/test_delegate.so\r\n> 20:08:53  \r\n> 20:08:53  \r\n> 20:08:53  During handling of the above exception, another exception occurred:\r\n> 20:08:53  \r\n> 20:08:53  Traceback (most recent call last):\r\n> 20:08:53    File \"/.../execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/python/interpreter_test.runfiles/org_tensorflow/tensorflow/lite/python/interpreter_test.py\", line 369, in testFail\r\n> 20:08:53      self._delegate_file, options={'fail': 'fail'})\r\n> 20:08:53  AssertionError: \"Failed to load delegate from .*\r\n> 20:08:53  Fail argument sent.\" does not match \"Failed to load delegate from /.../execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/python/interpreter_test.runfiles/org_tensorflow/tensorflow/lite/python/testdata/test_delegate.so\r\n> 20:08:53  \"\r\n> 20:08:53  \r\n> 20:08:53  ----------------------------------------------------------------------\r\n> 20:08:53  Ran 21 tests in 0.037s\r\n> 20:08:53  \r\n> 20:08:53  FAILED (failures=2, skipped=2)\r\n> 20:08:53  ================================================================================\r\n> 20:09:25  INFO: Elapsed time: 3377.988s, Critical Path: 296.39s\r\n> 20:09:25  INFO: 4472 processes: 4472 local.\r\n> 20:09:25  INFO: Build completed, 1 test FAILED, 4639 total actions\r\n> 20:09:25  //tensorflow/lite/python:interpreter_test                                FAILED in 6.9s\r\n> 20:09:25      FAILED  __main__.InterpreterDelegateTest.testDestructionOrder (0.0s)\r\n> 20:09:25      FAILED  __main__.InterpreterDelegateTest.testFail (0.0s)\r\n> 20:09:25  Test cases: finished with 130 passing and 2 failing out of 132 test cases\r\n> 20:09:25  \r\n> 20:09:25  Executed 9 out of 9 tests: 8 tests pass and 1 fails locally.\r\n> 20:09:25  There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.\r\n> 20:09:25  INFO: Build completed, 1 test FAILED, 4639 total actions\r\n> ", "comments": ["@akarmi Could you please resolve the conflicts? Thanks!", "It appears that the main issue has recently been resolved by [this commit](https://github.com/tensorflow/tensorflow/commit/b608d1180567fd303107a9381bbdf7dd7555d9f4). I resolved the conflict. Now, this PR only fixes an issue with the invalid pointer in tflite_plugin_destroy_delegate() and exception message filtering in one of the test assert statements.", "@aselle, can you please review this PR? It's been outstanding for quite some time. I believe it should be merged to ensure the complete resolution of the described issues. Thanks."]}, {"number": 31999, "title": "SparseTensor stopped working on tf.keras when moving from 2.0.0-beta1 to 2.0.0-rc0", "body": "I just moved from `2.0.0-beta1` to `2.0.0-rc0` and some code for handling sparse categorical variable stopped working for me.\r\n\r\nHere is some minimal code to reproduce the issue.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass SparseSlice(tf.keras.layers.Layer):\r\n    def __init__(self, feature_column):\r\n        super(SparseSlice, self).__init__()\r\n        self.fc = feature_column\r\n\r\n    def build(self, input_shape):\r\n\r\n        self.kernel = self.add_weight('{}_kernel'.format(self.fc.name), shape=(self.fc.num_buckets, ), dtype=tf.float32)\r\n\r\n    def call(self, input):\r\n        ids = self.fc._transform_input_tensor(input)\r\n        return tf.expand_dims(tf.gather(self.kernel, ids.values), axis=1)\r\n\r\n\r\nbatch_size = 10\r\nc = 'smth'\r\ncol = tf.feature_column.categorical_column_with_hash_bucket(c, 10000, dtype=tf.int64)\r\nexample_spec = tf.feature_column.make_parse_example_spec([col])\r\n\r\ninputs = tf.keras.layers.Input(name=c, shape=(None, ), batch_size=batch_size, sparse=True, dtype=tf.int64)\r\nsparse_out = SparseSlice(col)(inputs)\r\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(sparse_out)\r\n\r\nmodel = tf.keras.Model(inputs, output)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='mse')\r\n\r\n\r\nfeatures = {c: tf.sparse.SparseTensor(indices=[[i, 0] for i in range(batch_size)], values=np.random.randint(0, 1000, (batch_size, )).tolist(), dense_shape=(batch_size, 1))}\r\nys = tf.constant(np.random.rand(batch_size).tolist(), dtype=tf.float32)\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices((features, ys)).batch(batch_size)\r\n\r\nmodel.fit(x=dataset,\r\n          epochs=1\r\n          )\r\n```\r\non `2.0.0-rc0` I am getting the following error\r\n\r\n```\r\nValueError: The two structures don't have the same nested structure.\r\nFirst structure: type=SparseTensorSpec str=SparseTensorSpec(TensorShape([None, 1]), tf.int32)\r\nSecond structure: type=SparseTensor str=SparseTensor(indices=Tensor(\"smth/indices:0\", shape=(None, 2), dtype=int64), values=Tensor(\"smth/values:0\", shape=(None,), dtype=int64), dense_shape=Tensor(\"smth/shape:0\", shape=(2,), dtype=int64))\r\nMore specifically: Incompatible CompositeTensor TypeSpecs: type=SparseTensorSpec str=SparseTensorSpec(TensorShape([None, 1]), tf.int32) vs. type=SparseTensorSpec str=SparseTensorSpec(TensorShape([None, None]), tf.int64)\r\nEntire first structure:\r\n.\r\nEntire second structure:\r\n.\r\n```\r\nWhereas everything runs fine in `2.0.0-beta1`", "comments": ["Was able to reproduce the issue. Please find the associated colab link for [2.0.0-beta1](https://colab.sandbox.google.com/gist/gowthamkpr/6408be4542eb305927594dd16573af92/untitled108.ipynb) and [2.0.0-rc0](https://colab.sandbox.google.com/gist/gowthamkpr/4f25ab88488a7854065c145c46780512/untitled109.ipynb) ", "Hi all,\r\n\r\nThe code above is hitting a datatype check.\r\n\r\nMore specifically: Incompatible CompositeTensor TypeSpecs: type=SparseTensorSpec str=SparseTensorSpec(TensorShape([None, 1]), **tf.int32**) vs. type=SparseTensorSpec str=SparseTensorSpec(TensorShape([None, None]), **tf.int64**)\r\n\r\nI took a look at your code above (thanks so much for the repro colab, @gowthamkpr!) and it looks like calling .tolist() on your np.random.randint array is causing the data to be cast to int32; this causes the input SparseTensor type to be tf.int32, which is not compatible with your specified input type of tf.int64.\r\n\r\nTo fix this particular error, you can either remove .tolist(), or change your Input dtype to tf.int32.\r\n\r\nI will work with the internal team to try and make the reported error clearer. I apologize for the inconvenience and confusion this has caused!", "Thanks for your answer @markomernick. It works. I am still struggling a bit with these types of errors here and there. For example let's say I want to predict with exactly the model outlined above.\r\n\r\nif I simply run:\r\n\r\n```\r\nmodel.predict({c: np.random.randint(0, 1000, (batch_size, 1))})\r\n```\r\n\r\nI incur into\r\n\r\n```\r\nValueError: The two structures don't have the same nested structure.\r\nFirst structure: type=TensorSpec str=TensorSpec(shape=(10, 1), dtype=tf.int64, name=None)\r\nSecond structure: type=SparseTensor str=SparseTensor(indices=Tensor(\"smth/indices_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"smth/values_1:0\", shape=(None,), dtype=int64), dense_shape=Tensor(\"smth/shape_1:0\", shape=(2,), dtype=int64))\r\nMore specifically: Substructure \"type=SparseTensor str=SparseTensor(indices=Tensor(\"smth/indices_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"smth/values_1:0\", shape=(None,), dtype=int64), dense_shape=Tensor(\"smth/shape_1:0\", shape=(2,), dtype=int64))\" is a sequence, while substructure \"type=TensorSpec str=TensorSpec(shape=(10, 1), dtype=tf.int64, name=None)\" is not\r\nEntire first structure:\r\n.\r\nEntire second structure:\r\n.\r\n```` ", "Hi @cadama:\r\n\r\nIn that case, you're trying to pass a standard Tensor (First structure: type=**TensorSpec** str=TensorSpec(shape=(10, 1), dtype=tf.int64, name=None) into a Model that expects SparseTensors (Second structure: type=**SparseTensor** str=SparseTensor(indices=Tensor(\"smth/indices_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"smth/values_1:0\", shape=(None,), dtype=int64), dense_shape=Tensor(\"smth/shape_1:0\", shape=(2,), dtype=int64))). This won't work, because the underlying Tensorflow for your model expects 3 placeholder tensors (for sparse indices, values, and shape). We can't sparsify your tensor automatically, either, since it's not clear what values to drop.\r\n\r\nAgain, I apologize for the lack of clarity in the error message - we're working on it.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31999\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31999\">No</a>\n"]}, {"number": 31998, "title": "[TF 2.0] GRU layer doesn't work when called from tf.function", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDarwin Kernel Version 18.6.0\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\n2.0.0-dev20190826\r\n- Python version:\r\nPython 3.6.8 :: Anaconda, Inc.\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe code below raises error:\r\n\r\n> Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.\r\n\r\nAs noted in comments, the bug disappears if we don't use `tf.function`, or set `persistent=False` in gradient tape.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = tf.cast(np.random.randn(1, 100), tf.float32)\r\ny = tf.cast(np.random.randn(1, 100, 100), tf.float32)\r\nz = tf.cast(np.random.randn(1, 100), tf.float32)\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.layer = tf.keras.layers.GRU(100)\r\n\r\n    @tf.function # remove this and it works fine\r\n    def call(self, x, y):\r\n        z = self.layer(y, initial_state=x)\r\n        return z\r\n\r\nmodel = Model()\r\n\r\nwith tf.GradientTape(persistent=True) as tape: # if persistent=False it works fine\r\n    loss = tf.norm(model(x, y) - z)\r\ngrads = tape.gradient(loss, model.trainable_variables)\r\n```", "comments": ["Issue replicating for TF version 2.0, please find the [Gist](https://colab.research.google.com/drive/1wvCV4_fGL9DHqyyVrLreTStmk0tqGcqb) of Colab.Thanks!", "This looks like a bug in GradientTape, though it's unclear whether it's specific to GRU layer.", "Saurabh, looks WhileGrad / TensorList related?", "Yeah this seems to be failing when building the gradient of `TensorListPopBack`. It seems like with `persistent=True` this takes the path that outputs all necessary intermediates to support higher-order derivatives which triggers taking the derivative of `TensorListPopBack`. That path is not taken with `persistent=False` iiuc. I will look into a fix.", "@David-Mao,\r\nI was able to reproduce the error with TF v2.0. However, the issue seems to be fixed with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/7033da4af6575764dcda2cf9b10ac6ed/2-1-template.ipynb). Please find the attached gist. Thanks!", "> @David-Mao,\r\n> I was able to reproduce the error with TF v2.0. However, the issue seems to be fixed with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/7033da4af6575764dcda2cf9b10ac6ed/2-1-template.ipynb). Please find the attached gist. Thanks!\r\n\r\nAny updates regarding this issue? Thanks!\r\n", "Sorry I don't understand. Should we close this since this seems to be fixed in 2.1? Are you asking if this will be fixed in 2.0? I don't think we would want to do that now since the fix is already available in a public release.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31998\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31998\">No</a>\n", "This problem still exists in TF2.1/2.2, the strangest thing is that the GRU is OK now but the LSTM is not. Does anybody have a solution?\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model as M\r\nfrom tensorflow.keras import Input as I\r\n\r\nx = tf.cast(np.random.randn(1, 100, 100), tf.float32)\r\ny = tf.cast(np.random.randn(1, 100), tf.float32)\r\nz = tf.cast(np.random.randn(1, 100), tf.float32)\r\nu = tf.cast(np.random.randn(1, 100), tf.float32)\r\n\r\nclass Model(M):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = tf.keras.layers.GRU(100)\r\n        self(I(shape=(None, 100)), I(shape=(100,)))\r\n\r\n    @tf.function # remove this and it works fine\r\n    def call(self, x, y):\r\n        z = self.layer(x, initial_state=y)\r\n        return z\r\n\r\n\r\nmodel = Model()\r\n\r\nwith tf.GradientTape(persistent=True) as tape: # if persistent=False it works fine\r\n    loss = tf.norm(model(x, y) - z)\r\ngrads = tape.gradient(loss, model.trainable_variables)\r\n\r\nprint(\"###############SUCCESS################\")\r\n\r\nclass Model2(M):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.layer = tf.keras.layers.LSTM(100)\r\n        self(I(shape=(None, 100)), I(shape=(100,)), I(shape=(100,)))\r\n\r\n    @tf.function     # remove this and it works fine\r\n    def call(self, s, h, c):\r\n        z = self.layer(s, initial_state=(h, c))\r\n        return z\r\n\r\n\r\nmodel2 = Model2()\r\n\r\nwith tf.GradientTape(persistent=True) as tape: # if persistent=False it works fine\r\n    loss = tf.norm(model2(x, y, u) - z)\r\ngrads = tape.gradient(loss, model2.trainable_variables)\r\n\r\n\r\nprint(\"###############SUCCESS################\")\r\n```\r\n\r\nStill get this error:\r\n`ValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.`\r\n\r\n@bluefisher\r\nFinally, construct LSTM in this way works fine:\r\n```python\r\nself.layer = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(100))\r\n```\r\n\r\nBut, still unclear why this error happens, is there anything wrong with CuDNN when using `persistent=True`?", "@StepNeverStop,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose), so that we can track the issue there. Thanks!", "BTW, I checked the code from @StepNeverStop with tf-nightly and the issue doesn't repro, so it might be that the fix landed in 2.3, instead of 2.2. It would be worth double checking.", "@amahendrakar ,\r\nSince @mdanatg has verified that the code works with tf-nightly, I don't think it's necessary to open a new issue to track this problem.\r\nIf this error still exists in tf2.3, I'll back to the community for help.\r\nthx."]}, {"number": 31997, "title": "Limit the number of statements in exception raising test blocks to 1", "body": "We may locate the exact statement where the exception is raiased.", "comments": ["@autoih Can you please resolve conflicts? Thanks!", "Thanks, @gbaned, can you help me point out the conflicts?", "@autoih please do \"git fetch\", \"git merge\", resolve conflicts, and \"git push\" , please follow this instructions [here](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/resolving-a-merge-conflict-using-the-command-line)", "Thanks @gbaned, sometimes I can't see the results, but you can. I think the conflicts has been fixed now. "]}, {"number": 31996, "title": "import tensorflow as tf issue ", "body": "I tried many times, but still the same issue. \r\n\r\n```\r\nImportError                               Traceback (most recent call last)\r\n~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\n~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\n~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\n~\\Miniconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\n~\\Miniconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~\\Miniconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     26 \r\n     27 # pylint: disable=g-bad-import-order\r\n---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     29 from tensorflow.python.tools import module_util as _module_util\r\n     30 \r\n\r\n~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\n~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Khalil\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Khalil\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Khalil\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Khalil\\Miniconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Khalil\\Miniconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\u200b", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n\n* For TF-GPU - See point 1\n* For TF-CPU - See point 2\n\n-----------------------------------------------------------------------------------------------\n\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\n*TF Version >= 1.13 requires CUDA 10.0 and TF Version < 1.13 (till TF 1.5) requires CUDA 9.0.*\n\n* If you have above configuration and using _**Windows**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n  * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n* If you have above configuration and using _**Ubuntu/Linux**_ platform -\n  * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n  * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n* If error still persists then, apparently your CPU model does not support AVX instruction sets.\n  * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\n Therefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n* Try Google Colab to use TensorFlow.\n  * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true).You get pre-installed latest stable TF version. Also you can use```pip install``` to install any other preferred TF version.\n  * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n  * All you need is a good internet connection and you are all set.\n* Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*", "but before this issue, I was using TF version 1.13.1 on the same PC. Just I have installed my window and I'm facing this issue. \r\n", "Possibly you can check this... \r\n\r\nhttps://stackoverflow.com/questions/49932993/importerror-dll-load-failed-a-dynamic-link-library-dll-initialization-routin\r\n\r\nBest of luck, Umberto", "> Possibly you can check this...\r\n> \r\n> https://stackoverflow.com/questions/49932993/importerror-dll-load-failed-a-dynamic-link-library-dll-initialization-routin\r\n> \r\n> Best of luck, Umberto\r\nlet me check, thank you!", "You are welcome. I think you could try to install a previous version that does not have pre-built AVX instructions...", "> You are welcome. I think you could try to install a previous version that does not have pre-built AVX instructions...\r\n\r\nI think I can use version 1.5, buts its older version so what do say should I install?\r\n ", "and please let me know how do we check AVX instruction sets? will be appreciated! ", "> > You are welcome. I think you could try to install a previous version that does not have pre-built AVX instructions...\r\n> \r\n> I think I can use version 1.5, buts its older version so what do say should I install?\r\n\r\nWell that depends on what features you want to use... Unless you want to work with specific features (for example from TF2.0) you can safely install an older version. For example I have several projects that runs on the version 1.13...\r\n\r\nBest of luck, Umberto\r\n\r\n", "> and please let me know how do we check AVX instruction sets? will be appreciated!\r\n\r\nWell that depends what you mean. If you mean how to check if your CPU support specific instructions you could check coreinfo (you are on windows10 right?). Check this\r\n\r\nhttps://superuser.com/questions/1251865/is-there-a-way-to-tell-if-my-hardware-supports-specific-instructions\r\n\r\nBest of luck, Umberto", "> > and please let me know how do we check AVX instruction sets? will be appreciated!\r\n> \r\n> Well that depends what you mean. If you mean how to check if your CPU support specific instructions you could check coreinfo (you are on windows10 right?). Check this\r\n> \r\n> https://superuser.com/questions/1251865/is-there-a-way-to-tell-if-my-hardware-supports-specific-instructions\r\n> \r\n> Best of luck, Umberto\r\n\r\nYes, I have window 10\r\n", "> > and please let me know how do we check AVX instruction sets? will be appreciated!\r\n> \r\n> Well that depends what you mean. If you mean how to check if your CPU support specific instructions you could check coreinfo (you are on windows10 right?). Check this\r\n> \r\n> https://superuser.com/questions/1251865/is-there-a-way-to-tell-if-my-hardware-supports-specific-instructions\r\n> \r\n> Best of luck, Umberto\r\n\r\nthank you so much for your kind help. ", "@khaliltalib277, Did you check your CPU supports AVX instruction sets or not. ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31996\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31996\">No</a>\n"]}, {"number": 31995, "title": "NotImplementedError: tf.GradientTape.gradients() does not support graph control flow operations like tf.cond or tf.while at this time. Use tf.gradients() instead.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@Bakibiiiillah ,\r\nCan you please fill information asked in the template.Thanks!", "Actually @oanush I've sent the source code before, My objective: I want to implement fast gradient sign method attack on diabetic retinopathy dataset on kaggle using tensorflow. But I've faced some errror. So can anyone help me to fullfill my task? TIA", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [StackOverflow](http://stackoverflow.com/questions/tagged//tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!"]}, {"number": 31994, "title": "Autotune seems not improve performance", "body": "How autotune is implemented? it seems not improve performance, the dataset is still the bottleneck.", "comments": ["@chengmengli06 ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "I use tf 1.12, installed from binary, redhat system with 500G memory, 96 core CPU. The tfrecord is stored in a network file system(OSS), the pipeline looks that this:\r\nread from oss\r\nparse by TFRecord\r\nprefetch\r\nshuffle\r\ndecoding\r\nprefetch\r\nbatch\r\nThe last batch step is autotuned, while the previous steps are not autotuned.", "@chengmengli06 ,\r\nThank you for the information, In order to expedite the trouble-shooting process, please provide code snippet to reproduce the issue reported here.", "I cannot provide the code, my code access oss, which runs on PAI, and could not run locally.", "Without a reproducible example, there is not much we can do to help.\r\n\r\nAutotuning will provide performance that should be close the performance of hand-tuned input pipeline. If you have already hand-tuned your input pipeline, autotuning is not expected to improve your performance further. The main benefit of autotuning is that it avoids the need for users to hand-tuned their input pipelines.", "Closing this issue as it is not reproducible. Please add additional comments so that we can open this issue again. Thanks!"]}, {"number": 31993, "title": "Register flops for BatchMatMulV2", "body": "fix #22071", "comments": []}, {"number": 31992, "title": "[XLA] Change document path of Tile", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31992) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot  I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31992) for more info**.\n\n<!-- ok -->"]}, {"number": 31991, "title": "Iterate on Unknown Batch Size with Custom Layer", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **OS X 10.14.6**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**\r\n- TensorFlow installed from (source or binary): **Binary**\r\n- TensorFlow version (use command below): **1.14.0**\r\n- Python version: **3.6.6**\r\n- Bazel version (if compiling from source): **N/A**\r\n- GCC/Compiler version (if compiling from source): **N/A**\r\n- CUDA/cuDNN version: **N/A**\r\n- GPU model and memory: **N/A**\r\n\r\n**Describe the current behavior**\r\nI am attempting to build a custom TensorFlow Layer to perform K-Means clustering across channels of a given image. I am having difficulty creating this new layer to add to the model, as it seems that fundamentally, I don't have the ability to iterate over the batch size, which is unknown until runtime. I have tried a few alternatives such as the `@tf.function` function decorator and the `tf.scan` function, which have both been unsuccessful. \r\n\r\n**Describe the expected behavior**\r\nI was expecting that since the batch size is unknown until runtime, that TensorFlow would be able to handle this error, similar to how TensorFlow can accept an unknown dimension and generate a matrix/tensor with the unknown shape.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom sklearn.cluster import KMeans\r\n\r\n\r\nclass KMeansLayer(tf.keras.layers.Layer):\r\n    def __init__(self, num_clusters=8, n_init=5, trainable=False):\r\n        super(KMeansLayer, self).__init__()\r\n        self.clusters = num_clusters\r\n        self.n_init = n_init\r\n        self.trainable = trainable\r\n\r\n    def build(self, input_shape):\r\n        print('Input shape:', input_shape)\r\n        self.output_s = (input_shape[0], input_shape[1], input_shape[2], 1)\r\n        self.built = True\r\n\r\n    def call(self, input):\r\n\r\n        @tf.function\r\n        def KMeansBase(input_mat, clusters, n_init):\r\n            base_mat = tf.zeros((input_mat.shape[0], input_mat.shape[1] * input_mat.shape[2]))\r\n            for frame in range(input_mat.shape[0]):\r\n                init_mat = np.zeros((input_mat.shape[1] * input_mat.shape[2]))\r\n                reshape_mat = tf.reshape(input_mat[frame], shape=(input_mat.shape[1] * input_mat.shape[2], input_mat.shape[3]))\r\n                kmeans_init = KMeans(n_clusters=clusters, n_init=n_init)\r\n                class_pred = kmeans_init.fit_predict(reshape_mat.numpy())\r\n\r\n                for clust in range(clusters):\r\n                    init_mat[class_pred == clust] = tf.keras.backend.mean(tf.boolean_mask(reshape_mat, class_pred == clust), axis=1).numpy()\r\n                    init_mat[class_pred == clust] = np.mean(init_mat[class_pred == clust], axis=None)\r\n                base_mat = tf.compat.v1.scatter_update(base_mat, frame, tf.convert_to_tensor(init_mat))\r\n            base_mat = tf.reshape(base_mat, (input_mat.shape[0], input_mat.shape[1], input_mat.shape[2]))\r\n\r\n            return tf.expand_dims(base_mat, axis=-1)\r\n\r\n        return KMeansBase(input, clusters=self.clusters, n_init=self.n_init)\r\n\r\n\r\n\r\ninput_1 = tf.keras.Input(shape=(28, 28, 1), name='input_1', dtype='float32')\r\nconv_1 = tf.keras.layers.Conv2D(filters=3, kernel_size=3, strides=1, padding='same', data_format='channels_last', activation='elu', kernel_initializer='glorot_uniform')(input_1)\r\nkmeans_out = KMeansLayer(num_clusters=8, n_init=5)(conv_1)\r\n\r\n\r\nmodel = tf.keras.Model(inputs=[input_1], outputs=kmeans_out)\r\ntf.keras.utils.plot_model(model, show_shapes=True)\r\nmodel.compile(optimizer='adam', loss='mse', metrics=['mse'])\r\n```\r\n\r\n\r\n\r\nThe error that I get from running the above code is as follows:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"example_error_file.py\", line 44, in <module>\r\n    kmeans_out = KMeansLayer(num_clusters=8, n_init=5)(conv_1)\r\n  File \"~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 634, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\", line 149, in wrapper\r\n    raise e.ag_error_metadata.to_exception(type(e))\r\nTypeError: in converted code:\r\n\r\n    example_error_file.py:38 call *\r\n        return KMeansBase(input, clusters=self.clusters, n_init=self.n_init)\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:414 __call__\r\n        self._initialize(args, kwds, add_initializers_to=initializer_map)\r\n    /var/folders/6p/0r05_kf55273nh_nftm5q9tw0000gn/T/tmp0zjq0l_w.py:14 KMeansBase *\r\n        base_mat = ag__.converted_call('zeros', tf, ag__.ConversionOptions(recursive=True, force_conversion=False, optional_features=(), internal_convert_user_code=True), ((input_mat.shape[0], input_mat.shape[1] * input_mat.shape[2]),), None)\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1880 zeros\r\n        shape = ops.convert_to_tensor(shape, dtype=dtypes.int32)\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1087 convert_to_tensor\r\n        return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1145 convert_to_tensor_v2\r\n        as_ref=False)\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1224 internal_convert_to_tensor\r\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:305 _constant_tensor_conversion_function\r\n        return constant(v, dtype=dtype, name=name)\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:246 constant\r\n        allow_broadcast=True)\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:284 _constant_impl\r\n        allow_broadcast=allow_broadcast))\r\n    ~/fluoro/fenv/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py:467 make_tensor_proto\r\n        nparray = np.array(values, dtype=np_dt)\r\n\r\n    TypeError: __int__ returned non-int (type NoneType)\r\n```\r\n\r\n\r\nMy main question is: can TensorFlow not handle iterating over an unknown batch size, or am I missing some functionality?\r\n\r\nThank you for the help!", "comments": ["Hi @gowthamkpr , any advice? Thank you for your help!", "Hi, I tested in 2.0-rc0 (with Eager disabled) instead of 1.14 but intuitively the results should be similar.\r\n\r\nIn TF2, an issue would come from using `input_mat.shape[0]`, which will return None, when you probably should use `tf.shape(input_map)[0]`, which will return a dynamic scalar tensor pointing to inputs' actual batch size. But this might not be the case in 1.14, and at any rate, to answer your question, _in general, it is possible to handle dynamic batch_size_, typically with a tf.while_loop (in your case, AutoGraph is generating one based on your code).\r\n\r\nThat being said, I think in your case the core failure reason is that you are attempting to feed a (symbolic) tensor to a scikit-learn object suited to use numpy arrays. This would perhaps work with Eager enabled on EagerTensors (which will implicitly be transformed back-and-forth into numpy arrays), but not (I think) on symbolic ones.\r\n\r\nIn my humble opinion, you probably should implement (or find) a tensorflow version of the KMeans algorithm and use it (or, possibly, find an alternative clustering layer that is easier to write and might have some learnable weights). At any rate, I hope that this helps a bit, and that you will find a suiting solution soon!", "@jmd-0 Did @pandrey-fr answer solve your problem. Can I close the issue?", "Hi @pandrey-fr, thank you for your response! I tried to experiment with the different TF functions you suggested and was unable to do exactly what I wanted, so it seems that I should just try to use the TF implementation of the KMeans algorithm, as you suggested.\r\n\r\nAlso, thank you for the interesting tidbit about how TF generates the symbolic tensors for unknown dataset sizes. I need to be aware of that in the future.\r\n\r\nThanks again, @pandrey-fr ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31991\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31991\">No</a>\n", "You are very welcome ; good luck with the follow-up work :-)", "I'm facing same problem. Any one solve this kind of problem ?", "Any updates on this? I need a custom layer that does a for loop.", "@Andreasksalk \r\n\r\nLayer does not support iteration. you have to perform operation on entire batch", "So it is a completely no go to use packages that is not keras? I have to perform optimal transport on a set of tensors in a keras model. i have found a way to do this on a single input but cannot seem to find a way to implement this into the keras model.", "@Andreasksalk It depends on what you want to perform, and more specifically when you want it to happen. The overall issue is very simple: in order to train your model through backpropagation, tensorflow needs to be able to track what happens to the data, and hence to compute the gradients of the loss function relative to the model's trainable weights. But if you want to operate on your input data (before any trainable weights are being used), you should be able to do it - probably using a Lambda layer, and setting it to run eagerly.", "I am trying to implement a custom Keras layer which does random `shear`.\r\nI am trying to use `tf.keras.preprocessing.image.random_shear` which implements `random_shear` per image.\r\nSo, I have to iterate over the tensor and call this method for each input.\r\nHowever, the input is of the shape `(None, 32,32,3)`, thus I can't know the number of rows. I tried to use `tf.shape(inputs)[0]` but it did not help.\r\n\r\nAny other way can we do this?", "Hello @ashwanikumar04 ,\r\nI think you are looking for [tf.map_fn](https://www.tensorflow.org/api_docs/python/tf/map_fn)\r\n(`sheared = tf.map_fn(tf.keras.preprocessing.image.random_shear, inputs)`)"]}, {"number": 31990, "title": "Use arg 'axis' when calling 'tensorflow.python.ops.nn_impl.l2_normalize'", "body": "... instead of the deprecated arg `dim`.\r\n\r\nWhenever I use k-means, I get this warning:\r\n```\r\nW0826 17:01:44.808238 27644 deprecation.py:506] From C:\\Users\\rpherbig\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\contrib\\factorization\\python\\ops\\clustering_ops.py:740: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\ndim is deprecated, use axis instead\r\n```\r\n\r\nDue to the deprecation of the `dim` arg - see https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L592", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31990) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31990) for more info**.\n\n<!-- ok -->", "closing this PR as contrib folder will be depricated in 2.0, thank you.\r\nCC @mihaimaruseac", "@rthadur What about the changes to `tensorflow/python/ops/clustering_ops.py`? Will that also be affected by the deprecation of the `contrib` folder?", "Can you please open a new PR for the changes in `tensorflow/python/ops/clustering_ops.py`, please? Thank you.\r\n\r\nEdit: I see we typed the same at the same time. Please assign me to the new PR."]}, {"number": 31989, "title": "[XLA] Expose running DCE on a computation.", "body": "This is a useful utility when running optimisations passes which modify calls/loops etc and we don't need to run DCE on the whole module.", "comments": ["@sanjoy Please dispatch, I'm afraid I won't get to review this anytime soon.", "Changes merged internally , waiting for auto-merge to happen."]}, {"number": 31988, "title": "[XLA] BufferComparator: add int8 support", "body": "As part of int8 convolution using CuDNN work, this is a necessary enhancement.  Previously this change is reviewed under https://github.com/tensorflow/tensorflow/pull/30783.", "comments": ["@cheshire, please review and mover this PR quickly, as https://github.com/tensorflow/tensorflow/pull/30771 depends on it.", "Just added the missing kernel and tests.", "Could we change it even more to prevent the possibility of such collisions in the future?\nEg __xla_buffer_comparator_canonicalize?\n\nSent from my iPhone\n\n> On Aug 27, 2019, at 13:36, yongfeng-nv <notifications@github.com> wrote:\n> \n> @yongfeng-nv commented on this pull request.\n> \n> In tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:\n> \n> > @@ -46,12 +46,20 @@ static constexpr double kTolerance = 0.1f;\n>  //\n>  // #include<cuda_fp16.h>\n>  // extern \"C\" { // avoid name mangling\n> -// __device__ float canonicalize(float input) {\n> +// __device__ float canonicalize_fp16(float input) {\n> Change this function name, because I got a name collision error, when I compile this code to PTX.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n", "> Could we change it even more to prevent the possibility of such collisions in the future? Eg __xla_buffer_comparator_canonicalize?\r\n\r\nFunction names are updated.", "> Please change tabs to spaces\r\n\r\nTabs are replaced with spaces in the last commit.", "@yongfeng-nv Sorry I had to revert this. The PTX version got bumped in the commit, is it possible to re-generate PTX with the version which was originally specified?", "@cheshire Sure.  I will be back to work on this next week."]}, {"number": 31987, "title": "[TF 2.0.0rc0] Installation on Travis CI", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Xenial 16.04 (xenial distribution on Travis CI)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `pip`\r\n- TensorFlow version: `2.0.0rc0`\r\n- Python version: 3.6, 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: No\r\n\r\nTox on Travis CI does not seem to be able to fetch Tensorflow 2.0.0rc0 using pip. Corresponding build is here : https://travis-ci.org/sicara/tf-explain/jobs/576955077\r\nEverything is running fine locally. Any idea what is wrong with the build?", "comments": ["Hi. Can you add to the build script a `python -m pip install --upgrade pip` step? Or ensure that Travis runs with a fresh version of pip?", "Sorry, just saw this is the case already, sorry for the above spam", "I see you are missing an hyphen in your installation. ```pip install tensorflow==2.0.0rc0``` \r\n Perhaps you can try;\r\n```pip install tensorflow==2.0.0-rc0 ```", "@ymodak I'm not sure there is an hyphen (it's working without it locally with `pip`). Tried it, and execution still fails : [Related Travis Build](https://travis-ci.org/sicara/tf-explain/jobs/578062183)", "So there is no hyphen.\r\n```\r\npip install tensorflow==2.0.0rc0\r\n```\r\nThe above command should work. \r\n\r\nPerhaps you're not updating the pip version that you are actually using? Can you comment back with the pip version after the upgrade?\r\n\r\nAre you using a virtualenv? I tried using Python3.6 in a virtual environment and with both the latest pip and 19.0.3 (the one from your logs) and it seems to install correctly. The only other notable difference between our setups is I'm on Python3.6.8 which shouldn't matter.", "Also your [logs](https://travis-ci.org/sicara/tf-explain/jobs/578062183) expanded out show an error. Line 205 runs (pip install tensorflow==2.0.0rc0) but line 246 says ```ERROR: tb-nightly 1.15.0a20190806 has requirement setuptools>=41.0.0, but you'll have setuptools 40.8.0 which is incompatible.```\r\n\r\nAlso line 319 seems to add the hyphen.\r\n```py36 installdeps: pip, pytest, pytest-cov, pytest-mock, tensorflow==2.0.0-rc0```", "In the end, this seems to be related to `tox` more than tensorflow. I have moved the `tensorflow==2.0.0rc0` requirements from `deps` to `commands` in the `tox.ini` file and everything is running fine. Sorry for the bother and thanks for your time @av8ramit @ymodak @mihaimaruseac ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31987\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31987\">No</a>\n", "Hey @RaphaelMeudec not a bother at all! It happens all the time! This is a good data point for anyone else who may run into this issue."]}, {"number": 31986, "title": "Tensorflow Golang Gpu Support", "body": "Seems like golang port of tensorflow does not use my Gpu, however, strangely, they recognize my Gpu.\n\nThis is only message I've got.\n```\nStreamExecutor device(0): undefibed, undefined \n```\n\n I've installed all cuda and cudnn libraries and it works just fine in Python. What would be the problem?", "comments": ["@SeungheonOh Run this code in python/Jupyter. It will give you a list of devices. If you don't see GPU, then Tensorflow doesn't even see GPU. Usually, Tensorflow uses available GPU by default.\r\n\r\n```\r\nfrom tensorflow.python.client import device_lib\r\n\r\nprint(device_lib.list_local_devices())\r\n```\r\nAlso you can look at this [issue](https://www.reddit.com/r/learnmachinelearning/comments/8qzc5d/tensorflowgpu_doesnt_seem_to_use_my_gpu/) and let me know if it helps. Thanks!", "I have already :). Tensorflow does recognize gpu, but it doesn't use it for defualt\r\n\r\n\r\n```\r\n2019-08-26 18:57:34.068948: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-08-26 18:57:34.106055: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3400680000 Hz\r\n2019-08-26 18:57:34.106902: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562a8ad150d0 executing computations on platform Host. Devices:\r\n2019-08-26 18:57:34.106925: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-26 18:57:34.115748: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-08-26 18:57:34.162399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-26 18:57:34.162832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:\r\nname: GeForce GTX 1070 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.683\r\npciBusID: 0000:1f:00.0\r\n2019-08-26 18:57:34.162910: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory\r\n2019-08-26 18:57:34.162961: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory\r\n2019-08-26 18:57:34.163009: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory\r\n2019-08-26 18:57:34.163058: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory\r\n2019-08-26 18:57:34.163106: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory\r\n2019-08-26 18:57:34.163155: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory\r\n2019-08-26 18:57:34.214546: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-26 18:57:34.214567: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\r\n2019-08-26 18:57:34.270965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-26 18:57:34.270987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0\r\n2019-08-26 18:57:34.270994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N\r\n2019-08-26 18:57:34.272681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-26 18:57:34.273218: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562a8d04c470 executing computations on platform CUDA. Devices:\r\n2019-08-26 18:57:34.273228: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070 Ti, Compute Capability 6.1\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 6757113606190317736\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 14843729277319555468\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 10107529750069193992\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n]\r\n```\r\n \r\n", "also, if I execute this: \r\n```\r\nsession, err := tensorflow.NewSession(modelGraph, &tensorflow.SessionOptions{})\r\n\r\n\t\tdevices, err := session.ListDevices()\r\n\r\n\t\tfor _, dev := range devices {\r\n\t\t\tfmt.Println(dev.String())\r\n\t\t}\r\n```\r\nit give GPU list correctly\r\n\r\n```\r\n(Device: name \"/job:localhost/replica:0/task:0/device:CPU:0\", type CPU, memory limit 268435456 bytes)\r\n(Device: name \"/job:localhost/replica:0/task:0/device:XLA_CPU:0\", type XLA_CPU, memory limit 17179869184 bytes)\r\n(Device: name \"/job:localhost/replica:0/task:0/device:GPU:0\", type GPU, memory limit 7454916608 bytes)\r\n```", "Oof dumb mistake, I thought gpu was not in use because it was so slow, but in fact it was slow because session was loading in every loop lol sorry ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31986\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31986\">No</a>\n"]}, {"number": 31985, "title": "Mixed precision training with Keras in r1.13", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)\r\n- TensorFlow version (use command below): 1.13.0\r\n- Python version: 3.6.8\r\n\r\nThere seems to be no obvious way to have mixed precision training using keras layers\r\nin r1.13. \r\n\r\n1. The approach based on setting a custom getter doesn't work, e.g.,\r\n\r\n`with tf.variable_scope(custom_getter=custom_getter):\r\n    layer = tf.keras.layers.Dense(...)`\r\n\r\ndoes not invoke the `custom_getter`.\r\n\r\n2. The mixed precision policy is available only starting r1.14, i.e.,\r\n\r\n`self.policy = tf.keras.mixed_precision.experimental.Policy('infer_float32_vars')` \r\n\r\nresults in \r\n\r\n`AttributeError: module 'tensorflow.keras' has no attribute 'mixed_precision'`\r\n\r\nIs there a way to have mixed precision for keras layers in r1.13?\r\n\r\n\r\n\r\n", "comments": ["@eugene-cerebras Yes in TF 1.13(tf.keras) there is no 'mixed_precision' but it exists in 'tf.contrib'. I would recommend using TF 1.14 if you are looking for Mixed precision training with Keras. Thanks!", "@gowthamkpr thanks for the reply. I've checked TF 1.13 `tf.contrib` and it indeed contains mixed_precision, which provides a limited functionality for mixed precision training (loss scaling), but does not seem to have the Policy class. Will use 1.14.  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31985\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31985\">No</a>\n"]}, {"number": 31984, "title": "[TF 2.0.0rc0] Error when using cuDNN GRU", "body": "Upgrading from pip package `tensorflow-gpu==2.0.0-beta1` to `tensorflow-gpu=2.0.0-rc0` the cuDNN GRU op no longer seems to work:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nprint(tf.keras.layers.GRU(10)(tf.random.uniform((1,1,1))))\r\n```\r\nGives expected output for `beta1`:\r\n```\r\n2.0.0-beta1\r\ntf.Tensor(\r\n[[ 0.14719923 -0.10625355 -0.01931523 -0.01283051 -0.11508528 -0.05038062\r\n  -0.04895313 -0.03125525  0.12742375 -0.06588683]], shape=(1, 10), dtype=float32)\r\n```\r\nversus this for `rc0`\r\n```\r\n2.0.0-rc0\r\n---------------------------------------------------------------------------\r\nUnknownError                              Traceback (most recent call last)\r\n<ipython-input-1-5455006264d0> in <module>\r\n      3 print(tf.__version__)\r\n      4 \r\n----> 5 print(tf.keras.layers.GRU(10)(tf.random.uniform((1,1,1))))\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)\r\n    621 \r\n    622     if initial_state is None and constants is None:\r\n--> 623       return super(RNN, self).__call__(inputs, **kwargs)\r\n    624 \r\n    625     # If any of `initial_state` or `constants` are specified and are Keras\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    849           with base_layer_utils.autocast_context_manager(\r\n    850               self._compute_dtype):\r\n--> 851             outputs = self.call(cast_inputs, *args, **kwargs)\r\n    852           self._handle_activity_regularization(inputs, outputs)\r\n    853           self._set_mask_metadata(inputs, outputs, input_masks)\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)\r\n    342     else:\r\n    343       last_output, outputs, runtime, states = self._defun_gru_call(\r\n--> 344           inputs, initial_state, training, mask)\r\n    345 \r\n    346     if self.stateful:\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py in _defun_gru_call(self, inputs, initial_state, training, mask)\r\n    396       # Under eager context, check the device placement and prefer the\r\n    397       if can_use_gpu:\r\n--> 398         last_output, outputs, new_h, runtime = cudnn_gru(**cudnn_gru_kwargs)\r\n    399       else:\r\n    400         last_output, outputs, new_h, runtime = standard_gru(**normal_gru_kwargs)\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py in cudnn_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards)\r\n    536     outputs, h, _, _ = gen_cudnn_rnn_ops.cudnn_rnn(\r\n    537         inputs, input_h=init_h, input_c=0, params=params, is_training=True,\r\n--> 538         rnn_mode='gru')\r\n    539 \r\n    540   last_output = outputs[-1]\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)\r\n    107             input_mode=input_mode, direction=direction, dropout=dropout,\r\n    108             seed=seed, seed2=seed2, is_training=is_training, name=name,\r\n--> 109             ctx=_ctx)\r\n    110       except _core._SymbolicException:\r\n    111         pass  # Add nodes to the TensorFlow graph.\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py in cudnn_rnn_eager_fallback(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)\r\n    196   \"is_training\", is_training)\r\n    197   _result = _execute.execute(b\"CudnnRNN\", 4, inputs=_inputs_flat,\r\n--> 198                              attrs=_attrs, ctx=_ctx, name=name)\r\n    199   _execute.record_gradient(\r\n    200       \"CudnnRNN\", _inputs_flat, _attrs, _result, name)\r\n\r\n~/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     65     else:\r\n     66       message = e.message\r\n---> 67     six.raise_from(core._status_to_exception(e.code, message), None)\r\n     68   except TypeError as e:\r\n     69     keras_symbolic_tensors = [\r\n\r\n~/.local/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nUnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]\r\n```", "comments": ["I tried running the lines and didn't get the same error. Do you mind re-running with a clean python env? Thanks!", "I tried to run this as well after upgrading to 2.0.0rc0 and got the same error. I was able to correct this by downloading the newest version of CuDNN. ", "Thanks for looking into this!\r\n\r\nThe CUDNN available from apt specified in the tensorflow install guide ( https://www.tensorflow.org/install/gpu#ubuntu_1604_cuda_10 ) gave this error. Manually downloading a (newer) `.deb` file from nvidia seemed to solve the issue for me as well.\r\n\r\nNot sure if the issue is due to apt and a messy setup or if rc0 is not compatible with `7.4.1.5-1`.\r\n\r\nProblem solved for me anyhow.", "I can confirm this. Conv2D was not functioning for me on CuDNN v7.5.1.10 on Windows 10. I upgraded to CuDNN v7.6.3.30 and I can run my models again.", "https://github.com/tensorflow/docs/pull/961\r\n\r\nSorry about that, I am use to a different error and I also failed to update the docs.  :-(   Thank you for the confirmation.  Only the 18.04 instructions were correct and not totally as I would suggest cuDNN 7.6.2 or higher.  I see 7.6.3 looks to be out based on Path-A's comment.  and that is also fine.  \r\n\r\nFunny thing.  You can user a higher cuDNN than we compile with but your CUDA has to be the same version exactly. "]}, {"number": 31983, "title": "[TF 2.0.0rc0] dependency on functools32 prevents python3.7 installation", "body": "Somewhy poetry thinks, that `tensorflow-gpu = \"2.0.0-rc0\"` depends on functools32 on python3.7 environment, because of what `poetry install / update` fails.\r\n\r\nFound [this](https://github.com/tensorflow/tensorflow/issues/31767) issue, but it did not help because `tensorflow-gpu = \"2.0.0-beta1\"` works greate with poetry.\r\n", "comments": ["I also have trouble installing \"2.0.0-rc0\" because of functools32.\r\n\r\nAlso on my side installing \"2.0.0b1\" works.\r\n\r\nMy setup is quite a bit different though so I'm guessing that this issue covers a wide range of configs:\r\n\r\n```\r\npipenv run python3 --version\r\nPython 3.6.8\r\n\r\npipenv install --pre tensorflow==2.0.0-rc0\r\nInstalling tensorflow==2.0.0-rc0\u2026\r\nAdding tensorflow to Pipfile's [packages]\u2026\r\n\u2714 Installation Succeeded \r\nPipfile.lock (4b69c6) out of date, updating to (f871ce)\u2026\r\nLocking [dev-packages] dependencies\u2026\r\nLocking [packages] dependencies\u2026\r\n\u2718 Locking Failed! \r\nTraceback (most recent call last):\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/resolver.py\", line 126, in <module>\r\n    main()\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/resolver.py\", line 119, in main\r\n    parsed.requirements_dir, parsed.packages)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/resolver.py\", line 85, in _main\r\n    requirements_dir=requirements_dir,\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/resolver.py\", line 69, in resolve\r\n    req_dir=requirements_dir\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/utils.py\", line 726, in resolve_deps\r\n    req_dir=req_dir,\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/utils.py\", line 480, in actually_resolve_deps\r\n    resolved_tree = resolver.resolve()\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/utils.py\", line 385, in resolve\r\n    results = self.resolver.resolve(max_rounds=environments.PIPENV_MAX_ROUNDS)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/resolver.py\", line 102, in resolve\r\n    has_changed, best_matches = self._resolve_one_round()\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/resolver.py\", line 206, in _resolve_one_round\r\n    for dep in self._iter_dependencies(best_match):\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/resolver.py\", line 301, in _iter_dependencies\r\n    dependencies = self.repository.get_dependencies(ireq)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/repositories/pypi.py\", line 234, in get_dependencies\r\n    legacy_results = self.get_legacy_dependencies(ireq)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/repositories/pypi.py\", line 426, in get_legacy_dependencies\r\n    results, ireq = self.resolve_reqs(download_dir, ireq, wheel_cache)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/repositories/pypi.py\", line 297, in resolve_reqs\r\n    results = resolver._resolve_one(reqset, ireq)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/resolve.py\", line 260, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/resolve.py\", line 213, in _get_abstract_dist_for\r\n    self.require_hashes\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/operations/prepare.py\", line 294, in prepare_linked_requirement\r\n    abstract_dist.prep_for_dist(finder, self.build_isolation)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/operations/prepare.py\", line 127, in prep_for_dist\r\n    self.req.run_egg_info()\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/req/req_install.py\", line 474, in run_egg_info\r\n    command_desc='python setup.py egg_info')\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/utils/misc.py\", line 705, in call_subprocess\r\n    % (command_desc, proc.returncode, cwd))\r\npipenv.patched.notpip._internal.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/tmpjkc613_zbuild/functools32/\r\nFile \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/resolver.py\", line 126, in <module>\r\n    main()\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/resolver.py\", line 119, in main\r\n    parsed.requirements_dir, parsed.packages)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/resolver.py\", line 85, in _main\r\n    requirements_dir=requirements_dir,\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/resolver.py\", line 69, in resolve\r\n    req_dir=requirements_dir\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/utils.py\", line 726, in resolve_deps\r\n    req_dir=req_dir,\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/utils.py\", line 480, in actually_resolve_deps\r\n    resolved_tree = resolver.resolve()\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/utils.py\", line 385, in resolve\r\n    results = self.resolver.resolve(max_rounds=environments.PIPENV_MAX_ROUNDS)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/resolver.py\", line 102, in resolve\r\n    has_changed, best_matches = self._resolve_one_round()\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/resolver.py\", line 206, in _resolve_one_round\r\n    for dep in self._iter_dependencies(best_match):\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/resolver.py\", line 301, in _iter_dependencies\r\n    dependencies = self.repository.get_dependencies(ireq)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/repositories/pypi.py\", line 234, in get_dependencies\r\n    legacy_results = self.get_legacy_dependencies(ireq)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/repositories/pypi.py\", line 426, in get_legacy_dependencies\r\n    results, ireq = self.resolve_reqs(download_dir, ireq, wheel_cache)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/piptools/repositories/pypi.py\", line 297, in resolve_reqs\r\n    results = resolver._resolve_one(reqset, ireq)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/resolve.py\", line 260, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/resolve.py\", line 213, in _get_abstract_dist_for\r\n    self.require_hashes\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/operations/prepare.py\", line 294, in prepare_linked_requirement\r\n    abstract_dist.prep_for_dist(finder, self.build_isolation)\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/operations/prepare.py\", line 127, in prep_for_dist\r\n    self.req.run_egg_info()\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/req/req_install.py\", line 474, in run_egg_info\r\n    command_desc='python setup.py egg_info')\r\n  File \"/home/fgervais/.local/lib/python3.6/site-packages/pipenv/patched/notpip/_internal/utils/misc.py\", line 705, in call_subprocess\r\n    % (command_desc, proc.returncode, cwd))\r\npipenv.patched.notpip._internal.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1 in /tmp/tmpjkc613_zbuild/functools32/", "@PgLoLo ,\r\nCan you please provide details about what platform you are using (operating system, architecture).Thanks!", "I did more digging and I verified that \r\n`tensorflow==2.0.0-b1` is `manylinux1`\r\n`tensorflow==2.0.0-rc0` is `manylinux2010`\r\n\r\nSo I think the issue @PgLoLo [linked to](https://github.com/tensorflow/tensorflow/issues/31767#issuecomment-522786795) is the issue here.\r\n\r\n@PgLoLo I'm on `pipenv` and updating to `master` did bring `manylinux2010` support and made the installation of rc0 possible. I would suggest that you verify if poetry have support for `manylinux2010` on the version you are running.\r\n", "@PgLoLo ,\r\ncan you please refer @fgervais response to the issue ?Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31983\">No</a>\n", "Still having this issue, with `2.0.0-rc0` using `poetry` to install on a clean (no other deps) test project\r\n\r\n```\r\n(venv) PS C:\\tmp\\tftest> poetry add tensorflow-gpu=\"2.0.0-rc0\"\r\n\r\nUpdating dependencies\r\nResolving dependencies... (3.1s)\r\n\r\nWriting lock file\r\n\r\n\r\nPackage operations: 8 installs, 12 updates, 0 removals\r\n\r\n -> 1.17.2)g numpy (1.16.3\r\n -> 0.8.0)ng absl-py (0.7.1\r\n -> 1.23.0)g grpcio (1.20.1\r\n -> 2.10.0)g h5py (2.9.0\r\n -> 3.1.1)ng markdown (3.1\r\n -> 3.9.1)ng protobuf (3.7.1\r\n -> 0.15.6)g werkzeug (0.15.2\r\n  - Installing wheel (0.33.6)\r\n -> 0.8.0)ng astor (0.7.1\r\n  - Installing functools32 (3.2.3-2)\r\n\r\n[EnvCommandError]\r\nCommand ['C:\\\\tmp\\\\venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', '--no-deps', 'functools32==3.2.3-2'] errored with the following output:\r\nCollecting functools32==3.2.3-2\r\n  Using cached https://files.pythonhosted.org/packages/c5/60/6ac26ad05857c601308d8fb9e87fa36d0ebf889423f47c3502ef034365db/functools32-3.2.3-2.tar.gz\r\n    ERROR: Command errored out with exit status 1:\r\n     command: 'C:\\tmp\\venv\\Scripts\\python.exe' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\simon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-80qdzk9j\\\\functools32\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\simon\\\\AppData\\\\Local\\\\Temp\\\\pip-install-80qdzk9j\\\\\r\nfunctools32\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base pip-egg-info\r\n         cwd: C:\\Users\\simon\\AppData\\Local\\Temp\\pip-install-80qdzk9j\\functools32\\\r\n    Complete output (1 lines):\r\n    This backport is for Python 2.7 only.\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n\r\nadd [-D|--dev] [--git GIT] [--path PATH] [-E|--extras EXTRAS] [--optional] [--python PYTHON] [--platform PLATFORM] [--allow-prereleases] [--dry-run] [--] <name> (<name>)...\r\n```", "The problem is still presented in tensorflow2.0.0-rc1 and latest poetry release", "@oanush  can you please reopen this issue?", "Unfortunately we only support pip installation as of right now, which is why we made the move to manylinux2010. I'm not sure what I can do to help fix this issue, but I think once [this issue](https://github.com/sdispater/poetry/issues/1373) is resolved on poetry's side it will work.\r\n\r\nWe cannot provide manylinux1 whls as pypi will reject them and they are not even technically compliant with the packaging standard. Sorry for the inconvenience. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31983\">No</a>\n", "I commented on https://github.com/sdispater/poetry/issues/1330#issuecomment-533043208.\r\n\r\nWhat about a restriction for Python version like below?\r\nhttps://github.com/tensorflow/tensorflow/blob/c75bb66a99ad45e5a3c9fc4625c8abeb705520b5/tensorflow/tools/pip_package/setup.py#L56\r\n\r\nI don't test it yet, but I will."]}, {"number": 31982, "title": "Can't build 1.14.0 on Nvidia Xavier AGX", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, \r\n- Device : Nvidia Xavier AGX\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: n/a\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): 7.4.0\r\n- CUDA/cuDNN version: 10.0.326-1\r\n- GPU model and memory: GV10B, 16GB shared\r\n\r\nThe build process can't start out of the box. Problem is missing instructions for the toolchain.\r\nThe following two patches are needed to be able to build:\r\n```\r\n--- third_party/toolchains/cpus/arm/BUILD~      2019-06-19 00:48:23.000000000 +0200\r\n+++ third_party/toolchains/cpus/arm/BUILD       2019-08-26 16:49:33.597480129 +0200\r\n@@ -13,6 +13,7 @@\r\n         \"k8\": \":cc-compiler-local\",\r\n         \"piii\": \":cc-compiler-local\",\r\n         \"arm\": \":cc-compiler-local\",\r\n+        \"aarch64\": \":cc-compiler-local\",\r\n         \"s390x\": \":cc-compiler-local\",\r\n     },\r\n )\r\n```\r\nand\r\n```\r\n--- third_party/gpus/crosstool/BUILD.tpl~       2019-06-19 00:48:23.000000000 +0200\r\n+++ third_party/gpus/crosstool/BUILD.tpl        2019-08-26 16:50:25.987292816 +0200\r\n@@ -29,6 +29,7 @@\r\n         \"x64_windows|msvc-cl\": \":cc-compiler-windows\",\r\n         \"x64_windows\": \":cc-compiler-windows\",\r\n         \"arm\": \":cc-compiler-local\",\r\n+        \"aarch64\": \":cc-compiler-local\",\r\n         \"k8\": \":cc-compiler-local\",\r\n         \"piii\": \":cc-compiler-local\",\r\n         \"ppc\": \":cc-compiler-local\",\r\n```\r\nIt would be nice if these get incorporated into the mainline. Probably related to #22629", "comments": ["Hi. Do you want to make a pull request with those changes? Against `master` branch and then let's cherry-pick it on `r2.0` and `r1.15` branches", "@andreyhristov \r\n\r\nPlease,have a look on @mihaimaruseac's suggestion .Thanks!\r\n", "Hi,\r\nI am currently on vacation and don't have access to the hardware", "@andreyhristov \r\n\r\nany update on this issue?.Thanks!", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31982\">No</a>\n", "Seems has already been fixed in 1.15-rc2"]}, {"number": 31981, "title": "[XLA] Make sure HloComputation::Equals takes dangling side-effects into account.", "body": "Current implementation does not take dangling side-effect instructions such as outfeeds or rngs into account.\r\nFor example:\r\n```\r\nc1 (arg0.1: f32[], arg1.2: f32[2], arg2.3: f32[1,1,2,2]) -> () {\r\n  %arg0.1 = f32[] parameter(0)\r\n  %arg2.3 = f32[1,1,2,2]{3,2,1,0} parameter(2)\r\n  %arg1.2 = f32[2]{0} parameter(1)\r\n  // Do some computation\r\n  %result = fp32[] ....\r\n  %aa = token[] after-all()\r\n  %feed = token[] outfeed(result.35, aa)\r\n  ROOT %tuple.53 = () tuple()\r\n}\r\n\r\nc2 (arg0.1: f32[], arg1.2: f32[2], arg2.3: f32[1,1,2,2]) -> () {\r\n  %arg0.1 = f32[] parameter(0)\r\n  %arg2.3 = f32[1,1,2,2]{3,2,1,0} parameter(2)\r\n  %arg1.2 = f32[2]{0} parameter(1)\r\n  ROOT %tuple.53 = () tuple()\r\n}\r\n```\r\nthen `*c1 == *c2` evaluates to true.", "comments": ["Note that we could simplify this by saying \"two computations can never be equal if they have side effects\". And it seems like most instructions with side-effect override identical to `false` but there are exceptions like the all-reduce in this test.", "@georgepaw Can you please resolve conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@georgepaw gentle ping to resolve conflicts. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 31980, "title": "Fix compile error in add.cc when building Tensorflow Lite Micro for Mbed OS", "body": "Since the add kernel in Tensorflow Lite Micro depends on string.h and string_util.h by including tensor.h, it fails to compile for Mbed OS. This patch removes that dependency from add.cc.", "comments": ["@aselle Do you have the time to look at this? Thanks!", "This issue have seemingly been fixed in another PR, so I'm closing this."]}, {"number": 31979, "title": "Remove leftover printf from fully connected in Tensorflow Lite Micro.", "body": "", "comments": []}, {"number": 31978, "title": " Fix issue: reduce_max on empty int32 array", "body": "Fix the issue https://github.com/tensorflow/tensorflow/issues/31325\r\nreduce_max on empty int32 array should throws an error message instead of  returns -2147483648", "comments": ["@kelvinguu @ravikyram @jvishnuvardhan\r\nFix the issue https://github.com/tensorflow/tensorflow/issues/31325.\r\nPlease have a review", "> Can you add a test?\r\n> \r\n> It's unclear to me that the smallest integer is the wrong answer here (I'd definitely support returning -inf for empty floating point arrays, and I am worried this breaks that)\r\n\r\nHi @alextp Thanks for the review.\r\nAre you talking about this snippet of code?\r\nhttps://github.com/tensorflow/tensorflow/blob/2c9cd7db6c9b17956518dd4c76e2858dba4f685c/tensorflow/core/kernels/reduction_ops_common.h#L189-L194\r\nWhich handling the situation when the input is empty arrays\r\n", "With this PR, it do will fail the test when the input is an empty array.\r\n\r\nI didn't quite understand why it returnings -inf instead of break the test. \r\nI have also tried numpy, when the input is empty it will also failed.\r\n```\r\n>>> import numpy as np\r\n>>> np.max([])\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/core/fromnumeric.py\", line 2505, in amax\r\n    initial=initial)\r\n  File \"/usr/lib64/python2.7/site-packages/numpy/core/fromnumeric.py\", line 86, in _wrapreduction\r\n    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\nValueError: zero-size array to reduction operation maximum which has no identity\r\n``` ", "So as-is this is a backwards-incompatible change to TF, so we can't do this. If we want to do this we should make a reduce_max_v2 (and a reduce_min_v2 probably since I doubt the bug is in only one of them) and deprecate the original one.", "> So as-is this is a backwards-incompatible change to TF, so we can't do this. If we want to do this we should make a reduce_max_v2 (and a reduce_min_v2 probably since I doubt the bug is in only one of them) and deprecate the original one.\r\n\r\nThanks @alextp Should I close this one and open a  new PR with reduce_max_v2 ?", "Reusing this or opening a new is up to you.\n\nOn Tue, Aug 27, 2019 at 4:42 PM Leslie-Fang <notifications@github.com>\nwrote:\n\n> So as-is this is a backwards-incompatible change to TF, so we can't do\n> this. If we want to do this we should make a reduce_max_v2 (and a\n> reduce_min_v2 probably since I doubt the bug is in only one of them) and\n> deprecate the original one.\n>\n> Thanks @alextp <https://github.com/alextp> Should I close this one and\n> open a new PR with reduce_max_v2 ?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/31978?email_source=notifications&email_token=AAABHRNO5JIQIJEBYOIOT7DQGW3VJA5CNFSM4IPP7KDKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5JNRZQ#issuecomment-525523174>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRJ3AXMMKSG5RRO7H7LQGW3VJANCNFSM4IPP7KDA>\n> .\n>\n\n\n-- \n - Alex\n", "But let me run this proposal by the tf api owners tomorrow before you spend\ntoo much time on it.\n\nOn Tue, Aug 27, 2019 at 4:53 PM Alexandre Passos <apassos@google.com> wrote:\n\n> Reusing this or opening a new is up to you.\n>\n> On Tue, Aug 27, 2019 at 4:42 PM Leslie-Fang <notifications@github.com>\n> wrote:\n>\n>> So as-is this is a backwards-incompatible change to TF, so we can't do\n>> this. If we want to do this we should make a reduce_max_v2 (and a\n>> reduce_min_v2 probably since I doubt the bug is in only one of them) and\n>> deprecate the original one.\n>>\n>> Thanks @alextp <https://github.com/alextp> Should I close this one and\n>> open a new PR with reduce_max_v2 ?\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/pull/31978?email_source=notifications&email_token=AAABHRNO5JIQIJEBYOIOT7DQGW3VJA5CNFSM4IPP7KDKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5JNRZQ#issuecomment-525523174>,\n>> or mute the thread\n>> <https://github.com/notifications/unsubscribe-auth/AAABHRJ3AXMMKSG5RRO7H7LQGW3VJANCNFSM4IPP7KDA>\n>> .\n>>\n>\n>\n> --\n>  - Alex\n>\n\n\n-- \n - Alex\n", "> But let me run this proposal by the tf api owners tomorrow before you spend too much time on it.\r\n> [\u2026](#)\r\n> On Tue, Aug 27, 2019 at 4:53 PM Alexandre Passos ***@***.***> wrote: Reusing this or opening a new is up to you. On Tue, Aug 27, 2019 at 4:42 PM Leslie-Fang ***@***.***> wrote: > So as-is this is a backwards-incompatible change to TF, so we can't do > this. If we want to do this we should make a reduce_max_v2 (and a > reduce_min_v2 probably since I doubt the bug is in only one of them) and > deprecate the original one. > > Thanks @alextp <https://github.com/alextp> Should I close this one and > open a new PR with reduce_max_v2 ? > > \u2014 > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <#31978?email_source=notifications&email_token=AAABHRNO5JIQIJEBYOIOT7DQGW3VJA5CNFSM4IPP7KDKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5JNRZQ#issuecomment-525523174>, > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AAABHRJ3AXMMKSG5RRO7H7LQGW3VJANCNFSM4IPP7KDA> > . > -- - Alex\r\n> -- - Alex\r\n\r\nThanks, Alex. Please let me know the results after your discussion.", "tf-api-owners discussed and we feel that reductions returning the identity element of the group they are reducing over for empty arrays is the only reasonable behavior here (so reduce_sum returns 0, reduce_prod returns 1, reduce_max returns -infty, etc).\r\n\r\nIt looks weird for the integer case because the identity of the group is weird, but that's acceptable."]}, {"number": 31977, "title": "what's the mean of \"? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\" in tflite", "body": "When I using tensorflow-lite in Android, after change the model officially supported, an error happened but I can't solve it because I didn't find some guide on Internet about tensorflow-lite with details.\r\n\r\nThe input of my model has two inputs, 32*128*128*1 respectively, and I mimic the way to feed the inputs but failed\r\n\r\nHere is some code in my project:\r\n```\r\n@Override\r\n  protected void processImage() {\r\n    try {\r\n      float[][][][] stft=readTxt(\"file:///android_asset/stft.txt\");\r\n      float[][][][] mfcc=readTxt(\"file:///android_asset/mfcc.txt\");\r\n      final List<Classifier.Recognition> results = detector.recognizeImage(stft, mfcc);\r\n\r\n    }\r\n    catch(Exception e)\r\n    {\r\n      e.printStackTrace();\r\n      System.out.print(\"can't get Bitmap\");\r\n    }\r\n\r\n\r\n  }\r\n\r\n  @Override\r\n  public List<Recognition> recognizeImage(final float[][][][] stft,final float[][][][] mfcc) {\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    LOGGER.i(\"begin run model\");\r\n    Trace.endSection(); // preprocessBitmap\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n    imgDatam.rewind();\r\n    imgDatas.rewind();\r\n    for(int i=0; i<32; i++){\r\n      for(int j=0;j<128;j++){\r\n        for(int k =0;k<128;k++){\r\n          imgDatam.putFloat(mfcc[i][j][k][0]);\r\n          imgDatas.putFloat(stft[i][j][k][0]);\r\n        }\r\n\r\n      }\r\n\r\n    }\r\n\r\n    outputClasses = new float[32][NUM_DETECTIONS];\r\n\r\n    Object[] inputArray = {imgDatas,imgDatam};\r\n\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n\r\n    outputMap.put(0, outputClasses);\r\n\r\n    Trace.endSection();\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"run\");\r\n    try{\r\n      tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n    }\r\n    catch(Exception e)\r\n    {\r\n      e.printStackTrace();\r\n    }\r\n    LOGGER.i(\"finished\");\r\n    Trace.endSection();\r\n\r\n    // Show the best detections.\r\n    // after scaling them back to the input size.\r\n    final ArrayList<Recognition> recognitions = new ArrayList<>(NUM_DETECTIONS);\r\n\r\n    Trace.endSection(); // \"recognizeImage\"\r\n    return recognitions;\r\n  }\r\n```\r\nAnd the error shows:\r\n```\r\n2019-08-27 15:53:41.636 13951-13951/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------\r\n```\r\nThanks for your help anyway", "comments": ["@mmmmayi ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Thanks!\r\n", "> @mmmmayi ,\r\n> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Thanks!\r\n\r\nI used Tensorflow nightly to get tflite model, run the android application on HUAWEI M6 tablet, android version 9 , and here is my tflite model if you need:\r\nhttps://drive.google.com/drive/folders/1Ds8ihFsz9K5ZKPCS0FPQ2_qZBUOn3to3?usp=sharing", "And I think it's because of the inputs, I have two inputs and one output for my model, but I didn't find such demo to referred. And the log shows like this:\r\n```\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG: backtrace:\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #00 pc 0000000000022988  /system/lib64/libc.so (abort+116)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #01 pc 00000000000258a4  /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/lib/arm64/libtensorflowlite_jni.so\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #02 pc 0000000000128b84  /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/lib/arm64/libtensorflowlite_jni.so\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #03 pc 000000000012b1a8  /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/lib/arm64/libtensorflowlite_jni.so\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #04 pc 0000000000126cd8  /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/lib/arm64/libtensorflowlite_jni.so\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #05 pc 00000000001624ac  /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/lib/arm64/libtensorflowlite_jni.so\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #06 pc 0000000000165974  /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/lib/arm64/libtensorflowlite_jni.so\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #07 pc 000000000000ed10  /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #08 pc 00000000005833e0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #09 pc 000000000057a64c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #10 pc 00000000000d7e18  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #11 pc 000000000028c628  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #12 pc 0000000000286630  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+968)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #13 pc 0000000000542ed0  /system/lib64/libart.so (MterpInvokeStatic+204)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #14 pc 000000000056cb94  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #15 pc 00000000001770ee  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/split_lib_dependencies_apk.apk (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.run+158)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #16 pc 0000000000260334  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2508404583+488)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #17 pc 0000000000265e28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #18 pc 0000000000286614  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #19 pc 00000000005419c8  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #20 pc 000000000056ca14  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #21 pc 00000000001768f6  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/split_lib_dependencies_apk.apk (deleted) (org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs+10)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #22 pc 0000000000260334  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2508404583+488)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #23 pc 000000000052ff74  /system/lib64/libart.so (artQuickToInterpreterBridge+1020)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #24 pc 00000000005834fc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #25 pc 00000000000086b8  /dev/ashmem/dalvik-jit-code-cache (deleted) (org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage+1672)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #26 pc 000000000057a6dc  /system/lib64/libart.so (art_quick_osr_stub+44)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #27 pc 0000000000314578  /system/lib64/libart.so (art::jit::Jit::MaybeDoOnStackReplacement(art::Thread*, art::ArtMethod*, unsigned int, int, art::JValue*)+1996)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #28 pc 0000000000547fdc  /system/lib64/libart.so (MterpMaybeDoOnStackReplacement+144)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #29 pc 00000000005713f0  /system/lib64/libart.so (ExecuteMterpImpl+33136)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #30 pc 0000000000002696  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/split_lib_slice_0_apk.apk (deleted) (org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage+234)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #31 pc 0000000000260334  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2508404583+488)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #32 pc 0000000000265e28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #33 pc 0000000000286614  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #34 pc 0000000000542784  /system/lib64/libart.so (MterpInvokeInterface+944)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #35 pc 000000000056cc14  /system/lib64/libart.so (ExecuteMterpImpl+14740)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #36 pc 000000000001cdfc  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/org.tensorflow.lite.examples.detection-6r3SsvrXFX7DXP3Hy-vqUQ==/split_lib_slice_9_apk.apk (deleted) (org.tensorflow.lite.examples.detection.DetectorActivity$2.run+120)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #37 pc 0000000000260334  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2508404583+488)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #38 pc 0000000000265e28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2019-08-27 18:31:34.436 21208-21208/? A/DEBUG:     #39 pc 0000000000286614  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #40 pc 0000000000542784  /system/lib64/libart.so (MterpInvokeInterface+944)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #41 pc 000000000056cc14  /system/lib64/libart.so (ExecuteMterpImpl+14740)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #42 pc 0000000000caa5f6  /system/framework/boot-framework.vdex (android.os.Handler.handleCallback+4)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #43 pc 0000000000260334  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2508404583+488)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #44 pc 0000000000265e28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #45 pc 0000000000286614  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #46 pc 0000000000542ed0  /system/lib64/libart.so (MterpInvokeStatic+204)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #47 pc 000000000056cb94  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #48 pc 0000000000b46588  /system/framework/boot-framework.vdex (android.os.Handler.dispatchMessage+8)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #49 pc 0000000000260334  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2508404583+488)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #50 pc 0000000000265e28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #51 pc 0000000000286614  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #52 pc 00000000005419c8  /system/lib64/libart.so (MterpInvokeVirtual+588)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #53 pc 000000000056ca14  /system/lib64/libart.so (ExecuteMterpImpl+14228)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #54 pc 0000000000b4e3ac  /system/framework/boot-framework.vdex (android.os.Looper.loop+498)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #55 pc 0000000000260334  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2508404583+488)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #56 pc 0000000000265e28  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #57 pc 0000000000286614  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #58 pc 0000000000542ed0  /system/lib64/libart.so (MterpInvokeStatic+204)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #59 pc 000000000056cb94  /system/lib64/libart.so (ExecuteMterpImpl+14612)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #60 pc 0000000000b46050  /system/framework/boot-framework.vdex (android.os.HandlerThread.run+56)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #61 pc 0000000000260334  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.2508404583+488)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #62 pc 000000000052ff74  /system/lib64/libart.so (artQuickToInterpreterBridge+1020)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #63 pc 00000000005834fc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #64 pc 000000000057a388  /system/lib64/libart.so (art_quick_invoke_stub+584)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #65 pc 00000000000d7df8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+200)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #66 pc 0000000000477a58  /system/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #67 pc 0000000000478b14  /system/lib64/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue*)+424)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #68 pc 00000000004a460c  /system/lib64/libart.so (art::Thread::CreateCallback(void*)+1120)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #69 pc 0000000000084028  /system/lib64/libc.so (__pthread_start(void*)+36)\r\n2019-08-27 18:31:34.437 21208-21208/? A/DEBUG:     #70 pc 00000000000241dc  /system/lib64/libc.so (__start_thread+68)\r\n```", "actually, I tried tflite model produced by official demo which has the same input size with mine, it works,  but after I changed my model, it doesn't. I know maybe something is wrong with my model, but i can't find where to correct. Here are my models and python code to generate tflite model:\r\nhttps://drive.google.com/drive/folders/1Ds8ihFsz9K5ZKPCS0FPQ2_qZBUOn3to3?usp=sharing ", "I have found the reason: I used tf.nn.moments in my python code, seems isn't supported", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31977\">No</a>\n", "where can i find what is supported by the tflite ?    @mmmmayi \r\n"]}]