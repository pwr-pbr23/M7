[{"number": 16293, "title": "tf.errors.OutOfRangeError error not raised when using tf.train.MonitoredTrainingSession", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI am providing a mini snippet to reproduce bug.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7 - 64 bit\r\n- **TensorFlow installed from (source or binary)**:\r\nFrom binary for windows (tensorflow_gpu)\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nNot Applicable\r\n- **GCC/Compiler version (if compiling from source)**:\r\nNot Applicable\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.5\r\n- **GPU model and memory**:\r\nTitan X\r\n- **Exact command to reproduce**:\r\nJust run the snippet as it is.\r\n\r\n### Describe the problem\r\n\r\nWhen Dataset iterator reaches at the end it should raise `tf.errors.OutOfRangeError`. But when using `tf.train.MonitoredTrainingSession` with custom hook I do not get the error. Ideally I need to get the Error.\r\n\r\n### Source code / logs\r\n\r\nReproducible bug snippet:\r\n\r\n```python\r\n# coding=utf-8\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.GIT_VERSION, tf.VERSION)\r\n\r\n\r\nclass MyHook(tf.train.SessionRunHook):\r\n\r\n    def __init__(\r\n            self,\r\n            place_holders,\r\n            batch_size, epochs\r\n    ):\r\n        super(MyHook, self).__init__()\r\n        self._create_iterator(place_holders, batch_size, epochs)\r\n        self._session = None\r\n        self._handle = None\r\n\r\n    def _create_iterator(self, place_holders, batch_size, epochs):\r\n\r\n        self._dataset = tf.data.Dataset.from_tensor_slices(\r\n            place_holders\r\n        ).batch(\r\n            batch_size\r\n        ).repeat(\r\n            epochs\r\n        )  # type: tf.data.Dataset\r\n\r\n        self._iterator = self._dataset.make_initializable_iterator()\r\n        self._next_op = self._iterator.get_next()\r\n\r\n    def reinit(self, feed_dict):\r\n        self._session.run(self._iterator.initializer, feed_dict=feed_dict)\r\n\r\n    @property\r\n    def next(self):\r\n        return self._session.run(self._next_op)\r\n\r\n    def after_create_session(self, session, coord):\r\n        self._session = session\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    data_dict = {\r\n        'labels': np.arange(20, dtype=np.int32)\r\n    }\r\n\r\n    placeholders_dict = {\r\n        'labels': tf.placeholder(\r\n            dtype=data_dict['labels'].dtype,\r\n            shape=data_dict['labels'].shape,\r\n            name='labels')\r\n    }\r\n\r\n    feed_dict = {}\r\n    for k, v in placeholders_dict.items():\r\n        feed_dict[v] = data_dict[k]\r\n\r\n    hook_twice = MyHook(place_holders=placeholders_dict, batch_size=10, epochs=2)\r\n    hook_infinite = MyHook(place_holders=placeholders_dict, batch_size=10, epochs=None)\r\n\r\n    with tf.train.MonitoredTrainingSession(\r\n        hooks=[hook_twice, hook_infinite]\r\n    ):\r\n        hook_twice.reinit(feed_dict=feed_dict)\r\n        hook_infinite.reinit(feed_dict=feed_dict)\r\n        while True:\r\n            print(\"...\")\r\n            print(hook_twice.next)\r\n            print(hook_infinite.next)\r\n\r\n```\r\nAs you can see `hook_twice` should repeat twice on dataset and then throw Error. Whereas `hook_infinite` should repeat infinitely but because of `hook_twice` should be abrupt-ed. But the program runs without throwing error after two iterations because of `hook_twice`.\r\n\r\nOutput:\r\n\r\n```txt\r\nb'unknown' 1.4.0\r\n2018-01-22 19:55:01.223557: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2018-01-22 19:55:01.519963: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties: \r\nname: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 12.00GiB freeMemory: 11.78GiB\r\n2018-01-22 19:55:01.519963: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2)\r\n...\r\n{'labels': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\r\n{'labels': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\r\n...\r\n{'labels': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}\r\n{'labels': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}\r\n...\r\n{'labels': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\r\n{'labels': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}\r\n...\r\n{'labels': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}\r\n{'labels': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}\r\n...\r\n\r\nProcess finished with exit code 0\r\n\r\n```\r\n", "comments": ["Check pseudo code in  `tensorflow/python/training/session_run_hook.py` file line 78.\r\nMay be the bug is because the 'OutOfRange``exception is not raised but instead is allowed to gracefully break.\r\n\r\n![image](https://user-images.githubusercontent.com/2289914/35240089-a3397d64-ffb2-11e7-9602-5d07306c35ac.png)\r\n\r\nWhen `OutOfRange` error is raised the loop breaks instead of raising exception.", "I need to write try-catch block to address this issue."]}, {"number": 16292, "title": "Branch 182783262", "body": "", "comments": []}, {"number": 16291, "title": "[bug] Specify GPU device error when using session_options.config.mutable_gpu_options()->set_visible_device_list", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes. \r\nsession_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\nsession->reset(tensorflow::NewSession(session_options)); // Error in this line\r\nStatus session_create_status = (*session)->Create(graph_def); \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.9.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\n5.4\r\n- **CUDA/cuDNN version**:\r\nCUDA 8.0  /  cuDNN 6.0.21\r\n- **GPU model and memory**:\r\nQuadro P6000, 24G GPU memory\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI built the tensorflow C++ API from the source using Bazel 0.9.0. When I link the shared library libtensorflow_cc.so, my code works fine. (I do not need to link libtensorflow_framework.so, since I used '--config=monolithic' when I build tensorflow using bazel.)\r\nHowever, I want to specify the GPU device in my code using this function to set gpu options:\r\nsession_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\nsession->reset(tensorflow::NewSession(session_options)); // **Error in this line**\r\n\r\n### Source code / logs\r\nLogs:\r\n2018-01-22 09:39:57.262843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1202] Found device 0 with properties: \r\nname: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 23.87GiB freeMemory: 22.46GiB\r\n2018-01-22 09:39:57.262897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1296] Adding visible gpu device 0\r\n2018-01-22 09:39:57.584754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21801 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-01-22 09:39:57.584802: E tensorflow/core/common_runtime/gpu/process_state.cc:130] Invalid allocator type: 0\r\nSegmentation fault (core dumped)\r\n\r\n\r\nSource Codes:\r\n// Reads a model graph definition from disk, and creates a session object you\r\n// can use to run it.\r\nStatus LoadGraph(const string& graph_file_name, std::unique_ptr<tensorflow::Session>* session) \r\n{\r\n    tensorflow::GraphDef graph_def;\r\n    Status load_graph_status = ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);\r\n    if (!load_graph_status.ok()) \r\n    {\r\n        return tensorflow::errors::NotFound(\"Failed to load compute graph at '\",\r\n                                        graph_file_name, \"'\");\r\n    }\r\n    //tensorflow::SessionOptions options;\r\n    tensorflow::SessionOptions session_options;\r\n    session_options.config.mutable_gpu_options()->visible_device_list();\r\n    std::cout<<\"list GPU done\"<<std::endl;\r\n    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\n    std::cout<<\"GPU assign is done\"<<std::endl;\r\n   \r\n    session->reset(tensorflow::NewSession(session_options));\r\n    std::cout<<\"new session is created. \"<<std::endl;\r\n    Status session_create_status = (*session)->Create(graph_def);\r\n    std::cout<<\"Graph is loaded. \"<<std::endl;\r\n    if (!session_create_status.ok()) \r\n    {\r\n        return session_create_status;\r\n    }\r\n    return Status::OK();\r\n}", "comments": ["@mattdingmeng Please provide a minimum standalone example that reproduces the problem.\r\n\r\nAlso please provide the full stack traces after the crash.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "@tensorflowbutler @tatatodd \r\n\r\nThe issue still persists. I think this is a bug.\r\n\r\nI tried to debug a little bit and I found that when I called set_visible_device_list(\"1\"), it also set \"allocator_type\" to \"1\", which is an invalid value for allocator_type. That's why the error message is \"Invalid allocator type: 0\". \r\n\r\nIn other words, allocator_type and visible_device_list seem to share the same memory location.\r\nhttps://github.com/tensorflow/tensorflow/blob/3378865c5509dfb6d18e6f95f28757437f67d3da/tensorflow/core/protobuf/config.proto\r\n\r\nPlease see the code snippet below as an example\r\n\r\n```\r\nauto options = tensorflow::SessionOptions();\r\nconst auto& static_gpu_options = options.config.gpu_options();\r\nauto mutable_gpu_options = options.config.mutable_gpu_options();\r\n\r\nstd::cout << static_gpu_options.visible_device_list() << std::endl;     // print \"\"\r\nstd::cout << static_gpu_options.allocator_type() << std::endl;          // print \"\"\r\n\r\nmutable_gpu_options->set_visible_device_list(\"1\");\r\n\r\nstd::cout << static_gpu_options.visible_device_list() << std::endl;     // print \"1\"\r\nstd::cout << static_gpu_options.allocator_type() << std::endl;          // print \"1\"\r\n\r\nmutable_gpu_options->set_allocator_type(\"\");\r\n\r\nstd::cout << static_gpu_options.visible_device_list() << std::endl;     // print \"\"\r\nstd::cout << static_gpu_options.allocator_type() << std::endl;          // print \"\"\r\n```", "@ChengshuLi Thanks for debugging!  I agree that the results of your experiment are troubling; something definitely seems wrong.\r\n\r\n@asimshankar can you suggest someone to investigate?  FYI my totally wild guess (having not investigated at all) is that cc_enable_arenas is broken in our open source builds in some subtle way.", "@tatatodd @ChengshuLi \r\nThe same issue! More information supplied. I find that this wired bug occurs only in string field and all string field in the project. In other words, all string fields share the same memory location. Also, this bug still exists in tensorflow-1.6. Any solutions? Thanks!\r\n```c++\r\n#include \"tensorflow/core/protobuf/control_flow.pb.h\"\r\n#include \"tensorflow/core/protobuf/config.pb.h\"\r\n#include <iostream>\r\n\r\nint main() {\r\n  tensorflow::CondContextDef options;\r\n  tensorflow::GPUOptions gpu_options;\r\n\r\n  gpu_options.set_allocator_type(\"allocator_type\");\r\n  options.set_context_name(\"context\");\r\n  options.set_pred_name(\"pred\");\r\n  options.set_pivot_name(\"pivot\");\r\n  options.set_branch(1);\r\n\r\n  std::cout << \"context name \" << options.context_name() << std::endl; //print pivot\r\n  std::cout << \"pred name \" << options.pred_name()<< std::endl; //print pivot\r\n  std::cout << \"pivot name \" << options.pivot_name()<< std::endl; //print pivot\r\n\r\n  std::cout << \"allocator_type \" << gpu_options.allocator_type() << std::endl; //print pivot\r\n\r\n}\r\n```", "+1 I also am experiencing this in 1.6 under Ubuntu 16.04.\r\nThis is a critical bug, as it makes all gpu options unusable - I can no longer select which GPU I am using.\r\n\r\nThis only started occurring with recent builds - code that used to work now errors out if I try to select a GPU.", "@tatatodd @Kejie-Wang @tensorflowbutler \r\n+1, same problem.", "+1, same problem.", "same problem,waiting for response.", "+1, same problem.", "@mattdingmeng @tatatodd @ChengshuLi @jpapon \r\nFinally, I manage to solve it by adding a line `*protobuf*` into `tensorflow/tf_version_script.Ids`. \r\n```\r\ntensorflow {\r\n  global:\r\n    *tensorflow*;\r\n    *perftools*gputools*;\r\n    *TF_*;\r\n    *TFE_*;\r\n    *nsync_*;\r\n    *protobuf*;\r\n  local:\r\n    *;\r\n};\r\n```\r\nI have tested it works in tensorflow-1.5 by using `bazel build -c opt --config=cuda --config=monolithic --copt=-march=native --cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\" tensorflow:libtensorflow_cc.so` to compile. The `tf_version_scripts.Ids` used to control the exported symbols to prevent conflict with OpenCV ([#1924](https://github.com/tensorflow/tensorflow/issues/1924)). I export the symbols about protobuf in the library but I am not sure whether this solution may cause other problems. Anyway, it works for me now.", "@Kejie-Wang : Thanks. Though, without the protobuf symbols being exported by `libtensorflow_cc.so`, how were you linking in the protobuf library to your test program? I wonder if there is some discrepancy between the version of protobuf used by TensorFlow to generate the header files and the version of protobuf you're linking in when compiling your test program.\r\n\r\nIf you could share the exact command-line used to build your test program, that would help. I'd expect it would be something like:\r\n\r\n```sh\r\nbazel build -c opt --config=monolithic //tensorflow:libtensorflow_cc.so\r\ng++ -I. -ltensorflow_cc -L ./bazel-bin/tensorflow  -I bazel-genfiles <additional flags> test.cc\r\n```\r\n\r\nWhere `test.cc` is the contents of the program you [alluded to earlier](https://github.com/tensorflow/tensorflow/issues/16291#issuecomment-373283271) - and I'm particularly curious about that those \"additional flags\" are and/or which version of the protobuf libraries you're linking against.", "@asimshankar \r\nThanks for your reply!\r\n\r\nYeah, there is no protobuf symbols in `libtensorflow_cc.so` and therefore I install the protobuf-3.4 (same as the protobuf used in tensorflow-1.5) and linked it into my test program. And I compile my test program by\r\n```shell\r\ng++ test.cc -I /usr/local/tensorflow/include -L /usr/local/tensorflow/lib -ltensorflow_cc -lprotobuf\r\n```\r\nwhere I have install the tensorflow in `/usr/local/tensorflow`.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "how to solve this problem on windows with cmake build ?", "@Rongnian \r\nmy solution:\r\ntensorflow1.8\r\nc++\r\n\r\nhttps://blog.csdn.net/cdknight_happy/article/details/81100554", "@asimshankar Can you reopen this issue? I am still facing the same issue with 1.9", "@ChengshuLi  \r\nI would like to ask you if you solved this issue. because i figured it out the same as you,  the allocator_type and visible_device_list  point to the same memory. \r\nif you could provide me with the solution to avoid that form happen. \r\nI am using Tensorflow 1.71, thanks in advance.\r\n", "Is there a solution for this now -- I am Ok even with using older versions of tensorflow as I need this feature badly. With \"monolithic\" build , I am unable to load graphs on specific GPUs using SetDefaultDevice(). This looks like a very serious limitation -- that was pointed out two years ago here https://github.com/tensorflow/tensorflow/issues/5379  -- and still not solved !?\r\n", "@Rongnian \r\nin windows, add follow lib file to additional dependencies of you project by follow order.\r\n**tf_protos_cc.lib; libprotobuf.lib; tensorflow.lib;**\r\n\r\n**please pay attention to their order.** ", "@dshawul Did you figure out how to assign graph to specific gpu?", "@Kejie-Wang did you solve this?", "@kingstarcraft I think the issue is using tensorflow_cc.lib from the bazel build. When you link against this, the SetDefaultDevice(... doesn't work as it does if you built using cmake in previous versions. Now that cmake build is no longer supported this is becoming somewhat of a shortcoming for the bazel build.", "@asimshankar Can we re-open this issue? Seems to be pretty common and present for about a year.", "@Kejie-Wang Have you tried this with a later version of tensorflow and cuda 10?", "@azaks2 @smit-hinsu - mind taking a look?\r\n(I'm no longer actively working on TensorFlow)", "@smit-hinsu with some workarounds i think this actually does work. Not with monolithic build though.", "@ttdd11 seems you have solved this issue, could you please tell me? many thanks.", "Please update?  I'm still seeing this issue.", "@smit-hinsu any update? Don't know why TensorFlow is so poorly maintained", "> \u8bf7\u66f4\u65b0\uff1f\u6211\u8fd8\u5728\u770b\u8fd9\u4e2a\u95ee\u9898\u3002\r\n\r\n+1", "+1 I\u2019m seeing this on version 1.15.0, on Windows / CUDA 10, as well as a host of other issues. I don\u2019t think tensorflow_cc.lib / .dll works at all \u2018out of the box\u2019. I\u2019ve yet to see a working example building against the shared lib.", "+1 same problem on 1.15.0", "+1 same problem on 1.13.1", "+1  same problem on 1.14.0", "+1 same problem v2.2.0"]}, {"number": 16290, "title": "Bug in session initialization?", "body": "Initializing a variable depending on a placeholder does not work:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = np.random.normal(loc=5, size=[10, 25])\r\n\r\nsample_data = tf.placeholder(tf.float32)\r\n\r\nmu = tf.Variable(tf.reduce_mean(sample_data, axis=0),\r\n                 dtype=tf.float32,\r\n                 validate_shape=False,\r\n                 name=\"mu\")\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer(), feed_dict={sample_data: x})\r\n    print(sess.run(mu))\r\n\r\n########################################### REPEAT:\r\n\r\nx = np.random.normal(loc=5, size=[10, 25])\r\n\r\nsample_data = tf.placeholder(tf.float32)\r\n\r\nmu = tf.Variable(tf.reduce_mean(sample_data, axis=0),\r\n                 dtype=tf.float32,\r\n                 validate_shape=False,\r\n                 name=\"mu\")\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer(), feed_dict={sample_data: x})\r\n    print(sess.run(mu))\r\n```\r\n\r\n### Error Message:\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float\r\n> \t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nThe interesting part about this is that the first initialization works, but the second one does not.\r\nThis is really annoying if you don't know that behaviour and you try to build some application with interactive python shells.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\nv1.4.0-19-ga52c8d9 1.4.1\r\n- **Python version**: \r\n3.6.3", "comments": ["This will not work, if some variable (`mu` in this case) should be initialized depending on a placeholder.\r\n\r\nI'd like to train `mu` further after initializing, so I either have to get `tf.reduce_mean` produce an optimizable variable or I have to create a new `tf.variable` and initialize it with `tf.reduce_mean` like in this example.", "I just found out that resetting the graph solves the problem of repeated call:\r\n```\r\n########################################### REPEAT:\r\ntf.reset_default_graph()\r\n...\r\n```\r\nHowever, this seems to me like an ugly workaround as it requires me to re-build the whole graph after each run.\r\nIs there a better solution?", "It is not a bug. Most functions in tf including `placeholder` and `variable` mutates the current default graph by adding new operations to it. When you create `mu` for the second time, the first `mu` is still there, waiting to be initialized by the first `sample_data`. That is why you encounter the error, for you do not feed the old `sample_data`.", "OK, thank you for clarifying.\r\nClosing this now"]}, {"number": 16289, "title": "pip package build error", "body": "------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution**:\r\n\r\n> Linux Ubuntu 16.04\r\n\r\n\r\n- **TensorFlow installed from**:\r\n\r\n> source\r\n\r\n- **TensorFlow version**:\r\n\r\n> 1.4.0\r\n\r\n\r\n- **Python version**: \r\n\r\n> 2.7.12\r\n\r\n\r\n- **Bazel version :**\r\n\r\n> Build label: 0.9.0\r\n> Build target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n> Build time: Tue Dec 19 09:31:58 2017 (1513675918)\r\n> Build timestamp: 1513675918\r\n> Build timestamp as int: 1513675918\r\n\r\n- **GCC/Compiler version**\r\n\r\n> gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)\r\n\r\n### Describe the problem\r\n\r\nI am trying to build the pip package after configuration by using : \r\n\r\n`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nbut I am getting a weird error. I have been using tensorflow but trying a fresh install gives me this output ..\r\n\r\n### Terminal log\r\n```\r\n.....................\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):\r\n\tFile \"/home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD\", line 27\r\n\t\tcc_library(name = \"syclrt\", srcs = [sycl_libr...\")], <3 more arguments>)\r\n\tFile \"/home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD\", line 30, in cc_library\r\n\t\tsycl_library_path\r\nname 'sycl_library_path' is not defined\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)\r\nERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'\r\nERROR: /home/konmon01/tensorflow/tensorflow/core/BUILD:2215:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//tensorflow/core:sycl_runtime'\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed\r\nINFO: Elapsed time: 6.310s\r\nFAILED: Build did NOT complete successfully (100 packages loaded)\r\n    currently loading: tensorflow/core/kernels\r\n    Fetching https://bitbucket.org/eigen/eigen/get/429aa5254200.tar.gz; 32,768b\r\n```\r\n\r\n", "comments": ["I downgraded to bazel 0.8.0 and now it works. i am closing this issue.."]}, {"number": 16288, "title": "Tensorflow works in command prompt but not in Spyder", "body": "Hello.\r\nI'm new to Python so maybe I've missed something but anyway, here is my problem.\r\nI've installed tensorflow in Anaconda prompt by using \r\n```\r\nC:\\WINDOWS\\system32> conda create -n tensorflow python=3.6\r\nC:\\WINDOWS\\system32> activate tensorflow\r\n(tensorflow)C:\\WINDOWS\\system32> pip install --ignore-installed --upgrade tensorflow\r\n```\r\nInstalation was succesful, then I opened python and tried to import tensorflow to verify installation\r\n```\r\n(base) C:\\WINDOWS\\system32>python\r\nPython 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```\r\n\r\nAfter activating tensorflow, it works.\r\n```\r\n(tensorflow) C:\\WINDOWS\\system32>python\r\nPython 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n>>>\r\n```\r\nBut I cannot find a way to make it work in Spyder IDE. I always get error:\r\n```ModuleNotFoundError: No module named 'tensorflow'```", "comments": ["Have you tried activating the conda environment with tensorflow installed in it, then from within that environment launching the Spyder IDE? See: https://stackoverflow.com/questions/30170468/how-to-run-spyder-in-virtual-environment\r\n\r\n", "When I try `conda create -n tensorflow-gpu python=3.5.4`\r\nit throws the error 'CondaValueError: prefix already exists: C:\\Anaconda3\\envs\\tensorflow-gpu'\r\nAfter, `activate tensorflow-gpu` , this ipython or spyder fails to import tensorflow while console does it similar to OP's issue. Any idea why? (decided to add this as an issue https://github.com/tensorflow/tensorflow/issues/16379)", "@kubesji1 Did @quaeler advice help you?", "Sorry, I thought I've responded but I don't see my answer here. Yes, it helped.", "Great. I am closing the issue then. ", "activate tensorflow\r\nconda install spyder\r\n**activate tensorflow enviroment first and then start spyder each time.**\r\n\r\ngood luck, Walker", "In general, how do we ensure that a program which works in terminal also works in Spyder (say through os.system(\"terminal command\")?\r\nI am getting license enviroment error for some proprietary tools when I run in Spyder. It is not able to find the lincense. It works well in regular terminal."]}, {"number": 16287, "title": "[Bug] LuongMonotonicAttention in contrib/seq2seq/python/ops/attention_wrapper.py", "body": "`LuongMonotonicAttention.__init__(...)` calls its parent `_BaseAttentionMechanism` with `query_layer` as follows:\r\n```\r\n        query_layer=layers_core.Dense(\r\n            num_units, name=\"query_layer\", use_bias=False),\r\n```\r\nBut, it doesn't apply it on query in `LuongMonotonicAttention.__call__(...)`.\r\n```\r\n  def __call__(self, query, previous_alignments):\r\n    \"\"\"...\r\n    \"\"\"\r\n    with variable_scope.variable_scope(None, \"luong_monotonic_attention\",\r\n                                       [query]):\r\n      score = _luong_score(query, self._keys, self._scale)\r\n      score_bias = variable_scope.get_variable(\r\n          \"attention_score_bias\", dtype=query.dtype,\r\n          initializer=self._score_bias_init)\r\n      score += score_bias\r\n    alignments = self._probability_fn(score, previous_alignments)\r\n    return alignments\r\n```\r\nGuessing from the way `LuongAttention` works, there should be `query_layer=None` in `LuongMonotonicAttention.__init__(...)`.", "comments": ["Apart from this, I noticed that `AttentionWrapper.call(...)` has the following lines:\r\n```\r\n...\r\n    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)\r\n...\r\n      attention, alignments = _compute_attention(\r\n          attention_mechanism, cell_output, previous_alignments[i],\r\n          self._attention_layers[i] if self._attention_layers else None)\r\n...\r\n```\r\nBut as far as I know, decoders' state (along with memory) is used to compute alignments and attention. Is the above code buggy because it's passing cell output rather than the hidden state to `_compute_attention(...)`? Am I making a mistake in understanding these variables?", "I've notified the author.", "> there should be query_layer=None in LuongMonotonicAttention.__init__(...).\r\n\r\nThanks!  PR for a fix is here: #16602."]}, {"number": 16286, "title": "Removes redundant variable assignment", "body": "Addresses alert raised by lgtm.com:\r\nhttps://lgtm.com/projects/g/tensorflow/tensorflow/snapshot/e6183fbeecf069148371be83988e8e5db2b14185/files/tensorflow/python/framework/constant_op.py#xb77a2f6647d782be:1\r\n\r\nIt doesn't seem like assigning `attr_tshape = attr_tshape` does anything, so there's no need to keep it in.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "Signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@alextp The PR is obviously correct, but does this point to a latent bug?", "I think this was copied over from generated code hence the weirdness."]}, {"number": 16285, "title": "tf-seq2seq, nmt, tensorflow's seq2seq diff", "body": "google-seq2seq, nmt, tensorflow's seq2seq\r\nwhat is the diff about them", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "To be clear, I think you're referring to this:\r\n* [google seq2seq](https://github.com/google/seq2seq)\r\n* [nmt](https://github.com/tensorflow/nmt)\r\n* tensorflow [seq2seq](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq)\r\n\r\nThey are all reference implementations similar to what was described [here](https://research.google.com/pubs/pub45610.html). You may use them to do your own research. Hope they help!\r\n\r\n/CC @zffchen78 @ebrevdo \r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "PS: there is also [tensor2tensor](https://github.com/tensorflow/tensor2tensor) which implements the transform architecture as well as seq2seq."]}, {"number": 16284, "title": "[bug?] Tensorflow accepts CUDA_VISIBLE_DEVICES but still allocates memory on multiple GPUs", "body": "### System information\r\n- **I have written a custom script, but effect is also visible for just `import tensorflow as tf; sess = tf.Session()`**:\r\n- **Windows 7 Professional**:\r\n- **TensorFlow installed from binary (pip, in Anaconda environment)**:\r\n- **TensorFlow version 1.4.0**:\r\n- **Python version 3.5.4**: \r\n- **CUDA/cuDNN version 8.0/6.0**:\r\n- **4 GeForce GTX 1080Ti, 11GB**:\r\n\r\n### Problem description\r\nI am facing the following issue:\r\n\r\nIf I set\r\n`CUDA_VISIBLE_DEVICES=1`\r\nand start my python script, everything works as expected, only GPU 1 is used/only memory from GPU 1 is allocated. GPU1 hosts the Desktop Window Manager.\r\n\r\nIf I set\r\n`CUDA_VISIBLE_DEVICES=0 # or 2 or 3 `\r\nbefore running my python script\r\n`sess = tf.Session()`\r\nfaithfully reports only one available GPU (with the expected PCI bus id), see attached file ipython.txt.\r\n\r\nHowever, nvidia_smi.exe shows that memory on all remaining GPUs except for GPU1 is allocated. GPU-Util shows that expected GPU is used for actual computation, see attached file nvidia_smi_output.txt.\r\n\r\nI see this effect both for my actual tensorflow script as well for simple interactive python with\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n```\r\n\r\nUnfortunately, the dual boot Ubuntu is not working at the moment, but once it is running again, I can try to check whether a similar effect presents itself there.\r\nCould this be a bug, possibly related to Windows? Or a driver or hardware issue?\r\n\r\nAttached files:\r\n[ipython.txt](https://github.com/tensorflow/tensorflow/files/1651908/ipython.txt): ipython script and output.\r\n[nvidia_smi_output.txt](https://github.com/tensorflow/tensorflow/files/1651905/nvidia_smi_output.txt): output of nvidia-smi.exe\r\n", "comments": ["Are you using it exactly as \r\n`CUDA_VISIBLE_DEVICES=1 python oo.py`\r\n\r\nJust took another look at your description, my answer is not to your reported issue...", "Thanks for the comment, as additional information:\r\nI see this effect whether I set the environment variable in the shell before starting my python script/interactive session or if I set it via os.environ[\"...\"] inside the python script before importing tensorflow.", "Maybe it's a windows thing? I'm not sure if Windows honors this flag. Could you try with more logging on?\r\n\r\nCC @mrry @guschmue ", "May very well be a Windows thing... \r\nWith `tf.logging.set_verbosity` set to INFO or DEBUG, tensorflow does not print additional output for just `sess = tf.Session()`.\r\n\r\nIt seems to have some effect as it seems to differenciate between graphic card 1 and graphic cards [0, 2, 3]. If I do not set CUDA_VISIBLE_DEVICES at all, it allocates memory on all four cards.", "Just tried on tf-1.4/cuda8/windows10 and it is working as expected. \r\nBut yes, the logs from @e-ka  show that the tensorflow process (pid 8040) has the device 0,2,3 open where it should have only seen 1 device. Kind of odd since tensorflow is reporting only 1 device.\r\nThe driver on my box is a little older 388.19 (vrs 390.65) and I have windows10 instead of windows7.\r\nI can update my box to the latest driver to rule that out.\r\n ", "Thanks for testing! \r\nI don't think it's necessarily the driver version by itself. IIRC, I updated the driver & tensorflow version along the way to test whether this already fixes it, but to no avail. ", "Thank you @guschmue for testing!\r\n\r\n@e-ka I think it's `TF_CPP_MIN_LOG_LEVEL`?", "@drpngx sorry for the late response, I did not have access to the machine for the last week. Setting the verbosity via the environment variable TF_CPP_MIN_LOG_LEVEL should generally do the same as tf.logging in python, should it not? I also do not see a difference in the output...", "`C++` logging and pthon logging are different. If you set it correctly, you should definitely see a lot more output.", "Somewhat related to this: TF 1.4, Python 3.6.3 x64, Windows 10, CUDA 8, cuDNN 6, NVIDIA driver 385.69\r\n\r\nI was trying to set CUDA_VISIBLE_DEVICES=\"\" (as per #9201) to stop the GPU being grabbed at all and it goes ahead and open the device anyway. I only have one GPU on this laptop so I can't test what the behaviour is for multiple devices (the other machines I have access to are all running linux).\r\n\r\n    >>> import os\r\n    >>> os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\r\n    >>> import tensorflow as tf\r\n    >>> sess=tf.Session()\r\n    2018-02-12 16:22:11.752327: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n    2018-02-12 16:22:12.120254: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1030] Found device 0 with properties:\r\n    name: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705\r\n    pciBusID: 0000:01:00.0\r\n    totalMemory: 6.00GiB freeMemory: 4.96GiB\r\n    2018-02-12 16:22:12.120473: I C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n\r\nConfirmation that it is grabbing the GPU memory:\r\n\r\n    >nvidia-smi\r\n    Mon Feb 12 16:23:12 2018\r\n    +-----------------------------------------------------------------------------+\r\n    | NVIDIA-SMI 385.69                 Driver Version: 385.69                    |\r\n    |-------------------------------+----------------------+----------------------+\r\n    | GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n    |===============================+======================+======================|\r\n    |   0  GeForce GTX 1060   WDDM  | 00000000:01:00.0  On |                  N/A |\r\n    | N/A   57C    P2    27W /  N/A |   5763MiB /  6144MiB |      0%      Default |\r\n    +-------------------------------+----------------------+----------------------+\r\n\r\nWorkaround for now is to set CUDA_VISIBLE_DEVICES=\"-1\" which generates a seemingly benign error message as a side effect:\r\n\r\n    2018-02-12 16:09:01.981599: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:406] failed call to cuInit: CUDA_ERROR_NO_DEVICE\r\n\r\nedit: turns out that workaround came from #2175 ... woops", "I see TF version 1.4 was used when this issue was created. This may not exist with the latest version of Tensorflow. Please create a new issue if it still persists(with the latest TF version) by giving all the information asked as per [this template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks !", "I have 2 X 1080Ti GPUs.\r\nEach of the GPUs has 7.4GB or 8.2 GB out of the 11 GB being used when performing inferencing\r\nIs this correct functioning of the code?", "@e-ka Do you solve this problem? I have same too."]}, {"number": 16283, "title": "tf.edit_distance work wrang", "body": "### System information\r\n\r\n== cat /etc/issue ===============================================\r\nLinux asr-eval 4.4.77-1.el7.elrepo.x86_64 #1 SMP Sat Jul 15 11:17:37 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux asr-eval 4.4.77-1.el7.elrepo.x86_64 #1 SMP Sat Jul 15 11:17:37 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.1)\r\ntensorflow-gpu (1.5.0rc1)\r\ntensorflow-tensorboard (0.4.0rc3)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.5.0-rc1\r\ntf.GIT_VERSION = v1.5.0-rc0-8-gc678970\r\ntf.COMPILER_VERSION = v1.5.0-rc0-8-gc678970\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/nvidia/lib64:/usr/local/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nMon Jan 22 10:28:25 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.98                 Driver Version: 384.98                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  TITAN Xp            Off  | 00000000:0C:00.0 Off |                  N/A |\r\n| 23%   29C    P0    68W / 250W |     10MiB / 12189MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176\r\n\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\n('v1.5.0-rc0-8-gc678970', '1.5.0-rc1')\r\n\r\n### Describe the problem\r\n```\r\n(<tf.Tensor: id=1, shape=(2, 7), dtype=int32, numpy=\r\n array([[1, 2, 3, 4, 5, 0, 0],\r\n        [2, 2, 3, 5, 4, 0, 0]], dtype=int32)>,\r\n <tf.Tensor: id=81, shape=(2, 7), dtype=int32, numpy=\r\n array([[1, 2, 1, 3, 4, 0, 0],\r\n        [2, 2, 1, 2, 3, 0, 0]], dtype=int32)>)\r\n\r\nIn [160]: edit\r\nOut[160]: <tf.Tensor: id=154, shape=(2,), dtype=float32, numpy=array([2., 3.], dtype=float32)>\r\n```\r\nThe truth must [3, 3]\r\n\r\n### Source code / logs\r\n```\r\nt: (<tf.Tensor: id=1, shape=(2, 7), dtype=int32, numpy=\r\n array([[1, 2, 3, 4, 5, 0, 0],\r\n        [2, 2, 3, 5, 4, 0, 0]], dtype=int32)>,\r\nl: <tf.Tensor: id=81, shape=(2, 7), dtype=int32, numpy=\r\n array([[1, 2, 1, 3, 4, 0, 0],\r\n        [2, 2, 1, 2, 3, 0, 0]], dtype=int32)>)\r\n\r\nedit = tf.edit_distance(st, sl, normalize=False)\r\n\r\nIn [160]: edit\r\nOut[160]: <tf.Tensor: id=154, shape=(2,), dtype=float32, numpy=array([2., 3.], dtype=float32)>\r\n```", "comments": []}, {"number": 16282, "title": "Adding go_package to proto definition files necessary for Tensorflow serving", "body": "I've went through guidelines about calling Tensorflow serving in Python.\r\nThen I've decided to make it Go.\r\n\r\nYou can find my repo here:\r\nhttps://github.com/datainq/go-mnist-client\r\n\r\nManually preparing files is unmaintainable. What about we add `go_package` to proto files?\r\ne.g. for a `github.com/tensorflow/tensorflow/tensorflow/core/protobuf/saver.proto` it would be:\r\n```\r\ngo_package = \"github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf\";\r\n```\r\nI think keeping the generated files in go' subpath is a good idea.\r\n\r\nDoes it make sense? \r\n(@jhseu was the author of the initial Go code)\r\n\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A\r\n- **TensorFlow installed from (source or binary)**: N/A\r\n- **TensorFlow version (use command below)**: N/A\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Adding the `go_package` to protobuf files seems reasonable.\r\nMarking as Contributions Welcome, we'd welcome a PR.", "Added a PR #17262 for the fix."]}, {"number": 16281, "title": "After Working with Tensorflow cpu version for 2 days, it gave me an error on installation today", "body": "I had installed and used Tensorflow successfully but today when I opened my computer it gave me this error message:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/asus/PycharmProjects/untitled3/CNNCIFARTFNEW.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 60, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\imp.py\", line 297, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 54, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python36-32\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nError importing tensorflow.  Unless you are using bazel,\r\nyou should not try to import tensorflow from its source directory;\r\nplease exit the tensorflow source tree, and relaunch your python interpreter\r\nfrom there.\r\n\r\n\r\nPlease help. This is urgent.", "comments": ["Did you update any software? E.g. python? Did you try reinstalling it once it didn't work. Did you use virtualenv? If so, you need to re-activate your environment.\r\n", "Looks like it is working. Thanks. I just reinstalled it. I was using Conda. I also opened another project in pycharm and copied codes there."]}, {"number": 16280, "title": "Repair compilation error of tensorflow built with MKL-DNN", "body": "When we compile tensorflow with Intel **MKL-DNN**, it will meet a failure:\r\n\r\n`bazel build --copt -O3 --copt=-DINTEL_MKL_DNN --config=mkl -c opt //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n**error: 'mkldnn::algorithm' is not a namespace\r\n  using mkldnn::algorithm::lrn_across_channels;**\r\n\r\n Removing the 'algorithm' field in _tensorflow/core/kernels/mkl_lrn_op.cc_ can solve this problem and lead to successful compilation.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@starsblinking  Can you contact me? I'd like to learn more about what is causing this error because it is not happening in our builds.", "@gunan, @yifeif  is there a way for me to trigger an MKL build? Yifei's doc doesn't list MKL as one of the builds triggered by kokoro-run.", "@rmlarsen That is correct. Google CI nightlies are only testing MKL, not MKL-DNN. I'm testing this here at Intel to verify.", "Thanks @claynerobison. Let me know if this is safe to commit or not when you are done testing."]}, {"number": 16279, "title": "While using tf.while_loop , the _Slicehelper chooses strided_slice op(req 4 args) instead of slice op(req 3 args)", "body": "I am facing this issue while creating a decoder using tf.while_loop\r\n\r\n`\r\ntrain_decoder = tf.while_loop(self.coarse_decoder_condition, self.coarse_decoder_function,\r\n[self.iter_decoder_c, tf.zeros([1,self.c_dec_size]), tf.ones([self.c_dec_size])] ,\r\n\t\t\t\tshape_invariants =[self.iter_decoder_c.get_shape(),tf.TensorShape([None,self.c_dec_size]),tf.TensorShape([self.c_dec_size]) ] )`\r\n\r\n\r\nhere is what comes up in the cmd\r\n\r\n```\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2816, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2640, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2576, in _BuildLoop\r\n    c = ops.convert_to_tensor(pred(*packed_vars))\r\n\r\n  File \"C:\\Users\\HP\\Music\\Final\\nn.py\", line 200, in coarse_decoder_condition\r\n    return it[0] < self.oplen_c[0]\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice\r\n    name=name)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-\r\npackages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_he\r\nlper\r\n    op_def=op_def)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2958, in create_op\r\n    set_shapes_for_outputs(ret)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2209, in set_shapes_for_outputs\r\n\r\n    shapes = shape_func(op)\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2159, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 627, in call_cpp_shap\r\ne_fn\r\n    require_shape_fn)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\", line 691, in _call_cpp_sha\r\npe_fn_impl\r\n    raise ValueError(err.message)\r\nValueError: Index out of range using input dim 0; input has only 0 dims for 'Coarse_Decoder/while/strided_slice' (op: 'StridedSlice') with input shape\r\ns: [], [1], [1], [1] and with computed input tensors: input[3] = <1>.\r\n\r\n\r\n```\r\n\r\nEssentially what is happening is that the while_loop => buildloop => _SliceHelper\r\n\r\nthe _sliceHelper chooses strided_slice op(which requires 4 args instead of 3) instead of just slice op\r\n\r\nThis is causing me to get an error while passing 3 args , which is what my requirement is.\r\n\r\nA similar issue highlighted [here ](https://github.com/tensorflow/models/issues/817)\r\n\r\nI can't manually choose slice() over strided_slice()\r\n\r\nAny help would be appreciated @michaelisard", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "OS Platform and Distribution : Windows 7 Professional\r\nTensorFlow version 1.4.0 from pip\r\nGPU model and memory : currently running on CPU (i5) , 8GB RAM\r\nRest are not relevant to my case\r\nExact command to reproduce:\r\n\r\nI was creating a decoder rnn with GRU cell. Here is the function:\r\n\r\n````\r\ndef nl_decoder(self):\r\n\t\t\r\n\t\twith tf.variable_scope(\"NL_Decoder\"):\r\n\t\t\tself.decoder_nl_cell = tf.contrib.rnn.GRUCell(self.nl_dec_size)\r\n\t\t\t\r\n\t\t\tself.iter_decoder_nl = tf.constant(0)\r\n\t\t\tself.iter_decoder_nl_test = tf.constant(0)\r\n\t\t\t\r\n\t\t\ttrain_decoder = tf.while_loop(self.nl_decoder_condition,self.nl_decoder_function, [ self.iter_decoder_nl, tf.zeros([1,self.nl_dec_size]) , tf.ones([self.nl_dec_size])],\\\r\n\t\t\tshape_invariants = [self.iter_decoder_nl.get_shape(), tf.TensorShape([None,self.nl_dec_size]),\\\r\n\t\t\ttf.TensorShape([self.nl_dec_size])])\r\n\r\n\t\treturn train_decoder[1][1:]\r\n\r\n", "@Aashit-Sharma Please provide a minimum reproducible example.\r\n\r\nThe snippets of code you've provided above don't define self.nl_decoder_{condition,function}, or other critical pieces of information.", "```\r\ndef nl_decoder_condition(self,it,outputs,hidden):\r\n\t\t\treturn it[0] < self.oplen_nl[0]\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n\t\t# This is the step where we concatenate hidden state with context hidden state \t\t\t\r\n\r\ndef nl_decoder_function(self,it,outputs,hidden):\r\n\t\t\tout,hidden = self.decoder_nl_cell(tf.stack([tf.concat(0,[outputs[-1,:],self.nl_context_concat])]) , tf.stack([hidden]))\r\n\t\t\t\r\n\t\t\toutputs = tf.concat(0,[outputs,out])\r\n\t\t\treturn it+1,outputs,hidden[0,:]\r\n\r\n```\r\n\r\nSorry for the delayed response. These are all the functions necessary to run the nl_decoder . \r\n\r\n\r\nPlaceholders:\r\n`self.oplen_nl = tf.placeholder(tf.int32,[1],name=\"nl_oplen\")\r\n`\r\n\r\nthis comes in the graph function \r\n`\t\t\tself.nl_context_concat = tf.concat(0,[self.current_hidden_nl_context,self.encoded_prediction])\r\n`\r\n", "@Aashit-Sharma I think you've misunderstood my request.  Please provide a standalone example that exhibits the problem when it is run.  I.e. provide the full python program.  Otherwise it's hard to know exactly what's going wrong.  Thanks!", "Sure. MrRNN.py:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass Configuration:\r\n\tdef __init__(self):\r\n\t\t\r\n\t\tself.nl_embed_size = 50\r\n\t\tself.c_embed_size = 50\r\n\t\t\r\n\t\tself.nlvocab = 10\r\n\t\tself.cvocab = 10\r\n\t\t\r\n\t\tself.nl_enc_size = 100\r\n\t\tself.c_enc_size = 100\r\n\t\t\r\n\t\tself.nl_context = 100\r\n\t\tself.c_context = 100\r\n\t\t\r\n\t\tself.c_dec_size = 100\r\n\t\tself.nl_dec_size = 100\r\n\t\t\r\n\t\tself.pred_enc_size = 100\r\n\t\t\r\n\t\tself.learning_rate = 0.0002\r\n\t\t\r\n\t\tself.end_of_nl_utt = 0\r\n\t\tself.end_of_coarse_utt = 0\r\n\t\treturn\r\n\t\t\r\nclass MRRNN():\r\n\t\r\n\tdef __init__(self,config):\r\n\t\t\r\n\t\tself.nlvocab = config.nlvocab\r\n\t\tself.cvocab = config.cvocab\r\n\t\tself.nl_context = config.nl_context\r\n\t\tself.c_context = config.c_context\r\n\t\t\r\n\t\tself.nl_dec_size = config.nl_dec_size\r\n\t\tself.c_dec_size = config.c_dec_size\r\n\t\r\n\t\tself.nl_enc_size = config.nl_enc_size\r\n\t\tself.c_enc_size = config.c_enc_size\r\n\t\t\r\n\t\tself.nl_embed_size = config.nl_embed_size\r\n\t\tself.c_embed_size = config.c_embed_size\r\n\t\t\r\n\t\tself.pred_enc_size = config.pred_enc_size\r\n\t\tself.learning_rate = config.learning_rate\r\n\t\tself.end_of_nl_utt = config.end_of_nl_utt\r\n\t\tself.end_of_coarse_utt = config.end_of_coarse_utt\r\n\r\n\t\t#input embeddings\r\n\t\tself.ipemb_nl = tf.placeholder(tf.int32,[None],name=\"nl_embedding\")\t\t\r\n\t\tself.ipemb_c = tf.placeholder(tf.int32,[None],name=\"coarse_embedding\")\r\n\t\t\r\n\t\t#output nl/coarse length\r\n\t\tself.oplen_nl = tf.placeholder(tf.int32,[1],name=\"nl_oplen\")\r\n\t\tself.oplen_c = tf.placeholder(tf.int32,[1],name=\"c_oplen\")\r\n\r\n\t\t#tarlab => target label\r\n\t\tself.tarlab_nl = tf.placeholder(tf.int32,[None],name=\"tarlab_nl\")\r\n\t\tself.tarlab_c = tf.placeholder(tf.int32,[None],name=\"tarlab_c\")\r\n\t\t\r\n\t\t#prevcont => previous context(state)\r\n\t\tself.prevcont_nl = tf.placeholder(tf.float32,[1,self.c_context],name=\"prevcont_nl\")\r\n\t\tself.prevcont_c = tf.placeholder(tf.float32,[1,self.nl_context],name=\"prevcont_c\")\r\n\t\t#self.eou_c = tf.placeholder(self.end_of_coarse_utt,dtype)\r\n\t\t#self.eou_nl = tf.placeholder(self.end_of_nl_utt,dtype)\r\n\r\n\t\t\r\n\t\t#embedding variable\r\n\t\tself.nl_embedding = tf.Variable(tf.random_uniform([self.nlvocab,self.nl_embed_size],-1,1))\r\n\t\tself.c_embedding = tf.Variable(tf.random_uniform([self.cvocab,self.c_embed_size],-1,1))\r\n\t\t\r\n\t\t#nlop_emb :: cop_emb => NL output embedding :: Coarse ****\r\n\t\tself.nlop_emb = tf.Variable(tf.random_uniform([self.nlvocab,self.nl_dec_size],-1,1))\r\n\t\tself.cop_emb = tf.Variable(tf.random_uniform([self.cvocab,self.c_dec_size],-1,1))\r\n\t\t\r\n\t\t##GRAPH and LOSS##\r\n\t\t\r\n\t\tself._make_graph()\r\n\t\tself.loss_N_optimize()\r\n\t\tself.make_grad_acmul()\r\n\t\t\r\n\t\tinit = tf.initialize_all_vairables()\r\n\t\t\r\n\t\tself.sess = tf.Session()\r\n\t\tself.sess.run(init)\r\n\t\t\r\n\t\tself.writer = tf.summary.FileWriter(\"./log\",graph=self.see.graph)\r\n\t\t\r\n\t\tself.saver =tf.train.Saver()\r\n\t\t\r\n##########FUNCTIONS############################################################################################################################################################\r\n\t\t\r\n\t\r\n\t## makin the graf\r\n\t\r\n\tdef _make_graph(self):\r\n\t\t\r\n\t\tself.cRepresentation = self.coarse_embed_ip()\r\n\t\tself.encoded_coarse = self.coarse_encoder()\r\n\t\tself.current_hidden_coarse_context = self.coarse_context()\r\n\t\t\r\n\t\tself.coarse_prediction = self.coarse_decoder()\r\n\t\t\r\n\t\tself.encoded_prediction = self.coarse_prediction_encoder()\r\n\t\tself.nlRepresentation = self.nl_embed_ip()\r\n\t\t\r\n\t\tself.encoded_nl = self.nl_encoder()\r\n\t\tself.current_hidden_nl_context =self.nl_context()\r\n\t\t\r\n\t\tself.nl_context_concat = tf.concat(0,[self.current_hidden_nl_context,self.encoded_prediction])\r\n\t\t\r\n\t\tself.nl_prediction = self.nl_decoder()\r\n\t\t\r\n\t\tself.logits_nl,self.logits_coarse = self.compute_logits()\r\n\t\t\r\n\t\t\r\n##########NL TOKENS############################################################################################################################################################\r\n\t\r\n\tdef nl_embed_ip(self):\r\n\t\tselected_embedding = tf.nn.embedding_lookup(self.nl_embedding,self.ipemb_nl)\r\n\t\t\r\n\t\treturn tf.stack([selected_embedding])\r\n\t\r\n\tdef nl_encoder(self):\r\n\t\t\r\n\t\twith tf.variable_scope(\"NL_Encoder\"):\r\n\t\t\tcell = tf.contrib.rnn.GRUCell(self.nl_enc_size)\r\n\t\t\t_,hidden_state = tf.nn.dynamic_rnn(cell,self.nlRepresentation,dtype=tf.float32)\r\n\t\t\t\r\n\t\treturn [tf.stack([hidden_state[-1,:]]) ]\r\n\t\t\r\n\tdef nl_context(self):\r\n\t\r\n\t\twith tf.variable_scope(\"NL_Context\"):\r\n\t\t\tcell = tf.contrib.rnn.GRUCell(self.nl_context)\r\n\t\t\t\r\n\t\t\tself.reset_nl_state = cell.zero_state(1,dtype=tf.float32)\r\n\t\t\t_,hidden = tf.nn.static_rnn(cell,self.encoded_nl,initial_state = self.prevcont_nl)\r\n\t\t\t\t\t\t\t\r\n\t\treturn hidden[-1,:]\r\n\t\t\r\n###################################################################################################################################################\r\n\tdef nl_decoder_condition(self,it,outputs,hidden):\r\n\t\treturn it[0] < self.oplen_nl[0]\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n\t# This is the step where we concatenate hidden state with context hidden state \t\t\t\r\n\tdef nl_decoder_function(self,it,outputs,hidden):\r\n\t\tout,hidden = self.decoder_nl_cell(tf.stack([tf.concat(0,[outputs[-1,:],self.nl_context_concat])]) , tf.stack([hidden]))\r\n\t\t\r\n\t\toutputs = tf.concat(0,[outputs,out])\r\n\t\treturn it+1,outputs,hidden[0,:]\t\t\r\n###################################################################################################################################################\r\n\tdef nl_decoder(self):\r\n\t\t\r\n\t\twith tf.variable_scope(\"NL_Decoder\"):\r\n\t\t\tself.decoder_nl_cell = tf.contrib.rnn.GRUCell(self.nl_dec_size)\r\n\t\t\t\r\n\t\t\tself.iter_decoder_nl = tf.constant(0)\r\n\t\t\tself.iter_decoder_nl_test = tf.constant(0)\r\n\t\t\t\r\n\t\t\ttrain_decoder = tf.while_loop(self.nl_decoder_condition,self.nl_decoder_function, [ self.iter_decoder_nl, tf.zeros([1,self.nl_dec_size]) , tf.ones([self.nl_dec_size])],\\\r\n\t\t\tshape_invariants = [self.iter_decoder_nl.get_shape(), tf.TensorShape([None,self.nl_dec_size]),\\\r\n\t\t\ttf.TensorShape([self.nl_dec_size])])\r\n\r\n\t\treturn train_decoder[1][1:]\r\n\r\n\t\t\r\n############ COARSE TOKENS ##########################################################################################################################\r\n\t\r\n\tdef coarse_embed_ip(self):\r\n\t\t#this will go into the encoder\r\n\t\tselected_embedding = tf.nn.embedding_lookup(self.c_embedding,self.ipemb_c)\r\n\t\t\r\n\t\treturn tf.stack([selected_embedding])\r\n\t\r\n\tdef coarse_encoder(self):\r\n\t\r\n\t\twith tf.variable_scope(\"Coarse_Encoder\"):\r\n\t\t\tcell = tf.contrib.rnn.GRUCell(self.c_enc_size)\r\n\t\t\t_, hidden_state= tf.nn.dynamic_rnn(cell,self.cRepresentation,dtype=tf.float32)\r\n\t\t\t\r\n\t\treturn [tf.stack([hidden_state[-1,:]]) ]\r\n\t\t\r\n\tdef coarse_context(self):\r\n\t\t\r\n\t\twith tf.variable_scope(\"Coarse_Context\"):\r\n\t\t\tcell = tf.contrib.rnn.GRUCell(self.c_context)\r\n\t\t\t\r\n\t\t\tself.reset_coarse_state = cell.zero_state(1,dtype=tf.int64)\r\n\t\t\t_,hidden = tf.nn.static_rnn(cell,self.encoded_coarse,initial_state = self.prevcont_c)\r\n\t\t\t\r\n\t\treturn hidden[-1,:]\r\n\t\t\r\n#################################################################################################\r\n\t\t\t\r\n\tdef coarse_decoder_condition(self,it,outputs,hidden):\r\n\t\treturn it[0] < self.oplen_c[0]\r\n\t\t\r\n\t# This is the step where we concatenate hidden state with context hidden state  \r\n\tdef coarse_decoder_function(self,it,outputs,hidden):\r\n\t\tout,hidden = self.decoder_coarse_cell(tf.stack([tf.concat(0,[outputs[-1,:],self.current_hidden_coarse_context])]), tf.stack([hidden]))\r\n\t\t\r\n\t\toutputs = tf.concat(0,[outputs,out])\r\n\t\treturn it+1,outputs,hidden[0,:]\r\n\t\r\n####################################################################################################\r\n\r\n\tdef coarse_decoder(self):\r\n\t\r\n\t\twith tf.variable_scope(\"Coarse_Decoder\"):\r\n\t\t\tself.decoder_coarse_cell = tf.contrib.rnn.GRUCell(self.c_dec_size)\r\n\t\t\t\r\n\t\t\tself.iter_decoder_c = tf.constant(0.0)\r\n\t\t\t#self.iter_decoder_c_test = tf.constant(0)\r\n\t\t\t\r\n\t\t\t#while_loop for parallel iterations\r\n\t\t\ttrain_decoder = tf.while_loop(self.coarse_decoder_condition, self.coarse_decoder_function ,\\\r\n\t\t\t\tloop_vars=[self.iter_decoder_c,\\\r\n\t\t\t\t\ttf.ones([self.c_dec_size]),\\\r\n\t\t\t\t\ttf.zeros([1,self.c_dec_size])],\\\r\n\t\t\t\tshape_invariants = [\\\r\n\t\t\t\t\t\t\t\t\tself.iter_decoder_c.get_shape(),\\\r\n\t\t\t\t\t\t\t\t\ttf.TensorShape([self.c_dec_size]),\\\r\n\t\t\t\t\t\t\t\t\ttf.TensorShape([None,self.c_dec_size])\\\r\n\t\t\t\t\t\t\t\t\t] )\r\n\t\t\t\t\t\t\t\t\t\t\r\n\t\treturn train_decoder[1][1:]\r\n\t\t\t\t\r\n\r\n\t## PREDICTION ENCODER ##\r\n\t\r\n\tdef coarse_prediction_encoder(self):\r\n\t\twith tf.variable_scope(\"prediction_encoder\"):\r\n\t\t\tcell = tf.contrib.rnn.GRUCell(self.pred_enc_size)\r\n\t\t\t_,hidden_state = tf.nn.dynamic_rnn(cell,tf.stack([self.coarse_prediction]),dtype = tf.float32)\r\n\t\t\t\r\n\r\n\t\treturn hidden_state[-1,:]\r\n\t\t\t\r\n\t\t\r\n\tdef compute_logits(self):\r\n\t\tlogits_nl = tf.matmul(self.nlop_emb,tf.transpose(self.nl_prediction))\r\n\t\t\r\n\t\tlogits_coarse = tf.matmul(self.cop_emb,tf.transpose(self.coarse_prediction))\r\n\t\t\r\n\t\treturn tf.transpose(logits_nl),tf.transpose(logits_coarse)\r\n\t\t\r\n\t\r\n\tdef loss_N_optimize(self):\r\n\t\tnl_onehot_target = tf.one_hot(self.tarlab_nl,self.nlvocab)\r\n\t\t\r\n\t\tcoarse_onehot_target = tf.one_hot(self.tarlab_c,self.cvocab)\r\n\t\t\r\n\t\tmap_nl = tf.nn.softmax_cross_entropy_with_logits(self.logits_nl, nl_onehot_target)\r\n\t\tmap_coarse = tf.nn.softmax_cross_entropy_with_logits(self.logits_coarse, coarse_onehot_target)\r\n\t\t\r\n\t\tself.loss = (tf.reduce_sum(map_nl) + tf.reduce_sum(map_coarse))\r\n\t\tseld.optimzer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\r\n\t\t\t\r\n\t\r\n\t\r\n\tdef make_grad_acmul(self):\r\n\t\ttvs = tf.trainable_variables()\r\n\t\t\r\n\t\taccum_vars = [tf.Variable(tf.zeros_like(tv.initialized_value()) , trainable = False) for tv in tvs]\r\n\t\t\r\n\t\tzero_ops = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]\r\n\t\t\r\n\t\tgvs = self.optimizer.compute_gradients(self.loss, tvs)\r\n\t\t\r\n\t\taccum_ops = [accum_vars[i].assign_add(gv[0]) for i, gv in enumerate(gvs)]\r\n\t\t\r\n\t\tself.zero_grads = zero_ops\r\n\t\t\r\n\t\tself.grad_accumul = accum_ops\r\n\t\tself.grad_apply = self.optimizer.apply_gradients([(accum_vars[i],gv[1]) for i, gv in enumerate(gvs)])\r\n\t\t\r\n\t\r\n\tdef split_utterances(self,NL_X,C_X):\r\n\t\t#NL_X and C_X are batch of dialogues and their respective coarse utterances , respectively\r\n\t\tn_dialogues = len(NL_X)\r\n\t\tdialogues_nl = []\r\n\t\tdialogues_coarse = []\r\n\t\t\r\n\t\tfor i in xrange(n_dialogues):\r\n\t\t\t# splitting dialogues / utterances\r\n\t\t\tend_of_nl_utt = np.where( np.array(NL_X[i]) == self.end_of_nl_utt)[0]\r\n\t\t\tend_of_coarse_utt = np.where(np.array(C_X[i]) == self.end_of_coarse_utt)[0]\r\n\t\t\t\r\n\t\t\tcurrent_dialogue_nl = np.split(NL_X[i], end_of_nl_utt[:-1]+1)\r\n\t\t\tcurrent_dialogue_coarse = np.split(C_X[i], end_of_coarse_utt[:-1]+1)\r\n\t\t\t\r\n\t\t\tdialogues_nl.append(current_dialogue_nl)\r\n\t\t\tdialogues_coarse.append(current_dialogue_coarse)\r\n\t\t\t\r\n\t\t\r\n\t\treturn dialogues_nl,dialogues_coarse\r\n\t\t\r\n\t\t\r\n\tdef partial_fit(self,NL_X,C_X):\r\n\t\t\r\n\t\t#reset gradient computation\r\n\t\tself.sess.run(self.zero_grads)\t\t\r\n\t\tdialogues_nl,dialogues_coarse = self.split_utterances(NL_X,C_X)\r\n\t\t\r\n\t\t\r\n\t\t#Train\r\n\t\tfor dialogue in xrange(len(dialogues_nl)):\r\n\t\t\tfor u_id in xrange(len(dialogues_nl[dialogue]) - 1):\r\n\t\t\t\tif u_id ==0:\r\n\t\t\t\t\tprev_nl = np.zeros([1,self.nl_context])\r\n\t\t\t\t\tprev_coarse = np.zeros([1,self.c_context])\r\n\t\t\t\t\r\n\t\t\t\tfeed_dict = {\\\r\n\t\t\t\tself.ipemb_nl : dialogues_nl[dialogue][u_id],\\\r\n\t\t\t\tself.ipemb_c : dialogues_coarse[dialogue][u_id],\\\r\n\t\t\t\tself.tarlab_nl : dialogues_nl[dialogue][u_id+1],\\\r\n\t\t\t\tself.tarlab_c : dialogues_coarse[dialogue][u_id+1],\\\r\n\t\t\t\tself.oplen_nl : [len(dialogues_nl[dialogue][u_id+1])],\\\r\n\t\t\t\tself.oplen_c : [len(dialogues_coarse[dialogue][u_id+1])],\\\r\n\t\t\t\tself.prevcont_nl : prev_nl,\\\r\n\t\t\t\tself.prevcont_c : prev_coarse\\\r\n\t\t\t\t}\r\n\t\t\t\r\n\t\t\t\tprev_coarse,prev_nl,_ = self.sess.run(\\\r\n\t\t\t\t\t\t(\\\r\n\t\t\t\t\t\t\tself.current_hidden_coarse_context,\\\r\n\t\t\t\t\t\t\tself.current_hidden_nl_context,\\\r\n\t\t\t\t\t\t\tself.grad_accumul),\\\r\n\t\t\t\t\t\tfeed_dict=feed_dict)\r\n\t\t\t\tprev_coarse = [prev_coarse]\r\n\t\t\t\tprev_nl = [prev_nl]\r\n\t\tself.sess.run(self.grad_apply)\r\n\t\r\n\t\r\n\tdef cost(self,NL_X,C_X):\r\n\t\tdialogues_nl, dialogues_coarse = self.split_utterances(NL_X,C_X)\r\n\t\tfor dialogue in xrange(len(dialogues_nl)):\r\n\t\t\tfor u_id in xrange(len(dialogues_nl[dialogue])-1):\r\n\t\t\t\tif u_id == 0 :\r\n\t\t\t\t\tprev_coarse = np.zeros([1,self.c_context])\r\n\t\t\t\t\tprev_nl = np.zeros([1,self.nl_context])\r\n\t\t\t\t\ttotal_loss = 0\r\n\t\t\t\t\t\r\n\t\t\t\tfeed_dict = {\\\r\n\t\t\t\t\tself.ipemb_nl: dialogues_nl[u_id],\\\r\n\t\t\t\t\tself.ipemb_c: dialogues_coarse[u_id],\\\r\n\t\t\t\t\tself.tarlab_nl: dialogues_nl[u_id+1],\\\r\n\t\t\t\t\tself.tarlab_c: dialogues_coarse[u_id+1],\\\r\n\t\t\t\t\tself.oplen_nl: [len(dialogues_nl[u_id+1])],\\\r\n\t\t\t\t\tself.oplen_c: [len(dialogues_coarse[u_id+1])],\\\r\n\t\t\t\t\tself.prevcont_c: prev_coarse, \\\r\n\t\t\t\t\tself.prevcont_nl: prev_nl\\\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t\tprev_coarse,prev_nl,loss=self.sess.run(\\\r\n\t\t\t\t\t(\\\r\n\t\t\t\t\t\tself.current_hidden_coarse_context,\\\r\n\t\t\t\t\t\tself.current_hidden_nl_context,\\\r\n\t\t\t\t\t\tself.loss),\\\r\n\t\t\t\t\tfeed_dict=feed_dict)\r\n\t\t\t\t\t\r\n\t\t\t\tprev_coarse = [prev_coarse]\r\n\t\t\t\tprev_nl = [prev_nl]\r\n\t\t\t\ttotal_loss += loss\r\n\t\t\t\r\n\t\t\ttotal_loss /= float(len(NL_X))\r\n\t\t\treturn total_loss\r\n\t\t\t\r\n\t\r\n\tdef generate(self,dialogue_nl,dialogue_coarse,max_coarse_generation=10,max_nl_generation=10):\r\n\t\t\r\n\t\tfor u_id in xrange(len(dialogue_nl)-1):\r\n\t\t\tif u_id == 0:\r\n\t\t\t\tprev_coarse = np.zeros([1,self.c_context])\r\n\t\t\t\tprev_nl = np.zeros([1,self.nl_context])\r\n\t\t\t\t\r\n\t\t\tfeed_dict = {\\\r\n\t\t\t\tself.ipemb_nl: dialogue_nl[u_id],\\\r\n\t\t\t\tself.ipemb_c: dialogue_coarse[u_id],\\\r\n\t\t\t\tself.tarlab_nl: dialogue_nl[u_id+1],\\\r\n\t\t\t\tself.tarlab_c: dialogue_coarse[u_id+1],\\\r\n\t\t\t\tself.oplen_nl: [len(dialogue_nl[u_id+1])],\\\r\n\t\t\t\tself.oplen_c: [len(dialogue_coarse[u_id+1])],\\\r\n\t\t\t\tself.prevcont_c: prev_coarse, \\\r\n\t\t\t\tself.prevcont_nl: prev_nl\\\r\n\t\t\t}\r\n\t\t\tprev_coarse,prev_nl=self.sess.run(\\\r\n\t\t\t\t\t(\\\r\n\t\t\t\t\t\tself.current_hidden_coarse_context,\\\r\n\t\t\t\t\t\tself.current_hidden_nl_context),\\\r\n\t\t\t\t\tfeed_dict=feed_dict)\r\n\t\t\tprev_coarse = [prev_coarse]\r\n\t\t\tprev_nl = [prev_nl]\r\n\t\t\t\r\n\t\tu_id = len(dialogue_nl)-1\r\n\t\tfeed_dict = {\\\r\n\t\t\tself.ipemb_nl: dialogue_nl[u_id],\\\r\n\t\t\tself.ipemb_c: dialogue_coarse[u_id],\\\r\n\t\t\tself.prevcont_c: prev_coarse, \\\r\n\t\t\tself.prevcont_nl: prev_nl,\\\r\n\t\t\tself.oplen_nl: [max_nl_generation],\\\r\n\t\t\tself.oplen_c: [max_coarse_generation]\\\r\n\t\t}\r\n\t\t\r\n\t\tw_logits = self.sess.run(self.logits_nl,feed_dict=feed_dict)\r\n\t\tprediction = np.zeros([w_logits.shape[0]],dtype=int)\r\n\t\t\r\n\t\tfor i in xrange(len(prediction)):\r\n\t\t\tprobs = np.exp(w_logits[i])\r\n\t\t\tprobs /= probs.sum()\r\n\t\t\tprediction[i] = np.random.choice(self.nlvocab,p=probs)\r\n\t\treturn prediction \r\n\t\t\r\n\r\n\r\n\tdef save(self,file_path):\r\n\t\tself.saver.save(self.sess,file_path)\r\n\r\n\tdef restore(self,file_path):\r\n\t\tself.saver.restore(self.sess,file_path)\r\n\r\n\r\n\tif __name__ == \"__main__\":\r\n\r\n\t\tconfig = Configuration()\r\n\t\tconfig.nlvocab = 11\r\n\t\tconfig.cvocab = 11\r\n\t\tconfig.end_of_nl_utt = 10\r\n\t\tconfig.end_of_coarse_utt = 10\r\n\r\n\t\t# dummy input\r\n\t\tx_w = [ [ 3,5,8,3,10,2,3,5,9,9,10,8,7,6,1,0,10] ]\r\n\t\tx_z = [ [ 2,4,1,2,7,8,10,1,6,8,8,10,9,0,0,1,10] ]\r\n\t\tx_w_test = [ [3,5,8,3,10],[2,3,5,9,9,10] ] \r\n\t\tx_z_test = [ [2,4,1,2,7,8,10],[1,6,8,8,10] ]\r\n\r\n\t\tmodel = MRRNN(config)\r\n\t\tfor i in xrange(1000):\r\n\t\t\tmodel.partial_fit(x_w,x_z)\r\n\t\t\tloss = model.cost(x_w,x_z)\r\n\t\t\tprint (loss)\r\n\t\t\tprediction = model.generate(x_w_test,x_z_test,5,6)\r\n\t\t\t# should learn to predict the sequence [8,7,6,1,1,10]\r\n\t\t\tprint (prediction)\r\n\t\t\t\r\n\t\t\t\r\n\t\r\n\t\t\t\r\n\t\t\r\n\t'''\r\n\tKey points:\r\n\t- MrRNN is an advancement to the HRED model (proposed by Sordoni et al)\r\n\t- Utternace refers to dialogue in sequence ; it is , in turn , a sequence of words\r\n\t'''\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\r\n\t`\r\n```", "Generate.py -\r\n\r\n```\r\n\r\nimport pickle\r\nfrom mrrnn import MRRNN\r\nfrom mrrnn import Configuration\r\n\r\n\r\n# tokenid's for end of utterance\r\nend_nl = 12575\r\nend_c = 15\r\n\r\nif __name__ == \"__main__\":\r\n\r\n\t# import dictionaries and test data\r\n\tnldict_path = \"./data/Dataset.dict.pkl\"\r\n\twith open(nldict_path,\"rb\") as file:\r\n\t\tnlvocab = pickle.load(file)\r\n\t\tnlvocab = sorted( nlvocab,key=lambda tup: tup[1] )\r\n\t\r\n\tcdict_path = \"./data/AE_Rep/abstract_dict.pkl\"\r\n\twith open(cdict_path,\"rb\") as file:\r\n\t\tcvocab = pickle.load(file)\r\n\t\tcvocab = sorted(cvocab,key=lambda tup: tup[1] )\r\n\r\n\t\t\r\n\tnltest_path = \"./data/Test.dialogues.pkl\"\r\n\twith open(nltest_path,\"rb\") as file:\r\n\t\tnltest = pickle.load(file)\r\n\t\t\r\n\tctest_path = \"./data/AE_Rep/abstract_test.pkl\"\r\n\twith open(ctest_path,\"rb\") as file:\r\n\t\tctest = pickle.load(file)\r\n\r\n\tconfig = Configuration()\r\n\tconfig.learning_rate = 0.0003\r\n\tconfig.nlvocab = len(nlvocab) \r\n\tconfig.cvocab = len(cvocab)\r\n\tconfig.end_of_word_utt = end_nl \r\n\tconfig.end_of_coarse_utt = end_c\r\n\t\r\n\r\n#the model\r\n\tmodel = MRRNN(config)\r\n\tN_dialogue = 10\r\n\r\n\tfile_name = \"./ckpts/training_5/trained.ckpt\"\r\n\tmodel.restore(file_name)\r\n\tnlDialog,cDialog = model.split_utterances([nltest[N_dialogue]],[ctest[N_dialogue]])\r\n\tprediction = model.generate(nlDialog[0][:-1],cDialog[0][:-1],20,20)\r\n\t# print nlDialog[0]\r\n\tfor curr_utt in xrange(len(nlDialog[0])-1):\r\n\t\tcurr_str = \"\"\r\n\t\tfor k in xrange(len(nlDialog[0][curr_utt])-1):\r\n\t\t\tcurr_str += nlvocab[nlDialog[0][curr_utt][k]][0] + \" \"\r\n\t\t\tprint (curr_str)\r\n\r\n\tcurr_str = \"\"\r\n\tfor k in xrange(len(prediction)):\r\n\t\tcurr_str += nlvocab[prediction[k]][0] + \" \"\r\n\t\tprint (curr_str)\r\n```", "Train.py -\r\n\r\n```\r\n`import random\r\nimport pickle\r\nfrom mrrnn import MRRNN\r\nfrom mrrnn import Configuration\r\n\r\n\r\n\r\n# tokenid's for end of utterance\r\nend_nl = 12575\r\nend_c = 15\r\nif __name__ == \"__main__\":\r\n\r\n#Importing training , testing , validation and dictionary data\r\n\r\n\tnltrain_path = \"./data/Training.dialogues.pkl\"\r\n\twith open(nltrain_path,\"r\") as file:\r\n\t\tnltrain = pickle.load(file)\r\n\t\t\r\n\tctrain_path = \"./data/abstract.train.dialogues.pkl\"\r\n\twith open(ctrain_path,\"r\") as file:\r\n\t\tctrain = pickle.load(file)\r\n\r\n\t\t\r\n\tnltest_path = \"./data/Test.dialogues.pkl\"\r\n\twith open(nltest_path,\"r\") as file:\r\n\t\tnltest = pickle.load(file)\r\n\t\t\r\n\tctest_path = \"./data/abstract.test.dialogues.pkl\"\r\n\twith open(ctest_path,\"r\") as file:\r\n\t\tctest = pickle.load(file)\r\n\r\n\t\t\r\n\tnlvalid_path = \"./data/Validation.dialogues.pkl\"\r\n\twith open(nlvalid_path,\"r\") as file:\r\n\t\tnlvalid = pickle.load(file)\r\n\t\t\r\n\tcvalid_path = \"./data/abstract.valid.dialogues.pkl\"\r\n\twith open(cvalid_path,\"r\") as file:\r\n\t\tcvalid = pickle.load(file)\r\n\r\n\t\t\r\n\tnldict_path = \"./data/Dataset.dict.pkl\"\r\n\twith open(nldict_path,\"r\") as file:\r\n\t\tnlvocab = pickle.load(file)\r\n\t\tnlvocab = sorted( nlvocab,key=lambda tup: tup[1] )\r\n\t\r\n\tcdict_path = \"./data/abstract.dict.pkl\"\r\n\twith open(cdict_path,\"r\") as file:\r\n\t\tcvocab = pickle.load(file)\r\n\t\tcvocab = sorted(cvocab,key=lambda tup: tup[1] )\r\n\t\t\r\n\t\t\r\n\tconfig = Configuration()\r\n\tconfig.learning_rate = 0.0003\r\n\tconfig.nlvocab = len(nlvocab) #to be changed in mrrnn\r\n\tconfig.cvocab = len(cvocab)\r\n\tconfig.end_of_word_utt = end_nl \r\n\tconfig.end_of_coarse_utt = end_c\r\n\t\r\n\r\n#the model\r\n\tmodel = MRRNN(config)\r\n\r\n\tbatchSize = 80\t\r\n\tprint_every = 1\r\n\tn_train = len(nltrain)\r\n\tn_epochs = 5\r\n\tmaxtrain_id = n_train - batchSize\r\n\r\n\tsaveDir = \"./ckpts/training_5/\"\r\n\tfileRestore = \"./ckpts/training_5/trained.ckpt\"\r\n\tfileName = \"./ckpts/training_5/trained.ckpt\"\r\n\r\n\tif fileRestore:\r\n\t\tmodel.restore(fileRestore)\r\n\r\n\tloss = model.cost(nlvalid[:100],cvalid[:100])\r\n\tprint (loss)\r\n\r\n\tfor ep in xrange(n_epochs):\r\n\t\tstart_id = 0\r\n\t\titer = 0\r\n\t\twhile start_id < maxtrain_id:\r\n\t\t\tend_id = start_id + batchSize\r\n\t\t\tmodel.partial_fit(nltrain[start_id:end_id],ctrain[start_id:end_id])\r\n\t\t\t\r\n\t\t\tif not (iter % print_every):\r\n\t\t\t\tloss = model.cost(nlvalid[:10],cvalid[:10])\r\n\t\t\t\tmodel.save(fileName)\r\n\t\t\t\tprint (\"{0} of {1}: {2}\".format(start_id,n_train,loss))\r\n\t\t\t\r\n\t\t\tstart_id += batchSize\r\n\t\t\titer += 1\r\n`\r\n```", "@Aashit-Sharma Can you minimize this to a small standalone example that exhibits the problem?\r\n\r\nSorry for the back-and-forth.  But note that our policy on github issues is that it must be a bug or a feature request.  Without a small standalone example, it's impossible for me to know whether this is an actual bug.", "If you refer to my 1st and 2nd reply , where i mention of the main decoder function and in the following comment , of its sub-functions and placeholders (and graph variables) that is all the required stuff for the main function BUT the graph function does refer to other functions in the code to which it is related.\r\n```\r\n`def nl_decoder(self):\r\n\t\t\r\n\t\twith tf.variable_scope(\"NL_Decoder\"):\r\n\t\t\tself.decoder_nl_cell = tf.contrib.rnn.GRUCell(self.nl_dec_size)\r\n\t\t\t\r\n\t\t\tself.iter_decoder_nl = tf.constant(0)\r\n\t\t\tself.iter_decoder_nl_test = tf.constant(0)\r\n\t\t\t\r\n\t\t\ttrain_decoder = tf.while_loop(self.nl_decoder_condition,self.nl_decoder_function, [ self.iter_decoder_nl, tf.zeros([1,self.nl_dec_size]) , tf.ones([self.nl_dec_size])],\\\r\n\t\t\tshape_invariants = [self.iter_decoder_nl.get_shape(), tf.TensorShape([None,self.nl_dec_size]),\\\r\n\t\t\ttf.TensorShape([self.nl_dec_size])])\r\n\r\n\t\treturn train_decoder[1][1:]\r\n        \r\n def nl_decoder_condition(self,it,outputs,hidden):\r\n\t\t\treturn it[0] < self.oplen_nl[0]\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\n\t\t# This is the step where we concatenate hidden state with context hidden state \t\t\t\r\n\r\ndef nl_decoder_function(self,it,outputs,hidden):\r\n\t\t\tout,hidden = self.decoder_nl_cell(tf.stack([tf.concat(0,[outputs[-1,:],self.nl_context_concat])]) , tf.stack([hidden]))\r\n\t\t\t\r\n\t\t\toutputs = tf.concat(0,[outputs,out])\r\n\t\t\treturn it+1,outputs,hidden[0,:]\r\n\r\n            \r\nself.oplen_nl = tf.placeholder(tf.int32,[1],name=\"nl_oplen\")\r\n\r\n#this comes in the graph function\r\nself.nl_context_concat = tf.concat(0,[self.current_hidden_nl_context,self.encoded_prediction])`\r\n\r\n```\r\n\r\nThis can't be avoided as this is an implementation of HRED , which essentially involves an encoder , a context and a decoder RNN\r\n\r\nThe feed into the decoder RNN comes from context , to which comes from encoder .\r\n\r\n\r\nI think if you take a look at the command prompt log , you can do with just glancing through the code to specific areas. \r\n\r\nessentially these lines , where the slicehelper is referenced , which references the stridedslice instead of general slice method (which requires 3 params)\r\n\r\n  ```\r\nFile \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2576, in _BuildLoop\r\n    c = ops.convert_to_tensor(pred(*packed_vars))\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 538, in _SliceHelper\r\n    name=name)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 706, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n\r\n  File \"C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 5429, in strided_slice\r\n    name=name) \r\n```\r\n\r\nessentially it automatically references to strided slice instead of just tf.slice.  Just have a look at shape_invariants and the code around it in the decoder function\r\n\r\nFeel free to ask me any questions or correct me if I am wrong", "Can you try with TensorFlow 1.5? It looks like this code has changed, so maybe it's fixed. If not, can you please send us the relevant code/stack traces again?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 16278, "title": "__init__() got multiple values for argument 'strides'", "body": "    model = Sequential()\r\n    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))\r\n    print(model.output_shape)\r\n    model.add(Convolution2D(64, 3, 3, strides=(\r\n        1, 1), activation='relu', name='conv1_1'))\r\nabove is my code, I got error:\r\n__init__() got multiple values for argument 'strides'\r\nIf i don't use 'strides', it's fine. but the stride is 3. How should I set strides?", "comments": ["from tensorflow.contrib.keras.python.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\r\nfrom tensorflow.contrib.keras.python.keras import optimizers\r\nfrom tensorflow.contrib.keras.python.keras.models import Sequential\r\nfrom tensorflow.contrib.keras.python.keras.layers.convolutional import Convolution2D, ZeroPadding2D\r\nfrom tensorflow.contrib.keras.python.keras.layers.pooling import MaxPooling2D\r\nfrom tensorflow.contrib.keras.python.keras.layers.core import Activation, Dropout, Flatten, Dense\r\nfrom tensorflow.contrib.keras.python.keras import callbacks\r\nfrom tensorflow.contrib.keras.python.keras import backend as K\r\n\r\nThe version of tensorflow is 1.1.0", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16277, "title": "Control dependency does not ensure write observed by read", "body": "TF version 1.3.0\r\n\r\n```python\r\ndef sleep(t):\r\n    '''TF sleep'''\r\n    import time\r\n    def f(t):\r\n        time.sleep(t)\r\n        return np.array([], dtype=np.float32)\r\n    return tf.py_func(f, [t], [tf.float32])[0]\r\n\r\nwith tf.device('gpu'):\r\n    x = tf.Variable(0.)\r\nwith tf.control_dependencies([tf.identity(sleep(0.1))]):\r\n    with tf.device('gpu'):\r\n        mod = tf.assign(x, 100.)\r\nwith tf.device('cpu'):\r\n    a = x+1.\r\n    with tf.control_dependencies([tf.identity(mod)]):\r\n        b = x+2.\r\n        with tf.device('gpu'):\r\n            c = x+3.\r\n\r\nx.initializer.run()\r\nsess.run([a, b, c])\r\n# [1.0, 2.0, 103.0]\r\n```\r\n\r\nWhen a variable is read on another device, TF seems to copy once regardless of dependencies. I understand this is how TF works, but I think it would be nice to have dependencies ensure memory access order.", "comments": ["You are right. To ensure correct order, use:\r\n```python\r\nimport tensorflow.contrib.eager as tfe\r\nx = tfe.Variable(0.)\r\n```\r\nUse `tfe.Variable` instead of `tf.Variable`.\r\nSee #15077", "Thanks for the comment @ppwwyyxx!\r\n\r\nClosing this issue.  Please read #15077 and related issues for details."]}, {"number": 16275, "title": "minor spelling tweaks for lite docs", "body": "", "comments": []}, {"number": 16274, "title": "R1.4", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "1.4 has already been merged back into master."]}, {"number": 16273, "title": "Add test for float16 support with conv1d and update docstring", "body": "The float16 support for conv1d has already been in place. However there was no test for float16 with conv1d. This fix adds test case to cover float16 support with conv1d.\r\n\r\nIn addition, float64 support with conv1d is not possible because conv1d calls conv2d, which in turn does not support float64 yet (See #12941, #13097, and #12943)\r\nThe previous docstring incorrectly claimed float64 support with conv1d.\r\n\r\nThis fix also updates related docstring to remove float64 part.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16272, "title": "Add a rnn example on mnist dataset using tf library", "body": "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\r\nThis example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\r\nLinks:\r\n    [Long Short Term Memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\r\n    [MNIST Dataset](http://yann.lecun.com/exdb/mnist/).\r\n\r\nTraining and evaluation log:\r\nExtracting /tmp/mnist_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/mnist_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz\r\nStep 1, Minibatch Loss= 2.5586, Training Accuracy= 0.258\r\nStep 200, Minibatch Loss= 0.2419, Training Accuracy= 0.930\r\nStep 400, Minibatch Loss= 0.1863, Training Accuracy= 0.938\r\nStep 600, Minibatch Loss= 0.1000, Training Accuracy= 0.969\r\nStep 800, Minibatch Loss= 0.0935, Training Accuracy= 0.977\r\nStep 1000, Minibatch Loss= 0.0773, Training Accuracy= 0.969\r\nStep 1200, Minibatch Loss= 0.0500, Training Accuracy= 0.984\r\nStep 1400, Minibatch Loss= 0.0550, Training Accuracy= 0.977\r\nStep 1600, Minibatch Loss= 0.0615, Training Accuracy= 0.984\r\nStep 1800, Minibatch Loss= 0.0635, Training Accuracy= 0.977\r\nStep 2000, Minibatch Loss= 0.0550, Training Accuracy= 0.992\r\nTraining Finished!\r\nTesting Accuracy: 0.992188", "comments": ["@drpngx ptal?", "@drpngx Thanks for your review, updated a new iteration. Could you please kindly have a look?", "@drpngx could you please kindly have a look?", "I think the code is great for a tutorial. Perf and accuracy might not matter much.\r\n/CC @MarkDaoust just checking if we're good with adding that.", "@MarkDaoust could you please kindly have a check whether we're good to add this as drpngx suggested?", "@MarkDaoust could you please take look? Thanks.", "Hi, sorry about the delay here.\r\n\r\nIn general we're hoping to reduce the number of examples in the main repo (the [tensorflow/tensorflow/examples/](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples) directory.)\r\n\r\nIdeally we'd put new examples over in the models repo (under [tensorflow/models/samples](https://github.com/tensorflow/models/tree/master/samples/)).\r\n\r\nThe sub directories there are:\r\n\r\n    cookbook/ - For various api examples.\r\n    core/ - For examples used in featured docs on the site.\r\n    languages/ - For examples in other languages.\r\n    outreach/ - For examples used in blogs and stuff.\r\n\r\nWe haven't totally nailed down the directory structure of \"cookbook\" there yet. Let's copy [api_guides](https://www.tensorflow.org/api_guides/python/contrib.bayesflow.monte_carlo), and use a flat layout with dotted name paths to each module.\r\n\r\nSo, can you make a directory for this example in [tensorflow/models](https://github.com/tensorflow/models) under `tensorflow/models/samples/nn.rnn_cell/mnist_rnn/` and we'll go from there?", "Move this PR to [models](https://github.com/tensorflow/models), here is the PR: [3262](https://github.com/tensorflow/models/pull/3262). @MarkDaoust could you please help take a look?"]}, {"number": 16271, "title": "tf.pow(x, y) edge case with negative x (Bug)", "body": "I am using tf.pow for my project, but my losses are 'nan', so I setup the test cases as shown below.\r\nI found that whenever x is negative, tf.pow seems to output nan instead of the correct answer.\r\n\r\n>>> r = tf.pow(0.4,0.4)\r\n>>> r2 = tf.pow(-0.4,-0.4)\r\n>>> r3 = tf.pow(0.4,-0.4)\r\n>>> r4 = tf.pow(-0.4,0.4)\r\n>>> sess.run(r)\r\n0.69314486\r\n>>> sess.run(r2)\r\nnan\r\n>>> sess.run(r3)\r\n1.4426999\r\n>>> sess.run(r4)\r\nnan\r\n\r\nI appreciate for anyone of the community who can address this issue.\r\n\r\nRespectfully,", "comments": ["I don't think this is an error with TF.\r\n\r\n(-.4)^(-.4) and (-.4)^(.4) will not give answers in the range of real numbers so NaN is a reasonable result. To convince yourself of why, try to reason out what tf.pow(-1, .5) should print? Or put it more simply, what is the square root of -1?", "Closing as this is working as intended as @EugenHotaj indicates. For your, model you can deifne things such that you make sure you never take a power of a negative base. This can be accomplished in a number of ways... the simplest being\r\n\r\nmath.pow(math.max(epsilon, a) , b)\r\nwhere epsilon is some small constant like 1e-3 to make sure you don't make something 0 either (because 0^negative is NaN as well). You can ask on StackOverflow if you have more concrete questions about how to make your model more numerically robust.\r\nGood luck!", "But tensorflow does support complex number, right? \r\nI can even create variable type as this:<tf.Tensor 'Exp:0' shape=() dtype=complex128>\r\nsess.run(y3)\r\n(1.8369701987210297e-16+3j)\r\n\r\nI guess my question now is since tensorflow does support complex number, why does it output NaN?\r\nSome operations in tf does support, and some does not? If possible I wish this issue to still be open, so people can work on the operations that does not support complex numbers.", "@stefanjuang The numbers you are using are float, not complex. define them as complex in order to get correct results.", "I think there are still some not expected results. For instance, the cubic root of a negative number exists always and is negative. However, tf.pow(-27., 1/3) will output nan. To get the expected result for any float number x, you need to write tf.sign(x)*tf.pow(tf.abs(x), 1/3). In the realm of complex numbers, there are other considerations to make, but when dealing with reals is simply like that. "]}, {"number": 16270, "title": "udacity assignment 1 & 2", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->"]}, {"number": 16269, "title": "ContentTooShortError: <urlopen error retrieval incomplete: got only 246506328 out of 247336696 bytes>", "body": "I have started UDACITY deep learning course.\r\nI was copying the assignment 1 codes then I got the this error during downloading the notMNIST_large.tar.gz\r\nHere are some screenshots of the code.\r\nI am doing this on jupyter python3 notebook in ubuntu.\r\n\r\n![1](https://user-images.githubusercontent.com/25321783/35194207-ca56cdc6-fed5-11e7-9fe7-3232c3741689.png)\r\n![2](https://user-images.githubusercontent.com/25321783/35194208-cacbbc6c-fed5-11e7-9ff0-600527e4990a.png)\r\n![error](https://user-images.githubusercontent.com/25321783/35194209-cb3fbd6a-fed5-11e7-8162-9b999b6240bb.png)\r\n", "comments": ["Since this is custom code written by the professors teaching the udacity course, this is not the right forum. Please use the course forum, unless it is clear that it is a bug in TensorFlow. If you believe it to be a problem with the core TensorFlow library, please attach a text file of a reproducible test case.\r\n"]}, {"number": 16268, "title": "ValueError: Protocol message RewriterConfig has no \"layout_optimizer\" field.", "body": "I just start learning tensorflow object detection API.And now,I can use the train.py script to train my model,and use the eval.py script to evaluate normally,but when I use the export_inference_graph.py script to export .pb file,the following error occurred.my tf version is 1.4 and python version is 3.5,ubuntu14.04.thank you very much.\r\n\r\nTraceback (most recent call last):\r\n  File \"export_inference_graph.py\", line 110, in <module>\r\n    tf.app.run()\r\n  File \"/home/yj/anaconda3/envs/tensorflow3.4/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"export_inference_graph.py\", line 106, in main\r\n    FLAGS.output_directory)\r\n  File \"/home/yj/anaconda3/envs/tensorflow3.4/models/research/object_detection/exporter.py\", line 427, in export_inference_graph\r\n    input_shape, optimize_graph, output_collection_name)\r\n  File \"/home/yj/anaconda3/envs/tensorflow3.4/models/research/object_detection/exporter.py\", line 391, in _export_inference_graph\r\n    initializer_nodes='')\r\n  File \"/home/yj/anaconda3/envs/tensorflow3.4/models/research/object_detection/exporter.py\", line 72, in freeze_graph_with_def_protos\r\n    layout_optimizer=rewriter_config_pb2.RewriterConfig.ON)\r\nValueError: Protocol message RewriterConfig has no \"layout_optimizer\" field.\r\n\r\nI guess I installed a wrong protobuf version,but I tried 2.6.0 2.6.1 3.5.1,and the same error occured.", "comments": ["@jimo123 you might wanna refer to [this PR](https://github.com/tensorflow/models/pull/3106). I follow someone's advise over there. I changed `models/research/object_detection/exporter.py` line 71/72 from:\r\n```python\r\nrewrite_options = rewriter_config_pb2.RewriterConfig(\r\n          layout_optimizer=rewriter_config_pb2.RewriterConfig.ON)\r\n```\r\n to \r\n```python\r\nrewrite_options = rewriter_config_pb2.RewriterConfig()\r\n```\r\nIt worked ~~~ good luck!", "@Geoyi  thank you very much!! it does worked thanks again!!\r\n", "@Geoyi \r\nYou saved my day!", "@Geoyi : This saved my day too :)", " File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\models\\research\\object_detection\\export_inference_graph.py\", line 119, in <module>\r\n    tf.app.run()\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\models\\research\\object_detection\\export_inference_graph.py\", line 115, in main\r\n    FLAGS.output_directory, input_shape)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\exporter.py\", line 427, in export_inference_graph\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\exporter.py\", line 391, in _export_inference_graph\r\n    _write_frozen_graph(frozen_graph_path, frozen_graph_def)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\object_detection-0.1-py3.6.egg\\object_detection\\exporter.py\", line 72, in freeze_graph_with_def_protos\r\n    rewrite_options.optimizers.append('pruning')\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 484, in init\r\n    field = _GetFieldByName(message_descriptor, field_name)\r\n  File \"C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\", line 548, in _GetFieldByName\r\n    (message_descriptor.name, field_name))\r\nValueError: Protocol message RewriterConfig has no \"layout_optimizer\" field.\r\n\r\n\r\n\r\nI'm still getting this error :(\r\n", "@lekshmi17\r\nme too getting the same error \r\ndid you figured it out?"]}, {"number": 16267, "title": "Fix compilation error and warnings with CUDA=0", "body": "There is a compilation error when compiling verbs with CUDA=0. This is rarely used, but we still want it to compile of course.\r\nAlso there are some compilation warnings that shouldn't exist. Mostly about 'unused functions' when setting/unsetting certain preprocessor defines.", "comments": []}, {"number": 16266, "title": "No OpKernel was registered to support Op 'RandomShuffleQueueV2' with these attrs. ", "body": "I build a graph.pb by python.\r\n\r\n```\r\n  classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\r\n                                          hidden_units=[60, 60, 60],\r\n                                          n_classes=numbTrainClass,\r\n                                          model_dir=os.path.curdir + \"/tmp/app_predict_mode1-12\")\r\n```\r\n\r\nWhen I load it on android, \r\n\r\n```\r\ntf = new TensorFlowInterface(mContext.getAssets(), mPbFileName);\r\n...\r\ntf.run(null).\r\n```\r\nThis is error.\r\n```\r\njava.lang.IllegalArgumentException: No OpKernel was registered to support Op 'RandomShuffleQueueV2' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[Node: enqueue_input/random_shuffle_queue = RandomShuffleQueueV2[capacity=1000, component_types=[DT_INT64, DT_FLOAT, DT_INT32], container=\"\", min_after_dequeue=250, seed=0, seed2=0, shapes=[[], [9], []], shared_name=\"\", _device=\"/device:CPU:0\"]()]]\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Thank! This problem is fixed!  But I still can't retrain weight of neural network in android.", "Closing because this is an inactive issue of unclear nature.  If you believe there is a bug in TensorFlow, please reopen with a clear description of the suspected bug.", "Hey guys,  @tangtaozhanshen @poxvoculi , I am facing the same issue mentioned above.\r\nBelow is the error:\r\n\r\n`java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'RandomShuffleQueueV2' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                                                                        <no registered kernels>\r\n\r\n                                                                      \t [[Node: enqueue_input/random_shuffle_queue = RandomShuffleQueueV2[capacity=1000, component_types=[DT_INT64, DT_FLOAT, DT_INT32], container=\"\", min_after_dequeue=250, seed=0, seed2=0, shapes=[[], [50,50], []], shared_name=\"\"]()]]\r\n                                                                          at org.tensorflow.Session.run(Native Method)\r\n                                                                          at org.tensorflow.Session.access$100(Session.java:48)\r\n                                                                          at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n                                                                          at org.tensorflow.Session$Runner.run(Session.java:248)\r\n                                                                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n                                                                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n                                                                          at csulb.beachhacks.asl.TensorflowImageClassifier.recognizeImage(TensorflowImageClassifier.java:136)`\r\n\r\n\r\nPlease let me know, how did you solve the issue. Or still, this issue is to be fixed. Thanks", "@karannaik  \r\n You can read this. May be the solution. \r\n[ print_selective_registration_header.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py)", "@tangtaozhanshen Hey, I have a mistake similar to yours. It reminds me that I don't have \"CTCLoss\". How do you solve your problem? About \"RandomShuffleQueueV2\"\r\n"]}, {"number": 16265, "title": "ImportError: cannot import name tf", "body": "### System information\r\n- **Os version**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from**: binary (sudo pip  install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp27-none-linux_x86_64.whl\r\n)\r\n- **TensorFlow version** :1.4.1\r\n- **Python version**: 2.7\r\n\r\n### Describe the problem\r\nAfter Install when i run the following command it throws error;\r\n`from tensorflow import tf`\r\n\r\nIt throws\r\n`Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nImportError: cannot import name tf\r\n`\r\n", "comments": ["I think you meant `import tensorflow as tf`", "@ushnish, thanks for the suggestion. @rtamizh, please close this issue if the suggestion resolves it. Thanks!"]}, {"number": 16264, "title": "No module named tensorflow.python.platform", "body": "### System information\r\n**OS**\r\nLinux 4.4.0-109-generic 132-Ubuntu SMP Tue Jan 9 19:52:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n**Tensorflow**\r\nInstalled from source (git clone from this repository):\r\ntf.VERSION = 1.5.0-rc1\r\ntf.GIT_VERSION = v1.5.0-rc1-1379-g20f6af3\r\ntf.COMPILER_VERSION = v1.5.0-rc1-1379-g20f6af3\r\nSanity check: array([1], dtype=int32)\r\n**Python version**\r\nPython 2.7.12\r\n**Bazel version **\r\nBuild label: 0.9.0\r\n**GCC/Compiler version**\r\ngcc (Ubuntu 5.4.1-2ubuntu1~16.04) 5.4.1 20160904\r\nnvcc Cuda compilation tools, release 9.1, V9.1.85\r\n**CUDA/cuDNN version**\r\nCUDA Version 9.1.85\r\ncuDNN Version 7.0.5\r\n**GPU model and memory**\r\nGeForce 1080 Ti 11Gb\r\nDriver Version: 387.26 \r\n**Exact command to reproduce**\r\n\r\n1. git clone https://github.com/tensorflow/tensorflow \r\n2. ./configure\r\n\r\n```\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nYou have bazel 0.9.0 installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python2.7/dist-packages\r\n  /usr/lib/python2.7/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]\r\n/usr/lib/python2.7/dist-packages\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n\r\nNo jemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: \r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n\r\nNo Amazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.1\r\n\r\n\r\nPlease specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.0.5\r\n\r\n\r\nPlease specify the location where cuDNN 7.0.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n```\r\n\r\n3. bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=\"LD_LIBRARY_PATH=${LD_LIBRARY_PATH}\"\r\n\r\n```\r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 1505.551s, Critical Path: 92.22s\r\nINFO: Build completed successfully, 6091 total actions \r\n```\r\n\r\n4. bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\n\r\n`Sun Jan 21 09:39:44 +04 2018 : === Output wheel file is in: /tmp/tensorflow_pkg`\r\n\r\n5. sudo pip install /tmp/tensorflow_pkg/tensorflow-1.5.0rc1-cp27-cp27mu-linux_x86_64.whl\r\n\r\n```\r\nInstalling collected packages: tensorflow-tensorboard, tensorflow\r\nSuccessfully installed tensorflow-1.5.0rc1 tensorflow-tensorboard-0.4.0\r\n```\r\n6.  start python & post 'import tensorflow as tf' command\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n```\r\n\r\n", "comments": ["It is simple - just change the folder from the one where you run the configure/bazel and then try python import tensorflow as tf", "no idea mate ) Basically it looks like you can use the tensorflow from any directory except the one where you run the build scripts. ", "@abratchik you are right\r\n"]}, {"number": 16263, "title": "losses.softmax_cross_entropy documentation on tensor rank", "body": "### Documentation Issue\r\nIt seems `losses.softmax_cross_entropy()` works just fine with any tensor rank/shape (as long as the last dimension is classes) and not just shapes of `[batch_size, num_classes]` as the documentation indicates. (The documentation also indicates weights should be rank 0 or 1; also not true)\r\n\r\nIf this is the expected behavior, fixing the documentation would help people avoid doing unnecessary reshapes.", "comments": ["@wolffg, could you look at fixing this?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Added a PR #17443 for the fix.", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @wolffg: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}]