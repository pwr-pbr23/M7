[{"number": 19261, "title": "tf windows dll could you help me", "body": "I want to use tensorflow on the windows platform. Why is it so hard?\r\n\r\nAccording to the official website's instructions, cmake came out of the vc project, but compiled a bunch of errors\r\n\r\ni use tf r1.2 \uff01 please help me", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "If you need Tensorflow on windows, someone compiled a bunch of Tensorflow wheels for Windows and they are available here: https://github.com/fo40225/tensorflow-windows-wheel", "\r\nMy development environment\uff1a\r\nHave I written custom code     NO\r\nOS Platform and Distribution  Win7\r\nTensorFlow installed from       github\r\nTensorFlow version                 r1.2\r\nBazel version                           \uff1f\r\nCUDA/cuDNN version             NO   just for cpu   \r\nGPU model and memory         NO\r\nExact command to reproduce  NO\r\n\r\nMSBuild /p:Configuration=Release ALL_BUILD.vcxproj\r\n\r\n\r\nI follow README.md in \\tensorflow-r1.2\\tensorflow\\contrib\\cmake, \r\nI follow exactly the instructions README.md\r\n\r\nEvery time the error is different, \r\nThis morning was such an error \u201c-- The Fortran compiler identification is unknown\u201d\r\nThe compiler does not continue to run down\r\n\r\nyesterday\uff0cerror is  ftt2d complied failed\u3001sometime\uff0czlib  complied faile\uff0c\r\nSometimes more than one hundred errors\r\nSometimes more than two hundred errors", "@henrysky   thank you so much, I just need tf r1.2, and just for cpp,\r\n", "@tensorflowbutler \r\n\r\nthis morning\uff0ci try again\r\n\r\nthe error\uff1a\r\n\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \u201ccmd.exe\u201d\u5df2\u9000\r\norflow\\co\r\nntrib\\cmake\\build\\gemmlowp.vcxproj]\r\n\u5df2\u5b8c\u6210\u751f\u6210\u9879\u76ee\u201cD:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\gemmlowp.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807)\u7684\u64cd\u4f5c - \u5931\u8d25\u3002\r\n\r\n\r\n\u751f\u6210\u5931\u8d25\u3002\r\n\r\n\u201cD:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\gemmlowp.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n(CustomBuild \u76ee\u6807) ->\r\n  C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\Common7\\IDE\\VC\\VCTargets\\Microsoft.CppCommon.targets(171,5): error MSB6006: \u201ccmd.exe\u201d\u5df2\r\nnsorflow\\\r\ncontrib\\cmake\\build\\gemmlowp.vcxproj]\r\n\r\n    0 \u4e2a\u8b66\u544a\r\n    1 \u4e2a\u9519\u8bef\r\n\r\n\u5df2\u7528\u65f6\u95f4 00:02:15.57\r\n\r\n\r\ni changed \r\nold:  75d40ea8e68b0d1644f052fffe8f14a410b2a73d40ccb859a95c0578d194ec26\r\n\r\nactual:  1e40863d9f15dd6b15f8f18f49c500944be6118d9fe17e9dc58a1c709cadbb8a\r\n\r\ngemmlowp.vcxproj  is OK\uff01\r\n\r\n", "He did talk about compiling C++ standalone shared-lib on Windows and he did uploaded one for TF1.8\r\n\r\nThe discussion: https://github.com/fo40225/tensorflow-windows-wheel/issues/6\r\nThe TF1.8 C++ standalone shared-lib: https://github.com/fo40225/tensorflow-windows-wheel/tree/master/1.8.0/cpp\r\n\r\nHope this information can help you as I have no idea how to do that", "@henrysky  \r\nthank you! i just need TF1.2", "cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=D:/all_lib/swigwin-3.0.12/swig.exe -DPYTHON_EXECUTABLE=D:/all_lib/Python36/python.exe -DPYTHON_LIBRARIES=D:/all_lib/Python36/libs/python36.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX -Dtensorflow_BUILD_ALL_KERNELS=ON -Dtensorflow_BUILD_CC_EXAMPLE=ON -Dtensorflow_BUILD_PYTHON_BINDINGS=OFF -Dtensorflow_BUILD_PYTHON_TESTS=OFF -Dtensorflow_ENABLE_GRPC_SUPPORT=ON -Dtensorflow_ENABLE_SSL_SUPPORT=OFF -Dtensorflow_ENABLE_GPU=OFF -Dtensorflow_BUILD_CC_TESTS=OFF -Dtensorflow_BUILD_SHARED_LIB=ON\r\n\r\n\r\nMSBuild /p:Configuration=Release ALL_BUILD.vcxproj\r\n\r\n\r\n\r\n\r\n\u201cD:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\_beam_search_ops.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (1) ->\r\n\u201cD:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj\u201d(\u9ed8\u8ba4\u76ee\u6807) (3) ->\r\n(Link \u76ee\u6807) ->\r\n  tf_session_helper.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyBytes_FromStringAndSize\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::CopyStringToPyArrayElement\r\n(struct tagPyAr\r\nrayObject *,void *,struct TF_Tensor *,__int64,__int64)\" (?CopyStringToPyArrayElement@?A0x10c2f664@tensorflow@@YA?AVStatus@2@PEAUtagPyArrayObject@@PEAXPEAUTF_Tensor@@_J3@Z) \u4e2d\u88ab\u5f15\u7528 [D:\\all_lib\\tensorfl\r\now-\r\nr1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  py_func.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyBytes_FromStringAndSize [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  pywrap_tensorflow_internal.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyBytes_FromStringAndSize [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  tf_session_helper.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyBytes_AsString\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::PyArrayDescr_to_TF_DataType(struct\r\n_PyArray_Descr\r\n*,enum TF_DataType *)\" (?PyArrayDescr_to_TF_DataType@?A0x10c2f664@tensorflow@@YA?AVStatus@2@PEAU_PyArray_Descr@@PEAW4TF_DataType@@@Z) \u4e2d\u88ab\u5f15\u7528 [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\r\n\\py\r\nwrap_tensorflow_internal.vcxproj]\r\n  pywrap_tensorflow_internal.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyBytes_AsString [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  tf_session_helper.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyBytes_AsStringAndSize\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::PyBytesArrayMap<class <lambd\r\na_4d99011b9cff9\r\n377914df5087ee03b23> >(struct tagPyArrayObject *,class <lambda_4d99011b9cff9377914df5087ee03b23>)\" (??$PyBytesArrayMap@V<lambda_4d99011b9cff9377914df5087ee03b23>@@@?A0x10c2f664@tensorflow@@YA?AVStatus\r\n@1@PEAUtagPyArrayObject@@V<lambda_4d99011b9cff9377914df5087ee03b23>@@@Z) \u4e2d\u88ab\u5f15\u7528 [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  py_func.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyBytes_AsStringAndSize [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  pywrap_tensorflow_internal.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyBytes_AsStringAndSize [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  tf_session_helper.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyUnicode_FromString\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::GetPyArrayDescrForTensor(struct\r\n TF_Tensor cons\r\nt *,struct _PyArray_Descr * *)\" (?GetPyArrayDescrForTensor@?A0x10c2f664@tensorflow@@YA?AVStatus@2@PEBUTF_Tensor@@PEAPEAU_PyArray_Descr@@@Z) \u4e2d\u88ab\u5f15\u7528 [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\r\n\\bu\r\nild\\pywrap_tensorflow_internal.vcxproj]\r\n  pywrap_tensorflow_internal.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyUnicode_FromString [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  tf_session_helper.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyUnicode_AsUTF8AndSize\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::PyBytesArrayMap<class <lambd\r\na_4d99011b9cff9\r\n377914df5087ee03b23> >(struct tagPyArrayObject *,class <lambda_4d99011b9cff9377914df5087ee03b23>)\" (??$PyBytesArrayMap@V<lambda_4d99011b9cff9377914df5087ee03b23>@@@?A0x10c2f664@tensorflow@@YA?AVStatus\r\n@1@PEAUtagPyArrayObject@@V<lambda_4d99011b9cff9377914df5087ee03b23>@@@Z) \u4e2d\u88ab\u5f15\u7528 [D:\\all_lib\\tensorflow-r1.2\\tensorflow\\contrib\\cmake\\build\\pywrap_tensorflow_internal.vcxproj]\r\n  tf_session_helper.obj : error LNK2019: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 __imp_PyUnicode_AsASCIIString\uff0c\u8be5\u7b26\u53f7\u5728\u51fd\u6570 \"class tensorflow::Status __cdecl tensorflow::`anonymous namespace'::PyArrayDescr_to_TF_DataType(\r\nstruct _PyArray", "example_trainer.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: void __cdecl google::protobuf::internal::LogFinisher::operator=(class google::protobuf::internal::LogMessage &)\" (??4LogFinisher@internal@protobuf@google@@QEAAXAEAVLogMessage@123@@Z)\r\n1>example_trainer.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"class google::protobuf::internal::ExplicitlyConstructed<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > google::protobuf::internal::fixed_address_empty_string\" (?fixed_address_empty_string@internal@protobuf@google@@3V?$ExplicitlyConstructed@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@123@A)\r\n1>example_trainer.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: class google::protobuf::internal::LogMessage & __cdecl google::protobuf::internal::LogMessage::operator<<(char const *)\" (??6LogMessage@internal@protobuf@google@@QEAAAEAV0123@PEBD@Z)\r\n1>example_trainer.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: __cdecl google::protobuf::internal::LogMessage::~LogMessage(void)\" (??1LogMessage@internal@protobuf@google@@QEAA@XZ)\r\n1>example_trainer.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"public: __cdecl google::protobuf::internal::LogMessage::LogMessage(enum google::protobuf::LogLevel,char const *,int)\" (??0LogMessage@internal@protobuf@google@@QEAA@W4LogLevel@23@PEBDH@Z)\r\n1>example_trainer.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"private: void __cdecl google::protobuf::Arena::AddListNode(void *,void (__cdecl*)(void *))\" (?AddListNode@Arena@protobuf@google@@AEAAXPEAXP6AX0@Z@Z)\r\n1>example_trainer.obj : error LNK2001: \u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u7b26\u53f7 \"private: void * __cdecl google::protobuf::Arena::AllocateAligned(class type_info const *,unsigned __int64)\" (?AllocateAligned@Arena@protobuf@google@@AEAAPEAXPEBVtype_info@@_K@Z)\r\n1>..\\..\\bin\\win_trainer.exe : fatal error LNK1120: 7 \u4e2a\u65e0\u6cd5\u89e3\u6790\u7684\u5916\u90e8\u547d\u4ee4\r\n\r\n\r\n\r\nthis sample  example_trainer.cc from \\tensorflow-r1.2\\tensorflow\\cc\\tutorials\r\n\r\nThe link library is also compiled using this source code. tensorflow-r1.2\r\n\r\nWhy did this link error occur?\r\n\r\nthank you\uff01", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19260, "title": "[Lite] tf.strided_slice sometimes computes wrong indices", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution**: Ubuntu 16.04.2 LTS\r\n- **TensorFlow installed from**: source\r\n- **TensorFlow version**: v1.8.0-1520-g1f03f82 1.8.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version**: 0.13.0\r\n\r\n### Problem\r\nI tried using the [TOCO tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/toco) on a graph that contains a [`strided_slice`](https://www.tensorflow.org/api_docs/python/tf/strided_slice) op.\r\nThe code determining the fixed size of this op, fails at an assertion and throws an error (see below).\r\n\r\n### Logs\r\n2018-05-14 00:42:56.816500: F [tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1305](https://github.com/tensorflow/tensorflow/blob/1f03f829285ca0fbd47a99350e9f5d99aa10e9b9/tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc#L1305)] Check failed: dim_size > 0 (-1 vs. 0)Output size for an axis must be greater than 0. Axis 0 computes to size -1 for StridedSlice op with output \"stft/frame/strided_slice\".\r\n\r\n### Minimum Reproducible Example\r\n[Source files](https://github.com/tensorflow/tensorflow/files/1999035/mre.zip)\r\n(set `TF_ROOT` in `freeze` and `toco`)\r\n```\r\n./mre.py\r\n./freeze\r\n./toco\r\n```\r\nThis produces a directory \"model\" with the graph, weights and frozen graph.\r\nThe offending error is thrown by last step.", "comments": ["The failing assertion was introduced in commit 83418120b7c2659fedddd7c85b65d3c3e6aa94e3\r\n(Fixing a bug in strided slice. The op was not handling negative indices correctly)", "Hi Robin! Thank you for reporting this issue. I've reached out to the SWE who added the assertion to see if they'd have any thoughts.", "I've attached the full source above. For convenience, here are the important snippets.\r\n\r\nModel:\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.signal as signal\r\n\r\nx = tf.placeholder(tf.float32, [None, 9152], name=\"x\")\r\ny = tf.placeholder(tf.int64, [None], name=\"y\")\r\n\r\nv = tf.Variable(1.)\r\n\r\nmagspec = signal.stft(x, frame_length, frame_step, num_frames, None)\r\nmagspec = tf.abs(magspec) * v\r\n\r\nfeatures = tf.expand_dims(magspec, 3)\r\n\r\ny_conv = tf.identity(features, name=\"y_conv\")\r\n```\r\nFreezing:\r\n```\r\nbazel-bin/tensorflow/python/tools/freeze_graph \\\r\n  --input_graph=model.pb \\\r\n  --input_checkpoint=model.ckpt \\\r\n  --output_graph=frozen_model.pb \\\r\n  --output_node_names=y_conv\r\n```\r\nTOCO tool:\r\n```\r\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=frozen_model.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --output_file=model.tflite \\\r\n  --inference_type=FLOAT \\\r\n  --input_type=FLOAT \\\r\n  --input_arrays=x \\\r\n  --output_arrays=y_conv \\\r\n  --input_shapes=1,9152\r\n```\r\n\r\nError message:\r\n2018-05-14 00:42:56.816500: F [tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:1305](https://github.com/tensorflow/tensorflow/blob/1f03f829285ca0fbd47a99350e9f5d99aa10e9b9/tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc#L1305)] Check failed: dim_size > 0 (-1 vs. 0)Output size for an axis must be greater than 0. Axis 0 computes to size -1 for StridedSlice op with output \"stft/frame/strided_slice\".", "In my local run, the inputs to strided_slice were begin=[1], end=[0], strides=[1]. The strided_slice op is called as a part of the stft op.\r\n\r\nWith these inputs, strided_slice should either error out or return an empty tensor (since end < begin, which is causing the check to fail). We should likely do the latter since tensorflow itself doesn't error out. \r\n\r\nI'm looping in rryan@ to see if these inputs to strided_slice make sense for stft and whether returning an empty tensor makes sense.\r\n\r\n\r\nI've attached my model and frozen graph for debugging.\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/2023519/model.zip)\r\n", "@rryan: do you think these inputs are expected for stft and that returning an empty tensor makes sense?", "@Androbin what frame_length / frame_step / num_frames are you using here?", "@rryan: the info is in his attached source files.\r\n\r\nnum_frames = 49\r\nframe_length = 512\r\nframe_step = 180\r\n\r\n", "Hm, if I'm not mistaken, `stft/frame/strided_slice` is from [this line](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/signal/python/ops/shape_ops.py#L114). Axis is always `-1` when called from `tf.contrib.signal.stft`, so this boils down to `tf.range(tf.rank(x))[-1]`.", "Does toco handle negative slice indices?", "@rryan Could not handling the negative index appropiately lead to the values reported by @alanchiao?\r\n> the inputs to strided_slice were begin=[1], end=[0], strides=[1]", "check in `tf.concat`:\r\n```\r\n// Negative axis means the count starts at the back of the dims().\r\nint axis = op->axis;\r\nif (axis < 0) axis += first_input_array.shape().dims().size();\r\n```\r\n\r\ncheck in `tf.stack`:\r\n```\r\nint axis = op->axis;\r\nif (axis < 0) {\r\n  // Handle negative axis\r\n  axis += stacked_shape->dims().size() + 1;\r\n}\r\n```\r\n\r\nIsn't the second one wrong?", "Regarding `tf.strided_slice`, TOCO seems to be generally aware of negative indices.\r\nBut here it expects (stop_index - start_index) and op->strides[axis] to be of the same sign.\r\n```\r\nint dim_size = ceil(static_cast<float>(stop_index - start_index) / op->strides[axis]);\r\nCHECK_GT(dim_size, 0)\r\n```\r\nThe values `begin=[1], end=[0], strides=[1]` should be either of these:\r\n* `begin=[0], end=[1], strides=[1]`\r\n* `begin=[1], end=[0], strides=[-1]`", "> int axis = op->axis;\r\n> if (axis < 0) {\r\n>   // Handle negative axis\r\n>   axis += stacked_shape->dims().size() + 1;\r\n> }\r\n> i\r\n> Isn't the second one wrong?\r\n\r\nIt confused me too but I think it's ok:\r\n \r\n* `tf.stack(x, axis=0)` -> `[len(x), d0, ..., dn]`\r\n* `tf.stack(x, axis=1)` -> `[d0, len(x), ..., dn]`\r\n* `tf.stack(x, axis=rank(x))` -> `[d0, ..., dn, len(x)]`\r\n* `tf.stack(x, axis=-1)` -> `[d0, ..., dn, len(x)]`\r\n\r\nSo for `axis=-1`,  the code computes `axis = rank(shape) + axis + 1` -> `rank(shape) + -1 + 1` -> `rank(shape)`, which is the same as the `tf.stack(x, axis=rank(x))` case.\r\n\r\n", "I think it should be the call to [tf.strided_slice]( https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/signal/python/ops/shape_ops.py#L161) instead.\r\n\r\nThe pbtxt file shows the name \"stft/frame/strided_slice\" attached to a StridedSlice operator and there isn't any other StridedSlice op in the pbtxt file.\r\n\r\nThis is [what Tensorboard shows for the frozen graph](https://user-images.githubusercontent.com/4323109/40336477-a3d89a88-5d1e-11e8-834f-7e11e32cc625.png) and we can see the output feeds into the reshape op, matching the code.\r\n\r\n```\r\n    subframes = array_ops.reshape(array_ops.strided_slice(\r\n        signal, array_ops.zeros_like(signal_shape),\r\n        slice_shape), subframe_shape)\r\n```\r\n\r\nThe **subframes** variable (output of reshape) then feeds into a GatherV2 op, which matches the code below.\r\n\r\n```\r\nframes = array_ops.reshape(\r\n        array_ops.gather(subframes, selector, axis=axis),\r\n        array_ops.concat([outer_dimensions, [num_frames, frame_length],\r\n                          inner_dimensions], 0)) \r\n```\r\n\r\n\r\n", "@alanchiao I believe that's `stft/frame/StridedSlice`, not `stft/frame/strided_slice`. \r\n\r\nThere are 2 StridedSlice ops created by `tf.contrib.signal.stft`: [example colab](https://colab.research.google.com/drive/11o_xv1_h1LK3QWQAmLMrwySRM3emudoz#scrollTo=67b2nLDw81sZ). Note that numpy-style slices are `StridedSlice` ops under the covers.\r\n\r\nHere's the op definition for `stft/frame/strided_slice` which is the node the error message refers to. Its first input is `stft/frame/range`, which is the result of `tf.range(tf.rank(x))`\r\n```\r\nnode {\r\n  name: \"stft/frame/strided_slice\"\r\n  op: \"StridedSlice\"\r\n  input: \"stft/frame/range\"\r\n  input: \"stft/frame/strided_slice/stack\"\r\n  input: \"stft/frame/strided_slice/stack_1\"\r\n  input: \"stft/frame/strided_slice/stack_2\"\r\n  attr {\r\n    key: \"Index\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_INT32\r\n    }\r\n  }\r\n  attr {\r\n    key: \"begin_mask\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"ellipsis_mask\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"end_mask\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"new_axis_mask\"\r\n    value {\r\n      i: 0\r\n    }\r\n  }\r\n  attr {\r\n    key: \"shrink_axis_mask\"\r\n    value {\r\n      i: 1\r\n    }\r\n  }\r\n}\r\n```\r\n", "Here's a minimal reproduction of the failure:\r\nIn `tensorflow/contrib/lite/kernels/strided_slice_test.cc`:\r\n```c++\r\nTEST(StridedSliceOpTest, In1D_ShrinkAxisMask1_NegativeSlice) {\r\n  // This is equivalent to tf.range(4)[-1].\r\n  StridedSliceOpModel<> m({4}, {1}, {1}, {1}, 0, 0, 0, 0, 1);\r\n  m.SetInput({1, 2, 3, 4});\r\n  m.SetBegin({-1});\r\n  m.SetEnd({0});\r\n  m.SetStrides({1});\r\n\r\n  m.Invoke();\r\n  EXPECT_TRUE(m.GetOutputShape().empty());\r\n  EXPECT_THAT(m.GetOutput(), ElementsAreArray({4}));\r\n}\r\n```\r\n\r\n```\r\n[ RUN      ] StridedSliceOpTest.In1D_ShrinkAxisMask1_NegativeSlice\r\nERROR: third_party/tensorflow/contrib/lite/kernels/strided_slice_test.cc:397 \r\nValue of: m.GetOutput()\r\nExpected: has 1 element that is equal to 4\r\n  Actual: { -1.21979e-12 }, whose element #0 doesn't match\r\nStack trace:\r\n0x55ebb4b9a8ab: tflite::(anonymous namespace)::StridedSliceOpTest_In1D_ShrinkAxisMask1_NegativeSlice_Test::TestBody() @ ??:??\r\n0x7f6e29315fad: void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) @ ??:??\r\n0x7f6e29307ad1: void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) @ ??:??\r\n0x7f6e292fa485: testing::Test::Run() @ ??:??\r\n0x7f6e292fabdf: testing::TestInfo::Run() @ ??:??\r\n... gUnit internal frames ...\r\n```", "If I execute the following slice, both TensorFlow's slicing operator and `strided_slice` return the correct empty list:\r\n```\r\nprint sess.run(tf.constant([1,2,3,4])[-1:0:1])\r\nprint sess.run(tf.strided_slice([1, 2, 3, 4], [-1], [0], [1]))\r\n[]\r\n[]\r\n```\r\nWhich agrees with python's slicing:\r\n```\r\nfoo=[1,2,3,4]\r\nprint foo[-1:0:1]\r\n[]\r\n```\r\nAll good.\r\n\r\nWhere it gets weird is adding in the shrink_axis flag on the first dimension:\r\n```\r\nprint sess.run(tf.strided_slice([1, 2, 3, 4], [-1], [0], [1], shrink_axis_mask=1))\r\n4\r\n```\r\nIf we try to do the equivalent in numpy:\r\n```\r\nfoo=np.array([1,2,3,4])\r\nprint np.shape(foo[-1:0:1])\r\nprint foo[-1:0:1].squeeze(axis=0)\r\n\r\n(0,)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-24-fb8485f97d3e> in <module>()\r\n      1 foo=np.array([1,2,3,4])\r\n      2 print np.shape(foo[-1:0:1])\r\n----> 3 print foo[-1:0:1].squeeze(axis=0)\r\n\r\nValueError: cannot select an axis to squeeze out which has size not equal to one\r\n```\r\nwe get an error, which I believe is the correct response. This is also stated in the [documentation](https://www.tensorflow.org/api_docs/python/tf/strided_slice) for `strided_slice`.\r\n\r\n> If the ith bit of shrink_axis_mask is set, it implies that the ith specification shrinks the dimensionality by 1. begin[i], end[i] and strides[i] must imply a slice of size 1 in the dimension.\r\n\r\nI could be wrong in my understanding of the equivalence of what `shrink_axis_mask` and `np.squeeze` are doing, and there are some strange subtleties to TensorFlow's handling of zero and one dimensional scalars, but on the surface of it, this looks like a bug in TensorFlow's implementation of `shrink_axis_mask` in `strided_slice`. Or maybe this was an explicit design decision for some reason?", "And I suspect the desired behavior on the `strided_slice` operator would be generated with `end_mask=1`.", "@mjmatthews \r\n\r\nYea, the `end` is just `start + 1` in this case, so it makes sense the interval is not length 1.\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L488\r\n\r\nWhich violates the requirements of shrink_axis_mask:\r\n```\r\nIf the ith bit of shrink_axis_mask is set, it implies that the ith specification shrinks \r\nthe dimensionality by 1. begin[i], end[i] and strides[i] must imply a slice of size 1 \r\nin the dimension. For example in Python one might do foo[:, 3, :] \r\nwhich would result in shrink_axis_mask equal to 2.\r\n```\r\n\r\nThere is a workaround implemented in C++ here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/strided_slice_op.cc#L290-L300\r\n```\r\n        // If we are shrinking, the end index is now possibly incorrect. In\r\n        // particular foo[-1] produces sparse_begin = -1, sparse_end = 0.\r\n        // and canonical puts these to n-1 and 0, which implies a degenerate\r\n        // interval. Fortunately, it is now safe to re-create end as begin+1.\r\n        int64 x_fwd = begin_i < 0 ? dim_i + begin_i : begin_i;\r\n        begin_i = x_fwd;\r\n        end_i = begin_i + 1;\r\n```\r\n\r\nIt's unfortunate, but tf.lite should probably follow suit, right?\r\n\r\n/cc: @aselle ", "Anyway, @Androbin I noticed you're using tf.contrib.signal.stft here -- note that tf.lite doesn't support complex numbers at the moment so once you get past this bug you'll still have issues. \r\n\r\nI'm in the process of adding complex64 and RFFT/IRFFT support to tf.lite and I'll be making sure tf.contrib.signal.stft works in the process. complex64 is currently out for code review, so I'm hopeful this can make it into the next TensorFlow release.", "Thanks for your efforts @rryan.\r\n\r\nSlightly off topic: I'm actually using `tf.abs( tf.contrib.signal.stft( x ) )`.\r\nSince complex numbers are a combination of absolute value and \"argument\":\r\nCan this be rewritten as a single op which doesn't require complex numbers?\r\nMaybe this would also be somewhat more battery efficient?", "When shrinking an axis there is only a single index not a begin and end, so the end being wrong shouldn't matter. It becomes a useless parameter. So yes, follow suite with the tf implementation. I think your work around is correct.", "Great, thanks @aselle -- I'll go ahead and send out my fix for review.", "> Slightly off topic: I'm actually using tf.abs( tf.contrib.signal.stft( x ) ).\r\n> Since complex numbers are a combination of absolute value and \"argument\":\r\n> Can this be rewritten as a single op which doesn't require complex numbers?\r\n> Maybe this would also be somewhat more battery efficient?\r\n\r\nHm, you always have to compute both the real and imaginary parts of the DFT in order to get the magnitude so I think even if there are no complex-valued tensors the op would be calculating it internally. You can factor the calculation into a real and imaginary part to avoid needing a complex type in tf.lite itself, but this will probably be less efficient than computing them jointly because the real and imaginary parts share the same memory access patterns when they're being computed. \r\n\r\nOne thing that might help is doing a fixed-point FFT instead of floating point. I haven't thought much about how to support that with TensorFlow's `RFFT` op, but it should be do-able.\r\n\r\nIf you're building a mobile algorithm that operates in a streaming fashion you probably want to pass in frames of audio at a time to your tf.lite model. In this situation, `tf.contrib.signal.stft` isn't going to be appropriate because it's going to frame the audio you pass in for you, and it doesn't support being run in a stateful manner where you're feeding it chunks of audio at a time. You may want to fall back on `tf.contrib.signal.hann_window` (or any window) and `tf.spectral.rfft` to window and compute the RFFT on the incoming frame you're processing in this scenario.", "Hm one could have the fft return a buffer where the last last dimension of\nthe tenosor is 2-dimensional so...\nreal = foo[..., 0]\nim = foo[..., 1]\nThen you wouldnmt' strictly need complex numbers. Then you could add\ntensors using the existing add, but you'd have to be careful using\noperations that weren't  decoupled  in real and imaginary behavior like *,\nbut you could do real part by doing then\n\ntf.sqrt(tf.reduce_add(foo * foo, axis=-1))\n-A\n\n\n\n\nOn Tue, Jun 12, 2018 at 10:18 AM RJ Skerry-Ryan <notifications@github.com>\nwrote:\n\n> Slightly off topic: I'm actually using tf.abs( tf.contrib.signal.stft( x )\n> ).\n> Since complex numbers are a combination of absolute value and \"argument\":\n> Can this be rewritten as a single op which doesn't require complex numbers?\n> Maybe this would also be somewhat more battery efficient?\n>\n> Hm, you always have to compute both the real and imaginary parts of the\n> DFT in order to get the magnitude so I think even if there are no\n> complex-valued tensors the op would be calculating it internally. You can\n> factor the calculation into a real and imaginary part to avoid needing a\n> complex type in tf.lite itself, but this will probably be less efficient\n> than computing them jointly because the real and imaginary parts share the\n> same memory access patterns when they're being computed.\n>\n> One thing that might help is doing a fixed-point FFT instead of floating\n> point. I haven't thought much about how to support that with TensorFlow's\n> RFFT op, but it should be do-able.\n>\n> If you're building a mobile algorithm that operates in a streaming fashion\n> you probably want to pass in frames of audio at a time to your tf.lite\n> model. In this situation, tf.contrib.signal.stft isn't going to be\n> appropriate because it's going to frame the audio you pass in for you, and\n> it doesn't support being run in a stateful manner where you're feeding it\n> chunks of audio at a time. You may want to fall back on\n> tf.contrib.signal.hann_window (or any window) and tf.spectral.rfft to\n> window and compute the RFFT on the incoming frame you're processing in this\n> scenario.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19260#issuecomment-396667159>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAT52t5k10pIYP9JE65lp3IpVxlt0Hkoks5t7_fUgaJpZM4T8_nX>\n> .\n>\n", "rryan@\r\nAgreed, that 's unfortunate. \r\n\r\naselle@\r\nWouldn't it be more consistent to change https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L488 to something like:\r\n```\r\n    else:\r\n      begin.append(s)\r\n      if s is -1:\r\n        # User is requesting the last element. (s + 1) will result in a\r\n        # degenerate interval, so we must use the end mask instead.\r\n        end.append(s)\r\n        end_mask |= (1 << index)\r\n      else:\r\n        end.append(s + 1)\r\n      strides.append(1)\r\n      shrink_axis_mask |= (1 << index)\r\n```\r\nRather than the workaround in the CC op? The argument being that the CC interface appears to be attempting to be as faithful to python's slicing semantics as possible. The limitation with it is that an end index must be given, unlike in python. To maintain the semantics, we could just use the end mask, which would be equivalent to\r\n```\r\nprint sess.run(tf.constant([1,2,3,4])[-1: :1])\r\n[4]\r\n```\r\nvs\r\n```\r\nprint sess.run(tf.constant([1,2,3,4])[-1:0:1])\r\n[]\r\n```\r\nI'd also be in favor in changing the CC strided_slice inputs to only accept canonical indices altogether. But that might lead to some difficult migration and backward compatibility issues.\r\n\r\nAt the very least if we leave this as is and change the TOCO code to reflect TF's workaround, we should update the documentation to say that the end index and strides are ignored when the shrink axis bit is set. \r\n", "Well I agree we should fix array_ops.cc too, but you don't just need to fix the -1 case right? you need to fix -2 because otherwise you'll mill -2 which also needs the same treatment. Also, you have to consider that s may be a tensor itself so you can't actually compare it to 1 with an if condition. So you may need to do this:\r\n```python\r\n  else:\r\n    strides.append(1)\r\n    begin.append(s)\r\n    const_s = tensor_util.constant_value(s)\r\n    if const_s is not None:\r\n      end.append(s+1 if s>=0 else s-1)\r\n    else:\r\n      end.append(tf.where(s>=0, s+1, s-1))\r\n    shrink_axis_mask |= (1 << index)\r\n```\r\n\r\n", "I believe we're ok in the -2 case. The existing code should produce:\r\n```\r\nprint sess.run(tf.constant([1,2,3,4])[-2:-1:1])\r\n[3]\r\n```\r\nI think the s-1 case your pointing out would be for a negative stride, ie:\r\n```\r\nprint sess.run(tf.constant([1,2,3,4])[0])\r\nprint sess.run(tf.constant([1,2,3,4])[0:-1:-1])\r\nprint sess.run(tf.constant([1,2,3,4])[0:  :-1])\r\n1\r\n[]\r\n[1]\r\n```\r\nbut since we pick the stride to be positive, we can always use s+1. These are the one case where the interval crosses zero, causing the logic to differ and requiring the use of end_mask. \r\n\r\nI was unaware of s possibly being a tensor. If we're computing on an element-wise basis, wouldn't we have to set the end_mask as well too? Not quite sure what that would look like.", "Don't know if this covers any more ground, but I believe that the TensorFlow implementation of tf.strided_slice ignores the end index that is passed if shrink axis is true, because just naively adding 1 doesn't work for -1 case:\r\n\r\n```\r\nfoo = [0,1,2]\r\nfoo[-1]  ==> 2\r\n```\r\n, where you'd get -1 + 1 = 0 which is the beginning and would be a degenerate interval.\r\n\r\nI make a comment here to ignore it here\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/strided_slice_op.cc#L131\r\n\r\nSpecial rules for shrinking are here (only stride 1 is allowed even though the op's parameter space allowed it, it couldn't generated with getitem):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/strided_slice_op.cc#L280\r\nAnd also \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/util/strided_slice_op.cc#L290\r\n\r\nSo it is documented in the comments, but it might be helpful to document in the spec and docstrings on both python and tflite side.\r\n\r\n", "Nagging Assignees @rryan, @alanchiao: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This should be fixed by https://github.com/tensorflow/tensorflow/commit/9bcbfc1636d98e99d821c1a5292a5fe70663ccdb."]}, {"number": 19259, "title": "Fix the naming of _any_variable_initialized", "body": "This might be a very small issue, though the naming `_any_variable_initalized` seems to be a typo (`initalized` -> `initialized`). Since this is an interanl function, renaming should be safe.\r\n\r\nThis fix change the naming to `_any_variable_initialized`\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19258, "title": "Update TFLite Docs on tf.gather", "body": "Support was added in ea703f4e0e72d1e016f8157e206dcc9e80602862", "comments": []}, {"number": 19257, "title": "Shape validation of `max_features` in `QuantizedReluX`", "body": "In shape function of QuantizedReluX, shapes of `max_value` and\r\n`min_features` have been validated  but not `max_features`.\r\nThis fix add restriction to `max_features` as well.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19256, "title": "[Docs] Update two Wikipedia links", "body": "", "comments": ["Can you update this on the master branch as well/instead?", "My bad. I had to get rid of some cherry-picked commits from 1.8 and rebase.", "Failure unrelated: `tensorflow/contrib/data/python/kernel_tests/batch_dataset_op_test`"]}, {"number": 19255, "title": "Android build fixes", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 19254, "title": "Add additional shape validation to `compute_accidental_hits`", "body": "In `compute_accidental_hits`, the `sampled_candidates` must\r\nbe a vector, as is shown in the kernel implementation in\r\n`tensorflow/core/kernels/candidate_sampler_ops.cc`.\r\n\r\nThis fix adds shape validation of `sampled_candidates`\r\nin the shape function whenever possible.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 19253, "title": "Entropy for multinomial distribution", "body": "Have I written custom code: No\r\nOS Platform and Distribution: N/A\r\nTensorFlow installed from: N/A\r\nTensorFlow version: 1.8.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: N/A\r\n\r\nCurrently there does not appear to be an implementation of entropy for the Multinomial distribution. Is there a reason for this?\r\n\r\n[Multinomial](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/distributions/multinomial.py) inherits from [Distribution](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/distributions/distribution.py). However, Multinomial does not have override the the dummy entropy function in the Distribution class\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19252, "title": "Prebuilt binary APK TFLCameraDemo does not Install (Error analyzing Package; Android 7 and 5.1.1)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I apologize, but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new). Please provide all the information it asks. Thank you.\r\n", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I tried to install using ADB (Platform ver 26.1.1), but I receive the error:\nFailure [INSTALL_FAILED_OLDER_SDK]\nMaybe it depends on my environment and not the APK.\nThanks anyway for the interest.\nGreetings\n\n2018-06-02 9:15 GMT+02:00 Alfred Sorten Wolf <notifications@github.com>:\n\n> It has been 15 days with no activity and the awaiting response label was\n> assigned. Is this still an issue?\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/19252#issuecomment-394065201>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AiOsPSJmRrpuq4BlNMmfP6dNPqIMWNPrks5t4juogaJpZM4T8uC8>\n> .\n>\n", "If you're still having trouble, please reopen with details of the OS, etc. as shown in the template. Thanks!"]}, {"number": 19251, "title": "Update momentum.py", "body": "making the code more readable", "comments": ["These line breaks are correct under google python style \r\nhttps://google.github.io/styleguide/pyguide.html\r\n\r\nwhich stipulates a 80-char line width limit.\r\n\r\nI'm closing this PR. If you have questions or concerns, please feel free to comment on or reopen this PR."]}, {"number": 19250, "title": "In eager model, tfe.Checkpoint does not restore Variables that are not instance properties of tf.keras.Model instance", "body": "## System information\r\n\r\n+ Have I written custom code : Yes\r\n+ OS Platform and Distribution: Win7 X64\r\n+ TensorFlow installed from (source or binary): binary\r\n+ TensorFlow version (use command below): 1.8.0.dev20180329\r\n+ Python version: 3.5\r\n+ Bazel version (if compiling from source): N/A\r\n+ GCC/Compiler version (if compiling from source):N/A\r\n+ CUDA/cuDNN version: N/A\r\n+ GPU model and memory: N/A\r\n+ Exact command to reproduce: N/A \r\n\r\n\r\n## Problem\r\n\r\nI wrote a Text CNN in eager model:\r\n```python\r\nclass TextCnn(tf.keras.Model):\r\n    def __init__(self):\r\n        ......\r\n        self.conv_funcs = [tf.layers.Conv2D(filter_num,\r\n                                            [filter_size, embedding_size],\r\n                                            activation=tf.nn.relu,\r\n                                            name='conv_{}'.format(filter_size))\r\n                           for filter_size, filter_num in filter_size_num_list]\r\n\r\n```\r\nI set the list of Conv2D layers as an instance property(self.conv_funcs), and when I restore the model by tfe.Checkpoint, the weights of the Conv2D layers are not restored.\r\n\r\nHowever, I add the following code and the tfe.Checkpoint successfully restore the weights of the Conv2D layers:\r\n```python\r\n        self.conv_2 = self.conv_funcs[0]\r\n        self.conv_3 = self.conv_funcs[1] \r\n```\r\n\r\nIs it a bug that tfe.Checkpoint only restore the weights of instance properties of tf.keras.Model?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I have updated the post.", "Checkpoint does not save/restore lists. You have to assign the Convolutions individually to self for now I think. @allenlavoie correct me if I am wrong. ", "Yeah, for now we [don't recurse into lists/dictionary members of a `tf.keras.Model`](https://github.com/tensorflow/tensorflow/blob/f8b74d642420dcf2f5cab763b41884a05777ea45/tensorflow/python/keras/_impl/keras/engine/network.py#L344), so you'll have to explicitly assign member variables. Figuring out the right public API is on our [TODO list](https://github.com/tensorflow/tensorflow/blob/f27033fb1212d7031a359c913d0f59e976b14c14/tensorflow/contrib/eager/python/examples/rnn_ptb/rnn_ptb.py#L77)", "Mutable built-in data structures (lists, dicts) are going to be tricky to support, since we'd need to hook mutations for restore-on-create (monkey patching?). I'm fine with supporting tuples, though, and I will add checkpointable lists and dictionaries to `tf.contrib.checkpoint`.", "Nagging Assignee @allenlavoie: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @allenlavoie: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19249, "title": "build tensorflow on tx2 with cuda8.0 and cudnn6.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Tx2\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.6, r1.7, r1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:.13.0- (@non-git)\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0 \r\n- **CUDA/cuDNN version**: 8.0 / 6.0\r\n- **GPU model and memory**:  Denver2 8GB\r\n- **Exact command to reproduce**: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n```\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::Eigen::half, int, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::Eigen::half, long long, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::Eigen::half, long long, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel<float, int, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel<float, int, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel<float, long long, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel<float, long long, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel<double, int, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel<double, int, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel<double, long long, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel<double, long long, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<float> , int, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<float> , int, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<float> , long long, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<float> , long long, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<double> , int, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<double> , int, (bool)0> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<double> , long long, (bool)1> \") is not allowed\r\n\r\n./tensorflow/core/kernels/gather_functor_gpu.cu.h(57): error: calling a __host__ function(\"__builtin_expect\") from a __global__ function(\"tensorflow::GatherOpKernel< ::std::complex<double> , long long, (bool)0> \") is not allowed\r\n\r\n20 errors detected in the compilation of \"/tmp/tmpxft_0000294e_00000000-9_gather_functor_gpu.cu.compute_52.cpp1.ii\".\r\nERROR: /home/nvidia/tensorflow/tensorflow/core/kernels/BUILD:1201:1: output 'tensorflow/core/kernels/_objs/gather_functor_gpu/tensorflow/core/kernels/gather_functor_gpu.cu.pic.o' was not created\r\nERROR: /home/nvidia/tensorflow/tensorflow/core/kernels/BUILD:1201:1: not all outputs were created or valid\r\n```\r\n\r\nand this is my configure settings\r\n\r\n```\r\nYou have bazel 0.13.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3.5\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/lib/python3.5/dist-packages\r\n  /usr/lib/python3/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: \r\njemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: \r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: \r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: \r\nAmazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: \r\nApache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 8\r\n\r\n\r\nPlease specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 6\r\n\r\n\r\nPlease specify the location where cuDNN 6 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:/usr/libaarch64-linux-gnu\r\n\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: \r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nPlease specify the NCCL version you want to use. [Leave empty to default to NCCL 1.3]: \r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n```\r\n", "comments": ["Same issue here #19203 ", "Same problem.\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): r1.7\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):.13.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source):5.4.0\r\n- CUDA/cuDNN version: 8.0 / 6.0\r\n- GPU model and memory: Tesla P100, 16GB\r\n- Exact command to reproduce: bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package", "Indeed. Closing as a duplicate."]}, {"number": 19248, "title": "Add int64 support for output_shape of tf.nn.conv3d_transpose", "body": "This fix tries to address the issue raised in #18887 where the output_shape of tf.nn.conv3d_transpose only support int32 data types. The support of int64 has been added in this PR with test case covered.\r\n\r\nThis fix fixes #18887.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks!"]}, {"number": 19247, "title": "Could TensorFlow support tf.nn.separable_conv1d in future release?", "body": "Dear TensorFlower:\r\n\r\nCurrently TensorFlow only has tf.nn.separable_conv2d support and it is convenient to use.\r\n\r\nCould it is possible in future release tf.nn.separable_conv1d be added? Since I am intensively using TensorFlow for processing bio-signals, so it is more attractive for TensorFlow having such support at this regard.\r\n\r\nBest regards\r\n\r\nThe following fields are filled due to request, since it is not a issue but more like a feature, not sure the information below is helpful or not.\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: RedHat 7.3\r\nTensorFlow installed from: Customer build from source code\r\nTensorFlow version: 1.8\r\nBazel version: 0.12\r\nCUDA/cuDNN version:9.1/7.0.5\r\nGPU model and memory: P5000/16G\r\nExact command to reproduce: N/A\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Yes, the required information has been provided in the original post, however it is posted here again:\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: RedHat 7.3\r\nTensorFlow installed from: Customer build from source code\r\nTensorFlow version: 1.8\r\nBazel version: 0.12\r\nCUDA/cuDNN version:9.1/7.0.5\r\nGPU model and memory: P5000/16G\r\nExact command to reproduce: N/A\r\n", "Thanks, @mingyr . tf.layers.separable_conv2d is just a wrapper around tf.layers.SeparableConv2D, and tf.layers.SeparableConv1D already exists: https://www.tensorflow.org/versions/master/api_docs/python/tf/layers/SeparableConv1D . Can I suggest you use the SeparableConv1D class directly?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19246, "title": "TensorFlow toco - convert from *.pb to *.tflite error", "body": "When try to convert *.pb model to *.tflite (for running on android) using toco converter of TensorFlow, I got the following error:\r\n\r\n2018-05-11 20:38:10.841641: I tensorflow/cc/saved_model/loader.cc:242] Loading SavedModel with tags: { serve }; from: /home/user_name/Downloads/model_directory\r\n2018-05-11 20:38:10.847390: I tensorflow/cc/saved_model/loader.cc:291] SavedModel load for tags { serve }; Status: fail. Took 6042 microseconds.\r\n2018-05-11 20:38:10.848098: F tensorflow/contrib/lite/toco/toco_saved_model.cc:50] **Non-OK-status:** tensorflow::LoadSavedModel(tensorflow::SessionOptions(), tensorflow::RunOptions(), model_path, tags, bundle) **status: Not found:** Could not find meta graph def matching supplied tags: { serve }. To inspect available tag-sets in the SavedModel, please use the SavedModel CLI: `saved_model_cli`Failed to load exported model from /home/user_name/Downloads/model_directory. **Ensure the model contains the required tags 'serve'.**\r\n\r\n**Any idea how to solve it?**\r\n\r\nMachine details:\r\nOS Platform and Distribution - ubuntu x64, \r\npython version is 3.5.2, \r\nTensorFlow installed from - pip, \r\nTensorFlow version - cpu version 1.8.0, \r\nBazel version - 0.13.0, \r\nCUDA/cuDNN version - no cuda, \r\nGPU model and memory - no gpu, \r\nExact command to reproduce - no need\r\n", "comments": ["I solved it by re-save the model with simple_save functoin. here is the  code:\r\n    \r\n    tf.reset_default_graph()\r\n    sess = tf.Session()\r\n    new_graph = tf.Graph()\r\n    with tf.Session(graph=new_graph) as sess:\r\n        saver = tf.train.import_meta_graph('model_name.meta', clear_devices=True)\r\n        saver.restore(sess, \"./model_name\")\r\n        export_path = 'path to create the new model'\r\n        shutil.rmtree(export_path)\r\n        tf.saved_model.simple_save(sess, export_path, inputs={\"x\": x}, outputs={\"y_pred\": y_pred})", "avielas@, it looks like you were able to figure out the fix yourself! I'm marking this issue as resolved."]}, {"number": 19245, "title": "An error occurred while starting the kernel in CNN runing", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Win 8.1\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  1.7.0 \r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**: anaconda\r\n- **CUDA/cuDNN version**: 9.0.0  \r\n- **GPU model and memory**:GTX 740m\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI can run NN code but i can't run CNN code.\r\nwhen i run a CNN code, the kernel suddenly dies. I see this error:\r\n|An error occurred while starting the kernel|\r\n\r\n```python\r\n### Source code / logs\r\nfrom keras.datasets import mnist\r\ndef plot_history(net_history):\r\nhistory = net_history.history\r\nimport matplotlib.pyplot as plt\r\nlosses = history['loss']\r\nval_losses = history['val_loss']\r\naccuracies = history['acc']\r\nval_accuracies = history['val_acc']\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Loss')\r\nplt.plot(losses)\r\nplt.plot(val_losses)\r\nplt.legend(['loss', 'val_loss'])\r\nplt.figure()\r\nplt.xlabel('Epochs')\r\nplt.ylabel('Accuracy')\r\nplt.plot(accuracies)\r\nplt.plot(val_accuracies)\r\nplt.legend(['acc', 'val_acc'])\r\n#========================Load data\r\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n#================Data attributes\r\nprint(\"train_images dimentions: \", train_images.ndim)\r\nprint(\"train_images shape: \", train_images.shape)\r\nprint(\"train_images type: \", train_images.dtype)\r\n\r\nX_train = train_images.reshape(60000, 784)\r\nX_test = test_images.reshape(10000, 784)\r\n\r\nX_train = X_train.astype('float32')\r\nX_test = X_test.astype('float32')\r\n\r\nX_train /= 255\r\nX_test /= 255\r\n\r\nfrom keras.utils import np_utils\r\nY_train = np_utils.to_categorical(train_labels)\r\nY_test = np_utils.to_categorical(test_labels)\r\n\r\n#==============Creating our model====================================\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout\r\nfrom keras.optimizers import SGD\r\nfrom keras.losses import categorical_crossentropy\r\n\r\nmyModel = Sequential()\r\nmyModel.add(Dense(500, activation='relu', input_shape=(784,)))\r\nmyModel.add(Dropout(20))\r\nmyModel.add(Dense(100, activation='relu'))\r\nmyModel.add(Dropout(20))\r\nmyModel.add(Dense(10, activation='softmax'))\r\n\r\nmyModel.summary()\r\nmyModel.compile(optimizer=SGD(lr=0.001), loss=categorical_crossentropy, metrics=['accuracy'])\r\n\r\n#=====Train our model=============================================\r\nnetwork_history = myModel.fit(X_train, Y_train, batch_size=128, epochs=20, validation_split=0.2)\r\nplot_history(network_history)\r\n\r\n#=============Evaluation========================================\r\ntest_loss, test_acc = myModel.evaluate(X_test, Y_test)\r\ntest_labels_p = myModel.predict(X_test)\r\nimport numpy as np\r\ntest_labels_p = np.argmax(test_labels_p, axis=1)\r\n\r\n#=========== Change layers config================\r\nmyModel.layers[0].name = 'Layer_0'\r\nmyModel.layers[0].trainable = False\r\nmyModel.layers[0].get_config()\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow version\nBazel version\nExact command to reproduce", "yes\r\ni need still help.", "@fathi1989,\r\nExtremely sorry for the delayed response. Can you please confirm if your issue is resolved. Thanks! ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@fathi1989,\r\nCan you please respond to the above comment. Thanks! ", "Closing as stale. Please reopen if you'd like to work on this further.\n", "@fathi1989,\r\nThe code should work fine if you use Tensorflow GPU. Thanks! "]}, {"number": 19244, "title": "tf.contrib.data.prefetch_to_device not compatible with tf.data.Iterator.from_structure", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\ntensorflow-gpu binary\r\n- **Bazel Version**:\r\nN/A\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: \r\n3.6.3\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0 cuDNN 7.0.3\r\n- **GPU model and memory**:\r\nGTX 1070 8 GB VRAM\r\n- **Exact command to reproduce**:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nclass MyData(object):\r\n    def __call__(self):\r\n         return range(100)\r\n\r\nexpected_shapes = []\r\nexpected_types = tf.int32\r\niterator = tf.data.Iterator.from_structure(output_types=expected_types, output_shapes=expected_shapes)\r\ndataset = tf.data.Dataset.from_generator(MyData(), output_types=expected_types, output_shapes=expected_shapes)\r\n\r\nprefetch_op = tf.contrib.data.prefetch_to_device(device=\"/gpu:0\")\r\ndataset = dataset.apply(prefetch_op)\r\ninitializer = iterator.make_initializer(dataset)\r\n```\r\n\r\n### Describe the problem\r\n\r\nThis raises `NotImplementedError: prefetch_to_device() must be the last transformation in a dataset pipeline`. \r\n\r\nIt is not possible to apply this to the dataset after the initializer has been created, since a new dataset is returned, instead of it being modified in place.\r\n\r\nIf one reads through [this testcase](https://github.com/tensorflow/tensorflow/commit/4681562607bf4001ecd61492f1e7567be9212c6f), it is clear that it works when creating the iterator from the dataset.\r\n\r\nIt is  **not** clear from [the documentation of `make_initializer`](https://www.tensorflow.org/api_docs/python/tf/data/Iterator#make_initializer) that this function is a transformation of the dataset and thus counts as an additional step after prefetching. \r\nI am not sure if this is a bug/was overlooked, or is known to be not implemented.\r\n\r\n**Proposed short term solution**:\r\n1. Mention in the [documentation of `prefetch_to_device`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/data/prefetch_to_device), that it is not supported in combination with `make_initializer`.\r\n2. Mention in the documentation of `make_initializer` that this operation modifies the dataset\r\n(although I don't think this is the correct choice of words, the issue is with a call to `dataset._as_variant_tensor` in [`make_initializer` line 308](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/iterator_ops.py)).\r\n\r\n**Proposed longterm solution**:\r\n1. _This is already a TODO in line 289 of [prefetching_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/prefetching_ops.py)_:\r\nImplement `_as_variant_tensor` for `_PrefetchToDeviceDataset`.\r\n\r\n### Reason why this is needed:\r\n\r\nCreating the data pipeline using `from_structure` and  `make_initializer` allows to dynamically switch the input source to the network, e.g. between training and testing set after an epoch without having to reinitialize the graph or fall back to using feed dicts.\r\n\r\n### Source code / logs\r\n\r\nExact stack trace of the error:\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 14, in <module>\r\n    initializer = iterator.make_initializer(dataset)\r\n  File \"/home/veith/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 308, in make_initializer\r\n    dataset._as_variant_tensor(), self._iterator_resource, name=name)  # pylint: disable=protected-access\r\n  File \"/home/veith/.pyenv/versions/3.6.3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/prefetching_ops.py\", line 291, in _as_variant_tensor\r\n    raise NotImplementedError(\"`prefetch_to_device()` must be the last \"\r\nNotImplementedError: `prefetch_to_device()` must be the last transformation in a dataset pipeline.\r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes, this is still an issue.", "cc @mrry, I think the authority on this issue.", "@rohan100jain is working on a solution to this.", "Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @rohan100jain: It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Please use CopyToDevice + Prefetch instead of using prefetch_to_device directly. \r\n\r\nfor example\r\n\r\nds = ...\r\nds = ds.apply(prefetching_ops.copy_to_device(\"/gpu:0\")).prefetch(1)\r\n\r\nThis should give you a regular dataset and all the other iterator / dataset support that comes with it."]}, {"number": 19243, "title": "Add missing deps for simd_armv8a in jpeg.BUILD", "body": "Building for armv8 fails with error:\r\n\r\nERROR: .../bazel/external/jpeg/BUILD:288:1: undeclared inclusion(s) in rule '@jpeg//:simd_armv8a':\r\nthis rule is missing dependency declarations for the following files included by 'external/jpeg/simd/jsimd_arm64.c':\r\n'.../bazel/external/jpeg/jpegint.h'\r\n'.../bazel/external/jpeg/jerror.h'", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Crap, forgot a comma -.-", "@AD-530 no worries :-) Just update the PR and we'll run the tests again.", "@AD-530 can you fix the PR?", "Unfortunately that's not possible because the PR was created directly through the Github web interface. So the repo is \"unknown\" and it's not possible to push further changes. I'll create a regular PR"]}, {"number": 19242, "title": "Can TFLite treat input/output tensors as flat buffers just like TFMobile used to?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**:  3.5.4\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**:  feature?\r\n\r\n### Describe the problem\r\nTF Mobile allows treating input/output as flat arrays (e.g. float[]). Why does the TF Lite design deviate? Moreover, TF Lite treats inputs differently from outputs which feels odd, especially when chaining models.\r\n\r\nIs there a way to get the old behavior back? Essentially, treating input/output tensor as flat buffers (appropriately sized) enables abstracting over the exact network input/output shapes in app code, making it more reusable. One can think of the network + shape as an existential package, which is quite convenient.\r\n\r\n### Source code / logs\r\n[TF Lite Android example](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/end_of_first_codelab/android/tflite/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L103) requires instantiating `float[][]` which is hard-coding network output shape into the app. \r\n", "comments": ["You can pass a bytebuffer to TFlite using the Java API.  Can you please try this and let us know ?", "Nagging Assignee @petewarden: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 19241, "title": "[Eager] Fix for determining input / output shape of the model prior to Model.fit()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home Edition\r\n- **TensorFlow installed from (source or binary)**: Windows binary\r\n- **TensorFlow version (use command below)**: Github version 'v1.8.0-0-g93bc2e2072' 1.8.0TF (GPU) \r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0 / cuDNN 7.0.5\r\n- **GPU model and memory**: NVIDIA GTX 980M\r\n- **Exact command to reproduce**: Provided below as a standalone script\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThe issue is that in Eager mode, the two attributes of the Model subclass, `inputs` and `outputs` is undetermined until the first call to `Model.fit(...)`.\r\n\r\nWhen attempting to determine this prior to fitting the model, in the script at location **tensorflow/python/keras/_impl/keras/engine/training.py**, the entire input dataset `X` (passed to .fit(...)) is provided as the input `x` in Line 684 `if not self.inputs: self._set_inputs(x)` inside `_standardize_user_data`.\r\n\r\nDue to this, in eager execution mode, this call is deferred to `_eager_set_inputs(inputs)`. Here, `inputs` is the entire dataset numpy matrix / tensor, and a `Model.call(inputs)` is performed at line 909. \r\n\r\nSince the entire dataset is unable to fit in GPU memory for smaller GPU devices, it causes an OOM error.\r\n\r\nHowever, to determine the input / output shape/s of a model, a single sample tensor is sufficient.\r\n\r\nThe below fix shows that the solution is adequate, and can be implemented by simply extracting a single sample of the entire dataset to determine the input / output shape/s of a model during eager execution.\r\n\r\n**Note : **\r\nIn order to provide indicators to identify where the error occurs, I modified the above mentioned script to print 2 logs to the console, to describe the shape of the \"inputs\" parameter inside `_eager_set_inputs(inputs)`. The lines `Inside training._eager_set_inputs ...` and `***** Providing entire dataset into call *****` are the above logs.\r\n\r\n### Source code / logs\r\n\r\n```python\r\nimport os\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.keras.datasets import mnist\r\nfrom tensorflow.contrib.eager.python import tfe\r\n\r\n# enable eager mode\r\ntf.enable_eager_execution()\r\ntf.set_random_seed(0)\r\nnp.random.seed(0)\r\n\r\n# constants\r\nbatch_size = 128\r\nepochs = 10\r\nnum_classes = 10\r\n\r\n# dataset loading\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train = x_train.astype('float32') / 255.\r\nx_test = x_test.astype('float32') / 255.\r\nx_train = x_train.reshape((-1, 28, 28, 1))\r\nx_test = x_test.reshape((-1, 28, 28, 1))\r\n\r\n# one hot encode the labels. convert back to numpy as we cannot use a combination of numpy\r\n# and tensors as input to keras\r\ny_train_ohe = tf.one_hot(y_train, depth=num_classes).numpy()\r\ny_test_ohe = tf.one_hot(y_test, depth=num_classes).numpy()\r\n\r\nprint('x train', x_train.shape)\r\nprint('y train', y_train_ohe.shape)\r\nprint('x test', x_test.shape)\r\nprint('y test', y_test_ohe.shape)\r\n\r\nclass CNN(tf.keras.Model):\r\n\r\n    def __init__(self, num_classes):\r\n        super(CNN, self).__init__()\r\n\r\n        self.cnn1 = tf.keras.layers.Conv2D(16, (5, 5), padding='same', strides=(2, 2))\r\n        self.bn1 = tf.keras.layers.BatchNormalization()\r\n        self.cnn2 = tf.keras.layers.Conv2D(32, (5, 5), padding='same', strides=(2, 2))\r\n        self.bn2 = tf.keras.layers.BatchNormalization()\r\n        self.pool = tf.keras.layers.GlobalAveragePooling2D()\r\n        self.classifier = tf.keras.layers.Dense(num_classes)\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        # Used to print out the input shape of the entire dataset prior to training loop\r\n        print(inputs.shape)\r\n\r\n        x = self.cnn1(inputs)\r\n        x = self.bn1(x)\r\n        x = tf.nn.relu(x)  # layer 1\r\n        x = tf.nn.relu(self.bn2(self.cnn2(x))) # layer 2\r\n        x = self.pool(x)\r\n        output = self.classifier(x)\r\n\r\n        # softmax op does not exist on the gpu, so always use cpu\r\n        with tf.device('/cpu:0'):\r\n            output = tf.nn.softmax(output)\r\n\r\n        return output\r\n\r\n\r\ndevice = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\r\n\r\nwith tf.device(device):\r\n    # build model and optimizer\r\n    model = CNN(num_classes)\r\n    model.compile(optimizer=tf.train.AdamOptimizer(0.001), loss='categorical_crossentropy',\r\n                  metrics=['accuracy'])\r\n    \r\n    # suggested fix ; can be incorporated inside `_eager_set_inputs` or `_set_input`\r\n    # Fix = Use exactly one sample from the provided input dataset to determine \r\n    # input/output shape/s for the model\r\n\r\n    # dummy_x = np.zeros((1, 28, 28, 1))\r\n    # model._set_inputs(dummy_x)\r\n\r\n    # train\r\n    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\r\n              validation_data=(x_test, y_test_ohe), verbose=2)\r\n\r\n    # evaluate on test set\r\n    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=2)\r\n    print(\"Final test loss and accuracy :\", scores)\r\n\r\n```\r\n\r\nTruncated log without the fix : (Only tensor allocation summary dump after OOM is truncated)\r\n```python\r\n2018-05-12 00:22:44.208127: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-05-12 00:22:45.385242: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 980M major: 5 minor: 2 memoryClockRate(GHz): 1.1265\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.32GiB\r\n2018-05-12 00:22:45.385822: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-12 00:22:45.838116: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-12 00:22:45.838626: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:929]      0 \r\n2018-05-12 00:22:45.838912: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:942] 0:   N \r\n2018-05-12 00:22:45.839762: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3050 MB memory) -> physical GPU (device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\nx train (60000, 28, 28, 1)\r\ny train (60000, 10)\r\nx test (10000, 28, 28, 1)\r\ny test (10000, 10)\r\n\r\nInside training._eager_set_inputs ip shape/s [(60000, 28, 28, 1)]  <= Here the input is the entire dataset\r\n***** Providing entire dataset into call *****\r\n(60000, 28, 28, 1)\r\n\r\n2018-05-12 00:22:58.125845: W T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 717.77MiB.  Current allocation summary follows.\r\n\r\n<<< Truncated >>>\r\n\r\n2018-05-12 00:22:58.229153: W T:\\src\\github\\tensorflow\\tensorflow\\core\\framework\\op_kernel.cc:1318] OP_REQUIRES failed at fused_batch_norm_op.cc:263 : Resource exhausted: OOM when allocating tensor with shape[60000,16,14,14] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\nTraceback (most recent call last):\r\n  File \"D:/Users/Yue/PycharmProjects/eager-tutorials/tutorials/04_cnn.py\", line 83, in <module>\r\n    validation_data=(x_test, y_test_ohe), verbose=2)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\", line 1147, in fit\r\n    batch_size=batch_size)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\", line 685, in _standardize_user_data\r\n    self._set_inputs(x)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\", line 871, in _set_inputs\r\n    self._eager_set_inputs(inputs)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\", line 914, in _eager_set_inputs\r\n    ops.convert_to_tensor(inputs, dtype=K.floatx()))\r\n  File \"D:/Users/Yue/PycharmProjects/eager-tutorials/tutorials/04_cnn.py\", line 55, in call\r\n    x = self.bn1(x)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\base_layer.py\", line 314, in __call__\r\n    output = super(Layer, self).__call__(inputs, *args, **kwargs)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 717, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\layers\\normalization.py\", line 113, in call\r\n    output = super(BatchNormalization, self).call(inputs, training=training)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 501, in call\r\n    outputs = self._fused_batch_norm(inputs, training=training)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 396, in _fused_batch_norm\r\n    training, _fused_batch_norm_training, _fused_batch_norm_inference)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\utils.py\", line 206, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\", line 56, in smart_cond\r\n    return false_fn()\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\normalization.py\", line 393, in _fused_batch_norm_inference\r\n    data_format=self._data_format)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py\", line 904, in fused_batch_norm\r\n    name=name)\r\n  File \"D:\\Users\\Yue\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 3804, in _fused_batch_norm\r\n    _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[60000,16,14,14] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:FusedBatchNorm]\r\n\r\n```\r\n\r\nPartial log with the aforementioned fix\r\n```python\r\n2018-05-12 00:27:23.607020: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2018-05-12 00:27:24.748056: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1356] Found device 0 with properties: \r\nname: GeForce GTX 980M major: 5 minor: 2 memoryClockRate(GHz): 1.1265\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.32GiB\r\n2018-05-12 00:27:24.748771: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-05-12 00:27:25.196921: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-12 00:27:25.197231: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:929]      0 \r\n2018-05-12 00:27:25.197570: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:942] 0:   N \r\n2018-05-12 00:27:25.197993: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3050 MB memory) -> physical GPU (device: 0, name: GeForce GTX 980M, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\nx train (60000, 28, 28, 1)\r\ny train (60000, 10)\r\nx test (10000, 28, 28, 1)\r\ny test (10000, 10)\r\n\r\nInside training._eager_set_inputs ip shape/s [(1, 28, 28, 1)]  <= Here the \"dataset\" is the dummy batch\r\n***** Providing entire dataset into call *****\r\n(1, 28, 28, 1)\r\n\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/10\r\n(128, 28, 28, 1)\r\n(128, 28, 28, 1)\r\n(128, 28, 28, 1)\r\n(128, 28, 28, 1)\r\n(128, 28, 28, 1)\r\n... repeated for the entire training loop over the batch.\r\n```\r\n", "comments": ["@alextp can you PTAL?", "@anj-s I think this is in code you wrote", "I have a very similar, but it is related to saving a custom model as well.\r\nI built a custom layer that was applied in a custom model as follows\r\n```\r\nclass C3BR(tf.keras.Model):\r\n    ''' 3D Convolution + Batch Normalisation + Relu '''\r\n    def __init__(self, filterNum, kSize, strSize, padMode):\r\n        super(C3BR, self).__init__()\r\n        self.conv = layers.Conv3D(filters=filterNum, kernel_size=kSize, strides=strSize, padding=padMode, data_format='channels_first')\r\n        self.BN = layers.BatchNormalization(axis=1)\r\n    \r\n    def call(self, inputs, ifTrain):\r\n        x = self.conv(inputs)\r\n        if ifTrain == True:\r\n            x= self.BN(x)\r\n        return activations.relu(x)\r\n        \r\nclass SimpleUNet(tf.keras.Model):\r\n    \"\"\"\r\n    Serialise basic units so as to build up a double-layered encoder-decoder U-Net\r\n    Input:\r\n        inDim: (for initialisation) [modaility/channel, tensor dimensions]\r\n        classNum: background included\r\n        name: name for the net\r\n        inputs: 5D tf tensor of [mbSize, modaility/channel, tensor dimensions]. Inputs must be organised into channel first order\r\n    Returns:\r\n        outputs: 5D tf tensor of [mbSize, classNum, tensor dimensions]\r\n    \"\"\"\r\n    def __init__(self, inDim, classNum, name='SimpleUNet', **kwarg):\r\n        super(SimpleUNet, self).__init__(name=name, **kwarg)\r\n#    def __init__(self, inDim, classNum, name='SimpleUNet', **kwargs):        \r\n#        super(SimpleUNet, self).__init__(name=name, **kwargs)\r\n        self.inDim = inDim\r\n        self.classNum = classNum\r\n        dimEnSt1End = np.array(inDim)[1:]-2-2\r\n        dimEnSt2Ed = dimEnSt1End/2-2-2\r\n        dimBridgeEnd = (dimEnSt2Ed/2-2-2)*2\r\n        dimDEStd1End = (dimBridgeEnd-2-2)*2\r\n        self.outDim = dimDEStd1End-2-2-2\r\n        temp = ((dimEnSt2Ed - dimBridgeEnd)/2).astype('int32')\r\n        crop3d1 = tuple(np.tile(temp, (2, 1)).T)\r\n        temp = ((dimEnSt1End - dimDEStd1End)/2).astype('int32')\r\n        crop3d2 = tuple(np.tile(temp, (2, 1)).T)\r\n\r\n        self.en_st1_cbr1 = C3BR(32, 3, 1, 'valid')\r\n        self.en_st1_cbr2 = C3BR(64, 3, 1, 'valid')\r\n        self.en_st2_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')\r\n        self.en_st2_cbr1 = C3BR(64, 3, 1, 'valid')\r\n        self.en_st2_cbr2 = C3BR(128, 3, 1, 'valid')\r\n        self.bridge_mp = layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding='valid', data_format='channels_first')\r\n        self.bridge_cbr1 = C3BR(128, 3, 1, 'valid')\r\n        self.bridge_cbr2 = C3BR(256, 3, 1, 'valid')    \r\n        self.bridge_tconv1 = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')\r\n        self.de_3dcrop1 = layers.Cropping3D(crop3d1, data_format='channels_first')\r\n        self.de_st1_cbr1 = C3BR(256, 3, 1, 'valid')\r\n        self.de_st1_cbr2 = C3BR(128, 3, 1, 'valid')    \r\n        self.de_st1_tconv1 = layers.Conv3DTranspose(128, 2, strides=2, padding='valid', data_format='channels_first')\r\n        self.de_3dcrop2 = layers.Cropping3D(crop3d2, data_format='channels_first')\r\n        self.de_st2_cbr1 = C3BR(64, 3, 1, 'valid')\r\n        self.de_st2_cbr2 = C3BR(64, 3, 1, 'valid') \r\n        self.final_conv3D = layers.Conv3D(filters=self.classNum, kernel_size=3, strides=1, padding='valid', data_format='channels_first')\r\n                \r\n    @tf.function\r\n    def call(self, inputs, ifTrain=True):\r\n        # At present, do not use x = layers.Input(shape=self.inDim), because loads of issues about calling .numpy(), which even\r\n        # exist in tf's own functions\r\n        x = self.en_st1_cbr1(inputs, ifTrain)\r\n        xEnSt1End = self.en_st1_cbr2(x, ifTrain)\r\n        x= self.en_st2_mp(xEnSt1End)\r\n        x= self.en_st2_cbr1(x, ifTrain)\r\n        xEnSt2Ed = self.en_st2_cbr2(x, ifTrain)\r\n        x = self.bridge_mp(xEnSt2Ed)        \r\n        x = self.bridge_cbr1(x, ifTrain)\r\n        x = self.bridge_cbr2(x, ifTrain)      \r\n        xBridgeEnd = self.bridge_tconv1(x)\r\n        xCrop1 = self.de_3dcrop1(xEnSt2Ed)\r\n# printing information, that is where tf raises an error message\r\n        print(xBridgeEnd.shape)\r\n        print(xCrop1.shape)\r\n        x = layers.concatenate([xBridgeEnd, xCrop1], axis=1)\r\n        x = self.de_st1_cbr1(x, ifTrain)\r\n        x = self.de_st1_cbr2(x, ifTrain)\r\n        xDeSt1End = self.de_st1_tconv1(x)\r\n        xCrop2 = self.de_3dcrop2(xEnSt1End)\r\n        x = layers.concatenate([xDeSt1End, xCrop2], axis=1)\r\n        #print(x.shape)\r\n        x = self.de_st2_cbr1(x, ifTrain)\r\n        x = self.de_st2_cbr2(x, ifTrain)\r\n        x = self.final_conv3D(x)\r\n        outputs = activations.softmax(x, axis=1)\r\n        \r\n        return outputs\r\n        \r\n    def compute_output_shape(self):\r\n        # Override this function if you want to use the subclassed model in Kera's fit() method.\r\n        # Otherwise, this method is optional.\r\n        return tf.TensorShape(np.append(self.classNum, self.outDim))    \r\n\r\n```\r\n\r\nThen  I set it up and tried to use a pair of dummy tensors to set its inputs before saving the model\r\n```\r\ninDim = (4, 64, 64, 64)\r\nclassNum = 2\r\nTUNet1 = SimpleUNet(inDim, classNum)\r\nx=tf.random.uniform((16, 4, 64, 64, 64))\r\ny=tf.zeros((16, 2, 22, 22, 22), dtype=uint8)\r\nTUNet1._set_inputs(x, outputs=y)\r\n````\r\nSo far, it went well. but when I further call\r\n```\r\nTUNet1.save(r'...\\filename', save_format='tf')\r\n```\r\n\r\nThe following output is given\r\n```\r\n(None, 256, 18, 18, 18)\r\n(None, 128, 18, 18, 18)\r\n(None, 256, 18, 18, 18)\r\n(None, 128, 18, 18, 18)\r\n(None, 256, None, None, None)\r\n(None, 128, 18, 18, 18)\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-88-e56e75f05a97>\", line 1, in <module>\r\n    TUNet1.save(r'D:\\Work\\LiVision\\Tech\\Data\\NN\\Study_TF\\name\\weight', save_format='tf')\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\", line 975, in save\r\n    signatures, options)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\", line 115, in save_model\r\n    signatures, options)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save.py\", line 74, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\", line 870, in save\r\n    checkpoint_graph_view)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\signature_serialization.py\", line 64, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\", line 141, in list_functions\r\n    self._serialization_cache)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\", line 2422, in _list_functions_for_serialization\r\n    .list_functions_for_serialization(serialization_cache))\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\base_serialization.py\", line 91, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\layer_serialization.py\", line 79, in functions_to_serialize\r\n    serialization_cache).functions_to_serialize)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\layer_serialization.py\", line 94, in _get_serialized_attributes\r\n    serialization_cache)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\model_serialization.py\", line 53, in _get_serialized_attributes_internal\r\n    serialization_cache))\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\layer_serialization.py\", line 103, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save_impl.py\", line 166, in wrap_layer_functions\r\n    '{}_layer_call_and_return_conditional_losses'.format(layer.name))\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save_impl.py\", line 494, in add_function\r\n    self.add_trace(*self._input_signature)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save_impl.py\", line 413, in add_trace\r\n    trace_with_training(True)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save_impl.py\", line 411, in trace_with_training\r\n    fn.get_concrete_function(*args, **kwargs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save_impl.py\", line 538, in get_concrete_function\r\n    return super(LayerCall, self).get_concrete_function(*args, **kwargs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 776, in get_concrete_function\r\n    self._initialize(args, kwargs, add_initializers_to=initializer_map)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 408, in _initialize\r\n    *args, **kwds))\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1848, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2150, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2041, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 915, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 358, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save_impl.py\", line 515, in wrapper\r\n    ret = method(*args, **kwargs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save_impl.py\", line 475, in wrap_with_training_arg\r\n    return call_fn(*args, **kwargs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save_impl.py\", line 557, in call_and_return_conditional_losses\r\n    return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2658, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n\r\n  File \"D:\\TProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\", line 905, in wrapper\r\n    raise e.ag_error_metadata.to_exception(e)\r\n\r\nValueError: in converted code:\r\n    relative to D:\\:\r\n\r\n    ork\\LiVision\\Tech\\Product\\Alg\\Alg-Python\\MedImgProc\\MedNN.py:87 call  *\r\n        x = layers.concatenate([xBridgeEnd, xCrop1], axis=1)\r\n    ProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\merge.py:705 concatenate\r\n        return Concatenate(axis=axis, **kwargs)(inputs)\r\n    ProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:817 __call__\r\n        self._maybe_build(inputs)\r\n    ProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py:2141 _maybe_build\r\n        self.build(input_shapes)\r\n    ProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\tf_utils.py:306 wrapper\r\n        output_shape = fn(instance, input_shape)\r\n    ProgramFiles\\Anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\merge.py:391 build\r\n        'Got inputs shapes: %s' % (input_shape))\r\n\r\n    ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 256, None, None, None), (None, 128, 18, 18, 18)]\r\n\r\n```\r\nI cannot understand why, when saving a model, it calls the model several times, and then suddenly loses the shape in the middle of the stacks of layers.\r\nBy the way, I can actually train the model by going through many samples via writing custom training loops using tf.GradientTape() etc. But why such an error occurs when trying to save the model?\r\nBy the way2, I can save the weights by using model.save_weights(...)", "@alextp . Also, related to the issue in [another thread #27120](https://github.com/tensorflow/tensorflow/issues/27120)\r\nI tried to put \r\n```\r\nxBridgeEnd = layers.Conv3DTranspose(256, 2, strides=2, padding='valid', data_format='channels_first')(x)\r\n```\r\ndirectly in the SimpleUNet.call() method, tf gives me a\r\n```\r\nValueError: tf.function-decorated function tried to create variables on non-first call.\r\n```\r\nMay I learn the root cause and how to fix it? Thanks", "@alextp It is actually a bug, I think. Please see [my bug report](https://github.com/tensorflow/tensorflow/issues/35441)", "@yourtheron please file a separate issue for your problem which looks unrelated to this one", "@alextp . Yes, I have issued a bug report.", "assigning to @karmel to find the right owner for this issue.", "Hi @titu1994!\r\nWe are checking to see if you still need help on this issue. We recommend that you upgrade to 2.6 which is latest stable version of TF and let us know if the issue still persists in newer versions. Thanks!", "Yes it seems fixed in 2.6. Thanks !", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 19240, "title": "error compilation with openmp enabled  in tensorflow lite compilation", "body": "Hi , \r\nI building tensorflow lite shared library .\r\nI added flag -fopenmp to build_def.bzl to enable openmp:\r\n\r\n```\r\ndef tflite_copts():\r\n  \"\"\"Defines compile time flags.\"\"\"\r\n  copts = [\r\n      \"-DFARMHASH_NO_CXX_STRING\",\r\n  ] + select({\r\n          str(Label(\"//tensorflow:android_arm64\")): [\r\n              \"-std=c++11\",\r\n              \"-O3\",\r\n              \"-fopenmp\",\r\n          ],\r\n          str(Label(\"//tensorflow:android_arm\")): [\r\n              \"-mfpu=neon\",\r\n              \"-mfloat-abi=softfp\",\r\n              \"-std=c++11\",\r\n              \"-O3\",\r\n          ],\r\n          str(Label(\"//tensorflow:android_x86\")): [\r\n              \"-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\",\r\n          ],\r\n          str(Label(\"//tensorflow:ios_x86_64\")): [\r\n              \"-msse4.1\",\r\n          ],\r\n          \"//conditions:default\": [],\r\n  }) + select({\r\n      str(Label(\"//tensorflow:with_default_optimizations\")): [],\r\n      \"//conditions:default\": [\"-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK\"],\r\n  })\r\n```\r\n\r\nAnd I got error compilation,\r\n\r\n```\r\ntensorflow/contrib/lite/kernels/BUILD:43:1: C++ compilation of rule '//tensorflow/contrib/lite/kernels:eigen_support' failed (Exit 1)\r\nIn file included from tensorflow/contrib/lite/kernels/eigen_support.cc:17:\r\nIn file included from ./third_party/eigen3/Eigen/Core:1:\r\nexternal/eigen_archive/Eigen/Core:275:10: fatal error: 'omp.h' file not found\r\n#include <omp.h>\r\n         ^\r\n1 error generated.\r\nTarget //tensorflow/contrib/lite:libtensorflowLite.so failed to build\r\n```\r\n\r\nPlease advise how to fix it.\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes,\r\n1. added openmp flag.\r\n2. I changed BUILD file in tensorflow lite to create shared library\r\n```\r\ncc_binary(\r\n    name = \"libtensorflowLite.so\",\r\n    linkshared=1,\r\n\r\n    deps = [\r\n        \":framework\",\r\n\t\"//tensorflow/contrib/lite/kernels:builtin_ops\"\r\n    ],\r\n)\r\n```\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16\r\n- **TensorFlow installed from (source or binary)**:\r\nsources\r\n- **TensorFlow version (use command below)**:\r\nmaster\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.10\r\n- **GCC/Compiler version (if compiling from source)**:\r\nlatest\r\n- **CUDA/cuDNN version**:\r\nno\r\n- **GPU model and memory**:\r\nno\r\n- **Exact command to reproduce**:\r\n`bazel build  //tensorflow/contrib/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=arm64-v8a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=\"-std=c++11\"`\r\n", "comments": ["@petewarden: Is OpenMP compatible with lite?  Or should this be treated as an eigen issue?", "Nagging Assignee @petewarden: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @petewarden: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 19239, "title": "add type to T for FusedResizeAndPadConv2D and FusedPadConv2D", "body": "Issue: https://github.com/tensorflow/tensorflow/issues/19228", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 19238, "title": "Reproducibility Problem In tensorflow", "body": "If you open a GitHub issue, here is our policy:\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code - yes**:\r\n- **google colab**:\r\n- **google colab default notebook**:\r\n- **'1.7.0'**:\r\n- **Python version: '1.7.0'**: \r\n- **Bazel version : not applicable**:\r\n- **GCC/Compiler version : not applicable**:\r\n- **CUDA/cuDNN version : google colab default version**:\r\n- **GPU model and memory : google colab's k80 gpu**:\r\n- **Exact command to reproduce : code given**:\r\n\r\nhttps://colab.research.google.com/drive/1KXAa4OVXht0ZhfN73u5AqqfYKM2uqvN8#scrollTo=gtSwh3KykmxG\r\n\r\nI see some problem regarding Reproducibility of result. Here I have given a notebook. Each of the cell  contains same code with same seed of tensorflow except that in the middle cell contains only single placeholder extra that has nothing to do with the graph input. But I don't know why random number generation is different for just declaring a single placeholder extra. Is that an expected behavior? ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 19237, "title": "Improve shape function of `tf.image.draw_bounding_boxes`", "body": "The `tf.image.draw_bounding_boxes` requires `boxes` to be 3-D shape though there was no check on shape function.\r\n\r\nThis fix improves the shape function by restricting the boxes to 3-D.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @rmlarsen for the review. The PR has been updated. Please take a look.", "Thanks!"]}, {"number": 19236, "title": "add output_shape to tf.layers.conv2d_transpose", "body": "### System information\r\n- **Have I written custom code (N/A)**:\r\n- **OS Platform and Distribution (Windows 10)**:\r\n- **TensorFlow installed from (binary)**:\r\n- **TensorFlow version (1.8.0)**:\r\n- **Python version 3.6.1**: \r\n- **Bazel version (N/A)**:\r\n- **GCC/Compiler version (N/A)**:\r\n- **CUDA/cuDNN version** N/A:\r\n- **GPU model and memory** N/A:\r\n- **Exact command to reproduce** N/A:\r\n\r\n### Describe the problem\r\nCurrently tf.layers.conv2d_transpose doesn't have an output_shape parameter like tf.nn.conv2d_transpose does. As a result when using strides > 1 the output shape is not defined \r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/2118#issuecomment-215488127\r\n\r\nWould it be possible to have this added in to the layers api so we can specify output shape.\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Yes", "Could you use set_shape on the result?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 19235, "title": "Bottleneck retrained model predicts one class after applying transforms and opimize_for_inference. ", "body": "\r\nSummary : \r\n\r\n**Have I written custom code** - Mostly using the retrain script from the tensorflow for poets tutorial. I added custom code to validate network but didn't change the training procedure. \r\n**OS Platform and Distribution** -  Windows 7 x64. Training and evaluation is done on a windows machine using Anconda python distribution. Once I freeze the model I then copy it to a linux box with CentOS 7 x86_64 to do the transformation. The transformed model is then copied back to the windows box for further analysis before deployment.\r\n**TensorFlow installed from** - anaconda cloud \r\n**TensorFlow version** - 1.2.1 \r\n**Bazel version** - 0.8.1\r\n**CUDA/cuDNN version** - 8 / 7 \r\n**GPU model and memory** - GTX 970 4GB \r\n**Exact command to reproduce** - N/A\r\n\r\nI have asked the same question on stackoverflow \r\n\r\nhttps://stackoverflow.com/questions/50243492/inception-v3-retrained-model-incorrect-predictions-after-using-optimize-for-infe/50263837#50263837\r\n\r\nand opencv help \r\n\r\nhttp://answers.opencv.org/question/191168/inception-v3-retrained-model-incorrect-predictions-after-using-optimize_for_inference-and-graph_transform-tools/\r\n\r\nI can really use some suggestions: \r\n\r\n1. I followed the tensorflow for poets tutorial to retrain inception V3 for 3-class image task \r\n\r\nhttps://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0\r\n\r\n2. The network fits the data well and i'm able to validate the frozen model using tensorflow python which predicts unseen samples at 97 % accuracy \r\n\r\n3. The frozen model has to be integrated to a legacy application which uses opencv::DNN module. To get the forzen model ready I applied \r\n  - optimize_for_inference \r\n  - graph_transforms with \" remove_nodes(op=PlaceholderWithDefault) strip_unused_nodes(type=float, shape=\\\"1,299,299,3\\\") sort_by_execution_order\" flags \r\n\r\n4. Loaded transformed model with opencv and run some more validation before deployment. The transformed version could not match (not even close) the trained original model.  It mostly predicts a single class. \r\n\r\nI used opencv blobFromImage function with swapRB set to true. \r\n\r\nI tried to normalize input by subtracting ImageNet mean as well as my training set mean. Unsuccessful in both cases. \r\n\r\n\r\nI just have no idea what is causing for the model to be so different after its transformed. \r\n\r\nThanks for your time \r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Updated question with platform etc info ", "This was primarily due to the preprocessing and batch normalization that happens during training.To normalize an input at inference time please use 127.5 for RGB mean values and set the scale factor to 1/127.5 when calling opencv::dnn::blobFromImage. You might want to swap RB channels depending on how you read in the input during training. "]}, {"number": 19234, "title": "Add tf.print an an alias for tf.Print", "body": "Users with Python 3 or `from __future__ import print_function` can now\r\nuse lowercase `tf.print`.\r\n\r\nFixes #18053.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Wow, have I really not signed the CLA before? :)", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Note: I haven't added any tests for this, since I was unsure what the correct protocol is these days for tests about the toplevel `tensorflow` module.  Happy to add the trivial test if told where to put it.", "Huh, the `api_compatibility_test` is failing, and that's probably related to this:\r\n\r\n    >>> tf.keras.datasets.boston_housing.print_function\r\n    _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 65536)\r\n\r\nThat file has some very strange lines.  Why would one need to `del print_function`? \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/datasets/boston_housing/__init__.py#L25", "Hi Geoffrey, Thanks for the PR.\r\n\r\nDo you need to update the goldens perhaps? See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/api/tests/README.txt\r\n\r\n@fchollet Any idea what's up the Keras failures?", "The `api_compatibility_test` passes for me since I'm on Python 3, so updating the goldens has no effect.  I could try to do it on 2, but I'm leery of any situation where the goldens are generated differently with different versions of Python.", "The easy fix would to be make the `api_compatibility_test` simply ignore `print_function`, but it seems weird to have to do this.  And I already don't understand why `keras` has those del lines.", "For a long time, the following pattern has been standard in TF for files that were used for exporting API keywords:\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom blah import keywords\r\n\r\ndel absolute_import\r\ndel division\r\ndel print_function\r\n```\r\n\r\nThis is no longer the case since API export now relies on decorators. @annarev, what should we do with existing files using the above pattern?", "(Okay for API addition)", "create_python_api.py script generates __init__.py files that define TensorFlow API.\r\nCurrently, we don't have print_function included in these files because it is not a part of TensorFlow API.\r\n\r\nWhat is the usecase for having print_function available in TensorFlow API? Can users of TensorFlow API just import it before tensorflow?", "There's no use case for having `print_function` in the API, and I'm not really sure how it's getting into Keras modules that explicitly delete it.", "I think I figured out what was happening with the API.  Commit updated.", "Oh, I see: the `api_compatibility_test` is just skipped in Python 3.  That would explain why it is passing for me, and why the goldens don't update.  Golden update coming up.", "Golden update done.", "The failing `Ubuntu CC` tests appear unrelated.", "The Ubuntu Python3 PIP failure doesn't look related, but I can't see exactly what's failing so I'm not sure.", "@girving I can't see what's failing either :-(", "Ooh, all green!  Shall I merge?  Not sure I'm supposed to do that as a Xoogler.", "Merged. Thanks, Geoffrey."]}, {"number": 19232, "title": "Update how build statuses and artifacts are demoed in README.md", "body": "", "comments": []}, {"number": 19231, "title": ".", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}]