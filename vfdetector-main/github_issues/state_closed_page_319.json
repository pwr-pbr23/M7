[{"number": 44639, "title": "r2.4-rc1 cherry-pick request: [Intel MKL] Prevent unnecessary copies of oneDNN in .so files.", "body": "Original PR: #44629 \r\n\r\noneDNN was getting statically linked to all the .so created by TF, thereby increasing the installation significantly. Removed redundant dependencies of oneDNN. Also, in a few places the dependency was only with libiomp.so and not oneDNN, made appropriate changes to address it.", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44639) for more info**.\n\n<!-- need_author_consent -->", "Manually setting CLA to yes because this PR is cherry-picking an already merged PR #44629.", "Thanks a lot @penpornk for quick response."]}, {"number": 44638, "title": "cv2.cuda_GpuMat <-> TensorFlow vector / TensorFlow NumPy", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF  2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRecently presented [API between Numpy and TensorFlow](https://www.tensorflow.org/api_docs/python/tf/experimental/numpy), allows to speed up Numpy functions by utilinzf CUDA supporting GPU's, as demonstrated [here](https://www.youtube.com/watch?v=DpOq0PJ0H38&t). \r\nMany Computer Vision tools use OpenCV, Numpy and  TensorFlow functions to resolve Computer Vision tasks.\r\nOpenCV has a class allowing moving calculations intenstcive functions to GPU, via the [CUDA-accelerated Computer Vision](https://docs.opencv.org/master/d1/d1e/group__cuda.html). \r\nPyhton API is less documented but an introduction can be seen [here](https://www.learnopencv.com/getting-started-opencv-cuda-modul/). \r\nCurrnlty  OpenCV cuda_GpuMat, TensorFlow vectors / TensorFlow NumPy ndArrays are stored in CUDA memory.\r\nAn efficient and easy to use type conversion  between OpenCV cuda_GpuMat and TensorFlow vector / TensorFlow NumPy in both directions will allow development of efficient applications that utilize OpenCV, Numpy and  TensorFlow without obsolete memory copies.\r\n \r\n**Will this change the current api? How?**\r\nAdd new 0 copy convertions between tf.experimental.numpy and cv2.cuda_GpuMat classescv2.cuda_GpuMat\r\n\r\n**Who will benefit with this feature?**\r\nComputer Vision developers\r\n\r\n**Any Other info.**", "comments": ["TensorFlow supports DLPack for 0-copy conversions. Could that work here ? ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44637, "title": "False out of resource error for multi-gpu machine", "body": "Tensorflow 2.3.1 on Nvidia Docker image `tensorflow/tensorflow:latest-gpu-jupyter` \r\n\r\nI can train an LSTM in a Jupyter notebook using 2-out-of-4 of my GPUs, but when I increase to 4 I get the following NCCL error:\r\n\r\n```\r\nInternalError: 4 root error(s) found.\r\n  (0) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n\t [[node NcclAllReduce (defined at <ipython-input-17-7105c4ec4d5e>:2) ]]\r\n  (1) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n\t [[node NcclAllReduce (defined at <ipython-input-17-7105c4ec4d5e>:2) ]]\r\n\t [[GroupCrossDeviceControlEdges_1/Nadam/Nadam/update_0/Const/_529]]\r\n  (2) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n\t [[node NcclAllReduce (defined at <ipython-input-17-7105c4ec4d5e>:2) ]]\r\n\t [[GroupCrossDeviceControlEdges_2/Nadam/Nadam/update_0/Const/_509]]\r\n  (3) Internal:  NCCL: unhandled system error. Set NCCL_DEBUG=WARN for detail.\r\n\t [[node NcclAllReduce (defined at <ipython-input-17-7105c4ec4d5e>:2) ]]\r\n\t [[GroupCrossDeviceControlEdges_0/Nadam/Nadam/update_0/Const/_493]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_44874]\r\n```\r\n\r\nLooking at the NCCL_DEBUG logs, I find an out of memory error:\r\n\r\n![image](https://user-images.githubusercontent.com/3875113/98307202-17298400-1f7a-11eb-9e2d-0bcbf88aa36f.png)\r\n\r\nBut this is wrong, I have plenty on space left on all my devices, as shown here with nvidia-smi and TF_FORCE_GPU_ALLOW_GROWTH\r\n\r\n![image](https://user-images.githubusercontent.com/3875113/98307003-bc902800-1f79-11eb-9d2e-19365d22c3b7.png)\r\n\r\nDummy code:\r\n```python3\r\ndef get_compiled_model():\r\n    \r\n    the_input = keras.layers.Input(shape=(window_length, input_length))\r\n    embeddings = keras.layers.Embedding(num_addresses, embedding_dim)(the_input)\r\n\r\n    encoded = keras.layers.Bidirectional(keras.layers.LSTM(NUM_HIDDEN))(embeddings)\r\n\r\n    reconstr = keras.layers.RepeatVector(window_length)(encoded)\r\n    reconstr = keras.layers.Bidirectional(keras.layers.LSTM(NUM_HIDDEN, return_sequences=True))(reconstr)\r\n    \r\n    reconstr = keras.layers.TimeDistributed(keras.layers.Dense(output_dim, activation='relu'), \r\n                                                    name=\"the_output\")(reconstr)\r\n    \r\n    losses = {\r\n        \"the_output\": keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    }\r\n    lossWeights = LOSS_WEIGHTS\r\n        \r\n    model = keras.Model([\r\n        the_input, \r\n    ], [\r\n        reconstr\r\n    ])\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.Nadam(learning_rate=LEARNING_RATE),\r\n                  loss=losses)\r\n    \r\n    return model\r\n\r\ngpu_devices = [device for device in tf.config.list_physical_devices() if device.device_type=='GPU']\r\nif len(gpu_devices):\r\n    strategy = tf.distribute.MirroredStrategy(devices=['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3'])\r\n#     strategy = tf.distribute.MirroredStrategy()\r\n    with strategy.scope():\r\n        model = get_compiled_model()\r\nelse:\r\n    model = get_compiled_model()\r\n\r\n# train_generator is a custom keras.utils.Sequence \r\nmodel.fit(x=train_generator, epochs=200)\r\n```\r\n", "comments": ["Hi @devtronslab, can you provide reproducible code so I can run this on my end? Your code looks fairly standard to me so I suspect I won't actually be able to reproduce this error in my environment, but always good to try.\r\n\r\nAdditionally, can you try:\r\n-training with just GPUs 2 and 3\r\n-in the 4 GPU case set `tf.distribute.ReductionToOneDevice` so NCCL won't be used.\r\n\r\nAlso this sounds like it could be an issue you would want to post on the NCCL repo. I found the following two threads that might help here [#290](https://github.com/NVIDIA/nccl/issues/290) [#342](https://github.com/NVIDIA/nccl/issues/342)", "@nikitamaia This is the best I can do regarding code since I won't be able to provide you with a dataset. The only difference between what I've provided and what I use is the `embeddings` vector gets concatenated with 2 other single float inputs and there is a second output of a single float. This is supposed to be an autoencoder, so the second output is a recreation of one of the float inputs, and its loss is a simple MSE.\r\n\r\nAs I wrote originally I have tried with 2 GPUs, and this do not yield any of the errors. 3 throws NCCL errors.\r\n\r\nAdding `tf.distribute.ReductionToOneDevice` seems to work with 4 GPUs", "@nikitamaia Following one of the links you've provided, I increased the shared memory size of the docker container with the `--shm-size=256m` flag, and the errors have gone away. Closing this issue. Thanks for the help!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44637\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44637\">No</a>\n", "For anyone running into this issue on AWS Batch: The shared memory size is an available parameter you can customize when setting the job definition: https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html\r\n\r\nSetting the shared memory size at 256 solved this issue on AWS Batch."]}, {"number": 44636, "title": "[CherryPick r2.4] Re-apply cublasLt changes that were rolled-back", "body": null, "comments": []}, {"number": 44635, "title": "Check if broken bazel builds are caught by the github CI.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44634, "title": "enable use of <ctime> with micro_time with -DTF_LITE_USE_CTIME", "body": "This change:\r\n\r\n * uses a <ctime> based micro_time implementation by default for ohost-builds (previously needed TAGS=posix)\r\n * sets -DTF_LITE_USE_CTIME in the target makefiles for cross-compilation (instead of having to specialize micro_time.cc in the target folder).\r\n\r\nTested that we see the benchmark timing with the following commands:\r\n\r\n#### Linux:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 test_keyword_benchmark\r\n```\r\nOutput:\r\n```\r\nInitializeKeywordRunner() took 35 ticks (0 ms)\r\nKeywordRunNIerations(1) took 22 ticks (0 ms)\r\nKeywordRunNIerations(10) took 136 ticks (0 ms)\r\n```\r\n\r\n#### Linux + xtensa_hifimini\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa_hifimini OPTIMIZED_KERNEL_DIR=xtensa_hifimini XTENSA_CORE=mini1m1m_RG test_keyword_benchmark\r\n```\r\nOutput:\r\n```\r\nInitializeKeywordRunner() took 1389000 ticks (1389 ms)\r\nKeywordRunNIerations(1) took 89318 ticks (89 ms)\r\nKeywordRunNIerations(10) took 892739 ticks (892 ms)\r\n```\r\n\r\n#### Linux + hexagon: untested\r\n\r\nFixes: http://b/172372039", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44633, "title": "use the flatbuffers:runtime_cc bazel target instead of flatbuffers.", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44632, "title": "add per_replica support for keras", "body": "Adding Dataset PER_REPLICA support for Keras", "comments": ["> Code changes LGTM. Can we please add a test in input_lib_test.py to catch the issue that needs this fix?\r\n\r\nA test added for type_spec comparison to input_lib_type_spec_test.py, please have a look", "> > Code changes LGTM. Can we please add a test in input_lib_test.py to catch the issue that needs this fix?\r\n> \r\n> A test added for type_spec comparison to input_lib_type_spec_test.py, please have a look\r\n\r\nAlso added a second test for nest.flatten applied to iterator the same as Keras", "@kushanam Can you please check @guptapriya's comments and keep us posted ? Thanks!", "> @kushanam Can you please check @guptapriya's comments and keep us posted ? Thanks!\r\n\r\n@gbaned since the PR was approved based on my conversation with @guptapriya the nit test comment was decided come in a different PR. However if @guptapriya and @gbaned you want, I can commit the change", "> > @kushanam Can you please check @guptapriya's comments and keep us posted ? Thanks!\r\n> \r\n> @gbaned since the PR was approved based on my conversation with @guptapriya the nit test comment was decided come in a different PR. However if @guptapriya and @gbaned you want, I can commit the change\r\n\r\nI committed the changes anyhow to avoid delays.", "@kushanam can you please check sanity build failures ?", "> @kushanam can you please check sanity build failures ?\r\n\r\n@rthadur Just committed the pylint sanity fixes related to my changes."]}, {"number": 44631, "title": "TypeError: An op outside of the function building code is being passed", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: MacOs Catalina 10.15\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.7.7\r\n\r\n**Describe the current behavior**\r\nThe modelCheckpoint callback seems to trigger the issue, everything run smoothly without it. \r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nimport os\r\nimport time\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\ndef make_dataset(path, n_samples):\r\n    df = pd.read_csv(path, usecols=[6,9], nrows=n_samples)\r\n    df.columns = ['ratings', 'title']\r\n\r\n    text = df['title'].tolist()\r\n    text = [str(t).encode('ascii', 'replace') for t in text]\r\n    text = np.array(text, dtype='object')[:]\r\n\r\n    labels = df['ratings'].tolist()\r\n    labels = [1 if i>= 4 else 0 if i == 3 else -1 for i in labels]\r\n\r\n    labels = np.array(pd.get_dummies(labels), dtype=int)[:]\r\n\r\n    return labels, text\r\n\r\n\r\ndef get_model():\r\n    embed = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/1\",\r\n     output_shape=[50],\r\n     input_shape=[],\r\n     dtype=tf.string,\r\n     name='input',\r\n     trainable=False)\r\n\r\n    model = tf.keras.Sequential()\r\n    model.add(embed)\r\n    model.add(tf.keras.layers.Dense(16, activation='relu'))\r\n    model.add(tf.keras.layers.Dense(3, activation='softmax', name='output'))\r\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])\r\n    model.summary()\r\n    return model\r\n\r\ndef train(epochs=5, bs=32):\r\n    y_train, x_train = make_dataset('reviews_train.csv', n_samples=100000)\r\n    y_val, x_val = make_dataset('reviews_test.csv', n_samples=10000)\r\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n                                filepath='model_checkpoint',\r\n                                save_weights_only=False,\r\n                                monitor='val_acc',\r\n                                mode='max',\r\n                                save_best_only=True)\r\n    model = get_model()\r\n    model.fit(x_train, y_train, batch_size=bs, epochs=epochs, verbose=1,\r\n         validation_data=(x_val, y_val),\r\n         callbacks=[model_checkpoint_callback])\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n\r\n```\r\n**Other info / logs** \r\n`Traceback (most recent call last):\r\n  File \"train.py\", line 58, in <module>\r\n    train()\r\n  File \"train.py\", line 53, in train\r\n    callbacks=[model_checkpoint_callback])\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 108, in _method_wrapper\r\n    return method(self, *args, **kwargs)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1137, in fit\r\n    callbacks.on_epoch_end(epoch, epoch_logs)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 412, in on_epoch_end\r\n    callback.on_epoch_end(epoch, logs)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1249, in on_epoch_end\r\n    self._save_model(epoch=epoch, logs=logs)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\", line 1301, in _save_model\r\n    self.model.save(filepath, overwrite=True, options=self._options)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1979, in save\r\n    signatures, options)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 134, in save_model\r\n    signatures, options)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py\", line 80, in save\r\n    save_lib.save(model, filepath, signatures, options)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py\", line 985, in save\r\n    options=ckpt_options)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py\", line 1200, in save\r\n    file_prefix_tensor, object_graph_tensor, options)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py\", line 1145, in _save_cached_when_graph_building\r\n    save_op = saver.save(file_prefix, options=options)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/training/saving/functional_saver.py\", line 293, in save\r\n    tf_function_save()\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 846, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1848, in _filtered_call\r\n    cancellation_manager=cancellation_manager)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1924, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 550, in call\r\n    ctx=ctx)\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 75, in quick_execute\r\n    raise e\r\n  File \"/Users/antoine/.virtualenvs/amazon-review/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\r\n    inputs, attrs, num_outputs)\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: embeddings/part_0/Read/ReadVariableOp:0`\r\n", "comments": ["@charlet-antoine \r\n\r\nPlease, provide colab link or simple standalone code snippet with supporting files to reproduce the issue in our environment.It helps us in localizing the issue faster. Thanks!", "@ravikyram \r\nHi there, you can git clone the mini project on your local:\r\nhttps://github.com/charlet-antoine/tensorflow-serving-exemple.git", "@charlet-antoine \r\n\r\nI have tried in colab with TF version 2.3 and nightly version(`2.5.0-dev20201109`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c64fcae0bc93c8abeba883b2cfcb5513/untitled497.ipynb).\r\nThis issue is more suitable for TensorFlow Hub repo. Please post it on hub repo from [here](https://github.com/tensorflow/hub/issues/new). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44631\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44631\">No</a>\n"]}, {"number": 44630, "title": "tf-nightly container has CUDA error on 'docker run'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS\r\n- TensorFlow installed from (source or binary): Docker\r\n- Docker image: tensorflow/tensorflow:nightly-gpu-jupyter\r\n- GPU model and memory: GeForce GTX 108 / 261MiB / 11178MiB\r\n\r\n**Describe the problem**\r\nError regarding CUDA>=11.0 when running most recent nightly container\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nDockerfile\r\n```\r\nFROM tensorflow/tensorflow:nightly-gpu-jupyter\r\n\r\nRUN pip install -U pip\r\n\r\n# install packages\r\nRUN apt-get update -qq && apt-get install --no-install-recommends -y \\    \r\n    libsm6 \\\r\n    libxext6 \\\r\n    libxrender-dev \\\r\n && apt-get clean \\\r\n && rm -rf /var/lib/apt/lists/*\r\n\r\nCOPY requirements.txt /tmp/\r\nRUN pip install --requirement /tmp/requirements.txt\r\nCOPY . /tmp/\r\n\r\nWORKDIR /tf/\r\n\r\nCMD jupyter lab --ip=0.0.0.0 --notebook-dir=/tf --allow-root\r\n```\r\n\r\nrequirements.txt\r\n```\r\ntensorflow-hub\r\nimbalanced-learn==0.7.0\r\njupyterlab==2.1.2\r\nopencv-python==4.2.0.34\r\npandas==1.0.3\r\nPillow==7.1.2\r\nptvsd==4.3.2\r\nscikit-learn==0.23.2\r\nscipy==1.4.1\r\nseaborn==0.11.0\r\nsklearn==0.0\r\nxlrd==1.2.0\r\n```\r\n\r\nBuild image\r\n```\r\n$ docker build --no-cache=true -t tf:tf-nightly .\r\n```\r\n\r\nRun container:\r\n```\r\n$ nvidia-docker run --rm -d -v /home:/tf -p 9896:8888 tf:tf-nightly\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nError:\r\n```\r\ndocker: Error response from daemon: OCI runtime create failed: container_linux.go:345: starting container process caused \"process_linux.go:430: container init caused \\\"process_linux.go:413: running prestart hook 1 caused \\\\\\\"error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: requirement error: unsatisfied condition: cuda>=11.0\\\\\\\\n\\\\\\\"\\\"\": unknown.\r\n```", "comments": ["@tjdurant,\r\nThe latest TF-nightly release required CUDA 11 and cuDNN 8. \r\n\r\nCould you please specify the CUDA and cuDNN version installed on your machine and check if you are facing the same error with CUDA 11 and cuDNN 8 as well? Thanks!", "@amahendrakar , i'm sorry because I may have had a knowledge gap on this one. Is the CUDA version you're looking for the version on the local machine or in the Docker container? Or both?", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44630\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44630\">No</a>\n"]}, {"number": 44629, "title": "[INTEL MKL] Prevent unnecessary copies of oneDNN in .so files.", "body": "oneDNN  was getting statically linked to all the .so created by TF, thereby increasing the installation significantly. Removed redundant dependencies of oneDNN. Also, in a few places the dependency was only with libiomp.so and not oneDNN, made appropriate changes to address it.", "comments": []}, {"number": 44628, "title": "Move EthosU custom op out of AllOpsResolver", "body": "@tensorflow/micro\r\n\r\nThe EthosU custom op https://github.com/tensorflow/tensorflow/blob/33dbf934d3758dfee07c0b5f6f4a6a00939b1b42/tensorflow/lite/micro/all_ops_resolver.cc#L86-L92\r\n\r\nshould ideally only be used for EthosU applications and not be part of the AllOpsResolver.\r\n\r\nCreating this issue to figure out how we might achieve that.", "comments": ["This came up during the review of #43566 \r\n\r\ntagging @mansnils @freddan80 \r\n", "Thx, we'll have a look at this.", "Based on https://github.com/tensorflow/tensorflow/pull/44968#pullrequestreview-533759643 my original intent of not having any mention of Ethos-U from AllOpsResolver was overkill given the current status (only a single target-specific op). We will revisit this in the future, but the refactor of #44968 is still good to have.", "It makes sense. I think we should revisit the idea of externalizing ethosu.cc. Should we keep this ticket open for the purpose?", "sounds good, let's keep this ticket open.", "@advaitjain \r\nIs this still an issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44628\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44628\">No</a>\n"]}, {"number": 44627, "title": "[TFLite] Fix reference register using the default register functions instead of the reference one for some ops", "body": "Hi,\r\n\r\nFor some operators the reference register uses the default `Register_*` function when a `Register_*_REF` is available. This PR makes sure that the reference kernel is registered when available.\r\n\r\nThibaut", "comments": []}, {"number": 44626, "title": "ambiguous template instantiation error with Eigen code", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.4-rc0\r\n- Bazel version (if compiling from source): 3.4.1\r\n\r\n**Describe the problem**\r\n\r\n2.4 introduces a regression in the compilation fails on POWER systems with \"ambiguous template instantiation\".\r\nThe reason is that TF adds specializations for Eigen internal classes which conflict with specializations done by Eigen itself.\r\n\r\nIn this case https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/eigen_spatial_convolutions-inl.h adds `Eigen::internal::gemm_pack_rhs` specializations, e.g. at https://github.com/tensorflow/tensorflow/blob/300b62ff6a53a554bf8699f0632a0f517419b02c/tensorflow/core/kernels/eigen_spatial_convolutions-inl.h#L1044\r\n\r\nHowever the updated Eigen (in particular commit https://gitlab.com/libeigen/eigen/-/commit/6fe88a3c9db27c00a3817e391cf70116451bf046) adds an own specialization for POWER architectures, see e.g. https://gitlab.com/libeigen/eigen/-/commit/6fe88a3c9db27c00a3817e391cf70116451bf046#b3394fe72fd1b6f1213ded1197953458e6851483_23_2622\r\n\r\nThis then leads to the ambiguous template instantiation of \r\n```\r\ntemplate<class Scalar, class Index, class DataMapper, int nr, bool Conjugate, bool PanelMode>\r\nstruct Eigen::internal::gemm_pack_rhs<Scalar, Index, DataMapper, nr, 0, Conjugate, PanelMode>\r\n```\r\nbecause the Eigen version specializes the `Scalar`, while TF specializes all but Scalar, Index and nr.\r\n\r\nFor reference the types that this is tried to be instantiated are (taken from the log, slightly formatted): \r\n\r\n```\r\nScalar = double;\r\nIndex = long int;\r\nDataMapper = Eigen::internal::TensorContractionSubMapper<double, long int, 0, Eigen::TensorEvaluator<const Eigen::TensorReshapingOp<const Eigen::DSizes<long int, 2>, const Eigen::TensorImagePatchOp<-1, -1, const Eigen::TensorMap<Eigen::Tensor<const double, 4, 1, long int>, 16, Eigen::MakePointer> > >, Eigen::ThreadPoolDevice>, Eigen::array<long int, 1>, Eigen::array<long int, 1>, 2, true, true, 0, Eigen::MakePointer>;\r\nint nr = 4;\r\nbool Conjugate = false;\r\nbool PanelMode = false\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nCompile on POWER according to instructions\r\n\r\n**Any other info / logs**\r\n\r\nWhile this currently may only happen on POWER there is always the risk of breaking things when adding to internals of another library. So it should be checked, if that is really required.\r\nIf it is, I'd suggest to extract the 4 implementations of `gemm_pack_rhs` into 4 functions. Then specializations can be added for each datatype that Eigen also specializes for which avoids the ambiguity.\r\n\r\nA short term solution might also be to simply remove the Eigen specializations via a patch.", "comments": ["> A short term solution might also be to simply remove the Eigen specializations via a patch.\r\n\r\nI tried that and found it doesn't work. Multiple tests (e.g. //tensorflow/core/kernels:eigen_spatial_convolutions_test)  will fail due to wrong results. Hence I don't have an easy way anymore to compile and use TF on POWER", "Thats's strange, all these ugly specializations in `eigen_spatial_convolutions-inl.h` are supposed to be only for performance reasons, and not change how the data is packed.", "What I did was to remove the gemm_pack_rhs class specializations from `Eigen/src/Core/arch/AltiVec/MatrixProduct.h` and tests failed. However when I removed the whole header (there is a conditional include in `Core` which I commented) it worked, so I guess there is some dependency on the stuff TF introduces and some other things from the MatrixProduct.h which fail to meet implicit assumptions in the TF code.\r\n\r\nAs I don't want to remove the whole AltiVec code from Eigen (which is likely better optimized than the generic code from TF) I'm currently trying to remove the code TF introduces in eigen_spatial_convolutions-inl.h and eigen_cuboid_convolution.h, i.e. the gemm_pack_rhs and TensorContractionInputMapper specializations. I do that conditionally on `defined(EIGEN_VECTORIZE_ALTIVEC) || defined(EIGEN_VECTORIZE_VSX)` so if that works, it can be used upstream by TF.\r\n\r\nStill: Adding stuff to a foreign namespace even called `internal` is risky at best.", "```\r\nStill: Adding stuff to a foreign namespace even called internal is risky at best.\r\n```\r\n\r\nYes, and we had our fair share of ODRs from that :) But it brings very large improvement in practice, and too TF specific to be moved upstream. I remember it was brought up in some discussion with Eigen maintainers, and there is a desire to clean that up, so it could be upstreamed, but as far as I know, no one is working on it right now.", "I see. Do you recall where that was discussed?\r\nMaybe the Eigen maintainers and especially the authors of that AltiVec code could at least make something which is compatible with TF.\r\nAnd maybe someone familiar with that code could judge, which implementation is better on POWER. I'd naively expect the Eigen code is faster, as it uses vector intrinsics while (IIUC) the TF code relies on auto-vectorization. But it might as well be better to selectively remove some of the Eigen code instead. As mentioned above I wasn't able to remove just enough so the TF code works and you'd still get the vectorized code for the parts TF doesn't touch.", "Eigen template specialization in TF allows to bypass all DataMappers and read data directly from the underlying tensors. Reading data through data mappers incurs HUGE overheads in stride computations for TensorImagePatch operation.\r\n\r\nHowever it all worked with an assumption that packed block format is the same for all architectures. With AltiVec, TF packing specialization will produce data in format not compatible with gemm expectations. I'd say that disabling specializations on the TF side for AltiVec is the right choice here.\r\n\r\nRe discussion: I think it was in a live meeting we had with IBM/ARM few months ago.", "Hi all, I meant to respond here earlier.\r\nThis is an unfortunate problem caused by some recent enhancements to Eigen for existing Altivec instructions as well as a new ppc matrix unit. As noticed, it breaks TensorFlow because of their usage of the internal Eigen namespace and broad unchecked assumptions on data formats.\r\nIt's unfortunate, but for now, the best way around this is to just revert the enhancements altogether.\r\n\r\nI work on a project called [Open Cognitive Environment](http://github.com/open-ce) which supports ppc64le and we have an [Eigen fork](https://github.com/open-ce/libeigen-eigen) with the enhancements [reverted](https://github.com/open-ce/libeigen-eigen/commit/6fe88a3c9db27c00a3817e391cf70116451bf046) that works.\r\nThere is a corresponding TensorFlow [patch](https://github.com/open-ce/tensorflow-feedstock/blob/master/recipe/0303-Eigen-customized.patch) to point TensorFlow to the fork as well.\r\n\r\nThere are some folks at IBM that are looking at this. I think it would be easier to fix if the TensorFlow eigen code was upstreamed.  As mentioned it is specific to TensorFlow, but TensorFlow is a large and important consumer of Eigen and, despite steps towards a MLIR world, will continue to be for the foreseeable future.", "FWIW: In Easybuild we have a patch reverting the TF changes when on POWER which also resolves this:https://github.com/easybuilders/easybuild-easyconfigs/blob/develop/easybuild/easyconfigs/t/TensorFlow/TensorFlow-2.4.0_fix-eigen-on-power.patch", "Hi all,\r\n\r\nIIUC, the Flamefire's patch removes TF specialization + InputMappers, which forces both functions `CuboidConvolution` and `SpatialConvolution` to use the old/slow tensor code from Eigen (unsupported/Eigen/CXX11/src/Tensor). \r\n\r\nThere is some benchmark available to measure the impact of this change on performance?\r\n", "There are benchmarks for convolutions in `tensorflow/core/kernels/eigen_benchmark_cpu_test.cc`", ">  ... use the old/slow tensor code from Eigen (unsupported/Eigen/CXX11/src/Tensor).\r\n\r\nPower specializes the packing routines which should be quite fast.  Other than that, I'm not sure what other code there is", "> There is some benchmark available to measure the impact of this change on performance?\r\n\r\nI'd be interested in that too. However first and foremost it has to work at all so we have something to compare against.\r\nI remember trying to remove the Eigen code additions but ending up with wrong results from the calculations. Hence I went and removed the TF code instead.", "@Flamefire After some tests here, your patch ended up a good fix for this issue. Can I create a pull request? https://github.com/tensorflow/tensorflow/pull/47768", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44626\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44626\">No</a>\n"]}, {"number": 44625, "title": "TFLite quantization aware converter - 'std.constant' op requires attribute's type ('tensor<1x64xf32>') to match op's return type", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nhttps://github.com/UNCG-DAISY/TinyDuneCollision/blob/master/src/TinyML_End_to_End.ipynb\r\n\r\nSee Quantization aware training\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n~/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198                                                  debug_info_str,\r\n--> 199                                                  enable_mlir_converter)\r\n    200       return model_str\r\n\r\n~/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: /home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/layers/ops/core.py:53:0: error: 'std.constant' op requires attribute's type ('tensor<1x64xf32>') to match op's return type ('tensor<*xf32>')\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/layers/core.py:1198:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow_model_optimization/python/core/quantization/keras/quantize_wrapper.py:162:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/autograph/impl/api.py:302:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py:985:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/functional.py:508:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/functional.py:386:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/sequential.py:372:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py:985:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/saving/saving_utils.py:134:0: note: called from\r\n/home/sdmohant/.virtualenvs/python3deep/lib/python3.5/site-packages/tensorflow/python/keras/layers/ops/core.py:53:0: note: see current operation: %cst_13 = \"std.constant\"() {value = dense<[[0.000000e+00, -0.330023378, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, -0.330023378, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, -0.330023378, 0.000000e+00, 0.000000e+00, 0.330023378, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.282877177, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.282877177, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00]]> : tensor<1x64xf32>} : () -> tensor<*xf32>\r\n```\r\n\r\n**Model Summary**\r\n\r\n![Screen Shot 2020-11-05 at 11 03 24 AM](https://user-images.githubusercontent.com/2720456/98265106-afae0d00-1f56-11eb-95ca-2d9f2126ff8b.png)\r\n\r\n\r\n**Failure details**\r\nConverter is not able to create TFLite model\r\n\r\n\r\n\r\n", "comments": ["@somyamohanty,\r\nOn running the code, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory: '../data/CollisionData/'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/be7f7ad79baad2a33e7f290349ecae5b/44625.ipynb). \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide all the supporting files required to run the code. Thanks!", "@amahendrakar Sorry for the inability to provide the raw data (licensing issues exist). I have changed the repository and the code to utilize publicly available dataset instead. Should be able to run now. But the issue still persists. ", "@somyamohanty,\r\nSorry for the delayed response. I was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/5809cf01689e3e8c6de823c385f04af5/44625.ipynb#scrollTo=XICaRtaaYaaD).\r\n\r\nHowever with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/0bf05f99df3a019a650a135697e67739/44625-tf-nightly.ipynb), I was able to convert the model without any issues. Please find attached gist. Thanks!", "> @somyamohanty,\r\n> Sorry for the delayed response. I was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/5809cf01689e3e8c6de823c385f04af5/44625.ipynb#scrollTo=XICaRtaaYaaD).\r\n> \r\n> However with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/0bf05f99df3a019a650a135697e67739/44625-tf-nightly.ipynb), I was able to convert the model without any issues. Please find attached gist. Thanks!\r\n\r\nThanks, will try it out.", "I'm having the same problem. I also must use tf-gpu and can't just switch to tf-nightly due to cudnn/cuda dependencies."]}, {"number": 44623, "title": "TensorFlow 2.4 & roadmap", "body": "Hello,\r\n\r\nIt's been released ver. 2.4-rc0 of TensorFlow, can we expect ver.2.4.x this year?\r\n\r\nThank you.\r\n\r\n ", "comments": ["Most likely yes.", "@peter197321 \r\n\r\nPlease, close this thread as your question has been answered. Thanks!", "RC1 was just released. Depending on amount of cherrypicks and issues discovered, there might be a 2.4 final release around Thanksgiving.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Due to some delays, RC2 will be released this week (likely tomorrow) and final release will come after Thanksgiving.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "RC3 was released yesterday. Final release will happen next week if testing does not reveal more issues.", "Thank you for your support.", "Final release should come this week or the next.", "TF 2.4 has been released (forgot to notify the thread at the time). Closing this", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44623\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44623\">No</a>\n", "yes, thank you.\n\nOn Sun, Dec 27, 2020 at 1:24 AM tensorflow-butler[bot] <\nnotifications@github.com> wrote:\n\n> Are you satisfied with the resolution of your issue?\n> Yes\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44623>\n> No\n> <https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44623>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/44623#issuecomment-751410471>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALGLVQQLXENLTQZXM5J5PFDSWZ5E7ANCNFSM4TLRKI2Q>\n> .\n>\n"]}, {"number": 44622, "title": "Tensorflow 2.3.1 is not compatible with cuda 10.2 on Windows", "body": "**System information**\r\n\r\n- OS Platform: Windows 20H2 19042.572 64bit;\r\n- TensorFlow installed from (source or binary): source;\r\n- TensorFlow version: 2.3.1;\r\n- Python version: 3.8.6 64bit;\r\n- Bazel version (if compiling from source): 3.1.0;\r\n- Compiler version: MSVC 2019 16.7.7\r\n- CUDA/cuDNN version: CUDA 10.2 with cuDNN 7.6.5.\r\n\r\n\r\n**Describe the problem**\r\n\r\nBuild failed with cuda support:\r\n\r\n\r\n**Any other info / logs**\r\n\r\n```\r\n.\\tensorflow/core/kernels/cuda_sparse.h(39): error: identifier \"cusparseDnMatDescr_t\" is undefined\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(40): error: identifier \"cusparseSpMatDescr_t\" is undefined\r\n\r\n.\\tensorflow/core/kernels/cuda_sparse.h(41): error: identifier \"cusparseSpMMAlg_t\" is undefined\r\n\r\ntensorflow/core/kernels/tridiagonal_solve_op_gpu.cu.cc(98): warning: extra \";\" ignored\r\n\r\n3 errors detected in the compilation of \"C:/Users/SPINDE~1/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmp3nb_ygoq/tridiagonal_solve_op_gpu.cu.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 6783.098s, Critical Path: 4048.81s\r\nINFO: 5532 processes: 5532 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Analysis**\r\n\r\nThese problematic symbols are used in `tensorflow/core/kernels/cuda_sparse.h` (line 38-42): \r\n\r\n```cpp {.line-numbers}\r\n#if CUDA_VERSION >= 10020\r\nusing gpusparseDnMatDescr_t = cusparseDnMatDescr_t;\r\nusing gpusparseSpMatDescr_t = cusparseSpMatDescr_t;\r\nusing gpusparseSpMMAlg_t = cusparseSpMMAlg_t;\r\n#endif\r\n```\r\n\r\nBut in cuda 10.2 `cusparse.h` (`C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2\\include\\cusparse.h`), these symbols are defined in a `#if !defined(_WIN32) ... #endif` block (line 7341-7386, 7700):\r\n\r\n```cpp\r\n//##############################################################################\r\n//# SpMM APIs\r\n//##############################################################################\r\n\r\n#if !defined(_WIN32)\r\n\r\ntypedef enum {\r\n    CUSPARSE_MM_ALG_DEFAULT = 0,\r\n    CUSPARSE_COOMM_ALG1 = 1, // non-deterministc results\r\n    CUSPARSE_COOMM_ALG2 = 2, // deterministic results\r\n    CUSPARSE_COOMM_ALG3 = 3, // non-deterministc results, for large matrices\r\n    CUSPARSE_CSRMM_ALG1 = 4\r\n} cusparseSpMMAlg_t;\r\n\r\ntypedef struct cusparseSpMatDescr* cusparseSpMatDescr_t;\r\ntypedef struct cusparseDnMatDescr* cusparseDnMatDescr_t;\r\n\r\n#endif // !defined(_WIN32)\r\n```\r\n\r\nAccording to the [Microsoft documentation](https://docs.microsoft.com/en-us/cpp/preprocessor/predefined-macros?view=msvc-160):\r\n\r\n```\r\n_WIN32 Defined as 1 when the compilation target is 32-bit ARM, 64-bit ARM, x86, or x64. Otherwise, undefined.\r\n```\r\n\r\nSo these symbols are not defined when the compilation target is x64 on Windows and the build always fails.\r\n\r\nBy the way, the latest version in `master` branch has the same problem.", "comments": ["@spindensity,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the problem.\r\n\r\nAlso, check if you are facing the same issue with CUDA 10.1 as well? Thanks!", "> @spindensity,\r\n> Could you please provide the exact sequence of commands / steps that you executed before running into the problem.\r\n> \r\n> Also, check if you are facing the same issue with CUDA 10.1 as well? Thanks!\r\n\r\n@amahendrakar \r\n\r\nThanks.\r\n\r\nAs the above source code referred (`tensorflow/core/kernels/cuda_sparse.h`, line 38-42), TensorFlow does NOT use these symbols at all when building with CUDA 10.1: \r\n\r\n```cpp\r\n#if CUDA_VERSION >= 10020\r\nusing gpusparseDnMatDescr_t = cusparseDnMatDescr_t;\r\nusing gpusparseSpMatDescr_t = cusparseSpMatDescr_t;\r\nusing gpusparseSpMMAlg_t = cusparseSpMMAlg_t;\r\n#endif\r\n```\r\n\r\nSo CUDA 10.1 is OK and I have built TensorFlow 2.3.1 with it (CUDA 10.1) successfully:\r\n\r\nThe problem is these symbols are NEVER defined in CUDA 10.2 `cusparse.h` when the compilation target is x64 on Windows as my first post said, I think it's a bug of TensorFlow.\r\n\r\nThe commands I used are exactly the same as the instructions of the [official documentation](https://www.tensorflow.org/install/source_windows) and here is the `.tf_configure.bazelrc`:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Dev/BasicTools/python/virtualenvs/tf_build/Scripts/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Dev/BasicTools/python/virtualenvs/tf_build/lib/site-packages\"\r\nbuild --python_path=\"C:/Dev/BasicTools/python/virtualenvs/tf_build/Scripts/python.exe\"\r\nbuild --config=xla\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --config=cuda\r\nbuild:opt --copt=/arch:AVX2\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n\r\n", "Can you send a patch to master please and then a cherrypick to r2.3?\r\n\r\nCC @sanjoy ", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44622\">No</a>\n", "This is still an issue on Windows 10 with versions 2.3.1 and 2.4.0-RC4 and CUDA 10.2.\r\n", "> This is still an issue on Windows 10 with versions 2.3.1 and 2.4.0-RC4 and CUDA 10.2.\r\n\r\nSame over here, I even get multiple other errors as well. Any patch or recommendation for this yet ?", "We don't support 10.2 (we moved directly to 11.0 from 10.1).  But I will approve minor PRs to make TF buildable against 10.2.", "Please refer TF and cuda compatibility chart https://www.tensorflow.org/install/source#gpu\r\nThanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44622\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44622\">No</a>\n"]}, {"number": 44621, "title": "Updated mli_tf_utils.h as part of porting to the new TfLiteEvalTensor API.", "body": "This pull request is part of porting ARC optimized kernels to the new TfLiteEvalTensor API.\r\nThis one is related to arc specific utils convolution kernel.\r\nAlso a little update of MLI library URL.\r\n\r\nPR number: [5/5]\r\n\r\nOther related PRs:\r\n1. https://github.com/tensorflow/tensorflow/pull/44617\r\n2. https://github.com/tensorflow/tensorflow/pull/44618\r\n3. https://github.com/tensorflow/tensorflow/pull/44619\r\n4. https://github.com/tensorflow/tensorflow/pull/44620\r\n5. https://github.com/tensorflow/tensorflow/pull/44621\r\n\r\nFixes #43910", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44620, "title": "Updated ARC optimized pooling kernel to use the new TfLiteEvalTensor API.", "body": "This pull request is part of porting ARC optimized kernels to the new TfLiteEvalTensor API.\r\nThis one is related to pooling kernel.\r\n\r\nPR number: [4/5]\r\n\r\nOther related PRs:\r\n1. https://github.com/tensorflow/tensorflow/pull/44617\r\n2. https://github.com/tensorflow/tensorflow/pull/44618\r\n3. https://github.com/tensorflow/tensorflow/pull/44619\r\n4. https://github.com/tensorflow/tensorflow/pull/44620\r\n5. https://github.com/tensorflow/tensorflow/pull/44621\r\n\r\nFixes #43910", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44619, "title": "Updated ARC optimized fully_connected kernel to use the new TfLiteEvalTensor API.", "body": "This pull request is part of porting ARC optimized kernels to the new TfLiteEvalTensor API.\r\nThis one is related to fully connected kernel.\r\n\r\nPR number: [3/5]\r\n\r\nOther related PRs:\r\n1. https://github.com/tensorflow/tensorflow/pull/44617\r\n2. https://github.com/tensorflow/tensorflow/pull/44618\r\n3. https://github.com/tensorflow/tensorflow/pull/44619\r\n4. https://github.com/tensorflow/tensorflow/pull/44620\r\n5. https://github.com/tensorflow/tensorflow/pull/44621\r\n\r\nFixes #43910", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 44618, "title": "Updated ARC optimized depthwise_conv kernel to use the new TfLiteEvalTensor API.", "body": "This pull request is part of porting ARC optimized kernels to the new TfLiteEvalTensor API.\r\nThis one is related to depthwise convolution kernel.\r\n\r\nPR number: [2/5]\r\n\r\nOther related PRs:\r\n1. https://github.com/tensorflow/tensorflow/pull/44617\r\n2. https://github.com/tensorflow/tensorflow/pull/44618\r\n3. https://github.com/tensorflow/tensorflow/pull/44619\r\n4. https://github.com/tensorflow/tensorflow/pull/44620\r\n5. https://github.com/tensorflow/tensorflow/pull/44621\r\n\r\nFixes #43910", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@gerbauz Can you please resolve conflicts? Thanks!", "> @gerbauz Can you please resolve conflicts? Thanks!\r\n\r\nDone"]}, {"number": 44617, "title": "Updated ARC optimized convolution kernel to use the new TfLiteEvalTensor API.", "body": "This pull request is part of porting ARC optimized kernels to the new TfLiteEvalTensor API.\r\nThis one is related to convolution kernel.\r\n\r\nPR number: [1/5]\r\n\r\nOther related PRs:\r\n1. https://github.com/tensorflow/tensorflow/pull/44617\r\n2. https://github.com/tensorflow/tensorflow/pull/44618\r\n3. https://github.com/tensorflow/tensorflow/pull/44619\r\n4. https://github.com/tensorflow/tensorflow/pull/44620\r\n5. https://github.com/tensorflow/tensorflow/pull/44621\r\n\r\nFixes #43910", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@gerbauz  Can you please resolve conflicts? Thanks!", "> @gerbauz Can you please resolve conflicts? Thanks!\r\nDone\r\n"]}, {"number": 44616, "title": "Plans to release 2.4.0 as a stable release?", "body": "Hello, I was just wondering if a date when 2.4.0 will be released as a stable version is currently available?", "comments": ["As in every other release, we try to have at least 2 RCs before the final release. Each RC is triggered at least a week from the previous one.", "RC1 was just released. Depending on amount of cherrypicks and issues discovered, there might be a 2.4 final release around Thanksgiving.", "Due to some delays, RC2 will be released this week (likely tomorrow) and final release will come after Thanksgiving.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "RC3 was released yesterday. Final release will happen next week if testing does not reveal more issues.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Final release should come this week or the next.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "TF 2.4 has been released (forgot to notify the thread at the time). Closing this"]}, {"number": 44615, "title": "Could not find a version that satisfies the requirement tensorflow (from versions: none)", "body": "System information\r\n\r\n* OS Platform and Distribution: masOS Big Sur\r\n* TensorFlow installed from (source or binary): binary\r\n* TensorFlow version: none\r\n* Python version: 3.8.2\r\n* Installed using virtualenv? pip? conda?: virtualenv + pip\r\n\r\nDescribe the problem\r\nCould not find a version that satisfies the requirement tensorflow (from versions: none)\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem\r\n```bash\r\n\u279c  ~ python -m pip install --upgrade tensorflow\r\nDefaulting to user installation because normal site-packages is not writeable\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensor flow\r\n```\r\n```bash\r\n\u279c  ~ python3 -m pip install --upgrade tensorflow==2.3.0                                        \r\nDefaulting to user installation because normal site-packages is not writeable\r\nERROR: Could not find a version that satisfies the requirement tensorflow==2.3.0 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.3.0\r\n```", "comments": ["I was able to install and run Tensorflow before I upgrade to Big Sur.\r\nThen I installed the beta version, the environment still exist but it just can't run any thing so I tried reinstalled the whole thing again.(Though later I noticed the installation renewed the PATH setting, I had already uninstalled Tensorflow.)\r\nThen this error occurred.", "@H-111-di \r\n\r\nPlease, refer similar issue #39130\r\nPlease, uninstall everything and then install in new environment.\r\n\r\n```\r\nStep 1: pip install -U pip\r\nStep 2: pip install -U setuptools\r\nStep 3: pip install tensorflow\r\n```\r\n\r\nTensorFlow 2 packages require a pip version >19.0. Could you please upgrade pip using the below command and let us know if it works.\r\n\r\npip install --upgrade pip.\r\n\r\nFor more information please check this [TensorFlow guide](https://www.tensorflow.org/install#download-a-package). Thanks!", "Well, It didn't make any changes.\r\n\r\n```bash\r\n\u279c  ~ python -m pip install -U setuptools             \r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already up-to-date: setuptools in ./Library/Python/3.8/lib/python/site-packages (50.3.2)\r\n```\r\n```bash\r\n\u279c  ~ python -m pip install -U pip          \r\nDefaulting to user installation because normal site-packages is not writeable\r\nRequirement already up-to-date: pip in ./Library/Python/3.8/lib/python/site-packages (20.2.4)\r\n```\r\n\r\nAnd I'm not running on a 32bit python\r\n```bash\r\n\u279c  ~ python3 -c \"import platform; print(platform.architecture())\"                              \r\n('64bit', '')\r\n```\r\n\r\nAlso I can't install it through wheel files.\r\n```bash\r\n\u279c  ~ python -m pip install ~/Downloads/tensorflow-2.3.1-cp38-cp38-macosx_10_14_x86_64.whl\r\nDefaulting to user installation because normal site-packages is not writeable\r\nERROR: tensorflow-2.3.1-cp38-cp38-macosx_10_14_x86_64.whl is not a supported wheel on this platform.\r\n```", "Ummm, I tried the homebrew install version(both 3.7.9 and 3.8.6) and it works fine... But the requirements system in the homebrew version seems to be something hardcoded and you can't upgrade anything it included... \r\nSo it seems like Apple had done something to their new system or the preinstalled python3.8 and that didn't meet the requirements of Tensorflow.\r\n", "@H-111-di \r\n\r\nCould you please install python 3.8 from https://www.python.org/downloads/mac-osx/ and see if the issue still persists.Thanks!", "@ravikyram,\r\nI have the same problem with system python 3.8.2 and pyenv. \r\n\r\nI tried your suggestion to install Python 3.8.6 from the python.org website and it worked.\r\n\r\nI think pyenv may need specialized instructions to work with TF.\r\n\r\nMy system\r\nMacbook pro 2019 16-inch. macOS 11.0.1 (20B29)\r\nDarwin Kernel Version 20.1.0: Sat Oct 31 00:07:11 PDT 2020; root:xnu-7195.50.7~2/RELEASE_X86_64", "> @H-111-di\r\n> \r\n> Could you please install python 3.8 from https://www.python.org/downloads/mac-osx/ and see if the issue still persists.Thanks!\r\n\r\nsorry for late reply. That works fine for me and I think the issue get solved and I can get pretty much everything work now.\r\nThanks a lot for your suggestions.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44615\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44615\">No</a>\n", " pip install tensorflow", "This is the solution\r\n\r\nhttps://makeoptim.com/en/deep-learning/tensorflow-metal"]}, {"number": 44614, "title": "Different behavior of model.predict and .call if training=True at dropout layer with pre-set seed", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nDifferent behavior of `model.predict `and `.call (model(...)),` if `training=True` is used at dropout layer. It seems that the call method does not take into account pre-set dropout seeds, while the predict method does.\r\n\r\n**Describe the expected behavior**\r\nSame behavior of the predict and call method if the same dropout seed is set and `training=True`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef build_toy():\r\n    \r\n    inputs = layers.Input(shape=(1,), name=\"inputs\")\r\n    x = layers.Dense(1, activation=\"linear\", kernel_initializer=tf.keras.initializers.Constant(value=1), name=\"dense\") (inputs)\r\n    x = layers.Dropout(0.5, seed=12, name=\"dropout\") (x,  training=True)\r\n    out = x\r\n    \r\n    model = Model(inputs, out)\r\n    \r\n    return model\r\n\r\ndata = np.array(([1]))\r\n    \r\npredict_model = build_toy()\r\nprint(predict_model.predict(data))\r\n\r\ncall_model = build_toy()\r\nprint(call_model(data))\r\n\r\n```\r\n`predict_model` always predicts the same value every time the model is newly built, while `call_model(data) ` may changes.\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/e752bda68103226a37c72fce2e2379bc/44614.ipynb). Thanks!", "Was able to reproduce your issue in Tf Nightly 2.6.0-dev20210530, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/d9725afc44102625ff5c686ee8603dff/44614.ipynb). Thanks!", "Hi @marcohuber! I was able to resolve this issue by randomizing seed value . Attaching Gist in TF [2.7 ](https://colab.research.google.com/gist/mohantym/f6226fe79ea3a613ca5c794044a812aa/44614.ipynb#scrollTo=UMHAOybY7C5F)and [nightly](https://colab.research.google.com/gist/mohantym/17f98a6570db540c754553506844fd70/44614.ipynb#scrollTo=ITWRA5fO7Hco) for reference. Thank you!", "Hi @mohantym, Thanks for the fix!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44614\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44614\">No</a>\n"]}, {"number": 44612, "title": "Slow Interpreter creation with GpuDelegate option", "body": "I use TFLite 2.3.0 on Android device\r\n\r\n**System information**\r\n- Mobile device Xiaomi Redmi 5, GPU: Adreno(TM) 506\r\n\r\nTensorFlow libraries imported in standard way: \r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'\r\n```\r\nInstance creation:\r\n\r\n```\r\n    init {\r\n        val options = Interpreter.Options().apply {\r\n            if (compatList.isDelegateSupportedOnThisDevice) {\r\n                addDelegate(GpuDelegate(compatList.bestOptionsForThisDevice))\r\n            } else {\r\n                setNumThreads(DEFAULT_THREADS_COUNT)\r\n                setUseNNAPI(DEFAULT_USE_NNAPI_OPTIONS)\r\n            }\r\n        }\r\n        interpreter = Interpreter(loadModelFile(model), options)\r\n    }\r\n\r\n    private fun loadModelFile(data: ByteArray): ByteBuffer {\r\n        return ByteBuffer.allocateDirect(data.size).put(data).order(ByteOrder.nativeOrder())\r\n    }\r\n```\r\n\r\n**Describe the current behavior**\r\n`Interpreter(loadModelFile(model), options)` takes 2400ms with GpuDelegate instead of several hundreds ms without it. I have measured `loadModelFile(model)` separately and this fun did not takes significant time.\r\n\r\n**Describe the expected behavior**\r\nI do not know, may be it is normal but it is not so convenient that instance creation takes so much time, all performance improvement that get from gpu using leveled by this delay.", "comments": ["Hi @Demin92,\r\n\r\nThe delay is expected to compile GPU shader, especially when your model is big. :( Preparing the interpreter while the app starts, and using it later would make the delay much less visible to the user.\r\n\r\n@srjoglekar246 can you add some points?", "+1 to what @teijeong  said.\r\n\r\nEnsure that you are initializing the interpreter only once during init, since the shader compiling unfortunately does take some time.\r\n\r\nFor this reason, the GPU delegate is appropriate when the number of inferences are more than a few, to offset the extended compilation time :-). If this isn't true for your case, maybe increasing the number of threads to the CPU kernels (using `SetNumThreads`) might be more useful.", "Ok, thanks for your advices, I will try it)"]}, {"number": 44611, "title": "Where is the TRTEngine saved? ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n- TensorFlow version (use command below):\r\n2.3.0\r\n- Python version:\r\n3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n11.1\r\n- GPU model and memory:\r\n\r\nI followed this answer https://stackoverflow.com/a/60744486/3265888 on stackoverflow and expect to see TRTEngine being serialized into the assets folder. I followed the doc here: https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter and used the `convert.build(input_fn)` method and everything execute without error. But when I check the savedmodel, assets folder is empty. Also when I try to run that model on jetson, the loading takes really long so I'm guessing it's generating TRTEngine on the fly? Is it a bug? or what's the correct expectation should be? \r\n\r\nApart from the above, I also don't know if the `maximum_cached_engines` option takes effect or not cus when I loaded the model and do inference, when I dynamically change the input shape (the batch_size), it seems it cached more than 1 even I'm using 1 for that option cus the second inference with that batch size is much faster than the first one. I'm not sure if batch size is counted as different shape? or only the actual shape counts as that? \r\n\r\n\r\nThis is how I load the model and do inference:\r\n```\r\n  saved_model_loaded = tf.saved_model.load(\r\n      trt_model_dir, tags=[tag_constants.SERVING])\r\n  graph_func = saved_model_loaded.signatures[\r\n      signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n  graph_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)\r\n\r\n  graph_func(input)\r\n```", "comments": ["@ysyyork,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here and also the dataset you are using. Thanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44610, "title": "Convert Tensorflow model to TFLite model - problem with boxes after quantization", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (or github SHA if from source): tf-nightly==2.5.0.dev20201104\r\n\r\n\r\n\r\nI am currently working on converting a tensorflow model to a TFLite model. By helping people from the Tensorflow community, in particular @amahendrakar.  Currently, after many attempts, I am able to convert the tensorflow model to the TFLite model\r\nHowever, there was another problem with my work. Description of the problem:\r\n\r\n**STEP 1**\r\nIn the first step, I download the ```ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8``` model and I run object detector for test image. Result (Everything is OK  but colors on the image are different than in original image):\r\n![p1](https://user-images.githubusercontent.com/28406311/98230792-dcb0ee80-1f5b-11eb-954c-ee48938fe982.jpg)\r\n\r\n**STEP 2**\r\nExport model to tflite graph using\r\n```\r\n!python object_detection/export_tflite_graph_tf2.py \\\r\n    --pipeline_config_path \"/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\" \\\r\n    --trained_checkpoint_dir \"/content/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint\"\\\r\n    --output_directory \"/content/tflite_graph\"\r\n```\r\nand convert tflite graph to tflite file using\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/tflite_graph/saved_model')\r\ntflite_model = converter.convert()\r\n\r\nwith open('/content/tflite_model/model.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\nRun object detector. Result (Everything is OK):\r\n![p2](https://user-images.githubusercontent.com/28406311/98231056-35808700-1f5c-11eb-8114-425909f24bfe.jpg)\r\n\r\n**STEP 3**\r\nConvert tflite graph to tflite file with ```tf.lite.Optimize.DEFAULT```:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/tflite_graph/saved_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nmodel_quantization_1 = converter.convert()\r\n\r\nwith open('/content/tflite_model/model_quantization_1.tflite', 'wb') as f:\r\n  f.write(model_quantization_1)\r\n```\r\nRun object detector. Result (Everything is OK):\r\n![p3](https://user-images.githubusercontent.com/28406311/98231389-a758d080-1f5c-11eb-86b9-8d3ef85fbd51.jpg)\r\n\r\n**STEP 4**\r\nConvert tflite graph to tflite file with quantization:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('/content/tflite_graph/saved_model')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\nconverter.representative_dataset = representative_data_gen\r\nmodel_quantization_2 = converter.convert()\r\n\r\nwith open('/content/tflite_model/model_quantization_2.tflite', 'wb') as f:\r\n  f.write(model_quantization_2)\r\n```\r\nRun object detector. Result:\r\n![p4](https://user-images.githubusercontent.com/28406311/98231549-d707d880-1f5c-11eb-9e1a-2862bd6deb46.jpg)\r\n\r\nAs we can see the is a problem with boxes. The size od model is the same after run ```converter.convert()``` but there is a problem with boxes. Nest step will be run final TFLite model in google coral but now I would like to solve the current problem. \r\n\r\nFull code to reproduce the problem: \r\n[ssd_modilenet_to_google_coral_step_1](https://colab.research.google.com/drive/1UBikIZmoJTuxWMXhyggBugM6dF7AWK4c?usp=sharing)\r\n\r\nI'm not sure, but it seems to me that the problem lies in the ````representative_data_gen()``` function and in the preprocessing of the imege in the object detector.\r\n\r\nBut, there is one but. If i run model from step 2 the calculation are very fast unlike of step 3 and 4.\r\n\r\n\r\n", "comments": ["@Rariusz \r\nCould you please share the colab link mentioned in the issue.", "@Saduf2019  [ssd_modilenet_to_google_coral_step_1](https://colab.research.google.com/drive/1GYKWFR7A6WknexnFa312eCa3ZST58i_3?usp=sharing)", "I ran the code and able to replicate the issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/938d85ea70a041e963dd37209398d222/untitled465.ipynb).", "@Saduf2019 Thanks!!!. Any idea why there is a problem with boxes after full discretization? In addition, I believe that the model is not fully discretized because there are warnings after discretization.", "@Rariusz You need to set this to get the full benefits of quantization:\r\n\r\n```\r\n  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\r\n                                                                  tf.lite.OpsSet.TFLITE_BUILTINS]\r\n```\r\n\r\nAnd indeed, there might be issues between the preprocessing done in your representative dataset, and your inference code. To be safest, you can use [this function](https://github.com/tensorflow/models/blob/master/research/slim/preprocessing/inception_preprocessing.py#L258) for both the representative dataset & during inference.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44610\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44610\">No</a>\n"]}, {"number": 44608, "title": "Graph disconnected when using transfer learning", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: Python 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: (nvcc) 10.1, V10.1.243; \r\n- GPU model and memory: Nvidia 2080 Ti\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen trying to obtain values for intermediete layers of a pre-trained model, I get a \"Graph disconnected\" error, even though it is connected. I can train it, evaluate it, predict images, etc.\r\n\r\nFragment of code (the whole code is in a Colab - below):\r\n\r\n```\r\nimg_size = (299, 299)\r\nIMG_SHAPE = (*img_size, 3)\r\nbase_model = tf.keras.applications.InceptionResNetV2(input_shape=IMG_SHAPE,\r\n                                                    include_top=False,\r\n                                                    weights='imagenet')\r\nbase_model.trainable = False\r\npreprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input\r\n\r\ndata_augmentation = [\r\n  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\r\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2, fill_mode=\"constant\")  \r\n]\r\n\r\ndata_augmentation = tf.keras.Sequential(data_augmentation)\r\n\r\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\n\r\ntop_layers = []\r\ntop_layers.extend([\r\n    tf.keras.layers.Dense(512, activation=\"relu\", \r\n                          kernel_initializer=\"glorot_normal\", \r\n                          bias_initializer=\"glorot_uniform\"),\r\n    tf.keras.layers.Dropout(0.2)\r\n])\r\n\r\nprediction_layer = tf.keras.layers.Dense(1, activation=\"sigmoid\", \r\n                                         kernel_initializer=\"glorot_normal\", \r\n                                         bias_initializer=\"glorot_uniform\")\r\n\r\ninputs = tf.keras.Input(shape=(*img_size, 3))\r\nx = data_augmentation(inputs)\r\nx = preprocess_input(x)\r\nx = base_model(x, training=False)\r\nx = global_average_layer(x)\r\nx = tf.keras.layers.Dropout(0.2)(x)\r\nfor layer in top_layers:\r\n    x = layer(x)\r\noutputs = prediction_layer(x)\r\nmodel = tf.keras.Model(inputs, outputs)\r\n\r\n(...)\r\n\r\nmodel2 = tf.keras.Model(inputs=model.input, \r\n                        outputs=model.get_layer('inception_resnet_v2').output)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nPossibility to obtain values of intermediete layers of a pre-trained model - maybe possibility to see all intermediate layers in model.summary(), because currently it looks like this:\r\n\r\n```\r\nModel: \"functional_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_2 (InputLayer)         [(None, 299, 299, 3)]     0         \r\n_________________________________________________________________\r\nsequential (Sequential)      (None, 299, 299, 3)       0         \r\n_________________________________________________________________\r\ntf_op_layer_RealDiv (TensorF [(None, 299, 299, 3)]     0         \r\n_________________________________________________________________\r\ntf_op_layer_Sub (TensorFlowO [(None, 299, 299, 3)]     0         \r\n_________________________________________________________________\r\ninception_resnet_v2 (Functio (None, 8, 8, 1536)        54336736  \r\n_________________________________________________________________\r\nglobal_average_pooling2d (Gl (None, 1536)              0         \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 1536)              0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 512)               786944    \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 512)               0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1)                 513       \r\n=================================================================\r\nTotal params: 55,124,193\r\nTrainable params: 787,457\r\nNon-trainable params: 54,336,736\r\n_________________________________________________________________\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1cgwJho63-QtQYS3Tdk4O5jioDu_szYjl?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-9-a7165f00310b> in <module>()\r\n      1 model2 = tf.keras.Model(inputs=model.input, \r\n----> 2                         outputs=model.get_layer('inception_resnet_v2').output)\r\n\r\n5 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in __new__(cls, *args, **kwargs)\r\n    240       # Functional model\r\n    241       from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top\r\n--> 242       return functional.Functional(*args, **kwargs)\r\n    243     else:\r\n    244       return super(Model, cls).__new__(cls, *args, **kwargs)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in __init__(self, inputs, outputs, name, trainable)\r\n    113     #     'arguments during initialization. Got an unexpected argument:')\r\n    114     super(Functional, self).__init__(name=name, trainable=trainable)\r\n--> 115     self._init_graph_network(inputs, outputs)\r\n    116 \r\n    117   @trackable.no_automatic_dependency_tracking\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\r\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\r\n    456     try:\r\n--> 457       result = method(self, *args, **kwargs)\r\n    458     finally:\r\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _init_graph_network(self, inputs, outputs)\r\n    189     # Keep track of the network's nodes and layers.\r\n    190     nodes, nodes_by_depth, layers, _ = _map_graph_network(\r\n--> 191         self.inputs, self.outputs)\r\n    192     self._network_nodes = nodes\r\n    193     self._nodes_by_depth = nodes_by_depth\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py in _map_graph_network(inputs, outputs)\r\n    929                              'The following previous layers '\r\n    930                              'were accessed without issue: ' +\r\n--> 931                              str(layers_with_complete_input))\r\n    932         for x in nest.flatten(node.outputs):\r\n    933           computable_tensors.add(id(x))\r\n\r\nValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"input_1:0\", shape=(None, 299, 299, 3), dtype=float32) at layer \"conv2d\". The following previous layers were accessed without issue: []\r\n", "comments": ["UPDATE\r\n\r\nSo, when instead of\r\n```\r\nmodel2 = tf.keras.Model(inputs=model.input, \r\n                        outputs=model.get_layer('inception_resnet_v2').output)\r\n```\r\n\r\nI use\r\n\r\n```\r\nmodel2 = tf.keras.Model(inputs=model.input, \r\n                        outputs=model.get_layer('inception_resnet_v2').get_output_at(0))\r\n```\r\n\r\nThen it works! So I believe this issue can be closed now, but I think you should include it in the documentation (it is not included right now) - for this purpose, I am leaving this issue open.\r\n\r\nAll credit should go to [Mitiku](https://stackoverflow.com/users/5825953/mitiku) from [this Stackoveflow thread](https://stackoverflow.com/questions/55073785/disconnected-graph-when-concatenating-two-models). This is his answear:\r\n\r\n> The layer you are trying to use as output has two output nodes. The first connects the input of model2 to the output of the model2. The second output node connects the output of the model1 and the first layer of model2. By default, the layer output returns only the first output node. So what is happening is you are tying to connect the input of the model_merge(input of model1) with the first output node.\r\n> \r\n> To following code shows this. Individual output nodes of the layer can be accessed using the get_output_at() method of the layer.\r\n> \r\n> `layer_output = model_merge.layers[-2].output # The first output node`\r\n> `layer_output_1 = model_merge.layers[-2].get_output_at(0) # The first output node`\r\n> `layer_output_2 = model_merge.layers[-2].get_output_at(1) # The second output node`\r\n> Now the following two codes throws error because the graph is disconnected.\r\n> \r\n> `test2 = Model(inputs=model_merge.input, outputs=layer_output)`\r\n> and\r\n> \r\n> `test2 = Model(inputs=model_merge.input, outputs=layer_output_1)`\r\n> But the code below doesnot throw error, because the graph is connected.\r\n> \r\n> `test2 = Model(inputs=model_merge.input, outputs=layer_output_2)`", "Thanks for the issue. The docs are now updated. "]}, {"number": 44607, "title": "tensorflow lite Makefile issue : nnapi_delegate_provider.cc is not compiled and nnapi cannot be used from command line of benchmarck_model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): yocto build system\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): compiled from source\r\n- TensorFlow version: 2.3\r\n- Python version: NA\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): Not using bazel but Make\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: NA \r\n- GPU model and memory: NA\r\n\r\n**Describe the problem**\r\nI am trying to compile tensoflow-lite using the Makfile. I want to use nnapi. \r\nThe issue is: nnapi_delegate_provider.cc is not compiled so we cannot use the switch `--use_nnapi=1` with `benchmark_model`\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`make -f tensorflow/lite/tools/make/Makefile BUILD_WITH_NNAPI=true`\r\n\r\n**Any other info / logs**\r\nHere is a quick patch that solve the issue : [0001-Add-nnapi_delegate_provider.cc-to-the-list-of-source.txt](https://github.com/tensorflow/tensorflow/files/5492886/0001-Add-nnapi_delegate_provider.cc-to-the-list-of-source.txt)\r\nI would be very happy to open a merge request on it. But I need  feedback before, because I think the same problem happens for all delegates (I only tested nnapi).  I don't see why we filter out all delegates provider from `BENCHMARK_LIB_SRCS` variable:\r\n\r\n- right now, they are not added anywhere, so it does nothing (or I missed it inside the Makefile)\r\n- filter them out is not what should be done (I guess), but we should instead add the providers\r\n\r\n as needed\r\n\r\nWhat do you think? Maybe I completely miss something.. \r\nBest \r\n\r\n", "comments": ["@justeph Could you please try on the stable TF v2.6.0 and refer to the [TF lite NNAPI delegate](https://www.tensorflow.org/lite/performance/nnapi).Please  let us know if it helps ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44607\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44607\">No</a>\n"]}]