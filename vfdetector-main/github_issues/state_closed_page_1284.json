[{"number": 14606, "title": "Converting unsupported operation: Dequantize", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: centos 7\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI use the command:\r\nbazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/home/guoxiang/workspace/tensorflow/sms_model/optimized_sms.pb' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--output_file=/home/guoxiang/workspace/tensorflow/sms_model/sms_char_cnn.lite' '--inference_type=QUANTIZED_UINT8' '--input_type=QUANTIZED_UINT8' '--input_arrays=embedding_matrix' '--output_arrays=final_scores' '--input_shapes=1,5000,64,1' --logtostderr '--v=2'\r\n\r\nto convert a quantized model to a  lite model, got the following error:\r\nUnimplemented: this graph contains an operator of type (Unsupported TensorFlow op: Dequantize) for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n\r\nso , now qutianzed model is still not supported ?\r\n", "comments": ["I met the same problem. Have you solved it?", "We are planning to implement this op.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I met the same problem too. Have anyone solved it?\uff08use tf1.5\uff09", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "This commit added support for dequantize?\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/2b211b681ac6264c61372d10c496e234bf2eda9b#diff-b05587db08e591ba26c744470c15b6a8\r\n\r\nCan someone confirm? Thanks!", "That's partial support, designed to work with TOCO's --inference_input_type=QUANTIZED_UINT8. It still doesn't work with tensorflow's graph_transform quantization scheme (as far as I can tell)", "Thanks @andrehentz. If my graph has fake quantization nodes in it (e.g. the most recent mobilenet-v1), should I expect to run toco to get rid of the fake nodes and keep all the uint8 weights? And then if I run toco again to go from .pb to .lite? And the TFLite runtime is happy?", "@h8907283 yes, yes, and yes :)\r\n\r\nThat was the case even before that particular commit, so if you are still having trouble with that we can take a closer look.", "Couldn't possibly get a better answer than this. Thanks @andrehentz !!!", "Hi @h8907283, is there a way to directly transform saved tensorflow model into fake quantized model without retraining ?", "@czchao  Fake quantized graphs have additional nodes to facilitate the fixed-pt-forward-float-backprop training flow, if you follow Google's approach. My expectation is that I can obtain a graph with fake quantization nodes embedded and given a checkpoint, I can retrain. Eventually, I will be able to generate a float or quantized graph after training.", "@aselle I used --transform='remove_nodes(op=RandomShuffleQueueV2)'. But it still wrong: Unimplemented: this graph contains an operator of type(Unsupported Tensorflow op: RandomShuffleQueueV2) for which the quantized form is not yet implemented. What should I do? Many thanks.", "Nagging Assignee @andrehentz: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @andrehentz: It has been 48 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Update: TF Lite now includes an option to quantized the weights in a model. You can pass --quantize_weights=true to TOCO and have the large weight tensors automatically quantized. **This will affect accuracy, so be sure to check before deploying you model.**\r\n\r\nI'll close this issue. Please open separate issues if necessary.\r\n\r\nReference: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/toco_flags.proto#L176\r\n"]}, {"number": 14605, "title": "Linking error \"Undefined symbols for architecture x86_64:   \"nsync::nsync_mu_init(nsync::nsync_mu_s_*)\"\" on iOS", "body": "Hello,\r\n\r\nI downloaded the latest release 1.4.0 and builded it for iOS with build_all_ios.sh, and can successfully build libtensorflow-core.a, libprotobuf-lite.a and libprotobuf.a\r\n\r\nbut error happened when link these libs to iOS project, the detail info as follows, can anybody know how to fix it? thanks\r\n\r\nUndefined symbols for architecture x86_64:\r\n  \"nsync::nsync_mu_init(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::Env::Env() in libtensorflow-core.a(env.o)\r\n      __GLOBAL__sub_I_allocator.cc in libtensorflow-core.a(allocator.o)\r\n      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core.a(session_factory.o)\r\n      tensorflow::SessionFactory::GetFactory(tensorflow::SessionOptions const&, tensorflow::SessionFactory**) in libtensorflow-core.a(session_factory.o)\r\n      tensorflow::TrackingAllocator::TrackingAllocator(tensorflow::Allocator*, bool) in libtensorflow-core.a(tracking_allocator.o)\r\n      tensorflow::TrackingAllocator::TrackingAllocator(tensorflow::Allocator*, bool) in libtensorflow-core.a(tracking_allocator.o)\r\n  \"nsync::nsync_mu_lock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::FileSystemRegistryImpl::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<tensorflow::FileSystem* ()>) in libtensorflow-core.a(env.o)\r\n      tensorflow::FileSystemRegistryImpl::Lookup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libtensorflow-core.a(env.o)\r\n      tensorflow::FileSystemRegistryImpl::GetRegisteredFileSystemSchemes(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >*) in libtensorflow-core.a(env.o)\r\n      tensorflow::CPUAllocator::AllocateRaw(unsigned long, unsigned long) in libtensorflow-core.a(allocator.o)\r\n      tensorflow::CPUAllocator::DeallocateRaw(void*) in libtensorflow-core.a(allocator.o)\r\n      tensorflow::CPUAllocator::GetStats(tensorflow::AllocatorStats*) in libtensorflow-core.a(allocator.o)\r\n      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core.a(session_factory.o)\r\n      ...\r\n  \"nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)\", referenced from:\r\n      tensorflow::FileSystemRegistryImpl::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<tensorflow::FileSystem* ()>) in libtensorflow-core.a(env.o)\r\n      tensorflow::FileSystemRegistryImpl::Lookup(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) in libtensorflow-core.a(env.o)\r\n      tensorflow::FileSystemRegistryImpl::GetRegisteredFileSystemSchemes(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >*) in libtensorflow-core.a(env.o)\r\n      tensorflow::CPUAllocator::AllocateRaw(unsigned long, unsigned long) in libtensorflow-core.a(allocator.o)\r\n      tensorflow::CPUAllocator::DeallocateRaw(void*) in libtensorflow-core.a(allocator.o)\r\n      tensorflow::CPUAllocator::GetStats(tensorflow::AllocatorStats*) in libtensorflow-core.a(allocator.o)\r\n      tensorflow::SessionFactory::Register(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::SessionFactory*) in libtensorflow-core.a(session_factory.o)\r\n      ...\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)", "comments": ["try add ~/tensorflow/tensorflow/contrib/makefile/downloads/nsync/builds/lipo.ios.c++11/nsync.a into\r\nother linker flags", "Yes, it works, thanks so much !!!", "@apollo-time @llyyun I'm having trouble too - I tried adding `-lnsync` (since I include `nsync.a` in the project), and also tried including it directly via `-l~/tensorflow/tensorflow/contrib/makefile/downloads/nsync/builds/lipo.ios.c++11/nsync.a` but I still get \r\n```\r\nld: library not found for -l~/tensorflow/tensorflow/contrib/makefile/downloads/nsync/builds/lipo.ios.c++11/nsync.a\r\n```\r\n\r\n(I also confirmed that the file does indeed exist at that path)\r\n\r\nCould you please be more specific about how to add this to the linker flags? (I'm not good with wrangling the compiler, sorry)\r\n\r\n**Update**: I figured out you meant *just to add the path* without any flag like `-l`, which works. But I'm working on a cocoapod that includes a custom tensorflow build - and when cocoapods is instructed to include a library, it automatically adds the `-lnsync` flag. So I guess what I'm asking is, why is `nsync` included in a different way than other libraries?", "I ran into a similar problem building makefiles from **macos** and I solved a similar error using this:\r\n\r\n```\r\nmake -f tensorflow/contrib/makefile/Makefile HOST_LDOPTS=tensorflow/contrib/makefile/downloads/nsync/builds/default.macos.c++11/nsync.a\r\n```", "I'm having the same issue. @apollo-time can you please be more specific? Thanks."]}, {"number": 14604, "title": "Implement LoggingAsync for GRPC Worker Services", "body": "This allow RecvTensor events to show up in StepStats and in turn in Chrome Tracing format.", "comments": ["Can one of the admins verify this patch?", "@pbar @mrry @suharshs Could you please take a look at this?", "Reassigning this to @prb12, since he knows the ins and outs of the logging path better than I do. A few quick observations though:\r\n\r\n* Instead of modifying the `BaseRendezvousMgr` and related classes to capture this information, please use the existing stubs in `GrpcWorkerCache`:\r\n\r\n  https://github.com/tensorflow/tensorflow/blob/c0662f1620c2b97abb79b8ae6a8a30f7c7719475/tensorflow/core/distributed_runtime/rpc/grpc_worker_cache.cc#L86-L92\r\n\r\n* The changes in `master_session.cc` are potentially controversial, because they could cause a lot of additional logging to be generated when `FULL_TRACE` is specified. We might need an additional `TraceLevel` for this case.\r\n\r\n* Run `clang-format -style=Google` over the code to take care of style issues (which should trigger a failure in the presubmit).", "Thanks @mrry for the feed back:\r\n* I changed the implementation to use SessionMgr instead of *RendezvousMgr. \r\nNote that unlike other worker rpcs, `LoggingRequest` in  `LoggingAsync` rpc does not have any session handle, therefore all sessions should be touched. Both implementations (previous one, and this one) iterates over all sessions (previously through `RendezvousMgr` now `SessionMgr`), then uses `GrpcWorkerCache` calls in each session.\r\n\r\n* @pbar: Thoughts on changes in `master_session.cc`?", "@xldrx can you look into the merge conflicts. ", "Hey @sb2nov,\r\nIt seems that Paul is swamped. Could you review this PR?\r\n", "@xldrx I don't know enough to review this right now. @prb12 ping. ", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@prb12 Thanks for the review. \r\n> I'm assuming this doesn't have a noticeable performance impact when tracing is not enabled?\r\n\r\nThis PR does nothing when tracing is not enabled.\r\n\r\n> One minor question about the case where clear and retrieve are done at the same time.\r\n\r\nGreat observation. Fixed.", "@xldrx This appears to have numerous broken tests. Can you please take a look?", "@xldrx Thanks for the contribution!"]}, {"number": 14603, "title": "add Unspecified dimension to an existing tensor.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: osx\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**:  ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**:   Python 2.7.11\r\n- **Bazel version (if compiling from source)**: 0.7.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\n== cat /etc/issue ===============================================\r\nDarwin Pengs-MacBook-Pro.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\nMac OS X 10.12.6\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 9.0.0 (clang-900.0.38)\r\nTarget: x86_64-apple-darwin16.7.0\r\nThread model: posix\r\nInstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin\r\n\r\n== uname -a =====================================================\r\nDarwin Pengs-MacBook-Pro.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nprotobuf (3.4.0)\r\ntensorflow (1.3.0)\r\ntensorflow-serving-api (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 25, in <module>\r\n    from tensorflow.python.platform import self_check\r\nImportError: No module named platform\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tools/tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nsubgraph one have output:\r\n```\r\nstate = Tensor(80, 80, 4)\r\n```\r\nsubgraph two have input:\r\n```\r\nstates = Tensor(None, 80, 80, 4)\r\n```\r\nright now `tf.expand_dims` can only make state a `Tensor(1, 80, 80, 4)` which can not being assigned to `states`. so that i have to do multiple sess execution and make two graph\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@yupbank can you please describe the specific error messages that you're getting? It seems like the static shape (1,80,80,4) ought to be OK to pass to a function that expects the dynamic shape (None,80,80,4).", "say  i have a graph with two sub graph:\r\n```\r\ninput_images = tf.placeholder(tf.float32, shape=[None, 1024, 2024, 3])\r\ninput_state = resize_and_folder_multi_image_into_one_state(input_images)\r\ninput_state.shape == (80, 80, 4)\r\n\r\ninput_states = tf.expand_dims(input_state, 0)\r\n\r\ninput_states.shape == (1, 80, 80, 4)\r\n\r\n# i want to have a input_states acting both as middle node of graph \r\n\r\n# but also can acting as input of a graph,  \r\n\r\n# what operation can i do to make \r\n\r\n# input_states.shape compatible with (None, 80, 80, 4)\r\n \r\n\r\naction = q_function_accept_input_states_and_predict_actions(input_states)\r\n```\r\nthis would work but will break when i pass multi input_states data into `input_states`\r\n\r\nthis is my work around..  \r\n\r\nhttps://github.com/yupbank/dqn-tf/blob/master/trainer/task.py#L59\r\nhttps://github.com/yupbank/dqn-tf/blob/master/trainer/task.py#L64-L70\r\n\r\nbut my case it's lucky that both function `resize_and_folder_multi_image_into_one_state` and  `q_function_accept_input_states_and_predict_actions`  are zero centered \r\nand  the walk around would fail if any of them are not zero centered.. \r\n\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14602, "title": "Fix the mac build", "body": "This test times out at 300 seconds. It takes almost exactly 300 seconds.", "comments": ["Jenkins, test this please"]}, {"number": 14601, "title": "Conv2D operator with SAME padding when Stride > kernel size showing unexpected results", "body": "### System information\r\n- **Have I written custom code -- YES, only to demonstrate the problem (source code is below)**:\r\n- **OS Platform and Distribution (Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (binary (PIP))**:\r\n- **TensorFlow version (1.4.0)**:\r\n- **Python version (2.7.12)**: \r\n- **Bazel version (N/A)**:\r\n- **GCC/Compiler version (N/A)**:\r\n- **CUDA/cuDNN version (N/A)**:\r\n- **GPU model and memory (N/A -- CPU only)**:\r\n- **Exact command to reproduce (See Source Code Below)**:\r\n\r\n### Describe the problem\r\nThere is an inconsistency between the convolution documentation on padding with 'SAME' located [here](https://www.tensorflow.org/api_guides/python/nn#Convolution) and the behavior of the tf.nn.conv2d operator. In the example below I create a 3x1 input with values [[1.0][1.1][1.2]] and a 1x1 filter of value [1.0]. I specify the stride to be 1x3x1x1 which should result in only a single element be output and the padding to be 'SAME'. From the padding calculation in the above link: \r\n\r\npad_along_height:\r\n    \r\n    in_height ( = 3) % strides[1]( = 3) == 0 so\r\n    pad_along_height = max(filter_height ( = 1) - strides[1] ( = 3), 0)\r\n    pad_along_height = max(-2, 0) = 0\r\n\r\npad along_width:\r\n\r\n    in_width ( = 1) % strides[2] ( = 1) == 0 so\r\n    pad_along_width = max(filter_width( = 1) - strides[2] ( = 1), 0\r\n    pad_along_width = max(0,0) = 0\r\n\r\nMy hypothesis is that pad_along_* is not using the max(x,0) and as a result, pad_along_height = -2. Therefore pad_top = -1 and pad_bottom = -1. If that was the case, then our input is reduced to only the middle element [1.1] which explains why the TF result of the code below is 1.1 rather than the expected 1.0 (value of first input).\r\n\r\nIf I change the padding to be VALID (no padding) then this code below gives the result of 1.0 or if i instead change the stride to 1,2,1,1 i get the expected value of 1.0 (although in this case my hypothesis proposes that pad_bottom is still -1).\r\n\r\n### Source code / logs\r\n    import tensorflow as tf\r\n    import numpy as np\r\n\r\n    i = tf.constant((np.ones(3) + np.arange(3) * 0.1).reshape(1,3,1,1), dtype=tf.float32, name='input')\r\n    f = tf.constant(np.ones(1).reshape(1,1,1,1), dtype=tf.float32, name='filter')\r\n\r\n    conv = tf.nn.conv2d(input=i, filter=f, strides=(1,3,1,1), padding='SAME')\r\n\r\n    with tf.Session() as sess:\r\n        out = sess.run(conv)\r\n        print out\r\n\r\nOutput:\r\n`[[[[ 1.10000002]]]]`", "comments": ["I believe that `max(0, pad)` works well, see:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/69620e12bf3403a11a47b26006ebb22656a9f036/tensorflow/core/framework/common_shape_fns.cc#L42-L44\r\n\r\nAnd I find the stride information isn't packed into `args` in `LaunchDeepConvOp`, perhaps the problem is here but I am still not sure about it:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/69620e12bf3403a11a47b26006ebb22656a9f036/tensorflow/core/kernels/conv_ops.cc#L158-L169\r\n", "This only occurs on the CPU, not the GPU. The fact the CPU and GPU results differ is a serious issue.\r\n\r\n/CC @yzhwang @rmlarsen", "@Oewyn Thanks for reporting the issue.\r\nI believe the issue is due to code here:\r\nhttps://bitbucket.org/eigen/eigen/src/b6e6d0cf6a77518580b5c7070beda7101118067c/unsupported/Eigen/CXX11/src/Tensor/TensorImagePatch.h?at=default&fileviewer=file-view-default#TensorImagePatch.h-266\r\nAnd you are right on how to fix this too. We have noticed this internally too and have fixed the internal Eigen by adding the max(0, x), but since the OSS TensorFlow depends on the OSS Eigen which is hosted on bitbucket, I haven't yet create a pull request to make my change there. I will try to do so today.", "I have sent the pull request, after it gets merged, the bug should be fixed:\r\nhttps://bitbucket.org/eigen/eigen/pull-requests/352/update-the-padding-computation-for/diff", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "@yzhwang this appears to be merged. Is the issue fixed?", "@gunan A change to eigen that should fix this was just merged:\r\nhttps://bitbucket.org/eigen/eigen/commits/2355b229ea4c\r\nI'm not sure how do we update our eigen dependency. If it's not scheduled or done automatically, would you please update it to the latest version of eigen so that @Oewyn can verify if this will fix the issue? Thank you!", "@yzhwang @gunan I created a PR #16705 to update the Eigen library to `2355b229ea4c`. Please take a look.", "Thanks @gunan and @yongtang !"]}, {"number": 14600, "title": "[Speech commands] Add `num_classes=label_count` when constructing the\u2026", "body": "\u2026 confusion matrix\r\n\r\nThis seems like it should fix an issue where you get unlucky, the batch doesn't contain the largest label, and the returned matrix is smaller than other runs.", "comments": []}, {"number": 14599, "title": "Add compression support for TextLineReader", "body": "This fix tries to address the issue raised in #14593 where it was not possible to process compressed TextLineReader.\r\n\r\nThis fix add compression support for TextLineReader, similar to TFRecordReader.\r\n\r\nAdditional tests have been added.\r\n\r\nThis fix fixes #14593.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>\r\n", "comments": ["@rohan100jain can you take a look?", "(From API review)\r\n\r\nIs this still required after https://github.com/tensorflow/tensorflow/pull/15132 ? Once that PR is merged, then it seems like the behavior of this PR will be the same as connecting the existing `TextLineReader` op to the output of a `DecodeCompressed` op?\r\n\r\nAre we missing something?", "@asimshankar Yes I think with `decode_compressed` then most of the compression processing could be considered as apply the op to the input/output.\r\n\r\nThis PR was trying to address the issue raised #14593 which was early. \r\n\r\nLater on #14887 was opened, so PR #15132 was created.\r\n\r\nAt the moment there are several ops that support option of compression:\r\n```\r\nTextLineDataset\r\nTFRecordDataset\r\nTFRecordReader\r\nTFRecordReaderV2\r\nFixedLengthRecordReaderV2\r\n```\r\n\r\nSo there might be some overlap/inconsistency on which method/option to use when dealing with compressions.\r\n\r\nMaybe there could be some consolidation for that, or just leave old method alone if the API could not change any more?\r\n\r\nI am open either way. Or we could just close this PR for now.\r\n\r\n", "@yongtang do we still plan to proceed with this PR now that #15132 is in?", "I think I can close this PR. If there is a compelling need to add the option I can reopen, though don't see a big need for now. Thanks all for the help! \ud83d\udc4d "]}, {"number": 14598, "title": "upgrade minSdkVersion to 21", "body": "Fix all 56 lint errors about NewApi: Calling new methods on older versions", "comments": ["Can one of the admins verify this patch?", "@aselle is it okay if we bump the minSdkVersion?", "@aselle any comment on this?", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 210 days with no activity and the `awaiting review` label has been applied.", "Seems like this change was already made as part of 23d602a7da399ded85044a82235ef8cf22ef2be6"]}, {"number": 14597, "title": "Disable slice_op_test.", "body": "", "comments": ["Closing in favor of https://github.com/tensorflow/tensorflow/pull/14602"]}, {"number": 14596, "title": "Bug: tf.data.Dataset.map computes unrequested graph parts ", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: See code example at the bottom\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: Python 3.6.1 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n**Short**: `tf.Session.run` does not compute unnecessary things that are not requested, except for the case the tensorflow code in inside a `tf.data.Dataset.map`. \r\nSo is it possible to add this feature to `tf.data.Dataset.map`?\r\nMaybe the problem is in `tensorflow.python.framework.function.Defun`.\r\n\r\n**Long**: I want to build a fully featured input pipeline that provides everything. Than should tensorflow determine what is necessary to compute. When I tried to figure out if this is possible I found that the dataset has some code for parallel execution. So preprocessing should be inside the `Dataset` pipeline. \r\nWhen I looked at the source code I think the reason may be connected to `tensorflow.python.framework.function.Defun`, but I can not find the motivation to use Defun and the initial commit (2017-05-17) under contrib had already `Defun` used.\r\nWith my knowledge as a tensorflow beginner, I can only fix this when I ignore parallel execution (i.e. remove all `Defun`s), but then I can also do the transform after `Iterator.get_next()`.\r\n\r\nMaybe @mrry knows more about this?\r\n\r\n### Source code / logs\r\n\r\nHere a small example that demonstrates this behavior. (Node the `tf_sleep(idx, 0.1)` is a open end in the graph and the print should never be executed.)\r\n```python\r\nimport tensorflow as tf\r\nimport functools\r\n\r\n# -----------------simple print when this is executed------------------------------------\r\ndef sleep(tensor, seconds):\r\n    time.sleep(seconds)\r\n    print(f'Sleeped for {seconds}s')\r\n    return tensor\r\n\r\ndef tf_sleep(tensor, seconds):\r\n    return tf.py_func(sleep, [tensor, seconds], tensor.dtype, name='speep')\r\n# ------------------------------------------------------------------------------------------------\r\ndef transform(idx):    \r\n    tf_sleep(idx, 0.1)  # Dead graph end, should never be executed\r\n    return tf_sleep(idx, 0.2)\r\n    \r\nds = tf.data.Dataset.range(20)\r\nds = ds.map(transform)  # will produce \"Sleeped for 0.1s\"\r\n\r\niterator = ds.make_one_shot_iterator()\r\nentry = iterator.get_next()\r\n\r\nentry = transform(entry)  # does not produce \"Sleeped for 0.1s\"\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run(entry))\r\n# Output: \r\n# Sleeped for 0.20000000298023224s\r\n# Sleeped for 0.10000000149011612s  # <-- this should not be printed\r\n# Sleeped for 0.20000000298023224s\r\n# 0\r\n```\r\n", "comments": ["@mrry any idea?", "Yes, this is an unfortunate implementation detail of `Defun`, which I'm in the process of fixing. We should be able to prune from a function body all *stateless* nodes that do not influence either a return value or the input to a stateful node. This should apply to `Dataset.map()` and other `Dataset` transformations that are parameterized by a function.", "I am not sure if it is possible, but it would be really nice if there is a full analysis of the `Dataset` graph.\r\n - The simplest case is what you describe, prune the function body of `Defun`.\r\n - But what if you do `Dataset.map().map()`? Does the `Defun` allow detect _stateless_ nodes from the second map in the first map?\r\n - Maybe the most difficult but also the most interesting case. Is it possible to prune stateless nodes that are stateless because they are not used inside the main graph? This would be really nice because currently, my dataset has input arguments for the python function to only load the relevant part, but this could also be done from the static graph.\r\n", "Yes, we have plans to perform deeper rewrites on the `Dataset` graph that should cover at least the first two cases. The third case is difficult because an `Iterator` might have multiple consumers that you different sets of the outputs from a `Dataset`, and a new consumer that requires a superset of the existing consumers' outputs might arrive after we start processing. We'd need a more restrictive API to capture that potential optimization, and it might be worth trying to think of one. In the meantime, we'd probably encourage users to add a `Dataset.map()` to project away unused outputs, and use the second case to make that more efficient.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "The original issue was fixed in 6548e417f8d26e81d10ee577f8575b1cebc443a8, so I'm going to close this. "]}, {"number": 14595, "title": "Adding a custom Tensorflow Op under Windows/cmake does not work with TF_LoadLibrary", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have a custom fork (https://github.com/stefanseibert/tensorflow/tree/r1.3) which is forked from r1.3 and the only modifications are some commented out lines to be able to build AVX support as described in another ticket, and the recent added fix for wide strings.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10, 64 bit, cmake\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBuilt from source under windows with the cmake setup\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.3.1\r\n\r\n- **Python version**: \r\n3.6\r\n\r\n- **Bazel version (if compiling from source)**:\r\nDoes not apply\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nMicrosoft (R) Build Engine version 14.0.25420.1\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 8 / cuDNN 5.1`\r\n\r\n- **GPU model and memory**:\r\nGTX 980 Ti 6GB, 64GB main memory\r\n\r\n- **Exact command to reproduce**:\r\nDoes not apply\r\n\r\n### Describe the problem\r\nWhen using the cmake setup on windows to build Tensorflow from source and using AddUserOps which was added from @guschmue some time ago (example like https://gist.github.com/guschmue/2908e4411edc2faef6ddfe87a2ce4a1d), I am able to build GPU enabled tensorflow ops. I can load the DLL in python with tf.load_op_library() and can actually use it there. I can built GraphDefs with my custom Op and export it as protobuf file.\r\n\r\nWhen I try to use this graph for inference in another application where I use the C++ API its not possible for me to get the op loaded and registered. Loading the same DLL (as which with the python API succeeds) works on a system level with C/C++ (the DLL is loaded successfully as seen through procmon.exe or dependency walker and TF_LoadLibrary returns with status ok) but when trying to run the Graph afterwards the custom op is not recognized from Tensorflow and Tensorflow errors with \"Not found: Op type not registered...\". Trying to get the OpList afterwards with the Lib handle also returns no ops. So somehow the ops are not seen here even though they are recognized from python side. I tried a lot of different things to circumvent this like described in my Stack Overflow Question here:\r\n\r\nhttps://stackoverflow.com/questions/47309425/tensorflow-op-and-kernel-do-not-register-on-windows-with-cmake\r\n\r\nbut none of the approaches worked. It seems like that the op registration which is done after loading the DLL on python side is not properly done when performing the same operation with TF_LoadLibrary from C/C++. Since I cannot get tensorflow built with debug symbols I dont have a callstack where this registration fails unfortunately.\r\n", "comments": ["Have not tried tried this for some time but it used to work. Should still work because ops like gru use the same mechanism. I can take a look ... might take me a few days to get to it. For debug builds: they are broken a lot. I use RelWithDebInfo which has only the problem that combined pdb for the tensorflow.dll is >4GB which the linker fails on. My workaround is to delete some of the pdb's I don't care about before linking.", "Thanks for the response! Thats a good idea with deleting the pdb files. I will try to get a RelWithDebInfo build using that to see a bit more whats happening.", "Update:\r\nIn my setup I am linking to the Tensorflow C++ API through a self built tensorflow.dll and then building myself a DLL that is loaded into a 3rd party application. I managed to get it working by completely building and linking all the ops myself with msvc and nvcc and calling the Op/Kernel Registration Macros directly from my DLL code. With this setup I am not using the tensorflow build op setup at all. It seems to work fine so far. I can use the ops with my graphs. I only have to use the tensorflow build setup when I wanna use the python api to create the graph structures. Makes sense? I am not sure if the above created workflow is supposed to work.", "/CC @mrry, any comments?", "Nothing springs to mind. @guschmue Do you have any ideas why this might not be working in the C++ build?", "The workflow sound correct - as soon you load the dll with your ops the static initializers will register the op and they should be available in c++. Some people around me are actively using this.\r\n@stefanseibert, is this working for you after your last update?", "Due to time constraints I sticked with my own setup as described above which works fine. Should I close it? Seems to be specific to my setup.", "I have a similar issue regarding the tensorflow graph exporting part and loading.\r\n\r\nI am using code from [light headed rfcn](https://github.com/zengarden/light_head_rcnn) in that I am working on rfcn part inside [experiments](https://github.com/zengarden/light_head_rcnn/tree/master/experiments/lizeming/rfcn_reproduce.ori_res101.coco.baseline) folder.\r\nI am able to train it as well as test the same no issues.\r\n\r\nI am using simple script to load the checkpoint I am unable to do it yet.\r\n```\r\nimport os\r\nimport tensorflow as tf\r\n\r\n\r\n\r\nwith tf.Session() as ss:\r\n    cpath = r'/home/nithish/python/RFCN/light_head_rcnn/output/nithish/rfcn_reproduce.ori_res101.coco.baseline/model_dump'\r\n    saver = tf.train.import_meta_graph(os.path.join(cpath,'epoch_29.ckpt.meta'))\r\n    saver.restore(ss,os.path.join(cpath,'epoch_29.ckpt'))\r\n    \r\n    print('loading done!')  is the script.\r\n`\r\nThe problem i am facing is \r\n\r\n`\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"/home/nithish/my_install/miniconda3/envs/RFCN_Tensor/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1435, in import_meta_graph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"/home/nithish/my_install/miniconda3/envs/RFCN_Tensor/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1457, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"/home/nithish/my_install/miniconda3/envs/RFCN_Tensor/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"/home/nithish/my_install/miniconda3/envs/RFCN_Tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/nithish/my_install/miniconda3/envs/RFCN_Tensor/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 399, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"/home/nithish/my_install/miniconda3/envs/RFCN_Tensor/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 159, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'PSROIPool'\r\n\r\n`\r\n\r\nAnd found out that this PSROIPool is being called not from tensorflow but through \".so\" in the code there a function call to \"C/C++\" implementation of the code for this pooling.\r\n\r\nI wanted to freeze the graph and convert it to use 'tflite quantization' for which it needs `save_model.pb and variables`. I am facing same issue loading the model there also with same error. \r\n\r\nHow to incorporate this function call which is a \"c api '\" implementation for \"PSROIPool\" inside the `*.pb` file ?"]}, {"number": 14594, "title": "Unable to download  quantized Mobilenet TensorFlow Lite model ", "body": "### Describe the problem\r\nI am trying to build TensorFlow Lite Android app,  was following the [instruction](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite#building-in-android-studio-using-tensorflow-lite-aar-from-jcenter). \r\n\r\n> Download the quantized Mobilenet TensorFlow Lite model from [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip)\r\n> unzip and copy mobilenet_quant_v1_224.tflite to the assets directory: tensorflow/contrib/lite/java/demo/app/src/main/assets/\r\n\r\nhowever, the link is not open to the public \r\n\r\nhttps://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip\r\n\r\n```\r\n<Error>\r\n<Code>AccessDenied</Code>\r\n<Message>Access denied.</Message>\r\n<Details>\r\nAnonymous users does not have storage.objects.get access to download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip.\r\n</Details>\r\n</Error>\r\n```\r\n", "comments": ["Please try now, this has been fixed.\r\n\r\nAnitha"]}, {"number": 14593, "title": "TextLineReader is missing compression option (see TFRecordReader)", "body": "`tf.TextLineReader` does not currently provide a way to read GZIP encoded files\r\n\r\nThis option exists in `TFRecordReader`:\r\n```\r\noptions = tf_record.TFRecordOptions(TFRecordCompressionType.GZIP)\r\nreader = io_ops.TFRecordReader(name=\"test_reader\", options=options)\r\n```\r\n\r\nIt would be great to be able to ingest gzipped files into the TextLineReader as well:\r\n```\r\nreader = tf.TextLineReader(compression_type=\"gzip\")\r\n```\r\n\r\nThis `compression_type` argument could pass down to\r\n\r\n```\r\nrr = gen_io_ops._text_line_reader_v2(skip_header_lines=skip_header_lines, name=name, compression_type=compression_type)\r\n```\r\n in the same way the TFRecordReader passes down to `_tf_record_reader_v2`: \r\n```rr = gen_io_ops._tf_record_reader_v2(name=name, compression_type=compression_type)```\r\n\r\nWhich would seem to require implementing compression here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/text_line_reader_op.cc", "comments": ["Added a PR #14599 to support compression on TextLineReader. Please take a look.", "awesome. That looks like exactly what I want :)", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "@atomantic Please see the discussion in #14599. Now as #15132 has been merged, I think it is possible to read compressed format file and decode as needed. I will close this issue for now. Please feel free to continue discussion or re-open if needed."]}, {"number": 14592, "title": "add photo selection and some changes in ios simple app", "body": "add photo selection and image preview for tensorflow lite simple app", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@jhseu any update from this?", "@gorBaghdasaryan could you check what the problem is? `check_futures` failed to build.", "@drpngx  ```check_futures``` failed on master to ", "@gorBaghdasaryan what do you mean?", "@gunan @yifeif it looks like `kokoro:force-run` is not triggering, for some reason.", "Sometimes there may be some delay for builds to start, especially if the build infra becomes etmporarily overloaded.", "@dmaclach could you please take another look after the latest changes?", "BTW I am wandering why ```build_all_linux``` contains links to iOS specific files like Lunch Screen storyboard?\r\n\r\nHere the build invocation log:\r\n```\r\n  _bin/build-runfiles bazel-out/k8-opt/bin/tensorflow/contrib/makefile/build_all_linux.runfiles_manifest bazel-out/k8-opt/bin/tensorflow/contrib/makefile/build_all_linux.runfiles)\r\n_bin/build-runfiles (args bazel-out/k8-opt/bin/tensorflow/contrib/makefile/build_all_linux.runfiles_manifest bazel-out/k8-opt/bin/tensorflow/contrib/makefile/build_all_linux.runfiles): link or target filename contains space on line 2193: 'org_tensorflow/tensorflow/contrib/lite/examples/ios/simple/Launch Screen.storyboard /tmpfs/src/github/tensorflow/tensorflow/contrib/lite/examples/ios/simple/Launch Screen.storyboard'\r\n```", "Tests pass. @dmaclach please confirm you're OK with that.", "@dmaclach any chance to take a look?", "Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 60 days with no activity and the `awaiting review` label has been applied."]}, {"number": 14591, "title": "iOS Camera example for TensorFlow Lite.", "body": "The code is derived from the TensorFlow mobile camera example in\r\ntensorflow/tensorflow/examples/ios/camera/.", "comments": ["Can one of the admins verify this patch?", "@aselle @petewarden Could you review this?", "Can one of the admins verify this patch?"]}, {"number": 14590, "title": "Loading TF 1.1 model in TF 1.4", "body": "There is a model which has been trained in TF 1.1 (it's a seq2seq model with bahdanau attention). It uses DynamicAttentionWrapper (which has been renamed to AttentionWrapper). After updating TF to version 1.4 and switching to renamed AttentionWrapper the model can't be loaded. I get multiple errors like the following:\r\n\r\n```\r\n2017-11-15 19:45:26.550430: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/attention_layer/kernel/Adam not found in checkpoint\r\n2017-11-15 19:45:26.550440: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/attention_layer/kernel/Adam_1 not found in checkpoint\r\n2017-11-15 19:45:26.550481: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam not found in checkpoint\r\n2017-11-15 19:45:26.552201: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key encoder/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel/Adam not found in checkpoint\r\n2017-11-15 19:45:26.552300: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/attention_v/Adam_1 not found in checkpoint\r\n2017-11-15 19:45:26.552426: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel not found in checkpoint\r\n2017-11-15 19:45:26.552434: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam_1 not found in checkpoint\r\n2017-11-15 19:45:26.552440: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/multi_rnn_cell/cell_0/basic_lstm_cell/bias not found in checkpoint\r\n2017-11-15 19:45:26.552496: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel/Adam not found in checkpoint\r\n2017-11-15 19:45:26.553055: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key decoder/decoder/attention_wrapper/multi_rnn_cell/cell_0/basic_lstm_cell/bias/Adam_1 not found in checkpoint\r\n```\r\n\r\nSetting name to dynamic_attention_wrapper in AttentionWrapper constructor parameters does not resolve the issue. Am I missing something? Taking into account that production models take a lot of resources to be trained it would be good for them to be compatible between TF versions.  Thank you.", "comments": ["I am also getting a similar error. I do not get this error in tensorflow 1.3. Unfortunately I don't yet have a minimal example of this bug yet.", "In my case though, the problem is occurring even on new models, not models loaded from a checkpoint.", "Sorry if it's in contrib, it's not a supported scenario. @zach-nervana feel free to reopen if you have something we can repro.", "I'm still having this problem, but I do not have time to create a minimal reproducible example at the moment, so leaving it closed."]}, {"number": 14589, "title": "Tensorflow Lite Support for Raspberry PI", "body": "### Describe the problem\r\nAre you planning to support Tensorflow Lite on Raspberry Pi? Specifically Raspberry Pi 3.\r\n", "comments": ["Yes, this is something we plan to do.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "How's everything going?", "@sarahs1 is there an eta for this?", "I currently am working on this.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Hi, is there an ETA for this? And more: will it likely work... or are the drawbacks of low memory so bad, that it would make no fun?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "any updates?", "via https://news.ycombinator.com/item?id=16302881 in the comments i found a solution in https://pastebin.com/LDEGyG46, but i havent tried it yet. \r\nAnyone got some benchmark-fps from a mobilenet for a PI3? ", "looks like something was merged up yesterday: https://github.com/tensorflow/tensorflow/pull/16431", "This is now merged, so closing."]}, {"number": 14588, "title": "typo: s/cesnus/census/", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please"]}, {"number": 14587, "title": "Fix MPI compilation", "body": "Fixes #14558  introduced by 2ce0b9149741105795083c4bae8fb0b85cb9659d", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please"]}, {"number": 14586, "title": "TF Lite C++ standalone Interpreter", "body": "**Feature request**\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.7.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: None\r\n\r\nI deploy TensorFlow models to environments with very limited memory, and was very excited to see how tiny the TF Lite kernel is (several hundred kilobytes). I know the project is young, but I was wondering if there are any plans to support a standalone C++ build for the Lite architecture, for deployment to (Linux) environments that do not have a TensorFlow runtime. \r\n\r\nFor my purposes, this package should support the following:\r\n- Loading a model configuration\r\n- Loading input tensors\r\n- Invoking the model with input tensors\r\n- Storing the result in output tensors\r\n- Some serialization method for the input- and output tensors\r\n\r\nI would like to know:\r\n- Is this a possible feature for TF Lite?\r\n- What would the scope be of implementing this?\r\n- Are there any plans to do so?", "comments": ["I'm not exactly sure what you mean by \"standalone c++\" build, but I assume independent of TensorFlow tooling is wha tyou mean.\r\n\r\nThere is already a makefile that builds basically just the interpreter that @petewarden made. You may need to modify the ops to be driven by the \"reference ops\" rather than the \"optimized ops\" to avoid using neon. But this should be definitely possible, and something we hope to support better in the future.", "hi, asslle:\r\n\r\nMy goal is to use NDK to compile C++ static libraries that can be called by Android. I don't quite understand what you're saying \"modify the ops to be driven by the \"reference ops\" rather than the \"optimized ops\" to avoid using neon. \" PS: I refer to /tensorflow/contrib/lite/makefile", "aselle's point is that you can change the makefile to build a static library for Android, or any other system. However, a lot of the TF Lite operators depend on neon, which is available on Android but not on every other system. If you don't have neon (or a suitable replacement) you will have to manually edit the source code in kernels/*.cc. (On linux you may sometimes rely on sse as a neon replacement)\r\n\r\n", "where can i find the makefile made by @petewarden ?", "@WellDone2094 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/Makefile", "i don't know how i missed that", "@JulianNeeleman, is your question sufficiently answered?", "@aselle Yes, thank you.", "The link to the TF lite makefile posted by @JulianNeeleman is no longer available. Can someone update it with the new link? It is probably shifted here - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/lite/tools/make ", "you can find solution in my git \r\nhttps://github.com/wantt/tensorflow-lite", "@wantt Thanks! Will take a look.", "Where can I find a version of the TensorFlow Lite makefile?  ", "@wantt Thanks for git repo. On linux, I would able to compile it with `-no-pie` option. Could you please tell me how did you build `libtensorflow-lite.a` library? With `objdump` i can see that its for `arch=x86_64` did you compile it on `linux` or `macOS`? \r\n\r\nHas anyone tried on `macOS`? ", "I could able to run it on `macOS,` my working repo is here: https://github.com/milinddeore/TfLite-Standalone-build-Linux-MacOS"]}, {"number": 14585, "title": "how to change the gpu_fraction_per_process from default 1 to 0.5 or 0.7?", "body": "I want to change the gpu_fraction_per_process from default 1 to 0.5 or 0.7.\r\nNot by adding the below in my code\r\n```\r\nimport tensorflow as tf\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\r\n```\r\nIs there other method to change it in the tensorflow source file??\r\n\r\nIn the case of Keras, I can change it by modifying keras.json file.\r\nI spent a lot of time to find the way though, I cannot find the way ..\r\nIs it impossible???", "comments": ["`K.set_session`? ", "@yaroslavvb  I run my model in spark with GPU machines. The problem is that tensorflow takes all memory, so there is no memory space for spark rdd. This is why I want to limit the fraction of memory.\r\n`K.set_session` is  kinda not working for this case.  `K.set_session` works on only spark driver, not on workers.. So I want to set default  in Tensorflow itself. \r\nIs there any way to modify tensorflow source related to this and use it??", "Some hacky workarounds are discussed here -- https://github.com/tensorflow/tensorflow/issues/8040\r\n\r\nThere was some discussion for a generic mechanism to do global settings here https://github.com/tensorflow/tensorflow/issues/8136 but it stalled"]}, {"number": 14583, "title": "contrib.slim.conv2d doesn't check input dimension", "body": "tensorflow version: v1.4.0-rc1-11-g130a514 (installed from pip)\r\n\r\ncontrib.slim.conv2d doesn't check for input dimensionality. So if you give it a 5D tensor it instead performs a 3D convolution.\r\n\r\nHere is a quick way to see this:\r\n```\r\nsess = tf.InteractiveSession()\r\n\r\nimage_ph = tf.placeholder(tf.float32,\r\n                          shape=(None, None, 10, 10),\r\n                          name='image')\r\nimage = tf.expand_dims(image_ph, -1)\r\nconv = tf.contrib.slim.conv2d(image, 3, 3, padding=\"VALID\")\r\n\r\nsess.run(tf.global_variables_initializer())\r\nconv_out = conv.eval({image_ph: np.random.random((16, 20, 10, 10))})\r\n```\r\n\r\nconv_out will have shape `[16, 18, 8, 8, 3]`, meaning that the time dimension (dim 1) has decreased due to the VALID padding. \r\n\r\nAs a side note I think it would be convenient if all conv2d implementations let you specify which dimensions to act on (or acted on the last 3 dims of the input by default). This is useful for example if you have sequential data and want to apply a 2d convolution to each frame independently.  ", "comments": ["@sguada would it be appropriate to mark this \"contributions welcome\", or do we have internal plans to fix it?", "It should check it has the right rank.\r\nAllowing other dimensions to run the conv2d on, that would be difficult since cuDNN doesn't support it.", "in case anyone is interested I use the following code to get around the dimensionality requirements (works for any input with shape [batch, time, dims....] where the operation acts on dims):\r\n\r\n```\r\ndef frame_wise_op(inputs, operation, **kwargs):\r\n    inputs_flat = tf.reshape(inputs, [-1] + inputs.shape[2:].as_list())\r\n\r\n    outputs_flat = operation(inputs_flat, **kwargs)\r\n\r\n    output_shape = tf.concat([tf.shape(inputs)[:2], tf.shape(outputs_flat)[1:]], 0)\r\n    outputs = tf.reshape(outputs_flat, output_shape)\r\n\r\n    return outputs\r\n```\r\n\r\nexample use:\r\n```\r\nconv = frame_wise_op(inputs,\r\n                     operation=tf.layers.conv2d,\r\n                     filters=32,\r\n                     kernel_size=[5, 5])\r\nnet = frame_wise_op(net,\r\n                    operation=tf.layers.max_pooling2d,\r\n                    pool_size=[2, 2],\r\n                    strides=2)\r\n```", "+1\n\nOn 20 Nov 2017 5:14 p.m., \"Linus H\u00e4renstam-Nielsen\" <\nnotifications@github.com> wrote:\n\n> in case anyone is interested I use the following code to get around the\n> dimensionality requirements:\n>\n> def frame_wise_op(inputs, operation, **kwargs):\n>     inputs_flat = tf.reshape(inputs, [-1] + inputs.shape[2:].as_list())\n>\n>     outputs_flat = operation(inputs_flat, **kwargs)\n>\n>     output_shape = tf.concat([tf.shape(inputs)[:2], tf.shape(outputs_flat)[1:]], 0)\n>     outputs = tf.reshape(outputs_flat, output_shape)\n>\n>     return outputs\n>\n> example use:\n>\n> conv = frame_wise_op(inputs,\n>                      operation=tf.layers.conv2d,\n>                      filters=32,\n>                      kernel_size=[5, 5])\n> net = frame_wise_op(net,\n>                     operation=tf.layers.max_pooling2d,\n>                     pool_size=[2, 2],\n>                     strides=2)\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/14583#issuecomment-345744167>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AFjlg0M2IwADymzIAUktSfzOlHW5TgUaks5s4aV5gaJpZM4QeyoE>\n> .\n>\n", "where is the source code for this functionality to be found? sorry I am new and just curious as to how it works", "@gauravdesale I think the method is only an alias for `convolution`, that's why it doesn't check input size, see:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/0403762ced523c048914009b335c7626e10e5f2e/tensorflow/contrib/layers/python/layers/layers.py#L2945\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/0403762ced523c048914009b335c7626e10e5f2e/tensorflow/contrib/layers/python/layers/layers.py#L1048\r\n", "Hi, @Linusnie . How about using ` tf.nn.conv2d` ?", "Hey @facaiy, my problem is that I would like to use 2d convolution on multiple tensor dimensions. `tf.nn.conv2d` also requires a 4D input. Example:\r\n\r\n```\r\n>>> x = tf.placeholder(tf.float32, (5, 3, 9, 9, 1))\r\n>>> y = tf.nn.conv2d(x)\r\n```\r\nyields \r\n```\r\nValueError: Shape must be rank 4 but is rank 5 for 'Conv2D_1' (op: 'Conv2D') with input shapes: [5,3,9,9,1], []\r\n```", "@Linusnie seems like this is the intended behavior:\r\nhttps://github.com/tensorflow/tensorflow/blob/0403762ced523c048914009b335c7626e10e5f2e/tensorflow/contrib/layers/python/layers/layers.py#L1003\r\n\r\n```python\r\n    if input_rank == 3:\r\n      layer_class = convolutional_layers.Convolution1D\r\n    elif input_rank == 4:\r\n      layer_class = convolutional_layers.Convolution2D\r\n    elif input_rank == 5:\r\n      layer_class = convolutional_layers.Convolution3D\r\n```\r\n\r\nIt chooses the kind of convolution to perform based on the rank of the input tensor.\r\n", "@wdhorton yes, but my point is that a function called conv2d should never perform a 3d convolution, as it does right now when you use it on a 5d tensor. It should either flatten the input to 4d and do a 2d convolution or throw a dimension mismatch error.", "I agree with @Linusnie that `contrib.slim.conv2d` should check its input rank. However since it is under contrib module, I'm afraid that the issue might not be resolved quickly. So I prefer to use ` tf.layers.conv2d`.\r\n\r\nAbout the second point, as @sguada said:\r\n> Allowing other dimensions to run the conv2d on, that would be difficult since cuDNN doesn't support it.\r\n\r\nI think the feature will be useful, however it seems difficult to implement currently. And to get around the problem, we can reshape our data before and after calculation like you do.  cc @fchollet who might be interested.", "To clarify, both tf.layers.conv2d and slim.conv2d should check that the rank is 4D and raise an error otherwise.\r\nUsing conv2d in a 5D tensor is out of the current API, but it can be implemente with reshapes.", "Added a PR #18251 so that the shape could be checked for conv2d/conv3d."]}, {"number": 14582, "title": "Changed ffmpeg verbosity semantics", "body": "The `tf.contrib.ffmpeg.decode_*` and `tf.contrib.ffmpeg.encode_*` functions are extremely verbose and make it nearly impossible to see other printed messages.\r\n\r\nCorresponding methods for image such as `tf.image.decode_png` produce no output under normal conditions, and it would be nice for audio/video methods to align with this.\r\n\r\n### FFmpeg on valid MP3 file with `-loglevel info` and without `-hide_banner` (old semantics)\r\n\r\n```sh\r\n$ ffmpeg -loglevel info -nostats -i in.mp3 -acodec pcm_s16le -ar 44100 out.wav\r\nffmpeg version N-79546-g13406b6 Copyright (c) 2000-2016 the FFmpeg developers\r\n  built with gcc 5.3.0 (GCC)\r\n  configuration: --enable-gpl --enable-version3 --disable-w32threads --enable-avisynth --enable-bzlib --enable-fontconfig --enable-frei0r --enable-gnutls --enable-iconv --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libfreetype --enable-libgme --enable-libgsm --enable-libilbc --enable-libmodplug --enable-libmfx --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libopus --enable-librtmp --enable-libschroedinger --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvo-amrwbenc --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs --enable-libxvid --enable-libzimg --enable-lzma --enable-decklink --enable-zlib\r\n  libavutil      55. 22.100 / 55. 22.100\r\n  libavcodec     57. 35.100 / 57. 35.100\r\n  libavformat    57. 34.102 / 57. 34.102\r\n  libavdevice    57.  0.101 / 57.  0.101\r\n  libavfilter     6. 44.100 /  6. 44.100\r\n  libswscale      4.  1.100 /  4.  1.100\r\n  libswresample   2.  0.101 /  2.  0.101\r\n  libpostproc    54.  0.100 / 54.  0.100\r\n[mp3 @ 00000000010771e0] Estimating duration from bitrate, this may be inaccurate\r\nInput #0, mp3, from 'in.mp3':\r\n  Metadata:\r\n    title           : jet-60-low\r\n    artist          : Jon Dattorro\r\n    album           : Tinnitus\r\n    date            : 2003\r\n    comment         : 105 Hz Q 1\r\n    track           : 2\r\n    genre           : Noise\r\n  Duration: 01:00:00.11, start: 0.000000, bitrate: 64 kb/s\r\n    Stream #0:0: Audio: mp3, 22050 Hz, stereo, s16p, 64 kb/s\r\n[wav @ 0000000000edb020] Using AVStream.codec to pass codec parameters to muxers is deprecated, use AVStream.codecpar instead.\r\nOutput #0, wav, to 'out.wav':\r\n  Metadata:\r\n    INAM            : jet-60-low\r\n    IART            : Jon Dattorro\r\n    IPRD            : Tinnitus\r\n    ICRD            : 2003\r\n    ICMT            : 105 Hz Q 1\r\n    IPRT            : 2\r\n    IGNR            : Noise\r\n    ISFT            : Lavf57.34.102\r\n    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\r\n    Metadata:\r\n      encoder         : Lavc57.35.100 pcm_s16le\r\nStream mapping:\r\n  Stream #0:0 -> #0:0 (mp3 (native) -> pcm_s16le (native))\r\nPress [q] to stop, [?] for help\r\nsize=  620172kB time=01:00:00.09 bitrate=1411.2kbits/s speed= 225x\r\n```\r\n\r\n### FFmpeg on valid MP3 file with `-loglevel error` and with `-hide_banner` (new semantics)\r\n\r\n```sh\r\n$ ffmpeg -loglevel error -nostats -i in.mp3 -acodec pcm_s16le -ar 44100 out.wav -hide_banner\r\n```\r\n\r\n### FFmpeg on text file (invalid) with `-loglevel error` and with `-hide_banner` (new semantics)\r\n\r\n```sh\r\n$ ffmpeg -loglevel error -nostats -i in.txt -acodec pcm_s16le -ar 44100 out.wav -hide_banner\r\nOutput file #0 does not contain any stream\r\n```", "comments": ["Can one of the admins verify this patch?", "The PR will fix issue #14733 I think.", "Please merge this!", "Are any of these check failures related to my changes? Having trouble parsing"]}, {"number": 14581, "title": "Generate benchmark_model (android arm64-v8a) executable in case of tensorflow lite ", "body": "Hello i tried the sameway defined in the Readme.md of \"tensorflow/contrib/makefile\" folder to build benchmark_model for  tensorflow lite (tensorflow/contrib/lite/). However, it failed and seems that \"TARGET=ANDROID\" is not working in caseof Makefile of \"tensorflow/contrib/lite/\" folder.\r\n\r\nERROR LOG:\r\n/tensorflow/tensorflow/contrib/lite/downloads/gemmlowp/public/../internal/../internal/kernel_default.h:89:2: error: #error \"SIMD not enabled, you'd be getting a slow software fallback. Consider enabling SIMD extensions (for example using -msse4 if you're on modern x86). If that's not an option, and you would like to continue with the slow fallback, define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK.\"\r\n #error \\\r\nmake: *** [/tensorflow/tensorflow/contrib/lite/gen/obj/tensorflow/contrib/lite/interpreter.o] Error 1\r\n\r\n", "comments": ["Please use bazel for Android. Currently the makefile only supports IOS.", "Sorry.\r\nI am confused. \r\nTo build benchmark_model, I previously used makefile or bazel like below:\r\nbazel build -c opt \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --cpu=armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  tensorflow/contrib/lite/tools:benchmark_model\r\n\r\nNow how can I use bazel to generate \"benchmark_model\" using tensorflow/contrib/lite/tools/benchmark_model.cc file?  I tried this comment below but I receive error \" ERROR: no such target '//tensorflow/contrib/lite/tools:benchmark_model': target 'benchmark_model' not declared in package 'tensorflow/contrib/lite/tools' \"\r\n\r\nbazel build -c opt \\\r\n  --crosstool_top=//external:android/crosstool \\\r\n  --cpu=armeabi-v7a \\\r\n  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n  tensorflow/contrib/lite/tools:benchmark_model\r\n\r\n\r\nIn the /tensorflow/contrib/lite/tools/BUILD file,  there is no declaration of benchmark_model in tf_cc_binary variable", "Currently there is no build rule in bazel for that benchmark_model.cc. You can add a cc_binary to BUILD (look at other BUILD cc_binary examples in contrib/lite/BUILD for inspiration).", "I have some quick hacks to make basic benchmark_model. See https://github.com/tensorflow/tensorflow/compare/master...freedomtan:benchmark_model_tflite", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "It should be working now. I'll close this."]}, {"number": 14580, "title": "freeze_graph \"No variables to save\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI do not modify any of the source code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.4.0\r\n- **Python version**: \r\n3.5\r\n_ **Bazel version\r\nN\r\n_ **CUDA/cuDNN version\r\nN\r\n_ **GPU model and memory\r\nN\r\n\r\n### Describe the problem\r\nFirst, I download mobilenet_v1_0.25_224 pretrained model from [here](http://download.tensorflow.org/models/mobilenet_v1_0.25_224_2017_06_14.tar.gz)\r\n\r\nThen,Exporting the Inference Graph, I use the commond:\r\npython export_inference_graph.py \\\r\n  --alsologtostderr \\\r\n  --model_name=mobilenet_v1_025 \\\r\n  --image_size=224 \\\r\n  --output_file=/tmp/mobilenet_v1_025_224.pb\r\n\r\nNext, freeze graph, I use the command:\r\npython tensorflow/python/tools/freeze_graph.py --input_graph= tmp/mobilenet_v1_025_224.pb --input_checkpoint=tmp/mobilenet_v1_0.25_224.ckpt --input_binary=true --output_graph=tmp/frozen_mobilenet_v1_025_224.pb --output_node_names=MobileNetV1/Predictions/Reshape_1\r\n\r\nFinally, I got the error:\r\n I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTraceback (most recent call last):\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 350, in <module>\r\n    app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/libs/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 249, in main\r\n    FLAGS.saved_model_tags)\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 239, in freeze_graph\r\n    input_meta_graph_def, input_saved_model_dir, saved_model_tags.split(\",\"))\r\n  File \"tensorflow/python/tools/freeze_graph.py\", line 127, in freeze_graph_with_def_protos\r\n    saver = saver_lib.Saver(var_list=var_list)\r\n  File \"/home/libs/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1218, in __init__\r\n    self.build()\r\n  File \"/home/libs/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1227, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/libs/anaconda3/envs/python35/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1251, in _build\r\n    raise ValueError(\"No variables to save\")\r\nValueError: No variables to save\r\n\r\nAnother machine:\r\nwindows 7\r\ntensorflow 1.4.0\r\npython 3.6\r\n\r\nUsing the same operations, I can get the frozen .pb file successfully.\r\n\r\nAll source code are not modified.I also  changed the other models to test, and get the same error, need your help?", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler  I have updated the question, the Exact commands were already described ", "I have exactly the same problem. Still no response yet. ", "I have exactly the same problem. Still no response yet.", "This problem have occured when i changed model from \"ssd_mobilenet_v1_coco_11_06_2017\" to \"faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\".\r\nThe same also happens when i try \"faster_rcnn_resnet101_coco_11_06_2017\".\r\n\r\nBut it works for \"ssd_mobilenet_v1_coco_11_06_2017\".", "happens to me with mask_rcnn_inception_v2_coco", "Encountered a similar problem while trying to freeze graph using meta:\r\n\r\n```\r\npython freeze_graph.py \\\r\n--input_meta_graph_def=/path/graph-0000-1111111.meta \\\r\n--input_checkpoint=/path/min-validation_error.data-00000-of-00001 \\\r\n--output_graph=/tmp/frozen_graph.pb \\\r\n--output_node_names=super/cool/result\r\n```\r\n\r\nwhich resulted in next error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"freeze_graph.py\", line 382, in <module>\r\n    run_main()\r\n  File \"freeze_graph.py\", line 379, in run_main\r\n    app.run(main=my_main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/sey1pal/tf_gpu/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"freeze_graph.py\", line 378, in <lambda>\r\n    my_main = lambda unused_args: main(unused_args, flags)\r\n  File \"freeze_graph.py\", line 272, in main\r\n    flags.saved_model_tags, checkpoint_version)\r\n  File \"freeze_graph.py\", line 254, in freeze_graph\r\n    checkpoint_version=checkpoint_version)\r\n  File \"freeze_graph.py\", line 128, in freeze_graph_with_def_protos\r\n    var_list=var_list, write_version=checkpoint_version)\r\n  File \"/home/sey1pal/tf_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1293, in __init__\r\n    self.build()\r\n  File \"/home/sey1pal/tf_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1302, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"/home/sey1pal/tf_gpu/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1327, in _build\r\n    raise ValueError(\"No variables to save\")\r\nValueError: No variables to save\r\n```\r\n\r\nTF 1.6, freeze_graphs.py taken from master branch", "back to my issue: --input_meta_graph_def should be --input_meta_graph\r\n\r\nYet, it means that a call like:\r\n```\r\npython freeze_graph.py \\\r\n--input_checkpoint=/path/min-validation_error.data-00000-of-00001 \\\r\n--output_graph=/tmp/frozen_graph.pb \\\r\n--output_node_names=super/cool/result\r\n```\r\nwill fail with obscure 'ValueError(\"No variables to save\")'. This can be improved", "still getting this issue X_X\r\nany solutions ?", "okay , so the issue is more conceptual than code based : you (and I) aren't choosing the right nodes to optimize\r\n\r\nchoose the 'central' nodes , it'll work (until you get the next error)", "\u6211\u4e5f\u9047\u5230\u540c\u6837\u7684\u95ee\u9898", "still getting this issue\uff0cany solutions ?", "@jasminezz how do you resolved this problems", "I have the same issue, please can you suggest any solution?", "@yselivonchyk I think I know how to resolve this issue. Can you try this command instead:\r\npython freeze_graph.py \\\r\n--input_checkpoint=/path/min-validation_error \\\r\n--output_graph=/tmp/frozen_graph.pb \\\r\n--output_node_names=super/cool/result", "@Anastasiya888 Can you post your command as well? sometimes this error happens when the checkpoint is not loaded correctly.", "@Aashit-Sharma, I think @saeed68gm solution could have been what I did.\r\n\r\nShort answer: try a few different ways to point your script to the checkpoint (with, without extension, pointing to the checkpoint folder itself).\r\n\r\nI think in my case (given the time that already passed) the issue was that providing graph meta to the script was failing. When script could not read the meta it treated the argument as unused and failed a few nested calls later with an irrelevant error.", "same issue, any answer, work?", "@gargn this seems to be a common cause of confusion for users of freeze_graph. Could you look at whether we could improve the error messaging, handling, or documentation to help, since you're looking at that script? Thanks!", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks! "]}, {"number": 14579, "title": "Is it possible to optimize the network's likelihood function over a function of parameters? Or putting it the other way, can we optimize a function of parameter instead of parameters?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 14578, "title": "Implements LayerNormBasicGRUCell", "body": "LayerNormBasicGRUCell add layer normalization to basic GRU unit.\r\n\r\nLayer Normalization implementation is based on https://arxiv.org/abs/1607.06450 (\"Layer Normalization\", Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton) and is applied before the internal nonlinearities.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "Can you check the CLA? Our internal check doesn't indicate that you've signed it yet.", "> Can you check the CLA? Our internal check doesn't indicate that you've signed it yet.\r\n\r\nHi jhseu,\r\nMy company (Criteo) signed a CCLA and I'm part of the associated google group (criteo-contributors).  I did some check based on https://opensource.google.com/docs/cla/#troubleshoot but I didn't find anything wrong, so I don't understand why the bot cannot find the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "Hooray, he finally found it!", "@ebrevdo, can you take a look?", "Can one of the admins verify this patch?", "@KiewanVillatel any chance to address @ebrevdo comment?", "Hello @ebrevdo,\r\nI took into account your review, can you take a look please?\r\n", "Hi @jhseu @ebrevdo,\r\nWhat is the status of this pull request? Are you still interested in merging it? Or should we close it?", "I will do the review. Sorry for the delay\n\nOn Thu, May 17, 2018, 3:41 PM KiewanVillatel <notifications@github.com>\nwrote:\n\n> Hi @jhseu <https://github.com/jhseu> @ebrevdo <https://github.com/ebrevdo>\n> ,\n> What is the status of this pull request? Are you still interested in\n> merging it? Or should we close it?\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/14578#issuecomment-389868929>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AT_SbR5aQOdXGYKHH6xp274hLGyO0xSGks5tzX3xgaJpZM4QepbV>\n> .\n>\n", "Hi @KiewanVillatel ,\r\nWe are researchers working on identifying redundant development and duplicated pull requests. We have found there is a pull request: https://github.com/tensorflow/tensorflow/pull/15861 which might be a potentially duplicate to this one. We would like to build the link between developers to reduce redundant development. We would really appreciate if you could help us to validate and give us feedback.\r\nThank you very much for your time!\r\n", "@KiewanVillatel is this ready for review again? @drpngx for review", "Looks like there are still unaddressed comments. @KiewanVillatel mind commenting back? Thanks.", "Hi @drpngx,\r\nI pushed another commit adding some tests, did you take a look at it?", "@KiewanVillatel have you had a chance to look at comments?", "Hi @jhseu,\r\nI didn't address the comment about not using scopes. For this I need a helper that should be developed in #15861. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 28 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 14577, "title": "modify tensorflow/core/protobuf/config.proto", "body": "I try to modify config.proto.\r\nI want to change per_process_gpu_memory_fraction from 1 to 0.2.\r\nI modified it and builded it.\r\n\r\nAfter I modified it, I compiled by using bazel\r\n```\r\n. /configure\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_packag\r\n```\r\nBut I got error and I cannot compile.", "comments": ["What is the error log and your system information?", "I changed per_process_gpu_memory_fraction from 1 to 0.5 in cofig.proto\r\n![image](https://user-images.githubusercontent.com/9244296/32835879-24027c04-ca4b-11e7-83d0-29f99d20340a.png)\r\n\r\n![image](https://user-images.githubusercontent.com/9244296/32835622-1ef3b7ec-ca4a-11e7-8cbf-8838d4c7a86c.png)\r\n", "From the error message I'm guessing that you edited this line:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/31b79e42b9e1643b3bcdc9df992eb3ce216804c5/tensorflow/core/protobuf/config.proto#L21\r\n\r\n...to something like:\r\n\r\n```protobuf\r\n  double per_process_gpu_memory_fraction = 0.2;\r\n```\r\n\r\nThe `1` in the original code is not a default value. Rather, it is a protobuf field number, which controls how the protocol buffer is serialized, and it must be a unique integer. (Moreover, for backwards compatibility, it must never change.)\r\n\r\nThe default value for a `double`-type protobuf field is `0.0`. The code for handling that is in the C++ runtime, here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/31b79e42b9e1643b3bcdc9df992eb3ce216804c5/tensorflow/core/common_runtime/gpu/gpu_device.cc#L781"]}, {"number": 14576, "title": "FAIL://tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test with GPU support", "body": "\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source\r\n- **TensorFlow version (use command below)**:\r\n      TF-1.3.1\r\n- **Python version**:\r\n      Python 2.7.5 \r\n- **Bazel version (if compiling from source)**:\r\n      Bazel - 0.5.4\r\n- **CUDA/cuDNN version**:\r\n      cuda-8.0 and cuDNN-6.0.21\r\n- **GPU model and memory**:\r\n        0   Tesla P100-SXM2 16276MiB\r\n        1   Tesla P100-SXM2 16276MiB\r\n- **Exact command to reproduce**:\r\n     ` bazel test --config=opt --config=cuda -k //tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test`\r\n\r\n### Describe the problem\r\nThis test passed successfully with CPU only. However its failing for GPU , getting following error :\r\n`F tensorflow/core/common_runtime/direct_session_with_tracking_alloc_test.cc:141] Check failed: cost_models.size() == 1 (2 vs. 1)`\r\n\r\n(Test expects cost_models.size() == '1' , but we are getting '2' with GPU)\r\n\r\nI was looking into this test failure and I found some relevant discussion links -\r\n1) https://github.com/lukeiwanski/tensorflow/issues/75  - Here mentioned , this test is marked for no_gpu\r\n2) https://github.com/lukeiwanski/tensorflow/pull/115 - Here I found following comments:\r\n\r\nThe aim of this test is to ensure that there is at least one cost model (that of the CPU), as if there are no cost models then the test should fail and there is no point in trying to run the other tests on the cost\r\nmodel.\r\n\r\nThe number of cost models provided by a session should matches the number of devices used by that session. When additional devices are present there will be more than one cost model, so we should not be checking for equality, but rather that the number of cost models is at least one.\r\n\r\nAs per above comments , I think we should raise a PR with following changes to pass on GPU as well -\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.3.1/tensorflow/core/common_runtime/direct_session_with_tracking_alloc_test.cc#L141\r\nOriginal - `CHECK_EQ(cost_models.size(), 1);`\r\nUpdated to - `CHECK_GE(cost_models.size(), 1); `\r\n\r\nPlease provide your comments on this. Thanks!\r\n### Source code / logs\r\n```\r\n$  bazel test --config=opt --config=cuda -k //tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test\r\n\r\n...\r\n2017-11-15 08:42:51.345108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Tesla P100-SXM2-16GB, pci bus id: 0006:01:00.0)\r\n2017-11-15 08:42:51.352293: F tensorflow/core/common_runtime/direct_session_with_tracking_alloc_test.cc:141] Check failed: cost_models.size() == 1 (2 vs. 1)\r\n================================================================================\r\nTarget //tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test up-to-date:\r\n  bazel-bin/tensorflow/core/common_runtime_direct_session_with_tracking_alloc_test\r\nINFO: Elapsed time: 80.635s, Critical Path: 80.25s\r\n//tensorflow/core:common_runtime_direct_session_with_tracking_alloc_test FAILED in 75.0s\r\n```\r\n", "comments": ["It looks good, I think for these kind of one line changes you can send a PR directly, and explain your reasoning in the PR description.", "Sure @yaroslavvb , and thanks for your suggestion.\r\n\r\nI committed the changes in same PR - https://github.com/tensorflow/tensorflow/pull/14530 (which was created for https://github.com/tensorflow/tensorflow/issues/14515)"]}]