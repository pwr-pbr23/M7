[{"number": 21858, "title": "Merging 1.10.1 back into master.", "body": "", "comments": ["@av8ramit are we still working on this merge?", "No, too many conflicts. I'll create a new one."]}, {"number": 21857, "title": "Readme", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Hi @sam2702,\r\n\r\nThanks for the contribution! Before we can merge this, you'll need to sign the CLA linked above.\r\n\r\nThanks!", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Can I get any update"]}, {"number": 21856, "title": "Loading frozen graph changes behavior of `tf.placeholder()`", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no\r\n- **TensorFlow installed from (source or binary)**: `pip install tensorflow`\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Python version**: `Python 3.6.6 :: Anaconda, Inc.`\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Run the program below\r\n\r\n### Describe the problem\r\n\r\nUsually, `tf.placeholder(tf.float32, shape=[]).get_shape()` is an empty shape, `()`. However, if you import a specific frozen graph first, the result of `get_shape()` will be `unknown`.\r\n\r\nHere is an example of a script that reproduces the issue and prints `unknown`:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nIN_OUT = ['images_ph:0', 'inception_v3/logits/flatten/Reshape:0']\r\n\r\nwith open('inception-v3.pb', 'rb') as in_file:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(in_file.read())\r\n    tf.import_graph_def(graph_def, name='', return_elements=IN_OUT)\r\n\r\nprint('shape result', tf.placeholder(tf.float32, shape=[]).get_shape())\r\n```\r\n\r\nWhere `inception-v3.pb` can be downloaded at: https://storage.googleapis.com/agi-data/models/inception-v3.pb", "comments": ["This is a very interesting issue! According to [this comment](https://github.com/tensorflow/tensorflow/blob/1bc856ba29bd57378d5c1ca08afc255460597f7f/tensorflow/core/ops/array_ops.cc#L1972):\r\n\r\n> Placeholder has legacy behavior where we cannot tell the difference between a scalar shape attribute and 'unknown shape'.  So if the shape is a scalar, we return an unknown shape.\r\n\r\nImporting an old graphdef will trigger this behavior."]}, {"number": 21855, "title": "Tensorflow Lite issue on Raspberry Pi", "body": "Hi Everyone. I am working on running some scripts on Tensorflow on Raspberry Pi 3. When I try to import a tflite model into the tflite interpreter, it throws an error. Please help :) :) \r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Raspbian Stretch\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:Rasp Pi 3\r\n- **TensorFlow installed from (source or binary)**:Binary\r\n- **TensorFlow version (use command below)**:1.9\r\n- **Python version**:2.7\r\n- **Bazel version (if compiling from source)**:NA\r\n- **GCC/Compiler version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:-\r\n\r\n\r\n### Source code / logs\r\n\r\n###THE CODE###\r\n interpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')\r\n\r\n###ERROR###\r\n\r\n/home/pi/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 56, got 52\r\n  from tensorflow.python.framework import fast_tensor_util\r\nTraceback (most recent call last):\r\n  File \"mobilenet_int_tflite.py\", line 24, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path='mobilenet_v1_0.25_128_quant.tflite')\r\n  File \"/home/pi/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/interpreter.py\", line 50, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/home/pi/.local/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 53, in __getattr__\r\n    module = self._load()\r\n  File \"/home/pi/.local/lib/python2.7/site-packages/tensorflow/python/util/lazy_loader.py\", line 42, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\n  File \"/home/pi/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/home/pi/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\nImportError: /home/pi/.local/lib/python2.7/site-packages/tensorflow/contrib/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils39NeonMatrixBatchVectorMultiplyAccumulateEPKfiiS2_iPfi\r\n \r\n\r\n", "comments": ["Same problem as https://github.com/tensorflow/tensorflow/issues/21574", "@abhi-rf: Agree with @freedomtan that this seems like the same problem as #21574.  In that thread, it is suggested that upgrading to 1.10 fixed the problem.  Have you tried this?", "I have the same issue with raspbian 9.0, tensorflow 1.9.0, python 3.5.3 on raspberry pi 3B. I tried to solve it installing tensorflow 1.10.0 and even though i did it, it's still showing the same problem. did anyone solve it?", "I'am still having the same problem ", "[#23082](https://github.com/tensorflow/tensorflow/issues/23082)", "I have the same issue with raspbian 9.0, tensorflow 1.13.0, python 3.5.3 on raspberry pi 3B.", "I am having the same issue with Tensorflow 1.13 as well\r\n", "Me too.  TF1.13.1 on Pi3B. \r\n\r\nAny suggestions of how we might go about resolving this? ", "I have exactly the same issue with TensorFlow 1.13.1 on Raspberry Pi running Python 3.6.8. Here is the output: \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tflite.py\", line 20, in <module>\r\n    interpreter = tf.lite.Interpreter(MODEL_PATH)\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 54, in __init__\r\n    _interpreter_wrapper.InterpreterWrapper_CreateWrapperCPPFromFile(\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 61, in __getattr__\r\n    module = self._load()\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/util/lazy_loader.py\", line 44, in _load\r\n    module = importlib.import_module(self.__name__)\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/importlib/__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 28, in <module>\r\n    _tensorflow_wrap_interpreter_wrapper = swig_import_helper()\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_tensorflow_wrap_interpreter_wrapper', fp, pathname, description)\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/pi/.pyenv/versions/3.6.8/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\n  File \"<frozen importlib._bootstrap>\", line 684, in _load\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: /home/pi/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/_tensorflow_wrap_interpreter_wrapper.so: undefined symbol: _ZN6tflite12tensor_utils24NeonVectorScalarMultiplyEPKaifPf\r\n```\r\nAny ideas on how to sort this out?", "Install the tensorflow using wheel file.", "@harshgoyal020 which specific wheel build? Mind telling me more? ", "Get the wheel file from link https://github.com/lhelontra/tensorflow-on-arm/releases/download/v1.13.1/tensorflow-1.13.1-cp35-none-linux_armv7l.whl\r\n\r\nThen install it using command \r\n pip3 install tensorflow-1.13.1-cp35-none-linux_armv7l.whl   ", "@harshgoyal020 thanks a bunch. I'll give it a try. Given that the community builds work flawlessly, does that mean there's a bug in official builds that needs to be fixed somehow down the line? ", "Official builds has less support for Raspberry Pi as compared to build from scrach using bazel. But bazel build takes lots of time. So i tried using wheel file, I got sucesss with no error. Community need to update ", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 21854, "title": "[XLA] Allow for disabling these tests via manifest", "body": "Allow this test to be disabled via the manifest file.\r\n\r\n", "comments": ["Nagging Assignee @akshaym: It has been 35 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@vrv @hawkinsp  \r\n\r\nHi guys.  Sorry to pester you,  but I think that this minor change has been overlooked.", "Is there a need to add a tag to make the test happen?  \r\n", "cheers :)\r\n", "Sorry this took a while to commit. I don't review a lot of Github PRs so I didn't press the right buttons. I'll get it right next time :-)"]}, {"number": 21853, "title": "Apache Ignite Dataset", "body": "This is a proposal to add `IgniteDataset` that allows to work with [Apache Ignite](https://ignite.apache.org/). \r\n\r\nApache Ignite is a memory-centric distributed database, caching, and processing platform for transactional, analytical, and streaming workloads, delivering in-memory speeds at petabyte scale. This proposal is a part of a more global initiative to implement so called \"TensorFlow on Apache Ignite\" ([IGNITE-8335](https://issues.apache.org/jira/browse/IGNITE-8335), [Design Document](https://docs.google.com/document/d/1jROIahK1rc7bSgOvhJhfpMqIGvht_IE8zn5NAt6x8ks)).\r\n\r\nThe integration is based on Apache Ignite [Binary Client Protocol](https://apacheignite.readme.io/v2.6/docs/binary-client-protocol) and TensorFlow [tf.data.Dataset](https://www.tensorflow.org/guide/datasets). More information about supported features you can find in [README.md](https://github.com/dmitrievanthony/tensorflow/blob/apache-ignite-dataset/tensorflow/contrib/ignite/README.md) of this module.\r\n\r\nTests have also been added. They use docker to hide configuration complexity, so that the implemented functionality can be tested quite simply by manual run.", "comments": ["Thank you for reviewing, @mrry. Really hope it won't take more than two weeks to review and make this request merge-ready because in Apache Ignite we have big plans about it. Feel free to contact me any time if you have questions about this PR.\r\n\r\nI've updated the code in accordance with all comments you have so far, so I will be grateful if you could continue your review.", "Hi, @mrry, @martinwicke. Any updates?\r\n\r\nBy the way, could you please run CI tests so that I am able to fix now if something wrong?", "Guys, @mrry, @martinwicke, I really understand that you are under pressure of all these pull requests, but it's been more than 10 days since your last comment here, so let me ping you and ask to take a look.", "I've retriggered the tests. Sorry for the wait.", "Hi, @martinwicke. Thank you for restarting tests. Sorry for bothering, but I still can't fully reproduce CI build.\r\n\r\nI see that during the last run only one required test failed (one `pylint` check to be precise). I have just fixed it, so could you please restart tests again?\r\n\r\nAnd one more question. Should only `required` tests be green or all? I see that a lot of \"none-required\" tests fail in absolutely another modules with timeout and other types of exceptions, so I don't feel like I can fix them.", "@mrry can you take a look at the changes?", "Hi, @mrry, @martinwicke. I feel yourself a bit lost here. It's been 17 days since last @mrry comment and the PR is on the 3rd page already. Guys, could you please pay attention to this request?", "@dmitrievanthony sorry for all the delay. I re-triggered the tests, looks like there is some issue with the windows build and clang-format (The XLA tests fail a lot, I'm not sure they are related), can you take a look?\r\n\r\nAlso can you rebase and fix up the commits? The Merge Commits in the PR are weird i dont follow them. If you do an interactive rebase on origin/master you can drop them and fix up the commits.\r\n\r\nI'm not really familiar with these areas of TF so can't review all of it but I'll do what I can.", "Hi @perfinion. I've fixed code style and removed \"merge requests\", but as result I've recreated the branch, so could you please continue with #22210?\r\n\r\nRegarding builds on Windows I see the same issues I had and asked here: https://groups.google.com/a/tensorflow.org/forum/#!topic/build/ePYss0Kxcu4. It looks like we need to add `copt=-DWIN32_LEAN_AND_MEAN` on CI server for Windows builds. What do you think?", "oh, you could have just force-pushed to this branch name. but sure, I've run tests and re-assigned that PR instead."]}, {"number": 21852, "title": "Tensorflow 1.10 source compilation fails on aarch64", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: \r\n- **CUDA/cuDNN version**: 9.0, 7.0\r\n- **GPU model and memory**: DrivePX2\r\n- **Exact command to reproduce**:\r\n`bazel build -s --local_resources 3000,.8,1.0 --config=opt --config=cuda //tensorflow:libtensorflow.so //tensorflow:libtensorflow_cc.so //tensorflow/tools/pip_package:build_pip_package`\r\n\r\n### Describe the problem\r\n\r\nIf I try to build tensorflow, I get the following error pretty much immediately\r\n\r\n```\r\nERROR: No default_toolchain found for cpu 'aarch64'. Valid cpus are: [\r\n k8, \r\n piii,\r\n arm,\r\n darwin,\r\n ppc,\r\n x64_windows,\r\n]\r\n```\r\nI also tried recompiling Bazel 0.16.1 to include in the tools/cpp/CROSSTOOL a configuration using aarch64.\r\n", "comments": ["Nagging Assignee @rohan100jain: It has been 20 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Similar problem on nvidia Xavier.  Only happens when cuda is used (`--config=cuda` or equivalent `.rc` entry).", "This worked for me:\r\n\r\nadd \r\n\r\n```\r\ndefault_toolchain {\r\n  cpu: \"aarch64\"\r\n  toolchain_identifier: \"local_linux\"\r\n}\r\n```\r\n\r\nto `./third_party/gpus/crosstool/CROSSTOOL.tpl`.\r\nTested on TensorFlow 1.10.1 and 1.11.0-rc2.\r\n", "Seems like there is a workaround. @fferroni I'm closing this issue for now but feel free to reopen if the problem persists.", "apolotsk's fix worked for me.  I'm using an NVIDIA TX2 (basically Ubuntu 16.04) with Bazel 0.18.1 and TensorFlow 1.10.0.  I edited:\r\n\r\n`~/tensorflow/third_party/gpus/crosstool/CROSSTOOL.tpl`\r\n\r\nex. with the following commands:\r\n\r\n```\r\ncd ~/tensorflow/third_party/gpus/crosstool\r\nsudo gedit CROSSTOOL.tpl\r\n```\r\n\r\nAfter all the other `default_toolchain` blocks I pasted in:\r\n\r\n```\r\ndefault_toolchain {\r\n  cpu: \"aarch64\"\r\n  toolchain_identifier: \"local_linux\"\r\n}\r\n```\r\n\r\nAnd the TensorFlow compile worked.\r\n\r\n\r\n", "> apolotsk's fix worked for me. I'm using an NVIDIA TX2 (basically Ubuntu 16.04) with Bazel 0.18.1 and TensorFlow 1.10.0. I edited:\r\n> \r\n> `~/tensorflow/third_party/gpus/crosstool/CROSSTOOL.tpl`\r\n> \r\n> ex. with the following commands:\r\n> \r\n> ```\r\n> cd ~/tensorflow/third_party/gpus/crosstool\r\n> sudo gedit CROSSTOOL.tpl\r\n> ```\r\n> \r\n> After all the other `default_toolchain` blocks I pasted in:\r\n> \r\n> ```\r\n> default_toolchain {\r\n>   cpu: \"aarch64\"\r\n>   toolchain_identifier: \"local_linux\"\r\n> }\r\n> ```\r\n> \r\n> And the TensorFlow compile worked.\r\n\r\nHi,\r\n\r\nwhich command did you use to compile the source code? Can you share it?\r\n\r\nThanks.", "@apolotsk @rohan100jain \r\nHow could I fix that issue on **r1.12 or r1.13** branches? \r\nI placed \r\n```\r\ndefault_toolchain {\r\n  cpu: \"aarch64\"\r\n  toolchain_identifier: \"local_linux\"\r\n}\r\n```\r\nin both CROSSTOOL.tpl and CROSSTOOL_hipcc.tpl and neither didn't work\r\n", "I meet same error on r1.12 & r1.13", "> How could I fix that issue on r1.12 or r1.13 branches?\r\n\r\n@chichivica, @wormwang, these are the changes I've made to r1.13-r0 (not tested for r1.13):\r\n\r\n- In `./third_party/gpus/crosstool/BUILD.tpl`, changed\r\n  ```\r\n  cc_toolchain_suite(\r\n      name = \"toolchain\",\r\n      toolchains = {\r\n          ...\r\n      },\r\n  )\r\n  ```\r\n  to\r\n  ```\r\n  cc_toolchain_suite(\r\n      name = \"toolchain\",\r\n      toolchains = {\r\n          ...\r\n          \"aarch64\": \":cc-compiler-local\",\r\n      },\r\n  )\r\n  ```\r\n\r\n- In `./third_party/aws/BUILD.bazel`, changed\r\n  ```\r\n  cc_library(\r\n      name = \"aws\",\r\n      srcs = select({\r\n          ...\r\n          \"//conditions:default\": [],\r\n  ```\r\n  to\r\n  ```\r\n  cc_library(\r\n      name = \"aws\",\r\n      srcs = select({\r\n          ...\r\n          \"//conditions:default\": glob([\r\n              \"aws-cpp-sdk-core/source/platform/linux-shared/*.cpp\",\r\n          ]),\r\n  ```\r\n\r\n- In `./third_party/nccl/build_defs.bzl.tpl`, changed\r\n  ```\r\n      maxrregcount = \"-maxrregcount=96\"\r\n  ```\r\n  to\r\n  ```\r\n      maxrregcount = \"-maxrregcount=80\"\r\n  ```\r\n\r\n- Used Bazel 0.21.0.", "Changes made to r1.12:\r\n\r\n- In `./tensorflow/contrib/lite/kernels/internal/BUILD`, changed from\r\n  ```\r\n  \":arm\": [\r\n      \"-O3\",\r\n      \"-mfpu=neon\",\r\n  ],\r\n  ```\r\n  to\r\n  ```\r\n  \":arm\": [\r\n      \"-O3\",\r\n  ],\r\n  ```\r\n\r\n- In `./third_party/aws.BUILD`, changed from\r\n  ```\r\n  cc_library(\r\n      name = \"aws\",\r\n      srcs = select({\r\n          ...\r\n          \"//conditions:default\": [],\r\n  ```\r\n  to\r\n  ```\r\n  cc_library(\r\n      name = \"aws\",\r\n      srcs = select({\r\n          ...\r\n          \"//conditions:default\": glob([\r\n              \"aws-cpp-sdk-core/source/platform/linux-shared/*.cpp\",\r\n          ]),\r\n  ```\r\n\r\n- Used Bazel 0.15.2.", "for TF 1.12\r\n\r\nwhich is 'In ``, changed from' for aws sdk?\r\n\r\n", "In `./third_party/aws.BUILD`. Corrected.", "> > How could I fix that issue on r1.12 or r1.13 branches?\r\n> \r\n> @chichivica, @wormwang, these are the changes I've made to r1.13-r0 (not tested for r1.13):\r\n> \r\n> * In `./third_party/gpus/crosstool/BUILD.tpl`, changed\r\n>   ```\r\n>   cc_toolchain_suite(\r\n>       name = \"toolchain\",\r\n>       toolchains = {\r\n>           ...\r\n>       },\r\n>   )\r\n>   ```\r\n>   \r\n>   \r\n>   to\r\n>   ```\r\n>   cc_toolchain_suite(\r\n>       name = \"toolchain\",\r\n>       toolchains = {\r\n>           ...\r\n>           \"aarch64\": \":cc-compiler-local\",\r\n>       },\r\n>   )\r\n>   ```\r\n> * In `./third_party/aws/BUILD.bazel`, changed\r\n>   ```\r\n>   cc_library(\r\n>       name = \"aws\",\r\n>       srcs = select({\r\n>           ...\r\n>           \"//conditions:default\": [],\r\n>   ```\r\n>   \r\n>   \r\n>   to\r\n>   ```\r\n>   cc_library(\r\n>       name = \"aws\",\r\n>       srcs = select({\r\n>           ...\r\n>           \"//conditions:default\": glob([\r\n>               \"aws-cpp-sdk-core/source/platform/linux-shared/*.cpp\",\r\n>           ]),\r\n>   ```\r\n> * In `./third_party/nccl/build_defs.bzl.tpl`, changed\r\n>   ```\r\n>       maxrregcount = \"-maxrregcount=96\"\r\n>   ```\r\n>   \r\n>   \r\n>   to\r\n>   ```\r\n>       maxrregcount = \"-maxrregcount=80\"\r\n>   ```\r\n> * Used Bazel 0.21.0.\r\n\r\ntyvm bro! it worked perfect for me. tensorflow r1.13 && bazel 0.21"]}, {"number": 21851, "title": "How to install tensorflow in python3.7?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**:\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Last time I checked TF 1.10 doesn't officially support Python 3.7\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/501bb0d18c1891baa1b1904b281994323c52aa15/tensorflow/tools/pip_package/setup.py#L274\r\n\r\nYou probably need to build your own pip package and solve incompatibility issues. I remember that async was causing problem, but it seems that this has been solved. Not sure if there are others left.", "Duplicate of #20517. \r\n3.7 is not supported now.", "Closing as a duplicate."]}, {"number": 21850, "title": "Error while using create_inference_graph - Can't find a device placement for the op!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.10\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:  0.16.1\r\n- **GCC/Compiler version (if compiling from source)**:  5.4.0\r\n- **CUDA/cuDNN version**: 9.0/7.0.5\r\n- **GPU model and memory**: 4 x GTX 1080Ti 394.44 11GB VRAM each\r\n- **TensorRT version**:3.0.4\r\n\r\n### Describe the problem\r\nerror while running trt.create_inference_graph on a frozen graph pb\r\n\r\n### Source code\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.tensorrt as trt\r\nfrom tensorflow.python.framework import graph_io\r\n\r\n_input = 'input_1'\r\n_output = 'output_node0'\r\noutputs = [_output]\r\n\r\n\r\ndef get_frozen_graph():\r\n  with tf.gfile.FastGFile('model.h5.pb', \"rb\") as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n  return graph_def\r\n\r\n\r\nfrozen_graph_def = get_frozen_graph()\r\ntrt_graph_def = trt.create_inference_graph(frozen_graph_def, \r\n\t\t\t\t\toutputs,\r\n\t\t\t\t\tmax_batch_size=256, \r\n\t\t\t\t\tmax_workspace_size_bytes=1 << 30, \r\n\t\t\t\t\tprecision_mode='FP16')\r\ntf.reset_default_graph()\r\ng = tf.Graph()\r\nwith tf.Session(graph=g) as sess:\r\n\twith g.as_default():\r\n\t\ttf.import_graph_def(\r\n  \t\tgraph_def=trt_graph_def,\r\n  \t\tname='')\r\n\tgraph_io.write_graph(g, '.', 'trt_frozen.pb', as_text=False)\r\n```\r\n### logs\r\n2018-08-24 15:34:59.873109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-24 15:34:59.964048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-24 15:35:00.046908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-24 15:35:00.149867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-24 15:35:00.150535: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 4\r\n2018-08-24 15:35:00.150615: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2018-08-24 15:35:00.151247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.91GiB freeMemory: 10.38GiB\r\n2018-08-24 15:35:00.151374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 1 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2018-08-24 15:35:00.151494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 2 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:03:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2018-08-24 15:35:00.151608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 3 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2018-08-24 15:35:00.156119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-08-24 15:35:01.123215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-24 15:35:01.123254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 \r\n2018-08-24 15:35:01.123259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y \r\n2018-08-24 15:35:01.123263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y \r\n2018-08-24 15:35:01.123266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y \r\n2018-08-24 15:35:01.123269: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N \r\n2018-08-24 15:35:01.123691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10016 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-08-24 15:35:01.202813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10386 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-08-24 15:35:01.286295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10386 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-08-24 15:35:01.369518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10386 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)\r\n2018-08-24 15:35:01.827178: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2951] Segment @scope '', converted to graph\r\n2018-08-24 15:35:01.827279: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!\r\n2018-08-24 15:35:01.901005: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.\r\n2018-08-24 15:35:02.432465: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\n2018-08-24 15:35:02.433733: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\n2018-08-24 15:35:02.434291: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine my_trt_op_0 creation for segment 0, composed of 1089 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\r\n2018-08-24 15:35:02.751864: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2951] Segment @scope '', converted to graph\r\n2018-08-24 15:35:02.751964: E tensorflow/contrib/tensorrt/convert/convert_graph.cc:418] Can't find a device placement for the op!\r\n2018-08-24 15:35:02.856268: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Half2 support requested on hardware without native FP16 support, performance will be negatively affected.\r\n2018-08-24 15:35:02.904384: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\n2018-08-24 15:35:02.905595: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger resources.cpp (199) - Cuda Error in gieCudaMalloc: 2\r\n2018-08-24 15:35:02.906038: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:958] Engine my_trt_op_0 creation for segment 0, composed of 1089 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\r\n2018-08-24 15:35:03.064587: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-08-24 15:35:03.127441: W tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc:185] TensorRTOptimizer is probably called on funcdef! This optimizer must *NOT* be called on function objects.\r\n2018-08-24 15:35:03.160255: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:403] Optimization results for grappler item: tf_graph\r\n2018-08-24 15:35:03.160298: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 1098 nodes (-606), 1155 edges (-606), time = 96.013ms.\r\n2018-08-24 15:35:03.160304: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   layout: Graph size after: 1161 nodes (63), 1157 edges (2), time = 32.452ms.\r\n2018-08-24 15:35:03.160322: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 1161 nodes (0), 1157 edges (0), time = 808.131ms.\r\n2018-08-24 15:35:03.160329: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 1100 nodes (-61), 1157 edges (0), time = 80.726ms.\r\n2018-08-24 15:35:03.160332: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 1100 nodes (0), 1157 edges (0), time = 390.558ms.\r\n2018-08-24 15:35:03.160335: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:403] Optimization results for grappler item: my_trt_op_0_native_segment\r\n2018-08-24 15:35:03.160349: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 1090 nodes (0), 1147 edges (0), time = 65.266ms.\r\n2018-08-24 15:35:03.160434: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   layout: Invalid argument: The graph is already optimized by layout optimizer.\r\n2018-08-24 15:35:03.160438: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 1090 nodes (0), 1147 edges (0), time = 9.5ms.\r\n2018-08-24 15:35:03.160455: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   constant folding: Graph size after: 1090 nodes (0), 1147 edges (0), time = 53.271ms.\r\n2018-08-24 15:35:03.160458: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:405]   TensorRTOptimizer: Graph size after: 1090 nodes (0), 1147 edges (0), time = 9.467ms.\r\n2018-08-24 15:35:03.336425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-08-24 15:35:03.336534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-24 15:35:03.336542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 1 2 3 \r\n2018-08-24 15:35:03.336545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N Y Y Y \r\n2018-08-24 15:35:03.336548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 1:   Y N Y Y \r\n2018-08-24 15:35:03.336551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 2:   Y Y N Y \r\n2018-08-24 15:35:03.336553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 3:   Y Y Y N \r\n2018-08-24 15:35:03.336822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10016 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-08-24 15:35:03.336960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10386 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1)\r\n2018-08-24 15:35:03.337073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10386 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\n2018-08-24 15:35:03.337142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10386 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1)", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nExact command to reproduce", "I try master branch, same error.", "Yes it is still an issue ", "I'm getting this too. Using TensorRT and TF 1.12.\r\n\r\nUpdate: I don't see this error on Tensorflow 1.8 though.", "This happens when you define a graph while not assigning any device to certain subgraphs that later become TRT ops. This is fine because a TRT op has always a device assigned to it. So, it's safe to ignore this error msg. The message will go away once this PR is merged https://github.com/tensorflow/tensorflow/pull/23866\r\n\r\nThat being said, I agree that the segmenter needs to be smarter in handling device assignment in ops.", "Its present in the latest r1.12.0 (built from source cuda 10 cudnn 7.3.1 nccl 2.3.7 trt 5.0.2)", "Hi @srihari-humbarwadi , Could you please refer t[his document](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html) for using TF-TRT in latest version.\r\nWe  also see that you are using old version of Tensorflow which is officially considered as end of life, We recommend that you upgrade to 2.6 version and let us know if the issue still persists in newer versions .Please open a new issue in case you face any errors, we will get you the right help .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21850\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/21850\">No</a>\n"]}, {"number": 21849, "title": "Update ops.h -- Eliminate a Warning Message of Type Inconsistency", "body": "In this source code file, the type of t.NumElements() is a signed integer, while the type of v.size() is unsigned. For the \"!=\" operator between them, a warning message appears when TensorFlow is compiled with a C++ application.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Hi @tedlz123,\r\n\r\nI am going to close this as a duplicate of #21846. If I'm wrong, please re-open. \r\n\r\nThanks!", "I signed the CLA!"]}, {"number": 21848, "title": "Update ops.h -- Eliminate a Warning Message of Type Inconsistency", "body": "In this source code file, the type of t.NumElements() is a signed integer, while the type of v.size() is unsigned. For the \"!=\" operator between them, a warning message appears when TensorFlow is compiled with a C++ application.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Hi @tedlz123,\r\n\r\nI am going to close this as a duplicate of #21846. If I'm wrong, please re-open.\r\n\r\nThanks!", "I signed the CLA!"]}, {"number": 21847, "title": "Update ops.h -- Eliminate a Warning Message of Type Inconsistency", "body": "In this source code file, the type of t.NumElements() is a signed integer, while the type of v.size() is unsigned. For the \"!=\" operator between them, a warning message appears when TensorFlow is compiled with a C++ application.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "Hi @tedlz123,\r\n\r\nI am going to close this as a duplicate of #21846. If I'm wrong, please re-open.\r\n\r\nThanks!", "I signed the CLA!"]}, {"number": 21846, "title": "Update ops.h -- Eliminating a Compilation Warning Message of Type Inconsistency", "body": "In this source code file, the type of t.NumElements() is a signed integer, while the type of v.size() is unsigned. For the \"!=\" operator between them, a warning message appears when TensorFlow is compiled with a C++ application.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed the CLA!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 21845, "title": "android yolo sample output format is different from the official documentation", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  MacOS 10.13.5 (Non-Relevant)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:  Non-Relevant\r\n- **TensorFlow installed from (source or binary)**: Unsure\r\n- **TensorFlow version (use command below)**: org.tensorflow:tensorflow-android:1.9.0 Non-Relevant\r\n- **Python version**:  N/A\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:N/A\r\n\r\n### Describe the problem\r\n\r\nIn Android Yolo Sample Code\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L171\r\n\r\n```java\r\n        new float[gridWidth * gridHeight * (NUM_CLASSES + 5) * NUM_BOXES_PER_BLOCK];\r\n        ............\r\n        for (int y = 0; y < gridHeight; ++y) {\r\n           for (int x = 0; x < gridWidth; ++x) {\r\n               for (int b = 0; b < NUM_BOXES_PER_BLOCK; ++b) {\r\n                   ..........\r\n                    for (int c = 0; c < NUM_CLASSES; ++c) {\r\n                        classes[c] = output[offset + 5 + c];\r\n```\r\nalong with the following output decoding process, is not compatible with the original paper\r\nthese code assumes the output formats was \r\n\r\n **grid * grid * (class + 5) * box** \r\n\r\nbut in the original paper and other resource , this should be   \r\n\r\n**grid * grid * (class + box * 5)**\r\n\r\nas per in the paper [https://arxiv.org/pdf/1506.02640.pdf]( S * S * (B * 5 + C) )\r\nthis is totally different in format and size since every offset of the grid messed up\r\nthe paper indicates that the network only predicts one set of class probabilities per cell, regardless of the number of boxes B\r\n\r\n### Source code / logs\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L171\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L187-L213\r\n\r\nhttps://arxiv.org/pdf/1506.02640.pdf\r\n", "comments": ["@andrewharp , can you comment as to whether the Android Yolo detector demo is supposed to match the original paper output?", "@lolicon it must be you .pb file is not correct,you should sure it's ./flow --model cfg/tiny-yolo-voc.cfg --load bin/tiny-yolo-voc.weights --savepb , firstly it occurs to me, when i change the .pb file it's works?\r\nbut there is something wrong when i transplant my model, i didn't know how to modification the source code,can anyone help me,ths", "@lolicon,\r\nCan you please refer to the documentation of [YOLO Example](https://www.tensorflow.org/lite/examples/object_detection/overview) and let us know if the issue still persist? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 21844, "title": "How to reshape and slice a variable without converting it to an untrainable tensor?", "body": "[Feature Request]\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: pip-install\r\n- **TensorFlow version (use command below)**: v1.10.0-0-g656e7a2b34 1.10.0\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 7.5, V7.5.17\r\n- **GPU model and memory**: TITAN Xp \r\n- **Exact command to reproduce**:  Described below \r\n\r\n\r\n### Describe the problem\r\n\r\nI am using custom trainable variables in my layers, I pass the variables as a parameter to the layer (which are custom too).\r\n\r\nI want to set (or pass) only a slice of my trainable variables array to the layer. \r\n\r\nFor example: \r\n\r\n    custom_train_vars = tf.get_variable(name, [10,30,60,60], initializer=initializer, trainable=trainable)\r\n    layer_weight = custom_train_vars[2]\r\n    x = my_custom_conv_layer(x, layer_weight, args..)\r\n\r\n\r\nThe problem is after slicing (or reshaping with `tf.reshape`) the variable i.e. `layer_weight = custom_train_vars[2]`, it gets converted to the type `Tensor(\"strided_slice:0\",args..)`\r\n\r\nAnd I'm unable to use this as a training variable inside my custom layer, because my optimizer throws an error while finding the gradient of this Tensor. \r\n\r\nFor example:\r\n\r\n    Optimizer.compute_gradients(train_loss, layer_weight)\r\n\r\n Is there a workaround/feature where the slicing won't change the type of variable? \r\n\r\n", "comments": ["@tatatodd  Could you please review this issue? :) ", "@yashkant Can you please post the error the message you are getting?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 21843, "title": "How to use multi-gpu with estimator ??? Hard to find the answer with simple usage ???", "body": "pip install tensorflow-gpu==1.8.0", "comments": []}, {"number": 21842, "title": "Zombie python kernel when interrupted with Ctrl+C", "body": "Hi everyone.\r\nI am using tensorflow 1.5 and facing the following problem.\r\nThe system UI may stuck occasionally when a python kernel running tensorflow model is interrupted with Ctrl+C. \r\nBut I can still use ssh to log into the system and found the python kernel becomes a zombie process, which cannot be killed by terminating itself or its parent process.\r\nThe problem do not happen every time I interrupt the python program, thus I have no idea about how to  trac the cause. But since only the UI gets stuck, and *nvidia-smi* command give no response, I guess the problem may occur on releasing GPU memory or something related.   \r\n \r\nI've googled it, but did not get any helpful information.\r\nDoes anyone meet the same problem or have some idea on how to debug this problem?\r\n\r\n------------------------\r\n\r\n### System information\r\n- Ubuntu 16.04 LTS\r\n- Tensorflow-gpu r1.5 with CUDA 9.0 and cudnn 7.1, installed via pip method.\r\n- Anaconda Python 3.6\r\n- Nvidia Geforce GTX Titan X\r\n- Intel Core i7-4790\r\n- 32GB Memory\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "Sorry for not following the instruction of system information template.  \r\n\r\n--------------\r\n**Have I written custom code**  No\r\n**OS Platform and Distribution** Ubuntu 16.04 LTS\r\n**TensorFlow installed from** Binariy\r\n**TensorFlow version** r1.5\r\n**Bazel version** None\r\n**CUDA/cuDNN version** CUDA 9.0, cuDNN 7.1\r\n**GPU model and memory** Geforce GTX Titan X 12GB\r\n**Exact command to reproduce** Use Ctrl + C to interrupt program\r\n**Mobile device**  Not", "The problem machine is no longer in use so I can't provide more information. You can close the issue."]}, {"number": 21841, "title": "kernel register bug fix", "body": "If the right kernel found, don't set `was_attr_mismatch` true.", "comments": []}, {"number": 21840, "title": "Does tensorflow provide an operation like caffe average_loss operation?", "body": "due to the limiting of gpu, I want to update my weight after every two step training. Specifically, the network will firstly calculate the fisrt batch inputs and save the loss. And then the network calculate the next batch inputs and average these two losses and will update the weights once. It likes average_loss op in caffe, for [example](https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/voc-fcn8s/solver.prototxt) . and how to calculate the batchnorm update-ops.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing this issue due to lack of activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 21839, "title": "SSD_mobilenet_v1/0.75_quantized_coco trained model is not detecting anything after porting on Android [Detect app]", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n     No, I have only changed few lines in the config file.\r\n- **OS Platform and Distribution: 14.04 LTS\r\n- **Mobile device if the issue happens on mobile device**: Motoroloa, Android version 7.1.1\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: 4GB, 64-Bit\r\n- **Bazel Version**: 0.16.0\r\n\r\n### Describe the problem\r\nCould you please help me regarding the results of the trained model.\r\n\r\nI trained two different models from tensorflow zoo directory named ssd_mobilenet_v1_0.75_depth_quantized_coco and ssd_mobilenet_v1_quantized_coco each with 10000 training steps. I had changed the no of classes to 5 (for my data set) and used a batch size of 10 for both the models. However, the total no of of images are limited to 205.\r\n\r\nBoth the model trained well with all files generated. I **tested** the trained model using **webcam** and it worked perfectly fine.\r\n\r\nAs a next step, I used this blog by @achowdhery  [Blog Link](https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193) here and converted the .pb file into .tflite format. The generated detect.tflite is of size 3.2 Mb (This is what authors have also suggested in the blog that the file should be less than 4 Mb). However, after build over android device, **there is no detection at all**.\r\n\r\nI tried to hold it for at least 2 minutes for each classes (by showing images of each class), but there is no result in detection. It did not even overfit and show any false detection.\r\n\r\nWhat I have done wrong? Please share your thoughts.\r\n\r\nI am trying with both the models but no luck at all. I wonder the same model **(frozen graph) is working perfectly fine for webcam based detection.**\r\n\r\nThank you,", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nExact command to reproduce", "@tensorflowbutler Thank you I have now updated.\r\n", "Is there any update on the issue? ", "@alokranjan007 Please share the intermediate files: checkpoint, frozen graph, pipeline config, label map.txt to reproduce the error. Is this quantized or float model?", "@achowdhery Thank you for your concern. I am unable to upload here all files due to limited upload permission. Sorry for the inconvenience, I am attaching here 2 different folders. Each following folder contains below files.\r\n1. Source_file1: detect.tflite, label_map.txt and pipeline.config\r\n2. Source_file2: tflite_graph.pb\r\n\r\nCheckpoint file for 10000 training steps is of file size 44.7 Mb, which I am unable to upload due to limited upload permissions from GitHub.\r\n\r\nThe model is quantized version and I have downloaded it from tensorflow zoo directory. The folder name is \"ssd_mobilenet_v1_quantized_coco\".\r\n\r\n\r\nI had also tried with ssd_mobilenet_0.75_quantized_coco, but after training no detection on the detect app. However, when I am checking the trained model using webcam it is working fine for both the models.\r\n\r\nThank you for your time.\r\n\r\n\r\n\r\n\r\n\r\n\r\n[source_files1.tar.gz](https://github.com/tensorflow/tensorflow/files/2326689/source_files1.tar.gz)\r\n[source_file2.tar.gz](https://github.com/tensorflow/tensorflow/files/2326695/source_file2.tar.gz)\r\n", "@achowdhery thanks for responding. Do you have what you need from @alokranjan007 ?", "@cy89 I believe the source files have the necessary information. But, if any thing is missing then please let me know @achowdhery. I would be happy to learn more.\r\n\r\nThank you both for your help.", "Hey @cy89, Is there any update on the issue? I am still facing the same issue for detection.  ", "@achowdhery, gentle ping? ", "@cy89  Its been 14 days...is there any update on the issue?", "@alokranjan007 Is this still an issue? The app is working correctly on my end with Tensorflow Lite 1.12. May I request you to check with that version?", "please add comma between output layer members.  \r\nI mean once get tflite_graph.pb file as you know you have to use tflite_convert command to generate detect.tflite file. That command needs output_layers items and if you try to do detection you need 4 items like \"TFLite_Detection_PostProcess\" \"TFLite_Detection_PostProcess:1\" and so on...  your problem like mine I guessed \u0131 have forgetten commas between the members thats why detect.tflite did not run.  And I found  usefull app called [netron](https://github.com/lutzroeder/netron) will help you...\r\n\r\nActually , i am curious about how you could start the retraining of ssd_mobilenet_v1_quantized_coco model or other quantized model . I mean once I used the command starting with \"python train.py...\" I have face some errors that was so long ... (I did not get that error thats why i cannot mention it openly now.)", "Marking issue as resolved due to inactivity. Feel free to re-open this if it's unresolved or file a [new issue](https://github.com/tensorflow/tensorflow/issues/new/choose)"]}, {"number": 21838, "title": "Keras Layer add_variable not taking Dimension objects", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Heavily based on example custom layer on tensorflow's keras page.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab (Linux Ubuntu 17.10)\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: Unsure \r\n- **TensorFlow version (use command below)**: 1.10.0\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: Unsure. nvcc --version did not work.\r\n- **GPU model and memory**: K80 GPU (Memory unsure)\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nMy [question](https://stackoverflow.com/questions/51939618/what-could-cause-a-type-error-during-the-build-process-of-keras-layer) was answered on StackOverflow and a user recommended me to create a bug report here. I was creating a keras layer based on the NALU design, but I found that while on my local version of tensorflow (1.8.0) the code would run as expected, the same code would not run on Google Colab's version of tensorflow(1.10.0). The issue ended up being that tensorflow's Dimension type was not accepted in the Layer.add_variable method. It requires the dimensions to be converted to ints, which does not happen implicitly. I think that it would make sense for the method to accept Dimension objects as they are used to specify the shape of Tensors. As well, it provides backwards compatibility to previous versions of tensorflow, which do support pass Dimension objects to Layers.add_variable.\r\n\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nSource:\r\n```\r\nimport tensorflow as tf\r\n\r\nclass NALU(tf.keras.layers.Layer):\r\n    def __init__(self, num_outputs, **kwargs):\r\n        self.num_outputs = num_outputs\r\n        super(NALU, self).__init__(**kwargs)\r\n        \r\n    def build(self, input_shape):\r\n        shape = tf.TensorShape((input_shape[1], self.num_outputs)).as_list()\r\n        get = tf.keras.initializers.get\r\n        self.W_ = self.add_variable(\"W_\", shape=shape, initializer=get('glorot_uniform'))\r\n        self.M_ = self.add_variable(\"M_\", shape=shape, initializer=get('glorot_uniform'))\r\n        self.GAM = self.add_variable(\"GAM\", shape=shape, initializer=get('glorot_uniform')) # Gate add & multiply\r\n        self.GM = self.add_variable(\"GM\", shape=shape, initializer=get('glorot_uniform')) # Gate multiply\r\n        \r\n        super(NALU, self).build(input_shape)\r\n    \r\n    def call(self, x):\r\n        gam = tf.sigmoid(tf.matmul(x, self.GAM)) # The gate\r\n        gm = tf.sigmoid(tf.matmul(x, self.GM)) # The gate\r\n        \r\n        W = tf.tanh(self.W_) * tf.sigmoid(self.M_)\r\n        add = tf.matmul(x, W)\r\n        \r\n        # represents positive multiplication\r\n        m = tf.exp(\r\n            tf.matmul(tf.log(tf.abs(x) + 1e-14), W)\r\n        )\r\n        \r\n        mul = (-1 * m) * (1.0 - gm) + gm * m \r\n        \r\n        y = gam * add + (1.0 - gam) * mul\r\n        return y\r\n    \r\n    def compute_output_shape(self, input_shape):\r\n        # --------------IMPORTANT LINE---------------------\r\n        shape = tf.TensorShape(input_shape)\r\n        #  shape = tf.TensorShape(input_shape).as_list() is required to make the function work\r\n        # -------------------------------------------------\r\n        shape[-1] = self.num_outputs\r\n        return tf.TensorShape(shape)\r\n    \r\n    def get_config(self):\r\n        base_config = super(NALU, self).get_config()\r\n        base_config['num_outputs'] = self.num_outputs\r\n        \r\n    @classmethod\r\n    def from_config(cls, config):\r\n        return cls(**config)\r\n\r\ndef nalu_model():\r\n    inp = tf.keras.layers.Input(shape=(2,))\r\n    out = NALU(1)(inp)\r\n    \r\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\r\n    return model\r\n```\r\nTest Case:\r\n```\r\nimport numpy as np\r\n\r\ndef create_train():\r\n#     x_train = np.random.randint(-100, 100, size=(10000000, 2), dtype=np.int32)\r\n    x_train = np.random.uniform(-100, 100, size=(10000000, 2))\r\n\r\n    first_neg = np.copy(x_train)\r\n    first_neg[:, 0] *= -1\r\n\r\n    second_neg = np.copy(x_train)\r\n    second_neg[:, 1] *= -1\r\n\r\n    all_neg = x_train * -1\r\n\r\n    x_train = np.append(x_train, first_neg)\r\n    x_train = np.append(x_train, second_neg)\r\n    x_train = np.append(x_train, all_neg)\r\n    x_train = x_train.reshape((40000000, 2))\r\n\r\n    com = x_train[:,::-1]\r\n    x_train = np.append(x_train, com).reshape((80000000, 2))\r\n    y_train = x_train[:, 0] * x_train[:, 1]\r\n    \r\n    return x_train, y_train\r\n\r\nx_train, y_train = create_train()\r\nx_test = np.random.randint(-1000, 1000, size=(10000, 2))\r\ny_test = x_test[:, 0] * x_test[:, 1]\r\n\r\nmodel = nalu_model()\r\n\r\nmodel.compile(optimizer='RMSProp',\r\n              loss='MSE',\r\n              metrics=['accuracy', 'MAE'])\r\n\r\ncb = [tf.keras.callbacks.TensorBoard(log_dir='./add_logs', histogram_freq=1, batch_size=32, write_graph=True, write_grads=True),\r\n      tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00000001, patience=5, verbose=2, mode='auto')]\r\n\r\nmodel.fit(x_train, y_train, epochs=500, batch_size=256, shuffle=True, callbacks=cb, validation_split=0.4)\r\nmodel.evaluate(x_test, y_test)\r\n```\r\n\r\n\r\nError:\r\n```\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-7-55fb94a8f3b1> in <module>()\r\n     82 y_test = x_test[:, 0] * x_test[:, 1]\r\n     83 \r\n---> 84 model = nalu_model()\r\n     85 \r\n     86 model.compile(optimizer='RMSProp',\r\n\r\n<ipython-input-7-55fb94a8f3b1> in nalu_model()\r\n     48 def nalu_model():\r\n     49     inp = tf.keras.layers.Input(shape=(2,))\r\n---> 50     out = NALU(1)(inp)\r\n     51 \r\n     52     model = tf.keras.models.Model(inputs=inp, outputs=out)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    726         if all(hasattr(x, 'shape') for x in input_list):\r\n    727           input_shapes = nest.map_structure(lambda x: x.shape, inputs)\r\n--> 728         self.build(input_shapes)\r\n    729         self.built = True\r\n    730 \r\n\r\n<ipython-input-7-55fb94a8f3b1> in build(self, input_shape)\r\n      9         shape = tf.TensorShape((input_shape[1], self.num_outputs))\r\n     10         get = tf.keras.initializers.get\r\n---> 11         self.W_ = self.add_variable(\"W_\", shape=shape, initializer=get('glorot_uniform'))\r\n     12         self.M_ = self.add_variable(\"M_\", shape=shape, initializer=get('glorot_uniform'))\r\n     13         self.GAM = self.add_variable(\"GAM\", shape=shape, initializer=get('glorot_uniform')) # Gate add & multiply\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_variable(self, *args, **kwargs)\r\n    459   def add_variable(self, *args, **kwargs):\r\n    460     \"\"\"Alias for `add_weight`.\"\"\"\r\n--> 461     return self.add_weight(*args, **kwargs)\r\n    462 \r\n    463   def add_weight(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, getter)\r\n    563         use_resource=use_resource,\r\n    564         synchronization=synchronization,\r\n--> 565         aggregation=aggregation)\r\n    566 \r\n    567     if regularizer is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    533     new_variable = getter(\r\n    534         name=name, shape=shape, dtype=dtype, initializer=initializer,\r\n--> 535         **kwargs_for_getter)\r\n    536 \r\n    537     # If we set an initializer and the variable processed it, tracking will not\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in make_variable(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, synchronization, aggregation, partitioner)\r\n   1916       use_resource=use_resource,\r\n   1917       synchronization=synchronization,\r\n-> 1918       aggregation=aggregation)\r\n   1919   return v\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in variable(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource, synchronization, aggregation)\r\n   2441       use_resource=use_resource,\r\n   2442       synchronization=synchronization,\r\n-> 2443       aggregation=aggregation)\r\n   2444 \r\n   2445 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in <lambda>(**kwargs)\r\n   2423              synchronization=VariableSynchronization.AUTO,\r\n   2424              aggregation=VariableAggregation.NONE):\r\n-> 2425   previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n   2426   for getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access\r\n   2427     previous_getter = _make_getter(getter, previous_getter)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)\r\n   2393         collections=collections, validate_shape=validate_shape,\r\n   2394         caching_device=caching_device, name=name, dtype=dtype,\r\n-> 2395         constraint=constraint)\r\n   2396   elif not use_resource and context.executing_eagerly():\r\n   2397     raise RuntimeError(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint)\r\n    310           name=name,\r\n    311           dtype=dtype,\r\n--> 312           constraint=constraint)\r\n    313 \r\n    314   # pylint: disable=unused-argument\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint)\r\n    415               with ops.name_scope(\"Initializer\"), ops.device(None):\r\n    416                 initial_value = ops.convert_to_tensor(\r\n--> 417                     initial_value(), name=\"initial_value\", dtype=dtype)\r\n    418               self._handle = _eager_safe_variable_handle(\r\n    419                   shape=initial_value.get_shape(),\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in <lambda>()\r\n   1901         initializer = initializer(dtype=dtype)\r\n   1902       init_val = lambda: initializer(  # pylint: disable=g-long-lambda\r\n-> 1903           shape, dtype=dtype, partition_info=partition_info)\r\n   1904       variable_dtype = dtype.base_dtype\r\n   1905   if use_resource is None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py in __call__(self, shape, dtype, partition_info)\r\n    474       scale /= max(1., fan_out)\r\n    475     else:\r\n--> 476       scale /= max(1., (fan_in + fan_out) / 2.)\r\n    477     if self.distribution == \"normal\" or self.distribution == \"truncated_normal\":\r\n    478       # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\r\n\r\nTypeError: unsupported operand type(s) for /: 'Dimension' and 'float'\r\n```", "comments": ["Hi, I create PR #21865 to fix the issue."]}, {"number": 21837, "title": "Incorrect gradients when different Python variables are assigned the same tf.constant() value.", "body": "When different Python variables are assigned the same tf.constant() value, the computed gradients are incorrect.\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\ntfe = tf.contrib.eager\r\n\r\n# When `x` and `w` are assigned the same constant value, the gradient is\r\n# incorrect.\r\nx = tf.constant(3.0)\r\nw = tf.constant(3.0)\r\nwith tf.GradientTape() as g:\r\n  g.watch(x)\r\n  y = x * w\r\nprint(g.gradient(y, x))  # Prints 6.0 when the gradient should be 3.0.\r\n\r\n# I assume this happens because `x` and `w` reference the same object.\r\nprint(x is w)  # Prints True.\r\n\r\n# When `x` and `w` are assigned different constant values, the gradient is\r\n# correct.\r\nx = tf.constant(3.1)\r\nw = tf.constant(3.0)\r\nwith tf.GradientTape() as g:\r\n  g.watch(x)\r\n  y = x * w\r\nprint(g.gradient(y, x))  # Prints 3.0, the correct gradient.\r\n\r\n\r\n# This also happens with `tfe.gradients_function()`.\r\ndef multiply_by_3(x):\r\n  return tf.multiply(x, tf.constant(3.0))\r\n\r\ngrad = tfe.gradients_function(multiply_by_3)\r\n\r\nprint(grad(3.0))  # Prints 6.0 when the gradient should be 3.0.\r\nprint(grad(3.1))  # Prints 3.0, the correct gradient.\r\n```\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nOnly the code above.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMac OS X 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version**:\r\n1.10.0\r\n- **Python version**:\r\n2.7.15\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Exact command to reproduce**:\r\nRunning the code above.\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce\nMobile device", "@rmlarsen Will Tensorflow cache `tf.constant(3.0)` and reuse it for different object in the underlying implementation?", "@tensorflowbutler, yes, this is still an issue.", "Nagging Assignee @tatianashp: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@facaiy, @elliotwaite Sorry this fell through the cracks. Is this still an issue?", "It was last time I tested it on September 8th. I was using the latest stable release at the time. However, I haven't tested it since.", "@elliotwaite @tatianashp @facaiy I run the test script on the tf-nightly (1.12.0.dev20181005). The outputs are correct. ", "So we can close this issue?", "@feihugis @josh11b Sounds good."]}, {"number": 21834, "title": "Compiling tensorflow lite on gcc 4.7", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nRaspbian \r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nInstalled from source\r\n\r\n- **TensorFlow version (use command below)**:\r\nRelease 1.9.0\r\n\r\n- **Bazel version (if compiling from source)**:\r\nN/A using make to compile Tensorflow Lite\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC version 4.7.3\r\n- **CUDA/cuDNN version**:\r\nN/a\r\n- **GPU model and memory**:\r\nN/a\r\n- **Mobile device**:\r\nN/a\r\n\r\n- **Exact command to reproduce**:\r\n1. Run ./tensorflow/contrib/lite/download_dependencies.sh command from root directory \r\n2. Copy resulting downloads folder, ios_makefile.inc, rpi_makefile.inc and the Makefile found in ./tensorflow/contrib/lite folder into the root directory\r\n3. From the root directory run make \r\n4. Should result in \"gen\" folder with tensorflowlite.a library\r\n\r\n### Describe the problem\r\nI am trying to get tensorflow lite up and running on some older IOT devices that have gcc version 4.7 installed (not possible to upgrade from this version either). Have not been able to do so yet so I am wondering if anyone has had an success doing so or if anyone one knows what steps I would have to take to do so. \r\n\r\n### Source code / logs\r\nWhen attempting to compile on gcc 4.7 the make output is:\r\n\r\ng++ -mfpu=neon -pthread -fPIC --std=c++11 -O3 -DNDEBUG -I. -I/home/Documents/Test/ststicLibs/include/tensorflow-master/../../../ -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/ -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/eigen -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/gemmlowp -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/neon_2_sse -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/farmhash/src -I/home/Documents/Test/ststicLibs/include/tensorflow-master/downloads/flatbuffers/include -I/home/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/allocation.cc -o /home/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/allocation.o\r\nIn file included from /usr/include/c++/4.7/memory:86:0,\r\n                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,\r\n                 from ./tensorflow/contrib/lite/allocation.h:25,\r\n                 from tensorflow/contrib/lite/allocation.cc:29:\r\n/usr/include/c++/4.7/bits/unique_ptr.h: In instantiation of \u2018std::unique_ptr<_Tp [], _Dp>& std::unique_ptr<_Tp [], _Dp>::operator=(std::unique_ptr<_Up, _Ep>&&) [with _Up = char []; _Ep = std::default_delete<char []>; _Tp = const char; _Dp = std::default_delete<const char []>; std::unique_ptr<_Tp [], _Dp> = std::unique_ptr<const char []>]\u2019:\r\ntensorflow/contrib/lite/allocation.cc:102:36:   required from here\r\n/usr/include/c++/4.7/bits/unique_ptr.h:340:4: error: use of deleted function \u2018void std::unique_ptr<_Tp [], _Dp>::reset(_Up) [with _Up = char*; _Tp = const char; _Dp = std::default_delete<const char []>]\u2019\r\n/usr/include/c++/4.7/bits/unique_ptr.h:404:7: error: declared here\r\n/usr/include/c++/4.7/bits/unique_ptr.h:341:4: error: no match for \u2018operator=\u2019 in \u2018std::unique_ptr<_Tp [], _Dp>::get_deleter<const char, std::default_delete<const char []> >() = std::forward<std::default_delete<char []> >((* &(& __u)->std::unique_ptr<_Tp [], _Dp>::get_deleter<char, std::default_delete<char []> >()))\u2019\r\n/usr/include/c++/4.7/bits/unique_ptr.h:341:4: note: candidates are:\r\n/usr/include/c++/4.7/bits/unique_ptr.h:71:12: note: std::default_delete<const char []>& std::default_delete<const char []>::operator=(const std::default_delete<const char []>&)\r\n/usr/include/c++/4.7/bits/unique_ptr.h:71:12: note:   no known conversion for argument 1 from \u2018std::default_delete<char []>\u2019 to \u2018const std::default_delete<const char []>&\u2019\r\n/usr/include/c++/4.7/bits/unique_ptr.h:71:12: note: std::default_delete<const char []>& std::default_delete<const char []>::operator=(std::default_delete<const char []>&&)\r\n/usr/include/c++/4.7/bits/unique_ptr.h:71:12: note:   no known conversion for argument 1 from \u2018std::default_delete<char []>\u2019 to \u2018std::default_delete<const char []>&&\u2019\r\nMakefile:203: recipe for target '/home/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/allocation.o' failed\r\nmake: *** [/home/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/allocation.o] Error 1\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nCUDA/cuDNN version\nGPU model and memory\nMobile device", "It seems gcc 4.7 has a bug in its unique_ptr handling. You can change simple_memory_arena.cc\r\nto use a raw pointer. \r\n\r\n1. Change the header to have underlying_buffer_ be of type\r\n```cpp\r\nstd::unique_ptr<char[]> underlying_buffer_; \r\n```\r\nbecomes\r\n```cpp\r\n  char* underlying_buffer_;\r\n```\r\n2. Add a destroy to destructor\r\n```cpp\r\ndelete [] underlying_buffer_\r\n```\r\n\r\nChange all uses of underlying_buffer_ in\r\n```cpp\r\nTfLiteStatus SimpleMemoryArena::Commit(TfLiteContext* context) \r\n```\r\n\r\nI.e. where it uses reset() change it to\r\n```cpp\r\ndelete [] underlying_buffer_;\r\nunderlying_buffer_=new char[whateversize was theere];\r\n```\r\nand remove any .get()'s\r\n\r\n", "Sorry for the delay. \r\n\r\nI applied the changes you mentioned and then got a similar error for allocation.cc/h so I made similar changes there as well. However, when I build after these changes I get another error (below is a snip it of the make output):\r\n\r\ng++ -mfpu=neon -pthread -fPIC --std=c++11 -O3 -DNDEBUG -I. -I/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/../../../ -I/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/ -I/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/eigen -I/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/gemmlowp -I/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/neon_2_sse -I/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/farmhash/src -I/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/flatbuffers/include -I/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/ -I/usr/local/include -c tensorflow/contrib/lite/kernels/conv.cc -o /home/pi/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/kernels/conv.o\r\nIn file included from /home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/eigen/unsupported/Eigen/CXX11/ThreadPool:59:0,\r\n                 from ./tensorflow/contrib/lite/kernels/internal/optimized/eigen_tensor_reduced_instantiations_oss.h:78,\r\n                 from ./tensorflow/contrib/lite/kernels/internal/optimized/eigen_spatial_convolutions.h:37,\r\n                 from ./tensorflow/contrib/lite/kernels/internal/optimized/multithreaded_conv.h:31,\r\n                 from ./tensorflow/contrib/lite/kernels/internal/optimized/cblas_conv.h:28,\r\n                 from tensorflow/contrib/lite/kernels/conv.cc:28:\r\n/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h: In instantiation of \u2018static EigenForTFLite::NonBlockingThreadPoolTempl<Environment>::PerThread* EigenForTFLite::NonBlockingThreadPoolTempl<Environment>::GetPerThread() [with Environment = EigenForTFLite::StlThreadEnvironment]\u2019:\r\n/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:92:34:   required from \u2018void EigenForTFLite::NonBlockingThreadPoolTempl<Environment>::Schedule(std::function<void()>) [with Environment = EigenForTFLite::StlThreadEnvironment]\u2019\r\n./tensorflow/contrib/lite/kernels/internal/optimized/multithreaded_conv.h:44:34:   required from here\r\n/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/downloads/eigen/unsupported/Eigen/CXX11/src/ThreadPool/NonBlockingThreadPool.h:326:34: error: \u2018per_thread_\u2019 cannot be thread-local because it has non-trivial type \u2018EigenForTFLite::NonBlockingThreadPoolTempl<EigenForTFLite::StlThreadEnvironment>::PerThread\u2019\r\nMakefile:203: recipe for target '/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/kernels/conv.o' failed\r\nmake: *** [/home/pi/Documents/Test/ststicLibs/include/tensorflow-master/gen/obj/tensorflow/contrib/lite/kernels/conv.o] Error 1", "Nagging Assignee @aselle: It has been 59 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hello,\r\n\r\nCurrently having the same issue previously mentioned. Still interested in using tensorflow lite with gcc 4.7.\r\n\r\nThanks,"]}, {"number": 21833, "title": "ppc64le: //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test", "body": "Please assign this issue to me\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: PPC64LE Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: master branch git clone from 8/23/18 (Last commit 9289302ad3d7941ddb9ce2d0dff56b333cbcf208)\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.15.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0, 7\r\n- **GPU model and memory**: 4 V100 GPUs with 16 GB of memory each\r\n- **Exact command to reproduce**:\r\nazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_                              //tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test\r\n\r\n\r\n### Describe the problem\r\n\r\n5 of 6 testcases fail with error similar to:\r\n```\r\n======================================================================\r\nFAIL: testBadInput (__main__.ScatterAddNdimTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              on/kernel_tests/scatter_add_ndim_op_test.py\", line 75, in testBadInput\r\n    self.assertAllEqual(init_val, input_data.eval())\r\n  File \"/opt/anaconda2/lib/python2.7/contextlib.py\", line 35, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              .py\", line 1615, in assertRaisesWithPredicateMatch\r\n    str(e)))\r\nAssertionError: Exception of type <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_dev                              =\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=t                              ame:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_devic                              edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\n\r\n----------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Source code / logs\r\n```\r\n[root@690470e3d41a workspace]# bazel test --config=cuda --test_tag_filters=-no_oss,-oss_serial,-no_gpu,-benchmark-test --test_timeout 300,450,1200,3600 --local_test_jobs=4 --test_output=errors --build_tests_only --config=opt --run_under=//tensorflow/tools/ci_build/gpu_build:parallel_gpu_execute //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nWARNING: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nINFO: Analysed target //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test (129 packages loaded).\r\nINFO: Found 1 test target...\r\nFAIL: //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test (see /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test/test.log)\r\nINFO: From Testing //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test:\r\n==================== Test output for //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test:\r\nRunning test /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test  on GPU 0\r\n2018-08-21 21:51:07.422276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0004:04:00.0\r\ntotalMemory: 15.75GiB freeMemory: 15.32GiB\r\n2018-08-21 21:51:07.422327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-21 21:51:07.669332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-21 21:51:07.669384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-08-21 21:51:07.669393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-08-21 21:51:07.669859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-21 21:51:07.725500: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n2018-08-21 21:51:07.725565: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\nE2018-08-21 21:51:07.730994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-21 21:51:07.731026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-21 21:51:07.731034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-08-21 21:51:07.731041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-08-21 21:51:07.731432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-21 21:51:07.741437: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n2018-08-21 21:51:07.741477: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\nE2018-08-21 21:51:07.745582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-21 21:51:07.745601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-21 21:51:07.745608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-08-21 21:51:07.745615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-08-21 21:51:07.745978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-21 21:51:07.755530: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n2018-08-21 21:51:07.755565: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\nF2018-08-21 21:51:07.760553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-21 21:51:07.760575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-21 21:51:07.760583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-08-21 21:51:07.760590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-08-21 21:51:07.760945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-21 21:51:07.770746: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n2018-08-21 21:51:07.770780: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\nE2018-08-21 21:51:07.774856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-21 21:51:07.774874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-21 21:51:07.774882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0\r\n2018-08-21 21:51:07.774889: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N\r\n2018-08-21 21:51:07.775259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4838 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0004:04:00.0, compute capability: 7.0)\r\n2018-08-21 21:51:07.787517: E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n2018-08-21 21:51:07.787565: E tensorflow/core/common_runtime/executor.cc:697] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\nE.\r\n======================================================================\r\nERROR: test1dim (__main__.ScatterAddNdimTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/python/kernel_tests/scatter_add_ndim_op_test.py\", line 37, in test1dim\r\n    tensor_forest_ops.scatter_add_ndim(input_data, indices, updates).run()\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 2241, in run\r\n    _run_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 4986, in _run_using_default_session\r\n    session.run(operation, feed_dict)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\n\r\n======================================================================\r\nERROR: test3dim (__main__.ScatterAddNdimTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/python/kernel_tests/scatter_add_ndim_op_test.py\", line 50, in test3dim\r\n    tensor_forest_ops.scatter_add_ndim(input_data, indices, updates).run()\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 2241, in run\r\n    _run_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 4986, in _run_using_default_session\r\n    session.run(operation, feed_dict)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\n\r\n======================================================================\r\nERROR: testIncompleteIndices (__main__.ScatterAddNdimTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/python/kernel_tests/scatter_add_ndim_op_test.py\", line 85, in testIncompleteIndices\r\n    tensor_forest_ops.scatter_add_ndim(input_data, indices, updates).run()\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 2241, in run\r\n    _run_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 4986, in _run_using_default_session\r\n    session.run(operation, feed_dict)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\n\r\n======================================================================\r\nERROR: testNoUpdates (__main__.ScatterAddNdimTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorflow/contrib/tensor_forest/python/kernel_tests/scatter_add_ndim_op_test.py\", line 62, in testNoUpdates\r\n    tensor_forest_ops.scatter_add_ndim(input_data, indices, updates).run()\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              line 2241, in run\r\n    _run_using_default_session(self, feed_dict, self.graph, session)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              line 4986, in _run_using_default_session\r\n    session.run(operation, feed_dict)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                               line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                               line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                               line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                               line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\nInvalidArgumentError: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_dev                              =\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=t                              ame:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_devic                              edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\n\r\n======================================================================\r\nFAIL: testBadInput (__main__.ScatterAddNdimTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              on/kernel_tests/scatter_add_ndim_op_test.py\", line 75, in testBadInput\r\n    self.assertAllEqual(init_val, input_data.eval())\r\n  File \"/opt/anaconda2/lib/python2.7/contextlib.py\", line 35, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test.runfiles/org_tensorflow/tensorf                              .py\", line 1615, in assertRaisesWithPredicateMatch\r\n    str(e)))\r\nAssertionError: Exception of type <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>: AttrValue must not have reference type value of float_ref\r\n         for attr 'tensor_type'\r\n        ; NodeDef: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_dev                              =\"edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas); Op<name=_Recv; signature= -> tensor:tensor_type; attr=t                              ame:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>\r\n         [[Node: Variable/_5 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_devic                              edge_2_Variable\", tensor_type=DT_FLOAT_REF, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^ScatterAddNdim/indices, ^ScatterAddNdim/deltas)]]\r\n\r\n----------------------------------------------------------------------\r\nRan 6 tests in 1.097s\r\n\r\nFAILED (failures=1, errors=4)\r\n================================================================================\r\nTarget //tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test up-to-date:\r\n  bazel-bin/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test\r\nINFO: Elapsed time: 64.835s, Critical Path: 45.36s\r\nINFO: 1 process: 1 local.\r\nINFO: Build completed, 1 test FAILED, 5 total actions\r\n//tensorflow/contrib/tensor_forest:scatter_add_ndim_op_test              FAILED in 2.8s\r\n  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/tensor_forest/scatter_add_ndim_op_test/test.log\r\n```\r\n", "comments": ["I had read online that ScatterAddNdim had no GPU support, I looked at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/tensor_forest/BUILD#L460-L465, and see the testcase has a tag 'no-pip-gpu'. So I believe this test shouldn't run in a GPU unit test, but the unit test only exclude the test matching the tag no-gpu.\r\n\r\n@gunan - Can I get your opinion on how to fix it. I see three ways:\r\n1. Add no-gpu tag to the testcase in tensor_forest/BUILD\r\n2. Add -no-pip-gpu to the test_tag_filters parameter in bazel for the GPU unit test.\r\n3. I noticed linux/gpu doesn't have a test defined for contrib like linux/cpu , should gpu test not test contrib?\r\n\r\nCC @jayfurmanek, He would prefer to test as much as possible in the GPU test.", "I think 3. there is the most interesting one for me. We do ship most of contrib (with limited support) in PowerAI so maximum test coverage is ideal.\r\nHistorically, what was the idea behind that no-pip-gpu flag?", "Sorry for the late reply. \r\nIdeally, linux GPU should test contrib, too but we are already dedicating thousands of GPUs to tensorflow CI. We test contrib on GPU every night to get the most with less machines.\r\nSo I would go with (1).\r\nThe idea behind no_pip_gpu tag is at one point, we mass-tagged the tests to get all builds green and filed bugs about those."]}, {"number": 21832, "title": "Status: CUDA driver version is insufficient for CUDA runtime version", "body": "- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nKernel: 2.6.32-573.12.1.el6.x86_64 \r\nHost: RHEL 6.7\r\n Container: Ubuntu 16.04.5 LTS \r\n\r\n- **TensorFlow installed from (source or binary)**: \r\nSingularity \r\n\r\n- **TensorFlow version (use command below)**:\r\nTensorflow:1.10.0-devel-gpu-py3\r\n\r\n- **Python version**:\r\nPython 3.5.2\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nGCC 5.4.0\r\n\r\n- **CUDA/cuDNN version**:\r\n9\r\n\r\n- **GPU model and memory**:\r\nSingularity tensorflow:1.10.0-devel-gpu-py3:~> nvidia-smi\r\nThu Aug 23 00:24:41 2018\r\n+------------------------------------------------------+\r\n| NVIDIA-SMI 352.39 Driver Version: 352.39 |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r\n| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r\n|===============================+======================+======================|\r\n| 0 Tesla K80 Off | 0000:84:00.0 Off | 0 |\r\n| N/A 39C P0 58W / 149W | 22MiB / 11519MiB | 0% E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n- **Exact command to reproduce**:\r\n$ # install nvidia driver v352.39\r\n$ sudo singularity build --sandbox /path/to/sandbox docker://tensorflow/tensorflow/1.10.0-devel-gpu-py3\r\n$ singularity shell -nv /path/to/sandbox\r\nSingularity tensorflow:1.10.0-devel-gpu-py3:~> nvidia-smi\r\nThu Aug 23 00:24:41 2018\r\n+------------------------------------------------------+\r\n| NVIDIA-SMI 352.39 Driver Version: 352.39 |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\r\n| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\r\n|===============================+======================+======================|\r\n| 0 Tesla K80 Off | 0000:84:00.0 Off | 0 |\r\n| N/A 39C P0 58W / 149W | 22MiB / 11519MiB | 0% E. Process |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes: GPU Memory |\r\n| GPU PID Type Process name Usage |\r\n|=============================================================================|\r\n| No running processes found |\r\n+-----------------------------------------------------------------------------+\r\nSingularity tensorflow:1.10.0-devel-gpu-py3:~> python3\r\nPython 3.5.2 (default, Nov 23 2017, 16:37:01)\r\n[GCC 5.4.0 20160609] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n\r\n            from tensorflow.python.client import device_lib\r\n            print(device_lib.list_local_devices())\r\n            2018-08-23 00:26:35.424225: I\r\n            tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports\r\n            instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n            2018-08-23 00:26:38.208490: I\r\n            tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with\r\n            properties:\r\n            name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n            pciBusID: 0000:84:00.0\r\n            totalMemory: 11.25GiB freeMemory: 11.16GiB\r\n            2018-08-23 00:26:38.208576: I\r\n            tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu\r\n            devices: 0\r\n            Traceback (most recent call last):\r\n            File \"\", line 1, in\r\n            File\r\n            \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/device_lib.py\",\r\n            line 41, in list_local_devices\r\n            for s in pywrap_tensorflow.list_devices(session_config=session_config)\r\n            File\r\n            \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\",\r\n            line 1679, in list_devices\r\n            return ListDevices(status)\r\n            File\r\n            \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\",\r\n            line 519, in exit\r\n            c_api.TF_GetCode(self.status.status))\r\n            tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed.\r\n            Status: CUDA driver version is insufficient for CUDA runtime version\r\n\r\n\r\n### Describe the problem\r\nI built a tensorflow container with singularity. I think there might be a mismatch between the some of the card drivers and cuda libraries between the host and container. I have the container built as a sandbox so I'm able to make modifications quiet easily, I was curious if there's a way I can install appropriate cuda driver and runtimes to the container, and have the container run off those instead of pulling libraries from the host which are incompatible with the container? Is this the right way to do it? Or should I be updating the cuda drivers / libraries on the host to match the container? \r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nBazel version\nMobile device", "Have I written custom code\r\n    N/A\r\nBazel version\r\n    N/A\r\nMobile device\r\n    N/A", "Would https://github.com/NIH-HPC/gpu4singularity be viable for Singularity 2.6.0 with --nv flags or would I need to make additional modification to library paths?", "This is not a tensorflow issue: according to https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html your nvidia driver is not new enough for cuda 9.0", "Sure. But the question is more on how to integrate compatible drivers into a tensorflow container. The adage about containerization is: build once, run anywhere; and not: build once, run anywhere with Nvidia drivers v485 and above plus a kernel supporting experimental filesystem overlays. Even experimental / unofficial documentation on this scenario would be extremely helpful for most HPC environments that are still running epel6. \u00af\\_(\u30c4)_/\u00af", "The world is not perfect. I'm afraid \"build once, run anywhere with nvidia drivers>=384.81\" is the way to go. At least that's what nvidia says: https://github.com/NVIDIA/nvidia-docker/wiki/CUDA#requirements\r\n> Running a CUDA container requires a machine with at least one CUDA-capable GPU and a driver compatible with the CUDA toolkit version you are using.", "@mforde84  @tensorflowbutler   \r\n\r\nI hit exactly this problem and someone else with the same combination (`tensorflow 1.11` + CUDA runtime 9.0 + cudnn 7.3 + nvidia driver 390 ) hit this problem too, though nvidia driver `390` is new enough for CUDA runtime `9.0`.  This person opened an issue in the Nvidia DevTalk:\r\n\r\nhttps://devtalk.nvidia.com/default/topic/1042575/cuda-driver-version-is-insufficient-for-cuda-runtime-version/?offset=2#5289688\r\n\r\nAnd I downgraded the tensorflow version from `1.11` (the latest conda version) to `1.7` and this problem got solved. And my question is if the newer tensorflow, say `1.10+`, has a dependency on specific nvidia drivers /cuda versions?", "We upgraded to a recent version of drivers 396 and the issue resolved. ", "@mforde84  Thanks for the confirmation. That's what I was thinking too, but I had trouble upgrading to `396.54` due to a broken dependency, however, after having read your confirmation, I managed to install `396.54` and now it works with tensorflow `1.11.0`, Yoho! Thanks! Upgraded the ticket in the Nvidia DevTalk. ", "> tensorflow 1.11 + CUDA runtime 9.0 + cudnn 7.3 + nvidia driver 390 \r\nthe combo should have worked. Note with 396.54 there will be one more upgrade once TF switches to CUDA 10.", "@nicolefinnie , thanks, I downgraded the tensorflow version fromt o 1.7 and this problem got solved. ", "I tested the recommendations in this thread, but I was not able to install any other driver than 390 on Ubuntu 18.04 and downgrading tensorflow to 1.7 resulted in a new error message:\r\n```\r\n2018-10-17 09:12:21.434933: E tensorflow/stream_executor/cuda/cuda_dnn.cc:343] Loaded runtime CuDNN library: 7.1.2 but source was compiled with: 7.2.1.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\nSegmentation fault (core dumped)\r\n```\r\nWhich is strange, as I had installed version 7.3.1 on my system, but it seems that anaconda installs its own cudnn in the enviroment.", "> I tested the recommendations in this thread, but I was not able to install any other driver than 390 on Ubuntu 18.04 and downgrading tensorflow to 1.7 resulted in a new error message:\r\n> \r\n> ```\r\n> 2018-10-17 09:12:21.434933: E tensorflow/stream_executor/cuda/cuda_dnn.cc:343] Loaded runtime CuDNN library: 7.1.2 but source was compiled with: 7.2.1.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\r\n> Segmentation fault (core dumped)\r\n> ```\r\n> Which is strange, as I had installed version 7.3.1...\r\n\r\n@saskra ,I was use deepin15.8, nvidia-driver==390.67, cuda==9.0,cudnn==7.0, and miniconda installed tensorflow-gpu==1.7,and the problem got solved.", "Saskra are you running in a container? ", "No. But I now found the solution: Anaconda creates an environment with its own incompatible cudnn version which has to be overwritten manually. :-)", "> \r\n> \r\n> No. But I now found the solution: Anaconda creates an environment with its own incompatible cudnn version which has to be overwritten manually. :-)\r\n\r\nI have the same problem. :-(\r\nWhich version of which exact conda module did you have to use to overwrite?\r\n", "I have Ubuntu 18.04 which needs Nvidia driver 390. Anaconda brings cuDNN 7.2.1, which seems to be too old for this driver version: https://anaconda.org/anaconda/cudnn Now I am using the newest cuDNN version (7.3.1) as suggested by the official download site: https://developer.nvidia.com/rdp/cudnn-download btw: Anaconda's cuDNN version depends on its TensorFlow version, I have the newest one here as well (1.11).\r\n\r\nPS: I suggested to update the version: https://github.com/ContinuumIO/anaconda-issues/issues/10224", "@mforde84 Would you mind sharing how you upgraded it? ", "check whether your nvidia-driver support your cuda version from here https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html", "> @mforde84 Would you mind sharing how you upgraded it?\r\n\r\nAs for me, upgrading my driver worked out. I run a Windows 10 PC and use TF 1.13. \r\n( **NOTE:** _Just an aside, I needed to activate my virtual environment and start Jupyter notebook in that env before I was able to use TF in the notebook.) \r\n\r\nHere is how I upgraded my driver:\r\n\r\n1. Open `Device Manager`\r\n2. Expand the `display adapters`\r\n3. Locate your NVIDIA Graphics adapter\r\n4. Right-click and click `Update driver`\r\n\r\nAlternative\r\n-------------\r\n- I found this software ( `GeForce Experience` ) on the NVIDIA website for my graphics family which can also be downloaded, installed and used to update the driver(s). This should work as well, though I didn't go that way. ", "@mforde84 Maybe you can get the solution from there. https://stackoverflow.com/q/41409842/7121726", "Same issue here and I can't find an appropriate tensorflow version. I currently have ubuntu version `16.04.6`, driver version `410.78`, cuda version `10`, conda version `4.7.11` and none of the above-mentioned tensorflow versions works for me. I tried `1.13.1`, `1.7` and `1.14`.  \r\nAnaconda installs cudnn with version `7.6.0`. **Edit:** I forced conda to use the version `10.0` for cudatoolkit and not `cuda10.1_0` as it was before (according to @saskra's suggestion), but nothing changed unfortunately. \r\n\r\nUpdating anaconda also didn't help. In fact, `conda update --all` and `conda update conda` outputs many new errors like:\r\n`InvalidArchiveError('Error with archive ...  You probably need to delete and re-\r\ndownload or re-create this file.  Message from libarchive was:...`\r\n\r\nCreating a conda environment with my current specs or simply running my python script also produces various `InvalidArchiveError` messages like above:\r\n```name: my_env\r\nchannels:\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - keras=2.2.4\r\n  - nltk=3.3.0\r\n  - numpy=1.15.4\r\n  - pandas=0.23.4\r\n  - python=3.6.6\r\n  - scikit-learn=0.20.0\r\n  - scipy=1.1.0\r\n  - tensorflow=1.7\r\n  - tensorflow-gpu=1.7\r\n  - cython=0.29\r\n  - pip:\r\n    - fasttext==0.8.3\r\n    - fuzzywuzzy==0.17.0\r\n    - python-levenshtein==0.12.0\r\n    - subsample==0.0.6\r\n    - talos\r\n    - tabulate==0.8.3", "I had a similar issue using driver 384.130. Turns out that versions of the cudatoolkit inside anaconda environment and the cuda supported by my driver did not match.\r\n\r\nThese two links helped me identifying my driver and cuda version and, later, to install the correct version of tensorflow_gpu that matched the cuda in my machine\r\n\r\nTo select the appropriate version based on your cuda installation:\r\nhttps://www.tensorflow.org/install/source#tested_build_configurations\r\n\r\n|Version | Python version | Compiler | Build tools | cuDNN | CUDA|\r\n|-- | -- | -- | -- | -- | --|\r\n|tensorflow_gpu-1.14.0 | 2.7, 3.3-3.7 | GCC 4.8 | Bazel 0.24.1 | 7.4 | 10.0|\r\ntensorflow_gpu-1.13.1 | 2.7, 3.3-3.7 | GCC 4.8 | Bazel 0.19.2 | 7.4 | 10.0\r\ntensorflow_gpu-1.12.0 | 2.7, 3.3-3.6 | GCC 4.8 | Bazel 0.15.0 | 7 | 9\r\ntensorflow_gpu-1.11.0 | 2.7, 3.3-3.6 | GCC 4.8 | Bazel 0.15.0 | 7 | 9\r\ntensorflow_gpu-1.10.0 | 2.7, 3.3-3.6 | GCC 4.8 | Bazel 0.15.0 | 7 | 9\r\ntensorflow_gpu-1.9.0 | 2.7, 3.3-3.6 | GCC 4.8 | Bazel 0.11.0 | 7 | 9\r\n\r\nThe cuda versions may have minor-versions (9.0, 9.2), thus you should double check what exactly you are installing with conda.\r\nTo check what you have inside your conda enviroment and how to install a different version\r\nhttps://stackoverflow.com/a/55351774/2971299\r\n\r\n\r\nSo, I identified my cuda version\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2017 NVIDIA Corporation\r\nBuilt on Fri_Sep__1_21:08:03_CDT_2017\r\nCuda compilation tools, release 9.0, V9.0.176\r\n```\r\n\r\nAnd installed the correct anaconda environment:\r\n```\r\nconda create -n gpu tensorflow-gpu==1.9.0 jupyter\r\n```\r\n\r\n", "Thank you very much @agostini01 . I actually have all versions aligned correctly. The only thing that actually worked out is the second answer here: https://stackoverflow.com/questions/41402409/tensorflow-doesnt-seem-to-see-my-gpu\r\nI uninstalled tensorflow and reinstalled tensorflow-gpu. Apparently they don't go well together?\r\nNow Python sees my GPUs and when I do `watch-smi` I can see my job using them.", "@KonstantinaLazaridou no problems. I believe your suggested link is for when you are installing cuda system wide. \r\n\r\nThis line: `conda create -n gpu tensorflow-gpu==1.9.0 jupyter cudatoolkit==XX` should work as long as you match the anaconda tensorflow-gpu version with the correct anaconda cudatoolkit (XX) and \"system-wide installed\" cuda driver. Unfortunately I dont remember what to use for the `XX` value anymore. \r\n|Apparently they don't go well together?\r\nindeed! Nice catch. The advantage of using conda is that you can have tensorflow in one environment and tensorflow-gpu in another.\r\n\r\n", "@mforde84 I had a similar issue using driver 384.81,but  Nvidia recommended Tesla k80 need install driver 384.183.So upgraded to a recent version of drivers 396 is a good choice???\r\n**GPU Tesla k80\r\ntensorflow-gpu 1.10.0\r\nCDUNN 7.0.5\r\nCUDA 9.0**\r\n\r\n2019-12-17 09:55:46.558571: E tensorflow/stream_executor/cuda/cuda_dnn.cc:455] **could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED**\r\n2019-12-17 09:55:46.558747: E tensorflow/stream_executor/cuda/cuda_dnn.cc:463] **possibly insufficient driver version: 384.81.0**\r\n2019-12-17 09:55:46.558864: F tensorflow/core/kernels/conv_ops.cc:713] **Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)** ", "**### nvidia drivers mismatch**\r\n\r\nmy nvidia driver is 384.90.\r\n\r\n**before**:  error which is same as the title of the thread.\r\ntensorflow-gpu 1.15.0 with cudatoolkit 10.0.130 + cudnn 7.6.5\r\n\r\n**after**: Worked \r\ntensorflow-gpu 1.12.0 with cudatoolkit 9.0\r\n\r\n**solution**:\r\nconda uninstall cudatoolkit (10.0.130) \r\nconda install tensorflow-gpu 1.12 cudatoolkit=9.0\r\n\r\n![image](https://user-images.githubusercontent.com/1111545/76212556-b5c2e100-6243-11ea-9db4-c6f7e7840633.png)\r\n\r\n\r\n\r\n", "This error also occurs if you create a symbolic link for any CUDA shared object file with a higher version to a shared object. with a lower version.\r\n\r\nFor example, for me this error was occurring because I had a symbolic link from `/usr/local/cuda-10.0/lib64/libcudart.so` pointing towards: `/usr/local/cuda/lib64/libcudart.so.10.1`, among other symlinks.\r\n\r\nWhen I removed just this symlink, the error vanished, but I noticed that there was no significant difference between the training times between GPU and CPU, despite the GPU process showing up in `nvidia-smi`, while the other one obviously didn't. They were exactly the same. Weird issue. "]}, {"number": 21831, "title": "Running Tensorflow example graph with TensorRT 4 backend fails", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.10\r\n- **Python version**: python 3.5\r\n- **Bazel version (if compiling from source)**: 1.6\r\n- **GCC/Compiler version (if compiling from source)**: 5\r\n- **CUDA/cuDNN version**: 7.0\r\n- **GPU model and memory**: 4GB Quadro M4000M\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am trying to follow the examples provided in tensorflow/contrib/tensorrt/test and the one provided by the Google blog post [here](https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html) which provides code on running a more complex ResNet architecture [here](https://developer.download.nvidia.com/devblogs/tftrt_sample.tar.xz).\r\n\r\nThe tests test_tftrt.py and tf_trt_integration_test.py are marked as passed. However, the code provided in the other link fails. In both situations I get information like this, which claims it found no eligible GPUs (?) I am definitely using the GPU with Tensorflow though, I can see this in nvidia-smi and via a simple log placement test (separately).\r\n\r\n```\r\n2018-08-23 17:20:22.067239: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-08-23 17:20:22.078438: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:756] MULTIPLE tensorrt candidate conversion: 2\r\n2018-08-23 17:20:22.078599: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-08-23 17:20:22.079079: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-08-23 17:20:22.079471: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n2018-08-23 17:20:22.186648: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n```\r\n\r\nThe output message of running the ResNet execution is the following. It complains that it's out of memory, even though I try to reduce the batch size to 1, reduce the image size to a tiny size.\r\n\r\nAm I missing something? The 4GB memory is not huge, but it's the same as a DrivePX2 discrete GPU...\r\n\r\n```\r\nfrancesco@franny:~/Downloads/tftrt$ ./run_all.sh \r\n/home/francesco/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\nNamespace(FP16=True, FP32=True, INT8=True, batch_size=1, dump_diff=False, native=True, num_loops=10, topN=5, update_graphdef=False, with_timeline=False, workspace_size=2048)\r\nStarting at 2018-08-23 17:26:13.644094\r\n2018-08-23 17:26:13.719781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-08-23 17:26:13.720536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\nname: Quadro M2000M major: 5 minor: 0 memoryClockRate(GHz): 1.137\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 3.69GiB\r\n2018-08-23 17:26:13.720568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-23 17:26:14.058416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-23 17:26:14.058443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-23 17:26:14.058451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-23 17:26:14.058669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nINFO:tensorflow:Starting execution\r\n2018-08-23 17:26:14.508197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-23 17:26:14.508248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-23 17:26:14.508254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-23 17:26:14.508258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-23 17:26:14.508394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nINFO:tensorflow:Starting Warmup cycle\r\n2018-08-23 17:26:17.058524: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.25GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\n2018-08-23 17:26:17.072068: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.26GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\r\nINFO:tensorflow:Warmup done. Starting real timing\r\niter  0   0.025112714767456055\r\niter  1   0.025174999237060548\r\niter  2   0.02513185977935791\r\niter  3   0.025080199241638183\r\niter  4   0.02514298439025879\r\niter  5   0.025306134223937987\r\niter  6   0.02516295909881592\r\niter  7   0.02516087532043457\r\niter  8   0.025412368774414062\r\niter  9   0.025518035888671874\r\nComparison= True\r\nINFO:tensorflow:Timing loop done!\r\nimages/s : 39.7 +/- 0.2, s/batch: 0.02522 +/- 0.00014\r\nRES, Native, 1, 39.65, 0.21, 0.02522, 0.00014\r\nINFO:tensorflow:Running against TensorRT version 4.0.1\r\n2018-08-23 17:26:30.386025: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-08-23 17:26:30.831639: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'resnet_v1_50/', converted to graph\r\n2018-08-23 17:26:30.929338: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n2018-08-23 17:26:41.310315: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.00GiB.  Current allocation summary follows.\r\n2018-08-23 17:26:41.310388: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310413: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310434: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310453: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310473: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310492: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310515: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310535: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310555: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310574: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310594: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310622: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 620.0KiB allocated for chunks. 620.0KiB in use in bin. 620.0KiB client-requested in use in bin.\r\n2018-08-23 17:26:41.310643: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310666: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152): \tTotal Chunks: 1, Chunks in use: 1. 3.06MiB allocated for chunks. 3.06MiB in use in bin. 3.06MiB client-requested in use in bin.\r\n2018-08-23 17:26:41.310698: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310730: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310762: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310794: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310827: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310860: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310887: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456): \tTotal Chunks: 1, Chunks in use: 0. 1.97GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-08-23 17:26:41.310910: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 2.00GiB was 256.00MiB, Chunk State: \r\n2018-08-23 17:26:41.310937: I tensorflow/core/common_runtime/bfc_allocator.cc:652]   Size: 1.97GiB | Requested Size: 3.9KiB | in_use: 0, prev:   Size: 3.06MiB | Requested Size: 3.06MiB | in_use: 1\r\n2018-08-23 17:26:41.310959: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb03140000 of size 634880\r\n2018-08-23 17:26:41.310975: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0xb031db000 of size 3211264\r\n2018-08-23 17:26:41.310991: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Free  at 0xb034eb000 of size 2114605056\r\n2018-08-23 17:26:41.311006: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: \r\n2018-08-23 17:26:41.311026: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 634880 totalling 620.0KiB\r\n2018-08-23 17:26:41.311044: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 3211264 totalling 3.06MiB\r\n2018-08-23 17:26:41.311060: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 3.67MiB\r\n2018-08-23 17:26:41.311083: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: \r\nLimit:                  2118451200\r\nInUse:                     3846144\r\nMaxInUse:               1317554688\r\nNumAllocs:                  119350\r\nMaxAllocSize:           1212416000\r\n\r\n2018-08-23 17:26:41.311112: W tensorflow/core/common_runtime/bfc_allocator.cc:279] *___________________________________________________________________________________________________\r\n2018-08-23 17:26:41.311161: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory\r\n2018-08-23 17:26:41.311420: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger GPU memory allocation failed during tactic selection for layer: resnet_v1_50/conv1/Conv2D + (Unnamed Layer* 2) [Activation]\r\n2018-08-23 17:26:41.319106: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (119) - OutOfMemory Error in GpuMemory\r\n2018-08-23 17:26:41.319785: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:857] Engine creation for segment 0, composed of 452 nodes failed: Internal: Failed to build TensorRT engine. Skipping...\r\nINFO:tensorflow:Starting execution\r\n2018-08-23 17:26:45.089652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-23 17:26:45.089705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-23 17:26:45.089714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-23 17:26:45.089721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-23 17:26:45.089863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nINFO:tensorflow:Starting Warmup cycle\r\n... etc\r\n```\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["An update. I tried to test a smaller network below and now the native, TRT float32 and TRT float16 conversions work. \r\n```\r\n\r\ninputs = tf.keras.layers.Input((720, 640, 3), batch_size=1, name=\"input\")\r\n\r\nx = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\r\nx = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.MaxPool2D(padding=\"same\")(x)\r\n\r\nx = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.MaxPool2D(padding=\"same\")(x)\r\n\r\nx = tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.MaxPool2D(padding=\"same\")(x)\r\n\r\nx = tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = tf.keras.layers.MaxPool2D(padding=\"same\")(x)\r\n\r\nx = tf.keras.layers.Conv2D(1, (3, 3), dilation_rate=(2, 2), activation=\"selu\")(x) # unsupported by TensorRT, example\r\n\r\nmodel = tf.keras.models.Model(inputs, x)\r\n\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\nfrom tensorflow.python.framework import dtypes\r\nfrom typing import List\r\n\r\ndef freeze_graph(session, \r\n                 output_nodes: List[str], \r\n                 save_path: str):\r\n    \"\"\"\r\n    :param input_nodes: list of strings of input nodes in session graph\r\n    :param output_nodes: list of strings of output nodes in session graph\r\n    :param save_path: output file path\r\n    :param optimize: boolean flag. Will try to run some optimizations, assuming I/O is float32.\r\n    :return:\r\n    \"\"\"\r\n\r\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n        session,                                     # The session is used to retrieve the weights\r\n        session.graph_def,                           # The graph_def is used to retrieve the nodes\r\n        output_nodes                                 # The output node names are used to select the useful nodes\r\n    )\r\n\r\n    with tf.gfile.GFile(save_path, \"wb\") as f:\r\n        f.write(output_graph_def.SerializeToString())\r\n    print(\"%d ops in the final graph.\" % len(output_graph_def.node))\r\n\r\nfreeze_graph(tf.keras.backend.get_session(),\r\n             [o.name[:-2] for o in model.outputs],\r\n             \"encoder.pb\")\r\n```\r\n\r\nHowever, I still cannot get the INT8 version working.\r\n\r\n```\r\n2018-08-24 09:10:12.619360: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0\r\n2018-08-24 09:10:12.649301: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope '', converted to graph\r\n2018-08-24 09:10:12.656973: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\nRunning Calibration\r\nINFO:tensorflow:Starting execution\r\n2018-08-24 09:10:28.382906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-24 09:10:28.382955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-24 09:10:28.382963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-24 09:10:28.382968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-24 09:10:28.384566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nINFO:tensorflow:Starting Warmup cycle\r\n2018-08-24 09:10:28.662523: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:567] Starting calibration thread on device 0, Calibration Resource @ 0x7f39d8084890\r\n2018-08-24 09:10:28.676649: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Int8 support requested on hardware without native Int8 support, performance will be negatively affected.\r\nINFO:tensorflow:Warmup done. Starting real timing\r\niter  0   0.35218316555023194\r\nComparison= True\r\nINFO:tensorflow:Timing loop done!\r\nCreating inference graph\r\n2018-08-24 09:10:54.272182: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:153] Starting Calib Conversion\r\n2018-08-24 09:10:54.274204: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:159] Construction of static int8 engine is not implemented yet!. Dynamic engine will be constructed\r\n2018-08-24 09:10:59.975683: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger cudnnFusedConvActLayer.cpp (61) - Cuda Error in createFilterTextureFused: 11\r\n2018-08-24 09:10:59.978356: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger cudnnFusedConvActLayer.cpp (61) - Cuda Error in createFilterTextureFused: 11\r\n2018-08-24 09:10:59.978479: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:588] Calibration failed: Internal: Failed to build TensorRT engine\r\nINFO:tensorflow:Starting execution\r\n2018-08-24 09:11:00.175254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n2018-08-24 09:11:00.175287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-08-24 09:11:00.175312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n2018-08-24 09:11:00.175317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n2018-08-24 09:11:00.175460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2020 MB memory) -> physical GPU (device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0)\r\nINFO:tensorflow:Starting Warmup cycle\r\n2018-08-24 09:11:00.435002: I tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:491] import/my_trt_op_0 Constructing a new engine with batch size 1\r\n2018-08-24 09:11:00.446641: W tensorflow/contrib/tensorrt/log/trt_logger.cc:34] DefaultLogger Int8 support requested on hardware without native Int8 support, performance will be negatively affected.\r\n2018-08-24 09:11:03.973151: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger cudnnFusedConvActLayer.cpp (61) - Cuda Error in createFilterTextureFused: 11\r\n2018-08-24 09:11:03.973476: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger cudnnFusedConvActLayer.cpp (61) - Cuda Error in createFilterTextureFused: 11\r\n2018-08-24 09:11:03.973638: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:505] Engine creation for batch size 1 failed Internal: Failed to build TensorRT engine\r\n2018-08-24 09:11:03.973666: W tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:283] Engine retrieval for batch size 1 failed Running native segment\r\nTraceback (most recent call last):\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InternalError: Engine creation failed!\r\n\t [[Node: import/my_trt_op_0 = TRTEngineOp[InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data=\"1\\nOutputP...3d8bc23d\\n\", fixed_input_size=true, input_shapes=[[1,3,720,640]], max_cached_engines_count=1, output_shapes=[[1,256,45,40]], precision_mode=\"INT8\", segment_funcdef_name=\"my_trt_op_0_native_segment\", serialized_segment=\"\\nD\\n\\tInp...01\\002\\002\", static_engine=false, workspace_size_bytes=1073741824, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](import/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tftrt_sample.py\", line 310, in <module>\r\n    f.num_loops,dummy_input,timelineName)\r\n  File \"tftrt_sample.py\", line 199, in timeGraph\r\n    valt = sess.run(outlist)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InternalError: Engine creation failed!\r\n\t [[Node: import/my_trt_op_0 = TRTEngineOp[InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data=\"1\\nOutputP...3d8bc23d\\n\", fixed_input_size=true, input_shapes=[[1,3,720,640]], max_cached_engines_count=1, output_shapes=[[1,256,45,40]], precision_mode=\"INT8\", segment_funcdef_name=\"my_trt_op_0_native_segment\", serialized_segment=\"\\nD\\n\\tInp...01\\002\\002\", static_engine=false, workspace_size_bytes=1073741824, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](import/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer)]]\r\n\r\nCaused by op 'import/my_trt_op_0', defined at:\r\n  File \"tftrt_sample.py\", line 310, in <module>\r\n    f.num_loops,dummy_input,timelineName)\r\n  File \"tftrt_sample.py\", line 159, in timeGraph\r\n    return_elements=[\"conv2d_10/mul_1\"]\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 442, in import_graph_def\r\n    _ProcessNewOps(graph)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/framework/importer.py\", line 234, in _ProcessNewOps\r\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3289, in _add_new_tf_operations\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3289, in <listcomp>\r\n    for c_op in c_api_util.new_tf_operations(self)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3180, in _create_op_from_tf_operation\r\n    ret = Operation(c_op, self)\r\n  File \"/home/francesco/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInternalError (see above for traceback): Engine creation failed!\r\n\t [[Node: import/my_trt_op_0 = TRTEngineOp[InT=[DT_FLOAT], OutT=[DT_FLOAT], cached_engine_batches=[1], calibration_data=\"1\\nOutputP...3d8bc23d\\n\", fixed_input_size=true, input_shapes=[[1,3,720,640]], max_cached_engines_count=1, output_shapes=[[1,256,45,40]], precision_mode=\"INT8\", segment_funcdef_name=\"my_trt_op_0_native_segment\", serialized_segment=\"\\nD\\n\\tInp...01\\002\\002\", static_engine=false, workspace_size_bytes=1073741824, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](import/conv2d/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer)]]\r\n```\r\n\r\n\r\n", "@fferroni did you manage to get INT8 working?"]}, {"number": 21829, "title": "//tensorflow/contrib/fused_conv:fused_conv2d_bias_activation_op_test  running into `implementation not found` error", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code**: No\r\n- **OS Platform and Distribution**:\r\n`tensorflow/tensorflow:1.9.0-devel-gpu-py3` docker container running on `Linux Ubuntu 16.04` host\r\n- **Mobile device**:  Not Applicable\r\n- **TensorFlow installed from (source or binary)**: cloned from repo\r\n- **TensorFlow version (use command below)**: `b'v1.9.0-0-g25c197e' 1.9.0`\r\n- **Python version**: `Python 3.5.2`\r\n- **Bazel version (if compiling from source)**: `0.15.0`\r\n- **GCC/Compiler version (if compiling from source)**: `5.4.0`\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**: Not Applicable\r\n- **Exact command to reproduce**: \r\n```\r\nbazel test --config=cuda  //tensorflow/contrib/fused_conv:fused_conv2d_bias_activation_op_test \r\n```\r\n\r\n### Describe the problem\r\n\r\nI am unable to run the `fused_conv` example from the contrib area. The error I run into is shown in the `Source code / logs` section below. The problem seems to be that \r\n- the code in kernel implementation for the `fused_conv2d_bias_activation_op` calls routines that are present in the `_pywrap_tensorflow_internal.so` library\r\n- the shared library created for the custom op `_fused_conv2d_bias_activation_op.so` does **not** have a dependency on the `_pywrap_tensorflow_internal.so` library\r\n\r\nI think one way to get past this error would be add the missing library dependency, but I do not know how to go about doing that. \r\n\r\n\r\n### Source code / logs\r\n\r\nerror from bazel command output\r\n\r\n```\r\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\r\nExecuting tests from //tensorflow/contrib/fused_conv:fused_conv2d_bias_activation_op_test\r\n-----------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/contrib/fused_conv/python/ops/fused_conv2d_bia\\\r\ns_activation_op_test.py\", line 23, in <module>\r\n   from tensorflow.contrib.fused_conv.python.ops import fused_conv2d_bias_activation_op\r\n File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/contrib/fused_conv/__init__.py\", line 22, in <\\\r\nmodule>\r\n   from tensorflow.contrib.fused_conv.python.ops.fused_conv2d_bias_activation_op import *\r\n File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/contrib/fused_conv/python/ops/fused_conv2d_bia\\\r\ns_activation_op.py\", line 26, in <module>\r\n   resource_loader.get_path_to_datafile(\"_fused_conv2d_bias_activation_op.so\"))\r\n File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/contrib/util/loader.py\", line 56, in load_op_l\\\r\nibrary\r\n   ret = load_library.load_op_library(path)\r\n File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow/python/framework/load_library.py\", line 56, in\\\r\nload_op_library\r\n   lib_handle = py_tf.TF_LoadLibrary(library_filename)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/fused_conv2d_bias_activation_op_test.runfiles/org_tensorflow/tensorflow\\\r\n/contrib/fused_conv/python/ops/_fused_conv2d_bias_activation_op.so: undefined symbol: _ZN10tensorflow7functor10NHWCToNCHWIN5Eigen9GpuDeviceEfLi4EEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi4ELi1ElEELi16ENS2_11MakePointerEEENS7_INS8_IfLi4ELi1ElEELi16ESB_EE\r\n```\r\n\r\nnote the lack of dependency on `_pywrap_tensorflow_internal.so`\r\n\r\n```\r\nroot@fptitan1:~/tensorflow# ldd ./bazel-tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/python/ops/_fused_conv2d_bias_activation_op.so\r\n\tlinux-vdso.so.1 =>  (0x00007ffebf7a9000)\r\n\tlibtensorflow_framework.so => /root/tensorflow/./bazel-tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/fused_conv/python/ops/../../../../../_solib_local/_U_S_Stensorflow_Scontrib_Sfused_Uconv_Cpython_Sops_S_Ufused_Uconv2d_Ubias_Uactivation_Uop.so___Utensorflow/libtensorflow_framework.so (0x00007f93a5e5e000)\r\n\tlibm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f93a5b55000)\r\n\tlibdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f93a5951000)\r\n\tlibpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f93a5734000)\r\n\tlibrt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f93a552c000)\r\n\tlibstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f93a51aa000)\r\n\tlibgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f93a4f94000)\r\n\tlibc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f93a4bca000)\r\n\tlibcublas.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcublas.so.9.0 (0x00007f93a12c5000)\r\n\tlibcuda.so.1 => /usr/local/nvidia/lib64/libcuda.so.1 (0x00007f93a0725000)\r\n\tlibcudnn.so.7 => /usr/lib/x86_64-linux-gnu/libcudnn.so.7 (0x00007f938c734000)\r\n\tlibcufft.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcufft.so.9.0 (0x00007f9384693000)\r\n\tlibcurand.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcurand.so.9.0 (0x00007f938072f000)\r\n\tlibcudart.so.9.0 => /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0 (0x00007f93804c2000)\r\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f93a70d5000)\r\n\tlibnvidia-fatbinaryloader.so.390.30 => /usr/local/nvidia/lib64/libnvidia-fatbinaryloader.so.390.30 (0x00007f9380276000)\r\n```\r\n", "comments": ["@robieta ping. would you mind share how tests under `//tensorflow/contrib` are tested?", "I have the same problem,:\r\n_ZN10tensorflow7functor10NHWCToNCHWIN5Eigen9GpuDeviceEfLi4EEclERKS3_NS2_9TensorMapINS2_6TensorIKfLi4ELi1ElEELi16ENS2_11MakePointerEEENS7_INS8_IfLi4ELi1ElEELi16ESB_EE\r\n\r\nwhen compile contrib/fused_conv, it depends conv_ops_gpu_hdrs(NHWCToNCHWIN is define here)\r\n`cc_library(\r\n    name = \"conv_ops_gpu_hdrs\",\r\n    hdrs = [\"conv_ops_gpu.h\"],\r\n)`\r\nbut it just a header, and the implements is in a .cu.cc file which is not include in this cc_library.\r\n\r\nso how to import the unresolved symbols?????", "As contrib is deprecated, Please google the name of the module without the tf.contrib part to know it's new location and thus migrating your code accordingly by correcting the import statement.\r\n\r\nyou may also refer to the below links:\r\n[link](https://stackoverflow.com/questions/55082483/why-can-i-not-import-tensorflow-contrib-i-get-an-error-of-no-module-named-tenso)\r\n[link1](https://www.tensorflow.org/guide/migrate)"]}, {"number": 21828, "title": "I make my own Network to train machine and data is used Mnist But it's give Low accuracy and somtimes my softmax code not working as well.. so please tell me what's wrong with my code.... ", "body": "**somtimes softmax give null value after updating weights so it's throw divison by zero error please check my code and tell me what's wrong with it.\r\nUsed: python 3.6.5**\r\n\r\n\r\nimport numpy as np\r\nimport gzip\r\nimport math\r\nIMAGE_SIZE = 28\r\n\r\n\r\ndef sigmoid(x):\r\n    # return (2 / (1 + np.exp(-2 * (x)))) - 1\r\n    return 1.0 / (1.0 + np.exp(-x))\r\n\r\n\r\ndef derivation_sigmoid(x):\r\n    # return 1 - (x ** 2)\r\n    return x * (1.0 - x)\r\n\r\n\r\ndef softmax(x):\r\n    # print np.sum(np.exp(x))\r\n    return (np.exp(x)) / (np.sum(np.exp(x)))\r\n\r\n\r\ndef extract_data(filename, num_images):\r\n  with gzip.open(filename) as bytestream:\r\n    bytestream.read(16)\r\n    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\r\n    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\r\n    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\r\n    return data\r\n\r\n\r\n\r\ndef extract_labels(filename, num_images):\r\n  with gzip.open(filename) as bytestream:\r\n    bytestream.read(8)\r\n    buf = bytestream.read(1 * num_images)\r\n    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\r\n  return labels\r\n\r\n\r\ndef derivation_cross(x, y):\r\n    return -1 * ((x * (1 / y)) + (1 - x) * (1 / (1 - y)))\r\n\r\ndef derivation_softmax(x):\r\n    tem = sum(np.exp(x)) ** 2\r\n    # print tem\r\n    tem1 = sum(np.exp(x))\r\n    a = []\r\n    for i_ in range(len(x)):\r\n        t1 = (np.exp(x[i_]) * (tem1 - np.exp(x[i_]))) / tem\r\n        a.append(t1)\r\n    return a\r\n\r\ntrain_data_filename = \"/home/rootpranav/Downloads/train-images-idx3-ubyte.gz\"\r\ntrain_data = extract_data(train_data_filename, 60000)\r\n\r\ntrain_data_label = \"/home/rootpranav/Downloads/train-labels-idx1-ubyte.gz\"\r\ntarin_data_label_1 = extract_labels(train_data_label, 6000)\r\n\r\ninput_array = []\r\ncounter = 0\r\n\r\n\r\ninput_neurons = train_data.shape[1] * train_data.shape[1]   # Number of Feature\r\nout_neurons = 10\r\nhidden_layer_nurons = 15\r\nlr = 0.1\r\n\r\nbias_hidden = np.ones((15, 1))\r\nbias_output = np.ones((10, 1))\r\n\r\nwih = np.ones((15,784))\r\nwho = np.ones((10, 15))\r\n\r\ndef neural_network(wih, who, train_data, tarin_data_label_1):\r\n    for e_ in range(0, 1000):\r\n        for da_ in range(900, 980):\r\n            \r\n            c_error = []\r\n            m1 = np.matrix((train_data[da_]))\r\n            m2 = np.reshape(m1, (784, 1))\r\n            m2 = ((m2 * 1) / 255)\r\n            \r\n            hidden_sigmoid = sigmoid((np.dot(wih, m2)))\r\n            output = (np.dot(who, hidden_sigmoid))\r\n            output_softmax = softmax((np.dot(who, hidden_sigmoid)))\r\n\r\n            index = tarin_data_label_1[da_]\r\n            for i in range(0, 10):\r\n                if i != index:\r\n                    c_error.append([0])\r\n                else:\r\n                    c_error.append([1])\r\n            \r\n            a1 = np.array(c_error)\r\n            a2 = np.array(output_softmax)\r\n\r\n            # This is for Hidden -> Output change weight\r\n            cross_entropy_derivation = derivation_cross(a1, a2)\r\n\r\n            # print cross_entropy_derivation\r\n            softmax_derivation = derivation_softmax(output)\r\n            sigmoid_derivation = derivation_softmax(hidden_sigmoid)\r\n            sigmoid_derivation = np.reshape(sigmoid_derivation, (15, 1))\r\n\r\n            m = np.reshape(softmax_derivation, (10, 1))\r\n            d_output = cross_entropy_derivation * m\r\n\r\n            who -= (d_output.dot(hidden_sigmoid.T))\r\n\r\n            # this is for Hidden -> Input\r\n\r\n            d_hidden = who.T.dot(d_output)\r\n            d1 =  d_hidden * sigmoid_derivation\r\n            wih -= d1.dot(m2.T)\r\n            # print who[0]\r\n            test = np.argmax(output_softmax)\r\n            print test, index\r\n\r\n\r\n\r\nneural_network(wih, who, train_data, tarin_data_label_1)", "comments": ["First I guess this should not be posted here. So you initialized the weights to ones like `wih = np.ones((15,784))`, which makes the network symmetric and hard to train. You should initialize the weights to small small random values. ", " Already Assigned same things check my code  @nkcr7 ", "My Code Link\r\n**https://github.com/Sunil1997/MNIST_Nural/blob/master/test/NN1.py**\r\n\r\nwhen i start my above code after **1-2 iteration**,my **softmax function** given **Nan** value to me so **derivation of softmax** give me **division by Zero Error**.How that i don't know. If @nkcr7 @karmel know then tell me what can i do??", "You are posting it at wrong platform. It would be better if you can post it on stackoverflow as this is related to tensorflow related issues. ", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 21827, "title": "add div_no_nan in the math section of user_guide", "body": "Add document for #21784 #21621 .", "comments": ["Thanks for the fix."]}, {"number": 21826, "title": "merge_repeated option is confusing", "body": "I have the same question with [WIP: Remove invalid merge_repeated option from CTC beam decoder](https://github.com/tensorflow/tensorflow/pull/15586), it's a pity I haven't seen any changes for so long.\r\nGenerally I will use the default value of merge_repeated: True, but I found it's confusing, that is, I got the wrong anser, it has been explained well in [WIP: Remove invalid merge_repeated option from CTC beam decoder](https://github.com/tensorflow/tensorflow/pull/15586).\r\nAnd the top path in ctc_beam_search_decoder is similar with sequence in ctc_greedy_decoder, this is confusing, I have found the project [CRNN](https://github.com/Belval/CRNN/blob/master/CRNN/crnn.py)(line 167) and some other projects use the wrong settings.\r\nSo I think it's better to give a explain here, this has no conflict with the existing code.", "comments": ["Nagging Assignee @akshaym: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks!  We'll remove that argument in tf2 and no merging will happen."]}]