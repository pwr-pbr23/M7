[{"number": 16475, "title": "Branch 183446593", "body": "", "comments": []}, {"number": 16474, "title": "MKL: Making MKL-DNN default", "body": "Make Tensroflow use MKL DNN by default if --config=mkl is used when building", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "@agramesh1 Thanks for the PR. Please sign the CLA.", "@agramesh1 Please rebase.", "@rmlarsen ok will rebase.", "CLAs look good, thanks!\n\n<!-- ok -->", "@rmlarsen rebase completed.", "@agramesh1 Thanks!", "@tensorflow-jenkins test this please", "@andydavis1 Thanks, for reviewing. I can sheperd this PR along from here.", "@rmlarsen thanks."]}, {"number": 16473, "title": "Fixing hard_sigmoid's documentation to match impl", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "@kernhanda please sign the CLA, and make sure to use the same email address as you use in your github commits.", "PR is wrong. The original comment is correct in that -2.5 and 2.5 are the values the original value of `x` need to be to trigger the relevant return values. "]}, {"number": 16472, "title": "Merge pull request #1 from tensorflow/master", "body": "merge upstream changes", "comments": []}, {"number": 16471, "title": "Simplify Android/Tegra GPU makefile file lists", "body": "", "comments": []}, {"number": 16470, "title": "Fix build error with GCC 7.2.1 on AWS Linux 2", "body": "This fix fixes a build failure when compiling with GCC 7.2.1 on AWS Linux 2:\r\n```\r\ngcc version 7.2.1 20170915 (Red Hat 7.2.1-2) (GCC)\r\n```\r\n\r\nThe eror output was:\r\n```\r\n...\r\n./tensorflow/contrib/lite/toco/model.h:1567:25: error: 'std::function' has not been declared\r\n   void EraseArrays(std::function<bool(const string&)> discardable) {\r\n  .....\r\n```\r\n\r\nThis fix is related to #16046.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 16469, "title": "Branch 183429339", "body": "", "comments": []}, {"number": 16468, "title": "Keras multi input and estimator", "body": "If I've interpreted it correctly seems that there is some strange behavior with Keras multi inputs and the estimator.\r\n\r\n- Why input layers are renamed with `_1` suffix?\r\n- Why TB display a `_2` suffixed parallel sub-graph?\r\n\r\nI've attached a snippet runnable on [colab](http://colab.research.google.com/) and the TB rendered image.\r\n\r\n \r\n```import tensorflow as tf\r\nfrom tensorflow import keras as ks\r\nimport numpy as np\r\nfrom IPython.display import clear_output, Image, display, HTML\r\n\r\ndef strip_consts(graph_def, max_const_size=32):\r\n    \"\"\"Strip large constant values from graph_def.\"\"\"\r\n    strip_def = tf.GraphDef()\r\n    for n0 in graph_def.node:\r\n        n = strip_def.node.add() \r\n        n.MergeFrom(n0)\r\n        if n.op == 'Const':\r\n            tensor = n.attr['value'].tensor\r\n            size = len(tensor.tensor_content)\r\n            if size > max_const_size:\r\n                tensor.tensor_content = \"<stripped %d bytes>\"%size\r\n    return strip_def\r\n\r\ndef show_graph(graph_def, max_const_size=32):\r\n    \"\"\"Visualize TensorFlow graph.\"\"\"\r\n    if hasattr(graph_def, 'as_graph_def'):\r\n        graph_def = graph_def.as_graph_def()\r\n    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\r\n    code = \"\"\"\r\n        <script src=\"//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js\"></script>\r\n        <script>\r\n          function load() {{\r\n            document.getElementById(\"{id}\").pbtxt = {data};\r\n          }}\r\n        </script>\r\n        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\r\n        <div style=\"height:600px\">\r\n          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\r\n        </div>\r\n    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\r\n\r\n    iframe = \"\"\"\r\n        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\r\n    \"\"\".format(code.replace('\"', '&quot;'))\r\n    display(HTML(iframe))\r\n\r\nclass ExampleHook(tf.train.SessionRunHook):\r\n    def __init__(self):\r\n        print('Starting the session.')\r\n        return\r\n\r\n    def begin(self):\r\n        g = tf.get_default_graph()\r\n        show_graph(g)\r\n        print('Starting the session.')\r\n        \r\n        #for op in tf.get_default_graph().get_operations():\r\n          #print(str(op.name) )\r\n        \r\nmy_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n    x={\"input_rgb\": np.array(np.random.rand(5,5,3).astype(np.float32)), \"input_gray\": np.array(np.random.rand(5,5,1).astype(np.float32)), \r\n       \"input_mix\": np.array(np.random.rand(5,5,1).astype(np.float32))},\r\n    y= np.array(np.random.rand(5,5,1)),\r\n      batch_size=1,\r\n      num_epochs=1,\r\n      shuffle=False)\r\n\r\ninput_rgb = ks.layers.Input(shape=(1,5, 5, 3), name=\"input_rgb\")\r\ninput_gray = ks.layers.Input(shape=(1,5, 5, 1), name=\"input_gray\")\r\ninput_mix = ks.layers.Input(shape=(1,5, 5, 1), name=\"input_mix\")\r\nrgb_gray = ks.layers.concatenate([input_rgb, input_gray, input_mix], name=\"rbg_gray\")\r\nx = ks.layers.Dense(1, activation='relu',name=\"Dense_1\")(rgb_gray)\r\nx = ks.layers.Dense(1, activation='softmax',name=\"softmax\")(x)\r\nmodel = ks.models.Model(\r\n        inputs=[input_rgb, input_gray, input_mix],\r\n        outputs=[x])\r\nmodel.compile(loss={ 'softmax': 'binary_crossentropy'},optimizer=tf.keras.optimizers.Adam())\r\n\r\n\r\nest = ks.estimator.model_to_estimator(\r\n            keras_model=model)\r\n\r\nmodel.summary()\r\nprint(model.input_names)\r\npred = list(est.predict(\r\n    input_fn=my_input_fn,\r\n    predict_keys=None,\r\n    hooks=[ExampleHook()],\r\n))\r\n```\r\n![tb](https://user-images.githubusercontent.com/1710528/35459923-5eca90b8-02e2-11e8-8248-764671141850.png)", "comments": ["/cc @fchollet ", "@yifeif would you have any thoughts on this?", "Seems to be related to how keras creates the graph? The _2 suffixed parallel sub-graph is presented without creating the estimator.", "@yifeif Do you have the testing code without the estimator?", "I tried `show_graph(tf.get_default_graph())` without creating an estimator.", "Ok but _1 inputs are created by the estimator.", "Yea, looks like they were added when clone_model was called during inference. @fchollet any idea?", "Can we target this before 1.6 will be finalized?", "Is this related to other kind of issues? See https://github.com/tensorflow/tensorflow/issues/14504#issuecomment-367986842", "Have you really enough bandwidth to support multiple high level apis in shape? Ref. https://github.com/tensorflow/tensorflow/issues/16182", "Any news?", "Are you still interested or can I close this?", "@ewilderj How we can improve as community to handle this cases? I really belive that we need to find a [Kubernetes like approach](https://github.com/kubernetes/kubernetes/milestone/38). I know that the majority of the dev flow is executed behind the scenes internally and published regularly in brust PR. But I think that we need to adopt an approach like k8s where you can label estimated milestone for github issues.", "P.s. Users ping and bot nagging assignee are quite ugly without a sprint planning outlook.", "@bhack I described our current plans around community recently at the Dev Summit, [here's my talk](https://www.youtube.com/watch?v=nH6owuKfCbw). I'll set aside the discussion about governance models for now, and see if I can chase down what's going on with this issue. Obviously this does matter to us, and we are always looking to see how we can be better.\r\n\r\n", "@ewilderj Yes I saw you talk but anything specific about having an outlook over the internal planning about external (dev/users) issues. \r\nI mention you here cause it is just an example where we are going over and over production releases without having a tentative outlook label/milestone. We have there repository full of these examples. \r\nSo notify you on this or on another one is quite the same, especially for bugs on API that are no more in contrib.\r\nOf course I want to start to use again this API but I think that the general problem is most important that a single bug and also if I am sure that the team is trying to do the best(effort) work we need to improve communication especially where all the the WIP is in an internal repository.", "I'm talking with the team here about if there's anything we can do to better flag what's going on externally. And I do appreciate you keeping bringing this to my attention. We are in a unique position as a project precisely because we do open source the same code we use internally, hence a natural predisposition to some internal tools and practices. There's a considerable effort made to bridge these gaps and still lots of places we can do better: you've identified an important one. \r\n\r\nIn general, rationalizing the high level API situation is something we are working on. We are going to shortly refresh the roadmap again (on a quarterly cadence now, vs the previous yearly) which may not be at the fine detail level you need but is a start. Recently we started sharing the release branching dates with the SIG Build group, and now in future we'll do it to the developers@ mailing list. ", "Also it seems that it impact `with tf.name_scope(\"branch\"):`. \r\nI.e. In the above example `with tf.name_scope(\"branch\"):`  will be ignored for Tensorboard grouping. Instead directly workaround every name with `name=branch/layername` is working for TB grouping.", "@yifeif  Can you explain what is the impact of this? Are _2 suffixed parallel sub-graph layers allocating memory?", "@fchollet If in these [two lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/estimator.py#L296-L298) you are cloning the model  who will remove the graph that @yifeif  visualize in https://github.com/tensorflow/tensorflow/issues/16468#issuecomment-361443964 without the model to estimator conversion?", "Also are _1 input suffixs generated by https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/estimator.py#L109?", "I've also tried the to visualize the official [Shared vision model](https://keras.io/getting-started/functional-api-guide/#more-examples).\r\n```\r\ndigit_input = ks.Input(shape=(27, 27, 1))\r\nx = ks.layers.Conv2D(64, (3, 3))(digit_input)\r\nx = ks.layers.Conv2D(64, (3, 3))(x)\r\nx = ks.layers.MaxPooling2D((2, 2))(x)\r\nout = ks.layers.Flatten()(x)\r\n\r\nvision_model = ks.Model(digit_input, out)\r\n\r\n# Then define the tell-digits-apart model\r\ndigit_a = ks.Input(shape=(27, 27, 1))\r\ndigit_b = ks.Input(shape=(27, 27, 1))\r\n\r\n# The vision model will be shared, weights and all\r\nout_a = vision_model(digit_a)\r\nout_b = vision_model(digit_b)\r\n\r\nconcatenated = ks.layers.concatenate([out_a, out_b])\r\nout = ks.layers.Dense(1, activation='sigmoid')(concatenated)\r\n\r\nclassification_model = ks.Model([digit_a, digit_b], out)\r\n```\r\nAnd seems that there is still an external branch\r\n![keras_model](https://user-images.githubusercontent.com/1710528/39473072-5572d272-4d4d-11e8-88a8-1bde5d9fabc5.png)\r\n\r\n", "The layer_2 suffix of the parallel subgraph in https://github.com/tensorflow/tensorflow/issues/16468#issue-292028082 seems solved with TF 1.8 but still has  _1 suffix for Inputs and the last example has still an external branch to the model.", "@fchollet Is there anything that I could do to improve this investigation?", "Gently ping", "@tanzhenyu As it seems that you are playing in the `model_to_estimator` league can you take a look at this old boy?", "Okay. Marked.\n\nOn Mon, Jun 25, 2018 at 4:50 PM bhack <notifications@github.com> wrote:\n\n> @tanzhenyu <https://github.com/tanzhenyu> As it seems that you are\n> playing in the model_to_estimator league can you take a look at this old\n> boy?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/16468#issuecomment-400130274>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AOhAwbssX8mzXdWlyuAK6_C_e4SQC-wSks5uAXdLgaJpZM4Ru3NQ>\n> .\n>\n\n\n-- \n-Zhenyu\n", "@yifeif probably there is something in common with https://github.com/tensorflow/tensorflow/issues/9545 but I'am not sure that is a TB only bug cause the profiler give me duplicated memory consumption. What do you think?", "This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF, please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=16468\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=16468\">No</a>\n"]}, {"number": 16467, "title": "Fix a bug in PR #15906", "body": "@drpngx @rmlarsen @elgehelge", "comments": []}, {"number": 16466, "title": "[Feature request] Adding a PR curves to canned estimators for (binary) classifiers", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nThis is a feature request, and I'd be happy to **contribute** if you think it's a valuable addition. \r\nI have been using estimators both pre-made and custom for classification tasks. I like that sharing the use of a `head`  as defined in [tensorflow/python/estimator/canned/head.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/head.py) allows both the canned models and the custom ones to share prediction and evaluation metrics for comparison, however currently it feels that some key metrics are missing, mainly PR curves which are fully supported by tensorboard.  Currently, the `head` constructor allows a list of thresholds, although they are not used by default. The problem is that when used, it creates scalar summaries for the precision and recall at each threshold, which is not that useful as in general one wants to compare how different models compare precision wise while fixing recall and the other way round.\r\n\r\nAdding the PR summary op from [here](https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/pr_curve) would make the eval metrics more informative IMO.\r\n\r\nThanks for taking the time to read this!\r\n\r\n### Source code / logs\r\nAny code that uses pre-made estimator classifiers relies on the same head. [This](https://github.com/tensorflow/models/blob/master/official/wide_deep/wide_deep.py) is one example.", "comments": ["@eisenjulian In general, contributions are welcome. Thank you for your proposal.\r\n\r\n@roumposg - Do you think this will be a useful feature?", "Thanks for the idea. This is useful, but one problem I see is that it introduces a dependency of tensorflow on tensorboard, which we do not allow. One solution would be to port the plugin into tensorflow/contrib/tensorboard/plugins so we need the opinion of the tensorboard people. If this works and it does not break existing pipelines, it would be a good contribution.", "Makes sense, I imagined the added dependency would be an issue. Do I need to file a separate issue there to ask about the porting of the plugin? Or what's the best way to get their input?", "I am not sure. @tatianashp what is the way to proceed?", "Should I just submit a PR? Or bringing back the plugin code to `contrib.tensorboard`  is not a good idea? @tatianashp ", "Let's get the opinion of tensorboard people first. @jart Any thoughts?", "Hi @chihuahua, what do you think of the suggestion?", "That's a lot of nags...any news? I'd really love to see this.", "The comments above show us the real face of Google + open source. ", "@eisenjulian We see that you're using TF v1.x which is out of support window ,Please upgrade to TF v2.4 and later versions.Could you please refer to [migration guide](https://tensorflow.org/guide/migrate) and [canned_estimators](https://www.tensorflow.org/lattice/tutorials/canned_estimators) where Estimators are not recommended for new code. Estimators run v1. Session-style code which is more difficult to write correctly, and can behave unexpectedly, especially when combined with TF 2 code. Estimators do fall under our compatibility guarantees, but will receive no fixes other than security vulnerabilities.\r\n\r\nClosing this issue as a stale , please feel free to reopen if you still have a concern.\r\nThank you!"]}, {"number": 16465, "title": "bug with frame_step in tf.contrib.signal overlap_and_add inverse_stft", "body": "### System information\r\n- Based on example \r\n- Linux Ubuntu 16.04\r\n- installed from binary\r\n- v1.4.0-19-ga52c8d9, 1.4.1; also 1.5.0\r\n- Python 2.7.14 |Anaconda custom (64-bit)| (default, Oct 16 2017, 17:29:19). IPython 5.4.1\r\n- Cuda release 8.0, V8.0.61, cuDNN 6; also Cuda release 9.0, V9.0.176, cuDNN 7.0.5\r\n- Geforce GTX 970M, also GTX 1070, Driver Version: 384.111\r\n\r\n### Describe the problem\r\n\r\nA.) When I create frames from a signal with `frame_length=1024` and  `frame_step=256` (i.e. 25% hop size, 75% overlap) using a hann window (also tried hamming), and then I reconstruct with `overlap_and_add`, I'd expect the signal to be reconstructed correctly (because of COLA etc). But instead it comes out exactly double the amplitude. I need to divide the resulting signal by two for it to be correct. \r\n\r\nB.) If I use STFT to create a series of overlapping spectrograms, and then reconstruct with inverse STFT, again with `frame_length=1024` and `frame_step=256`, the signal is again reconstructed at double amplitude. \r\n\r\nI realise why these might be the case (unity gain at 50% overlap for hann, so 75% overlap will double the signal). But is it not normal for the reconstruction function to take this into account? E.g. librosa istft does return signal with correct amplitude while tensorflow returns double.\r\n\r\nC.) \r\nAt any other frame_step there is severe amplitude modulation going on. See images below. This doesn't seem right at all. \r\n\r\n**UPDATE**: If I explicitly set `window_fn=tf.contrib.signal.inverse_stft_window_fn(frame_step)` in `inverse_stft` the output is correct. So it seems the `frame_step` parameter in `inverse_stft` is not being passed into the window function (which is also what the results hint at).\r\n\r\n### Source code / logs\r\n\r\noriginal data:\r\n![22050 orig](https://user-images.githubusercontent.com/144230/35579363-64e51ef6-05de-11e8-8dc2-f4220265c2f8.png)\r\n\r\ntensorflow output from frames + overlap_and_add:\r\n![tensorflow 22050 frame l1024 s256](https://user-images.githubusercontent.com/144230/35579392-7907d798-05de-11e8-8971-64597d3e06d3.png)\r\n![tensorflow 22050 frame l1024 s512](https://user-images.githubusercontent.com/144230/35579393-792fb060-05de-11e8-93af-efc2bd30d058.png)\r\n![tensorflow 22050 frame l1024 s768](https://user-images.githubusercontent.com/144230/35579394-794b9898-05de-11e8-82e7-ecbb41feed9a.png)\r\n![tensorflow 22050 frame l1024 s1024](https://user-images.githubusercontent.com/144230/35579397-7a3116fc-05de-11e8-815d-d4df17e86dc5.png)\r\n\r\ntensorflow output from stft+istft:\r\n![tensorflow 22050 stft l1024 s256](https://user-images.githubusercontent.com/144230/35579401-7ed0727a-05de-11e8-9cf2-e8dd06df9e05.png)\r\n![tensorflow 22050 stft l1024 s512](https://user-images.githubusercontent.com/144230/35579402-7ee9d29c-05de-11e8-93e0-a6d676c7d5ae.png)\r\n![tensorflow 22050 stft l1024 s768](https://user-images.githubusercontent.com/144230/35579403-7f01f818-05de-11e8-8779-e50b824b8aec.png)\r\n![tensorflow 22050 stft l1024 s1024](https://user-images.githubusercontent.com/144230/35579404-7f272a34-05de-11e8-9b55-a41942db7eb1.png)\r\n\r\nlibrosa output from stft+istft:\r\n![librosa 22050 stft l1024 s256](https://user-images.githubusercontent.com/144230/35579408-834faee2-05de-11e8-8f44-e6ef0e797fb4.png)\r\n![librosa 22050 stft l1024 s512](https://user-images.githubusercontent.com/144230/35579409-8385fa6a-05de-11e8-8d36-b96c35ba862a.png)\r\n![librosa 22050 stft l1024 s768](https://user-images.githubusercontent.com/144230/35579410-83a5767e-05de-11e8-89bd-6d01c9a7cb8c.png)\r\n![librosa 22050 stft l1024 s1024](https://user-images.githubusercontent.com/144230/35579411-83c791c8-05de-11e8-8b6c-7ef5508a66e5.png)\r\n\r\ntensorflow code:\r\n\r\n    from __future__ import print_function\r\n    from __future__ import division\r\n    \r\n    import numpy as np\r\n    import scipy.io.wavfile\r\n    import math\r\n    import random\r\n    import matplotlib.pyplot as plt\r\n    \r\n    import tensorflow as tf\r\n    out_prefix = 'tensorflow'\r\n    \r\n    \r\n    def plot(data, title, do_save=True):\r\n        plt.figure(figsize=(20,5))\r\n        plt.plot(data[:3*frame_length])\r\n        plt.ylim([-1, 1])\r\n        plt.title(title)\r\n        plt.grid()\r\n        if do_save: plt.savefig(title + '.png')\r\n        plt.show()\r\n    \r\n    \r\n    def reconstruct_from_frames(x, frame_length, frame_step):\r\n        name = 'frame'\r\n        frames_T = tf.contrib.signal.frame(x, frame_length=frame_length, frame_step=frame_step)\r\n        windowed_frames_T = frames_T * tf.contrib.signal.hann_window(frame_length, periodic=True)\r\n        output_T = tf.contrib.signal.overlap_and_add(windowed_frames_T, frame_step=frame_step)\r\n        return name, output_T\r\n    \r\n    \r\n    def reconstruct_from_stft(x, frame_length, frame_step):\r\n        name = 'stft'\r\n        spectrograms_T = tf.contrib.signal.stft(x, frame_length, frame_step)\r\n        output_T = tf.contrib.signal.inverse_stft(spectrograms_T, frame_length, frame_step)\r\n        return name, output_T\r\n    \r\n    \r\n    def test(fn, input_data):\r\n        print('-'*80)\r\n        tf.reset_default_graph()\r\n        input_T = tf.placeholder(tf.float32, [None]) \r\n        name, output_T = fn(input_T, frame_length, frame_step)\r\n    \r\n        title = \"{}.{}.{}.l{}.s{}\".format(out_prefix, sample_rate, name, frame_length, frame_step)\r\n        print(title)\r\n    \r\n        with tf.Session():\r\n            output_data =  output_T.eval({input_T:input_data})\r\n    \r\n    #    output_data /= frame_length/frame_step/2 # tensorflow needs this to normalise amp\r\n        plot(output_data, title)\r\n        scipy.io.wavfile.write(title+'.wav', sample_rate, output_data)\r\n    \r\n    \r\n    def generate_data(duration_secs, sample_rate, num_sin, min_freq=10, max_freq=500, rnd_seed=0, max_val=0):\r\n        '''generate signal from multiple random sin waves'''\r\n        if rnd_seed>0: random.seed(rnd_seed)\r\n        data = np.zeros([duration_secs*sample_rate], np.float32)\r\n        for i in range(num_sin):\r\n            w = np.float32(np.sin(np.linspace(0, math.pi*2*random.randrange(min_freq, max_freq), num=duration_secs*sample_rate)))\r\n            data += random.random() * w\r\n        if max_val>0:\r\n            data *= max_val / np.max(np.abs(data))\r\n        return data\r\n        \r\n    \r\n    frame_length = 1024\r\n    sample_rate = 22050\r\n    \r\n    input_data = generate_data(duration_secs=1, sample_rate=sample_rate, num_sin=1, rnd_seed=2, max_val=0.5)\r\n    \r\n    title = \"{}.orig\".format(sample_rate)\r\n    plot(input_data, title)\r\n    scipy.io.wavfile.write(title+'.wav', sample_rate, input_data)\r\n    \r\n    for frame_step in [256, 512, 768, 1024]:\r\n        test(reconstruct_from_frames, input_data)\r\n        test(reconstruct_from_stft, input_data)\r\n    \r\n    print('done.')\r\n\r\nlibrosa code:\r\n\r\n    from __future__ import print_function\r\n    from __future__ import division\r\n    \r\n    import numpy as np\r\n    import scipy.io.wavfile\r\n    import math\r\n    import random\r\n    import matplotlib.pyplot as plt\r\n    \r\n    import librosa.core as lc\r\n    out_prefix = 'librosa'\r\n    \r\n    \r\n    def plot(data, title, do_save=True):\r\n        plt.figure(figsize=(20,5))\r\n        plt.plot(data[:3*frame_length])\r\n        plt.ylim([-1, 1])\r\n        plt.title(title)\r\n        plt.grid()\r\n        if do_save: plt.savefig(title + '.png')\r\n        plt.show()\r\n    \r\n    \r\n    def reconstruct_from_stft(x, frame_length, frame_step):\r\n        name = 'stft'\r\n        stft = lc.stft(x, n_fft=frame_length, hop_length=frame_step)\r\n        istft = lc.istft(stft, frame_step)\r\n        return name, istft\r\n    \r\n    \r\n    def test(fn, input_data):\r\n        print('-'*80)\r\n        name, output_data = fn(input_data, frame_length, frame_step)\r\n    \r\n        title = \"{}.{}.{}.l{}.s{}\".format(out_prefix, sample_rate, name, frame_length, frame_step)\r\n        print(title)\r\n    \r\n    #    output_data /= frame_length/frame_step/2 # tensorflow needs this to normalise amp\r\n        plot(output_data, title)\r\n        scipy.io.wavfile.write(title+'.wav', sample_rate, output_data)\r\n    \r\n    \r\n    def generate_data(duration_secs, sample_rate, num_sin, min_freq=10, max_freq=500, rnd_seed=0, max_val=0):\r\n        '''generate signal from multiple random sin waves'''\r\n        if rnd_seed>0: random.seed(rnd_seed)\r\n        data = np.zeros([duration_secs*sample_rate], np.float32)\r\n        for i in range(num_sin):\r\n            w = np.float32(np.sin(np.linspace(0, math.pi*2*random.randrange(min_freq, max_freq), num=duration_secs*sample_rate)))\r\n            data += random.random() * w\r\n        if max_val>0:\r\n            data *= max_val / np.max(np.abs(data))\r\n        return data\r\n        \r\n    \r\n    frame_length = 1024\r\n    sample_rate = 22050\r\n    \r\n    input_data = generate_data(duration_secs=1, sample_rate=sample_rate, num_sin=1, rnd_seed=2, max_val=0.5)\r\n    \r\n    title = \"{}.orig\".format(sample_rate)\r\n    plot(input_data, title)\r\n    scipy.io.wavfile.write(title+'.wav', sample_rate, input_data)\r\n    \r\n    for frame_step in [256, 512, 768, 1024]:\r\n        test(reconstruct_from_stft, input_data)\r\n    \r\n    print('done.')\r\n", "comments": ["/CC @rryan, can you take a look?", "Thanks very much for the detailed bug report @memo! I'll take a look, though I probably won't have time until 2/9.", "I am having a (probably) related problem when I try to use the istft to reconstruct a signal.\r\n\r\n![figure_1](https://user-images.githubusercontent.com/9143109/36382321-c9862ed6-1588-11e8-9687-ddb65640eef3.png)\r\n\r\n\r\n```python\r\nimport functools\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.signal.python.ops import window_ops\r\n\r\nsampling_rate = 44000\r\nfreq = 440\r\ncountOfCycles = 4\r\n_time = tf.range(0, 1024 / sampling_rate, 1 / sampling_rate, dtype=tf.float32)\r\nfirstSignal = tf.sin(2 * 3.14159 * freq * _time)\r\n\r\nwith tf.name_scope('Energy_Spectogram'):\r\n    fft_frame_length = 128\r\n    fft_frame_step = 32\r\n    window_fn = functools.partial(window_ops.hann_window, periodic=True)\r\n    stft = tf.contrib.signal.stft(signals=firstSignal, frame_length=fft_frame_length, frame_step=fft_frame_step,\r\n                                  window_fn=window_fn)\r\n    istft = tf.contrib.signal.inverse_stft(stfts=stft, frame_length=fft_frame_length, frame_step=fft_frame_step,\r\n    window_fn=tf.contrib.signal.inverse_stft_window_fn(fft_frame_step,\r\n                                           forward_window_fn=window_fn))\r\n\r\nwith tf.Session() as sess:\r\n    original, reconstructed = sess.run([firstSignal, istft])\r\n\r\nimport matplotlib.pyplot as plt\r\n\r\nplt.plot(original)\r\nplt.plot(reconstructed)\r\nplt.show()\r\n```\r\n\r\nNote that the problem is worse when you don't explicitly give the window (which in this case is the default one). Giving amplitude modulation all across the signal.", "For anyone else still having problems even when manually passing in the inverse window function to `inverse_stft`:\r\n\r\nI still had problems reconstructing the original signal, but I managed to fix the issue by manually zero-padding the signal by `frame_length - frame_step` on both sides. After taking the inverse transform, the zero-padded signal is reconstructed perfectly.\r\n```\r\n>>> signal = tf.constant(0.5 * np.sin(np.linspace(0., 440*2*np.pi, 16000)), dtype=tf.float32)\r\n>>> frame_length = 400\r\n>>> frame_step = 100\r\n>>> fft_length = 512\r\n>>> pad = frame_length - frame_step\r\n>>> stft = tf.contrib.signal.stft(tf.pad(signal, [[pad, pad]]), frame_length, frame_step, fft_length)\r\n>>> reconstructed = tf.contrib.signal.inverse_stft(\r\n...   stft, frame_length, frame_step, fft_length, window_fn=tf.contrib.signal.inverse_stft_window_fn(frame_step))\r\n>>> error = (reconstructed[pad:-pad] - signal).eval()\r\n>>> np.max(np.abs(error))\r\n2.0861626e-06\r\n```\r\n", "I've also noticed that this is mostly a border effect. Zero padding fixes it to some extent, but it's not optimal for my application. \r\nI'm also concerned about where does the problem comes from. Are either of the implementations for the stft or the istft reliable? ", "@rryan How is this going?", "Thanks a ton @nuchi, @memo, and @andimarafioti for your patience and helpful repro code. As you've summarized nicely, this is caused by at least two issues:\r\n\r\n* By default, `tf.contrib.signal.inverse_stft` does not assume that the input STFT was generated from `tf.contrib.signal.stft`, and therefore does not divide the window by the squared sum of its magnitude as librosa [does by default](https://github.com/librosa/librosa/blob/0dcd53f462db124ed3f54edf2334f28738d2ecc6/librosa/core/spectrum.py#L302-L311). To get this behavior, pass `window_fn=tf.contrib.signal.inverse_stft_window_fn(frame_step)`, which is designed to compute a reconstruction window given the window and `frame_step` used in a forward STFT.\r\n* `tf.contrib.signal.stft` does not center the framed windows as librosa does by default with `center=True`. This option doesn't exist in `tf.contrib.signal` yet (#15134) but it's simple to work around, since you just reflect-pad the input to `stft` and slice the result of `inverse_stft`as librosa does [here](https://github.com/librosa/librosa/blob/0dcd53f462db124ed3f54edf2334f28738d2ecc6/librosa/core/spectrum.py#L162-L164) and [here](https://github.com/librosa/librosa/blob/0dcd53f462db124ed3f54edf2334f28738d2ecc6/librosa/core/spectrum.py#L314-L317).\r\n\r\nHere is a replacement for `reconstruct_from_stft` that works with @memo's test case:\r\n```python\r\ndef reconstruct_from_stft(x, frame_length, frame_step):\r\n    name = 'stft'\r\n    center = True\r\n    if center:\r\n        # librosa pads by frame_length, which almost works perfectly here, except for with frame_step 256.\r\n        pad_amount = 2 * (frame_length - frame_step)\r\n        x = tf.pad(x, [[pad_amount // 2, pad_amount // 2]], 'REFLECT')\r\n    \r\n    f = tf.contrib.signal.frame(x, frame_length, frame_step, pad_end=False)\r\n    w = tf.contrib.signal.hann_window(frame_length, periodic=True)\r\n    spectrograms_T = tf.spectral.rfft(f * w, fft_length=[frame_length])\r\n        \r\n    output_T = tf.contrib.signal.inverse_stft(\r\n        spectrograms_T, frame_length, frame_step,\r\n        window_fn=tf.contrib.signal.inverse_stft_window_fn(frame_step))\r\n    if center and pad_amount > 0:\r\n        output_T = output_T[pad_amount // 2:-pad_amount // 2]\r\n    return name, output_T\r\n```\r\nHere is a [Colab notebook](https://colab.research.google.com/drive/1zrugaL6wWudcLMHvNQ8OL2lRZJIIeRlB#scrollTo=NL1IBl7JyZOy) demonstrating.", "@rryan \r\nThank you for your code!\r\nAnd I have a question.\r\n\r\nHow can I use my own wave file?\r\n\r\nI mean when I use my own wave file like...\r\n```\r\nsample_rate, input_data = wavfile.read('my-wave.wav')\r\n```\r\ninstead of input_data = generate_data(duration_secs=1, sample_rate=sample_rate, num_sin=1, rnd_seed=2, max_val=0.5)\r\n\r\nFiles like tensorflow.16000.stft.1024.wav, tensorflow.16000.stft.256.wav,tensorflow.16000.stft.768.wav,tensorflow.16000.stft.512.wav have significant noises.\r\n\r\nHow can I apply reconstruct_from_stft function for my own wave file?\r\n\r\nI want to try to train end-to-end noise reduction model like below.\r\n**wave file > input ( stft data ) > NN > output data ( stft data ) > wave file.**\r\n\r\nSince I'm new to DSP, probably I miss some basic things...\r\n\r\nMy wave file's sample_rate is 16000 and 10 seconds length.\r\n\r\nThanks in advance.\r\n\r\n\r\n\r\n", "@kouohhashi, what shape and type is `input_data`? If it's a `[samples]` or `[channels, samples]` ndarray with type float32 (scaled to the range `[-1, 1]`) then it should work with the example.", "@rryan \r\nThank you for responding me.\r\n\r\ninput_data was like: [    0.     0.     0. ... -5206. -4761. -3248.].\r\nSo I made it [-1, 1] range by dividing by 32768.0 because 32768.0 was the biggest.\r\nBut result was the same....\r\n\r\nBTW, I noticed one thing.\r\n**Bits per sample** seems to be changes.\r\n\r\nOriginal file:\r\nSample rate: 16 kHz\r\nBits per sample: 16\r\n\r\nNew file:\r\nSample rate: 16 kHz\r\nBits per sample: 32\r\n\r\nIs \"Bits per sample change\" the cause of problem?\r\n\r\nThanks,\r\n", "> Thanks a ton @nuchi, @memo, and @andimarafioti for your patience and helpful repro code. As you've summarized nicely, this is caused by at least two issues:\r\n> \r\n>     * By default, `tf.contrib.signal.inverse_stft` does not assume that the input STFT was generated from `tf.contrib.signal.stft`, and therefore does not divide the window by the squared sum of its magnitude as librosa [does by default](https://github.com/librosa/librosa/blob/0dcd53f462db124ed3f54edf2334f28738d2ecc6/librosa/core/spectrum.py#L302-L311). To get this behavior, pass `window_fn=tf.contrib.signal.inverse_stft_window_fn(frame_step)`, which is designed to compute a reconstruction window given the window and `frame_step` used in a forward STFT.\r\n> \r\n>     * `tf.contrib.signal.stft` does not center the framed windows as librosa does by default with `center=True`. This option doesn't exist in `tf.contrib.signal` yet (#15134) but it's simple to work around, since you just reflect-pad the input to `stft` and slice the result of `inverse_stft`as librosa does [here](https://github.com/librosa/librosa/blob/0dcd53f462db124ed3f54edf2334f28738d2ecc6/librosa/core/spectrum.py#L162-L164) and [here](https://github.com/librosa/librosa/blob/0dcd53f462db124ed3f54edf2334f28738d2ecc6/librosa/core/spectrum.py#L314-L317).\r\n> \r\n> \r\n> Here is a replacement for `reconstruct_from_stft` that works with @memo's test case:\r\n> \r\n> ```python\r\n> def reconstruct_from_stft(x, frame_length, frame_step):\r\n>     name = 'stft'\r\n>     center = True\r\n>     if center:\r\n>         # librosa pads by frame_length, which almost works perfectly here, except for with frame_step 256.\r\n>         pad_amount = 2 * (frame_length - frame_step)\r\n>         x = tf.pad(x, [[pad_amount // 2, pad_amount // 2]], 'REFLECT')\r\n>     \r\n>     f = tf.contrib.signal.frame(x, frame_length, frame_step, pad_end=False)\r\n>     w = tf.contrib.signal.hann_window(frame_length, periodic=True)\r\n>     spectrograms_T = tf.spectral.rfft(f * w, fft_length=[frame_length])\r\n>         \r\n>     output_T = tf.contrib.signal.inverse_stft(\r\n>         spectrograms_T, frame_length, frame_step,\r\n>         window_fn=tf.contrib.signal.inverse_stft_window_fn(frame_step))\r\n>     if center and pad_amount > 0:\r\n>         output_T = output_T[pad_amount // 2:-pad_amount // 2]\r\n>     return name, output_T\r\n> ```\r\n> \r\n> Here is a [Colab notebook](https://colab.research.google.com/drive/1zrugaL6wWudcLMHvNQ8OL2lRZJIIeRlB#scrollTo=NL1IBl7JyZOy) demonstrating.\r\n\r\nThank you @rryan , that worked well. Just wanted to say that for me what gave results the same as librosa is:\r\n`pad_amount = 2 * (frame_length - (frame_step * 2))`\r\n\r\nUsed:\r\n- librosa==0.6.3\r\n- tensorflow==2.1.0", "@memo It seems you are using older versions(1.x versions) of Tensorflow which is not actively supported. Since `contrib` has been depreciated in Tensorflow 2.x ,Please do upgrade to a latest Tensorflow version.Attaching [migration](https://www.tensorflow.org/guide/migrate) guide for reference. Thanks!", "@sushreebarsa yes, this issue is from 4 years ago! :) I'm not sure if it is still relevant. ", "@memo Thank you for your response!\r\nAs TF v1.x is not actively supported we recommend  to upgrade to 2.4 or later versions.If you face any issues after rewriting the code in TF v2, please raise a new ticket.\r\nClosing this issue for now ,please feel free to reopen the issue if you have any concern ?\r\nThanks! "]}, {"number": 16464, "title": "AssignAddVariableOp has no output", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0-rc1\r\n- **Python version**: NA (Using Go bindings)\r\n- **Bazel version (if compiling from source)**: 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.1\r\n- **CUDA/cuDNN version**: 9.1 / 7.0\r\n- **GPU model and memory**: GTX 1060 6GB\r\n- **Exact command to reproduce**: See below\r\n\r\n\r\n### Describe the problem\r\nAccording to the docs, AssignAddVariableOp \"Outputs the incremented value, which can be used to totally order the increments to this variable.\". Without this feature, I get non deterministic behavior when reading the value of the variable at the same time as I update it. However, at least in the Go bindings, it returns an operation which has no outputs. I can work around this problem by using two calls to `sess.Run()`, but this is inelegant.\r\n\r\n### Source code / logs\r\n```\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\ttf \"github.com/tensorflow/tensorflow/tensorflow/go\"\r\n\t\"github.com/tensorflow/tensorflow/tensorflow/go/op\"\r\n)\r\n\r\nfunc main() {\r\n\ts := op.NewScope()\r\n\tvalue1 := op.Const(s.SubScope(\"zero\"), float32(0))\r\n\tvalue2 := op.Const(s, float32(3.1415))\r\n\thandle := op.VarHandleOp(s, tf.Float, tf.ScalarShape())\r\n\tinit := op.AssignVariableOp(s, handle, value1)\r\n\tupdate := op.AssignAddVariableOp(s, handle, value2)\r\n\tfmt.Println(\"NumOutputs:\", update.NumOutputs())\r\n\tgraph, err := s.Finalize()\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tsess, err := tf.NewSession(graph, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_, err = sess.Run(nil, nil, []*tf.Operation{init})\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\t_, err = sess.Run(nil, []tf.Output{update.Output(0)}, nil)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n}\r\n```\r\n```\r\n$ go run assign_demo.go \r\nNumOutputs: 0\r\npanic: Tried to fetch data for 'AssignAddVariableOp:0', which produces no output.  To run to a node but not fetch any data, pass 'AssignAddVariableOp:0' as an argument to the 'target_node_names' argument of the Session::Run API.\r\n\r\ngoroutine 1 [running]:\r\nmain.main()\r\n\t/home/isaac/go/src/github.com/is8ac/gotf/assign_demo.go:32 +0x448\r\nexit status 2\r\n```", "comments": ["@asimshankar  Could you take a look at this Go question?", "@is8ac : Thanks for the report and sorry for the confusion. That documentation is incorrect, the `AssignAddVariableOp` does not return a tensor, as you surmised. I've sent out a change to fix the documentation which should sync to GitHub in the next day or two.\r\n\r\nThe correct way to achieve ordering is by adding control dependencies. Unfortunately, the Go API doesn't yet support control dependencies but it is trivial to add. I'll take a stab at that in the next day or two, or mark this as \"Contributions Welcome\" if I don't get to it.\r\n\r\nThanks!", "Thank you for clarifying.\r\nI look forward to being able to use control dependencies.", "Fix submitted internally, should be synced to github in the next day or so. Thanks!"]}, {"number": 16463, "title": "Improve profiler error message when graph_path is not available.", "body": "This fix tries to address the issue raised in #16451 to provide a better error message when graph_path is not available for profiler.\r\n\r\nPreviously if graph_path is not available, the process will crash\r\nwith not very imformative message and a core dump:\r\n```\r\n2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory\r\nAborted (core dumped)\r\n```\r\n\r\nWith this fix, the error message is improved to:\r\n```\r\nFailed to read graph_path: Invalid argument: Cannot parse proto file.\r\n```\r\nand the process exit with 1.\r\n\r\nThis fix fixes #16451.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang Thanks for the cleanup!"]}, {"number": 16462, "title": "How to create a model checkpoint based on each step rather than time interval.? using TensorFlow-Slim api.", "body": "slim.learning.train(\r\n    train_op,\r\n    logdir,\r\n    number_of_steps=1000,\r\n    **save_summaries_secs=300**,\r\n    **save_interval_secs=600**):\r\n\r\nThe above api only supports to capture model checpoints periodically, but I need to checkpoint based on each step. How do achieve this using TesorFlow-Slim API.?\r\n\r\nI am looking for parameters like this:\r\n\r\nsave_summaries_steps = 10,\r\nsave_interval_steps=10\r\n    where the value 10 is the number of steps and that should be configurable.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Thanks.. took your advice.  I've posted the same on stackoverflow, but can we take this as a feature request, as I did not find any way to achieve it in TF-Slim.? do I need to modify the content to make it a request.? I've recently joined so I may not have enough knowledge on github working process..", "It is a feature request if it cannot be done with existing functionality.\r\n\r\n@nsilberman Could you comment on @softmanu question/request?"]}, {"number": 16461, "title": "macOS mnist download error", "body": "```\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\r\n```\r\nMacOS python3.6 is wrong\r\nwindows is right.\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/funny/Documents/AIML/test.py\", line 8, in <module>\r\n    from tensorflow.examples.tutorials.mnist import input_data\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 63, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py\", line 76, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 37, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/context.py\", line 27, in <module>\r\n    from tensorflow.python.framework import errors\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors.py\", line 22, in <module>\r\n    from tensorflow.python.framework import errors_impl as _impl\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 27, in <module>\r\n    from tensorflow.python.util import compat\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/compat.py\", line 130, in <module>\r\n    remove_undocumented(__name__, _allowed_symbols)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/all_util.py\", line 103, in remove_undocumented\r\n    should_have = make_all(module_name, doc_string_modules)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/all_util.py\", line 55, in make_all\r\n    for m in _reference_pattern.finditer(doc_module.__doc__)\r\nTypeError: expected string or bytes-like object\r\n```", "comments": ["which tensorflow version have you installed? `tensorflow` or `tensorflow-gpu`\r\n\r\n- `macos 10.13.3`\r\n- `python 2.7.14` with `pip install tensorflow`, pass\r\n- `python 3.6.4` got same error issues, uninstall first and then `pip3 install tensorflow` fix the problem.", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Closing due to inactivity. Please reopen if this is still a problem."]}, {"number": 16460, "title": "Fix missing .", "body": "", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address on your commit.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot. The email used to register you as an authorized contributor must be the email used for the Git commit.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_sender_cla -->", "I signed it!", "@matthieudelaro please make sure your email is set correctly on your commits. See the link in the message from the CLA bot above.", "Ok. I'd like to put my email address on the CLA to my github email address then, because I'd rather not expose my private email address. How can I do this?", "As long as the email in your commits match the email with which you sign the CLA all is well. IF your commits have an email you don't want to expose, perhaps you can abandon this PR and create a new one with your github email?", "Overriding CLA check.", "@jhseu thanks.", "We couldn't verify the CLA so we had to revert. Feel free to send another pull request with the issues fixed. Thanks!"]}, {"number": 16459, "title": "Fix a couple minor typos for DataSet API", "body": "Fix a couple minor typos in docs of DataSet API", "comments": []}, {"number": 16458, "title": "How to parse multivalve feature using tf.feature_column and tf.data API ??", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["My current solution is use tf.decode_csv and tf.string_split to write the input_fn, then use dataset.padded_batch(), but it raise the all kinds of shape error.", "I apologize but I am having a hard time understanding what the problem is. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. \r\n\r\nIf you have question on how to use TensorFlow please ask on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow)."]}, {"number": 16457, "title": "Branch 183374082", "body": "", "comments": []}, {"number": 16456, "title": "'InputFnOps' object has no attribute 'receiver_tensors'", "body": "\r\n### System information\r\n==TensorFlow installed from (source or binary)==\r\nSource\r\n== Python version ==\r\nPython 2.7.13\r\n== cat /etc/issue ==\r\nLinux orion 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux\r\nVERSION_ID=\"9\"\r\nVERSION=\"9 (stretch)\"\r\n== are we in docker ==\r\nNo\r\n== compiler ==\r\nc++ (Debian 6.3.0-18) 6.3.0 20170516\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n== uname -a ==\r\nLinux orion 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux\r\n== check pips ==\r\nnumpy (1.12.1)\r\nprotobuf (3.5.1)\r\ntensorflow (1.3.0)\r\ntensorflow-tensorboard (0.1.8)\r\ntensorflow-transform (0.3.1)\r\n== check for virtualenv ==\r\nTrue\r\n== tensorflow import ==\r\ntf.VERSION = 1.3.0\r\ntf.GIT_VERSION = v1.3.0-rc2-20-g0787eee\r\ntf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee\r\nSanity check: array([1], dtype=int32)\r\n== env ==\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n### Describe the problem\r\nI ran into some incompatibility issues running tf.estimator.DNNClassifier with tf.contrib.learn.InputFnOps and tf.contrib.learn.Experiment.  \r\nThere has been a separate [solved issue](https://github.com/tensorflow/transform/issues/36): \r\nI have tried the bundle version, but it doesn't work for me:\r\ntensorflow==1.3\r\ntensorflow_transform==0.3.1\r\nsix==1.10.0\r\nHowever, if I switch to tf.contrib.learn.DNNClassifier, the issue go away.\r\nIt is also suggested to use tf.estimator.DNNClassifier rather than tf.contrib.learn.DNNClassifier.  I would like to get tf.estimator.DNNClassifier work.\r\n\r\n### Source code / logs\r\n```\r\ndef build_estimator(config):\r\n    m = tf.estimator.DNNClassifier(\r\n   # m = tf.contrib.learn.DNNClassifier(\r\n    \t\t\t\t\tconfig=config,\r\n    \t\t\t\t\tfeature_columns=deep_columns,\r\n                        hidden_units=[100, 100, 100],\r\n                        n_classes=2,\r\n                        optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\n                        )\r\n    return m\r\n```\r\n```\r\ndef json_serving_input_fn():\r\n\t\"\"\"Build the serving inputs.\"\"\"\r\n\tinputs = {}\r\n\tfor feat in INPUT_COLUMNS:\r\n\t\tinputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)\r\n\r\n\tfeatures = {\r\n\t  key: tf.expand_dims(tensor, -1)\r\n\t  for key, tensor in inputs.iteritems()\r\n\t}\r\n\treturn tf.contrib.learn.InputFnOps(features, None, inputs)\r\n```\r\n```\r\ndef _experiment_fn(run_config, hparams):\r\n    # num_epochs can control duration if train_steps isn't\r\n    # passed to Experiment\r\n    train_input = lambda: model.generate_input_fn(\r\n        hparams.train_files,\r\n        num_epochs=hparams.num_epochs,\r\n        batch_size=hparams.train_batch_size,\r\n    )\r\n    # Don't shuffle evaluation data\r\n    eval_input = lambda: model.generate_input_fn(\r\n        hparams.eval_files,\r\n        batch_size=hparams.eval_batch_size,\r\n        shuffle=False\r\n    )\r\n    return tf.contrib.learn.Experiment(\r\n        model.build_estimator(\r\n            config=run_config\r\n        )\r\n```\r\n\r\n\r\nError message:\r\nlocal/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py\", line 440, in export_savedmodel\r\n    serving_input_receiver.receiver_tensors,\r\nAttributeError: 'InputFnOps' object has no attribute 'receiver_tensors'\r\n", "comments": ["Please try using TensorFlow 1.5 and let us know if you still see an error.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Found the updated code for [census](https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/estimator/trainer/model.py) that works with tf 1.4"]}, {"number": 16455, "title": "Set training=True in BatchNormalization layer causes evaluation error in custom Estimator model", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu16.04\r\n- **TensorFlow installed from (source or binary)**: pip install \r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**:  3.5.2\r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**: GeForce 1080ti, 11G\r\n\r\n### Describe the problem\r\nI define a custom estimator model for classification following [this document](https://www.tensorflow.org/extend/estimators). **Cifar10** dataset is used for test and network framework is **xception** rewritten in tensorflow. But when using `estimator.train_and_evaluate()` to train and evaluate the model repeatedly, I find evaluation accuracy dones't improve while training accuracy is normally increasing with training. Inspired by tensorflow official [resnet estimator example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/resnet.py): <br>\r\n```python\r\n  with tf.variable_scope('conv_layer1'):\r\n    net = tf.layers.conv2d(\r\n        x,\r\n        filters=64,\r\n        kernel_size=7,\r\n        activation=tf.nn.relu)\r\n    net = tf.layers.batch_normalization(net)   # no training status, default is False\r\n```\r\n \r\nI turn off `training=is_training` in `tf.layers.batch_normalization()`, both training and evaluation do work normally. For estimator model_fn is used multiple times (see [issue 13895](https://github.com/tensorflow/tensorflow/issues/13895)),  so is this issue related to graph reuse in BN layer and if training status option could be set to enable BN layer to act differently during training and evaluation/predict?\r\n\r\nBTW, same issue occurs when using `keras` or `slim` instead of `tf.layers` to construct network architecture.\r\n\r\n### Source code / logs\r\n**Network architecture:** <br>\r\n```python\r\ndef tf_xception(features, input_shape, pooling=None, classes=2, is_training=True):\r\n    # is_training = False  # manually set False to disable training option\r\n    x = tf.layers.conv2d(features, 32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block1_conv1_bn')\r\n    x = tf.nn.relu(x, name='block1_conv1_act')\r\n    x = tf.layers.conv2d(x, 64, (3, 3), use_bias=False, name='block1_conv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block1_conv2_bn')\r\n    x = tf.nn.relu(x, name='block1_conv2_act')\r\n\r\n    residual = tf.layers.conv2d(x, 128, (1, 1), strides=(2, 2),\r\n                      padding='same', use_bias=False)\r\n    residual = tf.layers.batch_normalization(residual, training=is_training)\r\n\r\n    x = tf.layers.separable_conv2d(x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block2_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block2_sepconv2_act')\r\n    x = tf.layers.separable_conv2d(x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block2_sepconv2_bn')\r\n\r\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block2_pool')\r\n    x = tf.add(x, residual, name='block2_add')\r\n\r\n    residual = tf.layers.conv2d(x, 256, (1, 1), strides=(2, 2),\r\n                      padding='same', use_bias=False)\r\n    residual = tf.layers.batch_normalization(residual, training=is_training)\r\n\r\n    x = tf.nn.relu(x, name='block3_sepconv1_act')\r\n    x = tf.layers.separable_conv2d(x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block3_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block3_sepconv2_act')\r\n    x = tf.layers.separable_conv2d(x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block3_sepconv2_bn')\r\n\r\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block3_pool')\r\n    x = tf.add(x, residual, name=\"block3_add\")\r\n\r\n    residual = tf.layers.conv2d(x, 728, (1, 1), strides=(2, 2),\r\n                      padding='same', use_bias=False)\r\n    residual = tf.layers.batch_normalization(residual, training=is_training)\r\n\r\n    x = tf.nn.relu(x, name='block4_sepconv1_act')\r\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block4_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block4_sepconv2_act')\r\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block4_sepconv2_bn')\r\n\r\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block4_pool')\r\n    x = tf.add(x, residual, name=\"block4_add\")\r\n\r\n    for i in range(8):\r\n        residual = x\r\n        prefix = 'block' + str(i + 5)\r\n\r\n        x = tf.nn.relu(x, name=prefix + '_sepconv1_act')\r\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')\r\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv1_bn')\r\n        x = tf.nn.relu(x, name=prefix + '_sepconv2_act')\r\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')\r\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv2_bn')\r\n        x = tf.nn.relu(x, name=prefix + '_sepconv3_act')\r\n        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')\r\n        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv3_bn')\r\n\r\n        x = tf.add(x, residual, name=prefix+\"_add\")\r\n\r\n    residual = tf.layers.conv2d(x, 1024, (1, 1), strides=(2, 2),\r\n                      padding='same', use_bias=False)\r\n    residual = tf.layers.batch_normalization(residual, training=is_training)\r\n\r\n    x = tf.nn.relu(x, name='block13_sepconv1_act')\r\n    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block13_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block13_sepconv2_act')\r\n    x = tf.layers.separable_conv2d(x, 1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block13_sepconv2_bn')\r\n\r\n    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block13_pool')\r\n    x = tf.add(x, residual, name=\"block13_add\")\r\n\r\n    x = tf.layers.separable_conv2d(x, 1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block14_sepconv1_bn')\r\n    x = tf.nn.relu(x, name='block14_sepconv1_act')\r\n\r\n    x = tf.layers.separable_conv2d(x, 2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')\r\n    x = tf.layers.batch_normalization(x, training=is_training, name='block14_sepconv2_bn')\r\n    x = tf.nn.relu(x, name='block14_sepconv2_act')\r\n    # replace conv layer with fc\r\n    x = tf.layers.average_pooling2d(x, (3, 3), (2, 2), name=\"global_average_pooling\")\r\n    x = tf.layers.conv2d(x, 2048, [1, 1], activation=None, name=\"block15_conv1\")\r\n    x = tf.layers.conv2d(x, classes, [1, 1], activation=None, name=\"block15_conv2\")\r\n    x = tf.squeeze(x, axis=[1, 2], name=\"logits\")\r\n    return x\r\n```\r\n\r\n**model_fn:** <br>\r\n```python\r\ndef model_fn(features, labels, mode, params):\r\n    # check if training stage\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        is_training = True\r\n    else:\r\n        is_training = False\r\n    input_tensor = features[\"input\"]\r\n    logits = tf_xception(input_tensor, input_shape=(96, 96, 3), classes=10, is_training=is_training)\r\n    probs = tf.nn.softmax(logits, name=\"output_score\")\r\n    predictions = tf.argmax(probs, axis=-1, name=\"output_label\")\r\n    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), 10)\r\n    predictions_dict = {\"score\": probs,\r\n                        \"label\": predictions}\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions_output = tf.estimator.export.PredictOutput(predictions_dict)\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          predictions=predictions_dict,\r\n                                          export_outputs={\r\n                                              tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output\r\n                                          })\r\n    # calculate loss\r\n    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits)\r\n    accuracy = tf.metrics.accuracy(labels=labels,\r\n                                   predictions=predictions)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        lr = params.learning_rate\r\n        # train optimizer\r\n        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\r\n        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n        tensors_to_log = {'batch_accuracy': accuracy[1]}\r\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op,\r\n                                          training_hooks=[logging_hook])\r\n    else:\r\n        eval_metric_ops = {\"accuracy\": accuracy}\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          eval_metric_ops=eval_metric_ops)\r\n```\r\n\r\n**If the training status set True in training and False in evaluation**<br>\r\n**train logs:**<br>\r\n```\r\nINFO:tensorflow:Saving checkpoints for 1 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 2.3025837, step = 1\r\nINFO:tensorflow:batch_accuracy = 0.109375\r\nINFO:tensorflow:global_step/sec: 3.50983\r\nINFO:tensorflow:loss = 2.3048878, step = 101 (28.492 sec)\r\nINFO:tensorflow:Saving checkpoints for 185 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 2.3093615.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-185\r\nINFO:tensorflow:Saving checkpoints for 186 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 2.2975698, step = 186\r\nINFO:tensorflow:batch_accuracy = 0.09375\r\nINFO:tensorflow:global_step/sec: 3.47248\r\nINFO:tensorflow:loss = 2.3078504, step = 286 (28.798 sec)\r\nINFO:tensorflow:Saving checkpoints for 374 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 2.290754.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-374\r\nINFO:tensorflow:Saving checkpoints for 375 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 2.2987142, step = 375\r\nINFO:tensorflow:batch_accuracy = 0.140625\r\nINFO:tensorflow:global_step/sec: 3.50966\r\nINFO:tensorflow:loss = 2.0407405, step = 475 (28.493 sec)\r\nINFO:tensorflow:Saving checkpoints for 560 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 2.1280906.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-560\r\nINFO:tensorflow:Saving checkpoints for 561 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 2.0747793, step = 561\r\nINFO:tensorflow:batch_accuracy = 0.203125\r\nINFO:tensorflow:global_step/sec: 3.31447\r\nINFO:tensorflow:loss = 2.1767468, step = 661 (30.171 sec)\r\nINFO:tensorflow:Saving checkpoints for 740 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.9530052.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-740\r\nINFO:tensorflow:Saving checkpoints for 741 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 1.9676144, step = 741\r\nINFO:tensorflow:batch_accuracy = 0.296875\r\nINFO:tensorflow:global_step/sec: 3.50441\r\nINFO:tensorflow:loss = 1.8766258, step = 841 (28.536 sec)\r\nINFO:tensorflow:Saving checkpoints for 930 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.884157.\r\n...\r\nINFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-930\r\nINFO:tensorflow:Saving checkpoints for 931 into train_episode5/model.ckpt.\r\nINFO:tensorflow:loss = 1.8624167, step = 931\r\nINFO:tensorflow:batch_accuracy = 0.296875\r\nINFO:tensorflow:global_step/sec: 3.30778\r\nINFO:tensorflow:loss = 1.7580669, step = 1031 (30.232 sec)\r\nINFO:tensorflow:Saving checkpoints for 1112 into train_episode5/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 1.9509349.\r\n...\r\n```\r\n\r\n**eval log:**\r\n```\r\nINFO:tensorflow:Saving dict for global step 170: accuracy = 0.099306434, global_step = 170, loss = 2.3029344\r\nINFO:tensorflow:Saving dict for global step 348: accuracy = 0.11751261, global_step = 348, loss = 2.2920265\r\nINFO:tensorflow:Saving dict for global step 528: accuracy = 0.13106872, global_step = 528, loss = 2.5031097\r\nINFO:tensorflow:Saving dict for global step 697: accuracy = 0.085986756, global_step = 697, loss = 30.668789\r\nINFO:tensorflow:Saving dict for global step 871: accuracy = 0.10009458, global_step = 871, loss = 47931.96\r\n...\r\n```\r\n**accuracy round initial 0.1 and loss increase ridiculously !!!!**\r\n\r\n**If training status set False in both stage:**<br>\r\n**train log:** similar to above train log \r\n\r\n**eval log:** <br>\r\n```\r\nINFO:tensorflow:Saving dict for global step 185: accuracy = 0.1012768, global_step = 185, loss = 2.3037012\r\nINFO:tensorflow:Saving dict for global step 374: accuracy = 0.10001576, global_step = 374, loss = 2.3124988\r\nINFO:tensorflow:Saving dict for global step 560: accuracy = 0.20081967, global_step = 560, loss = 2.0881999\r\nINFO:tensorflow:Saving dict for global step 740: accuracy = 0.26134932, global_step = 740, loss = 2.0297167\r\nINFO:tensorflow:Saving dict for global step 930: accuracy = 0.26379257, global_step = 930, loss = 1.9529407\r\nINFO:tensorflow:Saving dict for global step 1112: accuracy = 0.3454445, global_step = 1112, loss = 1.832811\r\n```\r\n", "comments": ["It seems that my BN layer usage is wrong. Based on Tensorflow `tf.layers.batch_normalization()` [document](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization), \r\n> Note: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_optf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op\r\n\r\ntrain_op should be included in with control_dependancy scope:  <br>\r\n```python\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n  train_op = optimizer.minimize(loss)\r\n```\r\n\r\nModify upper script into following snippet and it works fine now!\r\n```python\r\n...\r\noptimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\r\n# train optimizer\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n...\r\n```", "Could you post a working example of how to properly set the value for `training` in `tf.layers.batch_normalization()`? \r\n\r\nAs per the docs (TensorFlow 1.4):\r\n\r\n> training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (normalized with statistics of the current batch) or in inference mode (normalized with moving statistics). **NOTE**: make sure to set this parameter correctly, or else your training/inference will not work properly.\r\n\r\nThis is how I'm setting it, but clearly it's not behaving as expected in `EVAL` and `PREDICT` modes:\r\n\r\n    def model_fn(features, labels, mode, params):\r\n      training = bool(mode == tf.estimator.ModeKeys.TRAIN)\r\n      extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\r\n      # ...\r\n\r\n      x = tf.layers.batch_normalization(x, training=training)\r\n\r\n     with tf.control_dependencies(extra_update_ops):\r\n         train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n\r\n<img width=\"722\" alt=\"tf-bn\" src=\"https://user-images.githubusercontent.com/852234/37413679-3069a75e-276d-11e8-8691-0cc05bf41ec1.png\">\r\n\r\nIn the graph above, the red and blue losses are for a network without `batch_normalization`. Then, using the same training and eval sets and the same architecture (plus `batch_normalization`), you can see that the pink training loss drops similar to how it did without BN, but the evaluation loss is way off. \r\n\r\nAlso, I'm graphing a histogram of the predicted values during training. They are always in a range of values (100, 300). Using the same training dataset to predict (`estimator.predict()`), the output is now in a range (15, 19). Way off. Looks like the `moving_mean` and `moving_variance` are not being saved correctly during training (or not used correctly during EVAL and PREDICT).\r\n\r\nThanks.", "@formigone I think you should put `tf.control_dependencies()` function inside your estimator TRAIN branch, for the moving average and variance are only need to update during training. Here is my example:\r\n```python\r\ndef model_fn(features, labels, mode, params):\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        is_training = True\r\n    else:\r\n        is_training = False\r\n    input_tensor = features[\"input\"]\r\n    logits = your_model_fn(input_tensor, is_training=is_training)\r\n    probs = tf.nn.softmax(logits, name=\"output_score\")\r\n    predictions = tf.argmax(probs, axis=-1, name=\"output_label\")\r\n    # provide a tf.estimator spec for PREDICT\r\n    predictions_dict = {\"score\": probs,\r\n                        \"label\": predictions}\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions_output = tf.estimator.export.PredictOutput(predictions_dict)\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          predictions=predictions_dict,\r\n                                          export_outputs={\r\n                                              tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output\r\n                                          })\r\n    # calculate loss\r\n    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), _NUM_CLASSES)\r\n    gamma = 1.5\r\n    weights = tf.reduce_sum(tf.multiply(onehot_labels, tf.pow(1. - probs, gamma)), axis=-1)\r\n    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits, weights=weights)\r\n    accuracy = tf.metrics.accuracy(labels=labels,\r\n                                   predictions=predictions)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        lr = 0.001\r\n        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        with tf.control_dependencies(update_ops):\r\n            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n        tensors_to_log = {'batch_accuracy': accuracy[1],\r\n                          'logits': logits,\r\n                          'label': labels}\r\n        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          train_op=train_op,\r\n                                          training_hooks=[logging_hook])\r\n    else:\r\n        eval_metric_ops = {\"accuracy\": accuracy}\r\n        return tf.estimator.EstimatorSpec(mode=mode,\r\n                                          loss=loss,\r\n                                          eval_metric_ops=eval_metric_ops)\r\n```", "As an update to my issue, what I was doing wrong in the sample I posted is that I was getting update ops collection\r\n\r\n```python\r\nextra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n```\r\n\r\nbefore I had added a `batch_normalization` ops to my graph. The documentation on this has been updated since then, so this requirement is now more clearly described in the batch norm code documentation.", "@formigone it might not be the case, but you can try lowering the momentum in `batch_normalization(..., momentum=0.99, ...)`, to e.g. `0.9`. The default can often be too \"smooth\", and as a result the validation metrics can be worse since the training forgets past values more slowly", "If you are using a model inherited from `tf.keras.Model` then\r\n\r\n    extra_update_ops = model.get_updates_for(features)\r\n\r\nwill do the trick.", "Could anyone give me an idea about how these tf.keras.layers updates can be accomplished by the EstimatorSpec in the estimator's model_fn? Without running these update ops, the tf.keras.BatchNormalization layer will not work properly with an estimator.", "@awerdich That would be\r\n\r\n```\r\ndef model_fn(features, labels, mode):\r\n\r\n    model = KerasModel()\r\n\r\n    training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n    y = model(features, training)\r\n\r\n    predictions = {\r\n        # Generate predictions (for PREDICT mode)\r\n        \"prob\": y,\r\n    }\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n\r\n    # Calculate Loss (for both TRAIN and EVAL modes)\r\n\r\n    loss = tf.reduce_mean(tf.keras.backend.binary_crossentropy(labels['y_true'], y))\r\n    # Configure the Training Op (for TRAIN mode)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.MomentumOptimizer(learning_rate=1e-4, momentum=0.9)\r\n\r\n        with tf.control_dependencies(model.get_updates_for(features)):\r\n            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n    # Add evaluation metrics (for EVAL mode)\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss)\r\n```", "This should be marked as the solution. That is the problem of Keras layers in tf estimator API that update ops are not attached to `tf.GraphKeys.UPDATE_OPS` and should be added using this line.\r\n\r\n\r\n> If you are using a model inherited from `tf.keras.Model` then\r\n> \r\n> ```\r\n> extra_update_ops = model.get_updates_for(features)\r\n> ```\r\n> \r\n> will do the trick.\r\n\r\n", "See: https://pgaleone.eu/tensorflow/keras/2019/01/19/keras-not-yet-interface-to-tensorflow/", "Why is this marked as closed? I think this should be fixed so that tf.keras.models.Model() captures the batchnorm variables so that when model.fit() is called, these parameters are updated."]}, {"number": 16454, "title": "ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible", "body": "Traceback (most recent call last):\r\n  File \"/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 558, in merge_with\r\n    new_dims.append(dim.merge_with(other[i]))\r\n  File \"/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 133, in merge_with\r\n    self.assert_is_compatible_with(other)\r\n  File \"/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 106, in assert_is_compatible_with\r\n    other))\r\nValueError: Dimensions 1069539296 and 13528529576648672 are not compatible\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Train.py\", line 117, in <module>\r\n    train()\r\n  File \"Train.py\", line 54, in train\r\n    train_op=Evaluation.trainning(loss=loss1, learning_rate=0.0001)\r\n  File \"/home/lihua/Documents/Projects/Project2018/trafficSignClassification/Evaluation.py\", line 36, in trainning\r\n    train_op = optimizer.minimize(loss, global_step= global_step)\r\n  File \"/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 315, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in gradients\r\n    in_grad.set_shape(t_in.get_shape())\r\n  File \"/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 443, in set_shape\r\n    self._shape = self._shape.merge_with(shape)\r\n  File \"/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 561, in merge_with\r\n    raise ValueError(\"Shapes %s and %s are not compatible\" % (self, other))\r\nValueError: Shapes (128, 4, 4, 1069539296) and (128, 4, 4, 13528529576648672) are not compatible\r\n\r\n\r\nubuntu16.04\r\ntensorflow 1.4\r\n\r\n\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and on what platform it happens. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 16453, "title": "Updated roadmap?", "body": "The [TensorFlow roadmap](https://www.tensorflow.org/about/roadmap) was last updated a year ago (January 2017); could there be an update from the team on where TensorFlow is going in 2018?", "comments": ["@qtdaniel Good point.\r\n\r\n@wolffg @rajatmonga @mkacholia we should update the roadmap on tensorflow.org.", "Does tensorflow have plan to implement Unicode string Tensor?", "Yes, we are looking to update the roadmap soon, and will work to keep it more up to date.\r\n\r\n@facaiy We do not have anything in the works to implement Unicode string Tensor. If this is something that is useful, may be worth opening it as a feature request, and share how you may do it. \r\n", "It could be useful to explicitly know what is in the next release targets. Can you start to use a specific release label for ISSUE/PR?", "Unfortunately I don't think we can commit to including individual issues/PRs to a given release unless they're part of a larger roadmap item. @aselle @av8ramit correct me if I'm wrong.", "@skye Why not? Many of your upstream updates comes from your internal branch that merge more or less regularly here on the public available repository. \r\nTagging \"community\" issue/pr on a target release could be useful to have a tentative effort. I.e. this is working for Kubernetes as a general reference.", "Yes, unfortunately that's true at the moment. Since we release on a regular cadence we don't commit to individual PRs/issues unless there is a special exception made. ", "@av8ramit But you target also review workload on user public PRs. So this could be useful for community PR and also to issue that are waiting for an internal upstream update. You can see many comment on issues that are not labeled as contribute welcome with comment like \"we are working on this\".", "Just as an example we need to always make explicit requests [like this](https://github.com/tensorflow/tensorflow/issues/16468#issuecomment-364678210) in meanwhile TF team is releasing subsequent RC.", "One reason for this is, when we tried to commit to certain features for releases, we saw our releases delayed by months (i.e. 1.5 release). When we move to modularizing tensorflow, and make more smaller component releases this will be feasible.\r\nHowever, with the current monolithic structure of tensorflow, committing to features for releases just cause us to penalize all of TF if a feature is particularly messy, or problematic.\r\n", "@gunan It is a good point but also on large repositories/communities like Kubernetes the release labels are just a desired target. When you are aware to skip the release target, best-effort, the label is re-target again.\r\nSo here we are not talking about managing release blocking labels.", "For the pure Roadmap https://github.com/tensorflow/tensorflow/pull/16984 is merged.", "@sandeepngupta has merged the new roadmap which will be populated to the site soon. Closing for now. "]}, {"number": 16452, "title": "Semantic Segmentation API", "body": "Hello, is there any plan to include Semantic Segmentation API in future Tensorflow releases, similar to [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection#tensorflow-object-detection-api) ?\r\n\r\nThere are other semantic segmentation repositories in Github, an awesome list is [here](https://github.com/mrgloom/awesome-semantic-segmentation), but I would like to see the implementation from Tensorflow organization.\r\nThanks", "comments": ["@dreamdragon Would you like to comment on this request?", "Check https://github.com/tensorflow/models/issues/2793 but we could use this issue to track the topic.", "Hello, maybe @afathi3 or @tatianashp  has got an ETA for semantic segmentation Tensorflow release?\r\nLast week [Detectron](https://github.com/facebookresearch/Detectron) Instance Segmentation was released, it could be interesting to compare these two software.\r\nThanks\r\n\r\n", "@jch1 Can you comment?", "/cc @gpapan @YknZhu any connection to your new [DeepLabv3+](https://arxiv.org/abs/1802.02611) paper/code?", "I would like to inform that February 9 Tensorflow Api release now support instance segmentation.\r\nRefer to [this section](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/instance_segmentation.md) for instructions.", "@RomRoc What about the code mentioned in the DeepLabv3+ paper?", "@bhack It's better to ask to Tensorflow team for forthcoming developments.", "@bhack We are working on cleaning up the codebase to release DeepLabv3+. Please stay tuned :)", "@YknZhu ping us here when you will have an update :wink:", "@YknZhu any plan to release a Pytorch version for that ?", "Haha no plans yet ;) But helping hands welcome!\n\n> On Mar 2, 2018, at 09:09, Edgar Riba <notifications@github.com> wrote:\n> \n> @YknZhu <https://github.com/yknzhu> any plan to release a Pytorch version for that ?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/16452#issuecomment-369984728>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AIdHDQAiHpwKSdkNew9Yj7UEU5wJv2PMks5taXyzgaJpZM4RuIwJ>.\n> \n\n", "@YknZhu Keep me updated, can convert to pytorch.", "https://github.com/tensorflow/models/pull/3551", "Thanks for the patience. Deeplabv3+ is now released at https://github.com/tensorflow/models/tree/master/research/deeplab :)  ", "@YknZhu Are you waiting also for https://github.com/tensorflow/models/pull/3244?", "Yup mobilenet deeplab models will be released after they integrated into tensorflow/models.\n\n> On Mar 12, 2018, at 3:19 PM, bhack <notifications@github.com> wrote:\n> \n> @YknZhu <https://github.com/yknzhu> Are you waiting also for tensorflow/models#3244 <https://github.com/tensorflow/models/pull/3244>?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/16452#issuecomment-372481066>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AIdHDVeqWRiIi_cV0XLaZZavO22B0mDfks5tdvSLgaJpZM4RuIwJ>.\n> \n\n", "I think that we can close this. Mobilenet backbone was merged in https://github.com/tensorflow/models/pull/3709", "Yes thanks."]}, {"number": 16451, "title": "Parameter parsing error messages", "body": "Parameter parsing error messages probably can be improved, e.g. \r\n\r\n`bazel-bin/tensorflow/core/profiler/profiler --profile_path=/tmp/for_tfprof/profile_20`\r\n\r\nruns ok, but if cd to bazel-bin/tensorflow/core/profiler/ and\r\n\r\n`./profiler --profile_path /tmp/for_tfprof/profile_20`\r\n\r\nresults in \r\n\r\n> ./profiler\r\n> --profile_path\r\n> /tmp/for_tfprof/profile_20\r\n> Reading Files...\r\n> Try to use a single --profile_path instead of graph_path,op_log_path,run_meta_path\r\n> 2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory\r\n> Aborted (core dumped)\r\n\r\n", "comments": ["I think the error message could be improved. Created a PR #16463 for the fix."]}, {"number": 16450, "title": "InvalidArgumentError (see above for traceback): sequence_length(0) <= 80 thrown by ctc_loss", "body": "I am encountering this error thrown by `ctc_loss` and I have no idea what it means nor how to resolve it.\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): sequence_length(0) <= 80\r\n\t [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](transpose_1/_455, Where/_445, GatherNd, reshape_1/_457)]]\r\n\t [[Node: CTCLoss/_459 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_7988_CTCLoss\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 16449, "title": "Placate pylint on jupyter_notebook_config.py", "body": "Eliminate the following pylint issues:\r\n```\r\nE: 18, 0: Undefined variable 'c' (undefined-variable)\r\nE: 19, 0: Undefined variable 'c' (undefined-variable)\r\nE: 20, 0: Undefined variable 'c' (undefined-variable)\r\nE: 26, 4: Undefined variable 'c' (undefined-variable)\r\nE: 28, 4: Undefined variable 'c' (undefined-variable)\r\nE: 29, 4: Undefined variable 'c' (undefined-variable)\r\n```", "comments": []}, {"number": 16448, "title": "embedding lookup table in tensorflow serving", "body": "Hi, I am trying to serve a NLP model in tensorflow serving. I am wondering how embedding matrix is being stored in tensorflow serving. If I deploy model to two servers, will the embedding matrix be a distributed table with sharding for looking up?", "comments": ["same question, how serving large embedding lookup?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "> same question, how serving large embedding lookup?\r\n\r\nhi, how do you solved this problem at last."]}, {"number": 16447, "title": "Fixing the url for the pip3.", "body": "", "comments": []}, {"number": 16446, "title": "use tflite bilinear op to resize input of label_image", "body": "replace previous naive `downsize()` function with a `resize()` using TF Lite RESIZE_BILINEAR operator", "comments": ["@freedomtan Please address @andrehentz 's comments."]}]