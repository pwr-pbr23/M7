[{"number": 6481, "title": "Merge pull request #6463 from gunan/cp", "body": "Cherrypick doc updates to r0.12 branch.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6480, "title": "Update README.md", "body": "Add instructions on where to run the inception download commands", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "Looks like there is still a problem with the CLA.\r\nCould you verify your email you used to sign the CLA is the same one as the email that is in your commit?", "@mortenjust the CLA still seems to be an issue.\r\nCould you make sure your commit email and the email you used to sign the CLA are the same?", "hey @gunan - thanks, should be good now. Thought it was enough to add my other CTA-signed emails to github. Anyway, signed the @genstart.dk email just now. ", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 6479, "title": "Bash argument completion for tensorboard", "body": "It be nice to have autocompletion for the tensorboard arguments.\r\nI experimented a bit https://github.com/Garoe/tensorflow/commit/3c217767cd47560c776232207a05b08b6f2cb166 with the [argcomplete](https://argcomplete.readthedocs.io/en/latest/), however the completion is too sluggish to be useful.\r\nThe entry point for the completion is after the flags are defined and the slugginesh is due to too much stuff being done before that.\r\nAnother option would be to use [native completion](https://github.com/scop/bash-completion), but that would require to duplicate the flag definitions for tensorboard.", "comments": ["@danmane, please consider this feature request. Thanks!", "I've put together a simple file using the native option https://github.com/Garoe/tensorflow/commit/887678ef4ed1da04a13c272e5d5bb8d5fc2ac536\r\nFor installation without root access, the file needs to be copied to  `~/.bash_completion.d/tensorboard` and the following line `. ~/.bash_completion.d/tensorboard` has to be added to a `~/.bash_completion` text file.\r\nI'd be happy to add the steps in the install process and create a proper pull request, but I'd really appreciate some pointers into what files should be modified as I'm not familiar with Bazel.", "Hmm, I think the installation should happen in the pip package setup, rather than in Bazel. \r\nI suspect either [this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py) or something else in the pip_package directory.\r\n\r\n@vrv care to comment?", "Honestly, I'd prefer this live as a nice external repository we point to, which we can link to somewhere.", "If it's in an external repository, would we depend on it and install it by default, or would this be only for people who really really care about bash completion and go through the work of installing it themselves?", "Hey!\r\nSo what is the status of this feature request?", "I've put the file in a [external repository](https://github.com/Garoe/tensorboard_bash_completion/blob/master/tensorboard) and it's updated for the parameters used in version 1.2\r\nAs I said before, I'd happy to write a pull request to include this by default or however they think it's best, so I'm still waiting on someone to answer @dandelionmane  question."]}, {"number": 6478, "title": "t.eval() built with slim outputs wrong predictions when input batch contains identical images", "body": "When evaluating a model built with slim on a batch that contains identical images, the output of the batch will be wrong (mostly the outputs will be the most frequent label). For example, if my evaluation set has 421 images and my batch size is 40, I filled the last batch with 19 identical images (the 421st one) to avoid tensor shape mismatch error. Then,  the output for the 21st to the 40th image of the last batch will be the most common label in training set. If I replace the 19 images with some randomly selected images, the outputs will be correct.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04.5\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n![image](https://cloud.githubusercontent.com/assets/3335135/21456479/56b99382-c8f6-11e6-88ee-1b83758fb56a.png)\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:111] successfully opened CUDA library libcurand.so locally\r\n0.11.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nThe problem can be reproduced with any model built purely with slim but evaluated with output.eval(feed_dict={batch_image:test_batch}) or sess.run(output, feed_dict={batch_image:test_batch})\r\nwhen the test batch contains several identical images. \r\n\r\n### What other attempted solutions have you tried?\r\nAvoid identical or similar images in evaluation batch.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Could you follow up with a quick concrete repro case that we can try running?", "Sure, I will wrap up a repro case in a couple of days.", "I put together a repro case in a .ipynb file. Please place the file in the slim folder (./models/slim) so that it can find the slim dataset lib. The 4th and 7th cell set the dataset path, please point it to the converted cifar dataset tfrecord folder. Then, you should be able to run it. \r\n\r\nAt the penultimate cell, you can see results of a batch with different images and most of the classifications match their label. However, at the last cell, when I fill the batch with identical images, the classification will always be 5. \r\n\r\n[slim_debug.zip](https://github.com/tensorflow/tensorflow/files/693863/slim_debug.zip)\r\n", "In test time you didn't seem to set batch_norm to is_training=False.\r\nThen with an identical batch it will certainly fail -- variance ended up being 0.", "Oh, thank you so much for pointing this out. I will have a try and let you know how it goes. ", "I set batch_norm to is_training=False but still have the same issue. Below is the updated evaluation/test graph:\r\n\r\n```\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n  batch_images = tf.placeholder(tf.float32, (None,32,32,3))\r\n  batch_labels = tf.placeholder(tf.float32, (None,10))    \r\n  is_training = tf.placeholder(tf.bool)\r\n  \r\n  with slim.arg_scope([slim.batch_norm], is_training=is_training):\r\n    features = classifier_features(batch_images, is_training)\r\n    outputs, output_logits = classifier_outputs(features, dataset.num_classes)\r\n  output_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, batch_labels)) \r\n  output_loss_reg = output_loss + tf.add_n(slim.losses.get_regularization_losses())\r\n\r\n  train_vars = tf.trainable_variables() \r\n```\r\n\r\nAm I doing it in the right way?", "I'm not sure what your classifier_features or classifier_outputs do.\r\nIf you are using drop_out you also need to set is_training=False. ", "classifier_features and classifier_outputs are used to define the model. Please see the 3rd cell in the attached .ipynb file in my second reply to michaelisard for more details. \r\n\r\nYes, I'm using drop_out and classifier_features(batch_images, is_training) will set is_training=False for drop_out", "It looks like code in that `.ipynb` no longer runs (mostly the problems are function signatures that were updated in 1.0).  \r\n\r\n@PhoenixDai, could you update and retry it, let us know if this is still a problem."]}, {"number": 6477, "title": "Sub Matrices, Partitioned Matrices and composite matrices through view on undeling data", "body": "There are times you want to address portions of a vector or matrix separately and also combine matrices and vectors to form new matrices. Essentially these matrices would be  view on the original data.\r\n\r\nE.g.\r\n\r\nConcatenate 2 matrices to create a view\r\nSub current matrix into smaller n * n matrices returning view on the sub matrices", "comments": ["I'm not sure what you are asking for. I think that the cases you mention are covered in the language and documented under [slicing and joining](https://www.tensorflow.org/api_docs/python/array_ops/slicing_and_joining). Are there particular cases you have run into where the performance is bad?", "Documentation is not clear if updates are propagated to the parent structure. Any operation on the sub matrix should be reflected in the main. There should be a way to do both: copy on update and propagate update (reflect update in parent)\r\n\r\n", "In general, most TensorFlow Ops act functionally, so you don't expect an operation on a slice to have a side effect on some other tensor that isn't expressed via a dataflow edge. The exception of course is variables, and there are sparse update operations for variables. I think TensorArray may also be relevant to your question, depending on what you're trying to achieve. @ebrevdo has the most context on TensorArray and the future plans for updating partial state.", "We don't create views via concatenate or most slicing ops because all Tensors must be memory aligned contiguous memory blocks.  In some cases, e.g. when a slice ends up being memory aligned by happy accident, it is a view to the original memory location and no copy is made.\n\nNote that whenever possible, a Tensor is itself a view (a reference)... operations that don't require copies, like tf.identity on the same device as the input, and some slicing ops, do not perform a copy but instead increment a reference counter to some memory location.", "TensorArray carries a list of Tensors inside it, so it may be considered a view... But any attempt to read out multiple Tensors, i.e. via the gather method, necessarily performs a copy for the reasons I gave above.", "Can it be made explicit:\r\n\r\n - update to a view will update other data structures (one copy of data)\r\n - update to view will only update data of the data structure (copy on write)\r\n\r\nE.g. \r\n\r\n1. I have a sub matrix and perform an update this should be reflected in the larger matrix this is a component of\r\n2. I update a slice for which I do not want to propagate changes.", "We do not allow in place updates of anything but Variables because this\nbreaks functional flow and makes it impossible to correctly calculate\ngradients.  In place operations on variables have no gradients partly\nbecause of this.\n\nOn Jan 6, 2017 3:06 AM, \"Suminda Dharmasena\" <notifications@github.com>\nwrote:\n\n> Can it be made explicit:\n>\n>    - update to a view will update other data structures (one copy of data)\n>    - update to view will only update data of the data structure (copy on\n>    write)\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6477#issuecomment-270882361>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim9PtGMnlfLWzX9xVQCPFHobIDj8iks5rPiAdgaJpZM4LU3gD>\n> .\n>\n", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 6476, "title": "Data Structures for Time Series, Vectorized Time Series and real time data processing", "body": "There are times you want a tensor to be circular along a dimension so new data can be efficiently and quickly appended along this dimension.", "comments": ["Could you provide a motivating example of what you want and how it could be used? Depending on what you are indicating, perhaps a TensorArray or queue might fit the bill.", "Question arises is not efficient is arrays for one by one shifting. A form of array can be used it the axis was circular.", "Could you give more details of what you want to use it for?", "I want to have a n size window in price data. As new data come in I want to update the price histories and signal histories with minimal time, overheads and memory. This is one use case.", "It's possible you could use TensorArray for this. Overall though I'm not sure you are going to be happy with the results. It sounds as if what you would like to do is some kind of windowed streaming computation (though you are keeping it quite mysterious). Right now, TensorFlow may not be well suited to that type of computation. For example, if you took the sum of your price data, then adjusted the window, then recomputed the sum, TensorFlow, would recompute the entire sum, whereas you would probably prefer it to simply subtract the entries that were removed from the window, and add the entries that entered the window. I'm not sure what the use cases are where the copy to shift the window in memory would take a large fraction of your processing time, but the downstream computation that was re-executed would be comparatively cheap. Of course I'm sure there are such cases, but without a good sense of them we would find it hard to prioritize support.\r\n\r\nI guess if you can motivate the use case we can see if it makes sense to consider a circular extension to TensorArray. CC @ebrevdo.", "You can currently use a TensorArray with dynamic sizing (it's one of the options) and use .gather to pull out windows, but you may be better off using some other approaches.  Why do you want all of this to happen inside a tensorflow graph?  Will you be creating an op that talks to some service to read off new data, or will you be feeding in the steam via multiple calls to session.run?  TensorArray is only useful in the former approach.", "Actually sorry,I see you want a circular buffer.  What kind of performance do you need?  Without knowing many more details about the problem and you setup it's impossible to help.", "Yes. It is windowed streaming with efficient transfer between structures which can have multiple dimensions. If the transfers between the data structures are globally optimised based on data propagation this would also help.\r\n\r\nI am looking for as much juice that can possibly got from the system with time for calculations being sub microseconds.\r\n\r\nEach axis or slice will be like a ring buffer to which you can add and consume data. Also ability to partition the streaming axis will help. E.g. some stock prices quotes do not update very often and I have a matrix of stock prices I will not get a complete vector of new stock prices each time. So you roll the slice or partition when new data comes in than whole row or column at a time though the latter maybe more optimal for cases you can update vector at a time.\r\n\r\nIn multiple dimensions what you are appending has the same dimensions as the table except for the time dimension on which you are rolling (circularly appending) on.", ">  For example, if you took the sum of your price data, then adjusted the window, then recomputed the sum, TensorFlow, would recompute the entire sum, whereas you would probably prefer it to simply subtract the entries that were removed from the window, and add the entries that entered the window.\r\n\r\nI did not have this in mind then I opened this issue but it is a good suggestion which will be useful to me also hence I opened: https://github.com/tensorflow/tensorflow/issues/6690", "I think this is a really interesting problem. In particular, I think it would be cool to combine the automatically-incrementalized differential dataflow in [Naiad](http://sigops.org/sosp/sosp13/papers/p439-murray.pdf) with automatic differentiation of the kind that TensorFlow does. However, this would be a major rewrite of a lot of TensorFlow, which is not currently architected for low-latency updates. As such, it's really out of scope for the GitHub issues forum.\r\n\r\nI will close this issue and #6690 but if you are interested in pursuing this kind of research out of the main TensorFlow branch please send me email at misard@google.com. ", "@sirinath btw, CPU kernel launch is about 1 microsecond so it seems hard to get to <1 usec with TF"]}, {"number": 6475, "title": "Training with local variables is very slow in distributed tf", "body": "I used local variables on worker tasks in a distributed RNN training. Every update go through 3 steps: 1) assign local variables on each worker with PS variables 2) training local variables in several loops/batches, and accumulate the gradients from each loop 3) apply the accumulated gradients to the PS variables. The first and second step is very slow: for step 1, it takes about 12 seconds; for step 2, every loop takes about 6 seconds. If I combine the three steps together (not use local variable and go through one batch data every update), it only takes about 0.7 second. So, I suspect that it is due to the bad performance of local variables. \r\n\r\n### Environment info\r\nUbuntu 14.04, CUDA 8.0, tensorflow 0.12.0-rc1", "comments": ["How big are your local variables? There's some slowness in current version if they are too big -- https://github.com/tensorflow/tensorflow/issues/6116 . Work-around is to use the patch from that issue or shard them into 100MB chunks. Also, what does your `session.run` looks like? If you do something like `sess.run(assign_op)` then you are fetching result to Python which is slow for grpc sessions (https://github.com/tensorflow/tensorflow/issues/6256), the solution is to do `sess.run(assign_op.op)`", "Yes, my variables are very big, totally larger than 400MB. `sess.run(assign_op.op)` solved my problem. Thanks a lot."]}, {"number": 6474, "title": "Change draw_bounding_box() label input to allow different number of inter-batch boxes.", "body": "This is a question or feature request regarding the following function\r\n`tf.image.draw_bounding_boxes(images, boxes, name=None)`\r\n### Current\r\nThe required bounding-box format is as follows:\r\n`[batch, num_bounding_boxes, 4]`, in which the labels are in `[y_min, x_min, y_max, x_max]` format.\r\nIf i understand correctly, this assumes that every image has the same number of bounding boxes.\r\n### Suggestion\r\nDoesn't it make a lot more sense to change the input format to the following:\r\n`[num_labels, 5]` in the following format `[id, y_min, x_min, y_max, x_max]` where id corresponds to the image in the batch.", "comments": ["@dandelionmane, please consider this feature request.", "Can we add also color and text to the interface?", "I don't own the tf.image module and don't intend to work on this. Going to mark it contributions welcome.", "@TimZaman, I believe it is this way so that ops can be created to produce tensors in a deterministic way. If a bounding box is unused it can be given dummy values. Most ops in TensorFlow work better with direct indexable dense formats rather than the id-based sparse format you suggest. So I am closing for now."]}, {"number": 6473, "title": "Errors_impl - NotFoundError - stringpiece", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04.1 LTS\r\n\r\nInstalled version of CUDA and cuDNN: None\r\n\r\nBinary pip package info:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0rc0-cp35-cp35m-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`: 0.12.0\r\n\r\n**The error comes from the binary packages indicated above.** I had no problems with the package builded from source.\r\n\r\nSource info:\r\n\r\n1. The commit hash (`git rev-parse HEAD`): 48fb73a1c94ee2409382225428063d3496dc651e\r\n2. The output of `bazel version`: \r\n```\r\nBuild label: 0.4.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 30 09:13:36 2016 (1480497216)\r\nBuild timestamp: 1480497216\r\nBuild timestamp as int: 1480497216\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI guess seeing the problem is that I use inside a new Op some functions from this part of the core: \r\n\r\n```\r\n#include \"tensorflow/core/lib/core/stringpiece.h\"\r\n```\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nThis thing is strange because as I said with the package created from the source code I have no error during the execution of the script, but with the binary package provided from the official website I had the runtime error below.\r\n\r\n\r\n### Logs or other output that would be helpful\r\n\r\n```tensorflow.python.framework.errors_impl.NotFoundError: /.../newop.so: undefined symbol: _ZN10tensorflow9LogMemory21RecordRawDeallocationERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEExPvPNS_9AllocatorEb```\r\n\r\n", "comments": ["No more problems with new version 0.12 on MacOS.", "The problem still in Linux as said above. I want to add another information: \r\n\r\nto compile the plugin correctly on Linux I had to add libprotobuf and libprotobuf_lite as dependencies. I took the libraries from the compiled directory after bazel build execution.", "@jart, could you take a look at this issue?", "So you're writing a custom op. Can you post your Bazel build configuration?", "I'm sorry but what do you mean as build configuration? \r\n\r\nBy the way I was working on a clean system with Ubuntu 16.04. The fresh installation has only the base things to build tensorflow, I followed the instructions from the main website and for bazel I used the repo installation. \r\n\r\nThe system was previously configured with Cuda framework with the proper video driver and toolkit, also complete with cudnn. \r\n\r\nTo compile the tensorflow library I added avx instructions set. \r\n\r\nTo build the custom op I'm using gcc as described in the tutorial and so I don't have a project with bazel for the custom op. \r\n\r\nHope these information could be useful for the moment. If you need more specific info I will try to respond as soon as I can. \r\n\r\nThank you for the support. ", "> Thank you for the support.\r\n\r\nI'm glad to be of service. But please note for future reference that support is community-driven on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow). That is a more appropriate venue for issues like these. We try to keep this issue tracker limited to bugs and feature requests.\r\n\r\nThat said, you're most likely forgetting to link one of the TensorFlow shared objects into your program. Wild guess but try saying `from tensorflow.python import pywrap_tensorflow` before you call `tf.load_op_library`.\r\n\r\nAlso, I strongly recommend using Bazel. I don't know why the documentation says to use gcc. Basically there's a directory called [tensorflow/user_ops](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/user_ops) that shows you how to do what you want to do. You can customize that directory to your heart's content.\r\n\r\nIf this doesn't solve your issue, let me know and I'll reopen this.", "I will switch to bazel but basically I followed these instructions :\r\n\r\nhttps://www.tensorflow.org/versions/r0.11/how_tos/adding_an_op/#building_the_op_library\r\n\r\nI wrote here because the problem happens only with the binary package provided on the official website and I didn't have any problems with the package compiled from source. For future similar cases I will use stackoverflow. \r\n\r\nThank you for the response. ", "Thank you for clarifying. So this only happens if you try to use something in stringpiece.h with the binary release, but compiling TensorFlow from source on your computer works fine. So maybe this is an ABI compatibility type issue. Maybe you're using a different version of GCC than what we used to build the release.\r\n\r\nHey @mrry would we consider something like this to be a bug? Does `undefined symbol: _ZN10tensorflow9LogMemory21RecordRawDeallocationERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEExPvPNS_9AllocatorEb` mean anything to you?", "Yes it looks like the extension and the binary package use a different definition of `std::string`. The demangled name from the error message expands to:\r\n\r\n    tensorflow::LogMemory::RecordRawDeallocation(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long long, void*, tensorflow::Allocator*, bool)\r\n\r\nWhen I look for the corresponding symbol in the binary package I get the following:\r\n\r\n```\r\n$ objdump -t lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so | grep RecordRawDealloc | c++filt\r\n0000000001f68110 g     F .text  00000000000003a3              tensorflow::LogMemory::RecordRawDeallocation(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, long long, void*, tensorflow::Allocator*, bool)\r\n```\r\n\r\nNote the difference between `std::__cxx11::basic_string<...>` in the extension and `std::basic_string<...>` in the binary package.\r\n\r\nThere are a few workarounds, which seem to involve involve defining `_GLIBCXX_USE_CXX11_ABI to `0` when you compile your extension. See [this Stack Overflow answer](http://stackoverflow.com/a/33395489/3574081) for details.", "I already compiled the extension with 'D_GLIBCXX_USE_CXX11_ABI=0' parameter but I have to try the suggested define inside my sources. I will do it soon but I think the problem is perfectly explained. \r\n\r\nIs it possible to add that suggestion inside the documentation? \r\n\r\nThank you so much for the support, please be patient for a response, I can't try the fix right now. ", "I just took a look at [the docs](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/adding_an_op/index.md), and apparently there is a note buried in there:\r\n\r\n> Note on gcc version 5: gcc5 uses the new C++\r\n> [ABI](https://gcc.gnu.org/gcc-5/changes.html#libstdcxx). The binary pip packages\r\n> available on the TensorFlow website are built with gcc4 that uses the older ABI.\r\n> If you compile your op library with gcc5, add `-D_GLIBCXX_USE_CXX11_ABI=0` to\r\n> the command line to make the library compatible with the older abi.\r\n\r\nIf there's something you'd like to improve there, please feel free to submit a PR!", "I solved the problem with the tip you gave me but I want to explain better what is the situation:\r\n\r\nI have a new op for TensorFlow and I used gcc to compile it and I already use the flag mentioned above, the one you have in the note of the documentation, but I forgot it for some libraries and so I had the mismatch of the functions.\r\n\r\nWith the correct build of the op (using -D_GLIBCXX_USE_CXX11_ABI=0) I had similar problems because I use a TensorFlow python package builded from sources, with bazel. Building from source without the option `-cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"` caused the same problem with a different function.\r\n\r\nFor the moment I don't have the project of the new op that uses the bazel toolchain, so I compiled again the TensorFlow package from sources with that option and all went well.\r\n\r\nIf I can guess a conclusion, working with the sources of TF and building the new op from there have no problem, despite the use or not of `-cxxopt=\"-D_GLIBCXX_USE_CXX11_ABI=0\"`. Otherwise if you have a compiled TF from sources and you want to use the **gcc** build option is better to work with the same ABI.\r\n\r\nI do not know if it is appropriate add a note on the building from source with that flag, to remain aligned with the official package. This is only a misunderstanding because you can choose to compile the new op with different options.\r\n\r\nI'm sorry for the delay but I made some tests to be sure about the situation.", "Thank you for providing more useful information for future people googling this issue. If you want to contribute to the documentation, we would be happy to review any pull requests you would be generous enough to contribute.", "I will try to write something for the documentations. Thank you for the support!"]}, {"number": 6472, "title": "Timeout does not work with session created with server", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem? None\r\n\r\n### Environment info\r\nOperating System: Ubuntu\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): CPU only\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)  \r\n\r\nc32663d119a4cbd9d4926d4eed2a0705f03a57f9\r\n\r\n2. The output of `bazel version` \r\n\r\nExtracting Bazel installation...\r\nBuild label: 0.4.3\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Dec 22 12:31:25 2016 (1482409885)\r\nBuild timestamp: 1482409885\r\nBuild timestamp as int: 1482409885\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nfrom datetime import datetime\r\nimport os.path\r\nimport time\r\n\r\nimport numpy as np\r\nimport random\r\nimport tensorflow as tf\r\n\r\nimport signal\r\nimport sys\r\nimport os\r\n\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import variables\r\nfrom tensorflow.python.ops import state_ops\r\nfrom tensorflow.python.ops import logging_ops\r\nfrom tensorflow.python.client import timeline\r\n\r\nfrom tensorflow.python.ops import data_flow_ops\r\n\r\n# (python test 1 worker > /tmp/output 2>&1 &) && (python test 0 ps > /tmp/output2 2>&1 &) && python test 0 worker\r\n\r\ntry:\r\n\r\n    index, job = int(sys.argv[1]), sys.argv[2]\r\n\r\n\r\n    ps_hosts = 'localhost:1234'\r\n    worker_hosts = 'localhost:1235,localhost:1236'\r\n    ps_hosts = ps_hosts.split(\",\")\r\n    worker_hosts = worker_hosts.split(\",\")\r\n\r\n    cluster_spec = tf.train.ClusterSpec({'ps' : ps_hosts, 'worker' : worker_hosts})\r\n    server = tf.train.Server(\r\n\t{'ps': ps_hosts,\r\n         'worker': worker_hosts},\r\n\tjob_name=job,\r\n\ttask_index=index)\r\n\r\n    a = data_flow_ops.FIFOQueue(-1, tf.float32)\r\n    b = a.dequeue()\r\n    sv = tf.train.Supervisor(is_chief=True, logdir='./test_logdir')\r\n    cfg = tf.ConfigProto(\r\n\tallow_soft_placement=True,\r\n\tlog_device_placement=False)\r\n    sess = sv.prepare_or_wait_for_session(server.target, config=cfg)\r\n    print(\"Testing timeout\")\r\n    opt = tf.RunOptions(timeout_in_ms=50)\r\n    sess.run(b, options=opt)\r\nexcept tf.errors.DeadlineExceededError:\r\n    print(\"Successfully timed out!\")\r\n```\r\n\r\nAbove I create a cluster and get a session from the server. I then try to dequeue an empty FIFOQueue with a timeout of 1 second. For some reason, on the latest commits of the tensorflow source, this does not successfully time out. This bug does not happen for the released versions of tensorflow (.11, .12).\r\n\r\nTo run:\r\n`(python script.py 1 worker > /tmp/output 2>&1 &) && (python script.py 0 ps > /tmp/output2 2>&1 &) && python script.py 0 worker`\r\n\r\nwhere script.py is the above example.", "comments": ["This looks like a bug in the (recently added) in-process shim for contacting the master without performing a full RPC. I'm testing a fix right now.", "This should be fixed at HEAD now that #6530 is merged. See e1362157e33a6fd6a66caefd083bd4afc27dc030 for the actual fix."]}, {"number": 6471, "title": "how to custom learning rate decay policy ?", "body": "as it shown in the learning rate deay **[API doc](https://www.tensorflow.org/api_docs/python/train/decaying_the_learning_rate)** , there are only 4 kinds of policies. but in caffe  policies are as follows:  \r\n \r\n1.   //    - fixed: always return base_lr.  \r\n2.   //    - step: return base_lr * gamma ^ (floor(iter / step))  \r\n3.   //    - exp: return base_lr * gamma ^ iter  \r\n4.   //    - inv: return base_lr * (1 + gamma * iter) ^ (- power)  \r\n5.   //    - multistep: similar to step but it allows non uniform steps defined by  \r\n6.   //      stepvalue  \r\n7.   //    - poly: the effective learning rate follows a polynomial decay, to be  \r\n8.   //      zero by the max_iter. return base_lr (1 - iter/max_iter) ^ (power)  \r\n9.   //    - sigmoid: the effective learning rate follows a sigmod decay  \r\n\r\nin issue #2922 they mentioNed on write a warpper for the existing method, but they did not implemented anything new ?  for example the **inv** `[base_lr * (1 + gamma * iter) ^ (- power)]` policy in caffe.\r\n\r\nso I wonder if there anyone who can help me out how to custom learning rate decay policy by myself, any suggestions will be sincerely gratitude. \r\n\r\n", "comments": ["This question is best asked on Stack Overflow which is geared toward helping users of TensorFlow with usage questions. GitHub is concerned mainly with bugs and feature requests. Thanks!"]}, {"number": 6470, "title": "how to use two optimizer in one model", "body": "I want to build a model that can use different optimizers to train different parts, just like the tf.contrib.learn.DNNLinearCombinedClassifier(), what should I do?\r\nthanks\r\n", "comments": ["I think this kind question might be better to be asked on stackoverflow.com but anyway I think something like this should work:\r\n\r\n`_, l1 = sess.run([optim1, loss1], feed_dict={input:batch_input})`\r\n`_, l2 = sess.run([optim2, loss2], feed_dict={input:batch_input})`", "Please follow up on [Stackoverflow](http://stackoverflow.com/questions/tagged/tensorflow) if this didn't solve your problem.", "One example of using two optimizers in [stackoverflow](https://stackoverflow.com/questions/34945554/how-to-set-layer-wise-learning-rate-in-tensorflow)"]}, {"number": 6469, "title": "ptb_rnn_lm.py error:tensorflow.python.framework.errors.InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420] ", "body": "Getting the error:\r\ntensorflow.python.framework.errors.InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]\r\n\r\nThe code is available here:\r\n[http://pastebin.com/FM6M43w7](http://pastebin.com/FM6M43w7)\r\n\r\nI am giving the full error log here:\r\n\r\nTraceback (most recent call last):\r\n  File \"ptb_word_lm.py\", line 364, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"ptb_word_lm.py\", line 350, in main\r\n    verbose=True)\r\n  File \"ptb_word_lm.py\", line 279, in run_epoch\r\n    vals = session.run(fetches, feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]\r\n\t [[Node: Train/Model/sequence_loss_by_example/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Train/Model/add, Train/Model/sequence_loss_by_example/Reshape/_129)]]\r\n\t [[Node: Train/Model/truediv/_153 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1023_Train/Model/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'Train/Model/sequence_loss_by_example/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\r\n  File \"ptb_word_lm.py\", line 364, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"ptb_word_lm.py\", line 326, in main\r\n    m = PTBModel(is_training=True, config=config, input_=train_input)\r\n  File \"ptb_word_lm.py\", line 147, in __init__\r\n    [tf.ones([batch_size * num_steps], dtype=data_type())])\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 1017, in sequence_loss_by_example\r\n    logit, target)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 764, in sparse_softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1857, in _sparse_softmax_cross_entropy_with_logits\r\n    features=features, labels=labels, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]\r\n\t [[Node: Train/Model/sequence_loss_by_example/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Train/Model/add, Train/Model/sequence_loss_by_example/Reshape/_129)]]\r\n\t [[Node: Train/Model/truediv/_153 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1023_Train/Model/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\n\r\n", "comments": ["The file you are suign has been moved here:\r\n`https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb`\r\n\r\nAre you sure you have all the updates that are made in that file?", "I am closing due to lack of activity but please reopen if necessary.", "Same problem here, plus a bunch of old version references like tf.contrib.rnn.BasicLSTMCell vs tf.nn.rnn_cell.BasicLSTMCell. It is an extra challenge for newbies when the tutorials are not running. ;)", "Same problems here too :) after finally solving legacy API mismatches, came into this logits issue.\r\n\r\nInterestingly it all mismatches by 20 units. It's either 420/400 when using small model, or 720/700 when using medium and large.", "Yep, same problem. Interestingly, the difference between the actual shape and mismatch shape is always the value of batch_size. Also, I printed the labels shape going into the seq2seq loss function, it is of the right shape there. I'm almost sure that the error is due to change in seq2seq code.\r\n\r\nAny help would be highly appreciated !", "Did anyone figure out a fix for this? I am also having the mismatches shape issue, off by 420 to 400. Thanks so much!", "Using code from 0.12 master branch fixed it for me.\r\n\r\nI think it is something to do with replacing. tf.strided_slice with\r\ntf.slice.\r\n\r\nEdit: Use code from here - https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/models/rnn/ptb/", "Hey, \n\n \n\nNo, unfortunately I already made the change from tf.strided_slice to tf.slice, so I don\u2019t think it\u2019s that. \u00a0Do you have any other suggestions? I really appreciate the help! \n\n \n\nAre you saying you used version 0.12.0 instead of 0.12.1? Because that would be helpful and I can just use that version perhaps. \n\n \n\nAll the best,\n\nJohn\n\n \n\nFrom: Rudraksh Tuwani <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nDate: Monday, January 23, 2017 at 9:16 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: John Larkin <john@johnjlarkin.com>, Comment <comment@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] ptb_rnn_lm.py error:tensorflow.python.framework.errors.InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420] (#6469)\n\n \n\nUsing code from 0.12 master branch fixed it for me.\n\nI think it is something to do with replacing. tf.strided_slice with\ntf.slice.\n\nOn 23-Jan-2017 7:35 PM, \"John Larkin\" <notifications@github.com> wrote:\n\n> Did anyone figure out a fix for this? I am also having the mismatches\n> shape issue, off by 420 to 400. Thanks so much!\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6469#issuecomment-274495625>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APnrjBIr98CCAiNLMNMpNlOqH65Q3Kqhks5rVLOogaJpZM4LUnZK>\n> .\n>\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n", "Sorry for not being clear enough.\r\n\r\nI meant use the tutorial code from 0.12 branch if you're using tf 0.12. or\r\nabove. \r\nhttps://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/models/rnn/ptb/\r\n\r\n\r\n\r\nI had replaced the  tf.strided_slice with tf.slice as well, but I guess\r\nthere are some additional changes to be made .\r\n\r\nOn 23-Jan-2017 8:03 PM, \"John Larkin\" <notifications@github.com> wrote:\r\n\r\n> Hey,\r\n>\r\n>\r\n>\r\n> No, unfortunately I already made the change from tf.strided_slice to\r\n> tf.slice, so I don\u2019t think it\u2019s that.  Do you have any other suggestions? I\r\n> really appreciate the help!\r\n>\r\n>\r\n>\r\n> Are you saying you used version 0.12.0 instead of 0.12.1? Because that\r\n> would be helpful and I can just use that version perhaps.\r\n>\r\n>\r\n>\r\n> All the best,\r\n>\r\n> John\r\n>\r\n>\r\n>\r\n> From: Rudraksh Tuwani <notifications@github.com>\r\n> Reply-To: tensorflow/tensorflow <reply@reply.github.com>\r\n> Date: Monday, January 23, 2017 at 9:16 AM\r\n> To: tensorflow/tensorflow <tensorflow@noreply.github.com>\r\n> Cc: John Larkin <john@johnjlarkin.com>, Comment <\r\n> comment@noreply.github.com>\r\n> Subject: Re: [tensorflow/tensorflow] ptb_rnn_lm.py error:tensorflow.python.\r\n> framework.errors.InvalidArgumentError: logits and labels must have the\r\n> same first dimension, got logits shape [400,10000] and labels shape [420]\r\n> (#6469)\r\n>\r\n>\r\n>\r\n> Using code from 0.12 master branch fixed it for me.\r\n>\r\n> I think it is something to do with replacing. tf.strided_slice with\r\n> tf.slice.\r\n>\r\n> On 23-Jan-2017 7:35 PM, \"John Larkin\" <notifications@github.com> wrote:\r\n>\r\n> > Did anyone figure out a fix for this? I am also having the mismatches\r\n> > shape issue, off by 420 to 400. Thanks so much!\r\n> >\r\n> > \u2014\r\n> > You are receiving this because you commented.\r\n> > Reply to this email directly, view it on GitHub\r\n> > <https://github.com/tensorflow/tensorflow/issues/\r\n> 6469#issuecomment-274495625>,\r\n> > or mute the thread\r\n> > <https://github.com/notifications/unsubscribe-auth/\r\n> APnrjBIr98CCAiNLMNMpNlOqH65Q3Kqhks5rVLOogaJpZM4LUnZK>\r\n> > .\r\n> >\r\n>\r\n>\r\n> \u2014\r\n> You are receiving this because you commented.\r\n> Reply to this email directly, view it on GitHub, or mute the thread.\r\n>\r\n> \u2014\r\n> You are receiving this because you commented.\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/tensorflow/tensorflow/issues/6469#issuecomment-274502258>,\r\n> or mute the thread\r\n> <https://github.com/notifications/unsubscribe-auth/APnrjJyV_XhwUHE7eZsHQ2mehft3-BJKks5rVLpWgaJpZM4LUnZK>\r\n> .\r\n>\r\n", "Oh no you\u2019re absolutely perfect! Ok I will try that. That helps a bunch. What version is the master currently on? It seems weird to me that they wouldn\u2019t have it be the most up to date version? \n\n \n\nThanks again! I really appreciate it.\n\n \n\nJohn\n\n \n\nFrom: Rudraksh Tuwani <notifications@github.com>\nReply-To: tensorflow/tensorflow <reply@reply.github.com>\nDate: Monday, January 23, 2017 at 9:39 AM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: John Larkin <john@johnjlarkin.com>, Comment <comment@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] ptb_rnn_lm.py error:tensorflow.python.framework.errors.InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420] (#6469)\n\n \n\nSorry for not being clear enough.\n\nI meant use the tutorial code from 0.12 branch if you're using tf 0.12. or\nabove.\n\nI had replaced the tf.strided_slice with tf.slice as well, but I guess\nthere are some additional changes to be made .\n\nOn 23-Jan-2017 8:03 PM, \"John Larkin\" <notifications@github.com> wrote:\n\n> Hey,\n>\n>\n>\n> No, unfortunately I already made the change from tf.strided_slice to\n> tf.slice, so I don\u2019t think it\u2019s that. Do you have any other suggestions? I\n> really appreciate the help!\n>\n>\n>\n> Are you saying you used version 0.12.0 instead of 0.12.1? Because that\n> would be helpful and I can just use that version perhaps.\n>\n>\n>\n> All the best,\n>\n> John\n>\n>\n>\n> From: Rudraksh Tuwani <notifications@github.com>\n> Reply-To: tensorflow/tensorflow <reply@reply.github.com>\n> Date: Monday, January 23, 2017 at 9:16 AM\n> To: tensorflow/tensorflow <tensorflow@noreply.github.com>\n> Cc: John Larkin <john@johnjlarkin.com>, Comment <\n> comment@noreply.github.com>\n> Subject: Re: [tensorflow/tensorflow] ptb_rnn_lm.py error:tensorflow.python.\n> framework.errors.InvalidArgumentError: logits and labels must have the\n> same first dimension, got logits shape [400,10000] and labels shape [420]\n> (#6469)\n>\n>\n>\n> Using code from 0.12 master branch fixed it for me.\n>\n> I think it is something to do with replacing. tf.strided_slice with\n> tf.slice.\n>\n> On 23-Jan-2017 7:35 PM, \"John Larkin\" <notifications@github.com> wrote:\n>\n> > Did anyone figure out a fix for this? I am also having the mismatches\n> > shape issue, off by 420 to 400. Thanks so much!\n> >\n> > \u2014\n> > You are receiving this because you commented.\n> > Reply to this email directly, view it on GitHub\n> > <https://github.com/tensorflow/tensorflow/issues/\n> 6469#issuecomment-274495625>,\n> > or mute the thread\n> > <https://github.com/notifications/unsubscribe-auth/\n> APnrjBIr98CCAiNLMNMpNlOqH65Q3Kqhks5rVLOogaJpZM4LUnZK>\n> > .\n> >\n>\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/6469#issuecomment-274502258>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/APnrjJyV_XhwUHE7eZsHQ2mehft3-BJKks5rVLpWgaJpZM4LUnZK>\n> .\n>\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n", "No problem!\r\n\r\nLet me know if it works for you. I'm  working on making a  simpler\r\nimplementation of the same on tensorflow  as  an exercise. I can send you\r\nthat if the current one doesn't work.", "Hi, I am using r.12.0 and I switched to branch r.12.0 as suggested, this saved me from the loads of trivialities encountered previously but the 420x400 dimension prob persists, would be great if anyone can help!", "Sorry for the bit of self promotion. Here's the link to my modified implementation of the same. I haven't changed it around much besides adding some extra comments and functions for getting predictions from user input.\r\n\r\nhttps://github.com/RudrakshTuwani/Neural_Networks/blob/master/Language%20Model.ipynb"]}, {"number": 6468, "title": "AttributeError: 'module' object has no attribute 'legacy_seq2seq'", "body": "I am running the ptb_rnn_lm.py. The tensorflow version is 0.12. The error is occurring in the line:\r\n\r\nloss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\r\n        [logits],\r\n        [tf.reshape(input_.targets, [-1])],\r\n        [tf.ones([batch_size * num_steps], dtype=data_type())])\r\n\r\nThe entire error log is:\r\n\r\nTraceback (most recent call last):\r\n  File \"ptb_word_lm.py\", line 364, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"ptb_word_lm.py\", line 326, in main\r\n    m = PTBModel(is_training=True, config=config, input_=train_input)\r\n  File \"ptb_word_lm.py\", line 144, in __init__\r\n    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 35, in __getattr__\r\n    return getattr(contrib, item)\r\nAttributeError: 'module' object has no attribute 'legacy_seq2seq'\r\n\r\nPls help soon !!\r\n", "comments": ["I even downloaded and manually copied the legacy_sec2sec folder into the contrib folder. Unfortunately this didnt work either. Pls help !!", "I have worked this out. I am using tf.nn.seq2seq.sequence_loss_by_example.This is working.", "It seemed like its working but it may not. Getting a new error, which may be connected to this change:\r\n\r\nraceback (most recent call last):\r\nFile \"ptb_word_lm.py\", line 364, in\r\ntf.app.run()\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\nsys.exit(main(sys.argv[:1] + flags_passthrough))\r\nFile \"ptb_word_lm.py\", line 350, in main\r\nverbose=True)\r\nFile \"ptb_word_lm.py\", line 279, in run_epoch\r\nvals = session.run(fetches, feed_dict)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 717, in run\r\nrun_metadata_ptr)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 915, in _run\r\nfeed_dict_string, options, run_metadata)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\ntarget_list, options, run_metadata)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]\r\n[[Node: Train/Model/sequence_loss_by_example/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Train/Model/add, Train/Model/sequence_loss_by_example/Reshape/_129)]]\r\n[[Node: Train/Model/truediv/_153 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1023_Train/Model/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\r\n\r\nCaused by op u'Train/Model/sequence_loss_by_example/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\r\nFile \"ptb_word_lm.py\", line 364, in\r\ntf.app.run()\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\nsys.exit(main(sys.argv[:1] + flags_passthrough))\r\nFile \"ptb_word_lm.py\", line 326, in main\r\nm = PTBModel(is_training=True, config=config, input_=train_input)\r\nFile \"ptb_word_lm.py\", line 147, in init\r\n[tf.ones([batch_size * num_steps], dtype=data_type())])\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/seq2seq.py\", line 1017, in sequence_loss_by_example\r\nlogit, target)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 764, in sparse_softmax_cross_entropy_with_logits\r\nprecise_logits, labels, name=name)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1857, in _sparse_softmax_cross_entropy_with_logits\r\nfeatures=features, labels=labels, name=name)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\nop_def=op_def)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\noriginal_op=self._default_original_op, op_def=op_def)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1298, in init\r\nself._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [400,10000] and labels shape [420]\r\n[[Node: Train/Model/sequence_loss_by_example/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](Train/Model/add, Train/Model/sequence_loss_by_example/Reshape/_129)]]\r\n[[Node: Train/Model/truediv/_153 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1023_Train/Model/truediv\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]", "I had the same problem as you, applied your change and now it works fine, so I don't think its about that line(model is still training i will update when it finishes)", "mlad3n I am sharing my code please please take a look:\r\n\r\n[http://pastebin.com/FM6M43w7](http://pastebin.com/FM6M43w7)\r\n\r\nThe error is as I stated above. I am new to tensorflow.", "My code:\r\nhttp://pastebin.com/6BSp8qhX\r\n\r\nI'm running it with python 3.5", "mlad3n, I tried it on python 2.7. I just changed the concat_v2 to concat, with the order of arguments changed as suggested [here](https://github.com/tensorflow/tensorflow/issues/6345). But the same issue I told before is coming up. Canu pls share ur reader.py code too? As the reader.py I had didnt contain ptb_producer. I manually added that function in my code. It would be a big help if u share the reader.py code. Thanks in advance !! Besides which version of tensorflow are u using?", "i tried mlad3n's code on python 3.5, the same exception of mismatched shape occured as sharod, i used the reader.py as given in the master branch.", "try change slice back into strided_slice would fix the problem.", "The file you are using has been moved here:\r\n`https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb`\r\n\r\nAre you sure you have all the updates that are made in that file?", "I assume tensorflow/models #824 will fix this but please respond to this issue once that PR lands, if it doesn't.", "Closing due to inactivity. I think the models repository was updated to fix this issue, too."]}, {"number": 6467, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 6466, "title": "CudnnLSTM dropout takes no effect", "body": "My environment is: Tensorflow 0.11.0rc2, in unbuntu 16.04, cuda8.0, cudnn5.1, GPU is GTX1080.\r\n\r\nI am using the CudnnLSTM from tensorflow.contrib.cudnn_rnn package. I found that the dropout setting in CudnnLSTM seems take no effect, and I checked that there is no test for dropout in op unit test. So I write a simple code to test it, the code is below:\r\n \r\n    import tensorflow as tf\r\n    from tensorflow.contrib.cudnn_rnn import CudnnLSTM\r\n\r\n    class Cudnn_model():\r\n      def __init__(self,dropout):\r\n        self.model = CudnnLSTM(\r\n            num_layers = 1,\r\n            num_units = 8,\r\n            input_size = 8,\r\n            input_mode = \"skip_input\",\r\n            direction = \"unidirectional\",\r\n            dropout = dropout,\r\n            )\r\n\r\n        params_size_t = self.model.params_size()\r\n        self.params = tf.Variable(tf.ones([params_size_t]), validate_shape=False)\r\n\r\n      def run_step(self,rnn_inputs):\r\n        outputs, output_h, output_c = self.model(\r\n                    input_data = rnn_inputs,\r\n                    input_h = tf.zeros([1,1,8]),\r\n                    input_c = tf.zeros([1,1,8]),\r\n                    params = self.params,\r\n                    is_training= True\r\n                   )\r\n        self.outputs = outputs\r\n        return outputs\r\n\r\n    def main():\r\n\r\n    inputs = tf.pack([[[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0]]])\r\n    m1 = Cudnn_model(dropout = 0.0)\r\n    output1 = m1.run_step(inputs)\r\n    m2 = Cudnn_model(dropout = 0.5)\r\n    output2 = m2.run_step(inputs)\r\n    output3 = tf.nn.dropout(output1,0.5)\r\n    output4 = m1.run_step(tf.nn.dropout(inputs,0.5))\r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    config.gpu_options.allow_growth = True\r\n    sess = tf.Session(config=config)\r\n    sess.run(tf.initialize_all_variables())\r\n\r\n    for i in range(5):\r\n        out1,out2,out3,out4 = sess.run([output1,output2,output3,output4])\r\n        print \" ----- Try time %d -----\" % i\r\n        print \"cndnn_dropout=0 : \", out1\r\n        print \"cudnn_dropout=0.5 : \", out2\r\n        print \"tf_out_dropout=0.5 : \", out3\r\n        print \"tf_in_dropout=0.5 : \", out4\r\n    return\r\n\r\nAnd the result is\uff1a\r\n\r\n    ----- Try time 0 -----\r\n    cndnn_dropout=0 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    cudnn_dropout=0.5 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    tf_out_dropout=0.5 [[[ 1.2165668   0.          0.          0.          0.          1.52103424\r\n    1.52239561  1.52289677]]]\r\n    tf_in_dropout=0.5 [[[ 0.6082834   0.6082834   0.6082834   0.76119781  0.6082834   0.76158684\r\n    0.6082834   0.6082834 ]]]\r\n\r\n     ----- Try time 1 -----\r\n    cndnn_dropout=0 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    cudnn_dropout=0.5 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    tf_out_dropout=0.5 [[[ 0.  0.  0.  0.  0.  0.  0.  0.]]]\r\n    tf_in_dropout=0.5 [[[ 0.6082834   0.74009657  0.6082834   0.76119781  0.6082834   0.76158684\r\n    0.6082834   0.6082834 ]]]\r\n\r\n     ----- Try time 2 -----\r\n    cndnn_dropout=0 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    cudnn_dropout=0.5 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    tf_out_dropout=0.5 [[[ 1.2165668   0.          0.          1.50730526  1.51733613  1.52103424\r\n    1.52239561  0.        ]]]\r\n    tf_in_dropout=0.5 [[[ 0.6082834   0.74009657  0.6082834   0.76119781  0.6082834   0.76158684\r\n    0.6082834   0.761594  ]]]\r\n\r\n     ----- Try time 3 -----\r\n    cndnn_dropout=0 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    cudnn_dropout=0.5 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    tf_out_dropout=0.5 [[[ 0.          0.          1.48019314  0.          0.          0.\r\n    1.52239561  1.52289677]]]\r\n    tf_in_dropout=0.5 [[[ 0.6082834   0.6082834   0.6082834   0.76119781  0.76154053  0.6082834\r\n    0.76159316  0.761594  ]]]\r\n\r\n     ----- Try time 4 -----\r\n    cndnn_dropout=0 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    cudnn_dropout=0.5 [[[ 0.6082834   0.70377535  0.74009657  0.75365263  0.75866807  0.76051712\r\n    0.76119781  0.76144838]]]\r\n    tf_out_dropout=0.5 [[[ 0.          1.40755069  0.          1.50730526  1.51733613  0.\r\n    1.52239561  1.52289677]]]\r\n    tf_in_dropout=0.5 [[[ 0.6082834   0.74009657  0.75866807  0.76119781  0.6082834   0.76158684\r\n    0.76159316  0.6082834 ]]]\r\n\r\nFrom the result I see that the cudnn_dropout = 0.5 takes no effect, the result is always same with cudnn_dropout = 0.0. ", "comments": ["@robotnc Sorry for my late response; I was on vacation. Yes, dropout is not supported yet, see [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc#L701).\r\n\r\nAdding @zheng-xq ", "If we do get dropout support, can we get _recurrent_ dropout, as that's the form that seems to actually help. Similar to what's in [LayerNormBasicLSTMCell](https://github.com/tensorflow/tensorflow/blob/cb17d1b0e2b581b5da1d9597b7929ba764749d38/tensorflow/contrib/rnn/python/ops/rnn_cell.py).", "Any updates on this? Not having any dropout support for cuDNN-based RNNs seems really limiting, especially considering how much further ahead PyTorch support is for this. This isn't exactly a new or rarely used feature.", "What's actually involved in adding (input) dropout support? Is it just wiring the places in [this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc#L701) which are marked with `/*dropout*/`?", "Hi all\uff1a\n\nI tried the FusedBlockLSTM in tf.contrib.rnn in Tensorflow V1.0, the speed is almost same with CudnnLSTM. And the FusedBlockLSTM can support Dropout, and is easier to use.\n\n\n> \u5728 2017\u5e744\u670822\u65e5\uff0c\u4e0a\u53485:36\uff0cMohammed AlQuraishi <notifications@github.com> \u5199\u9053\uff1a\n> \n> What's actually involved in adding (input) dropout support? Is it just wiring the places in this file <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc#L701> which are marked with /*dropout*/?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/6466#issuecomment-296310827>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ARpmyx4oomxYmgcQB-PXvAat4tW0Uhvbks5rySFTgaJpZM4LUiQt>.\n> \n\n", "My experience is very different. I still see a 3x gap between FusedBlockLSTM and CudnnLSTM, so I would still think this is very useful to have. And the issue is that the feature is present as an option, but it doesn't actually do anything. So it's very misleading as it is.", "\nWhat\u2019s your tensorflow version?  I found the new V1.0 is faster than V0.1x\n\n> \u5728 2017\u5e744\u670823\u65e5\uff0c\u4e0b\u53488:52\uff0cMohammed AlQuraishi <notifications@github.com> \u5199\u9053\uff1a\n> \n> My experience is very different. I still see a 3x gap between FusedBlockLSTM and CudnnLSTM, so I would still think this is very useful to have. And the issue is that the feature is present as an option, but it doesn't actually do anything.\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/6466#issuecomment-296441256>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ARpmyyzaGa3VGqyFWY561JLgDVMC6n1Iks5ry0mkgaJpZM4LUiQt>.\n> \n\n", "TF 1.1rc2. And like I mentioned there's not just a performance difference but a bona fide bug, because the CudnnLSTM API exposes a dropout option that does not do anything. If you don't mind reopen the ticket. Otherwise I'll start a new ticket.\r\n\r\nFYI this is on a Pascal Titan X with a bidirectional LSTM of 800 units (each way) and 700 timesteps.", "The dropout is still needed, reopen again.", "CudnnRNN dropout  change is submitted, please keep an eye on the nightly builds.", "@protoget has this been resolved?", "@protoget I see your commit last May, but I just tried again with TF 1.4.0rc0 and dropout still doesn't seem to do anything.", "@skye @alquraishi \r\nThe dropout is supported. \r\nDropout is applied between layers -- if you only have one layer, it doesn't show up even if dropout ratio isn't zero."]}, {"number": 6465, "title": "typo 'unit8' instead of 'uint8'", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Thanks for the fix, but we need the CLA to be signed.  Please let us know if you can sign it and we'll reopen.", "I signed it! Hope to contribute more substantially in the future. THanks", "Can one of the admins verify this patch?", "Can you double check the cla info?", "(you probably have to redo the git commit email)", "![image](https://cloud.githubusercontent.com/assets/1515904/21467269/65dddf16-c9e7-11e6-941e-314c529ad42f.png)\r\n\r\nI don't understand 'redo the git commit email'. I've only used the github.com web interface for making the patch. Do I have to open a new issue and we close this?", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "PR merged. Thank you, @velaia !"]}, {"number": 6464, "title": "Merge r0.12 back into master.", "body": "Also update docs.", "comments": ["Only doc changes. Merging."]}, {"number": 6463, "title": "Cherrypick doc updates to r0.12 branch.", "body": "", "comments": []}, {"number": 6462, "title": "Write rules to construct headers for jpeg-turbo using existing templates.", "body": "Fixes #6442 \r\nTest build:\r\nhttp://ci.tensorflow.org/view/Experimental/job/exp-win-bzl/8/console\r\n\r\nHad to use a few hacks to get this fully working.\r\nBazel did not allow me to add a `dict` type with a selector.\r\nIt even did not let me use a selector in `expand_template`'s substitutions field.\r\nSo I expanded the selector in genrule to accomodate simd/nosimd differences.\r\n\r\nI think I can also get rid of my `expand_template.bzl` , and use `expand_header_template` in `llvm.bzl`.\r\nBut I am not sure how nice cross 3p usage. Maybe I can create a folder `third_party/skylark` and pull the skylark rules we are defining to that folder. Or simply `third_party/common.bzl`\r\nPlease let me know which looks nicer.", "comments": ["All regular tests seem to have passed.\r\nIn windows, build has succeeded, but due to prolonged red build, some test failures are there.", "Done, now the expand template macro is under third_party:common.bzl\r\nPTAL.", "All changes done.\r\nPending test results to merge."]}, {"number": 6461, "title": "Branch 142778020", "body": "", "comments": []}, {"number": 6460, "title": "Slow Adam sparse updates in distributed TF", "body": "I am trying to train a model with the **tf.nn.embedding_lookup_sparse** operation. Small example: https://gist.github.com/Bobrosoft98/2d639d3924dfbc4ec7bc620fd5a4d480\r\n\r\nWhen I run this code with NUM_WORKERS = 1, the output is as follows\r\n\r\n```\r\n0 calc in 0.0176000595093\r\n0 apply in 0.184364080429\r\n0 calc in 0.0167639255524\r\n0 apply in 0.189659118652\r\n...\r\n```\r\n\r\nHowever, when I increase the number of workers to 30, every single process works more than 30 times slower:\r\n\r\n```\r\n6 calc in 0.432843923569\r\n11 calc in 0.787642002106\r\n3 calc in 0.440953016281\r\n14 calc in 0.377243995667\r\n20 calc in 0.569782018661\r\n...\r\n6 apply in 5.63959908485\r\n```\r\n\r\nThe CPU load is only ~50%, so there is a lot of resources available for the computation. This makes me think that sparse updates use locking, even though the **use_locking** flag is set to False by default. There is no such problem with other optimizers (I tried GradientDescentOptimizer and AdadeltaOptimizer). Also if I exclude sparse operations from the graph (commented lines), the problem disappears.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/464 - there was mentioned that Adam was slower on sparse updates in general\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: I used the CPU version\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.12.0-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\n0.12.0-rc1\r\n```", "comments": ["@theweiho can you comment?", "This seems to be more a sparse tensor/Adam interaction issue than an embedding issue. @concretevitamin - any idea about the sparse updates part?\r\n\r\nOr @vrv for general performance issues - or any idea who might know more about the Adam optimizer?", "+cc @mrry in case this could be solved by your recent optimizations on distributed runtime performance!", "I tried profiling the code as described [here](http://stackoverflow.com/a/37774470) and got interesting results.\r\nThis is what I get with GradientDescentOptimizer and 30 workers:\r\n<img width=\"1010\" alt=\"2017-01-11 21 22 17\" src=\"https://cloud.githubusercontent.com/assets/9952727/21861086/6a3148f8-d844-11e6-89b9-a1bdef990be9.png\">\r\nThis is AdamOptimizer with 30 workers\r\n<img width=\"1048\" alt=\"2017-01-11 21 22 55\" src=\"https://cloud.githubusercontent.com/assets/9952727/21861026/30a21568-d844-11e6-8933-329dc9e7f143.png\">\r\nAnd this is Adam with 1 worker\r\n<img width=\"908\" alt=\"2017-01-11 21 23 03\" src=\"https://cloud.githubusercontent.com/assets/9952727/21861027/30a45508-d844-11e6-9bd2-5a7149ca7bcb.png\">\r\nSo all the expensive operations are running on the PS.", "@concretevitamin Thanks for the hopefulness, but alas I don't think any of my recent optimizations will make this better.\r\n\r\n@Bobrosoft98 What is your CPU utilization like on the PS machine when running Adam with one worker? From a quick glance it looks like the ops duration might be dilating because of contention for the threadpools (in particular the intra-op parallelism) that do parallel work in a TF server. If 30 workers all attempts to send their gradients at approximately the same time, there will be 30x the amount of work to do at the PS.\r\n\r\nOne possible mitigation would be to use `tf.train.SyncReplicasOptimizer` to wrap the Adam optimizer. This would have the benefit that all of the gradients from the different workers would be aggregated (relatively cheaply) before the Adam update rule is evaluated. This should cut down on the amount of work, compared to an asynchronous setup that processes the incoming batches separately. (It's not a free lunch... the synchronous replication is more vulnerable to stragglers, and a little more tricky to set up... but it might be worth it for your case.)", "@mrry I run everything on a single machine, the CPU load with one worker is the same as with 30 workers (about 50% on all cores).\r\nStill, it's not clear for me why Adam has this problem, and other optimizers don't. What do mean these huge \"Mul\" and \"Assign\" blocks on the timeline? Why assigning something takes so much time?\r\nDoes Adam in TF update the whole embedding matrix at every iteration? This seems unreasonable: only a small number of its rows can be used in a batch.", "It seems to me that a problem is adam not working properly with sparse updates. Although it is said that \r\n>     Note that in dense implement of this algorithm, m_t, v_t and variable will \r\n>     update even if g is zero, but in sparse implement, m_t, v_t and variable \r\n>     will not update in iterations g is zero\r\nthe code of **_apply_sparse** looks like it is updating the whole matrix.", "I also met the same problem when using Adam optimizer solving a model involving big embedding matrix. Compared to Adagrad or Adadelta, Adam works 10 times slower. When I doubled the vocabulary size, the time consumption for Adam also doubled...", "There is a post in Tensorflow mail list that solves this problem: https://groups.google.com/a/tensorflow.org/forum/#!searchin/discuss/adam$20sparse/discuss/6GvpMz8kb-U/FaBAJkbvEQAJ\r\n", "LazyAdamOptimizer is supposed to make it faster -- https://github.com/tensorflow/tensorflow/commit/819a690d", "SparseAdam in pytorch is same as Tensorflow LazyAdam. The convergence of LazyAdam is lower than Adam.  The main problem is if the gradient is 0, we don't update m, and v. \r\nActually, m and v should be updated when other weights are updated.  This is why LazyAdam convergence is low.\r\nI have proposed a method to fix this issue. The performance is about the same as LazyAdam, and the convergence is the same as Adam.\r\nCould you please review my method: \r\nhttps://github.com/tensorflow/tensorflow/issues/23668\r\n"]}, {"number": 6459, "title": "cuda autoconf issues for cuda rpm install", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nNone\r\n\r\n### Environment info\r\nOperating System:\r\nFedora 25\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nHere's the problem.  CUDA is 8.0, and cuDNN is 5.1.5, but that's not necessarily relevant.\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n1cea4458efe1fc177436a6c04b3f804cc41214d7\r\n\r\n2. The output of `bazel version`\r\nBuild label: 0.4.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Dec 7 18:47:11 2016 (1481136431)\r\nBuild timestamp: 1481136431\r\nBuild timestamp as int: 1481136431\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nThe problem, such as it is, is that I installed cuda using the rpms provided by Nvidia.  These place cuda under /usr.  The big thing, building from source is that cudnn.h is now under /usr/include/cuda, as is the CUPTI directory.  There may be other differences.\r\n\r\n### What other attempted solutions have you tried?\r\nSince I was just trying to build the android demos, I ran configure without configuring cuda.  That let me build the android demos.  The pip install for tensorflow cuda works OK for my host.  It's more of a cosmetic thing since a few symlinks will get past it.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["We only have official support for ubuntu linux distribution.\r\nIf you, or anyone else would like to contribute fixes/improvements, we will be happy to review and accept.\r\n", "Automatically closing due to lack of recent activity. We hope that you were able to resolve it on your own. However, since this is a support issue rather than a bug or feature request, you will probably get more information by posting it on StackOverflow."]}, {"number": 6458, "title": "BasicLSTMCell problem", "body": "In the ptb_rnn_lm.py I have replaced tf.contrib.rnn.BasicLSTMCell by tf.nn.rnn_cell.BasicLSTMCell as suggested in [https://github.com/tensorflow/tensorflow/issues/6432](url) but now a new error is being shown:\r\n\r\nTraceback (most recent call last):\r\nFile \"ptb_word_lm.py\", line 362, in\r\ntf.app.run()\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\r\nsys.exit(main(sys.argv[:1] + flags_passthrough))\r\nFile \"ptb_word_lm.py\", line 324, in main\r\nm = PTBModel(is_training=True, config=config, input_=train_input)\r\nFile \"ptb_word_lm.py\", line 110, in init\r\nself._initial_state = cell.zero_state(batch_size, data_type())\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell.py\", line 166, in zero_state\r\nfor s in state_size_flat]\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell.py\", line 79, in _state_size_with_prefix\r\nresult_state_size = tensor_shape.as_shape(state_size).as_list()\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 815, in as_shape\r\nreturn TensorShape(shape)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 448, in init\r\nself._dims = [as_dimension(dims)]\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 374, in as_dimension\r\nreturn Dimension(value)\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 33, in init\r\nself._value = int(value)\r\nTypeError: int() argument must be a string or a number, not 'BasicLSTMCell'\r\n\r\nPls help soon !!", "comments": ["As stated in #6432 yours seems to be an issue with `ptb_word_lm.py`\r\n\r\nPlease, next time include clearer reproduction instructions, as we cannot help anything we cannot reproduce.\r\n\r\n@nealwu could you take a look at this, I tink you recently moved this model?", "Ok, I solved this problem. I had changed something in the code and had mistyped a few things. Thanks for the quick help."]}, {"number": 6457, "title": "Name clash with inspect.py", "body": "Ubuntu 14.04, TF r0.12\r\n\r\nI named a script `inspect.py` and then launched TF in the same folder. It crashes. \r\n\r\nReproducer: \r\n----------------\r\nIn terminal:\r\n```\r\ncd\r\ntouch inspect.py\r\npython -c \"import tensorflow as tf\"\r\n```\r\nIf I remove `inspect.py` it works again.\r\n", "comments": ["\r\nInspect is a system module used by TF's `remove_undocumented` to dynamically modify visible members, and on Anaconda 3.5 Python I also get this error because `~` is the first entry on sys.path, so anything you put there will shadow system modules of the same name. One work-around is to move the ~ path to the end of `sys.path`, ie\r\n\r\n```\r\nimport sys\r\nsys.path.append(sys.path[0])\r\ndel sys.path[0]\r\nimport tensorflow\r\n```\r\n\r\nBelow is original stack trace.\r\n```\r\n from tensorflow.python import *\r\n  File \"/Users/yaroslav/anaconda/envs/tf12-cpu/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 71, in <module>\r\n    from tensorflow.python.framework.framework_lib import *\r\n  File \"/Users/yaroslav/anaconda/envs/tf12-cpu/lib/python3.5/site-packages/tensorflow/python/framework/framework_lib.py\", line 73, in <module>\r\n    from tensorflow.python.framework.ops import Graph\r\n  File \"/Users/yaroslav/anaconda/envs/tf12-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 41, in <module>\r\n    from tensorflow.python.framework import registry\r\n  File \"/Users/yaroslav/anaconda/envs/tf12-cpu/lib/python3.5/site-packages/tensorflow/python/framework/registry.py\", line 28, in <module>\r\n    from tensorflow.python.platform import tf_logging as logging\r\n  File \"/Users/yaroslav/anaconda/envs/tf12-cpu/lib/python3.5/site-packages/tensorflow/python/platform/tf_logging.py\", line 249, in <module>\r\n    remove_undocumented(__name__, _allowed_symbols)\r\n  File \"/Users/yaroslav/anaconda/envs/tf12-cpu/lib/python3.5/site-packages/tensorflow/python/util/all_util.py\", line 102, in remove_undocumented\r\n    should_have = make_all(module_name, doc_string_modules)\r\n  File \"/Users/yaroslav/anaconda/envs/tf12-cpu/lib/python3.5/site-packages/tensorflow/python/util/all_util.py\", line 48, in make_all\r\n    in _inspect.getmembers(_sys.modules[module_name])])\r\nAttributeError: module 'inspect' has no attribute 'getmembers'\r\n```", "Closing since this seems python-related and not specific to TF."]}, {"number": 6456, "title": "tensorflow.python.framework.errors.InvalidArgumentError", "body": "I getting error when using  cifar10 model to train my tfrecord datasets  which are made with tf.python_io.TFRecordWriter().\r\n\r\non tensorflow0.8 ,it's running ok, but on tensorflow 0.10 and 0.11 ,there is the error : tensorflow.python.framework.errors.InvalidArgumentError\r\n\r\nFile \"/Users/yang/Documents/cifar10/cifar10_train.py\", line 133, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/yang/Documents/cifar10/cifar10_train.py\", line 129, in main\r\n    train()\r\n  File \"/Users/yang/Documents/cifar10/cifar10_train.py\", line 99, in train\r\n    _, loss_value = sess.run([train_op, loss])\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: Received a label value of 246 which is outside the valid range of [0, 20).  Label values: 10 104 185 191 178 50 116 159 192 201 173 173 81 246 110 116 153 10 114 193 167 208 173 225 97 113 84 82 179 101 90 190 107 208 155 229 239 163 28 71 173 192 200 178 83 125 238 146 213 70 34 121 129 33 207 124 157 70 117 147 80 30 153 231 156 63 130 147 143 205 86 60 97 90 202 94 127 91 191 127 123 199 201 69 220 185 152 175 86 121 60 132 73 109 100 163 218 162 201 202 40 108 63 116 195 105 124 195 107 96 86 152 58 141 166 28 55 135 128 49 180 119 237 38 59 189 202 66\r\n\t [[Node: cross_entropy_per_example/cross_entropy_per_example = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax_linear/softmax_linear, Cast_4)]]\r\n\r\nCaused by op u'cross_entropy_per_example/cross_entropy_per_example', defined at:\r\n  File \"/Users/yang/Documents/cifar10/cifar10_train.py\", line 133, in <module>\r\n    tf.app.run()\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/yang/Documents/cifar10/cifar10_train.py\", line 129, in main\r\n    train()\r\n  File \"/Users/yang/Documents/cifar10/cifar10_train.py\", line 72, in train\r\n    loss = cifar10.loss(logits, labels)\r\n  File \"/Users/yang/Documents/cifar10/cifar10.py\", line 286, in loss\r\n    logits, labels, name='cross_entropy_per_example')\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 764, in sparse_softmax_cross_entropy_with_logits\r\n    precise_logits, labels, name=name)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1857, in _sparse_softmax_cross_entropy_with_logits\r\n    features=features, labels=labels, name=name)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 748, in apply_op\r\n    op_def=op_def)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Received a label value of 246 which is outside the valid range of [0, 20).  Label values: 10 104 185 191 178 50 116 159 192 201 173 173 81 246 110 116 153 10 114 193 167 208 173 225 97 113 84 82 179 101 90 190 107 208 155 229 239 163 28 71 173 192 200 178 83 125 238 146 213 70 34 121 129 33 207 124 157 70 117 147 80 30 153 231 156 63 130 147 143 205 86 60 97 90 202 94 127 91 191 127 123 199 201 69 220 185 152 175 86 121 60 132 73 109 100 163 218 162 201 202 40 108 63 116 195 105 124 195 107 96 86 152 58 141 166 28 55 135 128 49 180 119 237 38 59 189 202 66\r\n\t [[Node: cross_entropy_per_example/cross_entropy_per_example = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT64, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](softmax_linear/softmax_linear, Cast_4)]]", "comments": ["@concretevitamin, is this possibly related to the new checpkpoint format?", "@aselle doesn't look like so to me.", "I didn't run the program from checkpoint. Instead, I run it from scratch. But I still get the same error. Anyone has any idea about it?", "any update on the error pls ?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "It looks like you're passing invalid values to sparse_softmax_cross_entropy_with_logits"]}, {"number": 6455, "title": "gradient for tf.gather_nd", "body": "I use tf.gather_nd to build a Value Iteration Block(NIPS2016 best paper, value iteration network). \r\nI need to pick several \"pixels\" (what the input (i,j) points to in the following picture) in the feature map Q_n.\r\n![image](https://cloud.githubusercontent.com/assets/6497205/21421213/a8e6a8be-c86c-11e6-8801-714ca47c4c2f.png)\r\n\r\n\r\nBut I got this error: NotImplementedError: Gradient for gather_nd is not implemented.\r\nI know gradient for tf.gather is implemented, but tf.gather could only slice the first dimentsion. It should be easy to transfer the gradient of tf.gather to tf.gather_nd.\r\nOr anyone could tell me how to implement this operation and train this?", "comments": ["This is a question better suited to stack overflow, please post it there and tag it with 'tensorflow'\r\n\r\nAlso, GatherNd does have a gradient as of November 2nd (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L355)  so you're probably using an older version of TensorFlow"]}, {"number": 6454, "title": "TypeError: zeros_initializer() got multiple values for keyword argument 'dtype'", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/models/issues/672 \r\n\r\n### Environment info\r\nOperating System: Ubuntu 16\r\n\r\nInstalled version of CUDA and cuDNN:  NO\r\nThe output of `bazel version`  0.4.2\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nExport a model for serving: Detailed steps at https://medium.com/osldev-blog/tensorflow-serving-practical-introduction-9ce29ccd63f#.8ul5tqwxe \r\nCommand:\r\nbazel-bin/tensorflow_serving/example/inception_export --checkpoint_dir=inception-v3 --export_dir=inception-export\r\n\r\n### Logs or other output that would be helpful\r\nWARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\r\nWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\nTraceback (most recent call last):\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/tf_serving/tensorflow_serving/example/inception_export.py\", line 169, in <module>\r\n    tf.app.run()\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/tf_serving/tensorflow_serving/example/inception_export.py\", line 165, in main\r\n    export()\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/tf_serving/tensorflow_serving/example/inception_export.py\", line 79, in export\r\n    logits, _ = inception_model.inference(images, NUM_CLASSES + 1)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/inception_model.py\", line 87, in inference\r\n    scope=scope)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/inception_model.py\", line 87, in inception_v3\r\n    scope='conv0')\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/scopes.py\", line 155, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/ops.py\", line 234, in conv2d\r\n    outputs = batch_norm(conv, **batch_norm_params)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/scopes.py\", line 155, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/ops.py\", line 90, in batch_norm\r\n    restore=restore)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/scopes.py\", line 155, in func_with_args\r\n    return func(*args, **current_args)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/inception_model/inception/slim/variables.py\", line 289, in variable\r\n    trainable=trainable, collections=collections)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 1063, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 889, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 347, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 332, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 683, in _get_single_variable\r\n    validate_shape=validate_shape)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variables.py\", line 225, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variables.py\", line 322, in _init_from_args\r\n    initial_value(), name=\"initial_value\", dtype=dtype)\r\n  File \"/serving/bazel-bin/tensorflow_serving/example/inception_export.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 672, in <lambda>\r\n    shape.as_list(), dtype=dtype, partition_info=partition_info)\r\nTypeError: zeros_initializer() got multiple values for keyword argument 'dtype'\r\n\r\n\r\n", "comments": ["#6202", "Did @yaroslavvb's referring issue solve your problem. If so, I will close this as a dupe of that one.\r\n", "I am also experiencing this issue.  The referring issue was not able to solve it for me", "Anyone have a solution for this? I'm experiencing the same issue while exporting v3 weights.", "Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!\r\n\r\nNote: The API for `zeros_iniailizer` was updated in recent TensorFlow versions.  Much example code is written to previous TensorFlow API versions, so it is very important that you include this information.  Specifically, issues like this are very unlikely to be a TensorFlow bug, but simply out of date example / model code used with a newer version of TensorFlow.\r\n\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 6453, "title": "better visualization name for mnist_with_summaries.py", "body": "**Before the modification**, the name showed in tensorboard:\r\n\r\n![before](https://cloud.githubusercontent.com/assets/16507370/21415270/11bd23c4-c843-11e6-96d4-ab5e8a76b425.png)\r\n\r\n\r\n**After the modification**:\r\n\r\n![after](https://cloud.githubusercontent.com/assets/16507370/21415229/9e755314-c842-11e6-8290-702f056bb3bd.png)\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Thanks for the contribution! As an example, it's probably better to keep the existing name for code understandability, even though it clashes with the scope above it."]}, {"number": 6452, "title": "Nvidia GTX 960: Abnormal Memory Usage", "body": "### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n-rw-r--r-- 1 root root   558720 Dec 17 21:39 libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Dec 17 21:39 libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Dec 17 21:39 libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Dec 17 21:39 libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Dec 17 21:39 libcudart_static.a\r\nlrwxrwxrwx 1 root root       13 Dec 18 16:34 libcudnn.so -> libcudnn.so.5\r\nlrwxrwxrwx 1 root root       17 Dec 18 16:34 libcudnn.so.5 -> libcudnn.so.5.1.5\r\n-rwxr-xr-x 1 root root 78065952 Dec 18 16:34 libcudnn.so.5.0.5\r\n-rwxr-xr-x 1 root root 79337624 Dec 18 16:34 libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Dec 18 16:34 libcudnn_static.a\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nJust any neural network model. Strangely enough CIFAR functioned fine.\r\nI used images of size 1750 * 1750.\r\n### What other attempted solutions have you tried?\r\n\r\nI ran it on CPU and there was no issue at all other than the slow training speed.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\nUnfortunately because the memory leak crashes my computer within seconds, I can only give a rough example. It roughly says\" GPU has failed to allocate ~~ around 8 GB of ram. The GTX 960 on my machine only has 4 GB of ram. I'm not sure why 8 GB was suddenly gobbled up at once.\r\n", "comments": ["Could you provide a specific model command line that did not work well, just so some concreate example to try and reproduce. Thanks!", "The main and process portion should contain a workable example. If you download the code, all you need is to provide images in a format similar to CIFAR10 but with 1750 * 1750 images. Before running main.py, you may want to reconfigure the directory names, as they are for my personal PC(I should fix it).\r\n\r\nThe code can be found here:\r\n\r\nhttps://github.com/StructML/Neural-Network-Prostate", "If you are not using tcmalloc, you should definitely try that first. It also might be the case that you are not using queues and instead trying to load all data on the GPU? How many images are you loading into memory. 8 GB of GPU memory would allow you to load 233 images (i.e. 1750*1750*sizeof(float)*3 bytes per image). Also, please reask this on stack overflow since it involves your custom code rather than a supported demo.", "Closing due to lack of recent activity. We will reopen when additional information becomes available. Thanks!", "(A change that makes jemalloc the default allocator that may help with this problem will be released soon.) @jhseu, when will this be available?", "Note that it won't help for GPU memory allocation.\r\n\r\nMy guess is that you're using a large batch size. If you do the calculation @aselle mentioned, you can estimate a lower bound for memory usage."]}]