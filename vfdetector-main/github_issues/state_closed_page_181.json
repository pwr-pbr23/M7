[{"number": 49327, "title": "Handle a special grappler case resulting in crash.", "body": "It might happen that a malformed input could be used to trick Grappler into trying to optimize a node with no inputs. This, in turn, would produce a null pointer dereference and a segfault.\n\nPiperOrigin-RevId: 369242852\nChange-Id: I2e5cbe7aec243d34a6d60220ac8ac9b16f136f6b", "comments": []}, {"number": 49326, "title": "Handle a special grappler case resulting in crash.", "body": "It might happen that a malformed input could be used to trick Grappler into trying to optimize a node with no inputs. This, in turn, would produce a null pointer dereference and a segfault.\n\nPiperOrigin-RevId: 369242852\nChange-Id: I2e5cbe7aec243d34a6d60220ac8ac9b16f136f6b", "comments": []}, {"number": 49325, "title": "Validate `MatrixDiagV{2,3}` arguments to prevent breakage.", "body": "PiperOrigin-RevId: 369056033\nChange-Id: Ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42", "comments": []}, {"number": 49324, "title": "Validate `MatrixDiagV{2,3}` arguments to prevent breakage.", "body": "PiperOrigin-RevId: 369056033\nChange-Id: Ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42", "comments": []}, {"number": 49323, "title": "Validate `MatrixDiagV{2,3}` arguments to prevent breakage.", "body": "PiperOrigin-RevId: 369056033\nChange-Id: Ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42", "comments": []}, {"number": 49322, "title": "Validate `MatrixDiagV{2,3}` arguments to prevent breakage.", "body": "PiperOrigin-RevId: 369056033\nChange-Id: Ic2018c297d3dd6f252dc1dd3667f1ed5cb1eaa42", "comments": []}, {"number": 49321, "title": "Fix `tf.raw_ops.ResourceCountUpTo` null pointer dereference.", "body": "PiperOrigin-RevId: 368294347\nChange-Id: I2c16fbfc9b4966c402c3d8e311f0d665a9c852d8", "comments": []}, {"number": 49320, "title": "Fix `tf.raw_ops.ResourceCountUpTo` null pointer dereference.", "body": "PiperOrigin-RevId: 368294347\nChange-Id: I2c16fbfc9b4966c402c3d8e311f0d665a9c852d8", "comments": []}, {"number": 49319, "title": "Fix `tf.raw_ops.ResourceCountUpTo` null pointer dereference.", "body": "PiperOrigin-RevId: 368294347\nChange-Id: I2c16fbfc9b4966c402c3d8e311f0d665a9c852d8", "comments": []}, {"number": 49318, "title": "Fix `tf.raw_ops.ResourceCountUpTo` null pointer dereference.", "body": "PiperOrigin-RevId: 368294347\nChange-Id: I2c16fbfc9b4966c402c3d8e311f0d665a9c852d8", "comments": []}, {"number": 49317, "title": "Remove duplicated entry of Env::Default", "body": "While checking duplicated symbols in .so/dll, noticed that\r\nboth `_xla_ops.so` and `_pywrap_tensorflow_internal.so` consists\r\nof Env::Default symbols exported.\r\n\r\nThis PR removed the duplication in `_xla_ops.so`.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 49316, "title": "Fix segfaults in `tf.raw_ops.SparseCountSparseOutput`.", "body": "PiperOrigin-RevId: 360547563\nChange-Id: I781c7af4b54a63d867c6e18d43a44d64a5c4e7c9", "comments": []}, {"number": 49315, "title": "Fix segfaults in `tf.raw_ops.SparseCountSparseOutput`.", "body": "PiperOrigin-RevId: 360547563\nChange-Id: I781c7af4b54a63d867c6e18d43a44d64a5c4e7c9", "comments": []}, {"number": 49314, "title": "Skip hdfs if -config=nohdfs is passed to bazel", "body": "While trying to build tensorflow locally, noticed that hdfs is linked\r\neven with -config=nohdfs. The issue comes form an incorrect options\r\nwithin build_config.bzl.\r\n\r\nThis PR fixes the issue.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mihaimaruseac I have updated the PR to removed the unneeded options in select (I think):\r\n```\r\nselect({\r\n        clean_dep(\"//tensorflow:no_hdfs_support\"): [],\r\n        \"//conditions:default\": [\r\n            clean_dep(\"//tensorflow/core/platform/hadoop:hadoop_file_system\"),\r\n        ],\r\n    }) \r\n```\r\n\r\nLet me know if this breaks anything. I will drop the change if this is the case."]}, {"number": 49313, "title": "[TFLite] Support GPU buffer binding with C API", "body": "The binding model's Input/Output with MTLBuffer/OpenGL SSBO is considerably effective for performance. But it is only available with C++ API, it will be great if we can use this feature with C API and eventually, Swift and Java.\r\n\r\nhttps://www.tensorflow.org/lite/performance/gpu_advanced#inputoutput_buffers_ios_c_api_only\r\n\r\nThis PR allows us to call `TFLGpuDelegateBindMetalBufferToTensor (Metal Delegate)` and `TfLiteGpuDelegateBindBufferToTensor (GL Delegate)` from the C API.\r\n", "comments": ["@impjdi could you review this PR?", "Hey @asus4 , can you provide some details how:\r\n\r\n1. How you use these APIs in your code? (A code snippet of inference would be good)\r\n2. What is the timeline for your project?\r\n\r\nInternally, we are also looking into some of these buffer interop use-cases across all delegates, and we hope to land something by next quarter. Since these APIs are experimental I don't mind landing them sooner, I just want to ensure that your direction isn't totally different from ours.", "Great to hear that you are planning this. I was testing to bind the OpenGL Buffer to the GPU Delegate V2 in [this branch](https://github.com/asus4/tensorflow/tree/bindbuffer-gpudelegate).\r\n\r\nI've been using TFLite C-API from Unity and I was hoping to use this feature as a lot of the process in Unity is on the GPU. \r\n\r\n1.  [inference code snippet in Unity C#](https://github.com/asus4/tf-lite-unity-sample/blob/26e49bf4a45a550f84f12635a97102a3e207009e/Assets/Samples/GpuBind/GpuBindSample.cs#L89-L132)\r\n\r\n2. There is no specific timeline in my project (AR + ML project), but I've [got issues regarding this issue.](https://github.com/asus4/tf-lite-unity-sample/issues/23)", "@asus4, @srjoglekar246  Any update on this PR? Please. Thanks!", "@asus4  Can you please check @yyoon's, @srjoglekar246's comments and keep us posted ? Thanks!", "@srjoglekar246 Here's a snippet of the output binding on Metal.\r\n\r\n```mm\r\n// Create the model and interpreter options.\r\nTfLiteModel* model = TfLiteModelCreateFromFile(\"/path/to/model.tflite\");\r\nTfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();\r\n\r\n// Create the metal delegate\r\nTfLiteDelegate* delegate = TFLGpuDelegateCreate(TFLGpuDelegateOptionsDefault());\r\nTfLiteInterpreterOptionsAddDelegate(options, delegate);\r\nTfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);\r\n\r\n// Allocate tensors and populate the input tensor data.\r\nTfLiteInterpreterAllocateTensors(interpreter);\r\nTfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\nTfLiteTensorCopyFromBuffer(input_tensor, input.data(), input.size() * sizeof(float));\r\n\r\n// Create metal buffer and bind to the output tensor[0].\r\nint channels = 4; // Shold be multiple of 4 on Metal GPU buffer.\r\nid<MTLBuffer> output_buffer = [self createMetalBufferWithWidth:width height:height channels:channels];\r\nint output_tensor_index = TfLiteInterpreterGetOutputTensorIndex(interpreter, 0);\r\nTFLGpuDelegateBindMetalBufferToTensor(delegate, output_tensor_index, output_buffer);\r\n\r\n// Interpreter doesn't copy the output data into CPU memory.\r\nTfLiteSetAllowBufferHandleOutput(interpreter, true);\r\n\r\nTfLiteInterpreterInvoke(interpreter);\r\n\r\n// Do post-process in metal buffer without copying the output to CPU memory.\r\n[self doPostProcess:output_buffer];\r\n```"]}, {"number": 49312, "title": "Resolves coredump caused by `tf.data.experimental.save` with prefetch", "body": "Repeat and prefetch in combination cause the snapshot reader Initialize function to be invoked multiple times.\r\nHowever, there is nothing to prefetch on the very last iteration. This results in Prefetch issuing a CancelThreads call while the snapshot thread is trying to initialize. See https://github.com/tensorflow/tensorflow/blob/6446dda92eaadf11d22377e2354307642d739d73/tensorflow/core/kernels/data/prefetch_dataset_op.cc#L151\r\n\r\nCurrently the dataset reference counting is done asymmetrically. The reference increment happens at the end of initialization, where as the reference decrement\r\nhappens in a destructor. When prefetch cancels the snapshot thread, it errors out of the initialization function. And stops calling the reference increment. However, the reference decrement happens regardless, as it is in the destructor which always is invoked during cleanup. This results in an attempt to decrement the null dataset pointer, and therefore a segmentation fault.\r\nThis is different from all other dataset ops, where the dataset reference increment happens in the constructor and the decrement happens in the destructor, which are symmetric.\r\n\r\nThe solution to this is to ensure that the dataset reference is always initialized to nullptr, and to check for null when decrementing the dataset reference.", "comments": ["@yangustc07 This is the cherry-pick to https://github.com/tensorflow/tensorflow/pull/49166", "Thanks, I'll add the release owner for review."]}, {"number": 49311, "title": "[ROCm] HipSparse csru2csr for ROCm 4.2+", "body": "Adding csru2csr functionality to HipSparse wrappers, inline with cuSparse implementations.\r\nIn conjunction with https://github.com/tensorflow/tensorflow/pull/49305. \r\n@cheshire  @chsigg for review. ", "comments": []}, {"number": 49310, "title": "Lifting variable on retrace", "body": "Explore the effect on tests to fix: https://github.com/tensorflow/tensorflow/issues/27120", "comments": ["@mdanatg As supposed we have 3 tests failing. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/49e6961ee63622bd2b3d7b8654b06ac8c54efd17/tensorflow/python/eager/def_function_test.py#L76-L83\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/49e6961ee63622bd2b3d7b8654b06ac8c54efd17/tensorflow/python/eager/def_function_test.py#L85-L94\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/dee93aa2d97ac276099bfe414c2911f4be2dd3c2/tensorflow/python/ops/parallel_for/control_flow_ops_test.py#L2431-L2444", "What we want to do here?", "The purpose of the PR was just to verify if by any chance it has a simple fix, right? The test results indicate that a straightforward refactoring doesn't work, so not much left to do but close the PR. A larger refactoring of the function objects is needed instead, but that's a significant project. We've been discussing it internally as well, but haven't started doing it yet.", "> The purpose of the PR was just to verify if by any chance it has a simple fix, right?\r\n\r\nPartially yes but also to find any action item on how to refactoring this. But if it is too complex or you want to talk about this internally it is ok.", "That we're discussing internally doesn't matter much. It's mainly the caution about complexity: in short, we have the polymorphic Function object doing the variable lifting, but it should be individual concrete functions who do it instead. And then we'd need to fix whatever issues come from refactoring that. Would definitely be useful to do, but not easy to pull through.", "To be clear, if you're interested in giving it a shot, it would be welcome!", "I was a little be confused by `_initialize`:\r\n\r\nAt the end of the function it seems that it is always going to raise the `ValueError`. Where is the alternative execution path of `_initialize`? ;\r\n\r\n```python\r\n    def invalid_creator_scope(*unused_args, **unused_kwds):\r\n      \"\"\"Disables variable creation.\"\"\"\r\n      raise ValueError(\r\n          \"tf.function-decorated function tried to create \"\r\n          \"variables on non-first call.\")\r\n\r\n    self._stateless_fn = self._defun_with_scope(invalid_creator_scope)\r\n```", "It will always raise an error if the function creates any variable. This is just half of the implementation - there is another branch of the code (different class, IIRC?) which doesn't have this check.\r\n\r\nLoosely speaking, there's the following logic:\r\n\r\n* 1st trace: a function is created without and invalid_creator_scope; then a second trace is immediately triggered\r\n* subsequent traces: another function is created, with invalid_creator_scope - this is the one we're looking at\r\n\r\nIt's been a while since I looked at the code and can't remember exactly what is where, but I'd start with tracking the `__call__` method of `Function` in `def_function.py` - that's the main entry point.", "@mdanatg I've added your test and hacked a little bit the `_call`. \r\nThis was just an hack to execute tests but it seems that the new test and all the other existing tests are passing.\r\n\r\nIs there something that it is not covered with existing tests?", "Hmm, if tests are passing, that's good news. I do suspect the coverage is not great, but it's still very encouraging. Let's try the following then: put all these changes behind a global flag (can just make a constant in def_function); set its value to False (disabled), and then let's merge the PR.\r\nOnce merged, I can flip the flag and run a much larger suite of tests that we can access internally.", "> I do suspect the coverage is not great,\r\n\r\nYes I had the same suspect, In any case I've submitted the constant flag.", "Just fixed a pylint long line", "Ok only `import/copybara` is on hold... let me know when you have any news.", "@bhack  One potential problem I came across is that when the flag is on, function invocation always uses the the function's `_stateful_fn`, which is slower than the stateless counterpart because it uses conditionals for each variable that's being read.\r\n\r\nI'm seeing some some training tests that use fixed random produce different results, but I'm not sure whether the test is just brittle and calling the other function is enough to break determinism or there's is something else going on.\r\n\r\nIt would be interesting to test with a small Keras Sequential model, and test (1) the performance and (2) the determinism of model.compile/model.train when given some fixed input data and a fixed random seed.", "> It would be interesting to test with a small Keras Sequential model, and test (1) the performance and (2) the determinism of model.compile/model.train when given some fixed input data and a fixed random seed.\n\nI suppose that we have many of these tests in the repo.\n\nWhere we could cherry pick one of these and add the function that you want to test?", "I think a practical option could be to set the flag from an environment variable, wait until the change gets into tf-nightly, then run some of the models in https://github.com/tensorflow/models with that version, toggling the flag to True through the env var.", "Mhh.. if you want to test perf reg on a e2e real model what is the status of the [internal benchmarks](https://github.com/tensorflow/build/issues/10)?", "@mdanatg @sganeshb Can you check internally if we could use that benchmark?", "\r\nWe have some candidate scripts for a few use cases, that we occassionally share with external collaborators.\r\nThese scripts can run the tensorflow/models benchmarks on a GPU machine.\r\n\r\nExample: https://github.com/tensorflow/benchmarks/blob/master/perfzero/dockertest/resnet50_synth.sh\r\n\r\nDo you have a model/benchmark of interest? We can setup an example script for you to try.", "@sganeshb Thank you. For this specific PR I think you can sync with @mdanatg internally.\r\n\r\nMore in general it could be nice to run a very small selection in the TF CI (/cc @mihaimaruseac) just as a canary to support PRs reviews (see https://github.com/tensorflow/build/issues/10).\r\n", "Thanks for the feedback.\r\nFor the short term, feel free to re-purpose this https://github.com/tensorflow/benchmarks/blob/master/perfzero/dockertest/resnet50_synth.sh to test out your env var on resnet50.\r\n\r\nWe'll look into providing a general test added to TF CI. We'll need to discuss internally on priorities given internal staffing and other resources for these efforts.", "Hi @bhack and @sganeshb, can you explain why the following code raises `ValueError: tf.function-decorated function tried to create variables on non-first call`? \r\n\r\n```python\r\nfrom tensorflow.python.eager import def_function  # def_function.function is the same as tf.function\r\nfrom tensorflow.python.ops import variables\r\n\r\n\r\ndef_function.ALLOW_DYNAMIC_VARIABLE_CREATION = True\r\n\r\n@def_function.function\r\ndef myfunc(val):\r\n    variables.Variable(val)\r\n\r\nmyfunc(0.3)\r\nmyfunc(0.9)\r\n```\r\nthe error occurs even when I hard-code `ALLOW_DYNAMIC_VARIABLE_CREATION` to True in my local venv's tensorflow installation\r\n\r\nCC @sjtusmartboy\r\n\r\nedit: accidentally published comment without adding code snippet", "@mdanatg Are we expecting this syntax?", "We don't, at least not for now. We're working to remove this limitation, but  we're forced to proceed slowly beacuse of the complexity of the issue, as well as the backward compatibility restrictions.\r\n\r\nOnly this syntax is expected to work:\r\n\r\n```\r\nvars = {}\r\n\r\n@def_function.function\r\ndef myfunc(val, key):\r\n    if key not in vars:\r\n      vars[key] = variables.Variable(val)\r\n```\r\n\r\nI can see why the name of the constant might be confusing, though; perhaps we should rename it.", "@mdanatg Do you will accept a failing test for this specific case like in https://github.com/tensorflow/tensorflow/pull/51374 ?", "Yep! I'm surprised we don't already have one.", "@sumanthratna I've added your use case as an expected failing test at https://github.com/tensorflow/tensorflow/pull/51538\r\nThe reference bug is always https://github.com/tensorflow/tensorflow/issues/27120"]}, {"number": 49309, "title": "tensorflow error code 3221225501", "body": "windows7 64bit \r\nwhat is the problem ?\r\n\r\nPython 3.8.6 (tags/v3.8.6:db45529, Sep 23 2020, 15:52:53) [MSC v.1927 64 bit (AM\r\nD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed with error code 3221225501 while importing _pywrap_\r\ntensorflow_internal\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <modu\r\nle>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, i\r\nn <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", l\r\nine 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed with error code 3221225501 while importing _pywrap_\r\ntensorflow_internal\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@cuneyttanrisever \r\nLooking at the error log, seems like it is similar to  this [issue](https://github.com/tensorflow/tensorflow/issues/35598) and let us know if its help.Thanks\r\n", "@cuneyttanrisever Can you give this issue a better title to make it more searchable?", "> @cuneyttanrisever\r\n> Looking at the error log, seems like it is similar to this [issue](https://github.com/tensorflow/tensorflow/issues/35598) and let us know if its help.Thanks\r\n\r\nI uploaded this link\r\nhttps://github.com/fo40225/tensorflow-windows-wheel/blob/master/1.14.0/py37/CPU/sse2/tensorflow-1.14.0-cp37-cp37m-win_amd64.whl\r\n\r\nimport tensorflow as ft \r\nthis error when the command is given:\r\nC:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureW\r\narning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a fu\r\nture version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\nC:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureW\r\narning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a fu\r\nture version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\nC:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureW\r\narning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a fu\r\nture version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\nC:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureW\r\narning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a fu\r\nture version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\nC:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureW\r\narning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a fu\r\nture version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\nC:\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureW\r\narning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a fu\r\nture version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n  \r\nthis error when the command is given:\r\n>>> model = tf.keras.Sequential([\r\n...     tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n...     tf.keras.layers.Dense(128, activation='relu'),\r\n...     tf.keras.layers.Dense(10)\r\n... ])\r\n\r\nWARNING:tensorflow:From C:\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\init\r\n_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_\r\nops) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nCall initializer instance with the dtype argument instead of passing it to the c\r\nonstructor\r\n\r\nworked as a result but why am I getting these errors\r\n\r\nThanks for your help", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@cuneyttanrisever \r\nCould you please confirm if the issue is resolved,if yes move this to close status.Thanks", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49309\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49309\">No</a>\n"]}, {"number": 49308, "title": "Fix build on AArch64", "body": "Execution platform: @local_execution_config_platform//:platform\r\nexternal/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64O0PreLegalizerCombiner.cpp:45:10: fatal error: AArch64GenO0PreLegalizeGICombiner.inc: No such file or directory\r\n   45 | #include \"AArch64GenO0PreLegalizeGICombiner.inc\"\r\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n\r\nCloses #49136", "comments": ["Thanks to @bhack ", "Thanks for the PR but I don't think to have the visibility on https://github.com/tensorflow/tensorflow/blob/3fd3ae1fbb10961dd1aa6805280674c781fd4609/third_party/llvm/llvm.autogenerated.BUILD#L3", "That's noticed. Just not found yet how to regenerate it.\r\n\r\nTF takes hours to build so this is first fix which had to check is this working fix. Next step is do it right.", "commit 04b415a8669e9c26cfefa4663113d8f857bfd8ae did the same.", "/cc @hawkinsp"]}, {"number": 49305, "title": "[ROCm] Update to use ROCm 4.2 (when building TF with --config=rocm)", "body": "/cc @cheshire @chsigg @sanjoy", "comments": []}, {"number": 49304, "title": "tf.data.experimental.service does not reach optimal consumption rate", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Using tensorflow/tensorflow:2.4.1-gpu docker image\r\n- GPU model and memory: nvidia v100 32GB\r\n\r\nCurrent behavior:\r\nTraining with `dataset = dataset.take(1).cache().repeat()` is 20% faster(iterations per sec) vs. training with the tf.data.experimental.service. Increasing the number of workers does not increase the number of iteration per second.\r\nWhen increasing the number of workers, I can see that each worker is sending less data per time, but the data received in the consumer per time stays the same(126 MB/s, which is only 10% of the network in bandwidth of the consumer). \r\n\r\nI have tried setting the `max_outstanding_requests` [distributed function](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/distribute) which did not seem to have any effect. \r\n\r\nHow can I optimize the data service?\r\n", "comments": ["@GuyPozner \r\nKindly share a colab gist or code such that we can replicate the issue reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49303, "title": "[MLIR] Add lowering of SoftsignOp and SoftsignGradOp", "body": "- Adds the lowering of TF::SoftsignGradOp from TF to HLO.\r\n- Adds the lowering of TF::SoftsignGradOp from TF to HLO.\r\n- These changes are part of `-xla-legalize-tf` pass.\r\n\r\n", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49303) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49303) for more info**.\r\n\r\n@googlebot I signed it!"]}, {"number": 49302, "title": "Cannot import tensorflow: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version: tensorflow 2.5.0\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: GCC 9.3.0\r\n- GPU model and memory: nvidia 940MX with 4GB memory\r\n\r\n\r\n\r\n**\r\nbut i failed to install it. After then that i am unable to import tensorflow even after i reinstalled tensorflow i get import error as:\r\n \r\n>>> import tensorflow\r\n2021-05-19 19:21:42.165198: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\r\n2021-05-19 19:21:42.165227: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/keshav/.local/lib/python3.8/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/home/keshav/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 48, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/home/keshav/.local/lib/python3.8/site-packages/tensorflow/python/keras/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"/home/keshav/.local/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 20, in <module>\r\n    from tensorflow.python.keras import metrics as metrics_module\r\n  File \"/home/keshav/.local/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py\", line 37, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"/home/keshav/.local/lib/python3.8/site-packages/tensorflow/python/keras/activations.py\", line 18, in <module>\r\n    from tensorflow.python.keras.layers import advanced_activations\r\n  File \"/home/keshav/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/__init__.py\", line 146, in <module>\r\n    from tensorflow.python.keras.layers.normalization import LayerNormalization\r\nImportError: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/home/keshav/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/normalization/__init__.py)\r\n\r\n**\r\n\r\n***i tried to install tensorflow on gpu and followed steps from following link and run the mentioned commands:\r\nhttps://towardsdatascience.com/installing-tensorflow-gpu-in-ubuntu-20-04-4ee3ca4cb75d\r\n$ sudo apt install nvidia-cuda-toolkit\r\n$ tar -xvzf cudnn-10.1-linux-x64-v7.6.5.32.tgz\r\n$ sudo cp cuda/include/cudnn.h /usr/lib/cuda/include/\r\n$ sudo cp cuda/lib64/libcudnn* /usr/lib/cuda/lib64/\r\n$ sudo chmod a+r /usr/lib/cuda/include/cudnn.h /usr/lib/cuda/lib64/libcudnn*\r\n$ echo 'export LD_LIBRARY_PATH=/usr/lib/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc\r\n$ echo 'export LD_LIBRARY_PATH=/usr/lib/cuda/include:$LD_LIBRARY_PATH' >> ~/.bashrc\r\n$ source ~/.bashrc\r\n$ pip install tensorflow==2.2.0\r\n\r\nafter i failed to run tensorflow by this method i tried to revert changes that i made during installation so i tried following commands\r\nsudo apt-get purge nvidia*\r\nsudo apt-get autoremove\r\nsudo apt-get autoclean\r\nsudo rm -rf /usr/local/cuda*\r\n\r\nand deleted cuda folder from usr/lib/cuda\r\n**\r\nnow i am unable to import tensorflow please help as i am short on time to complete my collage project\r\n\r\nthank you \r\n", "comments": ["Please check that you have the correct CUDA/Cudnn versions for TF 2.5. See https://github.com/tensorflow/tensorflow/issues/49175", "Also if you are working with a venv please try to clean it https://www.tensorflow.org/install/pip?hl=en#2.-create-a-virtual-environment-recommended", "@Keshav2829 ,\r\n\r\nCan you please take a look at this [link1](https://github.com/tensorflow/tensorflow/releases/tag/v2.5.0) for TF v2.5 compatible build configurations and [link2](https://github.com/tensorflow/tensorflow/issues/49017) with similar error log.It helps.\r\n\r\nThanks!\r\n", "thank for your reply @tilakrayal and @bhack \r\nas mentioned by you in comments i am not working with venv and from links that you have provided i have not found any solid solution. yes i have similar error log as mentioned in link2 but i am using ubuntu (pip)\r\n please help\r\n", "Can you try to make a 2.5.0 install with a fresh virtual env as in the official guide that I have mentioned?\nPlease verify Cuda/Cudnn version compatibility wtih 2.5.0?", " i tried with conda environment ... it works perfectly...\r\nthank you for assistance @bhack  @tilakrayal ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49302\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49302\">No</a>\n"]}, {"number": 49301, "title": "issue about feature_column with empty string feature", "body": "Hi,\r\nI am using tf1.15 with Py3.6.\r\nBelow is part of my code,\r\n```\r\nfeature_description = {\r\n    \"city\": FixedLenFeature(dtype=tf.string, shape=[], default_value=\"\")\r\n}\r\n\r\nfeature_columns = tf.feature_column.embedding_column(\r\n                tf.feature_column.categorical_column_with_hash_bucket(\r\n                   \"city\", hash_bucket_size=1  # 1 just for debug\r\n                ),\r\n                dimension=2\r\n)\r\n```\r\nAnd I generated my train samples with tfrecord format.  If some of my train samples do not contains the key \"city\", when I parse them in training, they will be filled the \"city\" key with value \"\", right? So I can train my model successfully.\r\nBut when I inference with the exported model, if the data to predict does not contain \"city\", should I feed it with \"\", or just left it empty? I check both of them, I get different results. If i left it empty, the city will get zero embedding [0,0], while non-zeros embedding for city with \"\". Is this a bug, or which is the correct way to set city value?", "comments": ["Sorry we don't support TF 1.x anymore. Please upgrade TF to see if your issue is still reproducible.", "@luckmoon \r\nTf 1.x is not actively supported, Can you please upgrade to tf 2.x and let us know if you still face the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49300, "title": "Fix 'Illegal ambiguous match'", "body": "bazel build for pip package //tensorflow/tools/pip_package:build_pip_package  on s390x gives the following error:\r\n\r\n```\r\nIllegal ambiguous match on configurable attribute \"deps\" in //tensorflow/core/common_runtime:core_cpu_internal:\r\n//tensorflow:linux_s390x\r\n//tensorflow:no_aws_support\r\nMultiple matches are not allowed unless one is unambiguously more specialized.\r\n```\r\n\r\nThis is because the select statement [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/default/build_config.bzl#L678-L686) matches both\u00a0//tensorflow:linux_s390x\u00a0and\u00a0//tensorflow:no_aws_support \r\n\r\n\r\nThis pr is to to remove `clean_dep(\"//tensorflow:no_aws_support\"): []` as AWS and HDFS support is disabled by default. ", "comments": ["@mihaimaruseac Please let us know the update", "/cc  @wdirons @jayfurmanek\r\n\r\n\r\n", "@Sidong-Wei FYI", "There seems to be an error with the license checks, can you look at the sanity build log please?\r\n\r\n```\r\nFAIL: mismatch in packaged licenses and external dependencies\r\nMissing the licenses for the following external dependencies:\r\n@aws//\r\n@aws-c-common//\r\n@aws-c-event-stream//\r\n@aws-checksums//\r\nPlease add the license(s) to //tensorflow/tools/lib_package:jnilicenses_generate.\r\n```\r\n\r\nhttps://source.cloud.google.com/results/invocations/4d313cfb-2451-4fe4-a6e8-268cf603fe0b/log", "Closing this as latest master working fine."]}, {"number": 49299, "title": "An idea of  `tf.one_hot`", "body": "According to my personal experience , the mechanism of  `tf.one_hot` is returning the value of `0,1` that match the index number as in [one_hot](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/one_hot). But it might cause an unforeseen problem that can not handle the input larger than depth like following code snippets described. What it be better if raise a warning or alert that telling the user what happend inside.\r\n\r\n### How to reproduce\r\nEnvironment: `tensorflow=1.15`\r\n```\r\n# init\r\nbatch_size = 1\r\nseq_len = 4\r\nvocab_size = 2\r\ndata_vocab_size = 6\r\n\r\ntoken_type_ids = tf.random.uniform([batch_size, seq_len], minval=0, maxval=data_vocab_size, dtype=tf.dtypes.int32)\r\n# calculate\r\nflat_token_type_ids = tf.reshape(token_type_ids, [-1])\r\none_hot_ids = tf.one_hot(flat_token_type_ids, depth=vocab_size)\r\nwith tf.Session() as sess:\r\n    np_one_hot_ids, np_flat_token_type_ids = sess.run([one_hot_ids, flat_token_type_ids])\r\n    print(np_flat_token_type_ids)\r\n    print(np_one_hot_ids)\r\n```\r\n>np_flat_token_type_ids: [1 5 3 0]\r\nnp_one_hot_ids:  \r\n[[0. 1.]\r\n [0. 0.]\r\n [0. 0.]\r\n [1. 0.]]\r\n", "comments": ["Sorry we don't support TF 1.x anymore. Please upgrade TF to the last release to verify that you can reproduce your issue.", "@zheyuye \r\nWe see that you are using tf version 1.15, Also as bhack mentioned above there is no support for 1.x, please upgrade to 2.x and let us know if you are facing the same and also refer [this](https://github.com/tensorflow/tensorflow/issues/18538) once,it helps.Thanks", "Thanks for the response, I am awared of this situation that TF 1.x is not supported anymore. I am going to close this issue, thanks again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49299\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49299\">No</a>\n"]}, {"number": 49298, "title": "Superflous keras-nightly dependency?", "body": "**Describe the problem**\r\n\r\nTF 2.5.0 added a dependency on keras-nightly for some reason with https://github.com/tensorflow/tensorflow/commit/d171d94d899cc73e18bd8b2b11b5c320ea7aac2c\r\n\r\nHowever at no place there is an actual `import keras` or similar in the TF code, hence that package is completely optional at least or superflous it seems.\r\n\r\nWhat was the reasoning for including that in the *required* dependencies of TF? Can it be removed?\r\n\r\nI'd guess usually people use tf.keras or install TF then keras (for some reason), i.e. keras depends on TF (more or less) but now there is a dependency cycle.", "comments": ["I don't know but probably it is part of the program to prepare and test the migration https://github.com/keras-team/keras#under-construction", "Oh great! Now even more confusion what \"Keras\" actually is :D\r\n\r\nWell in any case: In 2.5.0 there still is tf.keras, so what is the keras dependency for in TF 2.5.0?", "I suppose it is only to prepare/test the release infra when the standalone migration will completed. If you see the commit log Keras is now a sort of mirror/auto-sync https://github.com/keras-team/keras/commits/master", "Sure, but that still leaves the question: What is it good for in TF 2.5? Or should it be a dependency of TF at all because it creates a circular dependency, e.g. keras directly uses TF at https://github.com/keras-team/keras/commit/4a978914d2298db2c79baa4012af5ceff4a4e203#diff-b25a4ff4c4bcd43013e75cefce6cc1dbe27b94312054aa3e132bcf10973e1787R19", "If you want to have details I think that we could collect more info from @qlzh727\r\n", "Thanks for asking the question. In tf 2.5, keras package is included as a deps, but not actively used. We are in the final stage for switch tf to use keras as a dependency package (and the keras code in tensorflow/python/keras will be removed afterwards).\r\n\r\nAs for the dependency circle, there isn't any since tf shouldn't rely on any keras code at python level. There is only one way deps from Keras to TF. The keras PIP deps is added to TF so that tf.keras API could properly be exported. This is same as the tf estimator API.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49298\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49298\">No</a>\n", "> There is only one way deps from Keras to TF.\r\n\r\nSo that means that the keras package should have a dependency on the tensorflow package, shouldn't it?\r\n\r\n> The keras PIP deps is added to TF so that tf.keras API could properly be exported.\r\n\r\nI don't understand this. What does it mean to be \"properly exported\"? And why does this require keras to be installed (which is what the pip dep means)?", "> > There is only one way deps from Keras to TF.\r\n> \r\n> So that means that the keras package should have a dependency on the tensorflow package, shouldn't it?\r\n> \r\n> > The keras PIP deps is added to TF so that tf.keras API could properly be exported.\r\n> \r\n> I don't understand this. What does it mean to be \"properly exported\"? And why does this require keras to be installed (which is what the pip dep means)?\r\n\r\nI see why you get confused here.\r\n\r\nIf you thinking from API and layering perspective, for any python code, keras code can rely on TF for underlying ops and other lib, but not the other way around (tf low level ops, or high level API like tf.dataset or tf.distribute shouldn't aware of keras). In this sense, the dependency direction is from Keras to TF.\r\n\r\nWhen we talk about PIP package, the dependency is the other way. tf.keras is a namespace that contain keras APIs. When the actual keras code lives in keras-nightly PIP package, tf will need keras as a PIP dependency, so that tf.keras namespace can be properly populated. This is why keras-nightly (in future, keras PIP) is a TF PIP dependency.\r\n\r\nWe break this circle by not including TF in the Keras PIP package requirement, and only expecting user to have the proper tf PIP installed on their local env. See https://github.com/keras-team/keras/blob/master/keras/tools/pip_package/setup.py#L35, the required dependency doesn't have tf or tf-nightly listed.", ">  tf.keras is a namespace that contain keras APIs. When the actual keras code lives in keras-nightly PIP package, tf will need keras as a PIP dependency, so that tf.keras namespace can be properly populated. This is why keras-nightly (in future, keras PIP) is a TF PIP dependency.\r\n\r\nThanks for the explanation, please allow me to ask another question to be sure that I understood that correctly:\r\nThe goal is to move the current code under `tf.keras.*` into a separate (pip) package so that `tf.keras == keras`, i.e. `import tf.keras as keras` and `import keras` are essentially equivalent and the TF package does not contain the actual code of that namespace anymore. Is that correct?\r\nFrom what I see this is currently not done, i.e. TF 2.5.0 does still contain all the code under `tf.keras` and doesn't actually need `keras-nightly`, so that dependency is only a preparation for a future change removing that code, isn't it? In that case one could remove the keras dependency from 2.5.0.\r\n\r\nJust a thought here: If the goal is to have tf.keras basically be an alias to keras, then maybe it could be made optional: I.e. if a user tries to import tf.keras and keras is not installed an error is shown. Because from an engineering point of view the dependency is from keras to TF and that not being declared as such looks like a workaround to me. (see the comment in the keras setup.py, i.e. users could `pip install keras` and it won't work)\r\n\r\nAnd maybe a general question to keras because we need to teach the users at our HPC cluster on using it:\r\n\r\nPreviously I understood the following, could you please check if this (still) holds true:\r\n- \"Keras\" is an (abstract) API for machine learning (especially deep learning), i.e. it defines the interfaces\r\n- tf.keras is an implementation of that API using TF to run the operations (i.e. TF is the \"backend\")\r\n- \"keras\" (at least used to be) an implementation of the \"Keras\" API which can use different ML frameworks (e.g. TF or PyTorch, I think Theano was there too) as the backend\r\n\r\nI.e. the goal of the Keras API to me seemed to be to allow users to switch the implementations (backend) without changing their code when they code against the Keras API.\r\n\r\nWhat now confuses me is from the linked file this line: https://github.com/keras-team/keras/blob/4a978914d2298db2c79baa4012af5ceff4a4e203/keras/tools/pip_package/setup.py#L15\r\n\r\nIt says \"TensorFlow Keras.\" Does this mean that the \"keras\" package can only use TF as the backend and nothing else? So the keras (pip) package does no longer allow switching the backends? Or was there another package which did allow this?", "> > tf.keras is a namespace that contain keras APIs. When the actual keras code lives in keras-nightly PIP package, tf will need keras as a PIP dependency, so that tf.keras namespace can be properly populated. This is why keras-nightly (in future, keras PIP) is a TF PIP dependency.\r\n> \r\n> Thanks for the explanation, please allow me to ask another question to be sure that I understood that correctly:\r\n> The goal is to move the current code under `tf.keras.*` into a separate (pip) package so that `tf.keras == keras`, i.e. `import tf.keras as keras` and `import keras` are essentially equivalent and the TF package does not contain the actual code of that namespace anymore. Is that correct?\r\n\r\nThis is correct.\r\n\r\n> From what I see this is currently not done, i.e. TF 2.5.0 does still contain all the code under `tf.keras` and doesn't actually need `keras-nightly`, so that dependency is only a preparation for a future change removing that code, isn't it? In that case one could remove the keras dependency from 2.5.0.\r\n\r\nTF 2.5 is still using keras code under tensorflow/python/keras, and NOT using code from keras-nightly. You observation is correct. It is indeed a preparation. User could turn on the Keras pip package by config a env var (_PREFER_OSS_KERAS), see https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/api_template.__init__.py#L87\r\n> \r\n> Just a thought here: If the goal is to have tf.keras basically be an alias to keras, then maybe it could be made optional: I.e. if a user tries to import tf.keras and keras is not installed an error is shown. Because from an engineering point of view the dependency is from keras to TF and that not being declared as such looks like a workaround to me. (see the comment in the keras setup.py, i.e. users could `pip install keras` and it won't work)\r\n\r\nKeras couldn't be a optional deps package for tensorflow, since tf API namespace has a strong contract and we can't easily make tf.keras not available. When user \"pip install tensorflow\", we would expect keras pip to be installed as well.\r\n> \r\n> And maybe a general question to keras because we need to teach the users at our HPC cluster on using it:\r\n> \r\n> Previously I understood the following, could you please check if this (still) holds true:\r\n> \r\n> * \"Keras\" is an (abstract) API for machine learning (especially deep learning), i.e. it defines the interfaces\r\n> * tf.keras is an implementation of that API using TF to run the operations (i.e. TF is the \"backend\")\r\n> * \"keras\" (at least used to be) an implementation of the \"Keras\" API which can use different ML frameworks (e.g. TF or PyTorch, I think Theano was there too) as the backend\r\n> \r\n\r\nCurrently there isn't a difference between tf.keras and Keras, since keras package is now single backend (TF only). @fchollet will have more details.\r\n\r\n> I.e. the goal of the Keras API to me seemed to be to allow users to switch the implementations (backend) without changing their code when they code against the Keras API.\r\n> \r\n> What now confuses me is from the linked file this line: https://github.com/keras-team/keras/blob/4a978914d2298db2c79baa4012af5ceff4a4e203/keras/tools/pip_package/setup.py#L15\r\n> \r\n> It says \"TensorFlow Keras.\" Does this mean that the \"keras\" package can only use TF as the backend and nothing else? So the keras (pip) package does no longer allow switching the backends? Or was there another package which did allow this?\r\n\r\nYes, the current road map for keras is have TF as the single backend only. @fchollet \r\n", "Some of these are natural questions then soon or later we will need to put these somewhere in a visibile FAQ/Readme"]}, {"number": 49297, "title": "013057005309211", "body": "One master `const int left_x_index = in_x > 0.0 ? floorf(in_x) : 0;` so I think that we could close this\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3fd3ae1fbb10961dd1aa6805280674c781fd4609/tensorflow/core/kernels/image/resize_bilinear_op_gpu.cu.cc#L60-L63\r\nhttps://github.com/tensorflow/tensorflow/issues/38389#issuecomment-844015731\r\n_Originally posted by @bhack in https://github.com/tensorflow/tensorflow/issues/38389#issuecomment-844015731_", "comments": ["@Georgeiva ,\r\n\r\nPlease, fill issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) and also please provide more information/details to understand the issue.\r\n\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 49296, "title": "[MLIR][DISC] Integrate LLVM at llvm/llvm-project@851d02f61e945d335", "body": "Upgrade to use the new `reifyReturnTypeShapes` interface, which is more\r\nsafe to be used during dialect conversion (e.g. converting from tensor\r\nworld to buffer world).", "comments": ["851d02f61e945d335 is already integrated, can you rebase?", "By the way in general we handle the integration, if you have patches like this to help, then it is better to send them ahead of time and point them to us in the review in the LLVM repo so we can integrate these.\r\n\r\nThe reason is that we integrate LLVM/MLIR inside Google and we have a monorepo with all the project. During the integration we have to fix **all** projects at once.", "> 851d02f61e945d335 is already integrated, can you rebase?\r\n\r\n@joker-eph Hi, is this integration still internal now? It seems the latest tensorflow in github still not include the commit.  ", "> By the way in general we handle the integration, if you have patches like this to help, then it is better to send them ahead of time and point them to us in the review in the LLVM repo so we can integrate these.\r\n> \r\n> The reason is that we integrate LLVM/MLIR inside Google and we have a monorepo with all the project. During the integration we have to fix **all** projects at once.\r\n\r\nOh, I see. Thanks!", "Ah sorry I looked at the wrong dashboard, it hasn't landed yet.", "> Ah sorry I looked at the wrong dashboard, it hasn't landed yet.\r\n\r\nDo you mean the integration isn't ready external or even internal? And if it's integrated internal, how long would it take to be ready for outside access? ", "What you see on GitHub for TensorFlow is an immediate mirror of our internal mono-repository, there is no delay or manual process involved.", "> What you see on GitHub for TensorFlow is an immediate mirror of our internal mono-repository, there is no delay or manual process involved.\r\n\r\n\r\nOk. so, could you help to merge this pr since the integration isn't done yet? ", "Sorry: we **can't** possibly merge this PR since it depends on a new version of LLVM. The LLVM integration **must** come from us. This is again because TensorFlow is a mirror of our internal monorepo, and upgrading LLVM has to be done for **all** of Google projects (the ones living in our internal monorepo at least) using LLVM. We can't diverge between one project and another.\r\n\r\nWe integrate LLVM internally once or twice a day. If it does not happen it is because there are issues (bugs in LLVM, regressions, or a large number of API updates taking some time): we test all of Google code on every update before integrating it. So when TF uses a new version of LLVM it already passed all of our internal tests (both LLVM, clang, lld, lldb, and MLIR). It never takes more than 2 or 3 days to update though.", "Close this PR since it includes the llvm updates. I will separate the code changes from the llvm updates and send a new PR."]}]