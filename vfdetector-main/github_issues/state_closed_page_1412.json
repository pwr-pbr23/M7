[{"number": 10620, "title": "How to code this imaging android app from scratch? ", "body": "How can I code this imaging android app from scratch? ", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10619, "title": "Feature Request: Tensorboard visualization of what parts of a given training image were deemed as 'important' after a inception v3 has been retrained", "body": "After retraining inception v3 on my own image set, I'm running into where any new image that I ask it to recognize, that _shouldn't_ match any of the training categories, is confidently scored as one of the image categories. \r\n\r\nFor example, if I train it on two types of steering wheels that are exactly the same except one category/type has two buttons on the right. After training has finished, if I show inception an image of some other random thing, like a horse, it confidently scores it as one of the steering wheel categories! \r\n\r\nThe issue is that there is no way to get insight as to _why_ this is happening. In other words, there is no way to tell that the model has correctly learned that the real difference between the two steering wheel categories is presence or absence of the the two buttons on the right. It would be extremely useful to be able to upload an image in tensorboard and have it show you a heat map, overlaid on the image, of what specific parts of the image the model deemed important when classifying it as a particular category. \r\n\r\nTo elaborate further, if you have two categories that you retrained inception on, you would be able to upload an image, and you would see different regions of the image highlighted using two different colors (one color per category). Then you would be able to clearly see that the model has or has not correctly learned the real difference between the images. In addition, you could upload a image that should not be classified as one of the two categories and you would be able to see _why_ it incorrectly thought it was one of the two categories.\r\n![00c0c_jygsfznatmv_600x450](https://user-images.githubusercontent.com/13044220/27007360-baed3810-4e05-11e7-9e68-5e895fd5562b.jpg)\r\n![112_0905_04z-2010_toyota_prius-interior_view](https://user-images.githubusercontent.com/13044220/27007362-c6e05bc0-4e05-11e7-9280-4f40c4585545.jpg)\r\n", "comments": ["Since this is more of a machine learning research question (rather than a concrete bug or feature request), it really is better asked on StackOverflow. Thanks!"]}, {"number": 10618, "title": "crop_and_resize issue with box_index on 1.2rc2", "body": "I use crop_and_resize() in a standard way as such:\r\n```\r\npool = tf.image.crop_and_resize(features, boxes, box_indicies, pool_shape, method=\"bilinear\")\r\n```\r\n\r\nAnd it works great. Then I tried to train on multiple GPUs, and I got this error:\r\n```\r\nOutOfRangeError: box_index has values outside [0, batch_size)\r\n\t [[Node: tower_2/mask_rcnn/roi_align/CropAndResize = CropAndResize[T=DT_FLOAT, extrapolation_value=0, method=\"bilinear\", _device=\"/job:localhost/replica:0/task:0/gpu:2\"](tower_2/mask_rcnn/activation_40/Relu, tower_2/mask_rcnn/roi_align/StopGradient, tower_2/mask_rcnn/roi_align/StopGradient_1/_5871, tower_2/mask_rcnn/roi_align/CropAndResize/crop_size)]]\r\n\t [[Node: tower_5/mask_rcnn/mrcnn_mask/Reshape_1/_6389 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:5\", send_device_incarnation=1, tensor_name=\"edge_33119_tower_5/mask_rcnn/mrcnn_mask/Reshape_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n```\r\nI verified that box_index is within the correct range (all zeros in my case because I use one image per batch), and verified that all the other inputs to crop_and_resize() look good. I ended up spending almost two days checking and rechecking every line of my code to make sure I didn't make a mistake somewhere. It drove me nuts. Finally, I tried downgrading TF to 1.1 and suddenly everything worked. And to make sure this is the problem, I upgraded to 1.2rc2 again and got the error again. \r\n\r\n### A few points that might help:\r\n- On 1.1 it works whether I use 1 GPU or 8 GPUs. On 1.2rc2 it works on 1 GPU but fails on 8GPUs.\r\n- The way I do multi-GPU training is by running 8 copies of my model, one on each GPU (shared weights). My input batch size is 8 and I split the inputs by the batch dimension and feed one sample to each GPU, and then I concatenate the outputs to get a batch size of 8 again and then apply the loss function.\r\n- If you're wondering why I'm using 1.2rc2, it's because 1.1 has an issue that causes Batch normalization to run on CPU and it was fixed in 1.2rc2.\r\n\r\n### System information\r\n- Ubuntu 16.04\r\n- Python 3.5\r\n- TensorFlow 1.2.0-rc2. Installed using \r\n```\r\nsudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0rc2-cp35-cp35m-linux_x86_64.whl\r\n```\r\n- On EC2 p2.8xlarge (8 GPUs)\r\n", "comments": ["@gunan, it seems this is possibly a regression.", "@av8ramit a possible regression in 1.2. We may need to patch the release.", "any updates on this issue? I got the same error when using crop_and_resize with multiple GPUs", "@av8ramit was this issue resolved?\r\n@ChenChuang which OS/release did you run into this issue?", "@ChenChuang Glad to hear that someone else is seeing this problem as well. I still can't upgrade to 1.2 because of this issue.", "Investigating.", "@rmlarsen is this related to some of your kernel changes to crop_and_resize_op.cc?", "@gunan centos-7.2, tensorflow-1.2.0", "Is this still an issue? did it creep into 1.3 too? ", "I'm not seeing it in 1.3. At least in my case, it seems to have been fixed.", "Looks like the issue was resolved. Closing.", "I got the same error in 1.2 version when using multiple GPUs. \r\nUpdate to 1.4 then everything works fine. ", "Hi I have the same issue even when I am using tensorflow 1.4.\r\nI am doing multi-GPU inferencing and the problem happens at exactly the cropAndResize part"]}, {"number": 10617, "title": "nda", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": []}, {"number": 10616, "title": "[feature] Support S3 Filesystem for Tensorflow", "body": "At the moment Tensorflow supports distributed file system implementations such as HDFS or GCS which has been very useful for handling cases in a distributed environment. Both of them utilize Tensorflow's FileSystem C++ interface.\r\n\r\nIn case Tensorflow is used in AWS, it would be great to have a S3 Filesystem for Tensorflow as well.\r\n\r\nWe have a preliminary implementation of S3 Filesystem on Tensorflow, based on AWS's C++ SDK:\r\nhttps://github.com/tensorflow/tensorflow/compare/master...yongtang:s3\r\n\r\nThe code is placed under the directory of tensorflow/contrib/s3.\r\n\r\nAs AWS is widely used, I am just wondering if it make sense to have a pull request to add S3 File system on Tensorflow?\r\n\r\nIt will be better to have feedback from community so that we could improve our implementation and help those deploying Tensorflow on AWS.\r\n", "comments": ["@jhseu, it would be great if you could comment on this and provide guidance. @yongtang prepared a patch linked above.", "@yongtang Yes, please make a pull request with your changes! This would be a very useful feature for a lot of people.", "Closed through #11089 "]}, {"number": 10615, "title": "Add Multi-Dimentional LSTM", "body": "implementation of Multi-Dimentional LSTM described in https://www.cs.toronto.edu/~graves/nips_2008.pdf", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Hi, what's the difference between this and ndlstm?\n\nhttps://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/ndlstm/python/__init__.py\n\nOn Jun 11, 2017 9:28 AM, \"Shanqing Cai\" <notifications@github.com> wrote:\n\n> Assigned #10615 <https://github.com/tensorflow/tensorflow/pull/10615> to\n> @ebrevdo <https://github.com/ebrevdo>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/10615#event-1118575505>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7uy8WDV7J1R_wPj9TdNm9hTWP-Oks5sDBWWgaJpZM4N2LmH>\n> .\n>\n", "It splits images in blocks(rectangles). Lstm cell get states(each has his own gate) from both neighbors at once.  ndlstm just run bidirectional rnn on lines and columns (column lstm get the states from lines states as input).", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "This failed the sanity test, please try to build on your end.\r\n\r\n```\r\nERROR: /workspace/tensorflow/contrib/mdlstm/BUILD:12:1: no such target '//tensorflow/contrib:rnn_py': target 'rnn_py' not declared in package 'tensorflow/contrib' defined by /workspace/tensorflow/contrib/BUILD and referenced by '//tensorflow/contrib/mdlstm:mdlstm'.\r\nERROR: /workspace/tensorflow/contrib/mdlstm/BUILD:12:1: no such package 'tensorflow/python/framework': BUILD file not found on package path and referenced by '//tensorflow/contrib/mdlstm:mdlstm'.\r\nERROR: Analysis of target '//tensorflow/contrib/mdlstm:mdlstm' failed; build aborted.\r\nINFO: Elapsed time: 7.597s\r\n```", "can someone help me with including dependencies in the BUILD file?", "It's `//tensorflow/contrib/rnn:rnn_py` and `//tensorflow/python/core:framework` respectively.", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "what does it mean?\r\n`[Set GitHub commit status (universal)] PENDING on repos [] (sha:905bfc7) with context:tensorflow-pull-requests-cpu\r\nUnable to get pull request builder trigger!!\r\nSetting status of 954d61d3002774b373b2f698c93c27bf8d4d5ab4 to FAILURE with url https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu/5532/ and message: 'FAILURE`", "@tensorflow-jenkins test this please", "@johnsmithm The sanity check fails because of formatting errors in BUILD files and Python code:\r\n\r\n==== Summary of sanity check results ====\r\n1. do_pylint PYTHON2: Python 2 pylint\r\n  FAIL\r\n2. do_pylint PYTHON3: Python 3 pylint\r\n  FAIL\r\n3. do_buildifier: buildifier check\r\n  FAIL\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/5358/consoleFull", "thanks for information, i will fix the build as sugested and the pylint errors!", "@tensorflow-jenkins test this please", "@johnsmithm It looks like there are a few more errors to resolve. \r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/6413/", "i do not know how to add dependencies to pip_packages:\r\nMissing dependency: //tensorflow/contrib/mdlstm:mdlstm \r\nalso buildifier failed, and i do not know what to change.\r\nis there a documentation how to write BUILD file?", "@johnsmithm You should be able to add a dependence on //tensorflow/contrib/mdlstm:mdlstm to what ever target needs it in your new code.\r\n\r\nYou can format the BUILD file with the buildifier tool. There is a script to install it here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/ci_build/install/install_buildifier.sh\r\n\r\nIt might also add the missing dependency for you, but make sure it doesn't make unintended changes to other targets.", "@tensorflow-jenkins test this please", "@drpngx @ebrevdo can you help @johnsmithm fixing the missing pip dependency? Thanks.\r\n\r\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-sanity/5855/console", "I would try to add the dependency here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L155\r\nDoes this build under windows? If so, try this too:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/BUILD#L88", "@johnsmithm Any luck with this?", "@johnsmithm ping", "i still don't know how to sort the dependencies, what is the correct order.", "I'll close this PR due to inactivity. If you continue working on it, I'm happy to reopen it, or you can just start a new one."]}, {"number": 10614, "title": "Issue adding external Library (OpenCL)", "body": "Description of the problem / feature request / question:\r\n\r\nI am trying to use bazel to build TensorFlow Library. It builds fine.\r\n\r\nAdditional Feature :\r\nI would like to add OpenCL code in one of the files of TensorFlow. Added all the required code\r\nAND added the following in one of the build files (tensorflow/core/BUILD), considering 'opencl' as the root directory of opencl.\r\n\r\ncc_library(\r\nname = \"opencl\",\r\nhdrs=glob([\"opencl/include/CL/*h\"]),\r\nvisibility =[\"//visibility:public\"],\r\n)\r\n\r\ncc_library(\r\nname=\"all_kernels\" ,\r\nvisibility= [\"//visibility:public\"],\r\ncopts=tf_copts() + [\"-Ithird_party/opencl/include\"],\r\ndeps= [\r\n\"//third_party/opencl\",\r\n],\r\n\r\nIf possible, provide a minimal example to reproduce the problem:\r\n\r\nBy running\r\nbazel build //tensorflow/examples/android:tensorflow_demo --fat_apk_cpu=armeabi-v7a --copt=\"-Ithird_party/opencl/include\"\r\n\r\nIssues Faced while building :\r\nerror: undefined reference to 'clEnqueueReadBuffer'\r\nerror: undefined reference to 'clReleaseMemObject'\r\nerror: undefined reference to 'clReleaseMemObject'\r\n\r\netc\r\n\r\nEnvironment info\r\n\r\nOperating System: Ubuntu 17.04\r\n\r\nBazel version (output of bazel info release): release 0.5.1\r\n\r\nHave you found anything relevant by searching the web?\r\n\r\nhttps://stackoverflow.com/questions/37761469/how-to-add-external-header-files-during-bazel-tensorflow-build/37844376\r\n\r\nAnything else, information or logs or outputs that would be helpful?\r\n\r\nbazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo(conv_ops.o):conv_ops.cc:function matrixMul(float*, float*, int, int, int, int, int, int): error: undefined reference to 'clGetPlatformIDs'\r\nbazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo(conv_ops.o):conv_ops.cc:function matrixMul(float*, float*, int, int, int, int, int, int): error: undefined reference to 'clGetDeviceIDs'\r\nbazel-out/android-arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-fastbuild/bin/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo(conv_ops.o):conv_ops.cc:function matrixMul(float*, float*, int, int, int, int, int, int): error: undefined reference to 'clCreateContext'\r\n\r\nI tried linking directly to libOpenCL.so as shown below by referring https://bazel.build/versions/master/docs/tutorial/cpp.html#adding-dependencies-on-precompiled-libraries\r\n, but still same issue\r\n\r\ncc_library(\r\n    name = \"opencl\",\r\n    srcs = glob([\"lib/x86_64/*.so\"]),\r\n    hdrs = glob([\"include/CL/*.h\"]),\r\nvisibility = [\"//visibility:public\"],\r\n)\r\n\r\nPlease help me in resolving the issuee", "comments": ["The libOpenCL.so was red in color in terminal, which meant it was archived, replaced the file and issue is resolved", "I'm not sure entirely what you are trying to do, but are you aware of the OpenCL implementation effort discussed [here,](https://github.com/tensorflow/tensorflow/issues/22) with instructions on how to try it out [here](https://www.codeplay.com/products/computesuite/computecpp/guides/how-to-setup-tensorflow-with-computecpp)"]}, {"number": 10613, "title": "Binaries missing from the \"frontpage\"?", "body": "Hi,\r\n\r\nThe downloads for the latest RC which link to the ci domain all give me a 404 right now. I understand they may be CI builds that are somewhat superseded, but they cannot possibly be deleted so quickly, without more recent binary, can they?\r\nThanks!\r\n\r\n************************************************************\r\n\r\n\r\n\r\n\r\nPlease go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["Which links are you having problems with?\r\nWhich frontpage are you mentioning?", "https://github.com/tensorflow/tensorflow links to https://ci.tensorflow.org/view/Nightly/job/nightly-win/M=windows-gpu,PY=35/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.2.0rc2-cp35-cp35m-win_amd64.whl , for example. Not sure what to call the content of https://github.com/tensorflow/tensorflow, is there a canonical name better than \"frontpage\"?", "`www.tensorflow.org` is more commonly referred to as our frontpage, or main page.\r\n\r\nAnyway, the issue is caused by release tag update. Looks like we did not have a successful build since the latest release. Once all the build issues are resolved, the links will be back.\r\nWhen there are issues such as this, you can always click the build history and manually download  a binary from the latest successful run. In this case, if you modify the version string from `1.2.0rc2` to `1.2.0rc1`, it should work.\r\n\r\n\r\n", "All right, to my surprise anaconda seems to know where to find them anyway!"]}, {"number": 10612, "title": "Implement components for \"Self-normalizing networks\"", "body": "Hochreiter's group has recently come up with a new dropout technique and activation function in a recent paper (https://arxiv.org/abs/1706.02515). Presented experiments demonstrate that these lead to better learning in standard feed forward networks. \r\n\r\nI would like to implement these components in TensorFlow. Creating this issue to gauge the community's interest level in such components. Feedback will be extremely helpful.", "comments": ["Should \"alpha dropout\" be implemented by adding an alpha parameter to the existing dropout layer or should I implement a new layer altogether?", "The following need to be implemented in order to complete this task\r\n\r\n- [x] SELU Activation function\r\n- [ ] Alpha-dropout\r\n- [ ] The recommended weight initialization", "Can I work on Alpha dropout?\r\n", "Yeah, sure!\r\n", "Very interested in testing this! How is it comming out?", "Selu is already done. I don't think alpha dropout has been merged.", "When will this ship?", "Alpha-dropout just got merged. #11357"]}, {"number": 10611, "title": "[1.2.0 rc2] Tensorboard does not show graph on Windows 10", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nI used the following minimal example:\r\n```python\r\nimport tensorflow as tf\r\na = tf.constant(5)\r\nb = tf.constant(4)\r\nc = a+b\r\nwith tf.Session() as sess:\r\n  Writer = tf.summary.FileWriter(\"Test\", sess.graph)\r\n  Writer.close()\r\n```\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: \r\n\r\nI installed the wheel: https://ci.tensorflow.org/job/tf-master-win-gpu-cmake/lastSuccessfulBuild/artifact/cmake_build/tf_python/dist/tensorflow_gpu-1.2.0rc2-cp35-cp35m-win_amd64.whl\r\nWhich is the build \"694\" on Jenkins of the job \"tf-master-win-gpu-cmake\"\r\n\r\nOn version 1.1 (provided by pip), the graph was shown correctly. I needed to update to this version to get text-events in Tensorboard.\r\n\r\n- **TensorFlow version (use command below)**: 1.2.0-rc2 (build 694 of job \"tf-master-win-gpu-cmake\")\r\n- **Bazel version (if compiling from source)**: - \r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX680 with 4 GB\r\n- **Exact command to reproduce**:\r\n```\r\ntensorboard --logdir=Test\r\n```\r\n\r\nIf I use \"inspect\" I get the following output:\r\n\r\n```\r\ntensorboard --logdir=Test --inspect\r\n======================================================================\r\nProcessing event files... (this can take a few minutes)\r\n======================================================================\r\n\r\nFound event files in:\r\nTest\r\n\r\nThese tags are in Test:\r\naudio -\r\nhistograms -\r\nimages -\r\nscalars -\r\ntensor -\r\n======================================================================\r\n\r\nEvent statistics for Test:\r\naudio -\r\ngraph\r\n   first_step           0\r\n   last_step            0\r\n   max_step             0\r\n   min_step             0\r\n   num_steps            1\r\n   outoforder_steps     []\r\nhistograms -\r\nimages -\r\nscalars -\r\nsessionlog:checkpoint -\r\nsessionlog:start -\r\nsessionlog:stop -\r\ntensor -\r\n======================================================================\r\n\r\n```\r\nI'm using Chrome Browser for Tensorboard (Firefox does not show anything for that version).\r\n\r\n### Describe the problem\r\nNo Graph is shown on Tensorboard.\r\n\r\n### Source code / logs\r\nSee above", "comments": ["i had meet this problem too.someone can help?", "@zismylove\r\nDo you use exactly the same version of tensorflow like I did?\r\nAre you also working on Windows?\r\n\r\nMaybe this information is helpful for the devs.\r\n\r\nFurthermore there are new nightly builds available:\r\nhttps://ci.tensorflow.org/job/tf-master-win-gpu-cmake/ (698)\r\nMaybe this will work? I did not try so far...", "@Netzeband \r\nno,it don't work.", "@zismylove Thanks for testing.", "@Netzeband Can you show us what messages (if any) are printed to the console in Chrome/Windows?", "@deannalinchen It is simply the message, that no graph data was found:\r\n\r\n![nograph](https://user-images.githubusercontent.com/26524132/27052746-fc5e104c-4fba-11e7-8110-0cdc782ea423.PNG)\r\n\r\nThe strange thing is, that the graph was found by the inspect command.\r\n\r\nThanks for taking care about that issue.", "@Netzeband Can you show a screenshot of the Chrome debugger console output (as found when you use the inspect command), and show a screenshot of what you mean when you say \"the graph was found by the inspect command\"?", "@dandelionmane To be honest, I have no idea how to activate the Chrome debugger console. Can you explain it to me? If you mean the console, I can open with [Ctrl]+[Shift]+J, there is nothing inside. Do I need to activate any special mode? \r\n\r\nIn the very first post, I paste the output of the terminal, when I start tensorboard with `--inspect` argument. Here you can see, that the graph is found by tensorboard inside the event file.", " meet this problem too.", "#7856 may help you", "@kinderry Thanks for the hint with the other issue, but it seems to be a completely different topic. \r\nHere, I use a relative pathname. This means tensorboard is indeed started from the same drive, it is even started from the parent directory of the summary-dir. And furthermore all data are shown, except of the graph.", "@all\r\nGood news. I tried today the official Tensorflow 1.2 release from this night and the graph is shown correctly now. Thus this issue can be closed. \r\nThanks to all, who contributed to solve this issue.", "Try Firefox. I t worked for me ! ! !\r\nOn Chrome and IE, TensorBoard would not display Graphs, Histograms or Embeddings.\r\nHowever, FireFox 55.0.0 displayed everything.\r\nMy Chrome is at 62.0.3202.94\r\nMy IE is at 11.0x\r\nOn Windows 7\r\n\r\nBTW: This issue might me the same as #9701 ", "same problem firefox fixed it ", "I get this problem eventually. I don't know why sometimes it works and sometimes doesn't, but the work around is opening it with Firefox. \r\n\r\nI can't really understand how a Google Product like Tensorboard doesn't work on Chrome and does it on Firefox, but that's it."]}, {"number": 10610, "title": "how can convert pascal voc dataset  labels?", "body": "Hi,\r\nhow can conversioning  the pascal voc 2012 dataset and the berkely extended version.\r\nhas the tensorflow  solution for this  problem. or a link for download with prepossessing annotation.\r\n\r\nThanks.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 10609, "title": "Reshape cannot infer the missing input size for an empty tensor unless all specified", "body": "I trained faster-rcnn on coco dataset, when iter =11410, it appears a problem:\r\nimage: COCO_train2014_000000076146.jpg iter: 11410 / 200000, total loss: 3.3490, rpn_loss_cls: 0.7608, rpn_loss_box: 1.0145, loss_cls: 0.7640, loss_box: 0.8098, lr: 0.001000\r\nspeed: 0.327s / iter\r\n2017-06-08 22:59:42.143627: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: **Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero**\r\n[[Node: gradients/TopKV2_grad/Reshape = Reshape[T=DT_INT32, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](TopKV2/_131, gradients/TopKV2_grad/stack)]]\r\n2017-06-08 22:59:42.143670: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: **Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero**\r\n[[Node: gradients/TopKV2_grad/Reshape = Reshape[T=DT_INT32, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](TopKV2/_131, gradients/TopKV2_grad/stack)]]\r\n2017-06-08 22:59:42.143965: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n[[Node: gradients/TopKV2_grad/Reshape = Reshape[T=DT_INT32, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](TopKV2/_131, gradients/TopKV2_grad/stack)]]\r\n2017-06-08 22:59:42.144042: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n[[Node: gradients/TopKV2_grad/Reshape = Reshape[T=DT_INT32, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](TopKV2/_131, gradients/TopKV2_grad/stack)]]\r\n2017-06-08 22:59:42.145758: W tensorflow/core/framework/op_kernel.cc:1152] Invalid argument: Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\r\n[[Node: gradients/TopKV2_grad/Reshape = Reshape[T=DT_INT32, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](TopKV2/_131, gradients/TopKV2_grad/stack)]]\r\nTraceback (most recent call last):\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/train_net.py\", line 132, in \r\nrestore=bool(int(args.restore)))\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/../lib/fast_rcnn/train.py\", line 400, in train_net\r\nsw.train_model(sess, max_iters, restore=restore)\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/../lib/fast_rcnn/train.py\", line 255, in train_model\r\ncls_prob, bbox_pred, rois = sess.run(fetches=fetch_list, feed_dict=feed_dict)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\nrun_metadata_ptr)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\nfeed_dict_string, options, run_metadata)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\ntarget_list, options, run_metadata)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\nraise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: **Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero**\r\n[[Node: gradients/TopKV2_grad/Reshape = Reshape[T=DT_INT32, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](TopKV2/_131, gradients/TopKV2_grad/stack)]]\r\n\r\nCaused by op u'gradients/TopKV2_grad/Reshape', defined at:\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/train_net.py\", line 132, in \r\nrestore=bool(int(args.restore)))\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/../lib/fast_rcnn/train.py\", line 400, in train_net\r\nsw.train_model(sess, max_iters, restore=restore)\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/../lib/fast_rcnn/train.py\", line 142, in train_model\r\ngrads, norm = tf.clip_by_global_norm(tf.gradients(loss, tvars), 10.0)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in gradients\r\ngrad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 368, in _MaybeCompile\r\nreturn grad_fn() # Exit early\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in \r\ngrad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 577, in _TopKGrad\r\nind_2d = array_ops.reshape(op.outputs[1], array_ops.stack([-1, ind_lastdim]))\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2510, in reshape\r\nname=name)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\nop_def=op_def)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\noriginal_op=self._default_original_op, op_def=op_def)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in init\r\nself._traceback = _extract_stack()\r\n\r\n...which was originally created as op u'TopKV2', defined at:\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/train_net.py\", line 132, in \r\nrestore=bool(int(args.restore)))\r\n[elided 0 identical lines from previous traceback]\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/../lib/fast_rcnn/train.py\", line 400, in train_net\r\nsw.train_model(sess, max_iters, restore=restore)\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/../lib/fast_rcnn/train.py\", line 112, in train_model\r\nself.net.build_loss(ohem=cfg.TRAIN.OHEM)\r\nFile \"/home/yanchao/TFFRCNN/faster_rcnn/../lib/networks/network.py\", line 643, in build_loss\r\nrpn_cross_entropy_n_neg, _ = tf.nn.top_k(rpn_cross_entropy_n_neg, k=top_k)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1998, in top_k\r\nreturn gen_nn_ops._top_kv2(input, k=k, sorted=sorted, name=name)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2502, in _top_kv2\r\nname=name)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\r\nop_def=op_def)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\noriginal_op=self._default_original_op, op_def=op_def)\r\nFile \"/home/yanchao/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in init\r\nself._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): **Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero**\r\n[[Node: gradients/TopKV2_grad/Reshape = Reshape[T=DT_INT32, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](TopKV2/_131, gradients/TopKV2_grad/stack)]]\r\n\r\nProcess finished with exit code 1\r\n\r\nDoes anyone meet the same things? Or this is a bug of tensorflow. It's really a strange problem.", "comments": ["This error message is returned when you attempt to use `-1` as a \"wildcard dimension\" in a call to `tf.reshape()`.\r\n\r\nFor example:\r\n\r\n```python\r\nc = tf.random_uniform([4, 4])\r\nd = tf.reshape(c, [2, -1])\r\nprint d.shape  # (2, 8)\r\n```\r\n\r\nHowever, when a tensor is empty (i.e. it has 0 elements), there is more than one possible value for the wildcard to take:\r\n\r\n```python\r\nc = tf.random_uniform([4, 0])\r\nd = tf.reshape(c, [0, -1])\r\nprint d.shape  # Could be (0, 0), (0, 1), (0, 2), ...\r\n```\r\n\r\nThe only exception to this case is when the wildcard dimension must evaluate to 0. This will be the case if the number of input elements is 0, and all the specified dimensions for the new shape are non-zero:\r\n\r\n```python\r\nc = tf.random_uniform([4, 0])\r\nd = tf.reshape(c, [37, -1])\r\nprint d.shape  # (37, 0)\r\n```\r\n\r\nHope this makes sense!", "@Yc174 I ran into the same problem, did you manage to solve it?", "@eugi24 I only had a solution to make the program carry on when meeting this problem. Methods as below  \uff1a\r\nwhile iter < maxiter: \r\n      try:\r\n             Training subject\r\n      except:\r\n             continue", "@Yc174 Mmh ok, thank you anyway for giving me this idea :)", "I had this problem and for me, it was due to facing a null matrix during my computation\r\nso I just add a tf.cond for empty matrix", "HI\uff0cI also have the meet the same problem when I train the faster-rcnn using tensorflow, the error is \"InvalidArgumentError (see above for traceback): Reshape cannot infer the missing input size for an empty tensor unless all specified input sizes are non-zero\",  if you fix it?"]}, {"number": 10608, "title": "tensorflow/go: simplify 'range' on 'shape'", "body": "Found via 'gofmt'", "comments": ["Jenkins, test this please"]}, {"number": 10607, "title": "functional add_n: compose a new (differentiable) op from a list of ops without memory footprint", "body": "### Describe the problem\r\n\r\nIt is a feature request, and should be very related to the current high memory cost of tensorflow. \r\n\r\nIn my case, I am building a graph which I should be able to compute gradients. In this graph, I heavily use the `add_n` op which returns a tensor from a list of tensors. I want to remark that this is very inefficient for two reasons:\r\n\r\n1. Memory wise, it requires to cache all input tensors in the list before it actually aggregates. Technically, such process can be replaced by `accumulate_n` if one does not care computing its gradient. In facts, I have been tried with `add_n` with input list size goes to tens or a hundred, it quickly fills up the memory of GPUs. \r\n\r\n2. Besides high memory footprint, it also prohibits the use of multi-thread framework of tensorflow, ultimately affecting the efficiency. This is because the idea of `add_n` actually introduces an extra layer of tensors whose controlled dependency prevents deallocating any of these tensors computed in time. Consider the following example:\r\n\r\n```python\r\na=tf.Variable(0.2)\r\nb=tf.Variable(0.1)\r\n\r\nc=[tf.sin(a), tf.cos(a), tf.sin(b)]\r\nd=[tf.sin(a), tf.cos(b)]\r\n\r\ne=add_n(c)\r\nf=add_n(d)\r\n# ... initialization \r\nsess.run([e, f])\r\n```\r\nIn the above example, suppose given variables `a` and `b` what we really need is tensors `e` and `f`, but the introduction of tensor list `c` and `d` occupies what I consider as redundant memory. Note that the computation of tensors in `c` and `d` can be made multi-threaded / parallel if we have infinite memory. But the real situation is if the caching tensors in `c` already eat up all memory, no tenors in `d` will be executed simultaneously until `e` is complete (at which time `c` is released). \r\n\r\nIn fact, one can calculate `e` and its gradients `de/da`, `de/db` without explicitly storing any tensors in `c`. This trick is by introducing a functional which takes a set of ops and their inputs, and return a global summed-up tensor, which may look like:\r\n\r\n```python\r\n\r\na=tf.Variable(0.2)\r\nb=tf.Variable(0.1)\r\n\r\ne_func=add_n_functional([tf.sin, tf.cos, tf.sin])\r\ne=e_func([a, a, b])\r\nf_func=add_n_functional([tf.sin, tf.cos])\r\nf=f_fun([a, b])\r\n\r\n# ... initialization \r\nsess.run([e, f])\r\n```\r\nNote `add_n_functional` returns a new op from a list of given ops. With this new functional, we can avoid buffering a lot intermediate tensors. At the same time, `e_func` and `f_func` are still differentiable.\r\n\r\nI was thinking how to implement this idea at python level, but it seems to only be feasible via rewriting some C++ parts. I will appreciate if this feature is added to tensorflow in the near future.   \r\n\r\n\r\n\r\n", "comments": ["I attached two timelines of the same graph: one in cpu, in which multithreading is doing good, one with some ops in gpu, which has troubles to concurrently run multiple ops (due to lack of memory). \r\n\r\n<img width=\"1467\" alt=\"screen shot 2017-06-09 at 4 39 03 pm\" src=\"https://user-images.githubusercontent.com/1040227/27000602-fd16fe10-4d6a-11e7-9e67-5d00d4d87208.png\">\r\n<img width=\"1406\" alt=\"screen shot 2017-06-09 at 4 39 22 pm\" src=\"https://user-images.githubusercontent.com/1040227/27000603-ffa7bfde-4d6a-11e7-8f88-7b589e0bdb61.png\">\r\n", "@alextp, do you care to comment on this feature request?", "If we rewrite accumulate_n as an atomic op which has a gradient defined for it and which gets rewritten by the runtime into the current implementation this is feasible. See for example the code in parallel_concat (which currently doesn't have a defined gradient but very well could).", "@alextp Thanks for your comment. It would be a great feature for either adding gradient to accumulate_n. I am currently struggling to work around this issue by partially adopting the auto-diff system of tensorflow (by mixing my customized gradient with tensorflow's)", "I am looking into this. \r\n\r\n@alextp would you prefer the rewrite to produce the same subgraph of operators that the current Python implementation of accumulate_n produces (a combination of Merge, TemporaryVariable, ZerosLike, Assign, AssignAdd, and DestroyTemporaryVariable), or would it be better to add two new internal C++ operators to match the design of ParallalConcat?", "I think it's possible to make it run faster if we use internal operators\nlike ParallelConcat, but the path of least resistance is to mimic the\nexisting implementation.\n\nOn Fri, Aug 4, 2017 at 10:37 AM, Fred Reiss <notifications@github.com>\nwrote:\n\n> I am looking into this.\n>\n> @alextp <https://github.com/alextp> would you prefer the rewrite to\n> produce the same subgraph of operators that the current Python\n> implementation of accumulate_n produces (a combination of Merge,\n> TemporaryVariable, ZerosLike, Assign, AssignAdd, and\n> DestroyTemporaryVariable), or would it be better to add two new internal\n> C++ operators to match the design of ParallalConcat?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/10607#issuecomment-320308478>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxdeZHveFaTH5KjM4eD2V0Bb9Adbbks5sU1bPgaJpZM4N2BT2>\n> .\n>\n\n\n\n-- \n - Alex\n", "Update: Implementation of the rewrite is substantially complete. Will submit a PR once I've done some additional testing.", "Pull request created. See https://github.com/tensorflow/tensorflow/pull/13022", "Closing as this is resolved\r\n\r\n\r\n"]}, {"number": 10606, "title": "Skip configure bazel version check on empty version string.", "body": "Fix #10587", "comments": ["Jenkins, test this please.", "failures are irrelevant. Merging."]}, {"number": 10605, "title": "tensorflow/tools/git script breaks when git repo has packed references", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNot applicable to bug\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource build\r\n- **TensorFlow version (use command below)**:\r\ntip of tree\r\n- **Bazel version (if compiling from source)**:\r\n0.5.1\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\n```\r\ngit pack-refs\r\ngit gc\r\nbazel clean\r\nconfigure\r\nbazel build -c opt --copt=-g --copt=-mavx --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n- **Results**:\r\n```\r\nERROR: /local/chip/git/tensorflow-knureon/tensorflow/core/BUILD:1404:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /local/chip/git/tensorflow-knureon/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted.\r\n```\r\n\r\n\r\nThe problem is due to the fact that the link in tensorflow/tools/git/gen/branch_ref is supposed to point to a valid file in my .git directory: .git/refs/heads/tf-tip.  However, I have been doing some reorganizing of our tree structure, and in the process of removing deprecated branches my calls to git caused this file to be deleted.  So the .git/refs/heads directory is empty and the hash is found in .git/packed-refs:\r\n\r\nrdubielzig@swpl-000224:/local/chip/git/tensorflow-knureon/.git$ grep tf-tip packed-refs \r\n504965336d1432ee96b4f0e9b78f65ee201e9be5 refs/heads/tf-tip\r\n\r\n\r\nWORKAROUND:\r\nRecreate the missing ref file by writing  'tf-tip' in .git/refs/heads with the contents consisting of the hash above.", "comments": ["It is difficult to get bazel to both track the dependency information to update the git hash reliability in the binary and to print a warning. Generally speaking, you can also re-run ./configure. ", "FYI I'm also having this problem, and re-running ./configure isn't fixing it. I will try the trick of manually writing the file in .git/refs/heads...", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@case540, could you look into resolving this case, please?", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @aselle, @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The build issue should be fixed by https://github.com/tensorflow/tensorflow/pull/17162 now.\r\nThe git_version string won't be correct, but you should be able to build without having to hack your workspace at least.", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @case540: It has been 16 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Pretty sure TF will know build w/o needing a workaround in this case (pretty sure tf.__git_version__ will show up as 'unknown').\r\n\r\nClosing issue unless someone really needs tf.__git_version__ to be accurate when using packed references."]}, {"number": 10604, "title": "CONTRIBUTING.md include basic Docker CI command", "body": "This quick one liner CI test via Docker should be enough so many users don't even have to follow the \"more details\" link.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 10603, "title": "Revert the new non_max_suppression_v2 op for the 1.2 release so we can do API review", "body": "", "comments": ["@martinwicke fyi", "Jenkins, test this please", "Jenkins, test this please"]}, {"number": 10602, "title": "TensorBoard:  enable text-selection (for copy/paste) of tf.summary.text items", "body": "The way tensorboard is currently implemented seems to prevent one from selecting (in a web browser) the text of tf.summary.text summaries.  It would be nice if it were possible to copy/paste such text.", "comments": ["@dandelionmane Do you know what the problem might be?", "This is easy to fix; there's a `user-select: none` that should be overridden by the text dashboard.\r\n\r\nI've migrated this to https://github.com/tensorflow/tensorboard/issues/33, and should be able to fix it soon. :-)"]}, {"number": 10601, "title": "Fixing the broken 1.2 branch tests.", "body": "", "comments": []}, {"number": 10600, "title": "Fixes python doc for tanh (Resolves #10376)", "body": "", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please."]}, {"number": 10599, "title": "Pretrain NLP embeddings", "body": "I found a lot of pretrain CNN models in `tf.contrib.keras.applications`. Nevertheless, for NLP researchers there is no even well-know GoogleNews word2vec vectors or Glove, or...something. \r\n\r\nIs it possible to add functionality for simplified embedding download and future usage? Some official `maybe_download_google_news_w2v` or something like that.", "comments": ["This is outside the scope of TensorFlow Github issues, which are about bugs or feature requests to the core library."]}, {"number": 10598, "title": "Implements inverse hyperbolic operations (fix for #7531)", "body": "fixes #7531.", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "I ran the tests which failed here on my machine and found that the only cases which were failing are the ones in which `tf.acosh` evaluates to `inf/-inf` and `np.arccosh` evaluates to `nan/-nan`. <s>Should I change the test cases or do something else? Any ideas on how to proceed?</s> I have changed the test cases and also fixed a bug in test functions for `asinh` and `acosh`. The failed tests are passing now.", "@tensorflow-jenkins test this please", "The failures in tests/build seem unrelated to the changes made by this pull request.", "Jenkins, test this please.", "The cwise ops test failure (only on Windows, ugh) is probably caused by this CL -- the tolerances appear to be too tight.", "Any hints on how to proceed?", "I would look at which test fails, exactly, and adjust the tolerance for that test in the test code. I think the values are still close so this is not a real problem, just numerics and a particularly unfriendly function.", "If that is the case then this PR would have to wait for a few days since I do not have access to my machine and would be able to fix it only after the 16th. BTW since I do not have a windows machine, how can I identify the function which is failing?", "No problem.", "This test failure is not due to this PR. We'll review API changes on Monday, should be able to merge then.", "Great, thanks!", "Since the tests for `acosh` and `asinh` were passing, I have also added a commit which implements `atanh` thus completely resolving #7531. Please review.", "@tensorflow-jenkins test this please", "@rmlarsen `//tensorflow/cc:gradients_math_grad_test` is failing in _Linux CPU Tests_ and _MacOS CPU Tests_ but passes in _Linux CPU Tests (Python 3)_ and _Linux GPU_. Are these failures due to some bug in the code?", "I ran the test locally with python 2 and found that the test was failing due to some case for which numpy gave `nan` and tf gave `inf`. I have changed the test cases so that this does not happen. The test passes on my machine with python2 now.", "@lakshayg is inf the mathematically correct answer? We should strive for compatibility with numpy whenever possible, even for arguments that give nan/in, unless it is clearly wrong. Does this depend on the version of numpy? I'm OK with the PR as is, but perhaps add a TODO in the test code to investigate this edge case?", "@rmlarsen I just checked and found that numpy gave inf and tf gave nan. WolframAlpha says that the value should be `inf` (http://www.wolframalpha.com/input/?i=atanh(1%2B0i)). What is surprising is that the tf implementation of atanh uses std::atanh from the C library which gives inf for x=1 as well. I guess that I will have to check tensorflow's output again.\r\n\r\nAny ideas on how to fix this?\r\n\r\n", "@tensorflow-jenkins test this please", "@lakshayg could it be that in the gradient computation, TF ends up computing inf/inf or 0*inf, while you compute the numpy result more directly in the test? ", "The gradient for atanh(x) is 1 / (1 - x^2). Could that be causing the problem for x = 1?", "@lakshayg that should give 1/0 = inf. I'm not sure why it produces NaN instead of inf, but that is unrelated to your change.", "@rmlarsen I just compiled the binary and checked what it gave for x=1 and the answer is `inf`. I guess that everything is working fine and this PR can be merged.\r\n\r\n![screenshot from 2017-06-21 08-49-33](https://user-images.githubusercontent.com/7976315/27365620-9e85d0e0-565e-11e7-97dc-934197ea6c2f.png)\r\n", "@martinwicke This is ready to be merged. You added the API review tag. What needs to be done? ", "@lakshayg could you please rebase to resolve conflicts?", "@martinwicke I missed your earlier comment about API review last monday. I'll assign you on this PR and you can merge when it's approved.", "@rmlarsen rebase done!", "@lakshayg Thanks!\r\n@tensorflow-jenkins test this please", "How about renaming to arcsinh, arccosh, arctanh to match the numpy names?", "@jhseu I guess then we would have to rename asin, acos and atan as well. ", "Yes, but perhaps wait for API review for the final word.", "@jhseu We already have atan, atan2, asin etc. so it doesn't make sense unless we change the existing interface.", "@jhseu or @martinwicke : to confirm, good to go?", "Good for API review, but watch out, this will require Eigen changes to pull in.", "I see failures in windows CPU and GPU build due to additions from this change:\r\nhttp://ci.tensorflow.org/job/tensorflow-master-win-cmake-py/1101/console\r\n```\r\ncwise_ops_test.py\", line 411, in testComplex64Basic\r\ncwise_ops_test.py\", line 302, in testDoubleBasic\r\ncwise_ops_test.py\", line 208, in testFloatBasic\r\n```\r\n\r\nLooks like there is some flakiness in arccosh implementation on windows?\r\n", "@rmlarsen is out next week.\r\n\r\nIt was green when I merged. Looking at the code, there doesn't seem to be anything obviously special about cosh.\r\n\r\n@lakshayg could you take a look?", "It was suggested earlier https://github.com/tensorflow/tensorflow/pull/10598#issuecomment-308328786 that I change the tolerances for those but since the tests passed on the next build, I did not change the tolerances. Should I go ahead and change them? @gunan @drpngx @martinwicke", "It's not just the tolerance, check this out:\r\n\r\n```\r\n21:10:19 <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'tensorflow.python.ops.variables.Variable'> <class 'tensorflow.python.ops.variables.Variable'>\r\n21:10:19 <class 'tensorflow.python.framework.ops.Tensor'> <class 'tensorflow.python.framework.ops.Tensor'>\r\n21:10:19 not close where =  (array([7, 7], dtype=int64), array([6, 7], dtype=int64))\r\n21:10:19 not close lhs =  [ 1.  0.]\r\n21:10:19 not close rhs =  [    0.          1570.79632679]\r\n21:10:19 not close dif =  [  1.00000000e+00   1.57079633e+03]\r\n21:10:19 not close tol =  [  1.00000000e-05   1.57179633e-02]\r\n```"]}, {"number": 10597, "title": "tf.estimator generator_input_fn multi thread bug ", "body": "I would like to use python generators as input pipeline for tf.estimator's. Finally I found generator_input_fn (announced [here](https://youtu.be/5DknTFbcGVM?t=12m30s)).\r\n\r\nNevertheless, when I start first experiments, I found I bug with learning curve. After some experiments I found the purpose of it - all blows up if you set `num_threads` > 1 in generator_input_fn.\r\n\r\nFor example, here is loss plot with python generator and `num_threads=1`:\r\n![tf_py_generator_1_thread](https://user-images.githubusercontent.com/7606451/26978620-d58faec2-4d34-11e7-8f8b-14e4e50f6844.png)\r\n\r\nAnd with `num_threads=2`:\r\n![tf_py_generator_2_threads](https://user-images.githubusercontent.com/7606451/26978619-d58a9e3c-4d34-11e7-8f1b-8bab67a957aa.png)\r\n\r\n**But** if I use `tf.estimator.inputs.numpy_input_fn` with 4 thread all work pretty well:\r\n![tf_numpy_generator_4_threads](https://user-images.githubusercontent.com/7606451/26978621-d591c11c-4d34-11e7-85ce-2340bd2ed01c.png)\r\nExcept the fact, that I cannot save all my data in one numpy array (GBs of data).\r\n\r\nAny suggestions why so? Or I need just wait for TF 1.2 with working `tf.estimators.inputs.generator_input_fn`?\r\n\r\nJupyter notebook with my experiments [here](https://gist.github.com/Scitator/184c8d676f36a9b7c04fb504d9088590).", "comments": ["Unfortunately, I think we would need more information to have a sense where the problem is.  Learning curves that explode are often subtle machine learning issues, not bugs in TensorFlow.  It's entirely possible that it is a bug, though.  Could you do some more debugging to try to narrow down the cause?", "First of all, I always use grad clipping to prevent such problem, and come on, it's typical conv net for mnist :)\r\nAnd secondly, I already done some debugging before reporting an issue:\r\nDifferent combinations of `queue_capacity`, `num_epochs`, `shuffle` have no affect.\r\n`sparse_softmax_cross_entropy_with_logits` also works correct (I tried with just `softmax_cross_entropy_with_logits` - still have such issue).\r\nPython generator work pretty well, without any exceptions, *but* it still strange, that loss always explode at the end of first iteration over dataset.\r\n\r\nThat's why I think, that something go wrong with multi-threading here. Nevertheless, you can always check notebook and run it.", "@martinwicke Want to investigate?  I am unfamiliar with possible `tf.learn` threading issues.", "This is a bug in generator_input_fn -- I just looked and if I read it correctly, it makes a generator per thread (each thread calls the generator function passed to generator_input_fn). \r\n\r\nIf you'd like to fix that, we'd welcome a fix (although it looks non-trivial). Even a doc fix would be welcome.\r\n\r\nThe alternative is to write your generator to not use global state (in your case, `data_src.next_batch(1)` -- give each generator a separate copy of data_src).", "@martinwicke Thanks, now I understand. **And it's not a bug, it's a feature :)**\r\nPython generator cannot be used from several threads, so the best way to give each thread it own one.\r\nBut now we have an issue, when the data from threads is the same. For me, I solve it through adding a thread number parameter in `generator_input_fn` and split data correctly in it. [link](https://github.com/Scitator/rstools/blob/master/rstools/tf/data_iterator.py). \r\nAs I understand TF source code, it uses different seeds to prevent such same data yielding. Although, it only set it for python random, strange a bit. Looks like all this done for better code reusing, or it really works correctly....need to test.\r\n\r\nNevertheless, I cannot understand such loss explosion. But still, thanks for help.\r\n\r\nPS. Also update [notebook](https://gist.github.com/Scitator/184c8d676f36a9b7c04fb504d9088590) with simple solution. (still need to test memory usage with large data, but should work)", "Yes, you have to take care of data splitting yourself. Shuffling should work fine -- unless you set the random seed in each of the threads.\r\n\r\nI don't know what the loss is doing, I can imagine that it does all kinds of terrible things, for instance it may return all zeros, and only because you use gradient clipping do you even get a non-inf loss. I don't know. I'm pretty sure you didn't see valid data after the first epoch.", "I think that we could close this as we are not going to have features evolution for  `tf.estimator` right?", "@Scitator Estimators are not recommended for new code , could you please refer this [link](https://www.tensorflow.org/guide/estimator) and let us know if we can move this issue to closed status?\r\nThanks!  ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/10597\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/10597\">No</a>\n"]}, {"number": 10596, "title": "Can't import tensorflow 1.2 since last anaconda 4.4 update", "body": "I installed tf 1.2 from source on Ubuntu 16.04, and It worked as it should until I updated anaconda to 4.4 yesterday. It refuses to load and gives me this error:\r\n\r\n```\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/jingw222/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/home/jingw222/anaconda3/lib/python3.6/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: /home/jingw222/anaconda3/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /home/jingw222/anaconda3/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n```\r\n\r\nAnyone knows what went wrong?", "comments": ["@caisq Any ideas?  Not sure if you're the right person to ask.", "@jingw222 So you built TensorFlow from source. Could it be that your build environment uses a glibcxx  version that is unavailable in your instal environment? ", "Yeah, it was built from source and working fine until maybe the anaconda update. I highly suspect it was due to anaconda having upgraded some package, because  when I tried to install the package `libstdc++6` the terminal told me `libstdc++6 is already the newest version (5.4.0-6ubuntu1~16.04.4)`. Should I build it again?", "The was indeed introduced by anaconda upgrade, cause I had similiar issue with another ML library as well. After lots of searches through the web, I tried `conda install libgcc` a moment ago, and the problem disappeared. My guess is that after I updated anaconda, the `libstdc++6` library somehow didn't get installed in the conda environment. Therefore even though `libstdc++6 is already the newest version (5.4.0-6ubuntu1~16.04.4)`, it was not shared with the conda environment. Thank you guys anyway!", "glad you got it resolved, closing for now. Thanks!", "Same problem and `conda install libgcc` don't solve it.\r\n\r\nImportError: /home/bh/anaconda3/envs/keras2/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.22' not found (required by /home/bh/anaconda3/envs/keras2/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)\r\n"]}, {"number": 10595, "title": "Remove deprecated functions", "body": "Remove deprecated classes and functions marked for removal until May 2017.\r\nThis PR doesn't touch deprecated parameters nor deprecations for June 15.", "comments": ["Can one of the admins verify this patch?", "Thanks! I really want to accept this, but I cannot (it would save a lot of headache).\r\n\r\nI should never have put dates on these, and probably should remove them. We'd be breaking too many people and we won't actually remove these functions, probably until we reorganize contrib as a whole. \r\n\r\nI'm sorry about this. I should remove the dates, since they're misleading."]}, {"number": 10594, "title": "Windows GPU Installation Documentation", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYES\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\n1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n5 and 6 (that's the issue)\r\n- **GPU model and memory**:\r\nGTX 965M, 2GB memory\r\n- **Exact command to reproduce**:\r\nsess = tf.InteractiveSession()\r\n### Describe the problem\r\nThis is a request to add information to your installation website. The documentation for installing the gpu version in Windows omits a crucial piece of information: the version of cudnn64_X.dll must exactly match the version used when the binary for tensorflow was compiled. The installation instructions advise you to be sure that cudnn64_X.dll is in the path. However, this is of zero use if it is the wrong dll. \r\n\r\nI'm currently running tensorflow 1.1.0, which I installed from the binary distribution. That version requires cudnn64_5.dll, but this critical information is nowhere to be found in the documentation. Instead, I just got the error message that tensorflow failed to start. The current version from nVidia is cudnn64_6.dll. Tensorflow won't run if it doesn't find cudnn64_5.dll. However, if you rename version 6 to version 5, tensorflow will load, but it will stop with errors. (Actually, that's how I tracked down the problem - it mentioned version 5100 of the GPU, which I assumed meant release 5).\r\n\r\nPlease add to the installation page the required version of nVidia's dll: cudnn64_X.dll. Otherwise, the gpu version won't start, and it is remarkably difficult to figure out that the problem is as simple as using the wrong (e.g., latest) version of the cudnn64_x driver. \r\n\r\n### Source code / logs\r\nNA\r\n", "comments": ["@mrry Could you take a look?", "The documentation clearly states cuDNN 5.1 is required for GPU support on Windows. Nowhere mentions cuDNN 6 is supported though.\r\nhttps://www.tensorflow.org/install/install_windows#requirements_to_run_tensorflow_with_gpu_support\r\n\r\nOnly from TensorFlow 1.3 forward cuDNN 6 will be supported and 5.1 deprecated according to the [Release docs, under deprecation](https://github.com/tensorflow/tensorflow/releases).\r\n\r\n", "Thank you for the prompt response. I agree - the document states clearly the requirement for cuDNN5.1.\r\n\r\nIf I may offer a suggestion, it would be helpful to add the comment \"Tensorflow is compiled for a specific version of cuDNN, which may not be the most recent release from nVidia, and may not be upwardly compatible with the most recent release. TensorFlow 1.1 requires cuDNN 5.1. Versions 1.3 forward will require cuDNN 6.\"\r\n\r\nAlternatively, you could also just plainly state: \"Tensorflow 1.1.0 will not load if it cannot find cuDNN64_5.dll.\" That's pretty unambiguous!\r\n\r\nMore generally, you've gone to a lot of trouble to make tensorflow publicly available for teaching and experimentation. That's just outstanding! However, much of the audience you are targeting are scientists in other fields, not developers. For developers it's entirely obvious that when the documentation says \"the following NVIDIA software must be installed on your system.... cuDNN 5.1\" it means exactly that. However, Windows dlls are typically upwardly compatible, at least on the Microsoft side. I have become accustomed to installing the latest drivers, hoping (usually in vain) they will fix issues with previous releases. This is so ingrained in my thinking that I read this page multiple times, looking for help with cuDNN64_6.dll, and didn't pick up that the problem was that I had installed the latest version of the cuDNN driver, cuDNN64_6.dll, rather than the previous version, cuDNN64_5.dll. \r\n\r\nThe nVidia download page (https://developer.nvidia.com/rdp/cudnn-download) is a good example of clarity showing the precise match between cuDNN and CUDA versions. \r\n\r\nAgain, I appreciate the effort you are making to provide tensorflow to a wide audience interested in applying neural networks to data problems. In my field (drug behavior) this could be truly game changing. \r\n", "@StevenLShafer: Thank you for the lovely tone!  I'll make a PR with your language.", "Indeed, the current language directly \"common sense contradicts\" the requirements: \"If you have an earlier version of the preceding packages, please upgrade to the specified versions.\"  To any reader (developer or not), this seems to imply that if you have a later version you're fine.", "If you say, \"If your number is `< x`, change to `x`\", it means \"The number should be `>= x`\" in common language.  The current text is like saying, \"No small dogs allowed\" and being surprised when someone arrives with a large dog."]}, {"number": 10593, "title": "Improve docs for `parallel_stack`", "body": "For #10036", "comments": ["Can one of the admins verify this patch?", "Make the last sentence this:\r\n> Unlike `stack`, `parallel_stack` does NOT support backpropagation.", "LGTM", "Jenkins, test this please.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "Jenkins, test this please.", "Just noticing -- is it intentional you made this change to the 1.2 branch? Could you put it on master?\r\n\r\nThat will mean that 1.2 docs won't contain the fix, but I'd rather not mess with the release this late.", "No, that was by mistake @martinwicke ", "I'm sorry, I don't practice rebase that often..."]}, {"number": 10592, "title": "Update quantization docs to new tool", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 10591, "title": "Fix incorrect documentation in _SliceHelper #10494", "body": "Fixes #10494.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "@martinwicke Why do documentation changes even need a full CI test?", "They don't, a partial one would do, but we haven't set it up with two separate builds. We can do that, it's just that more interesting things keep happening. But the documentation is built as well, and doc changes can break for a variety of reasons, some of them subtle. \r\n\r\nThat said, I do at times merge without testing. Don't tell anyone. "]}]