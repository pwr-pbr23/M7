[{"number": 24260, "title": "Reduce lock contention: add step_resource_manager to hold step contai\u2026", "body": "\u2026ner.\r\n\r\nOnce step container hold by resource manager could introduce serious lock\r\ncontention. Because step_container would create every step which hold write\r\nlock in ResourceMgr's mutex, but for other container's such as Variable\r\nonly need write lock in first step, then following steps just hold read lock.\r\n\r\nEven the model not need step container, still try to hold write lock of\r\nResourceMgr in each step.\r\n\r\nThe design of step_resource_manager was removed in commit\r\n(b571f2e94760f8daed9b628dd9a39404183e3779), now add it back to\r\nreduce lock contention.", "comments": ["Can you show some evidence that lock contention is substantially improved by this? I think it adds quite a bit of complexity to the code.", "we using vtune to profiling the locks and waits, found the heavy lock contention. (There's no step_container related resource are used in the model).\r\nsampling time is 9s.\r\n\r\n![image](https://user-images.githubusercontent.com/2573025/49778563-c0d94980-fd40-11e8-8260-962e8e44d953.png)\r\n\r\n![image](https://user-images.githubusercontent.com/2573025/49842047-01929a80-fdf5-11e8-9f38-048f85234403.png)\r\n\r\n\r\nFrom the pics, clearly see the conflict between write lock in ClearContainer (which introduced by step_container) and read lock of ResourceMgr::DoLookup. For the step_container placed in ResourceMgr with other containers  break the design of read/write lock in ResourceMgr, and slow down the performance of ResourceMgr's lookup (most operations are just lookup).\r\n", "I'd really like to avoid reintroducing the step resource manager. Can lock sharding (one lock per container, say) be used to achieve the same reduction in contention?", "Another way is split the lock inside of ResourceMgr, such as:\r\n```\r\nclass ResourceMgr {\r\n public:\r\n  template <typename T>\r\n  Status LookupOrCreate(const string& container, const string& name,\r\n                        T** resource,\r\n                        std::function<Status(T**)> creator, ContainerType type = NormalContainer) TF_MUST_USE_RESULT;\r\n\r\n private:\r\n   mutable mutex mu_;\r\n   std::unordered_map<string, Container*> containers_ GUARDED_BY(mu_);\r\n\r\n   mutable mutex step_container_mu_;\r\n   std::unordered_map<string, Container*> step_containers_ GUARDED_BY(step_container_mu_);\r\n};\r\n```\r\none lock for normal containers's map, one for step containers's map.  For APIs, have to add additional args (ContainerType type) to flag to use which map. This solution have to change the APIs of ResourceMgr.", "Do you know what ops are triggering this extra contention on the step container?\r\n\r\nAs we move to tf v2 there will be essentially no use of the step container, so I wonder if you can migrate some of your graph towards the new stuff to make this go away.\r\n\r\nI also think unrelatedly that we can probably get a reduction in contention by sharding the resource manager mutex somehow. Do you want to take a stab at that?", "> Do you know what ops are triggering this extra contention on the step container?\r\n> \r\n> As we move to tf v2 there will be essentially no use of the step container, so I wonder if you can migrate some of your graph towards the new stuff to make this go away.\r\n> \r\n> I also think unrelatedly that we can probably get a reduction in contention by sharding the resource manager mutex somehow. Do you want to take a stab at that?\r\n\r\nIn our models, there's no ops using step container. The problems is even we don't use step_container,  TensorFlow's create and delete step_container every step which trigger the serious contention in ParameterServer. \r\n\r\nMy concern is that read-write lock separation has been implemented in ResourceMgr which reduce the lock contentions. But the step_container brings so many write operations that weakened the \"read-write lock separation\" optimization.\r\n\r\n ", "Interesting. So I think we should focus on making the unused-step-container\ncode path faster. Maybe if we lazily create it instead of forcefully\ncreating it? Maybe we need to store a dirty bit for it?\n\nOn Sun, Jan 6, 2019 at 6:30 PM Tongxuan Liu <notifications@github.com>\nwrote:\n\n> Do you know what ops are triggering this extra contention on the step\n> container?\n>\n> As we move to tf v2 there will be essentially no use of the step\n> container, so I wonder if you can migrate some of your graph towards the\n> new stuff to make this go away.\n>\n> I also think unrelatedly that we can probably get a reduction in\n> contention by sharding the resource manager mutex somehow. Do you want to\n> take a stab at that?\n>\n> In our models, there's no ops using step container. The problems is even\n> we don't use step_container, TensorFlow's create and delete step_container\n> every step which trigger the serious contention in ParameterServer.\n>\n> My concern is that read-write lock separation has been implemented in\n> ResourceMgr which reduce the lock contentions. But the step_container\n> brings so many write operations that weakened the \"read-write lock\n> separation\" optimization.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-451803258>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxYEd98LieNL5yW1qVjKAThIKkjzgks5vArFagaJpZM4ZKnwe>\n> .\n>\n\n\n-- \n - Alex\n", "> Interesting. So I think we should focus on making the unused-step-container code path faster. Maybe if we lazily create it instead of forcefully creating it? Maybe we need to store a dirty bit for it?\r\n> [\u2026](#)\r\n> On Sun, Jan 6, 2019 at 6:30 PM Tongxuan Liu ***@***.***> wrote: Do you know what ops are triggering this extra contention on the step container? As we move to tf v2 there will be essentially no use of the step container, so I wonder if you can migrate some of your graph towards the new stuff to make this go away. I also think unrelatedly that we can probably get a reduction in contention by sharding the resource manager mutex somehow. Do you want to take a stab at that? In our models, there's no ops using step container. The problems is even we don't use step_container, TensorFlow's create and delete step_container every step which trigger the serious contention in ParameterServer. My concern is that read-write lock separation has been implemented in ResourceMgr which reduce the lock contentions. But the step_container brings so many write operations that weakened the \"read-write lock separation\" optimization. \u2014 You are receiving this because your review was requested. Reply to this email directly, view it on GitHub <[#24260 (comment)](https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-451803258)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxYEd98LieNL5yW1qVjKAThIKkjzgks5vArFagaJpZM4ZKnwe> .\r\n> -- - Alex\r\n\r\nIt' a way to use lazily initialization of step container, as following:\r\n```\r\n\r\nclass ScopedStepContainer : public Container {\r\n public:\r\n  ScopedStepContainer(const int64 step_id,\r\n                      std::function<void(const string&)> cleanup)\r\n      : name_(strings::StrCat(\"__per_step_\", step_id)), cleanup_(cleanup) {}\r\n  ~ScopedStepContainer() {\r\n     if (!is_init) {\r\n       cleanup_(name_); \r\n     }\r\n  }\r\n\r\n  void set_init(bool init) {is_init_ = init;}\r\n\r\n private:\r\n  const string name_;\r\n  const std::function<void(const string&)> cleanup_;\r\n  bool is_init_;\r\n};\r\n\r\nclass ResourceMgr {\r\n public:\r\n  template <typename T>\r\n  Status LookupOrCreate(const Container& container, const string& name,\r\n                        T** resource,\r\n                        std::function<Status(T**)> creator) {\r\n   container->set_init(true);\r\n   //....\r\n }\r\nTF_MUST_USE_RESULT;\r\n};\r\n```\r\nthe solution have to change APIs (LookupOrCreate and Create in ResourceMgr). In this way could avoid the modes when do not use step container.\r\n\r\nBut I'm afraid that the solution can't reduce the lock contention between normal containers and step containers when we use step_containers' Resource such as stack ops, temporary variables and so on.\r\n\r\nBtw i recommend we could do some refactoring for the related code of ResourceMgr. \r\nclass StepContainer is like a \"Lazy Class\"(code smell) which do nothing except wrapper a container name & cleanup function. Probably class StepContainer or Container which wrapper ResourceMgr could be the API not ResourceMgr. Then we can wrapper the changes inside of class Container not as now we have to \"Shotgun Surgery\" so many places.", "Maybe we can record whether any call to create() was called between\ncreating and destroying a scoped step container, and skip destruction if no\nsuch thing happened?\n\nOn Tue, Jan 8, 2019 at 5:47 AM Tongxuan Liu <notifications@github.com>\nwrote:\n\n> Interesting. So I think we should focus on making the\n> unused-step-container code path faster. Maybe if we lazily create it\n> instead of forcefully creating it? Maybe we need to store a dirty bit for\n> it?\n> \u2026 <#m_-8817584779798990904_>\n> On Sun, Jan 6, 2019 at 6:30 PM Tongxuan Liu ***@***.***> wrote: Do you\n> know what ops are triggering this extra contention on the step container?\n> As we move to tf v2 there will be essentially no use of the step container,\n> so I wonder if you can migrate some of your graph towards the new stuff to\n> make this go away. I also think unrelatedly that we can probably get a\n> reduction in contention by sharding the resource manager mutex somehow. Do\n> you want to take a stab at that? In our models, there's no ops using step\n> container. The problems is even we don't use step_container, TensorFlow's\n> create and delete step_container every step which trigger the serious\n> contention in ParameterServer. My concern is that read-write lock\n> separation has been implemented in ResourceMgr which reduce the lock\n> contentions. But the step_container brings so many write operations that\n> weakened the \"read-write lock separation\" optimization. \u2014 You are receiving\n> this because your review was requested. Reply to this email directly, view\n> it on GitHub <#24260 (comment)\n> <https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-451803258>>,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAATxYEd98LieNL5yW1qVjKAThIKkjzgks5vArFagaJpZM4ZKnwe\n> .\n> -- - Alex\n>\n> It' a way to use lazily initialization of step container, as following:\n>\n>\n>\n> class ScopedStepContainer : public Container {\n>\n>  public:\n>\n>   ScopedStepContainer(const int64 step_id,\n>\n>                       std::function<void(const string&)> cleanup)\n>\n>       : name_(strings::StrCat(\"__per_step_\", step_id)), cleanup_(cleanup) {}\n>\n>   ~ScopedStepContainer() {\n>\n>      if (!is_init) {\n>\n>        cleanup_(name_);\n>\n>      }\n>\n>   }\n>\n>\n>\n>   void set_init(bool init) {is_init_ = init;}\n>\n>\n>\n>  private:\n>\n>   const string name_;\n>\n>   const std::function<void(const string&)> cleanup_;\n>\n> };\n>\n>\n>\n> class ResourceMgr {\n>\n>  public:\n>\n>   template <typename T>\n>\n>   Status LookupOrCreate(const Container& container, const string& name,\n>\n>                         T** resource,\n>\n>                         std::function<Status(T**)> creator) {\n>\n>    container->set_init(true);\n>\n>    //....\n>\n>  }\n>\n> TF_MUST_USE_RESULT;\n>\n> };\n>\n>\n> the solution have to change APIs (LookupOrCreate and Create in\n> ResourceMgr). I'm afraid that the solution can't reduce the lock contention\n> between normal containers and step containers when we use step_containers'\n> Resource such as stack ops, temporary variables and so on.\n>\n> Besides, class StepContainer is like a \"Lazy Class\"(code smell) which do\n> nothing except wrapper a container name & cleanup function. Probably class\n> Container which wrapper ResourceMgr could be the API not ResourceMgr. Then\n> we can wrapper the changes inside of class Container not as now we have to\n> \"Shotgun Surgery\" so many places.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-452302020>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxTDON_7TMWSOdsjt6qQpUy4gwl-nks5vBKFYgaJpZM4ZKnwe>\n> .\n>\n\n\n-- \n - Alex\n", "> Maybe we can record whether any call to create() was called between creating and destroying a scoped step container, and skip destruction if no such thing happened? On Tue, Jan 8, 2019 at 5:47 AM Tongxuan Liu <notifications@github.com> wrote:\r\n> [\u2026](#)\r\n> Interesting. So I think we should focus on making the unused-step-container code path faster. Maybe if we lazily create it instead of forcefully creating it? Maybe we need to store a dirty bit for it? \u2026 <#m_-8817584779798990904_> On Sun, Jan 6, 2019 at 6:30 PM Tongxuan Liu ***@***.***> wrote: Do you know what ops are triggering this extra contention on the step container? As we move to tf v2 there will be essentially no use of the step container, so I wonder if you can migrate some of your graph towards the new stuff to make this go away. I also think unrelatedly that we can probably get a reduction in contention by sharding the resource manager mutex somehow. Do you want to take a stab at that? In our models, there's no ops using step container. The problems is even we don't use step_container, TensorFlow's create and delete step_container every step which trigger the serious contention in ParameterServer. My concern is that read-write lock separation has been implemented in ResourceMgr which reduce the lock contentions. But the step_container brings so many write operations that weakened the \"read-write lock separation\" optimization. \u2014 You are receiving this because your review was requested. Reply to this email directly, view it on GitHub <#24260 (comment) <[#24260 (comment)](https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-451803258)>>, or mute the thread https://github.com/notifications/unsubscribe-auth/AAATxYEd98LieNL5yW1qVjKAThIKkjzgks5vArFagaJpZM4ZKnwe . -- - Alex It' a way to use lazily initialization of step container, as following: class ScopedStepContainer : public Container { public: ScopedStepContainer(const int64 step_id, std::function<void(const string&)> cleanup) : name_(strings::StrCat(\"__per_step_\", step_id)), cleanup_(cleanup) {} ~ScopedStepContainer() { if (!is_init) { cleanup_(name_); } } void set_init(bool init) {is_init_ = init;} private: const string name_; const std::function<void(const string&)> cleanup_; }; class ResourceMgr { public: template <typename T> Status LookupOrCreate(const Container& container, const string& name, T** resource, std::function<Status(T**)> creator) { container->set_init(true); //.... } TF_MUST_USE_RESULT; }; the solution have to change APIs (LookupOrCreate and Create in ResourceMgr). I'm afraid that the solution can't reduce the lock contention between normal containers and step containers when we use step_containers' Resource such as stack ops, temporary variables and so on. Besides, class StepContainer is like a \"Lazy Class\"(code smell) which do nothing except wrapper a container name & cleanup function. Probably class Container which wrapper ResourceMgr could be the API not ResourceMgr. Then we can wrapper the changes inside of class Container not as now we have to \"Shotgun Surgery\" so many places. \u2014 You are receiving this because your review was requested. Reply to this email directly, view it on GitHub <[#24260 (comment)](https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-452302020)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxTDON_7TMWSOdsjt6qQpUy4gwl-nks5vBKFYgaJpZM4ZKnwe> .\r\n> -- - Alex\r\n\r\nAs the code which i showed, when FindOrCreate is called, set step_container's init flag. When step_container is destroyed, check the init flag to do cleanup function. ", "Do you want to clean up this code and make it be the PR, then?\n\nOn Tue, Jan 8, 2019 at 6:29 PM Tongxuan Liu <notifications@github.com>\nwrote:\n\n> Maybe we can record whether any call to create() was called between\n> creating and destroying a scoped step container, and skip destruction if no\n> such thing happened? On Tue, Jan 8, 2019 at 5:47 AM Tongxuan Liu\n> notifications@github.com wrote:\n> \u2026 <#m_6348430004503453849_>\n> Interesting. So I think we should focus on making the\n> unused-step-container code path faster. Maybe if we lazily create it\n> instead of forcefully creating it? Maybe we need to store a dirty bit for\n> it? \u2026 <#m_-8817584779798990904_> On Sun, Jan 6, 2019 at 6:30 PM Tongxuan\n> Liu ***@***.> wrote: Do you know what ops are triggering this extra\n> contention on the step container? As we move to tf v2 there will be\n> essentially no use of the step container, so I wonder if you can migrate\n> some of your graph towards the new stuff to make this go away. I also think\n> unrelatedly that we can probably get a reduction in contention by sharding\n> the resource manager mutex somehow. Do you want to take a stab at that? In\n> our models, there's no ops using step container. The problems is even we\n> don't use step_container, TensorFlow's create and delete step_container\n> every step which trigger the serious contention in ParameterServer. My\n> concern is that read-write lock separation has been implemented in\n> ResourceMgr which reduce the lock contentions. But the step_container\n> brings so many write operations that weakened the \"read-write lock\n> separation\" optimization. \u2014 You are receiving this because your review was\n> requested. Reply to this email directly, view it on GitHub <#24260\n> <https://github.com/tensorflow/tensorflow/pull/24260> (comment) <#24260\n> (comment)\n> <https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-451803258>>>,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAATxYEd98LieNL5yW1qVjKAThIKkjzgks5vArFagaJpZM4ZKnwe\n> <https://github.com/notifications/unsubscribe-auth/AAATxYEd98LieNL5yW1qVjKAThIKkjzgks5vArFagaJpZM4ZKnwe>\n> . -- - Alex It' a way to use lazily initialization of step container, as\n> following: class ScopedStepContainer : public Container { public:\n> ScopedStepContainer(const int64 step_id, std::function<void(const string&)>\n> cleanup) : name_(strings::StrCat(\"per_step\", step_id)), cleanup(cleanup) {}\n> ~ScopedStepContainer() { if (!is_init) { cleanup_(name_); } } void\n> set_init(bool init) {is_init_ = init;} private: const string name_; const\n> std::function<void(const string&)> cleanup_; }; class ResourceMgr { public:\n> template Status LookupOrCreate(const Container& container, const string&\n> name, T resource, std::function<Status(T*)> creator) {\n> container->set_init(true); //.... } TF_MUST_USE_RESULT; }; the solution\n> have to change APIs (LookupOrCreate and Create in ResourceMgr). I'm afraid\n> that the solution can't reduce the lock contention between normal\n> containers and step containers when we use step_containers' Resource such\n> as stack ops, temporary variables and so on. Besides, class StepContainer\n> is like a \"Lazy Class\"(code smell) which do nothing except wrapper a\n> container name & cleanup function. Probably class Container which wrapper\n> ResourceMgr could be the API not ResourceMgr. Then we can wrapper the\n> changes inside of class Container not as now we have to \"Shotgun Surgery\"\n> so many places. \u2014 You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub <#24260 (comment)\n> <https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-452302020>>,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAATxTDON_7TMWSOdsjt6qQpUy4gwl-nks5vBKFYgaJpZM4ZKnwe\n> .\n> -- - Alex\n>\n> As the code which i showed, when FindOrCreate is called, set\n> step_container's init flag. When step_container is destroyed, check the\n> init flag to do cleanup function.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/24260#issuecomment-452547874>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxS63aAJgn0mZDQHyRs7UT-g_0P27ks5vBVQTgaJpZM4ZKnwe>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp i'll do the solution as we discussed. I just cleanup the previous code by force push which trigger PR close, please help me to reopen current PR.", "It doesn't let me reopen because it says \"there are no new commits\". Maybe starting a new one will work?"]}, {"number": 24259, "title": "Build On aarch64 Server", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: v1.11.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): 0.15.0\r\n- GCC/Compiler version (if compiling from source): 4.8.5\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory:\r\n\r\n**Describe the problem**\r\nI build tensorflow on aarch64 server. But always got error. I do not understand!!\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n./configure\r\n\r\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nYou have bazel 0.15.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python]: /usr/local/python3/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/python3/lib/python3.6/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/python3/lib/python3.6/site-packages]\r\n\r\nDo you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n\r\nNo jemalloc as malloc support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n\r\nNo Google Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: n\r\nNo Hadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: n\r\nNo Amazon AWS Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: n\r\nNo Apache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: n\r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: n\r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: n\r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with nGraph support? [y/N]: n\r\nNo nGraph support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: n\r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\nConfiguration finished\r\n\r\n\r\n\r\nbazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\n....................\r\nWARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2463:1: in includes attribute of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2548:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2562:1: in includes attribute of cc_library rule //tensorflow/core:stream_executor_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (305 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/com_google_absl/absl/base/BUILD.bazel:117:1: C++ compilation of rule '@com_google_absl//absl/base:base' failed (Exit 1)\r\nexternal/com_google_absl/absl/base/internal/cycleclock.cc:1:0: error: unknown value 'native' for -march\r\n // Copyright 2017 The Abseil Authors.\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 12.825s, Critical Path: 0.34s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\u279c  tensorflow git:(c19e293) bazel build --config=opt  --verbose_failures //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2463:1: in includes attribute of cc_library rule //tensorflow/core:framework_internal_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2548:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/core/BUILD:2562:1: in includes attribute of cc_library rule //tensorflow/core:stream_executor_headers_lib: '../../external/com_google_absl' resolves to 'external/com_google_absl' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/fei/tensorflow/tensorflow/tensorflow.bzl:1373:20\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nWARNING: /home/fei/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.\r\nINFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).\r\nINFO: Found 1 target...\r\nERROR: /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/external/gif_archive/BUILD.bazel:8:1: C++ compilation of rule '@gif_archive//:gif' failed (Exit 1): gcc failed: error executing command\r\n  (cd /home/fei/.cache/bazel/_bazel_fei/6d85cbe619157ee2e987c2321228a66d/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/bin:/usr/bin:/home/fei/bin:/usr/local/sbin:/usr/sbin \\\r\n    PWD=/proc/self/cwd \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/lib/dgif_lib.d -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -g0 '-march=native' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/gif_archive/lib/dgif_lib.c -o bazel-out/host/bin/external/gif_archive/_objs/gif/external/gif_archive/lib/dgif_lib.o)\r\nexternal/gif_archive/lib/dgif_lib.c:1:0: error: unknown value 'native' for -march\r\n /******************************************************************************\r\n ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2.137s, Critical Path: 0.07s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n", "comments": ["@wf0312 as the message said\r\n```\r\n error: unknown value 'native' for -march\r\n```\r\nPlease change the value of optimization of optimization flags\r\n```\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n```\r\neither during configuration or edit your configuration file after configuration.", "> @wf0312 as the message said\r\n> \r\n> ```\r\n>  error: unknown value 'native' for -march\r\n> ```\r\n> Please change the value of optimization of optimization flags\r\n> \r\n> ```\r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]:\r\n> ```\r\n> either during configuration or edit your configuration file after configuration.\r\n\r\n@wf0312  Any update ? Have you tried changing the value of optimization flags as asked above ?"]}, {"number": 24258, "title": "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory", "body": "**System information**\r\n- OS Platform and Distribution (Linux Ubuntu 16.04):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version: 1.12.0 (tensorflow-gpu)\r\n- Python version:2.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- CUDA/cuDNN version: 10\r\n- GPU model and memory: NVIDIA GTX 1060 with Max-Q ; 16 gb DDR4 \r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nexecuting \r\n```shell\r\npython -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"\r\n```\r\ngives the error \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/home/pence/venv/local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\n", "comments": ["1. Latest TensorFlow __supports__ CUDA 8.0-10.0. CuDNN 6-7.\r\n2. Each TensorFlow binary has to work with the version of CUDA and CuDNN it was built with. If they don't match, you have to change either the TensorFlow binary or the Nvidia softwares.\r\n3. Official TensorFlow binaries (the one downloaded by pip or conda) are built with CUDA 9.0, CuDNN 7 since TF 1.5. This means you have to use them with CUDA 9.0, CuDNN 7.\r\n4. If you don't like to change your Nvidia software, you can:\r\n(1) Use non-official binaries built by others. e.g.: https://github.com/mind/wheels/releases, https://github.com/hadim/docker-tensorflow-builder#builds, \r\nhttps://github.com/inoryy/tensorflow-optimized-wheels\r\n(2) Build the binaries by yourself from source  with your version of Nvidia software.", "@ppwwyyxx ; Thanks! building from source did work for me. But I am wondering why isn't still there some version of latest tensorflow which were built with CUDA 10? Technically its like one can't TF 1.12 won't work with CUDA 10 if proceeded via binary install eventhough TF1.12 supports CUDA 10! ", "If the binary install is on CUDA 10, other people on CUDA 9 will come out and complain about the same error.\r\nI think in the next version the binary build will be on CUDA 10 by default.", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24258)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24258)\r\n"]}, {"number": 24257, "title": "ImportError: DLL load failed: The specified module could not be found.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.12.0\r\n- Python version: 3.6.7(also tried 3.7.1)\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): Na\r\n- GCC/Compiler version (if compiling from source): Na\r\n- CUDA/cuDNN version: 9.0\r\n- GPU model and memory: GeForce GTX 1060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI executed \"pip install tensorflow\" command and then wanted to test it by opening python on command prompt. I typed \"python\" and \"import tensorflow\" then I got this error message below: \r\n \r\nPython 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\(User)\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Can you try the solution provided [here](https://gist.github.com/peterjc123/6b804651288e76db7b5fabe5348e1f03).", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 24256, "title": "Update TF Lite Android example", "body": "update to work in actual Android Studio", "comments": ["Thanks for PR. What Android devices has this been tested on?", "@achowdhery Samsung S8", "FWIW I just had to do the same changes for the TFLite Android example to compile. Tested on macOS 10.13.6 and Android Studio 3.0", "Nagging Reviewer @shashishekhar: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 29 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24255, "title": "Fix TF Lite Android Demo app build to work in Android Studio", "body": "**TESTING**\r\n\r\n- Before the fix I was unable to build in either Android Studio 3.1, or Android Studio 3.2.  \r\n- After the fix I could build and launch the app using Android Studio 3.2.1 and emulator Pixel XL API 24.\r\n\r\n**DETAIL**\r\n\r\nHere is a list of seven of the errors resolved by these updates:\r\n\r\n1) Disable Jack to fix compile errors.\r\n\r\nError was:\r\n```\r\norg.gradle.api.tasks.TaskExecutionException: Execution failed for task ':app:transformJackWithJackForDebug'.\r\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.executeActions(ExecuteActionsTaskExecuter.java:84)\r\n\tat org.gradle.api.internal.tasks.execution.ExecuteActionsTaskExecuter.execute(ExecuteActionsTaskExecuter.java:55)\r\n\t... (truncated)\r\n```\r\n\r\nSource: https://developer.android.com/studio/write/java8-support#disable_jack\r\n\r\n2) Update the plugin to support Java 8 features.\r\n\r\nError was:\r\n```\r\nJack is required to support java 8 language features. Either enable Jack or remove sourceCompatibility JavaVersion.VERSION_1_8.\r\n```\r\n\r\nSource: https://developer.android.com/studio/releases/gradle-plugin#updating-plugin\r\n\r\n3) Update repositories to include google()\r\n\r\nError was:\r\n```\r\nCould not find com.android.tools.build:aapt2:3.2.1-4818971.\r\n```\r\n\r\n4) Change 'compile' to 'implementation' in build.gradle\r\n\r\nError was:\r\n```\r\nConfiguration 'compile' is obsolete and has been replaced with 'implementation' and 'api'.\r\nIt will be removed at the end of 2018. For more information see: http://d.android.com/r/tools/update-dependency-configurations.html\r\n```\r\n\r\n5) Update build tools\r\n\r\nError was:\r\n```\r\nThe specified Android SDK Build Tools version (26.0.3) is ignored, as it is below the minimum supported version (28.0.3) for Android Gradle Plugin 3.2.1.\r\nAndroid SDK Build Tools 28.0.3 will be used.\r\nTo suppress this warning, remove \"buildToolsVersion '26.0.3'\" from your build.gradle file, as each version of the Android Gradle Plugin now has a default version of the build tools.\r\nUpdate Build Tools version and sync project\r\nOpen File\r\n```\r\n\r\n6) Update library dependencies\r\n\r\n![screen shot 2018-12-09 at 11 51 26 am](https://user-images.githubusercontent.com/739125/49700514-97d47f80-fbad-11e8-8222-74469ddb705d.png)\r\n\r\n![screen shot 2018-12-09 at 12 16 09 pm](https://user-images.githubusercontent.com/739125/49700518-a6229b80-fbad-11e8-9cf1-50637de78ed1.png)\r\n\r\n**MISC NOTES**\r\n\r\nAndroidManifest.xml shows this warning in Android Studio:\r\n```\r\nThe minSdk version should not be declared in the android manifest file. You can move the version from the manifest to the defaultConfig in the build.gradle file.\r\nOpen Manifest File\r\nRemove minSdkVersion and sync project\r\n```\r\n@jdduke requested we leave the minSdkVersion in AndroidManifest.xml so it can still be built with Bazel.  I added a [comment to the XML](https://github.com/tensorflow/tensorflow/pull/24255/files#diff-b2a1cafabbd44e7687f1ac02600d8da4R26\r\n) to warn other people not to try and fix that warning.", "comments": ["Nagging Reviewer @aselle: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "Can you check the latest version of the demo? It should have most of the changes you're trying to land. Give that a try and let us know, thanks.", "I confirm that the latest version of the demo app on master now builds and runs fine in Android Studio 3.2.1 on macOS 10.13.6.  Closing this PR. \ud83d\udc4d "]}, {"number": 24254, "title": "Right bracket missing in code provided by tensorflow tutorial", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nI believe that a right bracket is missing at the end of following code segment\r\n```\r\nplt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\r\n                                100*np.max(predictions_array),\r\n                                class_names[true_label]),\r\n                                color=color)\r\n```\r\nin part *Prediction*.\r\nAnd the turtorial link is https://www.tensorflow.org/tutorials/keras/basic_classification\r\nNot a big deal.\r\n", "comments": ["I copied the tutorial code in an IDE and I think the code segment has balanced braces. "]}, {"number": 24252, "title": "3D * 2D matrix multiplication without changing batch dimension", "body": "I'm trying to implement tensordot in other ways because of some tensorrt issues.\r\n\r\nIn tf.tensordot op, there is a batch-size modification in the process of 3d * 2d matmul.\r\n\r\n```\r\nM = tf.random_normal((batch_size, n, m))  # (3,6,9)\r\nN = tf.random_normal((m, p)) # (9,9)\r\n\r\nMT = tf.reshape(M, [batch_size*n, m]) # (18,9)\r\nMTN = tf.matmul(M_T, N) # (18,9)\r\n\r\nMN = tf.reshape(MTN, [batch_size, n, p]) # (3,6,9)\r\n```\r\n\r\nBut I want 3d*2d matmul without changing batch-size dimension.\r\nIs there a way?\r\n", "comments": ["It's not clear to me what you are trying to achieve (I guess depends on `axis` used in your tensordot op). But you may need to replicate `N` along the batch dimension (e.g. using `tf.tile`) first.", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n", "There is not direct function, but it can be implemented in a simple way as follows:\r\n```\r\n    MN = tf.matmul(M, tf.tile(N[None], [batch_size, 1, 1]))\r\n```"]}, {"number": 24251, "title": "Unicode header for cmake build", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: N/A\r\n- Installed using virtualenv? pip? conda?: no\r\n- Bazel version (if compiling from source): cmake\r\n- GCC/Compiler version (if compiling from source): MSVC 2015\r\n- CUDA/cuDNN version:  9.0\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\n[unicode_ops.cc](https://github.com/tensorflow/tensorflow/commits/d09a4a1cf2ef4fd0cda021ae73f942ae8b32598a/tensorflow/core/kernels/unicode_ops.cc) includes the ICU header, however this only available on  https://www.npcglib.org/~stathis/blog/precompiled-icu/ for a pre-built release, with 7zip format\r\n\r\nCan anyone help to add this to cmake contrib?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["We encourage users to use bazel to build TensorFlow from sources for windows users, Since we no longer support cmake builds unfortunately.", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24250, "title": "Bug Fix for verbs", "body": "This is to fix the bug of \"invalid use of member in static member function bug\" while building & \"local protection error when doing rdma send bug\" while using RDMA verbs as communication lib.\r\n\r\nRelated issues: #22455  #23220  #23618  #24135 ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "The recent bug was caused by commit 33170cc661f3838aa7d0d7fc19bb0c6ba4812a3c.\r\n\r\nThe new design of Allocator/SubAllocator is good but not working correctly with this part.\r\nThe commit author tried to distribute all the memory allocate operators to Allocator or SubAllocator depending on whether allocator visitors are used.\r\nHowever some parts of the graph process still default using \"cpu_allocator()\" API and this optimization do not work on them. For VERBS implementation, all memory need to be registered to the Infiniband Protect Domain while those Tensors created with \"cpu_allocator()\" API are not. That's the cause of the verbs running bug.\r\n\r\nThis patch has been checked with tensorflow/benchmarks, it should fix these two bugs.\r\n\r\nWaiting to migrate all the \"cpu_allocator()\" to \"ProcessState::singleton()\", and this patch will nolonger be needed.", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the fix! I will try to test it out soon.\r\n\r\nGentle ping @yanivbl6 @jeffdaily", "Preliminary tests have passed on my side. ``REGISTER_MEM_ALLOCATOR`` seems suspicious though. What do you think @poxvoculi? With this fix verbs wouldn't break in r1.13 at least.", "I am experiencing an hang during session initialization when building with verbs and the new patch. I experience the error even when not using distributed load. I will try playing with the bus_id and REGISTER_MEM_ALLOCATOR to see if it is related.", "The use of GetCPUAllocator() inside REGISTER_MEM_ALLOCATOR is slightly surprising, but if it works, fine.  I'll wait for @yanivbl6 to comment before approving.  I hope to clean up the rest of the cpu_allocator() uses within a month.", "Thanks for your reviews. Yeah, the ultimate solution is to move out all the \"cpu_allocator()\" calls. But for now, though it is not very good to use \"REGISTER_MEM_ALLOCATOR\" here, I just did not find any better way... :)", "@jcf94 I tried to run BERT with ParamaterServerStrategy but found the PS process hung up in GetCPUAllocator(int) and could not start up a server. Then i tried another simper model with only 1 layer and the PS process could start up server successfully. Here's the backtrace from gdb:\r\n\r\n(gdb) bt\r\n#0  syscall () at ../sysdeps/unix/sysv/linux/x86_64/syscall.S:38\r\n#1  0x00007f59044ccb41 in __cxa_guard_acquire ()\r\n   from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\r\n#2  0x00007f590df4d9cc in tensorflow::cpu_allocator() ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#3  0x00007f590e26fc95 in tensorflow::ProcessState::GetCPUAllocator(int) ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so\r\n#4  0x00007f590df4f4b9 in tensorflow::AllocatorFactoryRegistry::GetAllocator()\r\n    ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensor-flow/python/../libtensorflow_framework.so\r\n#5  0x00007f590df4d9dd in tensorflow::cpu_allocator() ()\r\n   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/../libtensorflow_framework.so", "I believe the problem occurs when [use_bfc_allocator ](https://github.com/tensorflow/tensorflow/blob/7b1e7138f1fc36dd83e0cb1e8ea9b49d2b576802/tensorflow/core/common_runtime/process_state.cc#L67) is false.\r\n\r\nwhen specifying TF_CPU_ALLOCATOR_USE_BFC=1, the 2 ps+ 2 workers finished successfully on my setup.\r\n\r\nI am not sure if enforcing use_bfc_allocator when compiling with verbs is legal- for the very least, we need to throw an error [here](https://github.com/tensorflow/tensorflow/blob/7b1e7138f1fc36dd83e0cb1e8ea9b49d2b576802/tensorflow/core/common_runtime/process_state.cc#L105), so it wouldn't hang.\r\n\r\nAs mentioned, this error will occur when BFC is turned off even when distributed tensorflow is not used.", "I used force_gpu_compatible session opts so I may not noticed. @CheukNgai could you try specify TF_CPU_ALLOCATOR_USE_BFC=1 environment variable and check again? ", "In our AMD fork, I can confirm that I needed `TF_CPU_ALLOCATOR_USE_BFC=1` to get non-distributed benchmarks to run with this patch.  Unfortunately, I won't be able to evaluate the grpc+verbs path for another week or so due to scheduled maintenance on the systems I've been using.  In any case, I do appreciate the progress made in getting verbs functioning again and understanding what the problem is.", "@byronyi It was found the a core dumped raised after i set up that env variable. Here's the log:\r\n\r\nF tensorflow/core/common_runtime/process_state.cc:124] Check failed: 0 == cpu_allocators_.size() (0 vs. 1)AddCPUAllocVisitor must be called prior to first call to ProcessState::GetCPUAllocator\r\nAborted (core dumped)", "@CheukNgai, are you running the tensorflow/benchmark?\r\nnote that there was an [issue](https://github.com/tensorflow/benchmarks/issues/257) in the benchmark, that caused this error, and was since fixed.\r\nThis problem would persist if the initial session is not started with distributed parameters.\r\nOr this could be a logical error where grpc init could be started before verbs init. I would have expected to encounter it if it was the case, unless there is some race condition.  ", "@yanivbl6 @byronyi My testing model is a distribute version BERT, this model runs normally in grpc or grpc+gdr. Then i tested a 1-layer model for comparison, i can see that verbs will init and start up server normally. I'm convinced that the verb hasn't init in the BERT situation where raised a core dump. Here's full log:\r\n\r\nINFO:tensorflow:ParameterServerStrategy with compute_devices = ('/replica:0/task:0/device:GPU:0', '/replica:0/task:0/device:GPU:1', '/replica:0/task:0/device:GPU:2', '/replica:0/task:0/device:GPU:3'), variable_device = '/device:CPU:0'\r\nINFO:tensorflow:TF_CONFIG environment variable: {u'cluster': {u'ps': [u'10.0.24.3:9108'], u'worker': [u'10.0.24.3:9100']}, u'rpc_layer': u'grpc+verbs', u'task': {u'index': 0, u'type': u'ps'}}\r\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\r\nINFO:tensorflow:RunConfig initialized for Distribute Coordinator with INDEPENDENT_WORKER mode\r\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpnJVBzP\r\nINFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6a56fc6450>, '_model_dir': '/tmp/tmpnJVBzP', '_protocol': 'grpc+verbs', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': <tensorflow.contrib.distribute.python.parameter_server_strategy.ParameterServerStrategy object at 0x7f6a5b237f90>, '_master': '', '_distribute_coordinator_mode': 'independent_worker'}\r\nWARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f6a529eda28>) includes params argument, but params are not passed to Estimator.\r\nINFO:tensorflow:***** Running training *****\r\nINFO:tensorflow:  Num orig examples = 87599\r\nINFO:tensorflow:  Batch size = 32\r\nINFO:tensorflow:  Num steps = 5474\r\nWARNING:tensorflow:From node1.py:685: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.data.experimental.map_and_batch(...)`.\r\nWARNING:tensorflow:From node1.py:665: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\n<DatasetV1Adapter shapes: {input_ids: (32, 384), start_positions: (32,), input_mask: (32, 384), end_positions: (32,), unique_ids: (32,), segment_ids: (32, 384)}, types: {input_ids: tf.int32, start_positions: tf.int32, input_mask: tf.int32, end_positions: tf.int32, unique_ids: tf.int32, segment_ids: tf.int32}>\r\nTrue\r\nINFO:tensorflow:Running `train_and_evaluate` with Distribute Coordinator.\r\nINFO:tensorflow:Running Distribute Coordinator with mode = 'independent_worker', cluster_spec = {u'ps': [u'10.0.24.3:9108'], u'worker': [u'10.0.24.3:9100']}, task_type = u'ps', task_id = 0, environment = None, rpc_layer = u'grpc+verbs'\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nINFO:tensorflow:Multi-worker ParameterServerStrategy with cluster_spec = {u'ps': [u'10.0.24.3:9108'], u'worker': [u'10.0.24.3:9100']}, task_type = u'ps', task_id = 0, num_ps_replicas = 1, is_chief = False, compute_devices = (u'/job:ps/replica:0/task:0/device:GPU:0', u'/job:ps/replica:0/task:0/device:GPU:1', u'/job:ps/replica:0/task:0/device:GPU:2', u'/job:ps/replica:0/task:0/device:GPU:3'), variable_device = <bound method _ReplicaDeviceChooser.device_function of <tensorflow.python.training.device_setter._ReplicaDeviceChooser object at 0x7f6a571e5b90>>\r\nINFO:tensorflow:Starting standard TensorFlow server, target = u'grpc+verbs://10.0.24.3:9108', session_config= allow_soft_placement: true\r\ngraph_options {\r\n  rewrite_options {\r\n    meta_optimizer_iterations: ONE\r\n  }\r\n}\r\n\r\n2018-12-14 05:24:44.271225: F tensorflow/core/common_runtime/process_state.cc:124] Check failed: 0 == cpu_allocators_.size() (0 vs. 1)AddCPUAllocVisitor must be called prior to first call to ProcessState::GetCPUAllocator\r\nAborted (core dumped)", "@ewilderj May I know when will r1.13 be branched? Hope we did not miss it with this PR :(", "Hi @CheukNgai , I tried several models using tensorflow/benchmarks (more than 1-layer, like VGG, ResNet, etc) and did not encounter your bugs.\r\nI guess maybe it's related to Estimator & ParameterServerStrategy ? If you're hurried to train your BERT, maybe try not to use Estimator and see whether bug still exist.\r\nI'll write a simple case in Estimator to check if bug can be reproduced here & try to figure out what happend.", "I just moved the REGISTER_MEM_ALLOCATOR to VerbsServer constructor, so at least this patch will never affect the other part when not using VERBs implementation.\r\n\r\nThe suspicious REGISTER_MEM_ALLOCATOR is under control now, :) @byronyi ", "> @ewilderj May I know when will r1.13 be branched? Hope we did not miss it with this PR :(\r\n\r\nAlas, it was December 13. I emailed the developers@tensorflow.org list with this news. Sorry!", "@xiejw, I tested the latest commit and it's working well from my side, so I approve of the patch.\r\n\r\nI also couldn't replicate the GetCPUAllocator error. @CheukNgai, can you provide a code for replication?", "@jcf94 @yanivbl6 @byronyi In my last try, i can start up server normally after setting up the global env RDMA_DEVICE and TF_CPU_ALLOCATOR_USE_BFC=1. Thanks all :\uff09", "@harshini-gadige Ready to pull?", "@yanivbl6  @byronyi Sorry, guys, i just found this error still exists when running the distribute BERT(this code runs normally in grpc and gdr). FYI, the simple model runs normally with grpc+verbs and it's hard for me to provide a minimal test code right now. I have attached the code in https://github.com/CheukNgai/verbs_test and guys can follow the README.md for replication.", "Today I submitted an internal change that makes cpu_allocator() map to ProcessState::GetCPUAllocator in real runtime instances.  It should be in tonight's build, and it should allow removal of the strange REGISTER_MEM_ALLOCTOR in this PR.  You may want to resync.", "OK, I'll try to follow then.", "FYI, related commit: 7dea8383bbb97a1e78cdece876e083b22191974e", "Thanks for your work @poxvoculi @byronyi \r\nREGISTER_MEM_ALLOCTOR is now removed, but the rest patch is needed.\r\n\r\ntensorflow/benchmarks with \"grpc+verbs\" runs well on my side."]}, {"number": 24249, "title": "Abseil dependency update", "body": "CMake abseil is not updated with latest tf build. Dependency update", "comments": ["@jackyko1991  Please address these failures. Thanks !", "@harshini-gadige I have updated my code by merge to master branch, let's see if it can pass the build"]}, {"number": 24248, "title": "Support for `skip_mismatch` in `tf.keras.engine.saving.load_weights_from_hdf5_group_by_name`", "body": "**System information**\r\n- TensorFlow version (you are using): \r\n```\r\ntensorflow                1.12.0          gpu_py36he68c306_0    anaconda\r\ntensorflow-base           1.12.0          gpu_py36h8e0ae2d_0    anaconda\r\ntensorflow-gpu            1.12.0               h0d30ee6_0    anaconda\r\n```\r\n\r\nThe feature is already supported in `keras-team/keras` and the behaviour and motivation is already explained in: https://github.com/keras-team/keras/pull/8462\r\n", "comments": ["It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n\r\n"]}, {"number": 24247, "title": "fix AttributeError: 'module' object has no attribute '???'", "body": "on windows python 2.7\r\n\r\nhttps://github.com/fo40225/tensorflow-windows-wheel/issues/62\r\n\r\n", "comments": []}, {"number": 24246, "title": "static lib name change", "body": "static lib name should be \"absl_malloc_internal.lib\" for msvc build", "comments": []}, {"number": 24245, "title": "[Bug Fix]Fix bug of passing different assignment_map object created in each replica thread in init_from_checkpoint", "body": "In Replica mode, The `call_for_each_replica` will be called in different thread local storage. The most objects created in `model_fn` in each replica thread will have different id(They are different object) although they share the same Python code.  If the `value` field of `assignment_map` has different object id in different thread, the `regroup` function will wrap them into `PerDevice` even if they have the same values. The original `checkpoint_utils_test` uses constant string to construct `assignment_map` and Python will do optimization to refer to the same constant string in different thread. That's why there was no such problem before.\r\n@yuefengz This is the problem I have encountered when I use `tf.train.list_variables` to construct `assignment_map`. Please take a look. Thanks", "comments": ["Nagging Reviewer @guptapriya: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied.", "@wangsiyu - thank you for catching this issue and sending the PR. Your solution works, but what do you think of this alternative (I think it is much simpler / less error-prone):\r\n\r\n```\r\ndef init_from_checkpoint(ckpt_dir_or_file, assignment_map):\r\n  fn = lambda _: _init_from_checkpoint(ckpt_dir_or_file, assignment_map)\r\n  if distribution_strategy_context.get_cross_replica_context():\r\n    fn()\r\n  else:\r\n    distribution_strategy_context.get_replica_context().merge_call(fn)\r\n\r\ndef _init_from_checkpoint(ckpt_dir_or_file, assignment_map):\r\n  .... # rest of the code remains the same \r\n```\r\n\r\n(Maybe rename `fn` to something better)\r\n\r\nI tested and it passes with your new test case. If you agree, you can update your PR to use this approach, and the extra tests are great. \r\n\r\n\r\n\r\n", "> @wangsiyu - thank you for catching this issue and sending the PR. Your solution works, but what do you think of this alternative (I think it is much simpler / less error-prone):\r\n> \r\n> ```\r\n> def init_from_checkpoint(ckpt_dir_or_file, assignment_map):\r\n>   fn = lambda _: _init_from_checkpoint(ckpt_dir_or_file, assignment_map)\r\n>   if distribution_strategy_context.get_cross_replica_context():\r\n>     fn()\r\n>   else:\r\n>     distribution_strategy_context.get_replica_context().merge_call(fn)\r\n> \r\n> def _init_from_checkpoint(ckpt_dir_or_file, assignment_map):\r\n>   .... # rest of the code remains the same \r\n> ```\r\n> (Maybe rename `fn` to something better)\r\n> \r\n> I tested and it passes with your new test case. If you agree, you can update your PR to use this approach, and the extra tests are great.\r\n\r\nThanks for your comments. This code change seems good. I will update my PR and tests soon.", "Sorry for pending a long time. I have refined the code and solve the conflicts for this PR. I think the lambda should be simplified as below. \r\n```\r\n  init_from_checkpoint_fn = lambda strategy : _init_from_checkpoint(\r\n      strategy, ckpt_dir_or_file, assignment_map)\r\n  if distribution_strategy_context.get_cross_replica_context():\r\n    init_from_checkpoint_fn(None)\r\n  else:\r\n    distribution_strategy_context.get_replica_context().merge_call(\r\n        init_from_checkpoint_fn)\r\n```\r\nThe only params In `init_from_checkpoint_fn` is used for `DistributionStrategy` to pass `strategy` itself. But it also pass an placeholder `None` in `CrossReplica` mode. \r\n\r\nPlease review again. Thanks.\r\n@guptapriya ", "@guptapriya  Sorry for the delayed response again. This code has been refined according to your comments. Thanks for you review. Please check it again.", "@guptapriya Hi, I have fixed lint issue. Please check it again. Thanks very much.", "The failure seems nothing to do with my code change."]}, {"number": 24244, "title": "build windows \"Failed to load the native TensorFlow runtime.\"", "body": "i got this error when i ran this\r\nMicrosoft Windows [Version 10.0.17134.407]\r\n(c) 2018 Microsoft Corporation. All rights reserved.\r\n\r\nC:\\Users\\Nichole>git bash\r\ngit: 'bash' is not a git command. See 'git --help'.\r\n\r\nThe most similar command is\r\n        stash\r\n\r\nC:\\Users\\Nichole>git\r\nusage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\r\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\r\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\r\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\r\n           <command> [<args>]\r\n\r\nThese are common Git commands used in various situations:\r\n\r\nstart a working area (see also: git help tutorial)\r\n   clone      Clone a repository into a new directory\r\n   init       Create an empty Git repository or reinitialize an existing one\r\n\r\nwork on the current change (see also: git help everyday)\r\n   add        Add file contents to the index\r\n   mv         Move or rename a file, a directory, or a symlink\r\n   reset      Reset current HEAD to the specified state\r\n   rm         Remove files from the working tree and from the index\r\n\r\nexamine the history and state (see also: git help revisions)\r\n   bisect     Use binary search to find the commit that introduced a bug\r\n   grep       Print lines matching a pattern\r\n   log        Show commit logs\r\n   show       Show various types of objects\r\n   status     Show the working tree status\r\n\r\ngrow, mark and tweak your common history\r\n   branch     List, create, or delete branches\r\n   checkout   Switch branches or restore working tree files\r\n   commit     Record changes to the repository\r\n   diff       Show changes between commits, commit and working tree, etc\r\n   merge      Join two or more development histories together\r\n   rebase     Reapply commits on top of another base tip\r\n   tag        Create, list, delete or verify a tag object signed with GPG\r\n\r\ncollaborate (see also: git help workflows)\r\n   fetch      Download objects and refs from another repository\r\n   pull       Fetch from and integrate with another repository or a local branch\r\n   push       Update remote refs along with associated objects\r\n\r\n'git help -a' and 'git help -g' list available subcommands and some\r\nconcept guides. See 'git help <command>' or 'git help <concept>'\r\nto read about a specific subcommand or concept.\r\n\r\nC:\\Users\\Nichole>git clone https://github.com/yenchenlin1994/DeepLearningFlappyBird.git\r\nCloning into 'DeepLearningFlappyBird'...\r\nremote: Enumerating objects: 246, done.\r\nremote: Total 246 (delta 0), reused 0 (delta 0), pack-reused 246\r\nReceiving objects: 100% (246/246), 66.29 MiB | 1.01 MiB/s, done.\r\nResolving deltas: 100% (121/121), done.\r\nChecking out files: 100% (49/49), done.\r\n\r\nC:\\Users\\Nichole>cd DeepLearningFlappyBird\r\n\r\nC:\\Users\\Nichole\\DeepLearningFlappyBird>python deep_q_network.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"deep_q_network.py\", line 4, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow_internal', [dirname(__file__)])\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Nichole\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow_internal\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nC:\\Users\\Nichole\\DeepLearningFlappyBird>Failed to load the native TensorFlow runtime.\r\n", "comments": ["Please provide following information asked by the template. Thanks!\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24243, "title": "failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error", "body": "**System information**\r\n- Ubuntu 16.04 LTS\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- CUDA: 9.0\r\n- CUDNN: 7.3\r\n- GPU model and memory: NVIDIA GeForce GTX 1050 Ti\r\n\r\n**Code**\r\n`import tensorflow as tf`\r\n`sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))`\r\n\r\n**Output**\r\n`2018-12-08 22:20:54.515356: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error`\r\n`Device mapping:\r\n/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\r\n/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device`\r\n\r\nI have tried `nvidia-smi` and it works as expected:\r\n\r\nSat Dec  8 22:24:22 2018\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 105...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   51C    P0    N/A /  N/A |    500MiB /  4038MiB |     14%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1357      G   /usr/lib/xorg/Xorg                           300MiB |\r\n|    0      2937      G   compiz                                        58MiB |\r\n|    0      9866      G   ...quest-channel-token=7879820401391808802    92MiB |\r\n|    0     26531      G   ...-token=5979D08CF023D5A85228AD97A282F087    47MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\nAlso, `nvidia-modprobe` is installed.\r\nEven rebooting has not solved the issue.", "comments": ["did you find the solution? I am facing similar problem...", "@SlyStrategist \r\n\r\nMy version of cuDNN was not compatible with tensorflow-gpu 1.12. \r\nThe correct version for tensorflow-gpu 1.12 can be found [here.](https://stackoverflow.com/questions/50622525/which-tensorflow-and-cuda-version-combinations-are-compatible/53727997#53727997)\r\n\r\nIf it still doesn't work, try rebooting.", "System reboot worked for me", "Reboot worked for me, too", "reboot worked for me, too. But still don't know how to avoid this happening"]}, {"number": 24242, "title": "Reduce lock contention in executor, change writelock to readlock", "body": "Ref tensor's lock should be read lock in Executor & mutable_input() of OpKernelContext, there's only read behavior of the ref tensor. \r\nIn large scale distributed ps-worker mode, ps could have serious lock conflict because the ref tensor's lock(variable lock) is shared between different graph. ", "comments": ["Can you address the test failures? Specially around locking code it's very important we get this right.", "1. MacOS Python2 and CC test failure: //tensorflow/core:platform_port_test GetCurrentCPU failure\r\n\"tensorflow/core/platform/port_test.cc:38\r\nExpected: (cpu) >= (0), actual: -1 vs 0\"\r\nThis is unrelated to the changes.\r\n\r\n2. MacOS Contrib test failure: tensorflow/contrib/compiler/xla_test, i ran locally (mac) passed, i don't think related my changes.\r\n"]}, {"number": 24241, "title": "window10+cuda10.0+anaconda3+tensorflow-r1.12", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):window 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.12\r\n- Python version:3.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):0.20\r\n- GCC/Compiler version (if compiling from source):Visual Studio 2015\r\n- CUDA/cuDNN version:10.0\r\n- GPU model and memory:6.0\r\n\r\n**Describe the problem**\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Soft/Anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/Soft/Anaconda3/lib/site-packages --python_path=C:/Soft/Anaconda3/python.exe --define with_ignite_support=true --action_env TF_NEED_OPENCL_SYCL=0 --action_env TF_NEED_ROCM=0 --action_env TF_NEED_CUDA=1 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 --action_env TF_CUDA_VERSION=10.0 --action_env CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0 --action_env TF_CUDNN_VERSION=7 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.0 --action_env TF_CUDA_CLANG=0 --config=cuda --config monolithic --copt=-w --host_copt=-w --verbose_failures --distinct_host_configuration=false --experimental_shortened_obj_file_path=true --define=no_tensorflow_py_deps=true --define=override_eigen_strong_inline=true\r\nERROR: Config value cuda is not defined in any .rc file\r\nINFO: Invocation ID: 958bce6e-94dc-4d56-932a-6fcdc505865d\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nINFO: Reading rc options for 'build' from d:\\code\\tensorflow-r1.12\\.tf_configure.bazelrc:\r\nbuild --action_env PYTHON_BIN_PATH=\"C:/Soft/Anaconda3/python.exe\"\r\nbuild --action_env PYTHON_LIB_PATH=\"C:/Soft/Anaconda3/lib/site-packages\"\r\nbuild --python_path=\"C:/Soft/Anaconda3/python.exe\"\r\nbuild --define with_ignite_support=true\r\nbuild:xla --define with_xla_support=true\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\"\r\nbuild --action_env TF_CUDA_VERSION=\"10.0\"\r\nbuild --action_env CUDNN_INSTALL_PATH=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.0\"\r\nbuild --action_env TF_CUDA_CLANG=\"0\"\r\nbuild --config=cuda\r\ntest --config=cuda\r\nbuild:opt --copt=/arch:AVX\r\nbuild:opt --define with_default_optimizations=true\r\nbuild --config monolithic\r\nbuild --copt=-w --host_copt=-w\r\nbuild --verbose_failures\r\nbuild --distinct_host_configuration=false\r\nbuild --experimental_shortened_obj_file_path=true\r\nbuild --define=no_tensorflow_py_deps=true\r\nbuild --define=override_eigen_strong_inline=true\r\nbuild:v2 --define=tf_api_version=2\r\n", "comments": ["Please take a look at following suggestion which worked for other users.\r\nhttps://github.com/tensorflow/tensorflow/issues/23401#issuecomment-434681778", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24240, "title": "build from source Windows toolchain for CPU error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source):0.20\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.0/7.4.1.5\r\n- GPU model and memory: AMD 2700\r\n\r\nBuilding tensorflow from source I followed this description\r\nhttps://medium.com/@amsokol.com/how-to-build-and-install-tensorflow-gpu-cpu-for-windows-from-source-code-using-bazel-d047d9342b44\r\n\r\nI launch bazel and get the following error \r\n(tensorflow) C:\\Users\\Stefan\\tensorflow-build\\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package\r\n\r\nWARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:\r\nc:\\users\\stefan\\tensorflow-build\\tensorflow/.bazelrc\r\nc:\\users\\stefan\\tensorflow-build\\tensorflow/tools/bazel.rc\r\nWARNING: Option 'experimental_shortened_obj_file_path' is deprecated\r\n\r\nINFO: Invocation ID: 1f1e1f31-ec58-4119-9c1d-385bb8e68863\r\nERROR: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for CPU 'x64_windows', you may want to add an entry for 'x64_windows|msvc-cl' into toolchains and toolchain_identifier 'local_windows' into the corresponding cc_toolchain rule (see --incompatible_disable_cc_toolchain_label_from_crosstool_proto).\r\nINFO: Elapsed time: 3.855s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nHow can I fix this?\r\n\r\nThanks for helping", "comments": ["TensorFlow officially supports NVIDIA\u00ae GPU card with CUDA\u00ae Compute Capability 3.5 or higher.\r\nUnfortunately AMD gpu cards are not supported at this time. However you can still use TensorFlow CPU version which we support across various platforms. ", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24239, "title": "AttributeError: module 'tensorflow' has no attribute 'lite'", "body": "I have tried to convert following keras model into tflite, but got following error. How I can solve this?\r\n\r\n**System information**\r\n- OS Platform: Windows 10\r\n- TensorFlow installed from using anaconda environment\r\n- TensorFlow version: 1.12\r\n- Python version: 3.6.7\r\n- Keras version: 2.2.4\r\n\r\n\r\n**Output from tflite_convert**\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\nAttributeError: module 'tensorflow' has no attribute 'lite'\r\n```\r\n\r\n**Source Code**\r\n\r\n```\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nfrom keras.layers import Activation, Dropout, Flatten, Dense\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\n\r\n# dimensions of our images.\r\nimg_width, img_height = 150, 150\r\n\r\ntrain_data_dir = 'D:\\\\My Projects\\\\Dataset\\\\dataset6_2clz\\\\train'\r\nvalidation_data_dir = 'D:\\\\My Projects\\\\Dataset\\\\dataset6_2clz\\\\validation'\r\n\r\n\r\n\r\nnb_train_samples = 75\r\nnb_validation_samples = 50\r\n#epochs = 50\r\n#batch_size = 16\r\nepochs = 5\r\nbatch_size = 4\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    input_shape = (3, img_width, img_height)\r\nelse:\r\n    input_shape = (img_width, img_height, 3)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, (3, 3), input_shape=input_shape))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(32, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Conv2D(64, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy'])\r\n\r\n\r\n\r\ntrain_datagen = ImageDataGenerator(\r\n    rescale=1. / 255,\r\n    shear_range=0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True)\r\n\r\n\r\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_data_dir,\r\n    target_size=(img_width, img_height),\r\n    batch_size=batch_size,\r\n    class_mode='binary')\r\n\r\nvalidation_generator = test_datagen.flow_from_directory(\r\n    validation_data_dir,\r\n    target_size=(img_width, img_height),\r\n    batch_size=batch_size,\r\n    class_mode='binary')\r\n\r\nmodel.fit_generator(\r\n    train_generator,\r\n    steps_per_epoch=nb_train_samples // batch_size,\r\n    epochs=epochs,\r\n    validation_data=validation_generator,\r\n    validation_steps=nb_validation_samples // batch_size)\r\n\r\n\r\n# Save tf.keras model in HDF5 format.\r\nkeras_file = \"7_try.h5\"\r\nmodel.save('7_try.h5')\r\n\r\n\r\n# Convert to TensorFlow Lite model.\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\ntflite_model = converter.convert()\r\nopen(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n```", "comments": ["In release 1.12, tensorflow lite was still in contrib. It was only after 1.12 that is was moved out of contrib.\r\n\r\ntry tf.contrib.lite.TFLiteConverter", "**I have already tried tf.contrib.lite.TFLiteConverter but it gives following error.**\r\n\r\n```\r\nRuntimeError: TOCO failed see console for info.\r\nb'Traceback (most recent call last):\\r\\n  File \"C:\\\\Users\\\\Asus\\\\Anaconda3\\\\envs\\\\py36\\\\lib\\\\\r\nsite-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 18,\r\n in swig_import_helper\\r\\n    fp, pathname, description = imp.find_module(\\'_tensorflow_wrap_toco\\',\r\ndirname(__file__)])\\r\\n  File \"C:\\\\Users\\\\Asus\\\\Anaconda3\\\\envs\\\\py36\\\\lib\\\\imp.py\", line 297, in\r\nfind_module\\r\\n    raise ImportError(_ERR_MSG.format(name), name=name)\\r\\nImportError: No\r\nmodule named \\'_tensorflow_wrap_toco\\'\\r\\n\\r\\nDuring handling of the above exception, another exception occurred:\\r\\n\\r\\nTraceback (most recent call last):\\r\\n  File \"C:\\\\Users\\\\Asus\\\\Anaconda3\\\\envs\\\\py36\\\\Scripts\\\\toco_from_protos-script.py\",\r\n line 6, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python.toco_from_protos import main\r\n\\r\\n  File \"C:\\\\Users\\\\Asus\\\\Anaconda3\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\toco_from_protos.py\", line 22, in <module>\\r\\n    from tensorflow.contrib.lite.toco.python import tensorflow_wrap_toco\\r\\n  File \"C:\\\\Users\\\\Asus\\\\Anaconda3\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 28, in <module>\\r\\n    _tensorflow_wrap_toco = swig_import_helper()\\r\\n  File \"C:\\\\Users\\\\Asus\\\\Anaconda3\\\\envs\\\\py36\\\\lib\\\\site-packages\\\\tensorflow\\\\contrib\\\\lite\\\\toco\\\\python\\\\tensorflow_wrap_toco.py\", line 20, in swig_import_helper\\r\\n    import _tensorflow_wrap_toco\\r\\nModuleNotFoundError: No module named \\'_tensorflow_wrap_toco\\'\\r\\n'\r\nNone\r\n```", "@lakwinc Is this still an issue for you? Can you try with the latest tf-nightly build?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I have converted\r\n\r\n> @lakwinc Is this still an issue for you? Can you try with the latest tf-nightly build?\r\n\r\nI have converted the model into .tflite using tf-nightly build. So how can I test the converted .tflite model to check whether it is converted correctly or not?", "when will tf 1.13 be released and solve this?", "@lakwinc Can you tell me how did you build using tf-nightly ? I am having the same issue as yours.", "I am not able to convert .pb tensorflow model to tflite model. Can anyone please suggest some other way for the conversion?", "> In release 1.12, tensorflow lite was still in contrib. It was only after 1.12 that is was moved out of contrib.\r\n> \r\n> try tf.contrib.lite.TFLiteConverter\r\n\r\ntry tf.contrib.lite.TocoConverter"]}, {"number": 24238, "title": "Update tables_initializer to link to guide", "body": "Links the `tables_initializer` docstring to the Low Level Intro guide.\r\n\r\nResolves #20629", "comments": ["Nagging Reviewer @ysuematsu: You have been added as a reviewer to this pull request. Please add your review or reassign. It has been 14 days with no activity and the `awaiting review` label has been applied."]}, {"number": 24237, "title": "Fix bazel test failure with mac llvm", "body": "This fix tries to address the issue raised in #24212 where the bazel test for trt_allocator_test fails with Mac llvm. The reason was that `1ul` is `unsigned long` while `uint64_t alignment` is `unsigned long long`.\r\n\r\nThis fix fixes #24212.\r\n\r\n(Update: Tried different ways, the issue is that, `uintptr_t` could be `unsigned long ` or `unsigned long long ` depending on if its gcc or llvm. So `static_cast<uint64>(1)` seems to be the way to make both Mac and Ubuntu work.)\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 24236, "title": "XLA support for Python  instance, class, and static methods", "body": "\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThe current XLA compile API does not support  instance methods.  Existing  code that uses Python objects along with TF api codes needs lot of refactoring to enable XLA. \r\n\r\n**Will this change the current api? How?**\r\nNo change in the  current API (e.g. xla.compile ). \r\n\r\n**Who will benefit with this feature?**\r\nAny  Python developer using XLA\r\n\r\n**Any Other info.**\r\n", "comments": ["Sorry, I don't understand the problem.\r\n\r\nWould you be willing to include a code sample of the thing that doesn't work?", "A Python function works , whereas method does not.  The following code ( modified mnist sample )  should  illustrate the problem  - if you run  it by uncommenting the appropriate line. \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.compiler import xla\r\n\r\n\r\ndef build_mnist_model_func(x, y_):\r\n    y = tf.keras.layers.Dense(MNIST.NUM_CLASSES).apply(x)\r\n    cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)\r\n    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n    return y, train_step\r\n\r\nclass MNIST :\r\n\r\n    # Size of each input image, 28 x 28 pixels\r\n    IMAGE_SIZE = 28 * 28\r\n    # Number of distinct number labels, [0..9]\r\n    NUM_CLASSES = 10\r\n    # Number of examples in each training batch (step)\r\n    TRAIN_BATCH_SIZE = 100\r\n    # Number of training steps to run\r\n    TRAIN_STEPS = 1000\r\n\r\n    def data_ready(self,):\r\n        # Loads MNIST dataset.\r\n        self.traind, self.testd = tf.keras.datasets.mnist.load_data()\r\n        self.train_ds = tf.data.Dataset.from_tensor_slices(self.traind).batch(MNIST.TRAIN_BATCH_SIZE).repeat()\r\n        self.test_ds = tf.data.Dataset.from_tensor_slices(self.testd).batch(MNIST.TRAIN_BATCH_SIZE)\r\n\r\n        self.iterator = tf.data.Iterator.from_structure(self.train_ds.output_types, self.train_ds.output_shapes)\r\n        self.images, self.labels = self.iterator.get_next()\r\n        self.images = tf.reshape(self.images, [-1, MNIST.IMAGE_SIZE])\r\n        self.images, self.labels = tf.cast(self.images, tf.float32), tf.cast(self.labels, tf.int64)\r\n\r\n    def build_mnist_model_meth(x, y_):\r\n        y = tf.keras.layers.Dense(NUM_CLASSES).apply(x)\r\n\r\n        cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)\r\n        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\n        return y, train_step\r\n    \r\n    def train (self, ): \r\n        # works \r\n        #[y] = xla.compile(build_mnist_model_func, inputs=[self.images, self.labels])\r\n        # does not work\r\n        #[y] = xla.compile(self.build_mnist_model_meth, inputs=[self.images, self.labels])\r\n\r\n        # Creates session and initialize all variables.\r\n        # xla.compile() doesn't work with Keras model.fit() API or TF eager mode yet.\r\n        self.sess = tf.Session()\r\n        self.sess.run(tf.global_variables_initializer())\r\n\r\n        # Feeds training dataset\r\n        self.sess.run(self.iterator.make_initializer(self.train_ds))\r\n\r\n        # Runs TRAIN_STEPS steps\r\n        for i in range(MNIST.TRAIN_STEPS):\r\n            self.sess.run(y)\r\n\r\n        print(\"Model trained for %s steps.\" % MNIST.TRAIN_STEPS)\r\n        return y\r\n\r\n    def test (self, y) :\r\n        # Tests trained model\r\n\r\n        # Feeds testing dataset\r\n        self.sess.run(self.iterator.make_initializer(self.test_ds))\r\n\r\n        # Calculates accuracy\r\n        correct_prediction = tf.equal(tf.argmax(y, 1), self.labels)\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n        print(\"Prediction accuracy after training: %s\" % self.sess.run(accuracy))\r\n\r\n        # Cleans up session\r\n        self.sess.close()\r\n\r\nif __name__ == '__main__' :\r\n    mnist = MNIST()\r\n    mnist.data_ready()\r\n    y = mnist.train()\r\n    mnist.test(y)\r\n```", "> #[y] = xla.compile(self.build_mnist_model_meth, inputs=[self.images, self.labels])\r\n\r\nA Python class method is a free function that takes N+1 inputs, the first being `self`.  Here `xla.compile` does not know to pass `self` to the method, so it will call it with the wrong number of arguments.  I think the fact that this does not work as-is is WAI?  I expect you would have exactly the same problem with any other Python callback-based API, e.g. the Python3 `futures` API.\r\n\r\nHere are three potential fixes.\r\n\r\n1. Use a closure:\r\n\r\n```\r\n  def build_model_helper(images, labels):\r\n    return self.build_mnnist_model_meth(images, labels)\r\n  xla.compile(build_model_helper, inputs=[self.images, self.labels])\r\n```\r\n\r\n2. Use a lambda (which is just a closure):\r\n```\r\nxla.compile(lambda images, inputs: self.mnist_model_meth(images, inputs), inputs=[self.images, self.labels])\r\n```\r\n\r\n3. Pass `self` as the 0'th argument:\r\n\r\n```\r\nxla.compile(self.mnist_model_meth, inputs=[self, self.images, self.labels])\r\n```", "Thanks for  the quick response. Here are my findings of your suggested workarounds.\r\n\r\n1. works with the following \r\n```\r\n    def build_mnist_model_meth(self, x, y_):\r\n        y = tf.keras.layers.Dense(MNIST.NUM_CLASSES).apply(x)\r\n\r\n        cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)\r\n        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\n        return y, train_step\r\n    \r\n    def train (self, ):  \r\n        def  build_mnist_model_xla_helper(images, labels):\r\n            return self.build_mnist_model_meth(images, labels) \r\n        # works \r\n        #[y] = xla.compile(build_mnist_model_func, inputs=[self.images, self.labels])\r\n        [y] = xla.compile(build_mnist_model_xla_helper, inputs=[self.images, self.labels])\r\n```\r\n\r\n\r\n3.   Does not work \r\n```\r\nTypeError: Failed to convert object of type <type 'instance'> to Tensor. \r\n```\r\n\r\nMight  help to mention the above workaround in XLA documentation.  \r\n\r\nThe  sample MNIST code as of now with the mods in 1 is as follows. \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.compiler import xla\r\nfrom tensorflow.python.client import timeline\r\n\r\n\r\ndef build_mnist_model_func(x, y_):\r\n    y = tf.keras.layers.Dense(MNIST.NUM_CLASSES).apply(x)\r\n    cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)\r\n    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n    return y, train_step\r\n\r\nclass MNIST :\r\n\r\n    # Size of each input image, 28 x 28 pixels\r\n    IMAGE_SIZE = 28 * 28\r\n    # Number of distinct number labels, [0..9]\r\n    NUM_CLASSES = 10\r\n    # Number of examples in each training batch (step)\r\n    TRAIN_BATCH_SIZE = 100\r\n    # Number of training steps to run\r\n    TRAIN_STEPS = 1000\r\n\r\n    def data_ready(self,):\r\n        # Loads MNIST dataset.\r\n        self.traind, self.testd = tf.keras.datasets.mnist.load_data()\r\n        self.train_ds = tf.data.Dataset.from_tensor_slices(self.traind).batch(MNIST.TRAIN_BATCH_SIZE).repeat()\r\n        self.test_ds = tf.data.Dataset.from_tensor_slices(self.testd).batch(MNIST.TRAIN_BATCH_SIZE)\r\n\r\n        self.iterator = tf.data.Iterator.from_structure(self.train_ds.output_types, self.train_ds.output_shapes)\r\n        self.images, self.labels = self.iterator.get_next()\r\n        self.images = tf.reshape(self.images, [-1, MNIST.IMAGE_SIZE])\r\n        self.images, self.labels = tf.cast(self.images, tf.float32), tf.cast(self.labels, tf.int64)\r\n\r\n    def build_mnist_model_meth(self, x, y_):\r\n        y = tf.keras.layers.Dense(MNIST.NUM_CLASSES).apply(x)\r\n\r\n        cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels=y_, logits=y)\r\n        train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\r\n\r\n        return y, train_step\r\n\r\n    def train (self, ):\r\n        def  build_mnist_model_xla_helper(images, labels):\r\n            return self.build_mnist_model_meth(images, labels)\r\n        # works \r\n        #[y] = xla.compile(build_mnist_model_func, inputs=[self.images, self.labels])\r\n        [y] = xla.compile(build_mnist_model_xla_helper, inputs=[self.images, self.labels])\r\n        # does not work\r\n        #[y] = xla.compile(self.build_mnist_model_meth, inputs=[self.images, self.labels])\r\n        #[y] = xla.compile(self.build_mnist_model_meth, inputs=[self, self.images, self.labels])\r\n\r\n        # Creates session and initialize all variables.\r\n        # xla.compile() doesn't work with Keras model.fit() API or TF eager mode yet.\r\n        self.sess = tf.Session()\r\n        options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        run_metadata = tf.RunMetadata()\r\n        self.sess.run(tf.global_variables_initializer())\r\n\r\n        # Feeds training dataset\r\n        self.sess.run(self.iterator.make_initializer(self.train_ds))\r\n\r\n        # Runs TRAIN_STEPS steps\r\n        for i in range(MNIST.TRAIN_STEPS):\r\n            self.sess.run(y, options=options, run_metadata=run_metadata)\r\n\r\n        print(\"Model trained for %s steps.\" % MNIST.TRAIN_STEPS)\r\n        return y\r\n\r\n    def test (self, y) :\r\n        # Tests trained model\r\n\r\n        # Feeds testing dataset\r\n        self.sess.run(self.iterator.make_initializer(self.test_ds))\r\n\r\n        # Calculates accuracy\r\n        correct_prediction = tf.equal(tf.argmax(y, 1), self.labels)\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n        options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        run_metadata = tf.RunMetadata()\r\n        print(\"Prediction accuracy after training: %s\" % self.sess.run(accuracy, options=options, run_metadata=run_metadata))\r\n\r\n        # Create the Timeline object, and write it to a json file\r\n        fetched_timeline = timeline.Timeline(run_metadata.step_stats)\r\n        chrome_trace = fetched_timeline.generate_chrome_trace_format()\r\n        with open('mnist-xla-test-timeline.json', 'w') as f:\r\n            f.write(chrome_trace)\r\n\r\n        # Cleans up session\r\n        self.sess.close()\r\n\r\nif __name__ == '__main__' :\r\n    mnist = MNIST()\r\n    mnist.data_ready()\r\n    y = mnist.train()\r\n    mnist.test(y)\r\n\r\n```", "OK, glad we got it working!", "> Might help to mention the above workaround in XLA documentation.\r\n\r\nFWIW patches are more than welcome, I believe the docs live in https://github.com/tensorflow/docs."]}, {"number": 24235, "title": "TF-TRT Enhance README", "body": "This enhances the readme file in TF-TRT with some links to examples and docs.", "comments": ["@smit-hinsu I was also thinking of not extending this doc anymore. Once contrib goes away, it will be automatically removed.\r\n\r\nWe also have https://github.com/tensorflow/tensorrt/blob/master/README.md which is currently more or less a duplicate of this one. That will be improved and extended over time.\r\n\r\n"]}, {"number": 24234, "title": "tf.contrib.eager.Saver.__init__ var_list argument should be optional", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12.0\r\n- Are you willing to contribute it (Yes/No): Yes!\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n[tf.contrib.eager.Saver](https://www.tensorflow.org/api_docs/python/tf/contrib/eager/Saver) (aka `tfe.Saver`) claims to be \"A tf.train.Saver adapter for use when eager execution is enabled.\" However, the current api for `__init__()` requires that a `var_list` argument be passed in. \r\n\r\nLooking at [the code](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/eager/python/saver.py#L138), the `var_list` argument is simply delegated to `tf.train.Saver`. The [tf.train.Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver#__init__) does _not_ require `var_list` to be pass in, having it default to `None`. I propose that `tfe.Saver` do the same.\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, but it is a backwards-compatible change. Users of `tfe.Saver` will no longer have to specify a `var_list` argument. Any arguments already present will not change as they will override the default value.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nMy use-case for this feature is that I've written a custom estimator that's run on Cloud ML Engine, and I would like to write a local script that can restore the estimator and be evaluated against held out set of data different from the evaluation set the estimator was trained/tuned with. I'd like to take advantage of eager execution to do this. Not having to invoke `tfe.Saver` with a spurious `None` argument would make the code align with the original `tf.train.Saver` APIs.\r\n\r\n**Any Other info.**\r\n\r\nThanks for the consideration! I know the team gets inundated with a ton of issues and as I mentioned above, I'm more than happy to make the change.\r\n\r\nIt also looks like there's a lot of other kwargs missing from `tfe.Saver`...perhaps it should just accept `*args` and `**kwargs` and forward them to `tf.train.Saver`?\r\n", "comments": ["The issue is that we've removed collections when eager execution is enabled, including the variables collection. Generally collections are just memory leaks waiting to happen executing eagerly, and the global behavior can be error-prone.\r\n\r\nCould you take a look at [object-based saving](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) and see if it works for you? That will collect variables through object dependencies without relying on global collections. It has been designed to be pleasant to use when executing eagerly, for example restoring variables on creation. We're moving away from `tf.train.Saver`, and `tfe.Saver` will be deprecated in the next release.", "> Generally collections are just memory leaks waiting to happen executing eagerly, and the global behavior can be error-prone.\r\n\r\nMakes sense. Thanks for pointing me to the `Checkpoint` reference! I actually think I don't even need that, since my model is an `Estimator` rather than `tf.keras.Model`. I realized I can just pass `tf.train.latest_checkpoint(...)` to `warm_start_from` and get it working:\r\n\r\n```python\r\nestimator = tf.estimator.Estimator(\r\n  model_fn,\r\n  warm_start_from=tf.train.latest_checkpoint(checkpoint_dir))\r\n```", "We have [a guide which includes collecting variables](https://www.tensorflow.org/alpha/guide/checkpoints) now. tfe.* is going away, and Estimator works fine as you say. So I'm closing this as fixed/obsolete."]}, {"number": 24233, "title": "Build Tensorflow 1.12-GPU with CUDA10 Failed On Windows", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MS Windows10 X64 1809 17763\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version:1.12\r\n- Python version:3.6\r\n- Installed using virtualenv? pip? conda?:pip\r\n- Bazel version (if compiling from source):0.15.0\r\n- GCC/Compiler version (if compiling from source):MSVC2015 Update3\r\n- CUDA/cuDNN version:10.0/7.4.1\r\n- GPU model and memory:NVIDIA Geforce RTX2080TI 11GB *2\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to compile tensorflow from source but failed.Follow the guide on the official website(https://tensorflow.google.cn/install/source_windows) to run bazel build and report an error:\r\n.\\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338:\r\n......\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 97.739s, Critical Path: 37.10s\r\nINFO: 234 processes: 234 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\nN/A", "comments": ["I see that you are using CUDA 10.0. TensorFlow officially supports CUDA 9.0. However you can still use CUDA 10.0 but for that you have to build TensorFlow from sources yourself. You can also take a look at an [attempt](https://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-windows/) made by other users to install the same. Thanks!", "> I see that you are using CUDA 10.0. TensorFlow officially supports CUDA 9.0. However you can still use CUDA 10.0 but for that you have to build TensorFlow from sources yourself. You can also take a look at an [attempt](https://www.python36.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-windows/) made by other users to install the same. Thanks!\r\n\r\nTrying to downgrade the CUDA version to 9.0, the same error message appears\uff1a\r\nERROR: D:/code/tensorflow-1.12.0/tensorflow/compiler/tf2xla/BUILD:97:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:cpu_function_runtime' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/william/_bazel_william/ll7iqrfr/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/william/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/william/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\william\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\william\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/_objs/cpu_function_runtime/cpu_function_runtime.o /c tensorflow/compiler/tf2xla/cpu_function_runtime.cc", "You also need to install [Visual C++ Build Tools 2015](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2015) and later [update environment path](https://www.tensorflow.org/install/gpu#windows_setup) for cuda,cuDNN,cupti.", "> You also need to install [Visual C++ Build Tools 2015](https://www.tensorflow.org/install/source_windows#install_visual_c_build_tools_2015) and later [update environment path](https://www.tensorflow.org/install/gpu#windows_setup) for cuda,cuDNN,cupti.\r\n\r\nI have reinstalled MS Visual C++ build tools 2015 Update3, reset the environment variable %PATH, bazel still reports an error.\r\n\r\nlog:\r\nSET PATH\r\nPath=C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\libnvvp;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files (x86)\\Windows Kits\\8.1\\Windows Performance Toolkit\\;C:\\Program Files\\Microsoft SQL Server\\120\\Tools\\Binn\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Intel\\WiFi\\bin\\;C:\\Program Files\\Common Files\\Intel\\WirelessCommon\\;C:\\Program Files\\Microsoft VS Code\\bin;C:\\msys64\\usr\\bin;C:\\bazel;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\extras\\CUPTI\\libx64;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin;C:\\Users\\william\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\;C:\\Users\\william\\AppData\\Local\\Programs\\Python\\Python36\\;C:\\Users\\william\\AppData\\Local\\Microsoft\\WindowsApps;C:\\protoc\\bin;C:\\Program Files\\Intel\\WiFi\\bin\\;C:\\Program Files\\Common Files\\Intel\\WirelessCommon\\\r\nPATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\r\n\r\n\r\nERROR: D:/code/tensorflow-1.12.0/tensorflow/compiler/tf2xla/BUILD:97:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:cpu_function_runtime' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command\r\n  cd C:/users/william/_bazel_william/ll7iqrfr/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\\\winrt;\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\IDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Common7\\Tools;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\bin\\x86;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;;C:\\Windows\\system32\r\n\r\n\r\nbazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package\r\n\u2026\u2026\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/william/AppData/Local/Programs/Python/Python36/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/william/AppData/Local/Programs/Python/Python36/lib/site-packages\r\n    SET TEMP=C:\\Users\\william\\AppData\\Local\\Temp\r\n    SET TF_CUDA_CLANG=0\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=7.0\r\n    SET TF_CUDA_VERSION=9.0\r\n    SET TF_CUDNN_VERSION=7\r\n    SET TF_NEED_CUDA=1\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\william\\AppData\\Local\\Temp\r\n  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/_objs/cpu_function_runtime/cpu_function_runtime.o /c tensorflow/compiler/tf2xla/cpu_function_runtime.cc\r\n\u2026\u2026\r\n.\\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338:\r\n\r\n", "@rootkitchao Is this still an issue for you?", "@ymodak Yes, I still have not been able to successfully compile Tensorflow-GPU with CUDA10.0.I decided to wait for Tensorflow 2.0.", "@chsigg and I ran into a similar issue.\r\nWe believe we are running into a bug in nvcc.", "> @chsigg and I ran into a similar issue.\r\n> We believe we are running into a bug in nvcc.\r\n\r\nThis is a bad news. Will this be fixed?", "It will, eventually.\r\nBut it is likely out of TF control. @chsigg is there a public bug with nvidia we can share?", "This is not an nvcc bug, but rather MSVC producing an unexpected size (24 bytes vs 16 bytes) for this class. See https://godbolt.org/z/YDOgqz\r\n\r\nThis is because MSVC does not merge bit-fields if the size of the types are different (4 bytes for kind_, 8 bytes for size_).\r\n\r\nChanging the type of Kind to uint64 should fix it.", "Hm, but that would also imply that it's not CUDA 10 specific, but XLA specific (because it's XLA code). Is that possible?", "This should be fixed now. I'm closing this issue, please let us know if you are still experiencing issues. Thanks for reporting!", "Thanks to everyone who helped solve this issue.After disabling XLA, I have been able to successfully compile Tensorflow-GPU with CUDA10.0 and got some FP16 performance improvements.", "SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.10240.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\shared;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\8.1\\include\\winrt;\r\nSET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\LIB\\amd64;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\ATLMFC\\LIB\\amd64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.10240.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\8.1\\lib\\winv6.3\\um\\x64;\r\n\r\nI want to know can I choose the include path, I have installed several visual studio"]}, {"number": 24232, "title": "tensorflow.keras: evaluate with custom loss and metric gives wrong output", "body": "This issue is about tensorflow.keras\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip virtual env\r\n- TensorFlow version (use command below):('v1.11.0-0-gc19e29306c', '1.11.0')\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Geforce 940mx\r\n\r\n**Describe the current behavior**\r\nI have a custom metric\r\n`def root_mean_squared_error(y_true, y_pred):\r\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))`\r\nwhile training I get the following output:\r\n\r\n> Train on 161532 samples, validate on 40385 samples\r\n> Epoch 1/5\r\n>  - 132s - loss: 3.6781 - root_mean_squared_error: 3.6781 - val_loss: 3.6463 - val_root_mean_squared_error: 3.6463\r\n> Epoch 2/5\r\n>  - 129s - loss: 3.6528 - root_mean_squared_error: 3.6528 - val_loss: 3.6287 - val_root_mean_squared_error: 3.6287\r\n> Epoch 3/5\r\n>  - 143s - loss: 3.6352 - root_mean_squared_error: 3.6352 - val_loss: 3.6210 - val_root_mean_squared_error: 3.6210\r\n> Epoch 4/5\r\n>  - 134s - loss: 3.6223 - root_mean_squared_error: 3.6223 - val_loss: 3.6369 - val_root_mean_squared_error: 3.6369\r\n> Epoch 5/5\r\n\r\nafter running `model.evaluate(val_data, val_label, verbose = 2)`\r\nI get the following output:\r\n> [3.028181584421266, 3.028181584421266]\r\nfor `model.metrics_names` \r\n> ['loss', 'root_mean_squared_error']\r\n\r\nafter running\r\n`from sklearn.metrics import mean_squared_error\r\nnp.sqrt(mean_squared_error(val_label,np.squeeze(model.predict(val_data, batch_size=256))))`\r\nI get\r\n> 3.7112323807806966`\r\n\r\n**Describe the expected behavior**\r\nall outputs should be the same\r\n\r\nMy model looks like this:\r\n\r\n```\r\ndef get_model(input_size):\r\n    input1 = tf.keras.layers.Input(shape=(input_size,))\r\n    x1 = Dense(input_size, activation='elu')(input1)  # input_size heisst, so viele neurons wie inputs\r\n\r\n    x2 = Dense(300, activation='relu')(x1)\r\n    concat1 = tf.keras.layers.Concatenate(axis=-1)([x1, x2])\r\n    x3 = Dense(300, activation='relu')(concat1)\r\n    concat2 = tf.keras.layers.Concatenate(axis=-1)([concat1, x3])\r\n    x4 = Dense(300, activation='relu')(concat2)\r\n    concat3 = tf.keras.layers.Concatenate(axis=-1)([concat2, x4])\r\n    x5 = Dense(300, activation='relu')(concat3)\r\n    concat4 = tf.keras.layers.Concatenate(axis=-1)([concat3, x5])\r\n    x6 = Dense(300, activation='relu')(concat4)\r\n\r\n    # output Layer\r\n    output = Dense(1, activation='linear')(x6)\r\n    model = tf.keras.models.Model(inputs=input1, outputs=output)\r\n    my_opt = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\r\n    my_loss = root_mean_squared_error\r\n    model.compile(optimizer=my_opt, loss=my_loss,\r\n                  metrics=[root_mean_squared_error])\r\n    return model\r\n```\r\n===========================\r\n\r\nIf I run the exact same thing with\r\n`def my_mean_squared_error(y_true, y_pred):\r\n    return K.mean(K.square(y_pred - y_true))`\r\nand\r\n`model.compile(optimizer=my_opt, loss=my_loss,\r\n                  metrics=[my_mean_squared_error,\"mse\"])`\r\nand\r\n`mean_squared_error(val_label,np.squeeze(model.predict(val_data, batch_size=256)))`\r\nit works fine.\r\n\r\nso it seems, that there is something wrong with K.sqrt()", "comments": ["Apologies for the delay in response. I created a short code snippet to compare the sqrt results by using tf.keras, keras and numpy. The results are identical and correct.\r\n``` python\r\nimport tensorflow as tf\r\nfrom keras import backend as K\r\nimport numpy as np\r\nx = tf.constant(9878389478434.12)\r\nsess = tf.Session()\r\nx_1 = sess.run(x)\r\nprint('tf.sqrt(x)= ' + str(sess.run(tf.sqrt(x))))\r\nprint('K.sqrt(x)= ' + str(sess.run(K.sqrt(x))))\r\nprint('np.sqrt(x_1)= ' + str(np.sqrt(x_1)))\r\n```\r\nOutput:\r\n```python\r\ntf.sqrt(x)= 3142990.5\r\nK.sqrt(x)= 3142990.5\r\nnp.sqrt(x_1)= 3142990.5\r\n```\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 24231, "title": "Adding Backwards Edge in Tensorflow Java", "body": "**System information**\r\n- TensorFlow version (you are using): 1.12\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nAs we started working with Tensorflow Java, we realized that it isn't yet possible to implement while loops in the graph because we cannot mutate the nodes after they have been created. Therefore, we cannot make a backwards edge to create the loop from the NextIteration node to the Merge node, as seen in [the design of while loops](https://github.com/tensorflow/tensorflow/blob/2a050766bf0556d7d92eea62d40fd2bebbcb399f/tensorflow/cc/ops/while_loop.cc#L148).\r\n\r\n**Will this change the current api? How?**\r\nYes. This will allow the graph to be mutated after a node has been created.\r\n\r\n**Who will benefit with this feature?**\r\nThose who want to use Tensorflow Java to make a while loop or other sorts of loops.\r\n\r\n**Any Other info.**\r\n", "comments": ["(Apologies for the delayed response, didn't notice this issue earlier).\r\n\r\nBackward edges are a bit iffy and making the API more prominent makes it easy to create invalid/incorrect graphs, which I'd like to avoid.\r\n\r\nEven in Python we are moving towards a representation of loops in the graph that does not involve backward edges (see https://github.com/tensorflow/tensorflow/blob/1d1aeb64b91effd4329965cc7954a3e21820c88b/tensorflow/python/ops/control_flow_util.py#L31 and the [RFC for functional while loops](https://github.com/tensorflow/community/blob/master/rfcs/20180821-differentiable-functional-while.md)).\r\n\r\nIf implementing while loops in Java, I'd recommend that we pursue this functional while loop approach instead. Thoughts?", "Thanks for the information @asimshankar! \r\n\r\nReading through the RFC, it seems that there is support for taking the gradient of the entire while loop; however, we were wanting to calculate gradients *within* the body of the while loop (for example, training a model via gradient descent). Are there any plans to add support for this use case?", "I'm probably missing something, but why do edges need to be updated when computing the gradient within a single iteration of the loop? The original issue description talks about creating an edge from a `NextIteration` to a `Merge`, but these don't appear within the body subgraph of the loop, right?\r\n\r\nCould you describe the graph that you want to create in a bit more detail? Thanks.", "You are correct that we do not need to use `updateEdge` for anything other than creating an edge from a `NextIteration` to a `Merge`. This issue description was purely to add support for creating a while loop.\r\n\r\nThe functional solution outlined in the RFC looks promising, but from playing around with the python `while_v2` implementation, it errors when we attempt to  generate gradients within the body. We understand this is a work in progress, so just wanted to ask: will it be possible to include `addGradients` (or rather, the subgraph generated by `addGradients`) in the body subgraph in order to perform gradient descent? \r\n", "Yes, that is certainly the intent - if it doesn't work, it would be a bug :)\r\n\r\nGiven that:\r\n- Shall we close this issue and the PR?\r\n- Could you find a new issue demonstrating the issue with gradients in Python with `while_v2`? Though, yeah this is a work in progress and @saxenasaurabh and @skye would know the current state best.", "Sounds good! Thanks so much for all your help @asimshankar and looking forward to any ideas @saxenasaurabh and @skye have for this. I'll get up a new issue tonight!", "Hi @samdow, sorry for missing this earlier! Please assign or mention me in any future while_loop issues you file.\r\n\r\nOne potential problem for Tensorflow Java is that while_v2 gradients are only implemented in Python, and not in the C API. So the design will have to be ported to the C API, or however Tensorflow Java is computing gradients (unless it's somehow calling the Python code)."]}, {"number": 24230, "title": "Problem importing Tensorflow", "body": "\r\n**System information**\r\n- OS Platform Windows 10 \r\n- Dell Inspiron 5500\r\n- TensorFlow installed from (source or binary): pip install\r\n- TensorFlow version:  N/A\r\n- Python version: Python 3.6.7 :: Anaconda custom (64-bit)\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: CUDA 9.0\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n Go into Python shell and import tensorflow and it failed...\r\n\r\n**Any other info / logs**\r\n(tensorflow) C:\\Users\\user>python\r\nPython 3.6.7 |Anaconda custom (64-bit)| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 88, in <module>\r\n    from tensorflow.python import keras\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\activations\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.activations import elu\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\__init__.py\", line 21, in <module>\r\n    from tensorflow.python.keras._impl.keras import activations\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras._impl.keras import backend as K\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py\", line 37, in <module>\r\n    from tensorflow.python.layers import base as tf_base_layers\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 25, in <module>\r\n    from tensorflow.python.keras.engine import base_layer\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\__init__.py\", line 23, in <module>\r\n    from tensorflow.python.keras.engine.base_layer import InputSpec\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 35, in <module>\r\n    from tensorflow.python.keras import backend\r\n  File \"C:\\Users\\user\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\backend\\__init__.py\", line 22, in <module>\r\n    from tensorflow.python.keras._impl.keras.backend import abs\r\nImportError: cannot import name 'abs'\r\n>>>", "comments": ["Please take a look at following issue and its workaround. \r\nhttps://github.com/tensorflow/tensorflow/issues/20778#issuecomment-410962482", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}]