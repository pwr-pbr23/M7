[{"number": 37952, "title": "Update in random_crop", "body": "As per the feature request #37874", "comments": []}, {"number": 37951, "title": "AttributeError: 'ObjectDetectionResult' object has no attribute 'image_id'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.2\r\n- Python version: 3.6.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source) : 0.24.1\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nI try to evaluate my TensorFlow Lite model using the tool in https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection: But when I try to process the dataset, I met some problem. Firstly, I use t the large dataset and limited RAM of my computer. Then I used the 2017 val COCO dataset and not met the memory problem now. But I meet the problem:  AttributeError: 'ObjectDetectionResult' object has no attribute 'image_id'.\r\n\r\nMy code for running :\r\nbazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival -- \\\r\n  --images_folder=/home/sicong/Documents/datasets/COCO/for_tflite_evaluation/val2014 \\\r\n  --instances_file=/home/sicong/Documents/datasets/COCO/annotations_trainval2017/annotations/instances_val2017.json \\\r\n  --whitelist_file=/home/sicong/Documents/datasets/udacity/Udacity_coco/image_id.txt \\\r\n  --output_folder=/home/sicong/Documents/datasets/COCO/for_tflite_evaluation \\\r\n  --num_images=1000\r\n\r\nError Information:\r\nINFO: Build completed successfully, 305 total actions\r\nTraceback (most recent call last):\r\n  File \"/home/sicong/.cache/bazel/_bazel_sicong/7b2e833de2ecb2ea31568e76c69b68a8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\", line 211, in <module>\r\n    _dump_data(ground_truths, args.images_folder, args.output_folder)\r\n  File \"/home/sicong/.cache/bazel/_bazel_sicong/7b2e833de2ecb2ea31568e76c69b68a8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\", line 152, in _dump_data\r\n    detection_result.image_id = image_dict['id']\r\nAttributeError: 'ObjectDetectionResult' object has no attribute 'image_id'\r\n\r\nIs there any solution? Thank you!\r\n", "comments": ["In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!", "Thank you for responses. \r\nI didn't  change any code and use the python binary as follows:\r\nbazel run //tensorflow/lite/tools/evaluation/tasks/coco_object_detection:preprocess_coco_minival --\r\n--images_folder=/home/sicong/Documents/datasets/COCO/for_tflite_evaluation/val2014\r\n--instances_file=/home/sicong/Documents/datasets/COCO/annotations_trainval2017/annotations/instances_val2017.json\r\n--whitelist_file=/home/sicong/Documents/datasets/udacity/Udacity_coco/image_id.txt\r\n--output_folder=/home/sicong/Documents/datasets/COCO/for_tflite_evaluation\r\n--num_images=1000", "hey @Sicongzai, I see that you have used the 2014 images with 2017 instances. That won't work, since the annotations don't correspond to the images in the folder. Try using the [2014 set](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection#preprocessing-the-minival-dataset) files if it works, or 2017 versions of both images & annotations. Mix-and-match will fail :-)", "hey @srjoglekar246  Thank you for your response.\r\n\r\nI try to use 2017 images with 2017 instances. But still met the same problem. The detail can be seen below:\r\nINFO: Running command line: bazel-bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival '--images_folder=/home/sicong/Documents/datasets/COCO/val2017' '--instances_file=/home/sicong/Documents/datasets/COCO/annotations_trainval2017/annotations/instances_val2017.json' '--whitelist_file=/home/sicong/Documents/datasets/COCO/coco_val_2017_image_id.txt' '--output_foldeINFO: Build completed successfully, 305 total actions\r\nTraceback (most recent call last):\r\n  File \"/home/sicong/.cache/bazel/_bazel_sicong/7b2e833de2ecb2ea31568e76c69b68a8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\", line 211, in <module>\r\n    _dump_data(ground_truths, args.images_folder, args.output_folder)\r\n  File \"/home/sicong/.cache/bazel/_bazel_sicong/7b2e833de2ecb2ea31568e76c69b68a8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\", line 152, in _dump_data\r\n    detection_result.image_id = image_dict['id']\r\nAttributeError: 'ObjectDetectionResult' object has no attribute 'image_id'\r\n\r\nWhen I try to use 2014 set, I met the memory problem: (I use CPU with about 4G Memory)\r\nINFO: Running command line: bazel-bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival '--images_folder=/home/sicong/Documents/datasets/COCO/for_tflite_evaluation/val2014' '--instances_file=/home/sicong/Documents/datasets/COCO/for_tflite_evaluation/annotations_trainval2014/annotations/instances_val2014.json' '--whitelist_file=/home/sicong/Documents/model_zoo/models/research/object_detection/data/mscoco_minival_ids.txt' '--output_folder=/homINFO: Build completed successfully, 1 total action\r\nTraceback (most recent call last):\r\n  File \"/home/sicong/.cache/bazel/_bazel_sicong/7b2e833de2ecb2ea31568e76c69b68a8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\", line 210, in <module>\r\n    args.num_images)\r\n  File \"/home/sicong/.cache/bazel/_bazel_sicong/7b2e833de2ecb2ea31568e76c69b68a8/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.runfiles/org_tensorflow/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/preprocess_coco_minival.py\", line 64, in _get_ground_truth_detections\r\n    data_dict = ast.literal_eval(annotation_dump.readline())\r\n  File \"/home/sicong/anaconda3/envs/venv/lib/python3.6/ast.py\", line 48, in literal_eval\r\n    node_or_string = parse(node_or_string, mode='eval')\r\n  File \"/home/sicong/anaconda3/envs/venv/lib/python3.6/ast.py\", line 35, in parse\r\n    return compile(source, filename, mode, PyCF_ONLY_AST)\r\nMemoryError\r\n\r\nOr if the codes are only fit to 2014 set? I also use another dataset like Udacity in COCO format, but still meet the first problem.", "Looks like the [BUILD rule](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/tasks/coco_object_detection/BUILD#L16) for that script is unable to link in the proto dependency. The proto does have the `image_id` attribute, as seen [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/evaluation/proto/evaluation_stages.proto#L234). You could try setting the python version in the BUILD rule to `PY2`, or run the script with python (instead of blaze).", "Both setting the pythonV2 and using python to run the script directly don't work. But, I am successful to do the evaluation after I reinstall tensorflow for source.\r\n\r\nThank you for all the help!  "]}, {"number": 37950, "title": "UnicodeDecodeError: \"utf-8\" codec can't decode byte in position : invalid start byte", "body": "**System information** \r\n\r\n- OS Platform and Distribution : ubuntu 18.04\r\nthe issue happens on mobile device: \r\n- TensorFlowinstalled with pip3  : v1.14.0\r\n\r\nI generate my own dataset .tfrecord with that modify code:\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport hashlib\r\nimport io\r\nimport logging\r\nimport os\r\n\r\nfrom lxml import etree\r\nimport PIL.Image\r\nimport tensorflow as tf\r\n\r\nfrom object_detection.utils import dataset_util\r\nfrom object_detection.utils import label_map_util\r\n\r\n\r\nflags = tf.app.flags\r\nflags.DEFINE_string('data_dir', '', 'Root directory to raw PASCAL VOC dataset.')\r\nflags.DEFINE_string('set', 'train', 'Convert training set, validation set or '\r\n                    'merged set.')\r\nflags.DEFINE_string('annotations_dir', 'Annotations',\r\n                    '(Relative) path to annotations directory.')\r\nflags.DEFINE_string('year', 'VOC2007', 'Desired challenge year.')\r\nflags.DEFINE_string('output_path', '', 'Path to output TFRecord')\r\nflags.DEFINE_string('label_map_path', 'data/pascal_label_map.pbtxt',\r\n                    'Path to label map proto')\r\nflags.DEFINE_boolean('ignore_difficult_instances', False, 'Whether to ignore '\r\n                     'difficult instances')\r\nFLAGS = flags.FLAGS\r\n\r\nSETS = ['train', 'val', 'trainval', 'test']\r\nYEARS = ['VOC2007', 'VOC2012', 'merged']\r\n\r\n\r\ndef dict_to_tf_example(data,\r\n                       dataset_directory,\r\n                       label_map_dict,\r\n                       ignore_difficult_instances=False,\r\n                       image_subdirectory='JPEGImages'):\r\n  \"\"\"Convert XML derived dict to tf.Example proto.\r\n\r\n  Notice that this function normalizes the bounding box coordinates provided\r\n  by the raw data.\r\n\r\n  Args:\r\n    data: dict holding PASCAL XML fields for a single image (obtained by\r\n      running dataset_util.recursive_parse_xml_to_dict)\r\n    dataset_directory: Path to root directory holding PASCAL dataset\r\n    label_map_dict: A map from string label names to integers ids.\r\n    ignore_difficult_instances: Whether to skip difficult instances in the\r\n      dataset  (default: False).\r\n    image_subdirectory: String specifying subdirectory within the\r\n      PASCAL dataset directory holding the actual image data.\r\n\r\n  Returns:\r\n    example: The converted tf.Example.\r\n\r\n  Raises:\r\n    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\r\n  \"\"\"\r\n  print(\" ---- {0} \\n {1} \\n {2}\".format(data['folder'], image_subdirectory, data['filename']))\r\n  img_path = os.path.join(data['folder'], image_subdirectory, data['filename'])\r\n  full_path = os.path.join(dataset_directory, img_path)\r\n  with tf.gfile.GFile(full_path, 'rb') as fid:\r\n    encoded_jpg = fid.read()\r\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\r\n  image = PIL.Image.open(encoded_jpg_io)\r\n  if image.format != 'JPEG':\r\n    raise ValueError('Image format not JPEG')\r\n  key = hashlib.sha256(encoded_jpg).hexdigest()\r\n\r\n  width = int(data['size']['width'])\r\n  height = int(data['size']['height'])\r\n\r\n  xmin = []\r\n  ymin = []\r\n  xmax = []\r\n  ymax = []\r\n  classes = []\r\n  classes_text = []\r\n  truncated = []\r\n  poses = []\r\n  difficult_obj = []\r\n  if 'object' in data:\r\n    for obj in data['object']:\r\n      difficult = bool(int(obj['difficult']))\r\n      if ignore_difficult_instances and difficult:\r\n        continue\r\n\r\n      difficult_obj.append(int(difficult))\r\n\r\n      xmin.append(float(obj['bndbox']['xmin']) / width)\r\n      ymin.append(float(obj['bndbox']['ymin']) / height)\r\n      xmax.append(float(obj['bndbox']['xmax']) / width)\r\n      ymax.append(float(obj['bndbox']['ymax']) / height)\r\n      classes_text.append(obj['name'].encode('utf8'))\r\n      classes.append(label_map_dict[obj['name']])\r\n      truncated.append(int(obj['truncated']))\r\n      poses.append(obj['pose'].encode('utf8'))\r\n\r\n  example = tf.train.Example(features=tf.train.Features(feature={\r\n      'image/height': dataset_util.int64_feature(height),\r\n      'image/width': dataset_util.int64_feature(width),\r\n      'image/filename': dataset_util.bytes_feature(\r\n          data['filename'].encode('utf8')),\r\n      'image/source_id': dataset_util.bytes_feature(\r\n          data['filename'].encode('utf8')),\r\n      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\r\n      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\r\n      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\r\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),\r\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),\r\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),\r\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),\r\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\r\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\r\n      'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\r\n      'image/object/truncated': dataset_util.int64_list_feature(truncated),\r\n      'image/object/view': dataset_util.bytes_list_feature(poses),\r\n  }))\r\n  return example\r\n\r\n\r\ndef main(_):\r\n  if FLAGS.set not in SETS:\r\n    raise ValueError('set must be in : {}'.format(SETS))\r\n  if FLAGS.year not in YEARS:\r\n    raise ValueError('year must be in : {}'.format(YEARS))\r\n\r\n  data_dir = FLAGS.data_dir\r\n  years = ['VOC2007', 'VOC2012']\r\n  if FLAGS.year != 'merged':\r\n    years = [FLAGS.year]\r\n\r\n  writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\r\n\r\n  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\r\n\r\n  for year in years:\r\n    logging.info('Reading from PASCAL %s dataset.', year)\r\n    examples_path = os.path.join(data_dir, 'ImageSets', 'Main',\r\n                                 'default.txt')\r\n    annotations_dir = os.path.join(data_dir, FLAGS.annotations_dir)\r\n    examples_list = dataset_util.read_examples_list(examples_path)\r\n    for idx, example in enumerate(examples_list):\r\n      if idx % 100 == 0:\r\n        logging.info('On image %d of %d', idx, len(examples_list))\r\n      path = os.path.join(annotations_dir, example + '.xml')\r\n      if not os.path.exists(path) : \r\n        continue\r\n      with tf.gfile.GFile(path, 'r') as fid:\r\n        xml_str = fid.read()\r\n      xml = etree.fromstring(xml_str)\r\n      data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\r\n\r\n      tf_example = dict_to_tf_example(data, FLAGS.data_dir, label_map_dict,\r\n                                      FLAGS.ignore_difficult_instances)\r\n      writer.write(tf_example.SerializeToString())\r\n\r\n  writer.close()\r\n\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()`\r\n\r\n```\r\n\r\nI use that command to train my model:\r\n```\r\n`\r\nexport PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\r\nPIPELINE_CONFIG_PATH=/data/ssd_mobilenet_v1_coco_2018_01_28/ssd_mobilenet_v1_coco_2018_01_28/pipeline.config\r\nMODEL_DIR=/data/training\r\nNUM_TRAIN_STEPS=1500\r\nSAMPLE_1_OF_N_EVAL_EXAMPLES=1\r\npython3 object_detection/model_main.py \\\r\n    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n    --model_dir=${MODEL_DIR} \\\r\n    --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n    --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \\\r\n    --alsologtostderr\r\n`\r\n```\r\n\r\nAfter few iterations I get that error:\r\n\r\n```\r\n`  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\r\n    self._evaluate(global_step_value)  # updates self.eval_result\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\r\n    self._evaluator.evaluate_and_export())\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\r\n    hooks=self._eval_spec.hooks)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 477, in evaluate\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 519, in _actual_eval\r\n    return _evaluate()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 501, in _evaluate\r\n    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1501, in _evaluate_build_graph\r\n    self._call_model_fn_eval(input_fn, self.config))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1534, in _call_model_fn_eval\r\n    input_fn, ModeKeys.EVAL)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1022, in _get_features_and_labels_from_input_fn\r\n    self._call_input_fn(input_fn, mode))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1113, in _call_input_fn\r\n    return input_fn(**kwargs)\r\n  File \"/home/label/models/research/object_detection/inputs.py\", line 625, in _eval_input_fn\r\n    params=params)\r\n  File \"/home/label/models/research/object_detection/inputs.py\", line 725, in eval_input\r\n    transform_input_data_fn=transform_and_pad_input_data_fn)\r\n  File \"/home/label/models/research/object_detection/builders/dataset_builder.py\", line 130, in build\r\n    num_additional_channels=input_reader_config.num_additional_channels)\r\n  File \"/home/label/models/research/object_detection/data_decoders/tf_example_decoder.py\", line 319, in __init__\r\n    default_value=''),\r\n  File \"/home/label/models/research/object_detection/data_decoders/tf_example_decoder.py\", line 64, in __init__\r\n    label_map_proto_file, use_display_name=False)\r\n  File \"/home/label/models/research/object_detection/utils/label_map_util.py\", line 172, in get_label_map_dict\r\n    label_map = load_labelmap(label_map_path_or_proto)\r\n  File \"/home/label/models/research/object_detection/utils/label_map_util.py\", line 139, in load_labelmap\r\n    label_map_string = fid.read()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 128, in read\r\n    pywrap_tensorflow.ReadFromStream(self._read_buf, length))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 98, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py\", line 117, in as_str_any\r\n    return as_str(value)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/compat.py\", line 87, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xb4 in position 10: invalid start byte\r\n`\r\n```\r\n\r\nMy tfrecord seems to be corrupted.\r\n", "comments": ["I modidy /models/research/object_detection/data_decoders/tf_example_decoder.py and put in constructor dct_method='INTEGER_FAST' ensure TF version 1.6 compatibility\r\n\r\nEdit : unformtunatly after more iteration I still have the same error", "@xav12358,\r\nCould you please check [this](https://github.com/tensorflow/tensorflow/issues/11312#issuecomment-313444730) comment form a similar issue and let us know if it helps? Thanks!", "> @xav12358,\r\n> Could you please check [this](https://github.com/tensorflow/tensorflow/issues/11312#issuecomment-313444730) comment form a similar issue and let us know if it helps? Thanks!\r\n\r\nAny updates regarding this issue? Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "i hava same problem,i start object_dection\\utils\\config_util.py:\r\nwhen i start this command : python train.py --logtostderr --train_dir=train_dir/ --pipeline_config_path=ssd_mobilenet_v1_coco.config\r\nerror:\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 163, in <module>\r\n    tf.app.run()\r\n  File \"E:\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"E:\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"E:\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"train.py\", line 91, in main\r\n    FLAGS.pipeline_config_path)\r\n  File \"G:\\python\u7ec3\u4e60\\Deep_learn\\objectDetec\\TrainMyModel\\object_detection\\utils\\config_util.py\", line 42, in get_configs_from_pipeline_file\r\n    proto_str = f.read()\r\n  File \"E:\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 128, in read\r\n    pywrap_tensorflow.ReadFromStream(self._read_buf, length))\r\n  File \"E:\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 98, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"E:\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 117, in as_str_any\r\n    return as_str(value)\r\n  File \"E:\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 87, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xc0 in position 365: invalid start byte\r\n\r\nsomeone can help me?", "Please fill in issue template", "> someone can help me?\r\n\r\n@linux-cj,\r\nCould you please provide the complete code to reproduce the issue reported here along with the TensorFlow version you are using? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Unicode errors should be fixed now. If you encounter new errors, please open a new issue, filling in the template.", "I'm going through this tutorial (https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) and when I try to train the model I get this error:\r\n\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x90 in position 2: invalid start byte\r\n\r\nthis is the whole log:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\WIndowsRepositories\\TensorFlow\\workspace\\training_demo\\model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\absl\\app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"d:\\WIndowsRepositories\\TensorFlow\\workspace\\training_demo\\model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 467, in train_loop\r\n    pipeline_config_path, config_override=config_override)\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\object_detection\\utils\\config_util.py\", line 138, in get_configs_from_pipeline_file\r\n    proto_str = f.read()\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 121, in read\r\n    return self._prepare_value(self._read_buf.read(length))\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 93, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 139, in as_str_any\r\n    return as_str(value)\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 118, in as_str\r\n    return as_text(bytes_or_text, encoding)\r\n  File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 109, in as_text\r\n    return bytes_or_text.decode(encoding)\r\n\r\n\r\nAny help is really apreciated, thank you!", "Please open a new issue and fill in issue template", "I'm not the last person but I got a similar problem that I post [here ](https://github.com/tensorflow/tensorflow/issues/45617) (I filled the issue template)\r\nThanks in advance !", "> I'm going through this tutorial (https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html) and when I try to train the model I get this error:\r\n> \r\n> UnicodeDecodeError: 'utf-8' codec can't decode byte 0x90 in position 2: invalid start byte\r\n> \r\n> this is the whole log:\r\n> \r\n> Traceback (most recent call last):\r\n> File \"d:\\WIndowsRepositories\\TensorFlow\\workspace\\training_demo\\model_main_tf2.py\", line 113, in\r\n> tf.compat.v1.app.run()\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n> _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\absl\\app.py\", line 303, in run\r\n> _run_main(main, args)\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n> sys.exit(main(argv))\r\n> File \"d:\\WIndowsRepositories\\TensorFlow\\workspace\\training_demo\\model_main_tf2.py\", line 110, in main\r\n> record_summaries=FLAGS.record_summaries)\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 467, in train_loop\r\n> pipeline_config_path, config_override=config_override)\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\object_detection\\utils\\config_util.py\", line 138, in get_configs_from_pipeline_file\r\n> proto_str = f.read()\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 121, in read\r\n> return self._prepare_value(self._read_buf.read(length))\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 93, in _prepare_value\r\n> return compat.as_str_any(val)\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 139, in as_str_any\r\n> return as_str(value)\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 118, in as_str\r\n> return as_text(bytes_or_text, encoding)\r\n> File \"D:\\ProgrammingWindows\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 109, in as_text\r\n> return bytes_or_text.decode(encoding)\r\n> \r\n> Any help is really apreciated, thank you!\r\n\r\nDid you find solution? Im getting the exact same error."]}, {"number": 37949, "title": "tensorflow Docker behaves different to host (produces nan)", "body": "\r\n**System information** \r\nHave I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Mint 19.3 (ubuntu)\r\nTensorFlow installed from (source or\r\nbinary): binary\r\nTensorFlow version (use command below): tf-nightly\r\nPython version: 3.8 and 3.6\r\n \r\ntwo systems:\r\nHost: NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1 TF: 2.2.0-dev20200325\r\nDocker: NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1 TF: 2.2.0-dev20200325\r\n\r\n**Describe the current behavior**\r\nIn Docker the gradient calculation produces only NANs\r\nOn the host the same model produces normal gradients\r\n\r\n**Describe the expected behavior**\r\nin docker the same gradients should be produced\r\n\r\n**Standalone code to reproduce the issue** \r\ncan not be provided, sorry, very big model, and problem is hard to locate.\r\nif i can locate the problem i will provide a Standalone code\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nGradient magnitude on host:\r\n```\r\n...\r\n    [3.31210213e-05 -2.52332793e-05 -2.79605388e-06 ... -1.31225511e-06 1.79109138e-05 3.6471597e-06]\r\n    [-4.21071627e-06 5.53799646e-06 4.40907201e-07 ... -7.86542842e-07 -6.31262401e-06 -7.09808887e-07]\r\n    [-2.62982558e-05 -6.01945476e-06 -3.60058812e-06 ... 4.37716608e-06 5.87437571e-05 4.91119135e-06]]\r\n```", "comments": ["UPDATE:\r\nOK i found the problem, this is a similar issue to  #511 and  #1244\r\n\r\nit had nothing to do with docker, but with gpu and cpu inferenz.\r\nOn the host the cuda lib could not be loaded and so it ran on cpu and worked, after fixing cuda, it stopt working on gpu on the host to.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37949\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37949\">No</a>\n"]}, {"number": 37948, "title": "[r2.2 Cherrypick]Disabling v2 in the disable_v2_behavior() method", "body": "PiperOrigin-RevId: 299012811\nChange-Id: I311f4b8a6ecbabb658717fff248bea072ff7a366", "comments": []}, {"number": 37947, "title": "Tf", "body": "This Pull request is to create a basic \"gaussian kernel op\". It is necessary image processing tool and is often used with other methods like CannyEdgeDetection etcc to reduce noise etc. So it is an important image processing tool.", "comments": ["@rthadur @tensorflow-jenkins @tensorflower-gardener @tanzhenyu Please provide a review.", "+1 on making this part of Addons, not TF core.", "Or actually keras-image as a separate repo for image related things, for what it's worth", "I have added the file to keras addons", "Please change PR title to something relevant. Please move implementation files to obey the layout of the rest of the code.\r\n\r\nPlease make sure you follow the links mentioned in the previous review and also consider whether this should be a part of TensorFlow Addons instead.", "@ghosalsattam Can you please check @mihaimaruseac's comments and keep us posted. Thanks!"]}, {"number": 37946, "title": "[XLA ]Better error message", "body": "Now we print part of the module name that caused the error, so it is easier to find the right file.", "comments": []}, {"number": 37945, "title": "tensorflow v2  save and load model", "body": "I got a problem for saving model with tensorflow 2.0 ,it's awful,I can not know how to save model with\r\nthat way,it wants me have a signatures for saving my model ,but I can not save my model like :\r\n\r\nclass Model(tf.keras.Model):\r\n\r\n  @tf.function\r\n  def call(self, x):\r\n    ...\r\n\r\nm = Model()\r\ntf.saved_model.save(\r\n    m, '/tmp/saved_model/',\r\n    signatures=m.call.get_concrete_function(\r\n        tf.TensorSpec(shape=[None, 3], dtype=tf.float32, name=\"inp\")))\r\n\r\nI don't know what \"get_concrete_function\" function is;\r\nSomebody can help me?", "comments": ["@wangweitong \r\nwith reference to your question on\"get_concrete_function\", please refer to these [link](https://www.tensorflow.org/guide/concrete_function#using_get_concrete_function), [link-2](https://stackoverflow.com/questions/57542301/is-it-possible-to-train-a-concrete-function)\r\n\r\nCan you share the problem faced and sample code for us to replicate the issue.", "ok ,I will share the code tomorrow ,thank you for your help !!\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:&nbsp;\"Saduf2019\"<notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2020\u5e743\u670827\u65e5(\u661f\u671f\u4e94) \u4e0b\u53485:49\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"2400\"<1097828409@qq.com&gt;;\"Mention\"<mention@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] tensorflow v2  save and load model (#37945)\r\n\r\n\r\n\r\n\r\n\r\n \r\n@wangweitong\r\n with reference to your question on\"get_concrete_function\", please refer to these link, link-2\r\n \r\nCan you share the problem faced and sample code for us to replicate the issue.\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.", "import os\r\nimport pickle\r\n\r\nimport tensorflow as tf\r\n\r\nfrom util.train_model_util_TensorFlow import train_test_model_demo\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\r\n\r\nEPOCHS = 5\r\nBATCH_SIZE = 2048\r\nAID_DATA_DIR = '../data/Criteo/forOtherModels/'  # \u8f85\u52a9\u7528\u9014\u7684\u6587\u4ef6\u8def\u5f84\r\n\r\n\r\nclass DeepFM(tf.keras.Model):\r\n    def __init__(self, num_feat, num_field, dropout_deep, dropout_fm,\r\n                 reg_l1=0, reg_l2=0, layer_sizes=[400, 400, 400], embedding_size=10):\r\n        super().__init__()\r\n        self.reg_l1 = reg_l1\r\n        self.reg_l2 = reg_l2  # L1/L2\u6b63\u5219\u5316\u5e76\u6ca1\u6709\u53bb\u4f7f\u7528\r\n        self.num_feat = num_feat  # denote as M M\u662f \u6709\u591a\u5c11\u4e2a\u7279\u5f81\u5217 14455 \u8fd9\u7684\u7d22\u5f15\u662f\u7528\u6765embedding \u7684\r\n        self.num_field = num_field  # denote as F f\u662f\u7279\u5f81\u6709\u591a\u5c11\u4e2a\u7279\u5f81\u5b57\u6bb5 39\u4e2a\r\n        self.embedding_size = embedding_size  # denote as K k\u662fembedding \u7684\u7ef4\u5ea6\r\n        self.layer_sizes = layer_sizes\r\n\r\n        self.dropout_deep = dropout_deep\r\n        self.dropout_fm = dropout_fm\r\n\r\n        # first order term parameters embedding\r\n        self.first_weights = tf.keras.layers.Embedding(num_feat, 1, embeddings_initializer='uniform')  # None * M * 1\r\n\r\n        # Feature Embedding\r\n        self.feat_embeddings = tf.keras.layers.Embedding(num_feat, embedding_size,\r\n                                                         embeddings_initializer='uniform')  # None * M * K\r\n\r\n        # \u795e\u7ecf\u7f51\u7edc\u65b9\u9762\u7684\u53c2\u6570\r\n        for i in range(len(layer_sizes)):\r\n            setattr(self, 'dense_' + str(i), tf.keras.layers.Dense(layer_sizes[i]))\r\n            setattr(self, 'batchNorm_' + str(i), tf.keras.layers.BatchNormalization())\r\n            setattr(self, 'activation_' + str(i), tf.keras.layers.Activation('relu'))\r\n            setattr(self, 'dropout_' + str(i), tf.keras.layers.Dropout(dropout_deep[i + 1]))\r\n\r\n        # \u6700\u540e\u4e00\u5c42\u5168\u8fde\u63a5\u5c42\r\n        self.fc = tf.keras.layers.Dense(1, activation=\"sigmoid\", use_bias=True)\r\n\r\n    # @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\r\n    # def serve(self, serialized):\r\n    #     print(\"a\")\r\n\r\n    def call(self, feat_data):\r\n        feat_index, feat_value = feat_data\r\n        feat_value = tf.expand_dims(feat_value, axis=-1)  # None * F * 1\r\n\r\n        # Step1: \u5148\u8ba1\u7b97\u4e00\u9636\u7ebf\u6027\u7684\u90e8\u5206 sum_square part\r\n        first_weights = self.first_weights(feat_index)  # None * F * 1\r\n        first_weight_value = tf.math.multiply(first_weights, feat_value)\r\n\r\n        y_first_order = tf.math.reduce_sum(first_weight_value, axis=2)  # None * F\r\n        y_first_order = tf.keras.layers.Dropout(self.dropout_fm[0])(y_first_order)  # None * F\r\n\r\n        # Step2: \u518d\u8ba1\u7b97\u4e8c\u9636\u90e8\u5206\r\n        secd_feat_emb = self.feat_embeddings(feat_index)  # None * F * K\r\n        feat_emd_value = tf.math.multiply(secd_feat_emb, feat_value)  # None * F * K(\u5e7f\u64ad)\r\n\r\n        # sum_square part\r\n        summed_feat_emb = tf.math.reduce_sum(feat_emd_value, axis=1)  # None * K\r\n        interaction_part1 = tf.math.pow(summed_feat_emb, 2)  # None * K\r\n\r\n        # squared_sum part\r\n        squared_feat_emd_value = tf.math.pow(feat_emd_value, 2)  # None * K\r\n        interaction_part2 = tf.math.reduce_sum(squared_feat_emd_value, axis=1)  # None * K\r\n        y_secd_order = 0.5 * tf.math.subtract(interaction_part1, interaction_part2)\r\n        y_secd_order = tf.keras.layers.Dropout(self.dropout_fm[1])(y_secd_order)\r\n\r\n        # Step3: Deep\u90e8\u5206\r\n        y_deep = tf.reshape(feat_emd_value, (-1, self.num_field * self.embedding_size))  # None * (F * K)\r\n        y_deep = tf.keras.layers.Dropout(self.dropout_deep[0])(y_deep)\r\n\r\n        for i in range(len(self.layer_sizes)):\r\n            y_deep = getattr(self, 'dense_' + str(i))(y_deep)\r\n            y_deep = getattr(self, 'batchNorm_' + str(i))(y_deep)\r\n            y_deep = getattr(self, 'activation_' + str(i))(y_deep)\r\n            y_deep = getattr(self, 'dropout_' + str(i))(y_deep)\r\n\r\n        concat_input = tf.concat((y_first_order, y_secd_order, y_deep), axis=1)\r\n        output = self.fc(concat_input)\r\n        return output\r\n\r\n  \r\n\r\n\r\nif __name__ == '__main__':\r\n    # \u83b7\u53d6\u7279\u5f81\u6620\u5c04\u8868\r\n    feat_dict_ = pickle.load(open(AID_DATA_DIR + '/feat_dict_10.pkl2', 'rb'))\r\n    print(len(feat_dict_))\r\n    # \u521b\u5efa\u6a21\u578b\r\n    deepfm = DeepFM(num_feat=len(feat_dict_) + 1, num_field=18,\r\n                    dropout_deep=[0.5, 0.5, 0.5, 0.5], dropout_fm=[0, 0],\r\n                    layer_sizes=[400, 400, 400], embedding_size=10)\r\n    train_label_path = AID_DATA_DIR + 'train_label'\r\n    # \u8fd9\u91cc\u7684\u7d22\u5f15\u662f\u6307\u7684\u6bcf\u4e2a\u7279\u5f81\u5bf9\u5e94\u5b57\u5178\u7684\u7d22\u5f15\r\n    train_idx_path = AID_DATA_DIR + 'train_idx'\r\n    train_value_path = AID_DATA_DIR + 'train_value'\r\n\r\n    test_label_path = AID_DATA_DIR + 'test_label'\r\n    test_idx_path = AID_DATA_DIR + 'test_idx'\r\n    test_value_path = AID_DATA_DIR + 'test_value'\r\n\r\n    # \u8bad\u7ec3\u5e76\u9884\u6d4b\r\n    train_test_model_demo(deepfm, train_label_path, train_idx_path, train_value_path, test_label_path, test_idx_path,\r\n                          test_value_path)\r\n    # deepfm.save_weights(filepath=\"./tfmodel.ckpt\")\r\n\r\n", "It's the code which I have trouble with", "@wangweitong \r\ni have tried to run the code shared by you on nightly, please find the [gist here](https://colab.sandbox.google.com/gist/Saduf2019/3c68f682d5ac697c927378f0a2c36cc5/37945.ipynb)\r\nplease share indented code so we could replicate the error faced by you.", "https://github.com/wangweitong/tensorflow_demo/blob/master/Model/DeepFM_TensorFlow.py\r\n\r\nIt's the code ,you can clone it and run DeepFM_TensorFlow.py ,I don't know how to save the model,\r\nsomebody help me please", "@wangweitong \r\nplease refer to this link for [save and load](https://www.tensorflow.org/tutorials/keras/save_and_load)", "It's not working. It throws exception:\r\nNotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\r\n\r\nI  want save the whole model not just weights", "Try Approach 2 from [Saving and Loading of Subclassed Models](https://www.tensorflow.org/guide/keras/save_and_serialize#approach_2) to save whole model.", "git@github.com:wangweitong/recommend_system.git\r\n\r\nThanks but it's not working,maybe I use that with wrong way,the code in the repository,you can \r\nrun it with tensorflow 2.1 ,the py's code name DeepFM_TensorFlow.py, data and code  are ready for\r\ndebugging,you can try it.", "> Try Approach 2 from [Saving and Loading of Subclassed Models](https://www.tensorflow.org/guide/keras/save_and_serialize#approach_2) to save whole model.\r\n\r\ngit@github.com:wangweitong/recommend_system.git\r\n\r\nThanks but it's not working,maybe I use that with wrong way,the code in the repository,you can\r\nrun it with tensorflow 2.1 ,the py's code name DeepFM_TensorFlow.py, data and code are ready for\r\ndebugging,you can try it.\r\n", "So\uff0canyone solved this problem?", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\nThanks!\r\n"]}, {"number": 37944, "title": "[Fix] Fix context leak in dlpack functions", "body": "Fix the context leak in the dlpack functions. The leakage runs out the thread limitation resources.\r\n\r\nAccording to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/c_api.h#L90-L101, the context needs to be manually deleted.\r\n\r\nReproduce code:\r\n```python\r\nimport tensorflow as tf\r\nimport torch as th\r\nfrom torch.utils.dlpack import to_dlpack, from_dlpack\r\n\r\n# This will hang when i becomes big\r\nfor i in range(10000):\r\n    print(i)\r\n    a = th.ones([100,100])\r\n    dlpack_arr = to_dlpack(a)\r\n    tf_tensor = tf.experimental.dlpack.from_dlpack(dlpack_arr)\r\n```", "comments": []}, {"number": 37943, "title": "when using tensorflow inside flask server, I am getting localhost container not found error.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: pip in a virtual environment\r\n\r\n**Describe the problem**\r\n\r\nI am trying to make a flask server and deploy my model there and connect that server to my android application. Without deploying the model in the server, the flask server and android app are connected successfully. But, after writing my model code in the flask server it gives the following error:\r\n\r\n-----------------------\r\n2020-03-26 17:52:10.868137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-03-26 17:52:10.868137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      \r\n2020-03-26 17:52:11.075640: W tensorflow/core/framework/op_kernel.cc:1651] OP_REQUIRES failed at resource_variable_ops.cc:660 : Not found: Container localhost does not exist. (Could not find resource: localhost/encoder_embedding/embeddings)\r\n[2020-03-26 17:52:11,075] ERROR in app: Exception on / [GET]\r\nTraceback (most recent call last):\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\flask\\app.py\", line 2446, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\flask\\app.py\", line 1951, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\flask\\app.py\", line 1820, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\r\n    raise value\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\flask\\app.py\", line 1949, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\flask\\app.py\", line 1935, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"H:\\PYTHON\\PyCharm Projects\\FlaskServer\\app.py\", line 302, in hello_world\r\n    reply = convert(\"Hi\")\r\n  File \"H:\\PYTHON\\PyCharm Projects\\FlaskServer\\app.py\", line 267, in convert\r\n    initial_state = model_encoder.predict(input_tokens)\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 908, in predict\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 723, in predict\r\n    callbacks=callbacks)\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 394, in model_iteration\r\n    batch_outs = f(ins_batch)\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\", line 3476, in __call__\r\n    run_metadata=self.run_metadata)\r\n  File \"H:\\PYTHON\\Environments\\Flask_env_PyCharm\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\", line 1472, in __call__\r\n    run_metadata_ptr)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Container localhost does not exist. (Could not find resource: localhost/encoder_embedding/embeddings)\r\n\t [[{{node encoder_embedding/embedding_lookup}}]]\r\n127.0.0.1 - - [26/Mar/2020 17:52:11] \"GET / HTTP/1.1\" 500 -\r\n\r\n-------------------------------\r\n\r\n\r\nI opened an issue previously for \".tflite\" conversion. But, as an alternative, I created a flask server. And now this is the only error I am facing. Before I tried with TensorFlow 2.0.0. But, as I had to use 3 or 4 lines of version 1 code, I installed 1.15.0.\r\n\r\nP.S: This issue is related to this [issue](https://github.com/tensorflow/tensorflow/issues/30352). I've tried it in many ways. But, ultimately I face failure. Please, help me fix this error so that I could close both the issues. You need not worry about the responses or results of the trained model. \r\n\r\n--------\r\n\r\nThe code and dataset are in this file:\r\n[code.zip](https://github.com/tensorflow/tensorflow/files/4387137/code.zip)\r\n\r\nI am also placing my drive link for checkpoint files so that you need not train the model.\r\nhttps://drive.google.com/open?id=1wdHQRGMMealZv2ygrSK5LKWGyNgxVNPT\r\n", "comments": ["This is duplicate of the issue #30352 Can you please take a look at this issue and let me know if it helps. Thanks!", "Yes, I also mentioned that I've tried it already. He used LSTM layers while I've used GRU layers. I also tried with graphs like him but, the outcome is only error.", "The issue was solved now. The duplicate issue was incomplete for mine. So, I followed [this](https://stackoverflow.com/questions/54652536/keras-tensorflow-backend-error-tensor-input-10-specified-in-either-feed-de) from StackOverflow and it worked. "]}, {"number": 37942, "title": "GPU-accelerated LSTMs crash randomly with: [ InternalError:  [_Derived_] Failed to call ThenRnnBackward with model config ]", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro N, Build 17763\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Pypi\r\n- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: CUDA 10.1, cudnn-10.1-windows10-x64-v7.6.5.32\r\n- GPU model and memory: GTX 1060, 6 GB\r\n\r\n**Describe the current behavior**\r\n\r\nDear Tensorflow-Developers,\r\n\r\nmy jupyter notebook that is training some LSTMs on the GPU crashes after some time with the following traceback:\r\n\r\n```\r\nInternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 100, 100, 1, 249, 32, 100] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[StatefulPartitionedCall_1]] [Op:__inference_distributed_function_7604]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n```\r\n\r\nThis crash happens after a random amount of epochs (sometimes 6, sometimes 130+ sometimes 300+). It also crashes on different Windows machines with different GPUs.\r\n\r\nPlease see this minimal notebook to reproduce the behaviour that also includes the whole stacktrace: https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5\r\n\r\nIn the stacktrace I can find the following line: \r\n`130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?`\r\n\r\nI wonder if this is connected to this issue? \ud83d\ude42 \r\n\r\nOn a CPU-training everything works well and stable.\r\n\r\nThank you kindly in advance for your consideration and great work. \ud83d\ude80 \r\n\r\n**Describe the expected behavior**\r\n\r\nThe GPU-accelerated LSTM should not crash randomly.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nhttps://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5\r\n\r\n**Other info / logs** \r\n\r\nFor the full traceback, please check the gist from above.\r\n", "comments": ["@jliebers This seems highly related to my issue posted earlier today as well: https://github.com/tensorflow/tensorflow/issues/37932 would love to get some feedback as well.\r\n\r\nI've tried limiting the memory usage, using the nightly build and setting the growth to True (`gpu = tf.config.experimental.list_physical_devices('GPU'), tf.config.experimental.set_memory_growth(gpu[0], True)` to see if it's an issue with GPU memory but I still get the following even when limiting to 2GB:\r\n\r\nIn Jupyter:\r\n\r\n```\r\n[_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 56, 64, 1, 90, 64, 64] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[PartitionedCall_2]] [Op:__inference_train_function_47260]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```\r\n\r\nIn my terminal window:\r\n\r\n```\r\n2020-03-26 21:34:20.415064: E tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1986): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.param", "@jliebers,\r\nI was able to run the above code without any issues for 500 epochs. However I'm facing an error stating \r\n```IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices``` on running the `model.evaluate()` cell. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/413e6d7a250b276a91b72c0166aa4663/notebook.ipynb). Thanks!", "@amahendrakar \r\n\r\nHi, I am sorry, the very last cell (line 198 to 206) contained a bug and should have been:\r\n\r\n```\r\nprint(\"Starting model.evaluate().\")\r\nif use_cpu:\r\n    with tf.device(\"/device:CPU:0\"):\r\n        evaluation = model.evaluate(x=DATA_NORMALIZED[split_index:],\r\n                                    y=LABELS_OHC[split_index:])\r\nelse:\r\n    evaluation = model.evaluate(x=DATA_NORMALIZED[split_index:],\r\n                                y=LABELS_OHC[split_index:])\r\nprint(\"Finished model.evaluate().\")\r\n```\r\n\r\nI corrected and re-ran your notebook.\r\n\r\nAlso, maybe a solution was found for my problem (kudos to @DietmarKracht). Let me quickly verify it. Will update this issue asap. \ud83d\ude42 ", "Hi, \r\n\r\nso the following **workaround** has been found thanks to @DietmarKracht. Now I am able to train LSTMs on my GPU without the error from the first post in this issue. \ud83c\udf89 \ud83c\udf89 \ud83c\udf89 \r\n\r\nTo train the LSTM-model on a GPU on my plattform (Windows 10, tf 2.1.0), the parameter `batch_input_shape` **must** be specified during the model creation in the very first layer and the parameter `input_shape` _must_ be omitted. The first layer of the LSTM-model should looks this: \r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(LSTM(100, \r\n    batch_input_shape=(batch_size, n_timesteps, n_features), \r\n    return_sequences=True))  # omit return_sequences if no other LSTM-layer follows\r\n[...]\r\n```\r\n\r\nNotice: I assume that if `batch_input_shape` is not specified it will default to some value and this issue arises randomly from the consequences. As it could not be reproduced in colab, I guess that it is a platform-specific platform (see first post for my specs).\r\n\r\n**Important:** An int, `batch_size`, is specified in `batch_input_shape=(batch_size, n_timesteps, n_features)`.  The int must divide divide the length of `X` (`X` is passed to `model.fit()`) _without_ any rest, i.e. `len(X) % batch_size == 0`! Additionally one must not use the `validation_split`-parameter in model.fit() (see issue #37840).\r\n\r\nThen it works without any issues with tf 2.1.0 on Windows 10 (finally! phew!). \ud83d\ude42 \r\n\r\nPlease find a minimal example (works for me on GPU and CPU) here: https://gist.github.com/jliebers/7effb38e836ab3c6e95bd122589f5f92\r\n\r\nSadly, it is nowhere mentioned in the documentation and it took us a week to solve this issue. I hope this post is helpful for people in the future.\r\n\r\nUpdate:\r\n\r\nShould the kernel die at any point with the following trace, then lower your batch_size to a smaller divisor of len(x):\r\n```\r\n2020-03-27 15:01:57.982960: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-03-27 15:01:57.983072: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n```", "After following the solutions suggested like:\r\n\r\n- Allowing GPU Memory Growth\r\n- Using `batch_input_shape` instead of `input_shape`\r\n- Using `drop_remainder=True` when creating batches\r\n\r\nI'm faced with the following after the first successfully trained model:\r\n\r\n```\r\n2020-03-27 17:21:28.275596: F .\\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)\r\n[I 17:21:29.843 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\nkernel 2bac517a-c195-47ca-952b-c25881cf0757 restarted\r\n```", "Yes, allowing GPU memory growth is also necessary, i.e. \r\n```\r\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\nfor device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\r\n```\r\n\r\nAnd I agree with the bullet points you posted. They are required to make it work on my setup.", "> Hi,\r\n> \r\n> so the following **workaround** has been found thanks to @DietmarKracht. Now I am able to train LSTMs on my GPU without the error from the first post in this issue. \ud83c\udf89 \ud83c\udf89 \ud83c\udf89\r\n> \r\n> To train the LSTM-model on a GPU on my plattform (Windows 10, tf 2.1.0), the parameter `batch_input_shape` **must** be specified during the model creation in the very first layer and the parameter `input_shape` _must_ be omitted. The first layer of the LSTM-model should looks this:\r\n> \r\n> ```\r\n> model = Sequential()\r\n> model.add(LSTM(100, \r\n>     batch_input_shape=(batch_size, n_timesteps, n_features), \r\n>     return_sequences=True))  # omit return_sequences if no other LSTM-layer follows\r\n> [...]\r\n> ```\r\n> \r\n> Notice: I assume that if `batch_input_shape` is not specified it will default to some value and this issue arises randomly from the consequences. As it could not be reproduced in colab, I guess that it is a platform-specific platform (see first post for my specs).\r\n> \r\n> **Important:** An int, `batch_size`, is specified in `batch_input_shape=(batch_size, n_timesteps, n_features)`. The int must divide divide the length of `X` (`X` is passed to `model.fit()`) _without_ any rest, i.e. `len(X) % batch_size == 0`! Additionally one must not use the `validation_split`-parameter in model.fit() (see issue #37840).\r\n> \r\n> Then it works without any issues with tf 2.1.0 on Windows 10 (finally! phew!). \ud83d\ude42\r\n> \r\n> Please find a minimal example (works for me on GPU and CPU) here: https://gist.github.com/jliebers/7effb38e836ab3c6e95bd122589f5f92\r\n> \r\n> Sadly, it is nowhere mentioned in the documentation and it took us a week to solve this issue. I hope this post is helpful for people in the future.\r\n> \r\n> Update:\r\n> \r\n> Should the kernel die at any point with the following trace, then lower your batch_size to a smaller divisor of len(x):\r\n> \r\n> ```\r\n> 2020-03-27 15:01:57.982960: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n> 2020-03-27 15:01:57.983072: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n> ```\r\n\r\nReally helps a lot! Thanks.", "@jliebers,\r\nIs this still an issue? Please feel free to close the issue if resolved. Thanks!", "I still have this issue, it happens with every type of RNN i'v tested. I tried the fixes above still nothing.\r\n\r\n```\r\nCUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2020-04-01 18:39:13.944813: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 16, 1, 264, 64, 16]\r\n2020-04-01 18:39:13.953384: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 16, 1, 264, 64, 16]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n2020-04-01 18:39:13.963840: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_8907_9081_specialized_for_StatefulPartitionedCall_at___inference_train_on_batch_10423_specialized_for_StatefulPartitionedCall_at___inference_train_on_batch_10423}} {{function_node __inference___backward_cudnn_lstm_with_fallback_8907_9081_specialized_for_StatefulPartitionedCall_at___inference_train_on_batch_10423_specialized_for_StatefulPartitionedCall_at___inference_train_on_batch_10423}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 16, 1, 264, 64, 16]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n         [[StatefulPartitionedCall]]\r\nTraceback (most recent call last):\r\n  File \"cartpole.py\", line 78, in <module>\r\n    dqn.fit(env, callbacks=callbacks, nb_steps=steps, visualize=False, verbose=1,nb_max_episode_steps=(120))\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\rl\\core.py\", line 205, in fit\r\n    metrics = self.backward(reward, terminal=done)\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\rl\\agents\\dqn.py\", line 327, in backward\r\n    metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1078, in train_on_batch\r\n    standalone=True)\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n  File \"C:\\Users\\decsg\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 16, 16, 1, 264, 64, 16]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n         [[StatefulPartitionedCall]] [Op:__inference_train_on_batch_10423] ```", "This is my model\r\n```\r\nmodel = Sequential()\r\nmodel.add(Reshape([264,16], batch_input_shape=(64,1,264,16)))\r\nmodel.add(LSTM(16, return_sequences=True))\r\nmodel.add(LSTM(16, return_sequences=True))\r\nmodel.add(LSTM(16, return_sequences=False))\r\nmodel.add(Dense(nb_actions, activation='softmax'))\r\nprint(model.summary())\r\n```", "@FunkyGibbon,\r\nCould you please create a new issue from [this](https://github.com/tensorflow/tensorflow/issues/new/choose) link and fill in the template details, so that we can track the issue there? Thanks!", "Hi, I have this issue since yesterday. Before that, everything was working fine. I have not updated either the tensorflow version or the gpu driver or anything else for that matter in the last couple of days. This issue suddenly appeared just yesterday on its own. Below is my model,\r\n\r\n```\r\ndef neural_network(vocab_size, embedding_dim, max_length, train_padded, train_labels, validation_frac, num_epochs):\r\n    model = Sequential()\r\n    model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))\r\n    model.add(Bidirectional(LSTM(64, return_sequences = True)))\r\n    model.add(GlobalAveragePooling1D())\r\n    model.add(Dropout(0.2))\r\n    model.add(Dense(50, activation = 'relu'))\r\n    model.add(Dropout(0.1))\r\n    model.add(Dense(1, activation = 'sigmoid'))\r\n    model.summary()\r\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n    history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n    return model, history\r\n```\r\n\r\nI am pretty sure I am not running out of memory as previously I have trained even bigger models (~20M params) but the above model has just 2M elements. Even the GPU usage barely exceeds 5%. I have GTX 1050ti 4GB. Also, I have successfully run the above model plenty of times before but only since yesterday this issue is coming up. \r\n\r\n```\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\nembedding (Embedding)        (None, 200, 128)          1920000   \r\n_________________________________________________________________\r\nbidirectional (Bidirectional (None, 200, 128)          98816     \r\n_________________________________________________________________\r\nglobal_average_pooling1d (Gl (None, 128)               0         \r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 128)               0         \r\n_________________________________________________________________\r\ndense (Dense)                (None, 50)                6450      \r\n_________________________________________________________________\r\ndropout_1 (Dropout)          (None, 50)                0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 1)                 51        \r\n=================================================================\r\nTotal params: 2,025,317\r\nTrainable params: 2,025,317\r\nNon-trainable params: 0\r\n```\r\n\r\nBelow is the exact error. \r\n\r\n```\r\nTrain on 143613 samples, validate on 15958 samples\r\nEpoch 1/5\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 122, in <module>\r\n    model, history = neural_network(vocab_size, embedding_dim, max_length, train_padded, toxicity[col], validation_frac, num_epochs)\r\n\r\n  File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 87, in neural_network\r\n    history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n    self.captured_inputs)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n    ctx, args, cancellation_manager=cancellation_manager))\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n    ctx=ctx)\r\n\r\n  File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n\r\n  File \"<string>\", line 3, in raise_from\r\n\r\nInternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[StatefulPartitionedCall_1]]\r\n\t [[Reshape_14/_46]] [Op:__inference_distributed_function_5894]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n```\r\n\r\nAlso, once this issue occurs, the kernel keeps crashing on its own _even if I am not compiling anything_. I am using Spyder IDE and even restarting the kernel does not help; it simply crashes after a few seconds. Below is the log for that (it is written on a red background)\r\n\r\n```\r\nAn error ocurred while starting the kernel\r\n2020\udae1\udea8\udae1\udea7 09:10:48.429373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.630422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.654232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020\udae1\udea8\udae1\udea7 10:25:24.655330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.660386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.665212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.667487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.672356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.675255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.685248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.686442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020\udae1\udea8\udae1\udea7 10:25:24.687128: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020\udae1\udea8\udae1\udea7 10:25:24.689769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\npciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\ncoreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n2020\udae1\udea8\udae1\udea7 10:25:24.690853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.691406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.691954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.692497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.693046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.693600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.694155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:24.695292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n2020\udae1\udea8\udae1\udea7 10:25:25.261369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020\udae1\udea8\udae1\udea7 10:25:25.261990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 \r\n2020\udae1\udea8\udae1\udea7 10:25:25.262349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N \r\n2020\udae1\udea8\udae1\udea7 10:25:25.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2990 MB memory) \u2011> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)\r\n2020\udae1\udea8\udae1\udea7 10:25:27.808120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n2020\udae1\udea8\udae1\udea7 10:25:28.068884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n2020\udae1\udea8\udae1\udea7 10:26:06.218534: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data\u2011>opaque(), input_h_desc.handle(), input_h_backprop_data\u2011>opaque(), input_c_desc.handle(), input_c_backprop_data\u2011>opaque(), workspace.opaque(), workspace.size(), reserve_space_data\u2011>opaque(), reserve_space_data\u2011>size())'\r\n2020\udae1\udea8\udae1\udea7 10:26:06.221888: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n2020\udae1\udea8\udae1\udea7 10:26:06.223371: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n2020\udae1\udea8\udae1\udea7 10:26:06.225063: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n[[StatefulPartitionedCall_1]]\r\n[[Reshape_14/_46]]\r\n2020\udae1\udea8\udae1\udea7 10:26:06.228126: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n[[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n[[StatefulPartitionedCall_1]]\r\n2020\udae1\udea8\udae1\udea7 10:35:24.183417: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non\u2011OK\u2011status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: unspecified launch failure\r\n```\r\n\r\nTo get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22. ", "> @jliebers,\r\n> Is this still an issue? Please feel free to close the issue if resolved. Thanks!\r\n\r\nAny updates regarding this issue? Thanks!\r\n", "> To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.\r\n\r\n@diggee,\r\nI'd request you to submit a new issue using this [link](https://github.com/tensorflow/tensorflow/issues/new/choose), so that we can track it there.", "> > To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.\r\n> \r\n> @diggee,\r\n> I'd request you to submit a new issue using this [link](https://github.com/tensorflow/tensorflow/issues/new/choose), so that we can track it there.\r\n\r\nalready did. ", "Hey all,\r\n\r\nso I could just verify that all of the problems in this issue are connected with the Windows-version of tensorflow. I installed a fresh Ubuntu Bionic and followed the instructions [0] to set it up with CUDA on my homeoffice-workstation with a GTX 1060 6GB and none of the described problems was ever encountered again. :/\r\n\r\nSo please save yourself the trouble and just use Linux to train your LSTMs on the GPU. Even the `validation_split`-parameter works there without any problem and I had not to set up any `batch_input_shape` with divisible sets for training, validation, etc. Now it is just so easy.\r\n\r\nCheers,\r\n\r\nJonathan\r\n\r\n[0] https://www.tensorflow.org/install/gpu#linux_setup", "@jliebers,\r\nI this still an issue? Please feel free to close the issue if resolved. Thanks!", "Hey @amahendrakar ,\r\n\r\nfor me personally it is not an issue anymore since I switched my OS.\r\n\r\nFor tensorflow though, which officially supports MS Windows, it is an open issue and of course for all Windows-users aswell. \ud83d\ude09 ", "@jliebers,\r\nThank you for the update. Will pass on the feedback to the concerned team. Marking this issue as closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37942\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37942\">No</a>\n", "> Hi, I have this issue since yesterday. Before that, everything was working fine. I have not updated either the tensorflow version or the gpu driver or anything else for that matter in the last couple of days. This issue suddenly appeared just yesterday on its own. Below is my model,\r\n> \r\n> ```\r\n> def neural_network(vocab_size, embedding_dim, max_length, train_padded, train_labels, validation_frac, num_epochs):\r\n>     model = Sequential()\r\n>     model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))\r\n>     model.add(Bidirectional(LSTM(64, return_sequences = True)))\r\n>     model.add(GlobalAveragePooling1D())\r\n>     model.add(Dropout(0.2))\r\n>     model.add(Dense(50, activation = 'relu'))\r\n>     model.add(Dropout(0.1))\r\n>     model.add(Dense(1, activation = 'sigmoid'))\r\n>     model.summary()\r\n>     model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n>     history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n>     return model, history\r\n> ```\r\n> \r\n> I am pretty sure I am not running out of memory as previously I have trained even bigger models (~20M params) but the above model has just 2M elements. Even the GPU usage barely exceeds 5%. I have GTX 1050ti 4GB. Also, I have successfully run the above model plenty of times before but only since yesterday this issue is coming up.\r\n> \r\n> ```\r\n> Model: \"sequential\"\r\n> _________________________________________________________________\r\n> Layer (type)                 Output Shape              Param #   \r\n> =================================================================\r\n> embedding (Embedding)        (None, 200, 128)          1920000   \r\n> _________________________________________________________________\r\n> bidirectional (Bidirectional (None, 200, 128)          98816     \r\n> _________________________________________________________________\r\n> global_average_pooling1d (Gl (None, 128)               0         \r\n> _________________________________________________________________\r\n> dropout (Dropout)            (None, 128)               0         \r\n> _________________________________________________________________\r\n> dense (Dense)                (None, 50)                6450      \r\n> _________________________________________________________________\r\n> dropout_1 (Dropout)          (None, 50)                0         \r\n> _________________________________________________________________\r\n> dense_1 (Dense)              (None, 1)                 51        \r\n> =================================================================\r\n> Total params: 2,025,317\r\n> Trainable params: 2,025,317\r\n> Non-trainable params: 0\r\n> ```\r\n> \r\n> Below is the exact error.\r\n> \r\n> ```\r\n> Train on 143613 samples, validate on 15958 samples\r\n> Epoch 1/5\r\n> Traceback (most recent call last):\r\n> \r\n>   File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 122, in <module>\r\n>     model, history = neural_network(vocab_size, embedding_dim, max_length, train_padded, toxicity[col], validation_frac, num_epochs)\r\n> \r\n>   File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 87, in neural_network\r\n>     history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n>     use_multiprocessing=use_multiprocessing)\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n>     total_epochs=epochs)\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n>     batch_outs = execution_function(iterator)\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n>     distributed_function(input_fn))\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n>     result = self._call(*args, **kwds)\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n>     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n>     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n>     self.captured_inputs)\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n>     ctx, args, cancellation_manager=cancellation_manager))\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n>     ctx=ctx)\r\n> \r\n>   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n>     six.raise_from(core._status_to_exception(e.code, message), None)\r\n> \r\n>   File \"<string>\", line 3, in raise_from\r\n> \r\n> InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> \t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> \t [[StatefulPartitionedCall_1]]\r\n> \t [[Reshape_14/_46]] [Op:__inference_distributed_function_5894]\r\n> \r\n> Function call stack:\r\n> distributed_function -> distributed_function -> distributed_function\r\n> ```\r\n> \r\n> Also, once this issue occurs, the kernel keeps crashing on its own _even if I am not compiling anything_. I am using Spyder IDE and even restarting the kernel does not help; it simply crashes after a few seconds. Below is the log for that (it is written on a red background)\r\n> \r\n> ```\r\n> An error ocurred while starting the kernel\r\n> 2020\udae1\udea8\udae1\udea7 09:10:48.429373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.630422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.654232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\n> coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.655330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.660386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.665212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.667487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.672356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.675255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.685248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.686442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.687128: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.689769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\n> coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.690853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.691406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.691954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.692497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.693046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.693600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.694155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:24.695292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> 2020\udae1\udea8\udae1\udea7 10:25:25.261369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> 2020\udae1\udea8\udae1\udea7 10:25:25.261990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 \r\n> 2020\udae1\udea8\udae1\udea7 10:25:25.262349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N \r\n> 2020\udae1\udea8\udae1\udea7 10:25:25.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2990 MB memory) \u2011> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)\r\n> 2020\udae1\udea8\udae1\udea7 10:25:27.808120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:25:28.068884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> 2020\udae1\udea8\udae1\udea7 10:26:06.218534: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR\r\n> in tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data\u2011>opaque(), input_h_desc.handle(), input_h_backprop_data\u2011>opaque(), input_c_desc.handle(), input_c_backprop_data\u2011>opaque(), workspace.opaque(), workspace.size(), reserve_space_data\u2011>opaque(), reserve_space_data\u2011>size())'\r\n> 2020\udae1\udea8\udae1\udea7 10:26:06.221888: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> 2020\udae1\udea8\udae1\udea7 10:26:06.223371: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> 2020\udae1\udea8\udae1\udea7 10:26:06.225063: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> [[StatefulPartitionedCall_1]]\r\n> [[Reshape_14/_46]]\r\n> 2020\udae1\udea8\udae1\udea7 10:26:06.228126: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> [[StatefulPartitionedCall_1]]\r\n> 2020\udae1\udea8\udae1\udea7 10:35:24.183417: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non\u2011OK\u2011status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: unspecified launch failure\r\n> ```\r\n> \r\n> To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.\r\n\r\nHEY, have you solved this problem with the soluion mentioned above by jlieber?\r\ni have crossed the same problem with you ", "> > Hi, I have this issue since yesterday. Before that, everything was working fine. I have not updated either the tensorflow version or the gpu driver or anything else for that matter in the last couple of days. This issue suddenly appeared just yesterday on its own. Below is my model,\r\n> > ```\r\n> > def neural_network(vocab_size, embedding_dim, max_length, train_padded, train_labels, validation_frac, num_epochs):\r\n> >     model = Sequential()\r\n> >     model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))\r\n> >     model.add(Bidirectional(LSTM(64, return_sequences = True)))\r\n> >     model.add(GlobalAveragePooling1D())\r\n> >     model.add(Dropout(0.2))\r\n> >     model.add(Dense(50, activation = 'relu'))\r\n> >     model.add(Dropout(0.1))\r\n> >     model.add(Dense(1, activation = 'sigmoid'))\r\n> >     model.summary()\r\n> >     model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n> >     history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n> >     return model, history\r\n> > ```\r\n> > \r\n> > \r\n> > I am pretty sure I am not running out of memory as previously I have trained even bigger models (~20M params) but the above model has just 2M elements. Even the GPU usage barely exceeds 5%. I have GTX 1050ti 4GB. Also, I have successfully run the above model plenty of times before but only since yesterday this issue is coming up.\r\n> > ```\r\n> > Model: \"sequential\"\r\n> > _________________________________________________________________\r\n> > Layer (type)                 Output Shape              Param #   \r\n> > =================================================================\r\n> > embedding (Embedding)        (None, 200, 128)          1920000   \r\n> > _________________________________________________________________\r\n> > bidirectional (Bidirectional (None, 200, 128)          98816     \r\n> > _________________________________________________________________\r\n> > global_average_pooling1d (Gl (None, 128)               0         \r\n> > _________________________________________________________________\r\n> > dropout (Dropout)            (None, 128)               0         \r\n> > _________________________________________________________________\r\n> > dense (Dense)                (None, 50)                6450      \r\n> > _________________________________________________________________\r\n> > dropout_1 (Dropout)          (None, 50)                0         \r\n> > _________________________________________________________________\r\n> > dense_1 (Dense)              (None, 1)                 51        \r\n> > =================================================================\r\n> > Total params: 2,025,317\r\n> > Trainable params: 2,025,317\r\n> > Non-trainable params: 0\r\n> > ```\r\n> > \r\n> > \r\n> > Below is the exact error.\r\n> > ```\r\n> > Train on 143613 samples, validate on 15958 samples\r\n> > Epoch 1/5\r\n> > Traceback (most recent call last):\r\n> > \r\n> >   File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 122, in <module>\r\n> >     model, history = neural_network(vocab_size, embedding_dim, max_length, train_padded, toxicity[col], validation_frac, num_epochs)\r\n> > \r\n> >   File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 87, in neural_network\r\n> >     history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n> >     use_multiprocessing=use_multiprocessing)\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n> >     total_epochs=epochs)\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n> >     batch_outs = execution_function(iterator)\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n> >     distributed_function(input_fn))\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n> >     result = self._call(*args, **kwds)\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n> >     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n> >     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n> >     self.captured_inputs)\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n> >     ctx, args, cancellation_manager=cancellation_manager))\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n> >     ctx=ctx)\r\n> > \r\n> >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n> >     six.raise_from(core._status_to_exception(e.code, message), None)\r\n> > \r\n> >   File \"<string>\", line 3, in raise_from\r\n> > \r\n> > InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > \t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > \t [[StatefulPartitionedCall_1]]\r\n> > \t [[Reshape_14/_46]] [Op:__inference_distributed_function_5894]\r\n> > \r\n> > Function call stack:\r\n> > distributed_function -> distributed_function -> distributed_function\r\n> > ```\r\n> > \r\n> > \r\n> > Also, once this issue occurs, the kernel keeps crashing on its own _even if I am not compiling anything_. I am using Spyder IDE and even restarting the kernel does not help; it simply crashes after a few seconds. Below is the log for that (it is written on a red background)\r\n> > ```\r\n> > An error ocurred while starting the kernel\r\n> > 2020\udae1\udea8\udae1\udea7 09:10:48.429373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.630422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.654232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> > pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\n> > coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.655330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.660386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.665212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.667487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.672356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.675255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.685248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.686442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.687128: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.689769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> > pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\n> > coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.690853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.691406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.691954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.692497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.693046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.693600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.694155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:24.695292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:25.261369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:25.261990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 \r\n> > 2020\udae1\udea8\udae1\udea7 10:25:25.262349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N \r\n> > 2020\udae1\udea8\udae1\udea7 10:25:25.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2990 MB memory) \u2011> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:27.808120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:25:28.068884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > 2020\udae1\udea8\udae1\udea7 10:26:06.218534: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR\r\n> > in tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data\u2011>opaque(), input_h_desc.handle(), input_h_backprop_data\u2011>opaque(), input_c_desc.handle(), input_c_backprop_data\u2011>opaque(), workspace.opaque(), workspace.size(), reserve_space_data\u2011>opaque(), reserve_space_data\u2011>size())'\r\n> > 2020\udae1\udea8\udae1\udea7 10:26:06.221888: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > 2020\udae1\udea8\udae1\udea7 10:26:06.223371: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > 2020\udae1\udea8\udae1\udea7 10:26:06.225063: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > [[StatefulPartitionedCall_1]]\r\n> > [[Reshape_14/_46]]\r\n> > 2020\udae1\udea8\udae1\udea7 10:26:06.228126: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > [[StatefulPartitionedCall_1]]\r\n> > 2020\udae1\udea8\udae1\udea7 10:35:24.183417: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non\u2011OK\u2011status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: unspecified launch failure\r\n> > ```\r\n> > \r\n> > \r\n> > To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.\r\n> \r\n> HEY, have you solved this problem with the soluion mentioned above by jlieber?\r\n> i have crossed the same problem with you\r\n\r\nHi, no I did not use the solution mentioned by jlieber as I use windows 10 and do not want to switch my OS. Like I mentioned earlier in this thread, the problem goes away if you restart Spyder. If you are working in a Jupyter notebook, just restart Jupyter itself. It is a bit irritating but this is the shortest workaround. However I still wonder what suddenly caused this issue to come up all of a sudden after using TF for 3+ months", "Yes. this looks like pure Windows related.\r\n\r\nThe issue can be similar to #39958 which I created yesterday. I'll try to assemble the minimum repo on windows. Heh...", "> > > Hi, I have this issue since yesterday. Before that, everything was working fine. I have not updated either the tensorflow version or the gpu driver or anything else for that matter in the last couple of days. This issue suddenly appeared just yesterday on its own. Below is my model,\r\n> > > ```\r\n> > > def neural_network(vocab_size, embedding_dim, max_length, train_padded, train_labels, validation_frac, num_epochs):\r\n> > >     model = Sequential()\r\n> > >     model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))\r\n> > >     model.add(Bidirectional(LSTM(64, return_sequences = True)))\r\n> > >     model.add(GlobalAveragePooling1D())\r\n> > >     model.add(Dropout(0.2))\r\n> > >     model.add(Dense(50, activation = 'relu'))\r\n> > >     model.add(Dropout(0.1))\r\n> > >     model.add(Dense(1, activation = 'sigmoid'))\r\n> > >     model.summary()\r\n> > >     model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n> > >     history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n> > >     return model, history\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > I am pretty sure I am not running out of memory as previously I have trained even bigger models (~20M params) but the above model has just 2M elements. Even the GPU usage barely exceeds 5%. I have GTX 1050ti 4GB. Also, I have successfully run the above model plenty of times before but only since yesterday this issue is coming up.\r\n> > > ```\r\n> > > Model: \"sequential\"\r\n> > > _________________________________________________________________\r\n> > > Layer (type)                 Output Shape              Param #   \r\n> > > =================================================================\r\n> > > embedding (Embedding)        (None, 200, 128)          1920000   \r\n> > > _________________________________________________________________\r\n> > > bidirectional (Bidirectional (None, 200, 128)          98816     \r\n> > > _________________________________________________________________\r\n> > > global_average_pooling1d (Gl (None, 128)               0         \r\n> > > _________________________________________________________________\r\n> > > dropout (Dropout)            (None, 128)               0         \r\n> > > _________________________________________________________________\r\n> > > dense (Dense)                (None, 50)                6450      \r\n> > > _________________________________________________________________\r\n> > > dropout_1 (Dropout)          (None, 50)                0         \r\n> > > _________________________________________________________________\r\n> > > dense_1 (Dense)              (None, 1)                 51        \r\n> > > =================================================================\r\n> > > Total params: 2,025,317\r\n> > > Trainable params: 2,025,317\r\n> > > Non-trainable params: 0\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > Below is the exact error.\r\n> > > ```\r\n> > > Train on 143613 samples, validate on 15958 samples\r\n> > > Epoch 1/5\r\n> > > Traceback (most recent call last):\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 122, in <module>\r\n> > >     model, history = neural_network(vocab_size, embedding_dim, max_length, train_padded, toxicity[col], validation_frac, num_epochs)\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 87, in neural_network\r\n> > >     history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n> > >     use_multiprocessing=use_multiprocessing)\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n> > >     total_epochs=epochs)\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n> > >     batch_outs = execution_function(iterator)\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n> > >     distributed_function(input_fn))\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n> > >     result = self._call(*args, **kwds)\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n> > >     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n> > >     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n> > >     self.captured_inputs)\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n> > >     ctx, args, cancellation_manager=cancellation_manager))\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n> > >     ctx=ctx)\r\n> > > \r\n> > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n> > >     six.raise_from(core._status_to_exception(e.code, message), None)\r\n> > > \r\n> > >   File \"<string>\", line 3, in raise_from\r\n> > > \r\n> > > InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > \t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > > \t [[StatefulPartitionedCall_1]]\r\n> > > \t [[Reshape_14/_46]] [Op:__inference_distributed_function_5894]\r\n> > > \r\n> > > Function call stack:\r\n> > > distributed_function -> distributed_function -> distributed_function\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > Also, once this issue occurs, the kernel keeps crashing on its own _even if I am not compiling anything_. I am using Spyder IDE and even restarting the kernel does not help; it simply crashes after a few seconds. Below is the log for that (it is written on a red background)\r\n> > > ```\r\n> > > An error ocurred while starting the kernel\r\n> > > 2020\udae1\udea8\udae1\udea7 09:10:48.429373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.630422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.654232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> > > pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\n> > > coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.655330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.660386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.665212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.667487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.672356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.675255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.685248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.686442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.687128: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.689769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> > > pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\n> > > coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.690853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.691406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.691954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.692497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.693046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.693600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.694155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:24.695292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:25.261369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:25.261990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 \r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:25.262349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N \r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:25.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2990 MB memory) \u2011> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:27.808120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:25:28.068884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > > 2020\udae1\udea8\udae1\udea7 10:26:06.218534: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR\r\n> > > in tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data\u2011>opaque(), input_h_desc.handle(), input_h_backprop_data\u2011>opaque(), input_c_desc.handle(), input_c_backprop_data\u2011>opaque(), workspace.opaque(), workspace.size(), reserve_space_data\u2011>opaque(), reserve_space_data\u2011>size())'\r\n> > > 2020\udae1\udea8\udae1\udea7 10:26:06.221888: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > 2020\udae1\udea8\udae1\udea7 10:26:06.223371: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > > 2020\udae1\udea8\udae1\udea7 10:26:06.225063: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > > [[StatefulPartitionedCall_1]]\r\n> > > [[Reshape_14/_46]]\r\n> > > 2020\udae1\udea8\udae1\udea7 10:26:06.228126: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > > [[StatefulPartitionedCall_1]]\r\n> > > 2020\udae1\udea8\udae1\udea7 10:35:24.183417: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non\u2011OK\u2011status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: unspecified launch failure\r\n> > > ```\r\n> > > \r\n> > > \r\n> > > To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.\r\n> > \r\n> > \r\n> > HEY, have you solved this problem with the soluion mentioned above by jlieber?\r\n> > i have crossed the same problem with you\r\n> \r\n> Hi, no I did not use the solution mentioned by jlieber as I use windows 10 and do not want to switch my OS. Like I mentioned earlier in this thread, the problem goes away if you restart Spyder. If you are working in a Jupyter notebook, just restart Jupyter itself. It is a bit irritating but this is the shortest workaround. However I still wonder what suddenly caused this issue to come up all of a sudden after using TF for 3+ months\r\n\r\nNoNoNo, i didnt well explain my question, i mean that by changing input_shape with batch_input_size etc. ", "> > > > Hi, I have this issue since yesterday. Before that, everything was working fine. I have not updated either the tensorflow version or the gpu driver or anything else for that matter in the last couple of days. This issue suddenly appeared just yesterday on its own. Below is my model,\r\n> > > > ```\r\n> > > > def neural_network(vocab_size, embedding_dim, max_length, train_padded, train_labels, validation_frac, num_epochs):\r\n> > > >     model = Sequential()\r\n> > > >     model.add(Embedding(vocab_size, embedding_dim, input_length = max_length))\r\n> > > >     model.add(Bidirectional(LSTM(64, return_sequences = True)))\r\n> > > >     model.add(GlobalAveragePooling1D())\r\n> > > >     model.add(Dropout(0.2))\r\n> > > >     model.add(Dense(50, activation = 'relu'))\r\n> > > >     model.add(Dropout(0.1))\r\n> > > >     model.add(Dense(1, activation = 'sigmoid'))\r\n> > > >     model.summary()\r\n> > > >     model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\r\n> > > >     history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n> > > >     return model, history\r\n> > > > ```\r\n> > > > \r\n> > > > \r\n> > > > I am pretty sure I am not running out of memory as previously I have trained even bigger models (~20M params) but the above model has just 2M elements. Even the GPU usage barely exceeds 5%. I have GTX 1050ti 4GB. Also, I have successfully run the above model plenty of times before but only since yesterday this issue is coming up.\r\n> > > > ```\r\n> > > > Model: \"sequential\"\r\n> > > > _________________________________________________________________\r\n> > > > Layer (type)                 Output Shape              Param #   \r\n> > > > =================================================================\r\n> > > > embedding (Embedding)        (None, 200, 128)          1920000   \r\n> > > > _________________________________________________________________\r\n> > > > bidirectional (Bidirectional (None, 200, 128)          98816     \r\n> > > > _________________________________________________________________\r\n> > > > global_average_pooling1d (Gl (None, 128)               0         \r\n> > > > _________________________________________________________________\r\n> > > > dropout (Dropout)            (None, 128)               0         \r\n> > > > _________________________________________________________________\r\n> > > > dense (Dense)                (None, 50)                6450      \r\n> > > > _________________________________________________________________\r\n> > > > dropout_1 (Dropout)          (None, 50)                0         \r\n> > > > _________________________________________________________________\r\n> > > > dense_1 (Dense)              (None, 1)                 51        \r\n> > > > =================================================================\r\n> > > > Total params: 2,025,317\r\n> > > > Trainable params: 2,025,317\r\n> > > > Non-trainable params: 0\r\n> > > > ```\r\n> > > > \r\n> > > > \r\n> > > > Below is the exact error.\r\n> > > > ```\r\n> > > > Train on 143613 samples, validate on 15958 samples\r\n> > > > Epoch 1/5\r\n> > > > Traceback (most recent call last):\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 122, in <module>\r\n> > > >     model, history = neural_network(vocab_size, embedding_dim, max_length, train_padded, toxicity[col], validation_frac, num_epochs)\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Documents\\Machine Learning\\Projects\\Classification\\jigsaw-toxic-comment-classification-challenge\\toxic_classifier.py\", line 87, in neural_network\r\n> > > >     history = model.fit(train_padded, train_labels, epochs = num_epochs, verbose = 2, validation_split = validation_frac)\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 819, in fit\r\n> > > >     use_multiprocessing=use_multiprocessing)\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 342, in fit\r\n> > > >     total_epochs=epochs)\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\", line 128, in run_one_epoch\r\n> > > >     batch_outs = execution_function(iterator)\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 98, in execution_function\r\n> > > >     distributed_function(input_fn))\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 568, in __call__\r\n> > > >     result = self._call(*args, **kwds)\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\", line 599, in _call\r\n> > > >     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 2363, in __call__\r\n> > > >     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1611, in _filtered_call\r\n> > > >     self.captured_inputs)\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 1692, in _call_flat\r\n> > > >     ctx, args, cancellation_manager=cancellation_manager))\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\", line 545, in call\r\n> > > >     ctx=ctx)\r\n> > > > \r\n> > > >   File \"C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\", line 67, in quick_execute\r\n> > > >     six.raise_from(core._status_to_exception(e.code, message), None)\r\n> > > > \r\n> > > >   File \"<string>\", line 3, in raise_from\r\n> > > > \r\n> > > > InternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > > \t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > > > \t [[StatefulPartitionedCall_1]]\r\n> > > > \t [[Reshape_14/_46]] [Op:__inference_distributed_function_5894]\r\n> > > > \r\n> > > > Function call stack:\r\n> > > > distributed_function -> distributed_function -> distributed_function\r\n> > > > ```\r\n> > > > \r\n> > > > \r\n> > > > Also, once this issue occurs, the kernel keeps crashing on its own _even if I am not compiling anything_. I am using Spyder IDE and even restarting the kernel does not help; it simply crashes after a few seconds. Below is the log for that (it is written on a red background)\r\n> > > > ```\r\n> > > > An error ocurred while starting the kernel\r\n> > > > 2020\udae1\udea8\udae1\udea7 09:10:48.429373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.630422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.654232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> > > > pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\n> > > > coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.655330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.660386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.665212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.667487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.672356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.675255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.685248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.686442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.687128: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.689769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: \r\n> > > > pciBusID: 0000:26:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1\r\n> > > > coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.690853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.691406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.691954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.692497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.693046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.693600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.694155: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:24.695292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:25.261369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:25.261990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] 0 \r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:25.262349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0: N \r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:25.263472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2990 MB memory) \u2011> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:27.808120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:25:28.068884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:26:06.218534: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR\r\n> > > > in tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data\u2011>opaque(), input_h_desc.handle(), input_h_backprop_data\u2011>opaque(), input_c_desc.handle(), input_c_backprop_data\u2011>opaque(), workspace.opaque(), workspace.size(), reserve_space_data\u2011>opaque(), reserve_space_data\u2011>size())'\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:26:06.221888: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > > 2020\udae1\udea8\udae1\udea7 10:26:06.223371: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:26:06.225063: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > > > [[StatefulPartitionedCall_1]]\r\n> > > > [[Reshape_14/_46]]\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:26:06.228126: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} {{function_node __inference___backward_cudnn_lstm_with_fallback_4410_4588_specialized_for_StatefulPartitionedCall_1_at___inference_distributed_function_5894}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 128, 64, 1, 200, 32, 64] \r\n> > > > [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n> > > > [[StatefulPartitionedCall_1]]\r\n> > > > 2020\udae1\udea8\udae1\udea7 10:35:24.183417: F .\\tensorflow/core/kernels/random_op_gpu.h:232] Non\u2011OK\u2011status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: unspecified launch failure\r\n> > > > ```\r\n> > > > \r\n> > > > \r\n> > > > To get rid of the recurrent kernel crashes I have to restart Spyder every time. None of this had ever occurred before yesterday and I can say for sure I have not updated anything in the last 1 month at least. my TF version is 2.1 and the GPU driver version is 441.22.\r\n> > > \r\n> > > \r\n> > > HEY, have you solved this problem with the soluion mentioned above by jlieber?\r\n> > > i have crossed the same problem with you\r\n> > \r\n> > \r\n> > Hi, no I did not use the solution mentioned by jlieber as I use windows 10 and do not want to switch my OS. Like I mentioned earlier in this thread, the problem goes away if you restart Spyder. If you are working in a Jupyter notebook, just restart Jupyter itself. It is a bit irritating but this is the shortest workaround. However I still wonder what suddenly caused this issue to come up all of a sudden after using TF for 3+ months\r\n> \r\n> NoNoNo, i didnt well explain my question, i mean that by changing input_shape with batch_input_size etc.\r\n\r\nOh, no I did not try that. but TBH I do not think that is a fix, cos I have been running LSTM cells without the 'batch_input_size' parameter and with the 'input_shape'. This issue just occurs randomly with me, it has got nothing to do with the parameters, for me at least. ", "In case it helps anyone. After doing as CherryCheek said, training did work in my case again.\r\nHowever, when I took a larger part of my dataset, it broke again with the error `'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles()`\r\n.Lowering my batch size from 128 to 64 solved that, now it runs well. However, it uses now only 3GB of 8GB available gpu memory. Still this is a better fix than downgrading the nvidia driver to 431.86 as mentioned here:\r\nhttps://github.com/tensorflow/tensorflow/issues/40403", "Same issue here on Windows 10.\r\nPython: 3.7.6\r\nCUDA: \r\ncudatoolkit                      8.0               4  pkgs/main\r\ncudatoolkit                      9.0               1  pkgs/main\r\ncudatoolkit                 10.0.130               0  pkgs/main\r\ncudatoolkit                 10.1.168               0  pkgs/main\r\ncudatoolkit                 10.1.243      h74a9793_0  pkgs/main\r\ncudatoolkit                  10.2.89      h74a9793_0  pkgs/main\r\ncudatoolkit                  10.2.89      h74a9793_1  pkgs/main\r\nTensorflow: 2.2.0\r\nGPU: RTX 2070 SUPER\r\n\r\nbatch_size = 8\r\nn_timesteps = 400\r\nn_features = 768\r\ninput_x = tf.keras.layers.Input(shape=(n_timesteps,n_features))\r\nbi_rnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, batch_input_shape=(batch_size, n_timesteps, n_features),\r\n                                                            kernel_regularizer=tf.keras.regularizers.l2(0.01), \r\n                                                            recurrent_regularizer=tf.keras.regularizers.l2(0.01), \r\n                                                            bias_regularizer=tf.keras.regularizers.l2(0.01)))(input_x)\r\nx = tf.keras.layers.Dropout(0.3)(bi_rnn)\r\nout = tf.keras.layers.Dense(49, activation=\"softmax\")(x) \r\nmodel = tf.keras.Model(inputs=input_x, outputs=out)\r\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\r\nmodel.summary()\r\n\r\n![image](https://user-images.githubusercontent.com/21985105/85174219-c8c55400-b229-11ea-9d76-651f006a4f5a.png)\r\n\r\nes = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=0.01, patience=5)\r\n\r\nhistory = model.fit(TAPEncoded_train, train_labels, batch_size = batch_size, epochs=10, validation_data=(TAPEncoded_val, validation_labels), verbose=2, callbacks=[es])\r\n\r\nThen runs for a couple epochs (6/10 last time) and either prints the error message or kernel just dies. \r\n", "New setup with half-precision and now batch_size of 16 but with only ~100K parameters:\r\n\r\n![image](https://user-images.githubusercontent.com/21985105/85180283-e5688880-b237-11ea-88b1-d04fa1827766.png)\r\n\r\nNow I get the error message:\r\n\r\n![image](https://user-images.githubusercontent.com/21985105/85180335-0b8e2880-b238-11ea-97e3-1f42da97cd6e.png)\r\n![image](https://user-images.githubusercontent.com/21985105/85180349-1943ae00-b238-11ea-9926-ac83e80a0d15.png)\r\n![image](https://user-images.githubusercontent.com/21985105/85180363-26f93380-b238-11ea-81f1-252941081b24.png)", "I have read that some people believe that this is a memory issue.\r\nI doubt it is. I tried a CNN model with the same data, not even restricting the batch size and with 4 times the parameters and the model ran extremely smoothly. \r\n\r\n![image](https://user-images.githubusercontent.com/21985105/85182321-32029280-b23d-11ea-8eaf-478953c9bd4f.png)\r\n![image](https://user-images.githubusercontent.com/21985105/85182363-49da1680-b23d-11ea-8c0f-9d03512364af.png)\r\n![image](https://user-images.githubusercontent.com/21985105/85182383-59595f80-b23d-11ea-8e47-0fc5c35386ce.png)\r\n\r\n(There are many things I don't know about the tensorflow LSTM implementation, but I don't understand how with quarter of the parameters, smaller batch size and the same data the kernel dies. To me this clearly shows a problem with LSTM under these versions and Windows.)\r\n\r\n\r\n", "> I have read that some people believe that this is a memory issue.\r\n> I doubt it is. I tried a CNN model with the same data, not even restricting the batch size and with 4 times the parameters and the model ran extremely smoothly.\r\n\r\nNo, this is not a memory issue. It is definitely a windows implementation of LSTM issue. Like you say, I have also run CNN models significantly bigger than the model that I was running LSTM for, and there was no issue. This error is also hard to reproduce as it does not always happen. Like I said before, if you restart the kernel (or IDE), it works fine then. You can also try the CUDA implementation of LSTM (CuDNNLSTM), I have found it to be more stable. \r\n\r\nThere are various fixes mentioned in various threads like downgrading gpu driver, using the _batch_input_shape_ parameter instead of the usual _input_shape_, allowing GPU growth etc etc but please know that at least for me none of the suggestions really made a consistent difference. There are times when I can run the LSTM model continuously for hours at a stretch with millions of params and times when the damn thing crashes with just ten thousand params. The only thing that seems to make a consistent difference is switching the OS to Linux but that is not an option for me. \r\n", "Since I switched to Linux (Ubuntu 18.04) I never had a problem with this issue anymore. Therefore, I highly recommend anyone to switch OS if you want GPU-accelerated LSTMs. It is 100% connected to Windows only and I did not find any working solution (workaround or such) for Windows.\r\n\r\nOnly solution I know is to switch to Linux. The problem then disappears.\r\n\r\nI mean this issue was closed, once I stated that I switched my OS but the underlying problem still exists. \ud83e\udd37\u200d\u2642\ufe0f ", "I had the same issue on Windows, in my case the error took place only when running Bidirectional LSTM on GPU with a small batch size. As you've experienced the error doesn't show up running on CPU. I managed to find a temporary solution by running the code on Colab with GPU enabled. On Colab the error doesn't persist so as you said it's an error related to Windows OS.", "I have the same issue with windows. My code in Collab runs fine but when I run Locally I get and error. Now, I am sure that is a problem of tensorflow with windows.\r\n\r\n", "Same issue again in another project. The length of the sequences seems to have an impact on stability.\r\nBtw when trying to restart the training, I get a gpu sync error with this in the console:\r\n```\r\ntensorflow/stream_executor/cuda/cuda_driver.cc:940] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure :: 0x00007FF9F4007E15   tensorflow::CurrentStackTrace\r\n0x00007FF9F3D5D3BE      tensorflow::MetaGraphDef_MetaInfoDef::_Internal::any_info\r\n0x00007FF9F3D63C8E      stream_executor::StreamExecutor::EnablePeerAccessTo\r\n0x00007FF9F21A1A96      tensorflow::StepStats::internal_default_instance\r\n0x00007FF9F21B2EE4      google::protobuf::RepeatedPtrField<tensorflow::InterconnectLink>::Add\r\n0x00007FF9F1DE34C7      std::vector<tensorflow::DtypeAndPartialTensorShape,std::allocator<tensorflow::DtypeAndPartialTensorShape> >::operator=\r\n0x00007FF9F1DC090B      absl::lts_2020_02_25::Span<tensorflow::Tensor const >::end\r\n0x00007FF9EDCEE68F      TFE_TensorHandleResolve\r\n0x00007FF9EDC923C3      TFE_Py_TensorShapeSlice\r\n0x00007FF9EDC8FF7A      std::_Tree<std::_Tmap_traits<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char> >,0>,tensorflow::monitoring::SamplerCell,std::less<std::array<std::basic_string<char,std::char_traits<char>,std::allocator<char>\r\n0x00007FFA5EC15DC7      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC1649C      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16C93      PyEval_EvalFrameDefault\r\n0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16C93      PyEval_EvalFrameDefault\r\n0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16C93      PyEval_EvalFrameDefault\r\n0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16C93      PyEval_EvalFrameDefault\r\n0x00007FFA5EC00192      PyEval_EvalCodeWithName\r\n0x00007FFA5EBFFE1A      PyFunction_FastCallDict\r\n0x00007FFA5EBFDEA3      PyTuple_New\r\n0x00007FFA5EC0E947      PyObject_FastCallKeywords\r\n0x00007FFA5EC0E6DA      PyObject_FastCallKeywords\r\n0x00007FFA5EC16759      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC17889      PyEval_EvalFrameDefault\r\n0x00007FFA5EC00192      PyEval_EvalCodeWithName\r\n0x00007FFA5EBFFE1A      PyFunction_FastCallDict\r\n0x00007FFA5EC1EB6D      PySlice_New\r\n0x00007FFA5EC17A04      PyEval_EvalFrameDefault\r\n0x00007FFA5EC00192      PyEval_EvalCodeWithName\r\n0x00007FFA5EC16727      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC17889      PyEval_EvalFrameDefault\r\n0x00007FFA5EC00192      PyEval_EvalCodeWithName\r\n0x00007FFA5EBDFDDF      PyErr_Clear\r\n0x00007FFA5EBDFCB5      PyErr_Clear\r\n0x00007FFA5EC15AF0      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC1665F      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16D3F      PyEval_EvalFrameDefault\r\n0x00007FFA5EBFC464      PyObject_GetAttrId\r\n0x00007FFA5EC5AB74      PyErr_NoMemory\r\n0x00007FFA5EBFC464      PyObject_GetAttrId\r\n0x00007FFA5EC5AB74      PyErr_NoMemory\r\n0x00007FFA5EBFC464      PyObject_GetAttrId\r\n0x00007FFA5EC15B11      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC1649C      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16C93      PyEval_EvalFrameDefault\r\n0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16D3F      PyEval_EvalFrameDefault\r\n0x00007FFA5EC165FB      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16C93      PyEval_EvalFrameDefault\r\n0x00007FFA5EC00192      PyEval_EvalCodeWithName\r\n0x00007FFA5EBFFE1A      PyFunction_FastCallDict\r\n0x00007FFA5EBFEDDA      PyMethodDef_RawFastCallDict\r\n0x00007FFA5EC1EAE4      PySlice_New\r\n0x00007FFA5EC17A04      PyEval_EvalFrameDefault\r\n0x00007FFA5EC00192      PyEval_EvalCodeWithName\r\n0x00007FFA5EC16727      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC17889      PyEval_EvalFrameDefault\r\n0x00007FFA5EBFC464      PyObject_GetAttrId\r\n0x00007FFA5ED4F296      PyAST_Optimize\r\n0x00007FFA5EC15AF0      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC1665F      PyMethodDef_RawFastCallKeywords\r\n0x00007FFA5EC16D3F      PyEval_EvalFrameDefault\r\n```\r\n\r\nIs there any update regarding this issue?", "Same issue here.", "Since there are many other issues referencing this one, can we open it again until a solution exists where we do not need to change the OS?", "> Since there are many other issues referencing this one, can we open it again until a solution exists where we do not need to change the OS?\r\n\r\n@MichaelJanz,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "@amahendrakar new issue was created, thank you for your help!", "> Since I switched to Linux (Ubuntu 18.04) I never had a problem with this issue anymore. Therefore, I highly recommend anyone to switch OS if you want GPU-accelerated LSTMs. It is 100% connected to Windows only and I did not find any working solution (workaround or such) for Windows.\r\n> \r\n> Only solution I know is to switch to Linux. The problem then disappears.\r\n> \r\n> I mean this issue was closed, once I stated that I switched my OS but the underlying problem still exists. \ud83e\udd37\u200d\u2642\ufe0f\r\n\r\nJust switching to Linux (Ubuntu 18.04) and following TF's [GPU installation guide](https://www.tensorflow.org/install/gpu) is not sufficient in fixing the issue. The [gist](https://gist.github.com/jliebers/995c3c4da4ad2a6f9376d31ee2470ec5) is still crashing with this error and occasionally ThenRnnForward. \r\n\r\nI believe OP is using an older nvidia driver than 440 or 450. I've tried both those versions without success. This issue is not reproducible in colab.", "I was having the same issue and couldn't solve it with any of the proposed fixes (except changing OS, I haven't tried that), but I found that when using recurrent dropout it causes some conflict with the cuDNN kernel when building the model (see warning below)\r\n\r\nWARNING:tensorflow:Layer gru1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\r\n\r\nI haven't run into the original error ever since. It's not a fix, but it might help someone.", "This is not a permanent solution, but I managed to make it work again by downgrading the NVIDIA driver to the last stable studio driver (431.86) as suggested here: [https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800/2](https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800/2)\r\n\r\nYou need to first download the corresponding studio driver from [NVIDIA](https://www.nvidia.com/Download/index.aspx?lang=en), then uninstall the whatever driver version you have now (in my case 442), then install the 431.86 again. This is trickier than it sounds, as NVIDIA utilities only allow you to downgrade to the previous version, and my case I was several versions ahead.\r\n\r\nI ended up using [DDU ](https://www.guru3d.com/files-details/display-driver-uninstaller-download.html)utility as suggested in other forums to wipe the previous driver from my machine, it did the job nicely (no safe mode was necessary). \r\n\r\nAlso, bear in mind that windows will try to automatically update the driver as soon as it gets a chance (creating the problem again). To avoid this you can disable automatic updates for your drivers following [these](https://www.windowscentral.com/how-disable-automatic-driver-updates-windows-10) instructions.\r\n\r\nBy the way, it wasn't necessary to apply the previous fixes suggested in the post (set batch_size or memory growth), just downgrading the driver did the trick.\r\n\r\nI hope this helps, I wasted several hours trying to make it work!", "I followed the downgrade advice and started with the lowest available driver via search which was 441.* . That driver created the same problem, so I tried finding the exact version mentioned here 431.86:\r\nhttps://www.nvidia.co.uk/content/DriverDownload-March2009/confirmation.php?url=/Windows/431.86/431.86-desktop-win10-64bit-international-nsd-whql.exe&lang=uk&type=TITAN", "> This is not a permanent solution, but I managed to make it work again by downgrading the NVIDIA driver to the last stable studio driver (431.86) as suggested here: https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800/2\r\n> \r\n> You need to first download the corresponding studio driver from [NVIDIA](https://www.nvidia.com/Download/index.aspx?lang=en), then uninstall the whatever driver version you have now (in my case 442), then install the 431.86 again. This is trickier than it sounds, as NVIDIA utilities only allow you to downgrade to the previous version, and my case I was several versions ahead.\r\n> \r\n> I ended up using [DDU ](https://www.guru3d.com/files-details/display-driver-uninstaller-download.html)utility as suggested in other forums to wipe the previous driver from my machine, it did the job nicely (no safe mode was necessary).\r\n> \r\n> Also, bear in mind that windows will try to automatically update the driver as soon as it gets a chance (creating the problem again). To avoid this you can disable automatic updates for your drivers following [these](https://www.windowscentral.com/how-disable-automatic-driver-updates-windows-10) instructions.\r\n> \r\n> By the way, it wasn't necessary to apply the previous fixes suggested in the post (set batch_size or memory growth), just downgrading the driver did the trick.\r\n> \r\n> I hope this helps, I wasted several hours trying to make it work!\r\n\r\nI took daviddiazsolis' advice and downgraded the driver to version 431.86. This was a 100% solution for me.\r\n\r\nI have been struggling with this issue for a while and have tried most or all of the other suggestions made in this thread without success. After downgrading the driver there has not been a single \"Failed to call ThenRnnBackward with model config\"-error.", "I have \"Windows 10 (build 2004)\" and the same problem with LSTM layers. And NVidia driver version 431.86 (Studio) or 431.36 (Game Ready) is not compatible with my version of Windows. So I'll have to wait for NVidia to fix and release the new driver.", "> Yes, allowing GPU memory growth is also necessary, i.e.\r\n> \r\n> ```\r\n> gpu_devices = tf.config.experimental.list_physical_devices('GPU')\r\n> for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\r\n> ```\r\n> \r\n> And I agree with the bullet points you posted. They are required to make it work on my setup.\r\n\r\nthis doesn't solve it for me", "I share my experience with the same problem:\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise, Build 2004\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\nTensorFlow installed from (source or binary): Pypi\r\nTensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\nPython version: 3.7.6\r\nBazel version (if compiling from source): -\r\nGCC/Compiler version (if compiling from source): -\r\nCUDA/cuDNN version: CUDA 10.1 (10.1.243), cudnn-10.1-windows10-x64-v7.6.5.32\r\nGPU model and memory: Quadro RTX 4000, 8 GB (Laptop version)\r\n\r\nI got similar error while testing different NVIDIA driver versions (ranging from 441.66 to the 456.38) that were available on NVIDIA's site. None of these driver versions could fix the problem where the training crashes after the first epoch in the middle of the second one, or somewhere in-between.\r\n\r\n**1st workaround**\r\n\r\nOne workaround that seems to work (I could get the training to finish) was following tips from https://github.com/tensorflow/tensorflow/issues/37942\r\nwhere I had to specify a fixed batch_size on the first layer of the model:\r\n\r\n```\r\nx = Input(shape=(timesteps,input_dim), batch_size=64) # need to have fixed batch_size for cudnn Rnns to work on Windows\r\n...\r\n```\r\nThis alone was **not enough** (the training still crashed randomly at some point, however, it got sometimes a bit further in the training epochs).\r\nI had to also specify\r\n\r\n```\r\nimport tensorflow as tf\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n        tf.config.experimental.set_memory_growth(gpu, True)\r\n```\r\nand **ensure** that that the input x given to the model in the model.fit() method must be divisible by the `batch_size` (there must be no incomplete batch with less than `batch_size` samples at the end), again following https://github.com/tensorflow/tensorflow/issues/37942. After that, the training did not crash on a couple of run attempts.\r\n\r\nHowever, this is only a workaround, which is quite annoying to do since it requires to add code that is needed only because of the Windows-related cuDNN bug.\r\n\r\n**2nd workaround - downgrading driver to 431.86**\r\n\r\nMultiple issues here (https://github.com/tensorflow/tensorflow/issues/41863, https://github.com/tensorflow/tensorflow/issues/41444) and on internet (https://forums.developer.nvidia.com/t/cudnn-lstm-is-broken-above-driver-431-60-unexpected-event-status-1-cuda/108800) related to this problem mention that the problem disappears if one fallbacks the NVIDIA driver version to 431.86. This version is not officially supported on my gpu (Quadro RTX 4000 notebook version), and NVIDIA does not directly even offer this specific version for this gpu (earliest available is 441.66). However, I still managed to install the unsupported version (found using some internet searches directly from NVIDIA's download repository), and the model training seems to work with this old, unsupported 431.86 driver version.\r\n\r\n**My other failed attempt - Tensorflow 2.3 compiled against CUDA 11 / cuDNN 8**\r\n\r\nI also tested to install Tensorflow 2.3 compiled for CUDA11.0 / cuDNN 8.0.2 from an unofficial wheel from here https://github.com/fo40225/tensorflow-windows-wheel. I had the specific CUDA 11 and cuDNN version installed and this Tensorflow 2.3 version compiled against these. In addition, I again tried all available NVIDIA driver versions versions (ranging from 441.66 to the 456.38) but I got the same error, so it seems that the problem cannot be solved by moving to a newer CUDA / cuDNN version.", "> Since I switched to Linux (Ubuntu 18.04) I never had a problem with this issue anymore. Therefore, I highly recommend anyone to switch OS if you want GPU-accelerated LSTMs. It is 100% connected to Windows only and I did not find any working solution (workaround or such) for Windows.\r\n> \r\n> Only solution I know is to switch to Linux. The problem then disappears.\r\n> \r\n> I mean this issue was closed, once I stated that I switched my OS but the underlying problem still exists. \ud83e\udd37\u200d\u2642\ufe0f\r\n\r\nI am having still trouble on with same error on Ubuntu 18.04 actually.", "same here: \"InternalError\" on Windows and Ubuntu 18.04 on LSTM layers", "@AinTziLLo,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n", "I am not sure if this is actually the cause. I use window 10, TF 2.2. I had this problem as well. I did the recommended fix like setting batch input shape and letting GPU grow and so on, but it didn't work for me.\r\n\r\nBut when I turned off antivirus(McAfee_real-time scan), it's been working really good. It had been 3 days since I turned it off and I have not encountered this error again since. It sounds weird and stupid but I think it's worth the shot.", "We had the same issue; we reduce the embedding size (of the embedding layer) and we dont have any issue now.. We'll try to downgrade to the  431.86 driver version to see if we can keep the same embedding size. ", "had same issue with bi-lstm, on windows 10, tensorflow 2.0.0, cuda 10.1 and cudnn 7.6.5.\r\nit worked but got stuck at end of 1st epoch.\r\n\r\ni deleted bi-lstm  and now its on the 3rd epoch.", "same issue with lstm on windows 10, sloved after updating gpu driver to Latest Version", "> same issue with lstm on windows 10, sloved after updating gpu driver to Latest Version\r\n\r\nThanks for the info. Please answer the questions:\r\n- What build \"Windows 10\" do you have?\r\n- What is the latest version of the GPU driver that you got working?\r\n- Which version CUDA, CUDNN, Tensorflow are you using?", "- Windows  Version 2004 OS Build 19041.746\r\n- GPU GTX 1060 Max-Q with driver Game Ready Version 461.40\r\n- CUDA 10.1 CUDNN 7.6.5 with Tensorlfow-gpu 2.3.1", "I confirm that after updating the NVidia driver to version 461.40, the problem with LSTM layers on Windows 10 disappeared.\r\nMy config is:\r\n- Windows 10 Pro build 2004\r\n- GTX970 + NVidia Game Ready Driver 461.40\r\n- CUDA 10.1\r\n- CUDNN 7.6.5\r\n- Python 3.7.7\r\n- tensorlfow-gpu 2.3.0", "Also in April 2021, I too confirm that after updating **NVidia driver to version 462.31**, the above described problem on Windows 10 disappeared. The problem arised when I was using an NVidia driver somewhere around version 452._._.\r\n\r\nMy system is:\r\n- Windows 10 Pro Version 1903\r\n- Quadro T1000\r\n- CUDA 11.2.162\r\n- cuDNN 8.2.0\r\n- Python 3.7.8\r\n- tensorflow 2.4.1 (always including full GPU support since several versions)", "I have confirmed just now, **updating the driver solve the problems** on Windows 10. Before my NVidia driver was 460.89 and after I upgraded to 470.41 it works flawlessly. "]}, {"number": 37940, "title": "Adapting hlo_legalize_to_lhlo for using buffer assignment", "body": "In this PR, we implemented our proposed [BufferAssignment](https://github.com/tensorflow/tensorflow/pull/37212) in HLO-to-LHLO legalization pipeline. Please note that this PR must be merged after BufferAssignment PR since it cannot be built.", "comments": ["@dfki-ehna Can you please resolve conflicts? Thanks!", "@dfki-ehna Can you please check build failures. Thanks!", "@dfki-ehna Can you please resolve conflicts? Thanks!", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 37939, "title": "Adapting xla_legalize_to_linalg for using buffer assignment", "body": "In this PR, we implemented our proposed [BufferAssignment](https://github.com/tensorflow/tensorflow/pull/37212) in HLO-to-Linalg legalization pipeline. Please note that this PR must be merged after BufferAssignment PR since it cannot be built.", "comments": ["@dfki-ehna Can you please resolve conflicts? Thanks!"]}, {"number": 37938, "title": "Anconda Tensorflow-GPU 2.1.0 cudart64_101.dll not found", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Conda\r\n- TensorFlow version: 2.1.0\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: Conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA GTX 1060 6GB\r\n\r\n**Describe the problem**\r\nCan't use tensorflow with GPU.\r\nThis warning always show up : \r\n2020-03-26 14:16:44.969758: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found\r\n2020-03-26 14:16:44.975297: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Installed CUDA on Windows\r\n2. Setting the Path as the tutorial said\r\n3. Installed tensorflow with \"conda install -c anaconda tensorflow-gpu\"\r\n\r\n**Any other info / logs**\r\nCupy works (need cuda also) on my current environment. So I wonder what's wrong.\r\nI found multiple cudart64_101.dll in these directories : \r\n1. On Cuda installation : C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin\r\n2. On Anaconda environment directory : C:\\Users\\user\\Anaconda3\\Library\\bin\r\n", "comments": ["@leocd91 \r\n\r\nplease refer to these issues with similar error:#36111 #33319 #31018 \r\nand these [comments](https://github.com/tensorflow/tensorflow/issues/35872#issuecomment-581681377) [for reference](https://github.com/tensorflow/tensorflow/issues/29831#issuecomment-504496546)", "Hey @leocd91 \r\nI believe you have installed cuDNN after CUDA? \r\nThe typical flow for installing tf-gpu would be something like this-\r\n- Download and install CUDA toolkit.\r\n- Download cuDNN, extract contents to CUDA toolkit path(replace if asked).\r\n- Check if CUDA_HOME is present. \r\n- Create an environment in python and do \"pip install tensorflow-gpu\" \r\nIf you've followed the above steps and are still getting error then it might be due to different versioning(refer [this link](https://www.tensorflow.org/install/source_windows#gpu)) \r\nWhat I would recommend you is to uninstall CUDA and then simply run \r\n```\r\nconda create --name tf_gpu\r\nconda activate tf_gpu\r\nconda install tensorflow-gpu\r\n```\r\nConda will take care of all CUDA dependencies.", "Hi, I already all those steps too, and any other suggestion #36111 #33319 #31018 with no results.\r\nokay, thanks @theadityasam for the suggestion, I'll try to uninstall the cuda then just use the one from anaconda.\r\nI'll inform you guys quickly.", "all suggestions done, .dll still not found.\r\nnow trying to remove all type of 10.1 cuda in windows and conda, and reinstall anaconda and then the tensorflow-gpu.", "still not found.\r\n\r\nI tried to downgrade to 1.15, still not found.", "wrestled the same problem for the whole evening, here's what fixed it for me (TF 2.1):\r\n* install the \"Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019\" \r\n* enable long paths: https://superuser.com/questions/1119883/windows-10-enable-ntfs-long-paths-policy-option-missing\r\n* install CUDA 10.2 (instead of 10.1) and the corresponding cudnn package\r\n* define CUDAPATH and PATH to point to the CUDA 10.2 installation directory\r\n* copy the cudart64_101.dll from the 10.1 directory into the 10.2 installation\r\n* install a vanilla python3.7.7 (https://www.python.org/downloads/) , *not* from the windows store\r\n* run a plain `pip3 install tensorflow`\r\n* test with a plain `python3 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"`\r\n\r\nAnd everything works. I'm not sure where that disparity between CUDA 10.2 and 10.1 comes from, especially since the documentation ultimately requires CUDA 10.1, but it is fine to load all the other dlls. \r\n\r\nIt definitely did not work for me to just use CUDA 10.1 and use it as part of the PATH.\r\n", "I have a similar setup as leocd91 (with a Titan Xp) and having the same Problem. Regardless of what I try, cudart64_101.dll isn't found. Tried with anaconda/miniconda and installing with conda or pip (and manual installation of CUDA/CUDNN). \r\nTwo more things that might have to do with this problem:\r\n1. I had to set the CONDA_DLL_SEARCH_MODIFICATION_ENABLE to be able to import other packages (such as numpy)\r\n2. conda install tensorflow=1.14 works perfectly fine (1.15 doesn't work though - dll not found)", "Conda packages are built and maintained by the community, and anaconda. I will redirect the issue to @jjhelmus", "leocd91 If you are using the Tensorflow package from Anaconda you can open an issue in https://github.com/ContinuumIO/anaconda-issues/issues. I can try to help with the additional information request in that template.", "The following command really helped. My conda version is 4.8.3 and my TF version is 2.1.0, installed prior to the below command (probably via pip). Thanks @theadityasam!\r\n\r\n`conda install tensorflow-gpu`", "Had exactly the same issue and followed the instructions on the [Anaconda Website](https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/) ... works perfectly fine now", "@leocd91 \r\nCan you please refer to above comment and let us know.", "ok, will try it again, but that tutorial is one of the tutorial that I have tried. ", "> lIf you are using the Tensorflow package from Anaconda you can open an issue in https://github.com/ContinuumIO/anaconda-issues/issues.\r\n\r\nClosing this issue since TF Anaconda build issues are tracked separately in the above tracker. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37938\">No</a>\n"]}, {"number": 37937, "title": "[ROCm] Gelu op", "body": "This PR creates a kernel-based Gelu activation op (which offers a substantial speedup over a basic Python implementation).", "comments": ["And can we replace this kernel implementation with a tf.function(experimental_compile=True) so we use XLA to generate the kernel implementation?", "There are already numerous other activations in TF core, what's one more? \r\nAnd Gelu is arguably more important than some of the existing activations (like e.g. Selu) because it's used by BERT.\r\n\r\nIt is part of the standard API in PyTorch: https://pytorch.org/docs/stable/nn.html#torch.nn.GELU\r\n\r\nI am not familiar with experimental_compile, it's worth testing, but we can merge this now and replace the kernel with Python+XLA later when it is confirmed to work correctly and match performance.\r\n\r\nAthough this is partly a question of ideology. I think that writing an explicit kernel once is better than relying on XLA to compile it every time it is loaded - TF takes way too long to load as-is (unless there are tangible benefits to XLA over handcoded kernel).", "See also https://github.com/tensorflow/tensorflow/pull/33945 /cc @WindQAQ\r\n@alextp Why with `tf.function` the backward seems so slow? See https://github.com/tensorflow/addons/issues/1722#issuecomment-619447106?", "To merge this we need the kernels for higher order derivatives. Without running a profiler though I can't tell why the backward step is slow. Can you tun the TPU profiler and post screenshots of a trace here?", "> There are already numerous other activations in TF core, what's one more?\r\n\r\n@ekuznetsov139  we had already this activation available in Tensorflow Addons and we was trying to upstream to core at https://github.com/tensorflow/tensorflow/pull/33945. We have a specific process for this [upstreaming activity documented](https://github.com/tensorflow/addons/blob/master/MIGRATION_TO_CORE.md).\r\nIn the meantime we are discussing about GELU (and other activations) custom kernel implementation removal for a python only composite ops at https://github.com/tensorflow/addons/issues/1752\r\n\r\n", "Closing this as there is already PR opened [here](https://github.com/tensorflow/tensorflow/pull/33945) , thank you for your contribution."]}, {"number": 37936, "title": "TFLu: remove -fno-builtin compiler flag", "body": "", "comments": ["What is the motivation for this patch?  Would be good if there was a description in the commit log!", "Good point, I will add a description. The motivation is that it disables optimization of standard C library functions, which may cause problems.", "Thanks for looking into this Mans! The background to this flag being present is that it helps us avoid code that introduces function calls like exit() that are not present on bare metal platforms without an OS. We normally link with --no-stdlib on those platforms, but because builtins are inserted inline during compilation, it's easy to accidentally use them and either end up with invisible code bloat or obscure linker errors caused by libc functions the builtins themselves call.\r\n\r\nCould you add a bit more about the binary size or other optimization problems you see with this flag enabled? Maybe we can turn it on in particular circumstances.", "I didn't know that was the background of the flag.\r\n\r\nFor many binaries, the file size is about 7% (~6% with cmsis-nn tag) percent bigger with the flag enabled.\r\n\r\nThere is also this issue reported with some significant performance improvement for cmsis-nn without the flag:\r\nhttps://github.com/tensorflow/tensorflow/issues/37361\r\nCMSIS-NN use optimized builtins for memset and memcpy frequently.", "Gentle ping for review"]}, {"number": 37935, "title": "Compile error on Windows", "body": "**System information** \r\n- OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from source\r\n- TensorFlow version 2.1\r\n- Python version: 3.7.1\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source): Visual Studio 2019\r\n- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5\r\n- GPU model and memory: NVidia GForce GTX 1060 3GB\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to build a Windows DLL exporting custom code dependent on Tensorflow:\r\n\r\nload(\"//tensorflow:tensorflow.bzl\", \"tf_cc_binary\")\r\n\r\ntf_cc_binary(\r\n    name = \"Inference_GPU.dll\",\r\n    linkshared = True,\r\n    srcs = glob([\"*.cpp\", \"*.h\"]),\r\n    defines\t= [\"_WINDOWS\", \"INFERENCE_NET_EXPORTS\", \"TF_GPU\"],\r\n    deps = [\r\n\t\"//tensorflow/cc:cc_ops\",\r\n        \"//tensorflow/cc:client_session\",\r\n        \"//tensorflow/core:tensorflow\",\t\r\n    ],\r\n\tcopts = [\"/O2\", \"/Gm-\", \"/Zi\", \"/FS\"],\r\n    linkopts = [\"/DEBUG\"]\r\n)\r\n\r\nRun the following bazel command:\r\nbazel --output_base=F:\\Bazel_cache build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/cc/Inference:Inference_GPU.dll --jobs 2\r\n\r\nBazel stopped with the following error:\r\n\r\nF:\\bazel_cache\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorExecutor.h(381): error: identifier \"std::conj<double> \" is undefined in device code\r\n\r\n4 errors detected in the compilation of \"F:/bazel_temp/nvcc_inter_files_tmp_dir/tmpc6wr1kw5/cwise_op_gpu_conj.cu.compute_75.cpp1.ii\".\r\n\r\nThank you,\r\nGeorge Scortaru\r\n", "comments": ["We never test with `/DEBUG` option, because binaries become too big for bazel to handle. That may be an issue in your case, too.\r\n\r\nTo me, this looks like some code meant for host ended up on a compilation unit for the device. @chsigg @sanjoy may have a better idea why would this happen.", "Thanks for looking into this.\r\n\r\nI tried without /DEBUG option. It stopped with the same error. I will try to downgrade Tensorflow to version to 2.0.\r\n\r\n", "Could you try qualifying the `conj` in cwise_op_gpu_conj.cu.cc with `::tensorflow::functor::`?", "I modified the cwise_op_gpu_conj.cu.cc:\r\n\r\n#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM\r\n\r\n#include \"tensorflow/core/kernels/cwise_ops_gpu_common.cu.h\"\r\n\r\nnamespace tensorflow {\r\nnamespace functor {\r\nDEFINE_UNARY1(::tensorflow::functor::conj, complex64);\r\nDEFINE_UNARY1(::tensorflow::functor::conj, complex128);\r\n}  // namespace functor\r\n}  // namespace tensorflow\r\n\r\n#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM\r\n\r\nNow I have the following error:\r\n\r\ntensorflow/core/kernels/scatter_nd_op_gpu.cu.cc(102): error: identifier \"std::operator -&lt;double&gt; \" is undefined in device code", "[Latest reasons](https://github.com/tensorflow/tensorflow/issues/39905#issuecomment-641588284) why nothing happens for last 6 months for Windows DLL for TF2.0, TF2.1 and TF2.2", "We need to invite more discussion on the future of WINDOW support for tensorflow.  6 months with no progress but challenges. **Google needs to hire those WHO CLIAM HERE are able to address these challenges IMMEDIATELY.** ", "The latest commit to address this problem is to get more Windows developers to spend MORE TIME trying [new Bazel  (from 2.0 to 3.0)](https://github.com/tensorflow/tensorflow/commit/b4b83222d470afbf0b83d12b0824c0f056235655)\r\n", "@georgescortaru \r\nCould you pease try on the lastest stable version of tf 2.4.1 and let us know if this is still an issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37935\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37935\">No</a>\n"]}, {"number": 37934, "title": "roadmap link broken", "body": "in [tensorflow.org](https://www.tensorflow.org/community) website the below roadmap link is  not working\r\n", "comments": ["https://www.tensorflow.org/community/roadmap\r\n", "Some information will be needed to solve this issue:\r\n\r\n1): Where should it even point to, the `Product Roadmap` button?\r\n2): Where is the code for the website where we can edit the href tag to a correct location?\r\n3): Is this even the right place to make the issue?\r\n\r\nHelp from moderators I think will be more accurate.\r\nWill it be a good idea to tag them here?", "https://www.tensorflow.org/community , this page has the roadmap button", "No, I mean, should the `Roadmap` button at https://www.tensorflow.org/community even go to https://www.tensorflow.org/community/roadmap ? \r\nIf yes, then the contents of https://www.tensorflow.org/community/roadmap should be updated.\r\nIf no, where else should it go? Is the 404 currently on purpose, a mistake, or a bug, or just a broken link?\r\nI don't have any experience with how the website is built or its routes, but I would love to help given the right directions.", "b/152751643", "Thanks. We'll fix the button since that page was out of date.\r\n", "closing as the roadmap link is not present in the community [webpage](https://www.tensorflow.org/community).\r\n\r\nre-open if required. ", "Can we remove it also from https://github.com/tensorflow/tensorflow/blob/master/README.md#resources? \r\nSee https://github.com/tensorflow/community/issues/206#issuecomment-593126257"]}, {"number": 37933, "title": "TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04):  Windows 8.1\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below):  Tensorflow 2.0\r\n- Python version: - Bazel\r\nversion (if compiling from source): Python 3.5\r\n\r\n**Describe the current behavior**\r\nI am trying to migrate TF1.14 code to TF 2.0. However, some functions are disappeared in TF2.0. \r\nI changed the codes like this:\r\nloss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[\r\n                self.winner_loc[0] + self.winner_loc[1]])\r\n            optimizer = tf.keras.optimizers.SGD(learning_rate=alpha_op)\r\n            optimizer.minimize(loss, self.weightage_vects)\r\nbut when I run the code, the error happen:\r\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable in the line:\r\noptimizer.minimize(loss, self.weightage_vects)\r\n\r\nI tried to change my code like:\r\nwith tf.GradientTape() as tape:\r\n             loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[\r\n                   self.winner_loc[0] + self.winner_loc[1]])\r\n            gradients = tape.gradient(target=loss, sources=self.weightage_vects)\r\n            optimizer.apply_gradients(zip(gradients, self.weightage_vects))\r\nbut other errors happened: TypeError: zip argument #1 must support iteration\r\n\r\nI don't know how to correct these codes. T.T Please Help!\r\n", "comments": ["@ellaJin, Did you follow the instructions mentioned in the [Tensorflow doc](https://www.tensorflow.org/guide/migrate). Provide the standalone code to reproduce the issue. Thanks! ", "> \r\n> \r\n> @ellaJin, Did you follow the instructions mentioned in the [Tensorflow doc](https://www.tensorflow.org/guide/migrate). Provide the standalone code to reproduce the issue. Thanks!\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport re\r\n\r\ninputshape = 512\r\n\r\n\r\nclass SOM2(object):\r\n    trianed = False\r\n\r\n    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None, features=None):\r\n\r\n        self.m = m\r\n        self.n = n\r\n        self.dim = dim\r\n\r\n        if alpha is None:\r\n            self.alpha = 0.3\r\n        else:\r\n            self.alpha = float(alpha)\r\n        if sigma is None:\r\n            self.sigma = max(m, n) / 2.0\r\n        else:\r\n            self.sigma = float(sigma)\r\n        self.vect_input = features\r\n        print(features.shape)\r\n        self.n_iterations = abs(int(n_iterations))\r\n        self.centroid_grid = np.zeros(shape=(m, n))\r\n        self.loaded_weights = None\r\n  \r\n        self.weightage_vects = tf.Variable(tf.random.normal(\r\n            [m * n, dim], mean=0.5))\r\n\r\n        self.location_vects = tf.constant(np.array(\r\n            list(self._neuron_locations(m, n))))\r\n\r\n        self.dense_input, self.output, self.dense_weights = self.dense(self.vect_input)\r\n\r\n    def _neuron_locations(self, m, n):\r\n\r\n        for i in range(m):\r\n            for j in range(n):\r\n                yield np.array([i, j])\r\n\r\n    def dense(self, inputs_vect):\r\n      \r\n        w1 = tf.Variable(tf.random.normal([tf.shape(inputs_vect)[1], self.dim]))    \r\n        b1 = tf.constant(0.1, shape=[self.dim])\r\n        mid1 = tf.constant(1.0, shape=[1, 175104])\r\n        z1 = tf.matmul(tf.matmul(mid1, inputs_vect), w1) + b1\r\n        dense_input = tf.nn.relu(z1)\r\n\r\n        w2 = tf.Variable(tf.random.normal([self.dim, self.m * self.n]))\r\n        b2 = tf.constant(0.1, shape=[self.m * self.n])\r\n        output = tf.matmul(dense_input, w2) + b2\r\n\r\n        return dense_input, output, w1\r\n\r\n    def train_operation(self):\r\n        for iter in range(self.n_iterations):\r\n            winner_index = tf.argmin(tf.sqrt(tf.reduce_sum(\r\n                tf.square(tf.subtract(self.weightage_vects, tf.stack(\r\n                    [self.dense_input[0] for i in range(self.m * self.n)]))), 1)), 0)\r\n            print('winner_index', winner_index)\r\n\r\n            slice_input = tf.pad(tf.reshape(winner_index, [1]),\r\n                                 np.array([[0, 1]]))\r\n            print('slice_Input:', slice_input)\r\n            self.winner_loc = tf.reshape(tf.slice(self.location_vects, slice_input,\r\n                                                  tf.constant(np.array([1, 2]), dtype=tf.int64)),\r\n                                         [2])  \r\n            print('winner_loc', self.winner_loc)\r\n\r\n            learning_rate_op = tf.subtract(1.0, tf.divide(iter,\r\n                                                          self.n_iterations))\r\n            alpha_op = learning_rate_op * self.alpha\r\n            sigma_op = self.sigma * learning_rate_op\r\n\r\n            bmu_distance_squares = tf.reduce_sum(tf.square(tf.subtract(\r\n                self.location_vects, tf.stack(\r\n                    [self.winner_loc for i in range(self.m * self.n)]))), 1) \r\n\r\n            neighbourhood_func = tf.exp(tf.negative(tf.divide(tf.cast(\r\n                bmu_distance_squares, \"float32\"), tf.square(sigma_op))))\r\n\r\n            learning_rate_op = tf.multiply(alpha_op, neighbourhood_func)  \r\n\r\n            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\r\n                learning_rate_op, np.array([i]), np.array([1])), [self.dim])\r\n                for i in range(self.m * self.n)])\r\n\r\n            weightage_delta = tf.multiply(\r\n                learning_rate_multiplier,\r\n                tf.subtract(tf.stack([self.dense_input[0] for i in range(self.m * self.n)]),\r\n                            self.weightage_vects)) \r\n\r\n            new_weightages_op = tf.add(self.weightage_vects,\r\n                                       weightage_delta)\r\n\r\n \r\n            self.weightage_vects.assign(new_weightages_op)\r\n   \r\n\r\n           tf.train.ProximalGradientDescentOptimizer(learning_rate=alpha_op).minimize(cross_entropy)\r\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[\r\n                self.winner_loc[0] + self.winner_loc[1]])\r\n            optimizer = tf.keras.optimizers.SGD(learning_rate=alpha_op)\r\n\r\n            optimizer.minimize(loss, self.weightage_vects)\r\n            # with tf.GradientTape() as tape:\r\n            #     loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.output, labels=[\r\n            #     self.winner_loc[0] + self.winner_loc[1]])\r\n            # gradients = tape.gradient(target=loss, sources=self.weightage_vects)\r\n            # optimizer.apply_gradients(zip(gradients, self.weightage_vects))\r\n\r\n        self.trianed = True\r\n\r\nHere is the code. Thanks!", "@ellaJin, Please follow the instructions mentioned [here](https://www.tensorflow.org/guide/migrate)  and use upgrade script to convert from Tf1 to Tf2. Find more details [here](https://www.tensorflow.org/guide/upgrade). Thanks ", "@ellaJin, Please update for the above comment. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37933\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37933\">No</a>\n", "Hello, \r\n\r\nI am having the same issue but I do not understand clearly the solutions proposed in the previous posts...\r\nI am new to TF.\r\n\r\nI get this error:\r\n\r\n`TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable`\r\n\r\nWhen I run this model in Tensorflow V2. The error comes from the last line.\r\n\r\n```python\r\nlatent_features = 8\r\nlearning_rate = 0.001\r\nnum_users = len( X )\r\nnum_items = len( X[0] )\r\n \r\nW = tf.Variable(tf.random.normal([num_users, latent_features], stddev=0.05, mean=0) )\r\nH = tf.Variable(tf.random.normal([latent_features, num_items], stddev=0.05, mean=0) )\r\n\r\nresult = tf.matmul(W, H)\r\ndiff_op = tf.math.subtract(result, X)\r\ncost = tf.reduce_sum( tf.square(diff_op) )\r\nopt = tf.optimizers.Adam(learning_rate = learning_rate)\r\nstep = opt.minimize(cost, var_list=[W,H] )\r\n```\r\n\r\nCan you help ? Thank you", "> Hello,\r\n> \r\n> I am having the same issue but I do not understand clearly the solutions proposed in the previous posts...\r\n> I am new to TF.\r\n> \r\n> I get this error:\r\n> \r\n> `TypeError: 'tensorflow.python.framework.ops.EagerTensor' object is not callable`\r\n> \r\n> When I run this model in Tensorflow V2. The error comes from the last line.\r\n> \r\n> ```python\r\n> latent_features = 8\r\n> learning_rate = 0.001\r\n> num_users = len( X )\r\n> num_items = len( X[0] )\r\n>  \r\n> W = tf.Variable(tf.random.normal([num_users, latent_features], stddev=0.05, mean=0) )\r\n> H = tf.Variable(tf.random.normal([latent_features, num_items], stddev=0.05, mean=0) )\r\n> \r\n> result = tf.matmul(W, H)\r\n> diff_op = tf.math.subtract(result, X)\r\n> cost = tf.reduce_sum( tf.square(diff_op) )\r\n> opt = tf.optimizers.Adam(learning_rate = learning_rate)\r\n> step = opt.minimize(cost, var_list=[W,H] )\r\n> ```\r\n> \r\n> Can you help ? Thank you\r\n\r\nSorry, I had changed my code in TF2.0, because I couldn't find a solution with the minimize function in TF2.0."]}, {"number": 37932, "title": "CUDNN_STATUS_INTERNAL_ERROR after hours of HyperParameter optimization.", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): **I'm using completely standard Tensorflow / Keras implementations, see code below.**\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): **Windows 10**\r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): **tensorflow==2.1.0**\r\n- Python version: - Bazel\r\nversion (if compiling from source): **Python 3.7**\r\n- CUDA/cuDNN version: - GPU model and memory: **Nvidia GTX 1070 8GB, Cuda compilation tools release 10.1, V10.1.243**\r\n\r\n**Describe the current behavior**\r\n\r\nI was training and tuning HyperParameters using \"Keras Tuner\" for about ~2 hours until I noticed that no new trials where created in the \"Tuner project\" folder, this is my training setup:\r\n![image](https://user-images.githubusercontent.com/17565925/77623713-e17ee000-6f40-11ea-9d7b-bc9e2fad9b5f.png)\r\n\r\nAnd this is my dynamic model:\r\n![image](https://user-images.githubusercontent.com/17565925/77623844-24d94e80-6f41-11ea-831c-55d1eb62ee31.png)\r\n\r\nAfter I noticed the stand-still I checked my terminal and got the following error:\r\n![Fejlmeddelse](https://user-images.githubusercontent.com/17565925/77624487-3111db80-6f42-11ea-84e4-70a111ff2478.png)\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect the training to keep running. It completed 48 permutations training each one 3 times, i.e. 144 models without error, so the sudden interruption and kernel restart is weird to me!\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nIn addition to the code referenced above, I've pretty much followed the setup in this Tensorflow tutorial: [Time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series#single_step_model)\r\n", "comments": ["@nistrup \r\nplease refer to these issues as per the error faced  ##29632 #33848 #28254 and [comments](https://github.com/tensorflow/tensorflow/issues/24496)\r\nlet us know if it [helps](https://github.com/tensorflow/tensorflow/issues/34695)", "@Saduf2019 Thank you!\r\n\r\nI seem to get a different error now after training several models successfully I get this in JupyterLab:\r\n\r\n```\r\nInternalError:  [_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 8, 128, 1, 90, 64, 128] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[StatefulPartitionedCall_2]] [Op:__inference_distributed_function_353504]\r\n\r\nFunction call stack:\r\ndistributed_function -> distributed_function -> distributed_function\r\n```\r\n\r\nAnd this in my terminal window:\r\n\r\n```\r\n2020-03-26 15:14:33.877175: E tensorflow/stream_executor/dnn.cc:596] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1921): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.params_handle(), params.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), input_desc.handles(), input_backprop_data->opaque(), input_h_desc.handle(), input_h_backprop_data->opaque(), input_c_desc.handle(), input_c_backprop_data->opaque(), workspace.opaque(), workspace.size(), reserve_space_data->opaque(), reserve_space_data->size())'\r\n2020-03-26 15:14:33.911329: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at cudnn_rnn_ops.cc:1922 : Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 8, 128, 1, 90, 64, 128]\r\n2020-03-26 15:14:33.938015: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 8, 128, 1, 90, 64, 128]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n2020-03-26 15:14:33.959686: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Internal: {{function_node __inference___backward_cudnn_lstm_with_fallback_350144_350320_specialized_for_StatefulPartitionedCall_2_at___inference_distributed_function_353504}} {{function_node __inference___backward_cudnn_lstm_with_fallback_350144_350320_specialized_for_StatefulPartitionedCall_2_at___inference_distributed_function_353504}} Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 8, 128, 1, 90, 64, 128]\r\n         [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n         [[StatefulPartitionedCall_2]]\r\n```\r\n\r\nAny ideas?\r\n\r\n---\r\n\r\n**EDIT**: I've tried limiting the memory usage, using the nightly build and setting the growth to True (`gpu = tf.config.experimental.list_physical_devices('GPU'), tf.config.experimental.set_memory_growth(gpu[0], True)` to see if it's an issue with GPU memory but I still get the following even when limiting to 2GB:\r\n\r\nIn Jupyter:\r\n\r\n```\r\n[_Derived_]  Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 56, 64, 1, 90, 64, 64] \r\n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\r\n\t [[PartitionedCall_2]] [Op:__inference_train_function_47260]\r\n\r\nFunction call stack:\r\ntrain_function -> train_function -> train_function\r\n```\r\n\r\nIn my terminal window:\r\n\r\n```\r\n2020-03-26 21:34:20.415064: E tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_INTERNAL_ERROR\r\nin tensorflow/stream_executor/cuda/cuda_dnn.cc(1986): 'cudnnRNNBackwardData( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, output_desc.handles(), output_data.opaque(), output_desc.handles(), output_backprop_data.opaque(), output_h_desc.handle(), output_h_backprop_data.opaque(), output_c_desc.handle(), output_c_backprop_data.opaque(), rnn_desc.param", "@nistrup \r\nwith reference to the error shared please find these example issues, let us know if it helps:\r\n\r\n#35950 #33536 #33924\r\n\r\n", "> @nistrup\r\n> with reference to the error shared please find these example issues, let us know if it helps:\r\n> \r\n> #35950 #33536 #33924\r\n\r\nI'm afraid none of this has worked. I'll spend some time trying to do a completely clean install and maybe try to downgrade my GPU drivers as well. So far nothing has worked. The models train completely fine using CPU only and breaks seemingly at random when using GPU, like in https://github.com/tensorflow/tensorflow/issues/37942", "Hi @nistrup ,\r\n\r\nplease check my issue #37942. I might have found a solution for your problem as well. \ud83d\ude42 ", "> Hi @nistrup ,\r\n> \r\n> please check my issue #37942. I might have found a solution for your problem as well. \ud83d\ude42\r\n\r\nThanks! It seems to have fixed one of my issues at least! Now I'm faced with this instead:\r\n\r\nAfter following the solutions suggested like:\r\n\r\n- Allowing GPU Memory Growth\r\n- Using `batch_input_shape` instead of `input_shape`\r\n- Using `drop_remainder=True` when creating batches\r\n\r\nI'm faced with the following after the first successfully trained model:\r\n\r\n```\r\n2020-03-27 17:21:28.275596: F .\\tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)\r\n[I 17:21:29.843 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports\r\nkernel 2bac517a-c195-47ca-952b-c25881cf0757 restarted\r\n```\r\n\r\nAnd the Jupyter Kernel crashes. Any ideas? :) ", "What is your batch_size? What value have you chosen?\r\n\r\n(As part of `batch_input_shape=(batch_size, n_timesteps, n_features)`)?", "> What is your batch_size?\r\n> \r\n> (As part of `batch_input_shape=(batch_size, n_timesteps, n_features)`)?\r\n\r\nCurrently 256, I'm using a setup where each HyperParameter permutation model is trained 3 times to give me an average score, the first model completes all 3 runs but then the kernel seems to die when starting the first run of the 2nd model.\r\n\r\n**Edit**:\r\n\r\nI should add that this is how I construct my training and validation data, where `BATCH_SIZE = 256`:\r\n\r\n```\r\ntrain_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\r\ntrain_data_single = train_data_single.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\r\n\r\nval_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\r\nval_data_single = val_data_single.batch(BATCH_SIZE, drop_remainder=True).repeat()\r\n```", "I also had this issues with such a high batch_size. `197` for me, for instance, did not work out and it lead to crashes with the same error. \r\n\r\nCurrently I use some value between 30 and 130 as `batch_size` and it works. Remember that it needs to be a divisor of len(X). \ud83d\ude04 ", "> I also had this issues with such a high batch_size. `197` for me, for instance, did not work out and it lead to crashes with the same error.\r\n> \r\n> Currently I use some value between 60 and 160 as `batch_size` and it works. Remember that it needs to be a divisor of len(X). \ud83d\ude04\r\n\r\nI just tried with BATCH_SIZE = 64 and the exact same thing happens, i.e. kernel crashes with `Check failed: work_element_count > 0 (0 vs. 0)`\r\n\r\nRegarding the fact that it needs to be a divisor of len(x_train), I think this should fix \"itself\" when constructing training data using the `.batch(BATCH_SIZE, drop_remainder=True)`", "Concerning `drop_remainder` I would think so too. \r\n\r\nI just checked and found that my error was slightly different:\r\n\r\n```\r\n2020-03-27 15:01:57.982960: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2020-03-27 15:01:57.983072: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\n```\r\n\r\nSo my advice maybe (lowering the batch_size) was not right.\r\n\r\nI fear, that you have gotten off the path that we shared, as your setup starts beginning to be different than mine. I really am not so happy about the current state of GPU-accelerated in Tensorflow/Keras, it is really tough to use. (Especially, as the CPU-equivalents _just_ work.) As I could not reproduce my problem with Google Colab, I really hope, that it is just an issue with my platform.\r\n\r\nGodspeed!", "I also think our paths have diverged slightly at this point! :)\r\n\r\nThanks a bunch for trying to help me out though, thoroughly appreciated!", "I think I will close this issue and create a new one related to my current issue instead since this seems to be unrelated at this point. Thanks! :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37932\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37932\">No</a>\n", "@nistrup \r\nCould you elaborate something? As I understood - when setting the `batch_input_shape` then you are forcing yourself to use the input data of given dimensions. So, then, if I later wanted to predict the outcome based on the input of batch size = 1 - that wouldn't be possible, is that correct?\r\n\r\nIn my current implementation I use a batch size of 2048 for training, and I want to be able to use a batch size of 1 for predictions later on. Will it still be possible to use your proposed solution? (sorry I am new in ML)\r\n\r\n\r\nOn a side note, I would like to ask whether there is a difference between using a dataset with a given batch size and how is it different/how does it work with conjunction with the back-size that we define for the model layers and fit method..?\r\n\r\ni.e. \r\n\r\n\tbatch_size = 2048\r\n\tepochs = 100\r\n\ttrain_size = TRAIN_X.shape[0]\r\n\tvalidation_size = VALIDATION_X.shape[0]\r\n\r\n\tcompute_steps_per_epoch = lambda x: int(math.ceil(1. * x / batch_size))\r\n\t\t\r\n\tsteps_per_epoch = compute_steps_per_epoch(train_size)\r\n\tval_steps = compute_steps_per_epoch(validation_size)\r\n\r\nwhat's the difference between\r\n\r\n\tfeatures = tf.convert_to_tensor(TRAIN_X, dtype=tf.float32)\r\n\tlabels = tf.convert_to_tensor(TRAIN_y, dtype=tf.float32)\r\n\r\n\tfeatures2 = tf.convert_to_tensor(VALIDATION_X, dtype=tf.float32)\r\n\tlabels2 = tf.convert_to_tensor(VALIDATION_y, dtype=tf.float32)\r\n\r\n\t# Train model\r\n\thistory = model.fit(\r\n\t\tx=features,\r\n\t\ty=labels,\r\n\t\tsteps_per_epoch=steps_per_epoch,\r\n\t\tepochs=epochs,\r\n\t\tvalidation_data=(features2, labels2),\r\n\t\tvalidation_steps=val_steps,\r\n\t\tcallbacks=[tensor_board, post_processing_callback, checkpoint],\r\n\t\tclass_weight=d_class_weights\r\n\t)\r\n\t\t\t\t\t\r\nand explicitly generating batches:\r\n\r\n\tbuffer_size_train = train_size\r\n\ttrain_data_ds = tf.data.Dataset.from_tensor_slices((TRAIN_X, TRAIN_y))\r\n\ttrain_data_batch = train_data_ds\\\r\n\t\t.shuffle(buffer_size_train)\\\r\n\t\t.batch(batch_size, drop_remainder=True)\\\r\n\t\t.repeat()\r\n\r\n\tbuffer_size_validation = validation_size\r\n\tvalidation_data_ds = tf.data.Dataset.from_tensor_slices((VALIDATION_X, VALIDATION_y))\r\n\tvalidation_data_batch = validation_data_ds\\\r\n\t\t.shuffle(buffer_size_validation)\\\r\n\t\t.batch(batch_size, drop_remainder=True)\\\r\n\t\t.repeat()\r\n\r\n\tsteps_per_epoch = compute_steps_per_epoch(train_size)\r\n\tval_steps = compute_steps_per_epoch(validation_size)\r\n\r\n\thistory = model.fit(\r\n\t\tx=train_data_batch,\r\n\t\tsteps_per_epoch=steps_per_epoch,\r\n\t\tepochs=epochs,\r\n\r\n\t\tvalidation_data=validation_data_batch,\r\n\t\tvalidation_steps=val_steps,\r\n\r\n\t\tcallbacks=[tensor_board, post_processing_callback, checkpoint],\r\n\t\tclass_weight=d_class_weights\r\n\t)\r\n\t\t\t\t\t\r\n**edit**: don't know if it currently matters but the model is built either:\r\n\r\n```\r\nmodel.add(LSTM(128, input_shape=(TRAIN_X.shape[1], TRAIN_X.shape[2]), return_sequences=False))\r\n```\r\n\r\nor\r\n\r\n```\r\n\r\nmodel.add(LSTM(128, batch_input_shape=(batch_size, TRAIN_X.shape[1], TRAIN_X.shape[2]), return_sequences=False))\r\n\r\nmodel.add(Dense(32, activation='relu'))\r\nmodel.add(Dropout(0.2))\r\n\r\n```\r\nI am trying to fix the issue mentioned in the subject :)\r\n\r\n", "I have recently had the exact same problem on Windows 10 and I figured out a solution.\r\n\r\nThe machine used has got a dual graphic card, one of them is an NVidia Quadro P5200 (16GB) and the second one a standard Intel 630 used for display. I am running Keras 2.3.1 (on top of TensorFlow GPU 2.1.0) with Python 3.7.7.\r\n\r\nThe problem mentionned above was occurring regardless of the batch_size (it also failed when batch_size was set to 1).\r\n\r\nWhat worked for me was to switch the NVidia card to `TCC` mode by running the following command (you need to **Run As Admin**).\r\n\r\n`\"C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe\" -g 0 -dm 1`\r\n\r\nThen reboot.\r\nThen go to the Device Manager and re-enable the device (I am still unsure why this steps is required, but it seems to be).\r\n\r\nAfter this, confirm the NVidia card has switched to TCC mode by running (you need to **Run As Admin**).\r\n\r\n`\"C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe\"`\r\n\r\nwhich should output something like this:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 451.77       Driver Version: 451.77       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro P5200        TCC  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   51C    P8     5W /  N/A |  15705MiB / 16296MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n(Note that once the card has switched to TCC mode, it disappear from the **Task Manager** performance tab.\r\n\r\nI hope this helps."]}, {"number": 37931, "title": "How can I get all tensor names in tflite model? Not only input_details and output_details", "body": "", "comments": ["Hey @taoja12 \r\nPlease provide necessary details like the TF version, environment etc. for your issue.\r\nAs for what I gather from your query, you use **tf.lite.Interpreter** to parse the **tflite** model\r\n```\r\ninterpreter = tf.lite.Interpreter(path=\"your_model.tflite\")\r\ninterpreter.allocate_tensors()\r\ninterpreter.get_tensor_details()\r\n```\r\nDo refer the docs - https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37931\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37931\">No</a>\n"]}, {"number": 37930, "title": "TFLu: Update MBED version to mbed-os-6.0.0-alpha-3", "body": "", "comments": ["Gentle ping for review", "@petewarden ping for review", "@petewarden ping for review"]}, {"number": 37929, "title": "[tflite] fix hexagon delegate profiling unit", "body": "it seems the unit of the number returned by GetCycles() is\r\nmicrosecond instead of millisecond. Dividing it by 1,000 so\r\nthat we can get more reasonable numbers when doing\r\nsomething like\r\n\r\n```\r\n./benchmark_model --graph=... --use_hexagon=1 --hexagon_profiling=1\r\n```", "comments": ["@karimnosseir gentle ping to review ", "Closing the PR, see comments discussion."]}, {"number": 37928, "title": "Building TensorFlow Lite NNAPI with C/C++ for Android: runtime link error libnnapi_delegate.so not found", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy Fold, Samsung Galaxy S10, Oppo ... etc Android Devices\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: latest (cloned 3 days ago)\r\n- Python version: 2.7\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): Apple clang version 11.0.0 (clang-1100.0.33.17)\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nI'm building TensorFlow Lite with C/C++ to run on Android devices. Building C, GPU delegate works well, but when I tried to link NNAPI delegate(```libnnapi_delegate.so```), library open fail on runtime.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nGot ```nnapi_delegate.so``` from\r\n```\r\nbazel build -c opt --config=android_arm64 //tensorflow/lite/delegates/nnapi:nnapi_delegate\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n### Error Log\r\n```\r\n2020-03-26 15:37:26.311 20452-20452/com.example.hellolibs E/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: com.example.hellolibs, PID: 20452\r\n    java.lang.UnsatisfiedLinkError: dlopen failed: library \"/Users/yonggyulee/Documents/GitHub/AI-Kit/tflite/c/hello-libs/app/src/main/cpp/../../../../distribution/tflite/lib/arm64-v8a/libnnapi_delegate.so\" not found\r\n        at java.lang.Runtime.loadLibrary0(Runtime.java:1016)\r\n        at java.lang.System.loadLibrary(System.java:1669)\r\n        at com.example.hellolibs.MainActivity.<clinit>(MainActivity.java:32)\r\n        at java.lang.Class.newInstance(Native Method)\r\n        at android.app.AppComponentFactory.instantiateActivity(AppComponentFactory.java:69)\r\n        at androidx.core.app.CoreComponentFactory.instantiateActivity(CoreComponentFactory.java:41)\r\n        at android.app.Instrumentation.newActivity(Instrumentation.java:1219)\r\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3040)\r\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3292)\r\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:78)\r\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:108)\r\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:68)\r\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1980)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:214)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7168)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:494)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:975)\r\n```\r\n\r\n### CMakeLists.txt\r\n```\r\ncmake_minimum_required(VERSION 3.4.1)\r\n\r\n# configure import libs\r\nset(distribution_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../../../../distribution)\r\n\r\nadd_subdirectory(\"${CMAKE_CURRENT_SOURCE_DIR}/opencv\")\r\n\r\nadd_library(lib_tflite SHARED IMPORTED)\r\nset_target_properties(lib_tflite PROPERTIES IMPORTED_LOCATION\r\n        ${distribution_DIR}/tflite/lib/arm64-v8a/libtensorflowlite_c.so)\r\n\r\nadd_library(lib_tflite_gpu SHARED IMPORTED)\r\nset_target_properties(lib_tflite_gpu PROPERTIES IMPORTED_LOCATION\r\n        ${distribution_DIR}/tflite/lib/arm64-v8a/libtensorflowlite_gpu_delegate.so)\r\n\r\nadd_library(lib_nnapi SHARED IMPORTED)\r\nset_target_properties(lib_nnapi PROPERTIES IMPORTED_LOCATION\r\n        ${distribution_DIR}/tflite/lib/arm64-v8a/libnnapi_delegate.so)\r\n\r\n# build application's shared lib\r\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=gnu++11\")\r\n\r\nadd_library(hello-libs SHARED\r\n        hello-libs.cpp)\r\n\r\ntarget_include_directories(hello-libs PRIVATE\r\n        ${distribution_DIR}/tflite/include\r\n        ${distribution_DIR}/tflite/include/tensorflow/lite/tools/make/downloads\r\n        ${distribution_DIR}/tflite/include/tensorflow/lite/tools/make/downloads/flatbuffers/include\r\n)\r\n\r\ntarget_link_libraries(hello-libs\r\n        android\r\n        lib_tflite\r\n        lib_tflite_gpu\r\n        neuralnetworks\r\n        lib_nnapi  # If i remove this line, there is no error, and everything including inference works well.\r\n        opencv\r\n        log)\r\n```\r\n", "comments": ["Solved with *static* library. But NNAPI delegate is slower than CPU or GPU.", "If the NNAPI delegate does not support some of the ops or parameter combinations in a model then it fallback on cpu for computing. This can cause it to execute slow performance than cpu.\r\nSee https://www.tensorflow.org/lite/performance/nnapi#use_supported_models_and_ops to know more.\r\nI will close this issue since the original problem is solved. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37928\">No</a>\n"]}, {"number": 37927, "title": "[MLIR][XLA] Add ConcatenateOp to LHLO/HLO emitters", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n@sherhut @pifon2a \r\n\r\nWe work on TensorFlow/MLIR to make mlir_gpu enable.", "comments": ["> Thanks. Could you add a test?\r\n\r\nThe test is added. Please help to review it. Thanks.", "Do I need to squash the two commits?", "> Do I need to squash the two commits?\r\n\r\nThat should not be needed. "]}, {"number": 37926, "title": "ValueError: Unknown floatx type: bfloat16 tf.keras.backend.set_floatx('bfloat16')", "body": "**System information** \r\nHave I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\nOS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Linux Mint 19.3 (ubuntu)\r\nTensorFlow installed from (source or\r\nbinary): binary\r\nTensorFlow version (use command below): tf-nightly\r\nPython version: 3.8 and 3.6\r\n\r\n**Describe the current behavior**\r\nwhen using layers with bfloat16 they are autocast to float32, and a warning/info gives a hint to use\r\n`tf.keras.backend.set_floatx('bfloat16')`\r\nwhen using the function a exception is raised\r\n`ValueError: Unknown floatx type: bfloat16`\r\n\r\n\r\n**Describe the expected behavior**\r\ndata type should be recognised\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nInfo from Tensorflow:\r\n```\r\nWARNING:tensorflow:Layer compress is casting an input tensor from dtype bfloat16 to the layer's dtype of float16, which is new behavior in TensorFlow 2.  The layer has dtype float16 because its dtype defaults to floatx.\r\n\r\nIf you intended to run this layer in float16, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\r\n\r\nTo change all layers to have dtype bfloat16 by default, call `tf.keras.backend.set_floatx('bfloat16')`. To change just this layer, pass dtype='bfloat16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\r\n```\r\nError if `tf.keras.backend.set_floatx('bfloat16')` is used:\r\n```\r\n2.2.0-dev20200325\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/__main__.py\", line 45, in <module>\r\n    cli.main()\r\n  File \"/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/../debugpy/server/cli.py\", line 427, in main\r\n    run()\r\n  File \"/home//.vscode-server/extensions/ms-python.python-2020.3.69010/pythonFiles/lib/python/debugpy/no_wheels/debugpy/../debugpy/server/cli.py\", line 264, in run_file\r\n    runpy.run_path(options.target, run_name=\"__main__\")\r\n  File \"/usr/lib/python3.6/runpy.py\", line 263, in run_path\r\n    pkg_name=pkg_name, script_name=fname)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 96, in _run_module_code\r\n    mod_name, mod_spec, pkg_name, script_name)\r\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/ShAReD_Net/training/run_train_train_model.py\", line 206, in <module>\r\n    main()\r\n  File \"/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/ShAReD_Net/training/run_train_train_model.py\", line 50, in main\r\n    tf.keras.backend.set_floatx(dtype.name)\r\n  File \"/home//Docker/volumes/3D_Person_Pose_Estimation_from_2D_Singelview_Image_Data/src/venv/lib/python3.6/site-packages/tensorflow/python/keras/backend_config.py\", line 108, in set_floatx\r\n    raise ValueError('Unknown floatx type: ' + str(value))\r\nValueError: Unknown floatx type: bfloat16```", "comments": ["@bela127, Please provide the standalone code to reproduce the issue. Thanks", "oh i see, the standalone code is given ;)\r\nits just that small\r\n\r\njust run:\r\n\r\n`tf.keras.backend.set_floatx('bfloat16')`", "@bela127, tf.keras.backend.set_floatx(value), here value should be `float16` or `float32` or `float64`. For more read the [Tensorflow doc](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx?version=nightly). Thanks ", "then the warning is very misleading, it explicitly tells to use` tf.keras.backend.set_floatx('bfloat16')` and with this restriction it is not practical to build models with `bfloat16`. which in fact is necessary for training without numeric instability.\r\n\r\nI think the message should be fixed , and/or\r\nshould i file a feature request?\r\n", "Thank you for this issue. The warning message is incorrect, as you noted.\r\n\r\nI can't think of a use case of setting floatx to bfloat16. If you have one, let me know, and we can consider allowing it to be set to bfloat16. Otherwise I'll fix the error message by removing the suggestion to call `set_floatx('bfloat16')`", "Hello,\r\n\r\nI have a verry big model, it only can fit in memory with bfloat16 or float16 on a rtx2080.\r\nFloat 16 gives me a lot of Nan because of a early overflow. Bfloat16 would have a larger dynamic with still less memory.\r\n\r\nThe only solution at the moment is training the model on CPU (a lot of ram is available) but it's very slow.", "Bfloat16 is not supported on Nvidia GPUs, such as the rtx2080, so you have to use float16 or float32. Neither bfloat16 or float16 is supported well on CPUs either, and will probably run slower. If running on the CPU and you have enough RAM, try float32 instead.\r\n\r\nAlso, most models require [mixed precision](https://www.tensorflow.org/guide/keras/mixed_precision) instead of just using float16/bfloat16 for numeric stability. Mixed precision might fix the NaN issue.", "I will try, thanks", "@bela127 As mentioned by @reedwm using `float16` results in numerical instability issues. There is a note on TF website about that. Please check [here](https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_floatx).\r\n\r\n> Note: It is not recommended to set this to float16 for training, as this will likely cause numeric stability issues. Instead, mixed precision, which is using a mix of float16 and float32, can be used by calling tf.keras.mixed_precision.experimental.set_policy('mixed_float16'). See the mixed precision guide for details. \r\n\r\n\r\nYou can check the example code in [this issue](https://github.com/tensorflow/tensorflow/issues/38457) that throws NaN randomly sometimes.\r\n\r\nIf you plan to use `mixed_precision` with GPU, add the following lines\r\n\r\n```\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n```\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!\r\n \r\nIf you plan to use `mixed_precision` with CPU, your code runs slower and code throws clear warning as shown below\r\n\r\n> WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\r\n> The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\r\n> If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once", "Closing this as this issue was resolved with the above comment. Please feel free to reopen if I am mistaken. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37926\">No</a>\n"]}, {"number": 37925, "title": "Added VariableV2 and VarHandleOp in python API of tesorflow", "body": "As per described in [#37855 (comment)](https://github.com/tensorflow/tensorflow/pull/37855#issuecomment-603986562), we also need to add `VariableV2` and `VarHandleOp` in python version of tf API.  \r\nAs of now, I have added the code in `python/ops/state_ops.py`. But it can be changed.  \r\n\r\n@mihaimaruseac , please review.", "comments": ["@mihaimaruseac , Is there anything I have to do?", "Not yet. Waiting for API review next week first", "This is available as tf.raw_ops.VarHandleOp, etc.", "Turns out we don't want public documentation for the `raw_ops`, @ashutosh1919 as they are supposed to only be internal.", "@mihaimaruseac , if you are closing this, then please reopen #37855 since that PR contains only descriptions about this two functions. "]}, {"number": 37924, "title": "_pywrap_file_io.BufferedInputStream raises UnicodeDecodeError", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Windows10 Home Edition\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary \r\n- TensorFlow version (use command below): 2.2.0rc1\r\n- Python version: 3.7.7 \r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source):  None\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: GeForce GTX 980Ti 38845MB\r\n\r\n**Describe the current behavior**\r\nIn jupyter notebook, I have tried TF Models tutorial(research/object_detection/object_detection_tutorial.ipynb) and got UnicodeDecodeError at following section.\r\n\r\nList of the strings that is used to add correct label for each box.\r\nPATH_TO_LABELS = 'models/research/object_detection/data/mscoco_label_map.pbtxt'\r\ncategory_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\r\n\r\nerror log:\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x8e in position 90: invalid start byte\r\n\r\n**Describe the expected behavior**\r\nUnicodeDecodeError don't happen. \r\n\r\n**Standalone code to reproduce the issue** \r\nfrom tensorflow.python import _pywrap_file_io\r\n_read_buf=_pywrap_file_io.BufferedInputStream('test', 1024*512)\r\n\r\n**Other info / logs** \r\nMy system language is Japanese and this may affect encode behavior.", "comments": ["@Syaryoh\r\nplease refer to this [issue](https://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s)\r\n\r\n[link1](https://www.edureka.co/community/51644/python-unicodedecodeerror-codec-decode-position-invalid) [link2](https://github.com/facebookresearch/VMZ/issues/62)  with similar error", "Thanks to reply.\r\nI'm sorry to my misunderstanding.\r\n\r\nThis problem occurs when the target file do not exist.\r\nIn my reproduce code, I have made empty file 'test' and the error has disappeared.\r\n\r\nIt would be appreciated if you could add error message for us to recognize the file not exist.", "@Syaryoh \r\nCan you please share simple sample code for us to replicate the issue faced by you,", "In this python code:\r\nfrom tensorflow.python import _pywrap_file_io\r\n_read_buf=_pywrap_file_io.BufferedInputStream('test', 1024*512)\r\n\r\nIf a file named 'test' do not exist, it raises UnicodeDecodeError.\r\n\r\n\r\nor this reproduces too:\r\nOpen the tutorial of TF Models(research/object_detection/object_detection_tutorial.ipynb) with Jupyter Notebook\r\nand change value of PATH_TO_LABELS to 'test'(not exist file). Then it reproduce.\r\n\r\n", "@Syaryoh\r\ncan you please share that test file, so we can replicate the issue", "@Syaryoh\r\ncan you please update as per above comment", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37924\">No</a>\n", "> In this python code: from tensorflow.python import _pywrap_file_io _read_buf=_pywrap_file_io.BufferedInputStream('test', 1024*512)\r\n> \r\n> If a file named 'test' do not exist, it raises UnicodeDecodeError.\r\n> \r\n> or this reproduces too: Open the tutorial of TF Models(research/object_detection/object_detection_tutorial.ipynb) with Jupyter Notebook and change value of PATH_TO_LABELS to 'test'(not exist file). Then it reproduce.\r\n\r\nthank you for the solution. I was trying to reach a directory full of photos but instead of photos directory I gave main directory path. e.g ; write `...--image_dir=D:\\\\AnacondaProjects\\\\tez\\\\Tensorflow\\workspace\\\\training_demo\\\\images\\\\test`  instead of` ...--image_dir=D:\\\\AnacondaProjects\\\\tez\\\\Tensorflow\\workspace\\\\training_demo\\\\images`"]}, {"number": 37923, "title": "How to get a flowchart for fireworks cnn optimization algorithm.", "body": "def fire_incept(x, fire=16, intercept=64):\r\n    x = Conv2D(fire, (5,5), strides=(2,2))(x)\r\n    x = LeakyReLU(alpha=0.15)(x)\r\n    \r\n    left = Conv2D(intercept, (3,3), padding='same')(x)\r\n    left = LeakyReLU(alpha=0.15)(left)\r\n    \r\n    right = Conv2D(intercept, (5,5), padding='same')(x)\r\n    right = LeakyReLU(alpha=0.15)(right)\r\n    \r\n    x = concatenate([left, right], axis=3)\r\n    return x\r\n\r\ndef fire_squeeze(x, fire=16, intercept=64):\r\n    x = Conv2D(fire, (1,1))(x)\r\n    x = LeakyReLU(alpha=0.15)(x)\r\n    \r\n    left = Conv2D(intercept, (1,1))(x)\r\n    left = LeakyReLU(alpha=0.15)(left)\r\n    \r\n    right = Conv2D(intercept, (3,3), padding='same')(x)\r\n    right = LeakyReLU(alpha=0.15)(right)\r\n    \r\n    x = concatenate([left, right], axis=3)\r\n    return x\r\n\r\nimage_input=Input(shape=input_shape)\r\n\r\nx = fire_incept((image_input), fire=16, intercept=16)\r\n\r\nx = fire_incept(x, fire=32, intercept=32)\r\nx = fire_squeeze(x, fire=32, intercept=32)\r\n\r\nx = fire_incept(x, fire=64, intercept=64)\r\nx = fire_squeeze(x, fire=64, intercept=64)\r\n\r\nx = fire_incept(x, fire=64, intercept=64)\r\nx = fire_squeeze(x, fire=64, intercept=64)\r\n\r\nx = Conv2D(64, (3,3))(x)\r\nx = LeakyReLU(alpha=0.1)(x)\r\n\r\nx = Flatten()(x)\r\n\r\nx = Dense(512)(x)\r\nx = LeakyReLU(alpha=0.1)(x)\r\nx = Dropout(0.1)(x)\r\n\r\nout = Dense(len(SPECIES), activation='softmax')(x)\r\n\r\nmodel_new = Model(image_input, out)\r\nmodel_new.summary()", "comments": ["@sree941, please share full code or link of google colab to reproduce issue.", "> @sree941, please share full code or link of google colab to reproduce issue.\r\n\r\nActually there is no issue with this code. I want a flowchart that describes this code. So I'm asking whether there is a code in python that produces the algorithm for this particular model. Or else can you just explain me this code please.", "This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "@sree941, You can try with `tf.keras.utils.plot_model`.", "> tf.keras.utils.plot_mode\r\n\r\nI tried and is getting an error like this:\r\nAttributeError: module 'tensorflow.python.keras.api._v1.keras.utils' has no attribute 'plot_model_new'", "@sree941, you need to pass model name in `plot_model` method.\r\n`tf.keras.utils.plot_model( model_new, to_file='model.png')`", "> `tf.keras.utils.plot_model( model_new, to_file='model.png')`\r\n\r\nyeah it's working thank you", "Closing the issue since its resolved. Thanks!"]}, {"number": 37922, "title": "Create a Sequential model in tensorflow", "body": "Hi All,\r\n         I have two models say M1, M2, I want to combine these models in such a way that the output of M1 is given as input to M2 and the gradient should be backpropagated till M1's input.", "comments": ["@saumya0303, you can do this by following code.\r\n```\r\nmodel = tf.keras.Sequential()\r\nmodel.add(M1)\r\nmodel.add(M2)\r\n```", "> @saumya0303, you can do this by following code.\r\n> \r\n> ```\r\n> model = tf.keras.Sequential()\r\n> model.add(M1)\r\n> model.add(M2)\r\n> M1,M2 = model.layers\r\n> M1.trainable = False\r\n> ```\r\nWhat is reason for M1.trainable=False?\r\n", "@nantha42, My mistake as I misunderstood the query as i assumed backpropagation till M1's output instead of M1's input. I have corrected it. ", "is it necessary that both M1 and M2 should be present in Keras library??", "@saumya0303, It is not necessary. It will work with M1 and M2 models created by you also.", "I am facing issue in adding a layer  to a pre-trained DeepSpeech model as I am not able to backproagate till that newly added layer input.", "@saumya0303, Can you share full code so that i can look at issue.", "@khimraj **https://github.com/mozilla/DeepSpeech/blob/master/DeepSpeech.py** this link correspond to the deepspeech code.I am facing issue with how to proceed by adding a layer in this code and backpropagate .", "This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "Thanks for the update."]}]