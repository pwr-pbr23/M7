[{"number": 11605, "title": "[solved] Reducing the binary size", "body": "Hi , all ~\r\n    i am now succeed in using a cross compiled lib project call tensorflow. \r\n    now i got tensorflow run on ios , android and linux , using one same app code . \r\n    any one has any questions can ask me for help.\r\n\r\n    so my question now is turned to \"Reducing the binary size\". \r\n    many posts told that to config  tf_op_files.txt, but my questions are:\r\n    1. need we also config other files such as  tf_pb_text_files.txt ,  proto_text_cc_files.txt  ... for reducing?\r\n    2. is there any other thinking for reducing than configuring the txt files Exhaustive Attackly ?", "comments": ["I'm assuming you're talking about Android?\r\n\r\n@andrewharp might know if there's a better procedure than culling ops by hand.\r\n\r\nThere is also `tf_compile`, which is kind of experimental right now.", "See [print_selective_registration_header.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/print_selective_registration_header.py) for a tool that will automatically generate a ops_to_register.h header given a GraphDef proto.\r\n\r\nYou shouldn't need to manipulate any of those other txt files.", "thx for the helps from  drpngx and andrewharp !\r\n\r\nmy goal is:\r\n 1 to do prediction on ios and android,  using a trained pb file .\r\n 2 using one same c++ code to call tensorflow in ios and android\r\n\r\ni tried  print_selective_registration_header.py\r\n", "print_selective_registration_header.py result:\r\n\r\n\r\n#ifndef OPS_TO_REGISTER\r\n#define OPS_TO_REGISTER\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || (strcmp(op, \"ArgMax\") == 0)\r\n     || (strcmp(op, \"BiasAdd\") == 0)\r\n     || (strcmp(op, \"Const\") == 0)\r\n     || (strcmp(op, \"Conv2D\") == 0)\r\n     || (strcmp(op, \"ExpandDims\") == 0)\r\n     || (strcmp(op, \"Identity\") == 0)\r\n     || (strcmp(op, \"MatMul\") == 0)\r\n     || (strcmp(op, \"MaxPool\") == 0)\r\n     || (strcmp(op, \"NoOp\") == 0)\r\n     || (strcmp(op, \"Placeholder\") == 0)\r\n     || (strcmp(op, \"Relu\") == 0)\r\n     || (strcmp(op, \"Reshape\") == 0)\r\n     || (strcmp(op, \"Squeeze\") == 0)\r\n     || (strcmp(op, \"StridedSlice\") == 0)\r\n     || (strcmp(op, \"_Recv\") == 0)\r\n     || (strcmp(op, \"_Send\") == 0)\r\n  ;\r\n}\r\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n\r\n    namespace {\r\n      constexpr const char* skip(const char* x) {\r\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\r\n      }\r\n\r\n      constexpr bool isequal(const char* x, const char* y) {\r\n        return (*skip(x) && *skip(y))\r\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\r\n                   : (!*skip(x) && !*skip(y));\r\n      }\r\n\r\n      template<int N>\r\n      struct find_in {\r\n        static constexpr bool f(const char* x, const char* const y[N]) {\r\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\r\n        }\r\n      };\r\n\r\n      template<>\r\n      struct find_in<0> {\r\n        static constexpr bool f(const char* x, const char* const y[]) {\r\n          return false;\r\n        }\r\n      };\r\n    }  // end namespace\r\n    constexpr const char* kNecessaryOpKernelClasses[] = {\r\n\"ArgMaxOp<CPUDevice, float>\",\r\n\"BiasOp<CPUDevice, float>\",\r\n\"ConstantOp\",\r\n\"Conv2DOp<CPUDevice, float>\",\r\n\"ExpandDimsOp\",\r\n\"IdentityOp\",\r\n\"MatMulOp<CPUDevice, float, false >\",\r\n\"MaxPoolingOp<CPUDevice, float>\",\r\n\"NoOp\",\r\n\"PlaceholderOp\",\r\n\"ReluOp<CPUDevice, float>\",\r\n\"ReshapeOp\",\r\n\"SqueezeOp\",\r\n\"StridedSliceOp<CPUDevice, float>\",\r\n\"RecvOp\",\r\n\"SendOp\",\r\n};\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#endif", "my  c++ code \uff1a\r\n\r\nint MyTensorflow::OfflineTensorFlowIsAf(deque<double> & section)\r\n{\r\n    Session* session;\r\n    GraphDef graph_def;\r\n    SessionOptions opts;\r\n    std::vector<Tensor> outputs;\r\n    std::vector<Tensor> y_outputs;\r\n    int i;\r\n    int sectionLen;\r\n\r\n//    QPrint(\"begin:%u\\n\", GetSystemTime());\r\n    QPrint(\"1MyTensorflow::OfflineTensorFlowIsAf:pbPathFile:%s\\n\", pbPathFile.c_str());\r\n    if (pbPathFile == \"\")\r\n    {\r\n        QPrint(\"MyTensorflow::OfflineTensorFlowIsAf:fatal error: pbPathFile is empty\\n\");\r\n        return 0;\r\n    }\r\n\r\n    TF_CHECK_OK(ReadBinaryProto(Env::Default(), pbPathFile, &graph_def));\r\n\r\n    TF_CHECK_OK(NewSession(opts, &session));\r\n\r\n    TF_CHECK_OK(session->Create(graph_def));\r\n\r\n    Tensor x(DT_FLOAT, TensorShape({1, 321}));\r\n\r\n    sectionLen = section.size();\r\n\r\n    for (i = 0; i < 321; i++)\r\n    {\r\n        if (i < sectionLen)\r\n        {\r\n            x.matrix<float>()(i) = section[i];\r\n        }\r\n        else\r\n        {\r\n            x.matrix<float>()(i) = 0;\r\n        }\r\n    }\r\n\r\n    TF_CHECK_OK(session->Run({{\"inputx\", x}}, {\"aFclass\"}, {}, &y_outputs)); // Get y_out\r\n\r\n    float y_out = y_outputs[0].scalar<int>()(0);\r\n\r\n    QPrint(\"MyTensorflow::OfflineTensorFlowIsAf:y_out:%f,sectionLen:%d\\n\", y_out, sectionLen);\r\n\r\n//    QPrint(\"end:%u\\n\", GetSystemTime());\r\n\r\n    if (0 == FloatCompares(y_out, 0))\r\n    {\r\n        return 0;\r\n    }\r\n    else\r\n    {\r\n        return 1;\r\n    }\r\n}", "i do reducing on linux , that's means ,  is adjust tf_op_files.txt, then build_all_linux.sh , then try my application code . until one day , there are no runtime erro. \r\n\r\ni tried to cut tf_op_files.txt to only one line left:\r\ntensorflow/core/kernels/ops_util.cc\r\n", "and adding these ones:\r\n\r\ntensorflow/core/kernels/argmax_op.cc\r\ntensorflow/core/kernels/quantized_bias_add_op.cc\r\ntensorflow/core/kernels/constant_op.cc\r\ntensorflow/core/kernels/deep_conv2d.cc\r\ntensorflow/core/kernels/xsmm_conv2d.cc\r\ntensorflow/core/kernels/identity_op.cc\r\ntensorflow/core/kernels/matmul_op.cc\r\ntensorflow/core/kernels/maxpooling_op.cc\r\ntensorflow/core/kernels/no_op.cc\r\ntensorflow/core/kernels/relu_op.cc\r\ntensorflow/core/kernels/reshape_op.cc\r\ntensorflow/core/kernels/strided_slice_op.cc\r\ntensorflow/core/kernels/sendrecv_ops.cc\r\n\r\naccording to \r\n\"ArgMaxOp<CPUDevice, float>\",\r\n\"BiasOp<CPUDevice, float>\",\r\n\"ConstantOp\",\r\n\"Conv2DOp<CPUDevice, float>\",\r\n\"ExpandDimsOp\",\r\n\"IdentityOp\",\r\n\"MatMulOp<CPUDevice, float, false >\",\r\n\"MaxPoolingOp<CPUDevice, float>\",\r\n\"NoOp\",\r\n\"PlaceholderOp\",\r\n\"ReluOp<CPUDevice, float>\",\r\n\"ReshapeOp\",\r\n\"SqueezeOp\",\r\n\"StridedSliceOp<CPUDevice, float>\",\r\n\"RecvOp\",\r\n\"SendOp\",", "so i did not find cc file in tf_op_files.txt according to : \"ExpandDimsOp\", \"PlaceholderOp\"\r\n\r\nso which cc files in tf_op_files.txt are according to \"ExpandDimsOp\", \"PlaceholderOp\" ?", "now here is my  tf_op_files.txt , and it cannot pass the compilation.\r\n\r\ntensorflow/core/kernels/ops_util.cc\r\ntensorflow/core/kernels/argmax_op.cc\r\ntensorflow/core/kernels/quantized_bias_add_op.cc\r\ntensorflow/core/kernels/constant_op.cc\r\ntensorflow/core/kernels/deep_conv2d.cc\r\ntensorflow/core/kernels/xsmm_conv2d.cc\r\ntensorflow/core/kernels/identity_op.cc\r\ntensorflow/core/kernels/matmul_op.cc\r\ntensorflow/core/kernels/maxpooling_op.cc\r\ntensorflow/core/kernels/no_op.cc\r\ntensorflow/core/kernels/relu_op.cc\r\ntensorflow/core/kernels/reshape_op.cc\r\ntensorflow/core/kernels/strided_slice_op.cc\r\ntensorflow/core/kernels/sendrecv_ops.cc", "seems that placeholder is in \uff1atensorflow/core/ops/array_ops.cc\r\n\r\ntensorflow::ops::Placeholder\r\n#include <array_ops.h>\r\n\r\nA placeholder op for a value that will be fed into the computation. \r\n\r\n", "it seems that \"ExpandDimsOp\", \"PlaceholderOp\" are all in tensorflow/core/ops/array_ops.cc\r\n\r\ntensorflow::ops::ExpandDims\r\n#include <array_ops.h>\r\n\r\nInserts a dimension of 1 into a tensor's shape. ", "struggled in getting familiar with tensor core for several nights, i finally found a way to do reducing.\r\n\r\n1. use  print_selective_registration_header.py  to find files probably cannot be cut in tf_op_files.txt, \r\n    just probably , not for sure.\r\n\r\n2. cut some of the files in tf_op_files.txt, if come into compile err or runtime err, restore some files.\r\n\r\n3. if there is no compile err and runtime err, continue to cut files.\r\n\r\n\r\nit's foolish, simple,  but works.", "I'm not quite sure what you're trying to do, but it looks you succeeded. I'm closing this issue but if you need further help, I recommend StackOverflow.", "@reedwm yes it's solved.\r\n\r\nandroid: \r\n[tf_op_files.txt](https://github.com/tensorflow/tensorflow/files/1255300/tf_op_files.txt)\r\n\r\nios:\r\n[tf_op_files.txt](https://github.com/tensorflow/tensorflow/files/1255302/tf_op_files.txt)\r\n\r\ncentos:\r\n[tf_op_files.txt](https://github.com/tensorflow/tensorflow/files/1255303/tf_op_files.txt)\r\n\r\n", "The pb generating files consume lots of binary space. for example, 160.1396kb by test_log.pb.o, 144.4404kb by meta_graph.pb.o which may not be used or appropriate on a mobile device. Is there any solution to remove or replace them?\r\n\r\nThe following file is a top size analysis by link map file, sorted by size.\r\n[text_section_size.txt](https://github.com/tensorflow/tensorflow/files/1449828/text_section_size.txt)\r\n\r\n  \r\n"]}, {"number": 11603, "title": "//tensorflow/python:nn_test is failing on ppc64le with AssertionError: False is not true ", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n     Ubuntu 16.04 (ppc64le)\r\n- **TensorFlow installed from (source or binary)**:\r\n      Installed from source (v1.2.1)\r\n- **TensorFlow version (use command below)**:\r\n      ('v1.2.1-0-gb4957ff', '1.2.1')\r\n- **Python version**: \r\n     Python 2.7.5\r\n- **Bazel version (if compiling from source)**:\r\n       0.4.5-2017-07-13 (@037b9b9)\r\n- **CUDA/cuDNN version**:\r\n     NA\r\n- **GPU model and memory**:\r\n      NA\r\n- **Exact command to reproduce**:\r\n      bazel test //tensorflow/python:nn_test\r\n\r\n### Describe the problem\r\nHere  testNaNs is failing , see relevant code-\r\nhttps://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/ops/nn_test.py#L835-L841\r\n\r\n```\r\ndef testNaNs(self):\r\n    # Test that relu(nan) = nan for various sizes.\r\n    for i in range(18):\r\n      x = np.zeros(i) + np.nan\r\n      with self.test_session():\r\n        z = nn_ops.relu(constant_op.constant(x)).eval()\r\n        print(\"\\n current i value is \", i)\r\n        print(\"\\n z value is \", z)\r\n        self.assertTrue(np.isnan(z).all())\r\n```\r\nThis test is failing ,because nn_ops.relu function returning incorrect results on ppc64le (0 vs expected nan).......(for i = 2 to 18 )\r\n\r\nfor` i = 0`  :   z value =   []    ............... (ok)\r\nfor `i = 1`  :   z value =  [ nan]  ........... (ok)\r\nfor `i = 2`  :   z value =  [ 0.  0.]  ...........(not ok on ppc64le, bcz `0. != nan`)\r\n\r\nIt looks like this is a bug in the nn_ops.relu function for ppc64le, currently I am trying to understand the reason. Please provide comments/suggestions if any. Thanks!\r\n### Source code / logs\r\n\r\n```\r\n$ bazel test --test_output=errors //tensorflow/python:nn_test\r\n\r\n......................../root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py:108: RuntimeWarning: divide by zero encountered in log\r\n  stirling_approx = z * np.log(z) - z + 0.5 * np.log(2. * np.pi * z)\r\n/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py:108: RuntimeWarning: invalid value encountered in multiply\r\n  stirling_approx = z * np.log(z) - z + 0.5 * np.log(2. * np.pi * z)\r\n................F.......\r\n======================================================================\r\nFAIL: testNaNs (__main__.ReluTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/725e77151072daec43bc353cb6fcb26c/execroot/tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py\", line 841, in testNaNs\r\n    self.assertTrue(np.isnan(z).all())\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 48 tests in 7.253s\r\n\r\nFAILED (failures=1)\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\n0.0208333333333\r\n0.00566666666667\r\n0.0075\r\nL2Loss gradient err = 9.6958e-12\r\nL2Normalize gradient err = 4.2424e-08\r\nL2Normalize gradient err = 5.45829e-07\r\nL2Normalize gradient err = 7.61142e-05\r\n================================================================================\r\n\r\n```", "comments": ["@npanpaliya any idea?", "Tried to run sample code using TF Relu API , but getting same issue -\r\n\r\n```\r\nx = np.zeros(2) + np.nan    // x = [nan nan]\r\nsess.run(tf.nn.relu(tf.constant(x)))           # fails on ppc64le: [0. 0.] vs expected [nan nan]\r\n```\r\nI was debugging this issue and could able to reach at https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/python/framework/op_def_library.py#L288  \r\n\r\nNow trying to understand apply_op function for Relu operation\r\n", "As per this doc - https://www.tensorflow.org/api_docs/python/tf/nn/relu\r\nRelu function computes rectified linear: `max(features, 0)`, so I tried computing maximum value \r\n\r\n```\r\nx = np.zeros(2) + np.nan   // x = [nan nan]\r\nnp.maximum(x, 0)              // OK on both the platform (X86 and ppc64le)\r\nsess.run(tf.maximum(x, 0))  // fails on ppc64le  returns [0. 0.] VS expected [nan nan]\r\n```\r\n\r\nAnyone has any idea ?", "Oh, I remember a PR where we changed that behavior to match `numpy`, for GPU and CPU. That is the behavior that we want. Do you want to send a PR?", "@drpngx tf and numpy aren't behaving same\r\nSo, do you want to modify this behavior of tf or modify the test to adjust as per current behavior of TF ?\r\n\r\nAnd yes, to fix this test failure , I want to send a PR. Can you please share the reference of your PR?", "In general we would like for them to behave the same, unless the behavior is undefined.", "Hi @drpngx , all\r\n\r\nInvestigating this further, the relu function from gen_nn_ops calls cwiseMax(..) in eigen here:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/relu_op_functor.h#L35\r\n\r\nTried to go down further from here, cwiseMax goes to the eigen source code, under eigen found a issue as below:\r\n\r\n**There is a bug filed under Eigen which switches the parameters to pmax etc**\r\n\r\nhttp://eigen.tuxfamily.org/bz/show_bug.cgi?id=1373\r\n\r\nThe corresponding PR makes changes to Eigen/src/core code files (PacketMath* etc), im not sure if these changes are present under the tensorflow source tree\r\n\r\nShould these changes be incorporated into tensorflow sources? (specifically the ppc changes in files AVX/PacketMath.h etc) \r\n\r\nAlso the eigen source files in tensorflow look different from the ones in the eigen repository (for eg. tf has file called PacketMathAVX2.h while eigen has AVX/PacketMath.h), if could get some pointers where these changes would need to be made in tensorflow if needed to fix this issue?\r\n\r\nI am still learning/understanding this part of the code, any further pointers will help a lot!\r\n\r\nThanks for any help!\r\n\r\n\r\n\r\n\r\n\r\n", "Thanks for figuring this out!\r\n\r\n@benoitsteiner and @rmlarsen are the experts.", "Hi @benoitsteiner , @rmlarsen ,\r\n\r\nWanted to check if can get any inputs on this issue.. thanks!", "Tried the change done in Eigen's PR in Power specific files Altivec/PacketMath.h. However, just swapping the arguments of max function didn't work on Power.\r\nThen I wrote a small program to see the behavior of vec_max and _mm_max_ps on Power and x86 respectively, which are called in Eigen's max function. And from the output of the program, it seems Power's builtin function for max calculation has a different behavior as compared to x86's intrinsics.\r\nChecked std::max too and it always gives 1st argument on both Power and x86.\r\n\r\n**Output on Power:**\r\n```\r\nOutput of vecMaxFunc(vector of NaNs, Vector of zeros)\r\nVec output at location [0] is 0\r\nVec output at location [1] is 0\r\nVec output at location [2] is 0\r\nVec output at location [3] is 0\r\n\r\nOutput of vecMaxFunc(vector of zeros, Vector of NaNs)\r\nVec output at location [0] is 0\r\nVec output at location [1] is 0\r\nVec output at location [2] is 0\r\nVec output at location [3] is 0\r\n\r\nstd::max NaN compared with 0: nan\r\nstd::max 0 compared with NaN: 0\r\n```\r\n\r\n\r\n**Output on X86:**\r\n```\r\nOutput of vecMaxFunc(vector of NaNs, Vector of zeros)\r\nVec output at location [0] is 0\r\nVec output at location [1] is 0\r\nVec output at location [2] is 0\r\nVec output at location [3] is 0\r\n\r\nOutput of vecMaxFunc(vector of zeros, Vector of NaNs)\r\nVec output at location [0] is nan\r\nVec output at location [1] is nan\r\nVec output at location [2] is nan\r\nVec output at location [3] is nan\r\n\r\nstd::max NaN compared with 0: nan\r\nstd::max 0 compared with NaN: 0\r\n```\r\n\r\nI'm still working on this to see if we can get some work-around or fix from IBM's respective team.\r\n", "I missed attaching my test program earlier.\r\n[testmax.cpp.txt](https://github.com/tensorflow/tensorflow/files/1535017/testmax.cpp.txt)\r\n", "max on Power currently uses the vec_max intrinsic, which maps to xsmaxsp instruction.  It appears that TensorFlow and Eigen want the \"c\" version of the instruction, xsmaxcsp.  There currently is no compiler intrinsic to access that version, but one can use GCC or LLVM inline assembly to generate xsmaxcsp instead of xsmaxsp.", "Note that xsmaxcsp (and xsmaxjsp) are new in Power ISA 3.0 (Power9).", "@drpngx : We want to understand the impact of this behavior of min/max function in the tensorflow (usage of Relu/Max operator). \r\nAs per [IEEE754 ](https://msdn.microsoft.com/en-us/library/windows/desktop/jj218760(v=vs.85).aspx#alpha_754_Deviations) standard, max(x, Nan) = max(Nan, x) = x. So, it seems Power's behavior is as per IEEE rule. \r\n", "OK, what would you need to understand?", "I want to understand the use or area/feature of tensorflow where Relu operator or Max operator is used. And specially this particular test case where these operators may have NaNs and based on the results, something is decided. So, in case we change the behavior of this operator for NaNs, what features would be impacted or where can we see the difference in the TF's functionality.", "How does TensorFlow rely on this specific NaN behavior that apparently is probed in the testsuite? Is this documented in TensorFlow as semantics of its API?", "This issue is fixed here - http://eigen.tuxfamily.org/bz/show_bug.cgi?id=1494\r\n\r\nPatch - http://eigen.tuxfamily.org/bz_attachmentbase/attachment.cgi?id=831"]}, {"number": 11602, "title": "There is a input sequence, how to calculate probability for a special output sequence?", "body": "\r\n", "comments": ["Hi, i trained a model of seq2seq, the result sequence of decoder is so good, but I try to calculate the probability for a special outputs sequence, I found it's so hard for me.", "The first thing I would do is to try and define your probability space. If you impose no requirements on the input sequence, you would have to perform an exhaustive search on your probability space. That would look something like `aaaaa`, `aaaab`, `aaaac`... for a five character input sequence. If you're accounting for simple punctuation (maybe `,.?!'` and space), upper and lower case letters, and numbers, this is about 70^5 (~1.7 * 10^9) possible five character input strings. Hundreds of millions of these are likely to be garbage strings that you wouldn't expect your model to ever encounter, but may still produce your desired output sequence! While interesting, the metric you're trying to find may not be as useful as you expect it to be unless you find a reasonable way of cutting down the sheer size of the probability space (which grows rather quickly).", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "OK, I got it.", "@eth-n  I try to use Trie to solve the problem,  limit the output sequence in fixed set and to got a best output sequence from the fixed outputs set, but the effect of generation is so terrible. Can you give me some suggestions, thanks a lot.", "Sorry @yick2232, I was intrigued by the abstract premise of your question and haven't tried anything along these lines before. I imagine someone's tried seq2seq visualization before; try looking very carefully through stack overflow, or look for blogs on the topic.\r\n\r\nOne of my coworkers wrote this post on visualizing RNNs, perhaps it'll help you in some regard: https://medium.com/datalogue/attention-in-keras-1892773a4f22", "@eth-n Thank you very much ! That is my issue,  I'll find the solution from other platform.  Thanks (^_^)", "@yick2232 ran into this by chance this morning, seems like you can use these techniques to look at `p(y|D, H), the distribution over network outputs y given data and a model` https://www.cs.cmu.edu/afs/cs/academic/class/15782-f06/slides/bayesian.pdf", "Thanks, I already finished the book that you gave me. I got a lot, and I find there is a discussion in [other issue of seq2seq](https://github.com/google/seq2seq/issues/81). They solve my problem.", "Can you elaborate on how you solved your problem? I seems to have the exact question.", "Okay, my problem is how to calculate the probability of a special output string.  I can got a logits of the whole vocabulary when the decoder run once.  And then, got the probability of  a character when the decoder run a step, the character in the specified output string.  In generally, you can take the specified character probability by normalize the logits, and take the log of the normalization result. Note that you should assign the current specified character as input of next decoder. Finally, just sum the probability of all specified character,  because of you have take the log of them. "]}, {"number": 11601, "title": "who can tell me ,where is gru backward code?i want modify it", "body": "who can tell me ,where is gru backward code?i want modify it\r\n\r\ni want implete https://arxiv.org/pdf/1606.03401.pdf\r\n\r\nreduce gru size", "comments": ["`tf.contrib.rnn.GRUCell` is implemented from [fine-grained ops](https://github.com/tensorflow/tensorflow/blob/cf7c008ab150ac8e5edb3ed053d38b2919699796/tensorflow/python/ops/rnn_cell_impl.py#L262) that have their own gradients, so there is no backward implementation for `GRUCell` itself. You can use `tf.contrib.rnn.GRUBlockCell`, which does have an explicit gradient implementation [here](https://github.com/tensorflow/tensorflow/blob/3737ac321e67410bf061257d5f644eae8abbf79b/tensorflow/contrib/rnn/kernels/gru_ops.h#L122).", "Thanks @rryan ! Closing."]}, {"number": 11600, "title": "Adding documentation about contrib.losses", "body": "-Add information for update from tf.contrib.losses.hinge_loss to tf.losses.hinge_loss\r\n-Add more notes of deprecation\r\n-Other minor corrections and updates", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.", "Jenkins, test this please", "Ah I misread which word was the noun.", "Jenkins, test this please"]}, {"number": 11599, "title": "Would you mind not calling the protobuf repository protobuf?", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n\r\nUbuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\n\r\nSource\r\n\r\n- **TensorFlow version (use command below)**:\r\n\r\n1.1.0\r\n\r\n- **Python version**: \r\n\r\n2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:\r\n\r\n0.5.2\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nCurrently the protobuf repo in workspace.bzl is called @protobuf. Unfortunately, in github.com/grpc/grpc, the protobuf is bind to @com_github_google_protobuf//:protobuf....So if I have anything that uses the GRPC repository it cannot be built together with Bazel.\r\n\r\nI tried rename all @protobuf// to @com_github_google_protobuf//, but it does not work, presumably because the patch file for protobuf get in the way.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["What do you mean? What problem are you seeing?", "In the workspace.bzl, tensorflow binds @protubuf to the protobuf repository, while https://github.com/grpc/grpc/blob/master/WORKSPACE binds @protobuf to the build target (not the repo). Suppose I have a third repository and pulling in both tensorflow and grpc as external repositories, the build will fail due to name clash, the error message is something like external/protobuf does not have attribute testonly, which basically says Bazel is trying to use @protobuf as a target but in fact the name is pulled in as a repo by tensorflow.", "Got it. I don't see a downside to renaming the protobuf. Mind sending a PR to @gunan ?", "Can I work on a PR for this issue?", "Sure.", "Hi @lakshayg , @jhseu is upgrading grpc. #11768 ", "Closing as this is resolved"]}, {"number": 11598, "title": "Unclear about how to make BeamSearchDecoder work", "body": "## UPDATE: In the latest tensorflow 1.2.1, this is no longer a problem.\r\n##  Please ignore this problem and install the latest tensorflow.\r\n\r\nHello, I am trying to understand the way to use BeamSearchDecoder in a seq2seq model by following the tutorial of [nmt](https://github.com/tensorflow/nmt#beam-search). However, both the documentation and error message seem to be very unclear for starters. I have wrote the minimal code for a common seq2seq purpose with beam search:\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [None, None])\r\nY = tf.placeholder(tf.int32, [None, None])\r\nX_seq_len = tf.placeholder(tf.int32, [None])\r\nY_seq_len = tf.placeholder(tf.int32, [None])\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\ndecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(128)\r\n\r\n# TRAINING DECODER\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = encoder_state,\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n# PREDICTING_DECODER\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [128]),\r\n    end_token = 2,\r\n    initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=10),\r\n    beam_width = 10,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = predicting_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.sample_id\r\n\r\n# LOSS\r\nmasks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)\r\nloss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)\r\n\r\n# BACKWARD\r\nparams = tf.trainable_variables()\r\ngradients = tf.gradients(loss, params)\r\nclipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))\r\n```\r\nThe error occurs at BeamSearchDecoder:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 48, in <module>\r\n    length_penalty_weight = 0.0)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py\", line 175, in __init__\r\n    initial_state, self._cell.state_size)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 319, in map_structure\r\n    assert_same_structure(structure[0], other, check_types=check_types)\r\n  File \"/Users/l2015005/anaconda2/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 146, in assert_same_structure\r\n    % (nest1, nest2))\r\nValueError: The two structures don't have the same number of elements. First structure: Tensor(\"tile_batch/Reshape:0\", shape=(20, ?, 128), dtype=float32), second structure: LSTMStateTuple(c=128, h=128).\r\n```", "comments": ["@ebrevdo do you mind having a look? Thanks.", "Your initial state should be decoder_cell(...).clone(cell_state=tile_batch(encoder_state, ...))", "Err decoder_cell.zero_state(...).clone(...)", "```\r\ninitial_state = decoder_cell.zero_state(128, tf.float32).clone(\r\n        cell_state=tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=10)),\r\n```\r\n@ebrevdo\r\nafter changing the code,\r\nthe error message says:\r\n```'LSTMStateTuple' object has no attribute 'clone'```\r\nI think this is because there is no attention wrapper, but I assume beam search can be done without attention", "If you don't use attention then you can directly pass the tiled encoder\nstate as the initial state (assuming your encoder and decoder cells are the\nsame architecture).\n\nOn Jul 19, 2017 10:02 PM, \"zhedongzheng\" <notifications@github.com> wrote:\n\n> initial_state = decoder_cell.zero_state(128, tf.float32).clone(\n>         cell_state=tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=10)),\n>\n> @ ebrevdo\n> if I don't use attention, the error says:\n> 'LSTMStateTuple' object has no attribute 'clone'\n> is beam search forced to be used with attention?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11598#issuecomment-316597391>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim_2vyHpyWMx2Zjp9TUufg4g9EbZkks5sPt9-gaJpZM4OcKdG>\n> .\n>\n", "@ebrevdo yes, that's exactly my problem at the first place. i directly pass the tiled encoder state, however the error message says:\r\n```\r\nValueError: The two structures don't have the same number of elements. First structure: Tensor(\"tile_batch/Reshape:0\", shape=(20, ?, 128), dtype=float32), second structure: LSTMStateTuple(c=128, h=128).\r\n```\r\nthe code and error in detail is at the top.\r\ncan we reopen this issue?", "Ah I see. Let me look more carefully.\n\nOn Jul 19, 2017 10:34 PM, \"zhedongzheng\" <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> yes, that's exactly my problem at\n> the first place. i directly pass the tiled encoder state, however the error\n> message says:\n>\n> ValueError: The two structures don't have the same number of elements. First structure: Tensor(\"tile_batch/Reshape:0\", shape=(20, ?, 128), dtype=float32), second structure: LSTMStateTuple(c=128, h=128).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11598#issuecomment-316601470>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimx74j5DyXqtWhQVnMSPYyWdA-iyYks5sPubngaJpZM4OcKdG>\n> .\n>\n", "Hm.  What version of tf are you using?", "@ebrevdo latest tf 1.2.1, sorry I should mention this earlier.", "I'll look more carefully tomorrow.", "hi  @ebrevdo  are you still investigating this?", "Try this:\r\n```\r\ninitial_state = tf.nn.rnn_cell.LSTMStateTuple([tf.contrib.seq2seq.tile_batch(o, multiplier=10) for o in encoder_state])\r\n```\r\nBecause encoder_state is a LSTMStateTuple, so you can't simply tile a tuple, but tile every element in a tuple, then pack it as LSTMStateTuple.", "Thanks @JerrikEph After changing code to this:\r\n``` python\r\n    initial_state = tf.nn.rnn_cell.LSTMStateTuple(tf.contrib.seq2seq.tile_batch(encoder_state[0], multiplier=10),\r\n                                                  tf.contrib.seq2seq.tile_batch(encoder_state[1], multiplier=10)),\r\n```\r\nand make sure\r\n```\r\nimpute_finished = False\r\n```\r\nIt is compiled correctly\r\nHowever, windows users may encounter issue [9832](https://github.com/tensorflow/tensorflow/issues/9832)\r\nIf you are not using windows, it should be fine.", "You closed the issue, can you record for posterity how you resolved it?\n\nOn Jul 28, 2017 2:31 AM, \"zhedongzheng\" <notifications@github.com> wrote:\n\n> Closed #11598 <https://github.com/tensorflow/tensorflow/issues/11598>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11598#event-1183431839>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyKluuFaQnfjTVQ48NBIvHKcnUD1ks5sSdSSgaJpZM4OcKdG>\n> .\n>\n", "okay, the key is to:\r\n* tile the encoder state separately for LSTMStateTuple\r\n* make sure impute_finished is False\r\nthe complete code is:\r\n``` python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.layers.core import Dense\r\n\r\n# INPUTS\r\nX = tf.placeholder(tf.int32, [None, None])\r\nY = tf.placeholder(tf.int32, [None, None])\r\nX_seq_len = tf.placeholder(tf.int32, [None])\r\nY_seq_len = tf.placeholder(tf.int32, [None])\r\n\r\n# ENCODER         \r\nencoder_out, encoder_state = tf.nn.dynamic_rnn(\r\n    cell = tf.nn.rnn_cell.BasicLSTMCell(128), \r\n    inputs = tf.contrib.layers.embed_sequence(X, 10000, 128),\r\n    sequence_length = X_seq_len,\r\n    dtype = tf.float32)\r\n\r\n# DECODER COMPONENTS\r\nY_vocab_size = 10000\r\ndecoder_embedding = tf.Variable(tf.random_uniform([Y_vocab_size, 128], -1.0, 1.0))\r\nprojection_layer = Dense(Y_vocab_size)\r\ndecoder_cell = tf.nn.rnn_cell.BasicLSTMCell(128)\r\n\r\n# TRAINING DECODER\r\ntraining_helper = tf.contrib.seq2seq.TrainingHelper(\r\n    inputs = tf.nn.embedding_lookup(decoder_embedding, Y),\r\n    sequence_length = Y_seq_len,\r\n    time_major = False)\r\ntraining_decoder = tf.contrib.seq2seq.BasicDecoder(\r\n    cell = decoder_cell,\r\n    helper = training_helper,\r\n    initial_state = encoder_state,\r\n    output_layer = projection_layer)\r\ntraining_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = training_decoder,\r\n    impute_finished = True,\r\n    maximum_iterations = tf.reduce_max(Y_seq_len))\r\ntraining_logits = training_decoder_output.rnn_output\r\n\r\n# PREDICTING_DECODER\r\npredicting_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\r\n    cell = decoder_cell,\r\n    embedding = decoder_embedding,\r\n    start_tokens = tf.tile(tf.constant([1], dtype=tf.int32), [128]),\r\n    end_token = 2,\r\n    initial_state = tf.nn.rnn_cell.LSTMStateTuple(\r\n        tf.contrib.seq2seq.tile_batch(encoder_state[0], multiplier=10),\r\n        tf.contrib.seq2seq.tile_batch(encoder_state[1], multiplier=10)),,\r\n    beam_width = 10,\r\n    output_layer = projection_layer,\r\n    length_penalty_weight = 0.0)\r\npredicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\r\n    decoder = predicting_decoder,\r\n    impute_finished = False,\r\n    maximum_iterations = 2 * tf.reduce_max(Y_seq_len))\r\npredicting_logits = predicting_decoder_output.sample_id\r\n\r\n# LOSS\r\nmasks = tf.sequence_mask(Y_seq_len, tf.reduce_max(Y_seq_len), dtype=tf.float32)\r\nloss = tf.contrib.seq2seq.sequence_loss(logits = training_logits, targets = Y, weights = masks)\r\n\r\n# BACKWARD\r\nparams = tf.trainable_variables()\r\ngradients = tf.gradients(loss, params)\r\nclipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(zip(clipped_gradients, params))\r\n```", "\r\n\r\n\r\n\r\n> zhedongzheng commented on Jul 30 \u2022 edited\r\n> okay, the key is to:\r\n> \r\n> tile the encoder state separately for LSTMStateTuple\r\n> make sure impute_finished is False\r\n> the complete code is:\r\n@zhedongzheng \r\nHi, Here why  should we make sure impute_finished is False.? I encounter an Error when I use impute_finished \uff1dTrue with BeamSearchDecoder using dynamic_decode . False , will be right without bug .The error points the shape of finished corresponding the  line 265 in https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/seq2seq/python/ops/decoder.py\r\nmy verison of tensorflow is r1.3.\r\nlog:Err : Shapes must be equal rank, but are 3 and 2. detail here:\r\nFile \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 286, in dynamic_decode\r\n    swap_memory=swap_memory)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2775, in while_loop\r\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2604, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2554, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 269, in body\r\n    _maybe_copy_state, decoder_state, state)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/util/nest.py\", line 379, in map_structure\r\n    structure[0], [func(*x) for x in entries])\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py\", line 265, in _maybe_copy_state\r\n    return new if pass_through else array_ops.where(finished, cur, new)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 2367, in where\r\n    return gen_math_ops._select(condition=condition, t=x, e=y, name=name)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2255, in _select\r\n    name=name)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2632, in create_op\r\n    set_shapes_for_outputs(ret)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1911, in set_shapes_for_outputs\r\n    shapes = shape_func(op)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1861, in call_with_requiring\r\n    return call_cpp_shape_fn(op, require_shape_fn=True)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 595, in call_cpp_shape_fn\r\n    require_shape_fn)\r\n  File \"/home/aisp/virtualenv/tensflow/local/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.py\", line 659, in _call_cpp_shape_fn_impl\r\n    raise ValueError(err.message)", "@lecun hi, this is related to the internal implementation of beam search decoder. Currently, impute_finished must be turned off when beam search decoding is performed. I think the TF team will make this right in next versions (1.4 or 1.5).", "Sounds like there's a bug between using impute_finished and the beam search\ndecoder.  We'll take a look next week.\n\nOn Sep 8, 2017 4:45 AM, \"zhedongzheng\" <notifications@github.com> wrote:\n\n> @lecun <https://github.com/lecun> hi, this is related to the internal\n> implementation of beam search decoder. Currently, impute_finished must be\n> turned off when beam search decoding is performed. I think the TF team will\n> make this right in next versions (1.4 or 1.5).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11598#issuecomment-328081801>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim7hHzFjXbgYTGKwE2OdXgf1-cz43ks5sgSi7gaJpZM4OcKdG>\n> .\n>\n", "@zhedongzheng @ebrevdo  Thanks very much. ", "Hi Eugene ( @ebrevdo ), is there any update on `inpute_finished=True` in dynamic_decode with a BeamSearchDecoder ? \r\n\r\nI still have this problem on tf1.4 and cannot tell how correct it is to have it on False, judging from the docstring. It says that the gradient back-propagation would be wrong for a padded batch, but we cannot use the BeamSearchDecoder in training anyway.", "You should be able to use `impute_finished=False` and then mask the outputs with `tf.sequence_mask` using the final sequence lengths returned by the decoder.  Does that solve your problem?", "Sorry for missing this, @ebrevdo.\r\nYes, as I don't use the decoder states, masking the outputs would do it.\r\nI am now concerned about `impute_finished` and `dynamic_decode` during training. \r\n\r\n```\r\nself._decoder_train_outputs, self._final_states, self._final_seq_lens = seq2seq.dynamic_decode(\r\n            self._decoder_train,\r\n            output_time_major=False,\r\n            impute_finished=True,\r\n            swap_memory=False)\r\nself._train_prediction = tf.argmax(self._decoder_train_outputs.rnn_output, axis=-1, output_type=tf.int32)\r\n```\r\n\r\nWhile checking for some predictions, I could see that sometimes the network predicts the EOS token before the actual EOS in the ground truth. However, decoding does not stop there during training, as `ScheduledEmbeddingTrainingHelper` will keep feeding the entire label stream. This results in a decoded sequence having multiple EOS tokens, and `impute_finished` adds zeros only after the last EOS in the ground truth. This also implies that there is a possibility for an EOS prediction to be sampled for being fed again to the decoder's input, thus adding some noise during training. Is there any way of preventing this behaviour ? It does not seem that `ScheduledEmbeddingTrainingHelper` could swap a sampled EOS token with the true label at the moment, as it does not take the `end_token` argument unlike `GreedyEmbeddingHelper` or `BeamSearchDecoder`.", "hi\uff0c@zhedongzheng\r\n\r\n\u4f60\u7684\u610f\u601d\u662f\u5728tensorflow 1.2.1\u4ee5\u53ca\u66f4\u9ad8\u7684\u7248\u672c\u4e2d\uff0c\r\ninitial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=10)\r\n\u548c\r\ninitial_state = tf.nn.rnn_cell.LSTMStateTuple(\r\n        tf.contrib.seq2seq.tile_batch(encoder_state[0], multiplier=10),\r\n        tf.contrib.seq2seq.tile_batch(encoder_state[1], multiplier=10)),,\r\n\u662f\u4e00\u4e2a\u6548\u679c\u4e86\u5417\uff1f", "@fword \u662f\u7684\uff0c\u56e0\u4e3atf\u57281.2.1\u4ee5\u540e\uff0ctile_batch()\u4f1a\u81ea\u52a8\u8bc6\u522b\u51faLSTMStateTuple\u5e76\u7ec4\u88c5\uff0c\u4e0d\u9700\u8981\u81ea\u5df1\u624b\u52a8\u89e3\u51b3\u4e86\r\n\r\nyes, after TF 1.2.1, The LSTM state tuple will be automatically and correctly handled by tile_batch()", "\u8c22\u8c22\r\nthanks", "@georgesterpu hi, I found the beam search does not stop  when generated EOS tokens. So, it will generate multiple EOS tokens,and beam_size sentences will alike. such as:\r\nbeam_size=5,result:\r\nsentence1\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u5c0f\u66e6\u4e5f\u662f\u5341\u5206\u62b1\u6b49\u7684 eos unk unk unk unk unk\r\nsentence2\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u975e\u5e38\u62b1\u6b49 eos eos eos eos eos eos eos eos eos eos eos\r\nsentence3\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u975e\u5e38\u62b1\u6b49 eos eos eos eos eos eos eos eos eos eos unk\r\nsentence4\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u975e\u5e38\u62b1\u6b49 eos eos eos eos eos eos eos unk unk unk unk\r\nsentence5\uff1a \u7ed9\u60a8\u5e26\u6765\u4e0d\u597d\u7684\u4f53\u9a8c\u4e86\uff0c\u975e\u5e38\u62b1\u6b49 eos unk unk unk unk unk unk unk unk unk unk\r\nDo you solve this problem?", "Input 0 of layer lstm_cell_3 is incompatible with the layer: expected ndim=2, found ndim=3. Full shape received: [512, None, 256]..\r\ni am getting this error when implementing beam search."]}, {"number": 11597, "title": " Not found: Key <variable_name> not found in checkpoint even though it exists in meta graph", "body": "### System information\r\nPython version: 3.6\r\nTensorflow version: 1.1\r\n\r\nHere are the commands needed to reproduce the error:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n    \r\ntf.reset_default_graph()\r\n\r\n# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\r\nx_data = np.random.rand(100).astype(np.float32)\r\ny_data = x_data * 0.1 + 0.3\r\n\r\n# Try to find values for W and b that compute y_data = W * x_data + b\r\n# (We know that W should be 0.1 and b 0.3, but TensorFlow will\r\n# figure that out for us.)\r\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0), name='W')\r\nb = tf.Variable(tf.zeros([1]), name='b')\r\ny = W * x_data + b\r\n\r\n# Minimize the mean squared errors.\r\nloss = tf.reduce_mean(tf.square(y - y_data))\r\noptimizer = tf.train.GradientDescentOptimizer(0.5)\r\nopt_op = optimizer.minimize(loss)\r\n\r\n# Track the moving averages of all trainable variables.\r\nema = tf.train.ExponentialMovingAverage(decay=0.9999)\r\nvariables = tf.trainable_variables()\r\nprint(variables)\r\naverages_op = ema.apply(tf.trainable_variables())\r\nwith tf.control_dependencies([opt_op]):\r\n    train_op = tf.group(averages_op)\r\n\r\n# Before starting, initialize the variables.  We will 'run' this first.\r\ninit = tf.global_variables_initializer()\r\n\r\nsaver = tf.train.Saver(tf.trainable_variables())\r\n\r\n# Launch the graph.\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\n# Fit the line.\r\nfor _ in range(201):\r\n    sess.run(train_op)\r\n\r\nw_reference = sess.run('W/ExponentialMovingAverage:0')\r\nb_reference = sess.run('b/ExponentialMovingAverage:0')\r\n\r\nsaver.save(sess, os.path.join(\"model_ex1\"))\r\n\r\ntf.reset_default_graph()\r\n\r\ntf.train.import_meta_graph(\"model_ex1.meta\")\r\nsess = tf.Session()\r\n\r\nprint('------------------------------------------------------')\r\nfor var in tf.global_variables():\r\n    print('all variables: ' + var.op.name)\r\nfor var in tf.trainable_variables():\r\n    print('normal variable: ' + var.op.name)\r\nfor var in tf.moving_average_variables():\r\n    print('ema variable: ' + var.op.name)\r\nprint('------------------------------------------------------')\r\n\r\n\r\nrestore_vars = {}\r\n\r\nema = tf.train.ExponentialMovingAverage(1.0)\r\nfor var in tf.trainable_variables():\r\n    print('%s: %s' % (ema.average_name(var), var.op.name))\r\n    restore_vars[ema.average_name(var)] = var\r\n\r\nsaver = tf.train.Saver(restore_vars, name='ema_restore')\r\n\r\nsaver.restore(sess, os.path.join(\"model_ex1\"))\r\n\r\nw_restored = sess.run('W:0')\r\nb_restored = sess.run('b:0')\r\n\r\nprint(w_reference)\r\nprint(w_restored)\r\nprint(b_reference)\r\nprint(b_restored)\r\n\r\n```", "comments": ["Can you explain what you are trying to do in following lines?\r\n```\r\nsaver = tf.train.Saver(restore_vars, name='ema_restore')\r\nsaver.restore(sess, os.path.join(\"model_ex1\"))\r\n```", "I'm telling the saver to load the EMA weights instead of the normal weights, or have I done it wrong?", "I think tf.train.saver() must be saved. Passing arguments to the constructor won't be sufficient I guess. Please refer, [https://www.tensorflow.org/api_docs/python/tf/train/Saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver)", "What do you mean by that? Haven't I already used saver.save after doing gradient descent?\r\n\r\n```\r\n# Fit the line.\r\nfor _ in range(201):\r\n    sess.run(train_op)\r\n\r\nw_reference = sess.run('W/ExponentialMovingAverage:0')\r\nb_reference = sess.run('b/ExponentialMovingAverage:0')\r\n\r\nsaver.save(sess, os.path.join(\"model_ex1\"))\r\n```", "I am referring `tf.train.Saver(restore_vars, name='ema_restore')`", "I've just looked at the docs, and I can't seem to find the code that says to save tf.train.saver() before loading it. If you got it working could you show me your code? Thanks", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nThat said, when you recreate the `ExponentialMovingAverage` after saving the checkpoint, it is unaware of the current variables and shadow variables, since you just created it. Then, when you call `ema.average_name(var)`, it recreates new shadow variables since it doesn't know about the old shadow variables. I'm not sure how to solve this while still using a `MetaGraph`; try asking StackOverflow or rebuild the model instead of using a `MetaGraph`.", "Hello all,\r\n\r\njust follow the below video and export your own model with in a 10 seconds\r\n\r\nhttps://youtu.be/w0Ebsbz7HYA"]}, {"number": 11596, "title": "Remove unittest import and duplicate logic from sparse_ops_test", "body": "", "comments": []}, {"number": 11595, "title": "[XLA] Fix plugin example buffer address for tuples", "body": "Currently the returned `se::DeviceMemoryBase(buf, size)` uses the incremented `buf`, which may cause deallocation error.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Mind handling the CLA? Thanks!", "sure, working on that with our company", "Close for now, will resubmit with the correct CLA"]}, {"number": 11594, "title": "Reverted NHWC workarounds.", "body": "This commit removes the NHWC workarounds we had created earlier. ", "comments": ["Can one of the admins verify this patch?", "Sorry. This was an incomplete pull request. Please ignore."]}, {"number": 11593, "title": "Fix issue #10399", "body": "Because of the bytes compatibility this function is returning bytes\r\nwhen it should be returning strings.\r\n\r\nFixes issue \"TypeError: Can't mix strings and bytes in path\r\ncomponents\" when using `tf.contrib.learn.learn_runner` with\r\nexperiments with an `export_strategy`.\r\n\r\nSee issue #10399 for a code example reference.\r\n\r\nI didn't find obvious mistakes, but this is my first contribution and\r\nI may have missed something.", "comments": ["Can one of the admins verify this patch?", "Sorry, I do not understand this fix. Do you have any particular error case you want to fix?\r\n\r\nas_bytes might not be the best choice here. But dropping it seems very bad, as it will trigger the mix strings and bytes error. \r\n\r\nPlease reopen if I missed something here.", "On Python 3, the referred bug happens due to the modfiied function returning paths as bytes, while other functions, namely list_directory in [file_io.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py) expect and deal with strings. If you are not comfortable dropping as_bytes, as per the poor type safety that Python gives us, I can resubmit the patch changing the conversion to `compat.as_str_any` instead, if that seems more reasonable. The rationale behind bug #10399 is that a `os..path.join` is happening between a something returned by this function (bytes) and something returned (down the stacktrace) by list_directory in file_io.py.\r\n\r\nOn Python 2, I haven't seen any regression and all tests in //tensorflow/python/... seem to be passing on my computer.", "No, not exactly. Unless I missed something, the underlying issue is all args passed to os.path.join should be same. It can be both bytes or strs, but not mix. \r\n\r\nYou fix simply dropped the as_bytes for both components in this file, which would make the mix happen  in this file not the gc.py (as the log shown in #10399). And #10399 seems be fixed by  https://github.com/tensorflow/tensorflow/pull/9719/files already. \r\n\r\nI am going to ping #10399 to see whether this got fixed first. \r\n\r\n\r\n"]}, {"number": 11592, "title": "Cross compile TensorFlow for armeabi-v7a failed", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n- **Python version**:\r\n 2.7.12\r\n- **Bazel version (if compiling from source)**:\r\n0.5.2\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\nbazel build -c opt tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI was able to cross-compile TensorFlow to armeabi-v7a using TensorFlow 1.0. But when I used the same command using TensorFlow 1.2.1, the compilation failed. Please find the detailed error message in the Source code/logs section. \r\n\r\nThe command I used is as follows:\r\nbazel build -c opt tensorflow/core:android_tensorflow_lib --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nERROR: /home/yitao/.cache/bazel/_bazel_yitao/c0c4f6e5ab173d18db201c4c55c4dc60/external/protobuf/BUILD:133:1: C++ compilation of rule '@protobuf//:protobuf' failed: false failed: error executing command /bin/false -MD -MF bazel-out/stub_armeabi-v7a-opt/bin/external/protobuf/_objs/protobuf/external/protobuf/src/google/protobuf/stubs/substitute.pic.d ... (remaining 26 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nTarget //tensorflow/core:android_tensorflow_lib failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 42.154s, Critical Path: 6.27s\r\n\r\n\r\n", "comments": ["@ychen404 Do you have the SDK API/build tools levels specified in WORKSPACE installed? The default changed between 1.0 and 1.2.", "@andrewharp Thanks for your reply. It's working now after specifying the proper SDK level.", "I am doing cross compile too now, but i had test many ways ,all failed,i download the tensorflow 1.2 source code and under linux i want to compile .so file for android,canyou help me ,the command of bazel build what", "The bazel command to cross-compile is as below: \r\nbazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \\\r\n   --crosstool_top=//external:android/crosstool \\\r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\r\n   --cpu=armeabi-v7a\r\n\r\nYou also need to edit you WORKSPACE file to make sure the paths and versions of SDK and NDK are correct. \r\n"]}, {"number": 11591, "title": "slice_input_producer bug (slows down exponentially for larger datasets)", "body": "My input pipeline involves sampling a filename and label with slice_input_producer, then data augmentation, then batching with tf.train.shuffle_batch. For smaller datasets (<1M), this works fine. For larger datasets, slice_input_producer slows down, even though all it's doing is sampling from two lists. It's bad enough that it's several orders of magnitude slower than all the rest of my input pipeline and training combined. I wrote a quick test to measure the time per call to slice_input_producer for different lengths of input lists:\r\n```\r\nimport time\r\n\r\nimport tensorflow as tf\r\n\r\nrept = 1000\r\nfor datasize in [1e3,1e6,1e7,1e8]:\r\n    sym_x = tf.train.slice_input_producer(\r\n        [list(range(int(datasize)))], shuffle=True,\r\n        capacity=10)\r\n    with tf.Session() as sess:\r\n        sess.run(tf.local_variables_initializer())\r\n        sess.run(tf.global_variables_initializer())\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n        start_time = time.time()\r\n        for r in range(rept):\r\n            x = sess.run(sym_x)\r\n        print('Datalength: %i  ---  time = %.3f ms/sample'\r\n              % (datasize,(time.time()-start_time)/rept*1000))\r\n        coord.request_stop()\r\n        coord.join(threads)\r\n        sess.close()\r\n```\r\n\r\nWhich gives:\r\n\r\n> Datalength: 1000  ---  time = 0.172 ms/sample\r\n> Datalength: 1000000  ---  time = 0.207 ms/sample\r\n> Datalength: 10000000  ---  time = 0.537 ms/sample\r\n> Datalength: 100000000  ---  time = 13.991 ms/sample\r\n\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n('v1.2.0-0-g12f033d', '1.2.0')", "comments": ["Thanks for reporting. If it's new code, I would recommend using `tf.contrib.data`.\r\n\r\n/CC: @mrry ", "I wrote a similar test for the tf.contrib.data functions I would be using for the same task above:\r\n\r\n```\r\nimport time\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops,dtypes\r\nfrom tensorflow.contrib.data import Dataset\r\nfrom tensorflow.contrib.data import Iterator\r\n\r\nrept = 1000\r\nfor datasize in [1e3, 1e6, 1e7, 1e8]:\r\n    data = list(range(int(datasize)))\r\n    data_tensor = ops.convert_to_tensor(data)\r\n    dataset = Dataset.from_tensor_slices(data_tensor)\r\n    dataset = dataset.shuffle(int(datasize))\r\n    iterator = Iterator.from_structure(dataset.output_types,\r\n                                       dataset.output_shapes)\r\n    init_op = iterator.make_initializer(dataset)\r\n    next_element = iterator.get_next()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        start_time = time.time()\r\n        for r in range(rept):\r\n            x = sess.run(next_element)\r\n        print('Datalength: %i  ---  time = %.3f ms/sample'%(\r\n        datasize, (time.time()-start_time)/rept*1000))\r\n        sess.close()\r\n```\r\n\r\nIt actually does worse:\r\n\r\n> Datalength: 1000  ---  time = 0.113 ms/sample\r\n> Datalength: 1000000  ---  time = 0.374 ms/sample\r\n> Datalength: 10000000  ---  time = 2.647 ms/sample\r\n> Datalength: 100000000  ---  time = 27.247 ms/sample\r\n\r\nIn both cases (tf.contrib.data and the old functions above), the overhead appears to be in shuffling larger datasets. The tf.contrib.data functions at least allow access to the buffer size for shuffling, so I can do shuffling on subsets of the data (not true random, but better than nothing). This solves the problem in practice for large datasets. Why not offer an option to do shuffling by random sampling with replacement? This would scale better - there's no need to actually shuffle a huge list, just sample randomly.\r\n\r\nIn practice, tf.contrib.data is about 3x slower than my old input pipeline on real disk reads, so @drpngx maybe you could expand on why you recommend tf.contrib.data. Is there a reason beyond being able to set the buffer size for the shuffle operation?", "Just to clarify, those two programs aren't equivalent. In the `tf.contrib.data` version, the equivalent to `capacity` is set to `int(datasize)`, whereas the `slice_input_producer` version uses `capacity=10`. This means that your `tf.contrib.data` version has to accumulate the entire input before producing the first record, whereas the `slice_input_producer` version accumulates only 10 records. The following program has more encouraging behavior:\r\n\r\n```python\r\nimport time\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops,dtypes\r\nfrom tensorflow.contrib.data import Dataset\r\nfrom tensorflow.contrib.data import Iterator\r\n\r\nrept = 1000\r\nfor datasize in [1e3, 1e6, 1e7, 1e8]:\r\n  with tf.Graph().as_default():\r\n    data = list(range(int(datasize)))\r\n    data_tensor = ops.convert_to_tensor(data)\r\n    dataset = Dataset.from_tensor_slices(data_tensor)\r\n    #dataset = Dataset.range(int(datasize))\r\n    dataset = dataset.shuffle(10)  #int(datasize))\r\n    iterator = Iterator.from_structure(dataset.output_types,\r\n                                       dataset.output_shapes)\r\n    init_op = iterator.make_initializer(dataset)\r\n    next_element = iterator.get_next()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(init_op)\r\n        start_time = time.time()\r\n        for r in range(rept):\r\n            x = sess.run(next_element)\r\n        print('Datalength: %i  ---  time = %.3f ms/sample'%(\r\n        datasize, (time.time()-start_time)/rept*1000))\r\n```\r\n\r\n> Datalength: 1000  ---  time = 0.123 ms/sample\r\n> Datalength: 1000000  ---  time = 0.109 ms/sample\r\n> Datalength: 10000000  ---  time = 0.102 ms/sample\r\n> Datalength: 100000000  ---  time = 0.714 ms/sample\r\n\r\nI suspect the problem actually stems from this line: \r\n\r\n```python\r\n    data_tensor = ops.convert_to_tensor(data)\r\n```\r\n\r\nEmbedding large tensors in the graph is slow, and you pay a noticeable and increasing penalty for both the `ops.convert_to_tensor()` call and the first `sess.run()` call after converting the tensor. Essentially, the tensor is copied several times, serialized and deserialized. I think the `1e8` case is slower because my local machine was running out of memory due to all of the copies!\r\n\r\nWe haven't historically used random sampling with replacement in TensorFlow because it's tricky to scale it to datasets that are larger than memory. However, it would certainly be possible to add a `Dataset` that implemented it.", "Maybe a better question - how should I be dealing with large datasets then? For instance, with 1e7 or 1e8 filenames, how do I sample from that if I shouldn't put the filenames into a tensor?", "That's a lot of filenames! Perhaps you could write them to an index file, one filename per line, and use `TextLineDataset(index_filename).flat_map(lambda filename: ...)` where `...` is the appropriate reading and parsing logic for a single file.", "Closing since a solution to your last question has been provided.", "@mrry What do you mean by index file? What format should I be writing the filenames/labels to for `TextLineDataset`?", "Hello.\r\n\r\nI experience the same behavior as @jrbtaylor on a much smaller dataset already (the MNIST data, 60K training examples).\r\nShuffling progressively slows down the learning iterations, up until the end of an epoch, then it continues at the initial speed again:\r\n\r\n- No shuffling:\r\n\r\n> Elapsed time for 100 iterations: 0.7071466445922852\r\nElapsed time for 100 iterations: 0.928595781326294\r\nElapsed time for 100 iterations: 0.9202249050140381\r\nElapsed time for 100 iterations: 0.9153792858123779\r\nElapsed time for 100 iterations: 0.9065940380096436\r\nElapsed time for 100 iterations: 0.9027049541473389\r\nEPOCH++\r\nElapsed time for 100 iterations: 0.903191328048706\r\nElapsed time for 100 iterations: 0.9209485054016113\r\nElapsed time for 100 iterations: 0.9005069732666016\r\nElapsed time for 100 iterations: 0.9149377346038818\r\nElapsed time for 100 iterations: 0.9105048179626465\r\nElapsed time for 100 iterations: 0.8968446254730225\r\nEPOCH++\r\nElapsed time for 100 iterations: 0.899205207824707\r\nElapsed time for 100 iterations: 0.9078717231750488\r\nElapsed time for 100 iterations: 0.9038975238800049\r\nElapsed time for 100 iterations: 0.9130148887634277\r\nElapsed time for 100 iterations: 0.9119799137115479\r\nElapsed time for 100 iterations: 0.9025707244873047\r\n\r\n- With shuffling\r\n> Elapsed time for 100 iterations: 0.756305456161499\r\nElapsed time for 100 iterations: 0.9620733261108398\r\nElapsed time for 100 iterations: 1.1773226261138916\r\nElapsed time for 100 iterations: 2.098752498626709\r\nElapsed time for 100 iterations: 3.113154411315918\r\nElapsed time for 100 iterations: 3.66085147857666\r\nEPOCH++\r\nElapsed time for 100 iterations: 1.9388504028320312\r\nElapsed time for 100 iterations: 0.9970040321350098\r\nElapsed time for 100 iterations: 1.2153730392456055\r\nElapsed time for 100 iterations: 2.1299166679382324\r\nElapsed time for 100 iterations: 3.369262933731079\r\nElapsed time for 100 iterations: 3.6159825325012207\r\nEPOCH++\r\nElapsed time for 100 iterations: 1.98651123046875\r\nElapsed time for 100 iterations: 1.0247056484222412\r\nElapsed time for 100 iterations: 1.1780986785888672\r\nElapsed time for 100 iterations: 1.7233843803405762\r\nElapsed time for 100 iterations: 3.0474486351013184\r\nElapsed time for 100 iterations: 3.7090988159179688\r\n\r\nI tested this both with a single-epoch dataset of which I capture the tf.errors.OutOfRangeError  followed by re-initilaizing the dataset, and with a dataset.repeat().\r\n\r\nSo my question now essentially is: **what is happening inside a dataset when shuffling that makes it so much slower?** (apologies if that's in the documentation and I missed it).\r\nOr **what am I doing wrong in its usage?**\r\n\r\nMy dataset pipeline is similar to @jrbtaylor's, I tested with the MNIST data with a batch size of 100.\r\n\r\n> mnist_imgs = tf.constant(img_filenames) # img_filenames is a python list of strings\r\n   mnist_labels = tf.constant(labels)\r\n   dataset = tf.data.Dataset.from_tensor_slices((mnist_imgs, mnist_labels))\r\n   dataset = dataset.shuffle(epoch_size) # OPTIONAL\r\n   dataset = dataset.repeat(nb_epochs_to_do)\r\n   dataset = dataset.map(_parse_function,num_parallel_calls=FLAGS.num_parallel_cores) # where _parse_function reads and decodes the image files\r\n   dataset = dataset.batch(FLAGS.batch_size)\r\n   dataset = dataset.prefetch(1)\r\n   iterator = dataset.make_initializable_iterator()\r\n\r\n\r\nThank you.\r\n\r\nKenneth", "@kvanhoey Please open a new issue about this, and include as many details as possible to reproduce the problem.", "Thanks. Just did that: https://github.com/tensorflow/tensorflow/issues/17958"]}, {"number": 11590, "title": "Branch 162404670", "body": "", "comments": []}, {"number": 11589, "title": "[beginner][bug] TensorFlow doesn't seem to be decomposing GradientDescent into XLA ops", "body": "### System information\r\n- **Tensorflow compiled on a branch of a fork**: https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops. Contains extra LOG(INFO) and std::cout statements to notify the names of the functions being called.\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.2.0-1878-ga5066f6', '1.2.1')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: release 0.5.2\r\n- **CUDA/cuDNN version**: CUDA 8\r\n- **GPU model and memory**: NVIDIA Quadro K1200,  4 GB\r\n- **Exact command to reproduce**: python <tflow_parent>/tensorflow/tensorflow/example/tutorials/mnist/mnist/mnist_softmax_xla.py\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1157069/tf_env.txt)\r\n\r\n### Context\r\nMy work involves securing Tensorflow's computations. The approach I've planned to take is to add XLA ops during the conversion of TF ops to XLA ops,  which are lowered into functions calls (in LLVM-IR) that send data to and receive data from a secure environment.\r\n\r\nI'm currently verifying if both training and inference TF Ops are lowered to XLA ops.\r\n\r\n### The Problem\r\nI've made some [changes to the code](https://github.com/singam-sanjay/tensorflow/tree/trace_learning_XLA_ops) to highlight when some OpKernels and XlaOpKernels are being invoked / lowered. I've used the mnist_softmax_xla.py example to observe tensorflow's behaviour.\r\n\r\nI found that \"ApplyGradientDescentGPU\" was being called 2000 times (2 layer network and 1000 iterations, clearly not JIT) instead of ResourceApplyGradientDescent, yet other Ops including MatMul and subtraction seem to have been optimized by XLA.\r\n\r\nWhy is gradient descent not optimized by XLA ?\r\n\r\n### Source code / logs\r\nThe output text lost its structure when I pasted it. So, here are the screenshots.\r\n![github 1](https://user-images.githubusercontent.com/6310523/28340192-80abeec2-6c2c-11e7-8fb3-d3ec6ae1676e.PNG)\r\n![github 2](https://user-images.githubusercontent.com/6310523/28340198-8593c450-6c2c-11e7-9c62-dd06e93e2b5a.PNG)\r\n![github 3](https://user-images.githubusercontent.com/6310523/28340204-8909b69e-6c2c-11e7-9e16-afe089ccde8e.PNG)\r\n![github 4](https://user-images.githubusercontent.com/6310523/28340210-8c3c4a02-6c2c-11e7-93b1-a416035b3ed8.PNG) Note that the first 2 kernels have been called a 100 times each.\r\n\r\n### Related\r\n Also, it would be helpful if someone could answer [Tensow - XLA | Passing tensors to external functions at runtime](https://stackoverflow.com/questions/45146444/tensorflow-xla-how-are-tf-ops-lowered-to-xla-for-training) , as it is **the** important assumption of my approach that Tensorflow can allow such XLA ops. I appreciate any information on its feasibility as I could start on a different approach as soon as possible.\r\n\r\nThanks,\r\nSanjay", "comments": ["@tatatodd Any thoughts ?", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly."]}, {"number": 11588, "title": "GPU kernel for SVD", "body": "### System information\r\n-Linux Ubuntu 16.04\r\n- TensorFlow installed from (source )\r\n- python v3.5 \r\n- **GPU 8gb**:\r\n\r\n### Describe the problem\r\nI tried to run code that used tf.svd on GPU but it gives me an error. Is it correct that tensorflow does not have GPU kernels for SVD?\r\n\r\n### Source code / logs\r\n[[Node: Svd = Svd[T=DT_FLOAT, compute_uv=false, full_matrices=false](x)]]\r\n[[Node: map_1/while/nuclear_norm_09660f0e_1 = nuclear_norm_09660f0e[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](map_1/while/Reshape_1, ^map_1/while/Identity)]]\r\n[[Node: Adagrad_1/update/_64 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_690_Adagrad_1/update\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]", "comments": ["Nope, currently there is no GPU kernel registered for SVD. \r\n\r\nI don't believe anyone is actively working on this, in which case contributions will be welcome.\r\n@rmlarsen might want to chime in with more detail.\r\n", "Ok\r\nis it possible to run  part of the loss function on CPU (the SVD part) and the other parts on GPU\r\nI tried with tf.device it still gives me an error ", "It would be tough as there are no such pipelines created ", "@rana-alshaikh : Not entirely sure what you mean, but yes, it is possible to place individual operations on specific devices. For details see https://www.tensorflow.org/tutorials/using_gpu#manual_device_placement\r\n\r\nIf you're running into trouble, I'd suggest posting on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow).", "@asimshankar What he is telling is he wants to run some only loss funciton on CPU and later run other training things on GPU which I am telling is not possible, you are telling usage of multiple GPU he is asking multiple CPU and GPU at the same time", "I might be misunderstanding, but why can't multiple devices be mixed? For example:\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\n\r\nwith tf.device('/gpu:0'):\r\n  # All these operations will run on GPU\r\n  x = tf.constant(1.0, name='X')\r\n  y = tf.add(x, x, name='Y')\r\nwith tf.device('/cpu:0'):\r\n  # This will copy the result of `y` to CPU and execute this multiply on CPU\r\n  z = tf.multiply(y, 2, name='Z')\r\n\r\n# Log device placement to see where ops are run\r\nwith tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\r\n  print(sess.run(z))\r\n```\r\n\r\nWill result in the following:\r\n\r\n```\r\nY: (Add): /job:localhost/replica:0/task:0/gpu:0\r\nZ: (Mul): /job:localhost/replica:0/task:0/cpu:0\r\nX: (Const): /job:localhost/replica:0/task:0/gpu:0\r\n4.0\r\n```", "@asimshankar Wow cool, sorry I was not aware  :) And can I do this in parallel the same thing that you did above instead of sequential for two different functions ? For example in GANs, I will run in parallel i.e the generator loss in CPU and discriminator loss in GPU. Can this be done as well ? ", "GPU for SVD seems done now? https://github.com/tensorflow/tensorflow/issues/13603\r\n", "Closing this since the feature is added"]}, {"number": 11587, "title": "Branch 162383623", "body": "", "comments": ["Need to disable some tests on mac first."]}, {"number": 11586, "title": "read_up() for slim parallel reader #11164", "body": "", "comments": ["Can one of the admins verify this patch?", "@sguada looks to be OOO at a conference this week, so this will have to wait until he gets back, I think.", "@tensorflow-jenkins test this please", "The failing TF Eager mode test is unrelated. Merging."]}, {"number": 11585, "title": "Keras API reproducibility", "body": "If any new option was added to ensure a reproducible results by setting some seed parameter\r\n?\r\nIn Keras the issue was discussed [here](https://github.com/fchollet/keras/issues/2743)\r\nAccording the issue the instability in results is due to weights random initialization.", "comments": ["It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 11584, "title": "Revert commits 1.3.0rc0", "body": "Reverting non_max_suppression_v2 op and @caisq tfdbg change for cloudml-gpu build.", "comments": ["The revert for non_max_suppression has more conflicts than expected. Pushing it back to rc1.", "Jenkins, test this please.", "http://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/5894/"]}, {"number": 11583, "title": "tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)", "body": "I try to run the Sequence-to-Sequence Models with tensorflow, but when I run the training set, I have the problem like this\r\n\r\n> Traceback (most recent call last):\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1022, in _do_call\r\n>     return fn(*args)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1004, in _run_fn\r\n>     status, run_metadata)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/contextlib.py\", line 89, in __exit__\r\n>     next(self.gen)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n>     pywrap_tensorflow.TF_GetCode(status))\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)\r\n> \t [[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"translate.py\", line 322, in <module>\r\n>     tf.app.run()\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"translate.py\", line 319, in main\r\n>     train()\r\n>   File \"translate.py\", line 210, in train\r\n>     target_weights, bucket_id, False)\r\n>   File \"/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py\", line 251, in step\r\n>     outputs = session.run(output_feed, input_feed)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n>     run_metadata_ptr)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n>     feed_dict_string, options, run_metadata)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n>     target_list, options, run_metadata)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[49] = 60000 is not in [0, 60000)\r\n> \t [[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]\r\n> \r\n> Caused by op 'model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15', defined at:\r\n>   File \"translate.py\", line 322, in <module>\r\n>     tf.app.run()\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n>     _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n>   File \"translate.py\", line 319, in main\r\n>     train()\r\n>   File \"translate.py\", line 178, in train\r\n>     model = create_model(sess, False)\r\n>   File \"translate.py\", line 136, in create_model\r\n>     dtype=dtype)\r\n>   File \"/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py\", line 179, in __init__\r\n>     softmax_loss_function=softmax_loss_function)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 1180, in model_with_buckets\r\n>     decoder_inputs[:bucket[1]])\r\n>   File \"/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py\", line 178, in <lambda>\r\n>     lambda x, y: seq2seq_f(x, y, False),\r\n>   File \"/Users/loohaze/Documents/models/tutorials/rnn/translate/seq2seq_model.py\", line 142, in seq2seq_f\r\n>     dtype=dtype)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 876, in embedding_attention_seq2seq\r\n>     initial_state_attention=initial_state_attention)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 772, in embedding_attention_decoder\r\n>     embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py\", line 772, in <listcomp>\r\n>     embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 111, in embedding_lookup\r\n>     validate_indices=validate_indices)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1359, in gather\r\n>     validate_indices=validate_indices, name=name)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n>     op_def=op_def)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\r\n>     original_op=self._default_original_op, op_def=op_def)\r\n>   File \"/Users/loohaze/anaconda/envs/python3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\r\n>     self._traceback = _extract_stack()\r\n> \r\n> InvalidArgumentError (see above for traceback): indices[49] = 60000 is not in [0, 60000)\r\n> \t [[Node: model_with_buckets/embedding_attention_seq2seq_2/embedding_attention_decoder/embedding_lookup_15 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@embedding_attention_seq2seq/embedding_attention_decoder/embedding\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](embedding_attention_seq2seq/embedding_attention_decoder/embedding/read, _recv_decoder15_0)]]\r\n> \r\n\r\nIt seems the problem happens randomly when running training set. It means when I have this problem,I can continue and this may not happen in next steps.\r\nI checked this problem on previous issues, but I am sure that I have set the parameter of vocabulary size\r\nHere are my input commands:\r\n\r\n> python translate.py --data_dir /private/tmp/ch-en  --train_dir /private/tmp/ch-en/train_result --size=256 --num_layers=2 --steps_per_checkpoint=200 --from_vocab_size=60000 --to_vocab_size=60000\r\n\r\nSo how to solve this problem?\r\n", "comments": ["Can you see how the embedding is created and with what size?", "@drpngx  sorry,I'm new to this model. Do you mean the translation model?  like `Creating 2 layers of 256 units` ? \r\nI have decreased the value of steps_per_checkpoint to 50, and now it's running smoothly.\r\n\r\n", "@Loohaze the embedding op is created by the seq2seq file. Could you check what embedding size is passed to it?", "@drpngx  I checked the embedding_size's value is 256. the function embedding_attention_seq2seq is called and the parameters are \r\n```\r\nencoder_inputs,   <class 'list'>\r\ndecoder_inputs,   <class 'list'>\r\ncell,   cell:<tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object>\r\nnum_encoder_symbols,   60000\r\nnum_decoder_symbols,   60000\r\nembedding_size,   256\r\nnum_heads=1,   1\r\noutput_projection=None,    \r\nfeed_previous=False,    False\r\ndtype=None,    float32\r\nscope=None,   None\r\ninitial_state_attention=False   False\r\n```", "It looks like the input that's fed to the embedding is bigger than the vocabulary size. How are you feeding the input?", "@drpngx  I don't modify the source code of feed for inputs\r\n```\r\n# Feeds for inputs.\r\n    self.encoder_inputs = []\r\n    self.decoder_inputs = []\r\n    self.target_weights = []\r\n    for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\r\n      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                name=\"encoder{0}\".format(i)))\r\n    for i in xrange(buckets[-1][1] + 1):\r\n      self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\r\n                                                name=\"decoder{0}\".format(i)))\r\n      self.target_weights.append(tf.placeholder(dtype, shape=[None],\r\n                                                name=\"weight{0}\".format(i)))\r\n\r\n```", "Automatically closing due to lack of recent activity. This also looks like a support issue, where further discussion would be better suited to [Stack Overflow](http://stackoverflow.com/questions/tagged/tensorflow). Thanks!"]}, {"number": 11582, "title": "Issue while using AttentionWrapper.", "body": "I am getting issue related to miss match of state and output. But I am unable to figure the issue.\r\nIt would be really appreciated if someone can guide me. Thanks in advance.\r\nI am using tensorfow-gpu==1.2.1, with 1080 Ti graphics.\r\n\r\nError is as below:\r\nValueError: Shapes (8, 522) and (8, 512) are incompatible\r\n\r\nError occurs in the file \"attention_wrapper.py\" in the method named \"call\" at line 708\r\n\r\ncell_output, next_cell_state = self._cell(cell_inputs, cell_state)\r\n\r\nI was able to figure out that it is adding the attention_size to the shape and so there is a mismatch.\r\nBut I have no idea how to fix it.\r\nThe code is as below, hyper-parameters are declared as below (test purpose).\r\n`\r\nbatch_size= 8\r\nnumber_of_units_per_layer= 512\r\nnumber_of_layers = 3\r\nattn_size= 10\r\ndef build_decoder_cell(enc_output, enc_state, source_sequence_length, attn_size, batch_size):\r\n\r\n    encoder_outputs = enc_output\r\n    encoder_last_state = enc_state\r\n    encoder_inputs_length = source_sequence_length\r\n\r\n    attention_mechanism = attention_wrapper.LuongAttention(\r\n            num_units=attn_size, memory=encoder_outputs,\r\n            memory_sequence_length=encoder_inputs_length,\r\n            scale=True,\r\n            name='LuongAttention' )\r\n\r\n    # Building decoder_cell\r\n    decoder_cell_list = [\r\n        build_single_cell() for i in range(num_layers)]\r\n\r\n    decoder_initial_state = encoder_last_state\r\n\r\n    def attn_decoder_input_fn(inputs, attention):\r\n        #if not self.attn_input_feeding:\r\n        #    return inputs\r\n\r\n        # Essential when use_residual=True\r\n        _input_layer = Dense(size, dtype=tf.float32,\r\n                            name='attn_input_feeding')\r\n        return _input_layer(array_ops.concat([inputs, attention], -1))\r\n\r\n\r\n    # AttentionWrapper wraps RNNCell with the attention_mechanism\r\n    # Note: We implement Attention mechanism only on the top decoder layer\r\n    decoder_cell_list[-1] = attention_wrapper.AttentionWrapper(\r\n        cell=decoder_cell_list[-1],\r\n        attention_mechanism=attention_mechanism,\r\n        attention_layer_size=attn_size,\r\n        #cell_input_fn=attn_decoder_input_fn,\r\n        initial_cell_state=encoder_last_state[-1],\r\n        alignment_history=False,\r\n        name='Attention_Wrapper')\r\n\r\n    # To be compatible with AttentionWrapper, the encoder last state\r\n    # of the top layer should be converted into the AttentionWrapperState form\r\n    # We can easily do this by calling AttentionWrapper.zero_state\r\n\r\n    # Also if beamsearch decoding is used, the batch_size argument in .zero_state\r\n    # should be ${decoder_beam_width} times to the origianl batch_size\r\n    #batch_size = self.batch_size if not self.use_beamsearch_decode \\\r\n    #    else self.batch_size * self.beam_width\r\n    initial_state = [state for state in encoder_last_state]\r\n\r\n    initial_state[-1] = decoder_cell_list[-1].zero_state(\r\n        batch_size=batch_size, dtype=tf.float32)\r\n    decoder_initial_state = tuple(initial_state)\r\n\r\n    return tf.contrib.rnn.MultiRNNCell(decoder_cell_list), decoder_initial_state`\r\n\r\nThank you once again.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 11581, "title": "Model average replicas optimizer", "body": "We have implemented a new replicas_optimizer \"ModelAverageOptimizer\" to reduce cross-node communication cost. It is mentioned in the following issue:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/10449\r\n\r\n# Model Average\r\n\r\nIn a typical synchronous training environment (N-replica synchronous training), gradients will be averaged each step, and then applied to the variables, after that replicas can fetch the new variables and continue. However, in a model-average training environment, model parameters will be averaged every 'ma_intervals' steps. In the interval between two \"average operation\", each worker trained its local model, there are no data transfer at all between workers or ps, which can significantly accerlate parallel training.\r\n\r\nReference:  https://arxiv.org/pdf/1410.7455v8.pdf\r\n\r\n\r\n## BMUF\r\n\r\nReference:  http://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/0005880.pdf\r\n\r\nBMUF brings in 'Momentum' and can also work with a Nesterov momentum scheme on the based of Model Average method.\r\n\r\nblock\\_momentum\\_rate: It brings in the historical blockwise gradients. The block momentum is usually set according to the number of workers: block\\_momentum = 1.0 - 1.0/num_of_workers. The default value is 0.0. When using default value, the naive ModelAverage method is applied, the original learning rate of local optimizer should be multiply by num_of_workers. While when the value is (0.0,1.0), the BMUF method is applied, the learning rate of local optimizer can be unchanged.\r\n\r\nuse\\_Nesterov: means the Nesterov-style momentum update is applied on the block level. The default value is true. This can accelerate training with non-zero block_momentum_rate.\r\n\r\nblock\\_learning\\_rate: block\\_learning\\_rate is always 1.0 or slightly higher than 1.0.\r\n\r\n```python             \r\n  def __init__(self,\r\n               replicas_to_aggregate,                                                                 \r\n               ma_intervals,\r\n               total_num_replicas=None,\r\n               block_momentum_rate=0.0,\r\n               use_Nesterov=True,\r\n               block_learning_rate=1.0):\r\n    \"\"\"Construct a model_average optimizer.\r\n```\r\n\r\n# Result\r\nWe have benchmarked it on several in-house models, the results showed a good convergence speedup and training-data processing almostly reaches linear speedup.\r\n\r\n| device config (GPU M40)\t| convergence speed-up | computation speed-up |\r\n|---|---|---|\r\n|4 GPUs within 2 nodes|3.4|3.9|\r\n|8 GPUs within 4 nodes|5.7|7.5|\r\n|16 GPUs within 8 nodes|9.2|14.6|\r\n\r\nAlso\uff0cwe made a experiment with ResNet on cifar10 (baseline code is from: https://github.com/tensorflow/models/tree/master/resnet):\r\n\r\n| device config (GPU P100)\t| convergence speed-up |\r\n|---|---|\r\n|2 GPUs within 1 nodes|2.03|\r\n|4 GPUs within 1 nodes|3.67|\r\n|8 GPUs within 1 nodes|6.57|\r\n\r\n# API\r\nBy now, it is implemented as a tensorflow API using Python\uff0c doesn't have to change the core/distributed runtime code.\r\n\r\n\r\n```python  \r\n  # Create any optimizer to update the variables, say a simple SGD:\r\n  opt = GradientDescentOptimizer(learning_rate=0.1)\r\n             \r\n  # Create a ModelAverageOptimizer to update the global variables:\r\n  ma = tf.contrib.model_average.ModelAverageOptimizer(replicas_to_aggregate=50, \r\n                                                      ma_intervals=100)\r\n             \r\n  # create the hook which handles model average operations.\r\n  ma_replicas_hook = ma.make_ma_run_hook()\r\n  # And also, create the hook which handles initialization and queues.\r\n  ma_hook = ma.make_session_run_hook(is_chief)\r\n```\r\n  \r\n\r\nThanks.", "comments": ["Can one of the admins verify this patch?", "Derek, any opinion about whether to merge this in contrib or leave it outside of TF?", "It seems like a reasonable peer to the DCASGD contribution in `tf.contrib.opt` (added in PR #9551). I'm not going to be able to review it for at least two weeks though, so somebody else should have a look at the details.", "@mrry Could you please assign this code review to some of your colleagues? We have carefully tested this code merge and implemented it wishing others could also benefit from this PR.\r\n\r\nThanks", "@jhseu Any suggestions for assigning another reviewer?", "@vrv since you reviewed #9551, mind reviewing this one as well? (or reassign)", "@abenmao Could you please provide the ResNet experiment result we make locally? Also I think the experimental code could be provided so that others could easily re-produce our experiment.", "@ry we made a experiment with ResNet classifier on cifar10. The speedup is already added in the above \"Result\" part. The experimental code will be provided soon. @yangjunpro ", "@abenmao @yangjunpro Experiment code likely won't help me, as I need to interface with our internal systems to test. I will hopefully have some results in a few days.\r\n\r\nRegarding the `model_average_device_setter`: Is it possible to share more code with the existing implementation? Or just use the `ps_strategy` and `ps_ops` arguments of the existing?\r\n\r\nRegarding the tests in `tensorflow/contrib/model_average/tests/simple`: Can you restructure the tests to use `tensorflow.python.platform.test`? Maybe follow some of this code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/sync_replicas_optimizer_test.py \r\n or https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/delay_compensated_gradient_descent_test.py \r\n\r\nRegarding the location of this code: Since this is an optimizer, it might make sense to move this to `tensorflow/tensorflow/contrib/opt/python/training`. What do you think?", "Hi, am new in tensorflow. Recently we use your code for distributed training in a model average fashion. But we found an error like cannot declare MODEL_VARIALBES when calling an 'ModelAverageOptimizer! Please using ' 'GLOBAL_VARIABLES, LOCAL_VARIABLES or '                        'TRAINAVLE_VARIALBES instead. \r\nWe are confused about the different variable types, could you please drop  us a link for the detail?\r\nAdditionally, do we need to set the variables to be averaged as the local variable? \r\nThx.", "@zhouh Do you have a variable with collections = [ops.GraphKeys.MODEL_VARIABLES] ?  Can you remove that ? ref:  https://www.tensorflow.org/api_docs/python/tf/GraphKeys", "@abenmao Thank you for your quick reply. But am  still confused about the variable type.\r\nAccording to the ref (https://www.tensorflow.org/api_docs/python/tf/GraphKeys), the MODEL_VARIABLE is just a subset of GLOBAL_VARIABLE, and in such case, we can only use LOCAL_VARIABLE instead?  Actually we only use the default setting of get_variable(), it's confused for us to capture the MODEL_VARIABLE  exception.", "@zhouh This error message is triggered by \"if len(tf.model_variables()) != 0\".\r\n get_variables() is default to GLOBAL_VARIABLES, which is OK.", "@abenmao @yangjunpro (cc @zhouh) I'm also hitting the MODEL_VARIALBES issue. It seems ModelAverageOptimizer assumes there's not any MODEL_VARIALBES, but that's a bad assumption. For example the batch_norm implementation distributed with TF creates MODEL_VARIALBES: https://github.com/tensorflow/tensorflow/blob/5e5453099ad4a720172b352e8436e961d9d4235e/tensorflow/contrib/layers/python/layers/layers.py#L704\r\n\r\nHave you considered using the \"slot\" mechanism of Optimizer to store local variables?", "@ry  @yangjunpro \r\nRegarding the model_average_device_setter:  Used the ps_strategy in replica_device_setter\r\n\r\nRegarding the tests in tensorflow/contrib/model_average/tests/simple:  remove this and add two tests by using tensorflow.python.platform.test .\r\n\r\nRegarding the location of this code: already moved to tensorflow/tensorflow/contrib/opt/python/training. \r\n\r\nThe \"MODEL_VARIALBES assumption\"  is fixed. ", "Thanks for fixing this. Another issue is that when the global variable set includes variables with type as int32, it may cause an error:\r\n\r\n```\r\n\r\n  File \"/data01/home/zhouhao.nlp/workspace/nmt/src/seq2seq/seq2seq_runner.py\", line 304, in distributed_run\r\n    ma_hook = ma.make_ma_run_hook()\r\n  File \"/data01/home/zhouhao.nlp/workspace/nmt/src/seq2seq/utils/model_average/python/model_average_optimizer.py\", line 453, in make_ma_run_hook\r\n    global_step=self._super_global_step)\r\n  File \"/data01/home/zhouhao.nlp/workspace/nmt/src/seq2seq/utils/model_average/python/model_average_optimizer.py\", line 307, in apply_model_average\r\n    block_grad = math_ops.multiply(self._block_learning_rate, gk_new)\r\n  File \"/data01/home/zhaochengqi/tf11/local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 286, in multiply\r\n    return gen_math_ops._mul(x, y, name)\r\n  File \"/data01/home/zhaochengqi/tf11/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1377, in _mul\r\n    result = _op_def_lib.apply_op(\"Mul\", x=x, y=y, name=name)\r\n  File \"/data01/home/zhaochengqi/tf11/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 526, in apply_op\r\n    inferred_from[input_arg.type_attr]))\r\nTypeError: Input 'y' of 'Mul' Op has type int32 that does not match type float32 of argument 'x'. \r\n```\r\nThis error occurs due to our global variable set includes some untrainable int32 variables such as the global_step, which will lead to exception in the process of averaging parameters. How about using the TRAINABLE_VARIABLES to replace the local and global variables for averaging?\r\n\r\nDo I mis-understand something?", "Sorry for the last comment, I find that you have used the trainable variables for averaging:\r\n`local_vars = variables.trainable_variables()` \r\nI forgot to set the global_step as untrainable.", "I'm getting this error trying to run this:\r\n\r\n```\r\nTop-level exception: Cannot colocate nodes 'modelAverage_nesterov313/cond_627/read/Switch_modelAverage_nesterov313_0' and 'modelAverage_g313: Cannot merge devices with incompatible jobs: '/job:ps/task:1' and '/job:worker'\r\n\t [[Node: modelAverage_nesterov313/cond_627/read/Switch_modelAverage_nesterov313_0 = Switch[T=DT_FLOAT, _class=[\"loc:@modelAverage_g313\"], _device=\"/job:worker\"](modelAverage_nesterov313/cond/Merge, cond_627/pred_id)]]\r\nCaused by op u'modelAverage_nesterov313/cond_627/read/Switch_modelAverage_nesterov313_0'\r\n```\r\n\r\nThis is with merge_devices set to either True or False.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "https://gist.github.com/ry/a869cbf66a68777b714343f42853df0a  Here is a test case which repeats the error I'm seeing.", "Thank you for your contribution. I have been using your code to accelerate my in-house neural machine translation system, but I find it does not converge. The loss stoped at 4.5, which should be 1.5 instead. Do you have any general idea for the issue? Here is my code.\r\n\r\n`\r\n\r\n\r\n def distributed_run(self, server, cluster, num_workers, gpu_options):\r\n\r\n        # begin model average distributed training\r\n\r\n        if self.job_name == 'ps':\r\n            server.join()\r\n        elif self.job_name == 'worker':\r\n            is_chief = (self.task_index == 0)\r\n\r\n            with tf.device(device_setter.model_average_device_setter(\r\n                    worker_device=\"/job:worker/task:%d/gpu:%d\" %\r\n                            (self.task_index, self.gpu_id), cluster=cluster)):\r\n\r\n\r\n                # Create model template function\r\n                model_template_builder = tf.make_template(\"\", self.seq2seqmodel.build, create_scope_now_=False)\r\n                source_ids, source_seq_length, target_ids, target_seq_length = self._build_input_fields()\r\n\r\n                seq2seq_info = model_template_builder(source_ids, source_seq_length, target_ids, target_seq_length)\r\n                loss = seq2seq_info.loss\r\n\r\n                loss = tf.reduce_mean(loss)\r\n\r\n                train_op, global_steps_tensor = create_train_op(self.opt_params, loss)\r\n\r\n                if self.do_evaluation and is_chief:\r\n                    tf.logging.info(\"Building evaluate model\")\r\n                    self.dev_evaluate_helper.build((source_ids, source_seq_length))\r\n\r\n                ma = model_average.ModelAverageOptimizer(4, 10)\r\n\r\n\r\n        # build the distributed operation\r\n        config = tf.ConfigProto(gpu_options=gpu_options)\r\n        config.log_device_placement = False\r\n        config.allow_soft_placement = True\r\n\r\n        ma_hook = ma.make_ma_run_hook()\r\n        ma_replicas_hook = ma.make_session_run_hook(is_chief)\r\n\r\n\r\n        save_checkpoints_steps = self.training_options['save_checkpoints_steps']\r\n        max_train_epochs = self.training_options['train_epochs']\r\n        max_train_steps = self.training_options['train_steps']\r\n        display_steps = self.training_options['display_steps']\r\n\r\n\r\n        #\r\n        #\r\n        #\r\n        # if is_chief:\r\n        #     merged_summary = tf.summary.merge_all()\r\n        #     summary_writer = tf.summary.FileWriter(os.path.join(self.output_dir, \"events\"), self.sess.graph)\r\n        #\r\n\r\n        self.distributed_sess = tf.train.MonitoredTrainingSession(master=server.target,\r\n                                                 is_chief=is_chief,\r\n                                                 checkpoint_dir=self.output_dir,\r\n                                                 save_checkpoint_secs=save_checkpoints_steps,\r\n                                                 save_summaries_steps=display_steps,\r\n                                                 hooks=[ma_replicas_hook, ma_hook],\r\n                                                 config=config)\r\n\r\n        train_data = TrainTextIterator(self.training_options['train_sources'],\r\n                                       self.training_options['train_targets'],\r\n                                       self.vocab_source,\r\n                                       self.vocab_target,\r\n                                       maxlen_src=self.training_options[\"source_max_seq_len\"],\r\n                                       maxlen_trg=self.training_options[\"target_max_seq_len\"],\r\n                                       batch_size=self.training_options['batch_size'])\r\n\r\n\r\n        # not need saver now\r\n        # saver = tf.train.Saver()\r\n        # model_out = os.path.join(self.output_dir, \"model-ckpt\")\r\n\r\n        # for the distributed training process, the MonitoredTrainingSession\r\n        # can help initialization and restore automatically\r\n        #\r\n        # # reload\r\n        # checkpoint_path = tf.train.latest_checkpoint()\r\n        # if checkpoint_path:\r\n        #     tf.logging.info(\"reloading models...\")\r\n        #     saver.restore(self.sess, checkpoint_path)\r\n        # else:\r\n        #     tf.logging.info(\"initializing model parameters...\")\r\n        #     self.sess.run(tf.global_variables_initializer())\r\n        # dump model details \"model_analysis.txt\"\r\n        #\r\n        # if  is_chief:\r\n        #     dump_model_analysis(self.output_dir)\r\n        #\r\n\r\n        # self.sess.graph.finalize()\r\n\r\n        estop = False\r\n        ud_accumulate = 0.\r\n\r\n        for eidx in range(max_train_epochs):\r\n\r\n            tf.logging.info(\"Epooch: %d\" % eidx)\r\n\r\n            if self.distributed_sess.should_stop():\r\n                break\r\n\r\n            for (x, lengths_x), (y, lengths_y) in train_data:\r\n                ud_start = time.time()\r\n                # update\r\n\r\n\r\n                _, global_step, training_loss \\\r\n                    = self.distributed_sess.run([train_op, global_steps_tensor, loss],\r\n                                    feed_dict={source_ids:x, source_seq_length:lengths_x,\r\n                                               target_ids:y, target_seq_length:lengths_y})\r\n                ud_accumulate += time.time() - ud_start\r\n\r\n                # display training loss\r\n                if global_step % display_steps == 0 and is_chief:\r\n                    tf.logging.info(\"Epoch %d  Update %d\\tLoss=%f   UD %f s/update\" \\\r\n                                    % (eidx, global_step, training_loss,\r\n                                       ud_accumulate / display_steps))\r\n                    # summary_writer.add_summary(summary, global_step)\r\n                    ud_accumulate = 0.\r\n\r\n                # greedy inference for developing\r\n                if global_step % save_checkpoints_steps == 0 and is_chief:\r\n                # if  is_chief:\r\n\r\n                    tf.logging.info(\"Saving models... global step=%d\" % global_step)\r\n                    # saver.save(self.sess, model_out, global_step=global_step)\r\n                    # evaluate model with BLEU score\r\n                    if self.do_evaluation and self.eval_options[\"start_eval_at\"] <= global_step:\r\n                        tf.logging.info(\"Evaluating with BLEU score......\")\r\n                        eval_start_time = time.time()\r\n                        samples, bleu = self.dev_evaluate_helper.evaluate(self.sess,\r\n                                                                          global_step)\r\n                        tf.logging.info(\"Evaluating DEVSET: BLEU=%f  GlobalStep=%s   UD %.2f\"\r\n                                        % (bleu, global_step, time.time()-eval_start_time))\r\n                        for idx, (s, p) in enumerate(zip(*samples)):\r\n                            tf.logging.info(\"Sample%d Source: %s\" % (idx, s))\r\n                            tf.logging.info(\"Sample%d Prediction: %s\\n\" % (idx, p))\r\n                if global_step > max_train_steps:\r\n                    estop = True\r\n                    break\r\n            if estop:\r\n                tf.logging.info(\"Trained maximum steps: %d\" % max_train_steps)\r\n                break\r\n        #saver.save(self.sess, model_out, global_step=global_step)\r\n`", "@ry I modified your test case above and it's OK to run on my machine. I think maybe you should use a MonitoredSession rather than a general Session. https://gist.github.com/abenmao/fdc7205ea55929c8084d9d024421fe5b\r\nHere is the modified test case code. If you can run it ?", "@abenmao This might be due to some differences between our internal TF and the public one. I don't think MonitoredSession should be necessary to run the train op - only that the queue runners are started.\r\n\r\nI've modified my test case and tested it on the open source version - I get a different error but it's still failing. Can you add this to your to your branch?  https://github.com/ry/tensorflow/commit/764cd38b6407421e4fe23cf8cd9d2518ef043c8d\r\n\r\nI haven't tried your example yet - I'll get back to you on that soon.", "@zhouu \r\nYou can try to set \"use_nesterov=Flase\". \r\nIn my model average test, I found BMUF may not converge in CNN. ", "Hi @abenmao,\r\nI have looked at your implementation, which helps me a lot to develop my optimizer. But I have some questions:\r\n1) If I set global_step variable with your model_average_device_setter, the variable will be re-set to worker device, right?\r\n2) Do you think it is import or not to get global training loss/accuracy in training process? Since we can not get those through your implementation.\r\n3) Did you test the convergence using model average with momentum in CNN based model? Since MA with momentum performed badly in my test.", "@jinxin0924 Thank you very much. I will try it as you said:)", "@zhouh Any chance to push new changes?", "@zhouh ping", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "CLAs look good, thanks!\n\n<!-- ok -->", "No, thanks. I just use the code for training my neural systems. Thanks again for the nice work.", "@abenmao What are the next steps here? This still fails? Should we close this PR or are you interested in fixing the issues noted above?", "Closing since it looks like this isn't going to get done soon.  We can reopen with a new PR if someone is interested.", "@ry @abenmao \r\nI still have problems with `model_average_model`\r\n\r\nI use `tensorflow_gpu==1.8.0` in `anaconda2.5`, `model_average_optimizer_test.py` can run easily, however some errors occured with my own code.\r\n\r\nAlso, I use `tensorflow.contrib.opt.python.training.model_average_optimizer`, other errors comes: [https://github.com/tensorflow/tensorflow/issues/19617](url)\r\n\r\nerrors are shown below.\r\n\r\n\tInvalidArgumentError (see above for traceback): Cannot colocate nodes 'modelAverage/nesterov/v1/cond_3/read/Switch_modelAverage/nesterov/v1_0' and 'modelAverage/g/v1: Cannot merge devices with incompatible jobs: '/job:ps/task:0' and '/job:worker/task:1'\r\n\t[[Node: modelAverage/nesterov/v1/cond_3/read/Switch_modelAverage/nesterov/v1_0 = Switch[T=DT_FLOAT, _class=[\"loc:@modelAverage/g/v1\"], _device=\"/job:worker/task:1\"](modelAverage/nesterov/v1/cond_1/Merge, cond_3/pred_id)]]\r\n\r\nThe run commad is\r\n\r\n\t~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=0\r\n\t~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=worker --task_id=1\r\n\t~/anaconda2/bin/python test.py --server_hosts=localhost:12222 --worker_hosts=localhost:12223,localhost:12224 --job_name=server --task_id=0\r\n\r\nand code is \r\n\r\n    import os\r\n    import time\r\n     \r\n    import json\r\n    import copy\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    from tensorflow.python.framework import constant_op\r\n    import model_average_optimizer\r\n     \r\n    flags = tf.flags\r\n    flags.DEFINE_string(\"server_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n    flags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\n    flags.DEFINE_string(\"job_name\", \"\", \"Either 'server' of 'worker'\")\r\n    flags.DEFINE_integer(\"task_id\", 0, \"Task Id for Each workers\")\r\n     \r\n    FLAGS = flags.FLAGS\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n     \r\n    def workers_ps_creator(args):\r\n        ps_hosts = args.server_hosts.split(\",\")\r\n        worker_hosts = args.worker_hosts.split(\",\")\r\n        num_workers = len(worker_hosts)\r\n     \r\n        cluster = tf.train.ClusterSpec({\"ps\": ps_hosts,\"worker\": worker_hosts})\r\n        gpu_options = tf.GPUOptions(allocator_type='BFC', allow_growth=True)\r\n        if args.job_name == \"server\":\r\n            server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\r\n                    job_name='ps',\r\n                    task_index=args.task_id,\r\n                    default_session_config=tf.ConfigProto(gpu_options=gpu_options, device_count={\"GPU\":0}),\r\n                    protocol=\"grpc\")\r\n        elif args.job_name == \"worker\":\r\n            server_def = tf.train.ServerDef(cluster=cluster.as_cluster_def(),\r\n                    job_name=\"worker\",\r\n                    task_index=args.task_id,\r\n                    default_session_config = tf.ConfigProto(gpu_options=gpu_options),\r\n                    protocol=\"grpc\")\r\n        server = tf.train.Server(server_def)\r\n        return server, cluster, num_workers, gpu_options\r\n     \r\n    def Model(sync_opt):\r\n        if FLAGS.task_id == 0:\r\n            var_0 = tf.get_variable(initializer = 0.0, name = 'v0')\r\n            var_1 = tf.get_variable(initializer = 1.0, name = 'v1')\r\n            grads_0 = constant_op.constant(-1.0)\r\n            grads_1 = constant_op.constant(-1.0)\r\n        else:\r\n            var_0 = tf.get_variable(initializer = 7.0, name = 'v0')\r\n            var_1 = tf.get_variable(initializer = 8.0, name = 'v1')\r\n            grads_0 = constant_op.constant(-2.0)\r\n            grads_1 = constant_op.constant(-2.0)\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        with tf.control_dependencies(update_ops):\r\n            train_op = sync_opt.apply_gradients([[grads_0, var_0], [grads_1, var_1]],\r\n                global_step = tf.train.get_or_create_global_step())\r\n        return train_op\r\n     \r\n    def test():\r\n        server, cluster, num_workers, gpu_options = workers_ps_creator(FLAGS)\r\n     \r\n        if FLAGS.job_name == \"server\":\r\n            server.join()\r\n        elif FLAGS.job_name == \"worker\":\r\n            is_chief = (FLAGS.task_id == 0)\r\n            #Between-graph replication\r\n            worker_device = \"/job:worker/task:%d\" % (FLAGS.task_id)\r\n            with tf.device(model_average_optimizer.model_average_device_setter(\r\n                    worker_device=worker_device, cluster=cluster)):\r\n                #create model\r\n                lr = tf.Variable(1, trainable=False)\r\n                opt = tf.train.GradientDescentOptimizer(lr)\r\n                train_model = Model(opt)\r\n                ma_opt = model_average_optimizer.ModelAverageOptimizer(num_workers, 3)\r\n    \t\r\n                ## store local variables\r\n                collection_key = tf.GraphKeys.SAVERS\r\n                saver = tf.train.Saver(tf.local_variables(), save_relative_paths=True)\r\n                if saver is not None: tf.add_to_collection(collection_key, saver)\r\n    \t\r\n            sess_config = tf.ConfigProto(gpu_options=gpu_options)\r\n            sess_config.log_device_placement = False\r\n            sess_config.allow_soft_placement = True\r\n    \t\r\n            ma_hook = ma_opt.make_ma_run_hook()\r\n            ma_replica_hook = ma_opt.make_session_run_hook(is_chief)\r\n            all_hooks = [ma_replica_hook, ma_hook]\r\n    \t\r\n            tf.logging.info('Start Sess')\r\n            with tf.train.MonitoredTrainingSession(master=server.target,\r\n                        is_chief=is_chief,\r\n                        hooks=all_hooks) as sess:\r\n                tf.logging.info(\"is chief: %s, len: %s\", is_chief, num_workers)\r\n                for i in range(4):\r\n                    sess.run(train_op)\r\n                    pp1 = sess.run(tf.get_default_graph().get_tensor_by_name('v0:0'))\r\n                    pp2 = sess.run(tf.get_default_graph().get_tensor_by_name('v1:0'))\r\n                    tf.logging.info(\"%d %.2f %.2f\" % (FLAGS.task_id, pp1, pp2))\r\n            sv.stop()\r\n            tf.logging.info(\"done\")\r\n    \t\r\n    def main(_):\r\n        test()\r\n    \t\r\n    if __name__ == '__main__':\r\n        tf.app.run()\r\n"]}, {"number": 11580, "title": "python: remove the TRAINABLE_RESOURCE_VARIABLES graph key", "body": "The graph key `TRAINABLE_RESOURCE_VARIABLES` seems to have been removed, drop the lines in this PR.", "comments": ["Can one of the admins verify this patch?", "We can't make this change because it'd mean breaking an API which existed in tf 1.0.", "@alextp and it would be deprecated later?", "If you want to mark it as deprecated I'm fine with that.\n\nOn Tue, Jul 18, 2017 at 11:49 PM, Yazhong Liu <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> and it would be deprecated later?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11580#issuecomment-316289117>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxScfyp1hav2Y3VIV8axH-az3c2GJks5sPacKgaJpZM4ObX3D>\n> .\n>\n\n\n\n-- \n - Alex\n", "Yea, I think that's better for developers :)", "@alextp By the way, could you show which API would be broken on tf 1.0? Any why this PR applied would effect on a released version? The latest version seems 1.2?", "If someone out there has code which uses\ntf.GraphKeys.TRAINABLE_RESOURCE_VARIABLES their code would break. What I\nmeant to say is that removing tensorflow APIs since version 1.0 is not done\n(until 2.0).\n\nOn Wed, Jul 19, 2017 at 9:36 AM, Yazhong Liu <notifications@github.com>\nwrote:\n\n> @alextp <https://github.com/alextp> By the way, could you show which API\n> would be broken on tf 1.0? Any why this PR applied would effect on a\n> released version? The latest version seems 1.2?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11580#issuecomment-316444049>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxZS39wApFAluE-i51jtm82WvT5BXks5sPjCkgaJpZM4ObX3D>\n> .\n>\n\n\n\n-- \n - Alex\n", "This line is not safe to remove as someone could theoretically depend on\nit. Core tf itself does not.\n\nOn Wed, Jul 19, 2017 at 10:16 AM, Yazhong Liu <notifications@github.com>\nwrote:\n\n> *@yorkie* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/python/training/optimizer.py\n> <https://github.com/tensorflow/tensorflow/pull/11580#discussion_r128309230>\n> :\n>\n> > @@ -367,9 +367,7 @@ def compute_gradients(self, loss, var_list=None,\n>      if grad_loss is not None:\n>        self._assert_valid_dtypes([grad_loss])\n>      if var_list is None:\n> -      var_list = (\n> -          variables.trainable_variables() +\n> -          ops.get_collection(ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n> +      var_list = variables.trainable_variables()\n>\n> @alextp <https://github.com/alextp> So this line is safe to remove as\n> your consideration is from community not the core?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/11580#pullrequestreview-50976953>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxSJvI61gdvFIm9Izfls2xcQCCI4fks5sPjnngaJpZM4ObX3D>\n> .\n>\n\n\n\n-- \n - Alex\n"]}, {"number": 11579, "title": "Update workspace.bzl to use latest farmhash commit to support s390x", "body": "Update workspace.bzl to use latest farmhash commit to support s390x.\r\nPlease review", "comments": ["Can one of the admins verify this patch?", "@gunan Could you please review changes?", "Jenkins, test this please", "@jart for the bazel mirror"]}, {"number": 11578, "title": "Cannot build TensorFLow with GPU support when using TensorFlow as an external dependency", "body": "Currently my project uses TensorFlow as an external dependency (local repository). When I build my project with CPU only, the build is successful. However if I add --config=-cuda the build will succeed but without the GPU part (yes, I have run ./configure). The warning I get is: WARNING: Config values are not defined in any .rc file: cuda. This does not happen if I build my project inside TensorFlow's source tree.\r\n\r\n", "comments": ["Try these:\r\nhttps://github.com/tensorflow/models/issues/467\r\nhttps://github.com/tensorflow/tensorflow/issues/2413\r\n", "I forgot to say I'm building the C++ version.  The above issues don't apply to my case.", "The second link also serves the issue for gcc compiler it should work for your case.\r\n", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. \r\n\r\nSpecifically, what steps do I need to take to reproduce your problem.\r\n\r\nThank you."]}, {"number": 11577, "title": "Merge pull request #1 from tensorflow/master", "body": "Updated on 2017/07/18", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Not sure what the purpose of this pull request is. Feel free to reopen with explanation."]}, {"number": 11576, "title": "Fix typo in exception.", "body": "Title says it all.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "@hgaiser can you sign the CLA please?", "@tensorflow-jenkins test this please", "Done!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 11575, "title": "unique in special axis", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.2\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\ncuDNN v5.1\r\n- **GPU model and memory**:\r\npascal titan x\r\n- **Exact command to reproduce**:\r\nNone\r\n\r\n**Feature Request**\r\nFor getting unique row, we can use numpy's `unique` by specifying the axis. However, in tensorflow, `unique` only support 1D tensor, which is not convenient.", "comments": ["@aselle WDYT?", "@aselle I think this would be useful, at the moment I have to do this:\r\n\r\n```\r\nassert_max = tf.Assert(tf.reduce_all(indices < 256), [indices])\r\nwith tf.control_dependencies([assert_max]):\r\n  indices = tf.identity(indices)\r\nindices = tf.cast(indices, tf.uint8)\r\n\r\n# find unique indices, because SparseTensor needs this\r\nindices_pad = tf.pad(indices,[[0,0],[0,1]])  # shape to (N, 4)\r\nassert(indices_pad.get_shape()[1].value == 4)\r\nindices_bc  = tf.bitcast(indices_pad, tf.int32)\r\nunique32, _ = tf.unique(indices_bc)\r\nindices = tf.cast(tf.bitcast(unique32, tf.uint8)[:,:3],tf.int64)\r\n```", "Added a PR #12952 for the support of `axis` for `tf.unique`.", "This feature is still not supported (at least in the Python API). An easy workaround is described in this answer: https://stackoverflow.com/a/60046022/3924118. ", "+1, see #15644, this is currently still not a feature in tf 2.2", "@zaqqwerty  - one workaround (no gradients available, tf 2.4.1) - \r\n```\r\nx.shape  # (100, 2)\r\nunique_results = tf.raw_ops.UniqueV2(x=not_unique, axis=0).y\r\nunique_results.shape  # (89, 2)\r\n```\r\n\r\nSee here: \r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops\r\nhttps://www.tensorflow.org/api_docs/python/tf/raw_ops/UniqueV2\r\n"]}]