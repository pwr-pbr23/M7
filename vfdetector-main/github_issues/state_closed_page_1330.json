[{"number": 13199, "title": "R1.3", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "Hi @TheOnlyAymes  r1.3 fixes were already merged into master. Please reopen if there is a fix missing."]}, {"number": 13198, "title": "Updating the README with the correct nightly links to tf_nightly.", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @alanyee, @tjrana and @andrewharp to be potential reviewers."]}, {"number": 13197, "title": "\"In-Graph Replication\" Multi-GPU training in local/single machine not working.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Wrote a minimal version custom code using TensorFlow API's (see attachment)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Installed through pip \r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: Python 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: Cuda Version 8.0, cuDNN version 1.6\r\n- **GPU model and memory**: GeForce GTX Titan X GPU 12 GB memory\r\n- **Exact command to reproduce**: sh run_dist_tf_exp.sh (attached as zip file)\r\n[distributed_tf_issue.zip](https://github.com/tensorflow/tensorflow/files/1319333/distributed_tf_issue.zip)\r\n\r\n\r\n### Describe the problem\r\nWe are trying to get single machine multi-gpu training with Distributed Tensor Flow for increasing model throughput. Our set-up is as follows, We have a single compute machine running Ubuntu 16.04 with 8 GPU's and we would like to enable \"Data Parallelism\" by training model on multiple(4 GPU's) device present in the local machine to increase throughput. \r\n\r\nI will explain three scenarios below which uses minimal code that just runs a LinearClassifier (see attached code), \r\n\r\nScenario 1:\r\ntf_config set to run single worker and single parameter server config below,\r\n```\r\ntf_config = {\r\n    \"cluster\": {\r\n        'ps': ['127.0.0.1:9000'],\r\n        'worker': ['127.0.0.1:9001']\r\n    }\r\n  }\r\n```\r\nDoes not run, execution just freezes. Also tried the suggestion that came in logs to use `cloud` as environment in tf_config, still didn't work. Read similar issue written [here](https://github.com/tensorflow/tensorflow/issues/8796) commented out the line in [Experiment.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336) that checks if Environment is not LOCAL. By doing so it was successfully running training and worker exited after finished with training_steps. Should that line be commented for local distributed training runs?\r\n\r\nScenario 2:\r\ntf_config set to run two worker and single parameter server config below,\r\n```\r\ntf_config = {\r\n    \"cluster\": {\r\n        'ps': ['127.0.0.1:9000'],\r\n        'worker': ['127.0.0.1:9001', '127.0.0.1:9002']\r\n    }\r\n  }\r\n```\r\nHad commented out [Experiment.py] (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/experiment.py#L336) line as explained in Scenario 1, Did not work, the script was just hanging forever.\r\n\r\nScenario 3:\r\ntf_config set to run one worker and a single parameter server config similar to Scenario 1.\r\n\r\nChange done here was `learn_runner.run(..., schedule='continuous_train_and_eval)`, In continuous_train_and_eval mode the training just froze. Is continuous_train_and_eval not supported with distributed training? Seems to have some problem. Only change here compared to the working Scenario 1 setup with Experiment.py line commented out was changing the schedule to continuous_train_and eval.\r\n\r\n\r\n### Source code / logs\r\nFollowing modification was done to Experiment.py in run function, Had to comment out LOCAL environment check.\r\n```\r\n    if (#config.environment != run_config.Environment.LOCAL and\r\n        config.environment != run_config.Environment.GOOGLE and\r\n        config.cluster_spec and config.master):\r\n      self._start_server()\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13196, "title": "ModuleNotFoundError: No module named '_pywrap_tensorflow_internal'", "body": "Hi - Thanks for all your hard work on this! - I've been having a problem getting Tensorflow-GPU to work on my Windows 10 notebook with a GTX 1080. I've tried to make sure all the paths are correct, etc. and have followed all the tips I can find.\r\n\r\nI ran the tensorflow_self_check.py script and got the following result:\r\n\r\nPS D:\\Users\\Frank Davidson\\Documents\\python> python .\\tensorflow_self_check.py\r\nERROR: Failed to import the TensorFlow module.\r\n\r\n- Python version is 3.6.\r\n\r\n- TensorFlow is installed at: C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\r\n\r\n- All required DLLs appear to be present. Please open an issue on the\r\n  TensorFlow GitHub page: https://github.com/tensorflow/tensorflow/issues\r\nPS D:\\Users\\Frank Davidson\\Documents\\python>\r\n\r\nHere is the full stack trace when I try to import tensorflow:\r\n\r\nPS C:\\Users\\Frank Davidson> python\r\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_i\r\nmport_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <modul\r\ne>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_i\r\nmport_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_i\r\nmport_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 978, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 961, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 950, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 648, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 560, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 205, in _call_with_frames_removed\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <modul\r\ne>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Program Files\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_i\r\nmport_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Program Files\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nAny help is greatly appreciated!\r\n\r\nFrank", "comments": ["Did you try this? (linked from the error message's link)\r\nhttps://stackoverflow.com/questions/35953210/error-running-basic-tensorflow-example", "Hi @aselle - If you mean to cd out of the git directory, I am not starting there to begin with. Am I understanding you correctly? The paths all seem to have the requisite files in them. The only weird thing was that on my Windows machine the \"lib\" directory is \"Lib\" with a capital \"L\". Doesn't seem to change anything when I change it though.\r\n", "Not really sure what is going on. as far as lib vs Lib, windows is case-preserving not case-sensitive. Try using python 3.5 rather than 3.6. Try using the non-gpu version first and see if you can get that to work.\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 162 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 13195, "title": "Branch 169436243", "body": "", "comments": []}, {"number": 13194, "title": "Branch 169431251", "body": "", "comments": ["@caisq, thanks for your PR! By analyzing the history of the files in this pull request, we identified @hawkinsp, @eliben and @xiejw to be potential reviewers.", "@tensorflow-jenkins test this please"]}, {"number": 13193, "title": "Allow partial shape inference for `tf.nn.conv2d_transpose`", "body": "\r\nThis fix tries to address the issue raised in #8972 where it was not possible to infer a partial shape for\r\n`tf.nn.conv2d_transpose`. In case any dimensions are not defined, the shape will be totally undefined.\r\n\r\nThis fix utilizes `tensor_util.constant_value_as_shape` so that partial shape is possible.\r\n\r\nThis fix fixes #8972.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "@tensorflow-jenkins test this please", "Thanks @cwhipkey for the help. The PR has been updated. Please take a look.", "Thanks @cwhipkey for the review. The PR has been updated. Please take a look.", "Thanks @cwhipkey. The `InputTensorShapeOrUnknown` and related unit tests have been removed.", "Jenkins, test this please."]}, {"number": 13192, "title": "fixed doc typo", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 13191, "title": "after the training process of DNNRegressor model i ran the log to check for the loss during training but the loss is not decreasing gradually and infact in my case its starting from 12654.4 and at final step=5131.29", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13190, "title": "TF_AddGradients gradients returns wrong result when multiple outputs specified", "body": "### System information\r\nDarwin Mac-Admin.local 15.6.0 Darwin Kernel Version 15.6.0: Thu Jun 23 18:25:34 PDT 2016; root:xnu-3248.60.10~1/RELEASE_X86_64 x86_64\r\nMac OS X 10.11.6\r\n\r\n### Describe the problem\r\nHi. I've added a unit test for TF_AddGradients API (see code below) which is similar to\r\n[this python test](https://github.com/tensorflow/tensorflow/blob/ca3bc0f1c2f917cf6e7c49d58f5ec604a9af9367/tensorflow/python/ops/gradients_test.py#L337)\r\n\r\nIn the test, I provide two outputs \r\ny[0]=x[0] ** 2 \r\ny[1] = y[0] ** 8\r\n\r\nwhere input x[0]=3.\r\nAccording to the [documentation](https://github.com/tensorflow/tensorflow/blob/03619fab3f4dd6f28b67418455a953b0fccdd9bf/tensorflow/c/c_api.h#L1018)  result should be calculated by formula d(y_1 + y_2 + ...)/dx_1 and be equal to 17502, but the API prints 6.\r\n\r\nWhat am I missing? Thanks.\r\n\r\n### Source code / logs\r\n\r\n```\r\n    TF_Status* s = TF_NewStatus();\r\n    TF_Graph* graph = TF_NewGraph();\r\n\r\n    const int ny = 2;\r\n    const int nx = 1;\r\n    TF_Output inputs[nx];\r\n    TF_Output outputs[ny];\r\n    TF_Output grad_outputs[nx];\r\n\r\n    TF_Operation* ph0 = Placeholder(graph, s);\r\n\r\n    TF_Operation* y0 = Square(graph, s, ph0, \"Square0\");\r\n    TF_Operation* y1 = Square(graph, s, y0, \"Square1\");\r\n    TF_Operation* y2 = Square(graph, s, y1, \"Square2\");\r\n    inputs[0] = TF_Output{ph0, 0};\r\n    outputs[0] = TF_Output{y0, 0};\r\n    outputs[1] = TF_Output{y2, 0};\r\n\r\n    TF_AddGradients(graph, outputs, ny, inputs, nx, nullptr, s, grad_outputs);\r\n    EXPECT_EQ(TF_OK, TF_GetCode(s)) << TF_Message(s);\r\n\r\n    std::unique_ptr<CSession> csession(new CSession(graph, s));\r\n\r\n    std::vector<TF_Output> grad_outputs_vec;\r\n    grad_outputs_vec.assign(grad_outputs, grad_outputs + 2);\r\n    csession->SetInputs({{ph0, Int32Tensor(3)}});\r\n    csession->SetOutputs(grad_outputs_vec);\r\n    csession->Run(s);\r\n    ASSERT_EQ(TF_OK, TF_GetCode(s)) << TF_Message(s);\r\n\r\n    TF_Tensor* out0 = csession->output_tensor(0);\r\n    int* data = static_cast<int*>(TF_TensorData(out0));\r\n    ASSERT_EQ(17502, *data); \r\n\r\nFailure\r\n      Expected: 17502\r\nTo be equal to: *data\r\n      Which is: 6\r\n```\r\n\r\n", "comments": ["@suharshs @skye : Mind taking a look?", "This indeed seems strange. I can reproduce this in python as well along with some other examples to try to narrow it down.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nx = tf.placeholder(tf.float32)\r\n\r\ny0 = x * x\r\ny1 = y0 * y0\r\n# y2 == y3 == y4 == y5, but they y2 and y3 are incorrect.\r\ny2 = y1 * y1\r\ny3 = y0 * y0 * y0 * y0\r\ny4 = x * x * x * x * x * x * x * x\r\ny5 = tf.pow(y0, 4)\r\n\r\nga = tf.gradients([y0], x) # Correct : 6.0\r\ngb = tf.gradients([y1], x) # Correct : 108.0\r\ngc = tf.gradients([y2], x) # Correct : 17496.0\r\ngd = tf.gradients([y3], x) # Correct : 17496.0\r\nge = tf.gradients([y4], x) # Correct : 17496.0\r\ngf = tf.gradients([y5], x) # Correct : 17496.0\r\n\r\ng0 = tf.gradients([y0, y1], x) # Correct       : 114.0\r\ng1 = tf.gradients([y1, y2], x) # Correct       : 17604.0\r\ng2 = tf.gradients([y0, y2], x) # Incorrect     : 6.0\r\ng3 = tf.gradients([y0, y3], x) # Incorrect     : 4380.0\r\ng4 = tf.gradients([y0, y4], x) # Correct       : 17502.0\r\ng5 = tf.gradients([y0, y5], x) # Correct       : 17502.0\r\ng6 = tf.gradients([y0, y1, y2], x) # Correct   : 17610.0\r\n\r\nwith tf.Session() as sess:\r\n    print(sess.run([ga, gb, gc, gd, gf], feed_dict={x: 3.0}))\r\n    print(sess.run([g0, g1, g2, g3, g4, g5, g6], feed_dict={x: 3.0}))\r\n```\r\n\r\nIt seems that building up x ^ 8 by using intermediate tensors via multiplication results in errors. Curiously using tf.pow(y0) vs y0* y0 * y0 *y0 results in different gradients when paired with y0. I don't know the issue yet, but will take another look tomorrow. ", "@Dorokhov please take another look. I was mistaken in my previous comments. I went ahead and deleted my old comments so as to not cause more confusion :)", "Huh, looks like I was using a old version of tensorflow. The issue in python was resolved here: https://github.com/tensorflow/tensorflow/commit/fb56fc90167c3919cb59f753f233ef2a41469cb2#diff-7d33d81b07b3cb1679ed1b011a66e447\r\n\r\nI will send a similar fix for C++. Thanks."]}, {"number": 13189, "title": "Non-scalar Multinomial draws", "body": "This PR tries to fix the issue in [#12804](https://github.com/tensorflow/tensorflow/issues/12804) where supports total_count to be a non-scalar tensor and broadcasts by `map_fn`.\r\n\r\nThe reason why I use `map_fn` to broadcast manually is that current `tf.multinomial` sampler op don't support to produce different total_count within a batch, and `map_fn` can enhance this underlying op effectively without modifying it.\r\n\r\nIt's a little dilemma if we modify the underlying op to support different total_count within a batch, because that will produce variable-length draws within a batch, unless we add a new function with different interface for multinomial sampler.", "comments": ["Can one of the admins verify this patch?", "@jvdillon Could you please review my code? Thank you very much~", "Looking now...", "Overall, looks good--just a few nitpicks.   I had actually hoped we could change the underlying C++ sampler, but I suppose this is a reasonable stop-gap measure.  I'll approve once the minor style issues have been addressed.", "@jvdillon \r\n\r\nThank you for getting this reviewed, I have modified the code according to your suggestion. By the way, I am glad to try to help changing the underlying sampler in the future.", "@jvdillon is this good to go?", "Sorry for my slowness to review.  Thank you for your contribution.  Please feel free to reach out to me regarding issues that come up in your improvement in the underlying multinomial sampler.  It can also be made much more efficient than it currently is.", "Jenkins, test this please.", "@jinze1994 there seems to be test failure with the change https://ci.tensorflow.org/job/tensorflow-pull-requests-cpu-python3/6702/consoleFull", "@jvdillon @sb2nov \r\n\r\nSorry for my delay to handle the test failure. I think this test failure occurred because there were not enough sample time when check the covariance from sampling, which led to some deviation. And I fix it by augmenting the sample time.\r\n\r\nAs for improving in the underlying multinomial sampler, thanks for jvdillon's support first, but I am not sure if I have enough time to achieve that. If I have enough time in the future, I will create a new PR for modify the underlying sampler.", "Thanks again!  (Right, fixing the C++ sampler should definitely be a separate PR, when/if you get to it.)", "@sb2nov @jvdillon \r\nMy pleasure. So could you please test this PR again?", "@jvdillon Hi bro~ Could you please test this PR again? Or is there anything I could do for this PR?", "Jenkins, test this please.", "@jvdillon @sb2nov \r\nI've already handled the test failure, could you please test it again? Thank you very much.", "Jenkins, test this please.", "How are things coming along? Anything I can help with?", "@tensorflow-jenkins test this please", "@jvdillon @vrv \r\nI haven't figured out the failure reason of those check above, but it seems not to be related with this PR.\r\nSo is there anything I can do to avoid it and modify?", "Breakages seem unrelated.", "@jvdillon @vrv Is this good to merge?", "@vrv seems like the breakages are unrelated. Maybe kick off another run and see if they all pass this time?", "@tensorflow-jenkins test this please", "@jvdillon \r\nSorry for that there are still some test failure, but I don't know how to handle that.\r\nShould I merge with the current master branch to repair those BUILD failure?", "Jenkins, test this please.", "Jenkins, test this please.", "@jvdillon @vrv \r\nI have rebased this PR with current tensorflow master, and it should solve the BUILD failures. So please test it again. Thank you.", "Jenkins, test this please.", "Those failures seem unrelated with this PR."]}, {"number": 13188, "title": "third_party/zlib: use -DZ_HAVE_UNISTD_H instead of suppressing warnings", "body": "Hi,\r\n\r\nI noticed that you built zlib by suppressing warnings about it using undeclared functions. However, I discovered that adding `copts = [\"-DZ_HAVE_UNISTD_H\"]` could make zlib to include unistd.h and therefore get rid of the warnings completely.\r\n\r\nWhile this change is very minor, I think declaring the macro is better than suppressing the warnings.", "comments": ["Thanks for pointing this out @ahyangyi. Would you be willing to submit a PR that makes this change?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "I think the issue has been resolved in #13237. Thanks for the contribution!"]}, {"number": 13187, "title": "parse_example is awfully slow", "body": "@skearnes\r\nyou have indicated in this post of yours https://github.com/tensorflow/tensorflow/issues/390 way back in 2015 that parse_example is about 30 times faster than parse_single_example.\r\nI have tried different options to modify my simple training script which only prints about 100000 tfrecords batched in 1000 and just does a print of feature and label after session.run(feature, label). Feature is a sparseTensor BTW.\r\nCan you please put a test sample which proved that parse_example was that fast. parse_single_example was taking ~320 secs, now parse_example takes ~240 secs.\r\n\r\n@Admin, please do not close this issue and refer to stackoverflow, as I don't think this is something to do with API usage or wrong parameters.\r\nThis is to do with the performance of queues (enqueue, dequeue) & threads", "comments": ["It's possible there was performance regression in parse_example \r\n\r\nFor this to get resolved there needs to be at minimum a reproducible example, and ideally, some investigation of the cause of the slowness\r\n\r\n1. It takes 240 secs, how much do you expect it to take? Is it possible that it's as as fast as it can be? Similar discussion with parse_csv slow\r\nhttps://github.com/tensorflow/tensorflow/issues/11713\r\n\r\n2.  [gperftools](https://github.com/gperftools/gperftools) profile to show where time is spent is useful, like here\r\nhttps://github.com/tensorflow/tensorflow/issues/3009#issuecomment-228867287\r\n\r\n", "just to add more information to debug, here is my lscpu\r\n\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                4\r\nOn-line CPU(s) list:   0-3\r\nThread(s) per core:    1\r\nCore(s) per socket:    2\r\nSocket(s):             2\r\nNUMA node(s):          1\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 79\r\nModel name:            Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz\r\nStepping:              1\r\nCPU MHz:               2095.148\r\nBogoMIPS:              4190.29\r\nHypervisor vendor:     VMware\r\nVirtualization type:   full\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              256K\r\nL3 cache:              40960K\r\nNUMA node0 CPU(s):     0-3\r\n\r\n\r\n", "Each TFRecord is a sparse structure of feature shape  1000 222215 viz. basically I am making batches of 1000 features. TFRecordWriter works faster as compared to burning a 1000 examples of (1x222215)  . My labels are on one-hot with 58565 labels and 1000 rows again to match 1000 features.\r\nUsing SparseFeature and parse_example is faster compared to using parse_single_example and/or VarLenFeature. But not significantly fast. (I didn't do decode_csv, it would have been worse anyway.)\r\nWith this structure, I am able to train 400000 examples trained in batches of 1000 (each TFRecord) and 5 epochs in 861-900 secs. Is this the best I can get. I am ONLY printing the tensors. No matmuls, no optimizers.\r\nI use 4 producer queues (string_input_producer), to read 8 TFRecord files (2 each) and shuffle_batch with 1-2 threads. If I use 4 threads, as is the case of shuffle_batch_join which creates as many number of threads to enqueue as the tensors in the list viz., 4 in my case, I get Timedout error waiting for notification. So I believe producers don't get enough time if I do that.\r\nNow the question back, is this the best I can get on the said hardware?", "That was the story with 1.2.1. I thought I should upgrade to 1.3 and now I did a bazel build hoping for better. Unfortunately, it's got worse. Same run above, now takes 1184 secs.\r\nDoes Tensorflow require some minimum hardware to show it's muscles.", "I think the input pipeline is getting slower. I am sure if I do away with TFRecords and queues and feed the sparse matrix directly, processing using scipy or some other python library, it's been running faster earlier, though it was single threaded and may not be using all the logical cores concurrently.", "There are many ways to get slow performance that is not a bug in TensorFlow (ie, wrong choice of threads, wrong data format, wrong design of pipeline etc). Stackoverflow can help with that with tag \"tensorflow\". It could be a bug in `parse_example` op. If you think it's a bug in `parse_example`, it would help with extra details I mentioned above (reproducible example + performance profile)", "I reduced the number of labels in the training examples from about 70000, which is the super set to about 1500 which is the number labels that's in the training set. Now the dry run, (WITHOUT matmuls, cost optimizers etc.) runs very fast. About 0.5 mn training examples pipelined and get enqueued/dequeued in about 17 secs and 5 epochs. That's about 1 million training examples in about 8-9 secs. I am not sure how & why this correlates internally, the features are SparseFeature and label is FixedLenFeature.\r\nNow the big question, if I have to train with all the 70000 labels, will the performance again take a beating.", "Update: I think the slowness has nothing to do with the number of labels. I removed one-hot dense vector and made it one-hot sparse and it's as fast as otherwise viz., its just a fixedLenFeature with the batch of numbers.\r\nWhat helped me is to write a batch of say 1000 rows into tfrecords and read it as a single_example (though I know its a sparse-batch example) and treat it accordingly. Writing a batch of examples into tfrecords is definitely faster than writing each feature row/matrix, vector.\r\nBut parse_example is nowhere faster than parse_single_example as claimed here #390. ", "@balaji-in-git ,I think parse_example is much faster than parse_single_example. in my case , speed change from 25000 sample/second to 32000 sample/second", "@balaji-in-git, can you provide code to verify this regression? Something that used to be fast but is now slow?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "Nagging Awaiting Response: It has been 14 days with no activityand the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Closing due to lack of activity but please reopen if there's new information."]}, {"number": 13186, "title": "Renaming checkpoint directory", "body": "If you want to rename a checkpoint directory, you currently need to also do a find-replace in the 'checkpoint' file. It might be nice to have that handled automatically.\r\n", "comments": ["I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 13185, "title": "Fix random crashes in SessionRun()", "body": "This patch fixes https://github.com/tensorflow/tensorflow/issues/13129\r\n\r\nThe problem was with usage of runtime.SetFinalizer() in golang bindings. When a variable become unreferenced GC can harvest it any moment after. According to this https://tip.golang.org/pkg/runtime/#SetFinalizer:\r\n\r\n> For example, if p points to a struct that contains a file descriptor d, and p has a finalizer that closes that file descriptor, and if the last use of p in a function is a call to syscall.Write(p.d, buf, size), then p may be unreachable as soon as the program enters syscall.Write. The finalizer may run at that moment, closing p.d, causing syscall.Write to fail because it is writing to a closed file descriptor (or, worse, to an entirely different file descriptor opened by a different goroutine). To avoid this problem, call runtime.KeepAlive(p) after the call to syscall.Write.\r\n\r\nIn our case unreferenced variable was \"feeds\", i.e. input tensors. After investigations I have found out that func (t *Tensor) finalize() { C.TF_DeleteTensor(t.c) } was called a way before SessionRun() was finished and that was the cause of crash in my case.\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "Jenkins, test this please", "The test failures are unrelated. Merging the PR."]}, {"number": 13184, "title": "A fix for https://github.com/tensorflow/tensorflow/issues/13129", "body": "The problem was with usage of runtime.SetFinalizer() in golang bindings. When a variable become unreferenced GC can harvest it any moment after. According to this https://tip.golang.org/pkg/runtime/#SetFinalizer:\r\n\r\n> For example, if p points to a struct that contains a file descriptor d, and p has a finalizer that closes that file descriptor, and if the last use of p in a function is a call to syscall.Write(p.d, buf, size), then p may be unreachable as soon as the program enters syscall.Write. The finalizer may run at that moment, closing p.d, causing syscall.Write to fail because it is writing to a closed file descriptor (or, worse, to an entirely different file descriptor opened by a different goroutine). To avoid this problem, call runtime.KeepAlive(p) after the call to syscall.Write.\r\n\r\nIn our case unreferenced variable was \"feeds\", i.e. input tensors. After investigations I found out that func (t *Tensor) finalize() was called a way before SessionRun() was finished and that was the cause of crash.\r\n\r\n\r\n", "comments": ["Can one of the admins verify this patch?", "Please ignore this one"]}, {"number": 13183, "title": "\"Controlling Dreams\" in TensorFlow", "body": "Hi guys,\r\n\r\nI am trying to re-implement \"controlling dreams\" using TensorFlow which was implemented in the original google deepdreams algorithm using Caffe (see the last part of https://github.com/google/deepdream/blob/master/dream.ipynb). But I think I got problems. This \"controlling dreams\" part does not exist in this tutorial. I am looking for someone, who knows Caffe and deepdream, to help.\r\n\r\nI have never used Caffe before, but I tried to understand what they were doing in the script. To \"control the dream\", they firstly input a small guide image (roughly 240 x 240) into the neural network, and extract the features in each channel in certain layer specified. These features are called \"guide features\". Then they input the large image, which they want to modify, into the model. The large image was \"octaved\". For each octave image, they input the octave image into the neural network, extract the features in each channel in the same layer specified to the guide image. Then they have to find the best matching guide features of certain channel to each of the features in the layer of octave image. Here \"the best matching\" means the dot product of the two feature vectors is the largest.\r\n\r\nHere comes the question. They extract the guide features beforehand. Therefore the matrix shape of \"guide features\" is constant. But the shape of the feature matrix in octave can vary. This means that I may not be able to calculate the dot product of these two matrices. I believe I did not understand correctly their algorithm. I would like to have someone to explain to me or provide me a reference. Thank you.\r\n\r\nBest,\r\n\r\nLei\r\n\r\n\r\n", "comments": ["This is not the right platform to ask the question since it is not a bug or feature request. Thanks!"]}, {"number": 13182, "title": "No OpKernel was registered to support Op 'Transpose' with these attrs", "body": "### System information\r\n- **Windows 10**:\r\n- **TensorFlow installed from source**:\r\n- **TensorFlow version r1.3**:\r\n- **Python version 3.5.3**: \r\n- **Bazel version n/a**:\r\n- **CUDA/cuDNN version n/a**:\r\n- **GPU model and memory n/a**:\r\n- **Exact command to reproduce n/a**:\r\n\r\n### Describe the problem\r\nThe 'Transpose\" Op is not supported sufficiently in C++ environment. I have got this below. Does transpose operation ever works in a pure C++ project? \r\n\r\n### Source code / logs\r\nNo OpKernel was registered to support Op 'Transpose' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  device='CPU'; T in [DT_INT64]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_BOOL]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_RESOURCE]; Tperm in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tperm in [DT_INT32]\r\n", "comments": ["It is my mistake of Transpose operation usage.  I have used a permutation tensor with DT_INT64 instead of DT_INT32 type.\r\n\r\n> Scope root = Scope::NewRootScope();\r\n>   \r\n> Tensor x_tensor( DT_INT64, TensorShape( { i64TestSize, 3i64 } ) );\r\n> Tensor perm_tensor( DT_INT64, TensorShape( { 2i64 } ) );\r\n>  \r\n> // Filling x_tensor and perm_tensor with some values\r\n>  \r\n> auto y_transpose = Transpose( root, x_tensor, perm_tensor );\r\n> \r\n> session.Run( {}, { transposed_x }, &outputs );\r\n\r\nIt is sometimes not obvious which type should be used for tensors in a processing chain. If a graph is not working, please check that the output type of the node is identical to the input type of the successor node."]}, {"number": 13181, "title": "E tensorflow/core/platform/cloud/http_request.cc:514", "body": "I run the train.py  in the object detection api, use the code in the terminal \r\npython3 train.py --logtostderr --train_dir=training --pipeline_conf\r\nig_path=training/ssd_mobilenet_v1_pets.config \r\n\r\nencounter with the problems like that,\r\nPlease use tf.global_variables instead.\r\n2017-09-20 21:10:31.065044: E tensorflow/core/platform/cloud/http_request.cc:514] The transmission has been stuck at 0 bytes for 61 seconds and will be aborted.\r\n2017-09-20 21:10:31.065307: I tensorflow/core/platform/cloud/retrying_utils.cc:77] The operation failed and will be automatically retried in 0.980452 seconds (attempt 1 out of 10), caused by: Unavailable: Error executing an HTTP request (HTTP response code 0, error code 42, error message 'Callback aborted')\r\n2017-09-20 21:11:33.330807: E tensorflow/core/platform/cloud/http_request.cc:514] The transmission has been stuck at 0 bytes for 61 seconds and will be aborted.\r\n2017-09-20 21:11:33.331297: I tensorflow/core/platform/cloud/retrying_utils.cc:77] The operation failed and will be automatically retried in 1.92775 seconds (attempt 2 out of 10), caused by: Unavailable: Error executing an HTTP request (HTTP response code 0, error code 42, error message 'Callback aborted')\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **Ubuntu 16.04)**:\r\n- **TensorFlow install from pip**:\r\n- **TensorFlow 1.3.0**:\r\n- **Python 3.5**: \r\n- **Only CPU**:\r\n-: i also install python2 and anaconda 2.7 in my linux\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\npython3 train.py --logtostderr --train_dir=training --pipeline_config_path=training/ssd_mobilenet_v1_pets.config \r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13180, "title": "how to avoid the flag --whole-archive when using the static library in windows", "body": "Hi everyone,\r\n\r\nI have built the static library of tensorflow for C++ in windows. And I am trying to use this library in qtCreator, however I have the issue (no session factory registered for the given options). But, it looks adding these flags in qtCreator doesn\u00b4t work so I still have the issue. Does someone know how to build the library in Tensorflow  so I can use the library without the flag -whole-archive?\r\n", "comments": ["See https://github.com/tensorflow/tensorflow/issues/4242 for explanation why you need whole-archive flag for TensorFlow. Basically if you don't add it, then the linker will strip all the op definition code. ", "Hi yaroslavvb,\r\n\r\nI have already check it that entry, but the issue I have is the flag /wholearchive doesn't work in qt. I am compiling my project with the compiler Microsoft visual 14 in Qt, however It seems there is a problem with Qt and it ignores the flag, so I still have the error \"not session found ...\". You can see below the discussion about this issue in Qt.\r\n\r\nhttps://forum.qt.io/topic/83457/build-settings-for-the-compiler/11\r\n\r\nCould you tell me please if I still need this flag if I build the tensorflow library with bazel in Windows? \r\nCould you give me some advise please?\r\n\r\nYour help is very much appreciated.", "I would expect this flag to be always needed. Default behavior of linkers is to strip out unused symbols, so this flag is what tells the linker to not do this", "It makes sense, but do you know if this flag is still needed if I build the .dll instead of the static library?", "cc @allenlavoie who's been doing some restructuring of TensorFlow linking options", "Thanks for the information.\r\n\r\nKind regards.", "Hi yaroslavvb,\r\n\r\nI have been able to build the tensorflow .dll  and it looks like the flag /wholearchive with the dinamic library is not needed.  \r\nMy project is working only linking with the following dependencies:\r\n libprotobuf.lib and tensorflow.lib. \r\n\r\nThank you very much for your help, it has been very useful.\r\n\r\nKind regards.", "OK, I guess it makes sense, dynamic linkers don't strip out symbols\u00a0by default, as opposed to static linkers, since they don't know which symbols are needed"]}, {"number": 13179, "title": "Function categorical_column_with_identity with big number as num_buckets parameter causes training hang and crash", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: not use GPU\r\n- **GPU model and memory**: no GPU\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen I try to use use_id or item_id as an feature and process it through function ```categorical_column_with_identity```, because of the id category will be very big out of upper limit of int32, the training job will hang a few minutes then crash.  The crash without any useful information to reminder that this problem was caused by the ```num_buckets=${a_big_number}```.\r\n\r\nMaybe this kinds of feature like uid or item_id should not be used like this, and I believe if there are some diagnostics information will be better.\r\n\r\nAnd any suggestions about how to process this kinds of feature like uid or item_id, these features are very big number. I think this problem appeared in many cases.\r\n\r\nThanks\r\n\r\n### Source code / logs\r\n```python\r\n# coding: utf-8\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column as fc\r\n\r\nf = fc.embedding_column(\r\n    fc.categorical_column_with_identity(key='vid', num_buckets=1500000000),\r\n    dimension=10)\r\ne = tf.estimator.DNNClassifier(\r\n    feature_columns=[f],\r\n    hidden_units=[10])\r\n\r\n\r\ndef input_fn():\r\n    return (\r\n        {\"vid\": tf.identity(tf.constant(\r\n            [1000, 1000, 100, 1000, 10000, 1000, 1000, 1000, 100],\r\n            name=\"vid\"), name=\"vid_trainning\")},\r\n        tf.constant([1, 0, 1, 0, 0, 0, 0, 0, 1])\r\n    )\r\n\r\ne.train(input_fn, steps=10)\r\n```\r\n", "comments": ["It seems likely that the hang and crash is due to the python interpreter running out of memory.  It would be difficult to anticipate the bound at which that is likely to happen on a given computer.  The documentation [here](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity) does suggest that using large indexes is not a good idea.  Are you really using all or most of the values in this  1.5e9 range?  The documentation suggests using categorical_column_with_hash_bucket if not.", "In my view, it should be python memory problem i believe. And for the\nfeatures like uid and itemid, very large number should be normal case.\n\nUsing categorical_column_with_hash_bucket, I believe will meet same\nproblem, because the samples is bigger than 1e10.\n\n2017-09-26 5:02 GMT+08:00 Paul Tucker <notifications@github.com>:\n\n> It seems likely that the hang and crash is due to the python interpreter\n> running out of memory. It would be difficult to anticipate the bound at\n> which that is likely to happen on a given computer. The documentation here\n> <https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_identity>\n> does suggest that using large indexes is not a good idea. Are you really\n> using all or most of the values in this 1.5e9 range? The documentation\n> suggests using categorical_column_with_hash_bucket if not.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/13179#issuecomment-332010731>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AF3zuhLNZrhqXikL5K-hJi6BLlXlFl6tks5smBTagaJpZM4Pd0Et>\n> .\n>\n\n\n\n-- \n---------------------------------------\n\u62d3\u5b87\n\n\u5b81\u9759\u4ee5\u81f4\u8fdc\n", "Recommendation is using categorical_column_with_hash_bucket with a smaller bucket_size. \r\nHaving said that, it would be great to improve error message. Right now nobody is looking into this specific error case. Contributions welcome!", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Closing by assuming that previous answer solved the issue. Please open if again if that's not the case."]}, {"number": 13178, "title": "Non-scalar Multinomial draws", "body": "This PR tries to fix the issue in [#12804](https://github.com/tensorflow/tensorflow/issues/12804) where  supports total_count to be a non-scalar tensor.\r\n\r\nI have already test this feature in local, but I haven't found any file to add test code, please let me know if I need to add any test code.", "comments": ["Can one of the admins verify this patch?", "@jinze1994, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jvdillon, @tensorflower-gardener and @yifeif to be potential reviewers.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_author_cla -->"]}, {"number": 13177, "title": "cmake: No session factory registered for the given session options ", "body": "Hi everyone,\r\n\r\nI have built the static library for tensorflow with cmake in Windows and I have the error \"No session factory registered for the given session options \". This issue is fixed with the flag /Wholearchive, however it seems there is not equivalent flag for Qt creator. So, my question is if someone knows if the static library built with bazel has the same issue (add the flag /wholearchive) in windows? I have read that someone people didin\u00b4t have this issue with bazel in Linux, but it would be great if someone can confirm it for Windows.\r\n\r\nThanks for your help.", "comments": ["looks this was fixed in your other issue, feel free to close if it was addressed", "This is a duplicate of #13180. Please see that issue for more information.", "@hemecha  I am currently having the same issue. I was able to build TensorFlow using CMake and use it in Visual Studio with the /WHOLEARCHIVE flag, but I'm having trouble finding an alternative in Qt. Would you be able to shed some light regarding how you generated libprotobuf.lib and tensorflow.lib and then linked them in Qt?\r\n\r\nThanks in advance!", "Just following up on my previous comment - I ended up getting TensorFlow to work on Qt using your recommendation in #13180.\r\n\r\nFor reference: I built TensorFlow using CMake as per the instructions in [this post](https://medium.com/@shiweili/building-tensorflow-c-shared-library-on-windows-e79c90e23e6e). Once built, I was able to run the C++ API code from [this post](https://joe-antognini.github.io/machine-learning/windows-tf-project) in Qt by adding the corresponding Include Directories and linking only libprotobuf.lib and tensorflow.lib (both of which should be in the CMake build directory) rather than the libraries listed in the post. "]}, {"number": 13176, "title": "Raspberry Pi Makefile issues with proto_text", "body": "### System information\r\n- **Raspberry Pi on Ubuntu Mate 16.04 (Also tried Raspbian Stretch but GUI would freeze a lot and still gave similar errors)**:\r\n\r\n### Describe the problem\r\nBuilding tensorflow from source using makefile using the code provided at: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile. I've included two log files in which i ran the initial setup and ran into a zlib.h error which i resolved by running 'sudo apt-get install libz-dev' but then re-run the last make-f line which gave me another error.\r\n\r\n### Error:\r\n\r\n```\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::mutex()':\r\nenv.cc:(.text._ZN10tensorflow5mutexC2Ev[_ZN10tensorflow5mutexC5Ev]+0xc): undefined reference to `nsync::nsync_mu_init(nsync::nsync_mu_s_*)'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::lock()':\r\nenv.cc:(.text._ZN10tensorflow5mutex4lockEv[_ZN10tensorflow5mutex4lockEv]+0xc): undefined reference to `nsync::nsync_mu_lock(nsync::nsync_mu_s_*)'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::unlock()':\r\nenv.cc:(.text._ZN10tensorflow5mutex6unlockEv[_ZN10tensorflow5mutex6unlockEv]+0xc): undefined reference to `nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/contrib/makefile/Makefile:632: recipe for target '/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text' failed\r\nmake: *** [/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\r\n```\r\n\r\n### Logs\r\nOutput file 1 (initial run, stopped at zlib error):  https://pastebin.com/dmqWYAs6\r\nOutput file 2 (current error after fixing zlib error):  https://pastebin.com/aE51br80\r\n\r\n#### What I've tried\r\nLooked at similar issues but they were related to iOS and didn't make sense to me. \r\n", "comments": ["#12810 followed what mjmanning mentioned and it's not giving errors now but does freeze up everything (CPU and MEMORY hit 100%) when it gets to compiling training_ops\r\n\r\nEdit: Attempting to use a swap drive and see how much further I can get", "Consider using a cross compiler on a bigger host to produce the raspberry pi binary. I don't know how to do this most easily, but probably others have done this.", "Currently sitting on this error:\r\n\r\n```\r\n(.text._ZN10tensorflow12RemoteCallOp12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow12RemoteCallOp12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x300): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(remote_fused_graph_execute_op.o): In function `tensorflow::RemoteFusedGraphExecuteOp::RemoteFusedGraphExecuteOp(tensorflow::OpKernelConstruction*)':\r\nremote_fused_graph_execute_op.cc:(.text._ZN10tensorflow25RemoteFusedGraphExecuteOpC2EPNS_20OpKernelConstructionE[_ZN10tensorflow25RemoteFusedGraphExecuteOpC5EPNS_20OpKernelConstructionE]+0xf2): undefined reference to `google::protobuf::MessageLite::ParseFromString(std::string const&)'\r\nremote_fused_graph_execute_op.cc:(.text._ZN10tensorflow25RemoteFusedGraphExecuteOpC2EPNS_20OpKernelConstructionE[_ZN10tensorflow25RemoteFusedGraphExecuteOpC5EPNS_20OpKernelConstructionE]+0x1e4): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saved_tensor_slice.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SavedSlice*)':\r\nsaved_tensor_slice.pb_text.cc:(.text+0x630): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saved_tensor_slice.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SavedSliceMeta*)':\r\nsaved_tensor_slice.pb_text.cc:(.text+0x146c): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(memmapped_file_system.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::MemmappedFileSystemDirectoryElement*)':\r\nmemmapped_file_system.pb_text.cc:(.text+0x354): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(saver.pb_text.o): In function `tensorflow::internal::ProtoParseFromScanner(tensorflow::strings::Scanner*, bool, bool, tensorflow::SaverDef*)':\r\nsaver.pb_text.cc:(.text+0x698): undefined reference to `google::protobuf::internal::fixed_address_empty_string'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/lib/libtensorflow-core.a(cluster.pb_text.o):cluster.pb_text.cc:(.text+0x6e4): more undefined references to `google::protobuf::internal::fixed_address_empty_string' follow\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/contrib/makefile/Makefile:566: recipe for target '/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark' failed\r\nmake: *** [/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1\r\n```\r\n\r\nGoing to try to 'make CXX=g++-4.8' for building protobuf\r\nIf I can't fix this, I'll move onto someone else's compiled library and try to install that.", "Using 'make CXX=g++-4.8' resulted in:\r\n\r\n```\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/grappler/costs/op_performance_data.pb.o:(.rodata._ZTVN6google8protobuf8internal12MapEntryImplIN10tensorflow6OpInfo16OpInfo_AttrEntryENS0_7MessageENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_9AttrValueELNS1_14WireFormatLite9FieldTypeE9ELSF_11ELi0EE15MapEntryWrapperE[_ZTVN6google8protobuf8internal12MapEntryImplIN10tensorflow6OpInfo16OpInfo_AttrEntryENS0_7MessageENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_9AttrValueELNS1_14WireFormatLite9FieldTypeE9ELSF_11ELi0EE15MapEntryWrapperE]+0x2c): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/grappler/costs/op_performance_data.pb.o:(.rodata._ZTVN6google8protobuf8internal8MapEntryIN10tensorflow6OpInfo16OpInfo_AttrEntryENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_9AttrValueELNS1_14WireFormatLite9FieldTypeE9ELSE_11ELi0EEE[_ZTVN6google8protobuf8internal8MapEntryIN10tensorflow6OpInfo16OpInfo_AttrEntryENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_9AttrValueELNS1_14WireFormatLite9FieldTypeE9ELSE_11ELi0EEE]+0x2c): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'\r\n/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/grappler/costs/op_performance_data.pb.o:(.rodata._ZTVN6google8protobuf8internal12MapEntryImplIN10tensorflow6OpInfo16OpInfo_AttrEntryENS0_7MessageENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_9AttrValueELNS1_14WireFormatLite9FieldTypeE9ELSF_11ELi0EEE[_ZTVN6google8protobuf8internal12MapEntryImplIN10tensorflow6OpInfo16OpInfo_AttrEntryENS0_7MessageENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS3_9AttrValueELNS1_14WireFormatLite9FieldTypeE9ELSF_11ELi0EEE]+0x2c): undefined reference to `google::protobuf::Message::InitializationErrorString[abi:cxx11]() const'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/contrib/makefile/Makefile:631: recipe for target '/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text' failed\r\nmake: *** [/home/sensor1/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\r\n```\r\n\r\nIt would've nice to get the makefiles working but going to move on and try other methods of running tensorflow on Pi3. Might come back to it later on and try the compile_pi_protobuf script.", "@ZairMahmood Have you found any solution to this yet or did you just use someone else's compiled library? Myself am stuck on the same issues, ubuntu 16 Raspberry Pi 3, trying to makefile ends up in the following: \r\n\r\n```\r\n/home/pi/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::mutex()':env.cc:.text._ZN10tensorflow5mutexC2Ev[_ZN10tensorflow5mutexC5Ev]+0x18): undefined reference to `nsync::nsync_mu_init(nsync::nsync_mu_s_*)'\r\n/home/pi/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::lock()':env.cc:(.text._ZN10tensorflow5mutex4lockEv[_ZN10tensorflow5mutex4lockEv]+0x18): undefined reference to `nsync::nsync_mu_lock(nsync::nsync_mu_s_*)'\r\n/home/pi/tensorflow/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/platform/env.o: In function `tensorflow::mutex::unlock()':env.cc:(.text._ZN10tensorflow5mutex6unlockEv[_ZN10tensorflow5mutex6unlockEv]+0x18): undefined reference to `nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)'\r\ncollect2: error: ld returned 1 exit status\r\ntensorflow/contrib/makefile/Makefile:631: recipe for target '/home/pi/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text' failed\r\nmake: *** [/home/pi/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1\r\n```\r\n", "I'm currently using someone else's precompiled library:\r\nhttp://tuatini.me/building-tensorflow-as-a-standalone-project\r\nI'll revisit compiling it myself way later on.", "What about reducing the GPU memory allocation like to 16MB? Did you try a USB drive for swap drive? Maybe the screen blanking should be disabled too on Pi? And I was thinking to disable GUI, then go ahead and do it via SSH too. So the the CPU hasn't much to do besides this build..\r\n\r\nEdit: sorry I didn't notice you are using Ubuntu Mate 16.04\r\n\r\nEdit: I just tried this today, I used the GUI, but reduced GPU right down to 16MB. I got way past compiling training_ops, but then I left the Pi and the screen blanking kicked in, when I came back it had crashed! It's suspicious it would crash at that very time (a 15 min period) after it had been doing fine for several hrs! During that period, I'd been making sure screen blanking didn't kick in. So maybe screen blanking is a problem. Anyway, I will disable that and try again tmrw! ", "There's this project for pre-compiled too: https://github.com/samjabrahams/tensorflow-on-raspberry-pi\r\nThe wheels are here: https://github.com/samjabrahams/tensorflow-on-raspberry-pi/releases", "I have solved this problem with the help of bro @umang6891 at https://github.com/tensorflow/tensorflow/issues/12482.\r\nFollow these steps:\r\nFirst, run \u2018export -p\u2019 to confirm the variable HOST_NSYNC_LIB and TARGET_NSYNC are right there.\r\nIf not, run:\r\nexport HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh`\r\nexport TARGET_NSYNC_LIB=\"$HOST_NSYNC_LIB\"\r\nthen add it in the file mutex.h(/tensorflow/tensorflow/core/platform/default/mutex.h)\r\n#include \"tensorflow/contrib/makefile/downloads/nsync/public/nsync_cv.h\"\r\n#include \"tensorflow/contrib/makefile/downloads/nsync/public/nsync_mu.h\"\r\nHope that it can work on you Rpi.", "Thanks! I will try it out!", "@Kyle-Kuzma I definitely tried that when I was having this issue and still didn't work. Possibly fixed in 1.4? Haven't tried yet since I'm pretty satisfied with Tuatini's solution.", "How do you solve this problem, please?\uff0cI tried so many ways that I didn't use it.thanks", "@qyhsxdx Hi! It has been such a long time since I last compiled tensorflow on my own pi3. I suggested that you turn to the compiled versions, see [tensorflow on arm](https://github.com/lhelontra/tensorflow-on-arm/releases).  But first ,run\r\n`strings /lib/arm-linux-gnueabihf/libm.so.6|grep GLIBC`\r\nto check whether your device are supported by GLIBC 2.23.", "@forskamse ok,thank you !", "@forskamse thx this hellped me pass the optimiser compilation error", "> I have solved this problem with the help of bro @umang6891 at #12482.\r\n> Follow these steps:\r\n> First, run \u2018export -p\u2019 to confirm the variable HOST_NSYNC_LIB and TARGET_NSYNC are right there.\r\n> If not, run:\r\n> export HOST_NSYNC_LIB=`tensorflow/contrib/makefile/compile_nsync.sh`\r\n> export TARGET_NSYNC_LIB=\"$HOST_NSYNC_LIB\"\r\n> then add it in the file mutex.h(/tensorflow/tensorflow/core/platform/default/mutex.h)\r\n> #include \"tensorflow/contrib/makefile/downloads/nsync/public/nsync_cv.h\"\r\n> #include \"tensorflow/contrib/makefile/downloads/nsync/public/nsync_mu.h\"\r\n> Hope that it can work on you Rpi.\r\n\r\nGlad I could help :)"]}, {"number": 13175, "title": "BUG: same feature column creates duplicate tensors for DNNLinearCombinedRegressor", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.11.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nIt seems that the same feature column creates two tensors for `DNNLinearCombinedRegressor` (see `voc_embed` in graph below). Is this the right behavior we expected?\r\n\r\nThe behavior stems from that feature column is processed by DNN and Linear independently, see code: [dnn](https://github.com/tensorflow/tensorflow/blob/b46340f40fe5e2ec9bfcd385b07cfb914055fb51/tensorflow/python/estimator/canned/dnn_linear_combined.py#L154) and [linear](https://github.com/tensorflow/tensorflow/blob/b46340f40fe5e2ec9bfcd385b07cfb914055fb51/tensorflow/python/estimator/canned/dnn_linear_combined.py#L196)\r\n\r\n### Source code / logs\r\n\r\n```python\r\n# coding: utf-8\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column as fc\r\n\r\nf = fc.embedding_column(\r\n        fc.categorical_column_with_vocabulary_list(\"voc\", [\"a\", \"b\"]),\r\n        dimension=8)\r\ne = tf.estimator.DNNLinearCombinedRegressor(\r\n        model_dir=\"/tmp/tf/test_dnn\",\r\n        linear_feature_columns=[f],\r\n        dnn_feature_columns=[f],\r\n        dnn_hidden_units=[10])\r\n\r\ndef input_fn():\r\n    return ({\"voc\": tf.identity(\r\n                        tf.constant([\"a\", \"b\", \"a\"], name=\"voc_input\"),\r\n                        name=\"identity\")},\r\n            tf.constant([1, 0, 1]))\r\n\r\ne.train(input_fn, steps=10)\r\n```\r\n\r\n![graph-large_attrs_key _too_large_attrs limit_attr_size 1024 run 1](https://user-images.githubusercontent.com/1112263/30632078-63853988-9e19-11e7-94c8-c660c6ed55e2.png)\r\n", "comments": ["It doesn't look like ideal behavior, but perhaps it's not unexpected.  @ispirmustafa ", "yes. it's not ideal. we can improve it.", "How about converting `feature_column` to `_LazyBuilder` at first, and the feeding `_LazyBuilder` to DNN and LR? I guess the solution could solve the problem (and is lightweight).", "Oh, I forget the optimizer problem: if a tensor is owned by both DNN and LR network, does it bring conflict as the tensor is trained by two optimizer?", "Hi, @ispirmustafa . Does anyone work on it? Do you have any good idea? Thanks.", "Hi @facaiy ,\r\nas you suggested using _LazyBuilder works. We should not open _LazyBuilder as a public API though. \r\nalso sending _LazyBuilder around will complicate the underlying functions. \r\nIf you find a way which does not complicate API, would be happy to review it.\r\nnobody is working on it yet. contributions welcome.\r\n\r\n", "Sure, I'd like to take a try. Thank you for explanation, @ispirmustafa .", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Sorry for the delay. I found the problem is in a dilemma, because feature columns use different optimizer for DNN and Linear part.\r\n\r\nI'll close the issue since I have no good idea of how to fix it, and duplication features are acceptable for me.\r\n  "]}, {"number": 13174, "title": "variable assign incorrect", "body": "**When the two assignment operations are the same:**\r\nvar = tf.Variable(1)\r\nassign_1 = var.assign(tf.multiply(var, 2))\r\nassign_2 = var.assign(tf.multiply(var, 2))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(var.initializer)\r\n    sess.run([assign_1, assign_2])\r\n    print(sess.run(var))  # >>  2\r\n\r\n**When the two assignment operations are not the same:**\r\nvar = tf.Variable(1)\r\nassign_1 = var.assign(tf.multiply(var, 2))\r\nassign_2 = var.assign(tf.multiply(var, 3))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(var.initializer)\r\n    sess.run([assign_1, assign_2])\r\n    print(sess.run(var))  # >>  6", "comments": ["This is better question for stackoverflow. (you are missing some control dependencies to make sure assign_2 looks at the updated value of var)", "```python\r\n# coding: utf-8\r\nimport tensorflow as tf\r\n\r\nvar = tf.Variable(1)\r\nassign_1 = var.assign(tf.multiply(var, 2))\r\n\r\nwith tf.control_dependencies([assign_1]):\r\n    assign_2 = var.assign(tf.multiply(var, 2))\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(var.initializer)\r\n    sess.run([assign_2])\r\n    print(sess.run(var))        # = 4\r\n```"]}, {"number": 13173, "title": "Optimizer loss function without desired paramater", "body": "Hi all,\r\nFor Tensorflow, Is it possible to train  a neural network to optimize a loss function, e.g, minimize cost function, without a desired parameter? And what is optimizer in Tensorflow for the objective?\r\nthank you\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 13172, "title": "send/recv operators bug in constructing the graph.", "body": "Running the following snippet several times shows some weird operator in the graph.\r\n```python\r\nimport tensorflow as tf\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    x = tf.constant(1.0)\r\n\r\ngraph_def = tf.get_default_graph().as_graph_def()\r\noptions = tf.RunOptions(output_partition_graphs=True)\r\nmetadata = tf.RunMetadata()\r\nconfig = tf.ConfigProto(device_count={\"CPU\": 8},\r\n                        inter_op_parallelism_threads=1,\r\n                        intra_op_parallelism_threads=1)\r\nsess = tf.Session(config=config)\r\nsess.run(x, options=options, run_metadata=metadata)\r\nfor partition_graph_def in metadata.partition_graphs:\r\n   print \"\\n\" * 5\r\n   print partition_graph_def\r\n```\r\nin the cpu part of graph, it shows\r\n```text\r\nnode {\r\n  name: \"Const/_1\"\r\n  op: \"_Recv\"\r\n  device: \"/job:localhost/replica:0/task:0/cpu:0\"\r\n  attr {\r\n    key: \"client_terminated\"\r\n    value {\r\n      b: false\r\n    }\r\n  }\r\n  attr {\r\n    key: \"recv_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/cpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/gpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device_incarnation\"\r\n    value {\r\n      i: 1\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_name\"\r\n    value {\r\n      s: \"edge_5_Const\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_type\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n}\r\nnode {\r\n  name: \"_send_Const_0\"\r\n  op: \"_Send\"\r\n  input: \"Const/_1\"\r\n  device: \"/job:localhost/replica:0/task:0/cpu:0\"\r\n  attr {\r\n    key: \"T\"\r\n    value {\r\n      type: DT_FLOAT\r\n    }\r\n  }\r\n  attr {\r\n    key: \"client_terminated\"\r\n    value {\r\n      b: true\r\n    }\r\n  }\r\n  attr {\r\n    key: \"recv_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/cpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device\"\r\n    value {\r\n      s: \"/job:localhost/replica:0/task:0/cpu:0\"\r\n    }\r\n  }\r\n  attr {\r\n    key: \"send_device_incarnation\"\r\n    value {\r\n      i: -3155862594799619836\r\n    }\r\n  }\r\n  attr {\r\n    key: \"tensor_name\"\r\n    value {\r\n      s: \"Const:0\"\r\n    }\r\n  }\r\n}\r\nversions {\r\n  producer: 21\r\n}\r\n```\r\nThere is a weird operator in it. ` name: \"_send_Const_0\"` just send tensor from cpu0 to cpu0. This device already has the right result of `Add` operator.\r\nAnd I notice the `send_device_incarnation` is a random number, which generates by [New64](https://github.com/tensorflow/tensorflow/blob/754048a0453a04a761e112ae5d99c149eb9910dd/tensorflow/core/common_runtime/device.cc#L43). \r\n\r\nThe whole log of this snippet available in https://gist.github.com/dzhwinter/3c2257351774260382dd3e130aaa072b. Thanks.", "comments": ["Not sure if this is a bug. Optimizers can do things behind the covers, extra send/recv ops are only a problem if they cause a significant performance penalty.\r\n\r\nThis copy could represent a copy of 1.0 from Python runtime to TensorFlow runtime. Also, constants have special optimizations, you get more pure look at underlying processes by using dynamic values (ie, tf.placeholder) and disabling optimizations with `config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0))`", "This is not a bug. TensorFlow adds one op per tensor that you fetch (and one per tensor that you feed) in order to transfer the data to/from the client. Notice that the op has `\"client_terminated\": true` in its definition, which means that the value will be consumed by the client.\r\n\r\nNote that in versions of TensorFlow since 858e0afcc45c39b6428bf82ab1444323e925cfd8 (i.e. 1.2 onwards) this is actually implemented using the `\"_Retval\"` (and `\"_Arg\"` for feeding) ops, which are a little more self-explanatory."]}, {"number": 13171, "title": "tf.reduce_*(mean/sum) runs very slow on GPU", "body": " \r\nI notice that tf.reduce_*(mean/sum) runs very slow on GPU in some cases, which can happen in the following simple examle:\r\n\r\nx = tf.Variable(tf.ones([80, 80, 80, 80])) # 4-D tensor.\r\ny = tf.reduce_sum(x, [0, 2, 3]) # Sum over all dims except the 2nd.\r\n\r\nThe execution time on GPU is very large and is approximate same as (or more than) the time on CPU, which probably means the GPU is not used at all. The same result can be obtained by choosing the other axes, except for the first and the last axes, in which case the execution on GPU is significantly faster than CPU.\r\n\r\nHere is the [code](https://gist.github.com/vs-zhehangd/8a547094cfa0efc181b814bfb20b31ce) that reproduces the problem. You can run with `--keep_dim k` k=0,1,2,3 to select different axes.\r\n\r\nI am using the following PC system:\r\n\r\n* Kubuntu 16.04\r\n* TensorFlow 1.2.1 ('v1.2.0-5-g435cdfc', '1.2.1')\r\n* Python 2.7\r\n* GeForce GTX 1080 Ti\r\n* CUDA-8.0\r\n\r\nThe execution time on GPU, CPU, and NumPy is given as follows:\r\n\r\n exec time (s) | [1,2,3] | [0,2,3] | [0,1,3] | [0,1,2] |\r\n|---------------|---------|---------|---------|---------|\r\n| GPU           | 0.00918 | 0.40572 | 0.55388 | 0.01905 |\r\n| CPU           | 0.05921 | 0.22461 | 0.56524 | 0.16172 |\r\n| NumPy         | 0.24799 | 0.24847 | 0.24886 | 0.26601 |\r\n\r\n\r\nSimilar result was observed also on my laptop.\r\n\r\n-------------------------------------------------------\r\n\r\nI met this problem when I tried to implement Batch Normalization (initially when I ran tf.nn.moment(), then realized the key was tf.reduce_*). I wanted to collect means of different channels. Using NHWC data format the problem does not matter as C is the last dimension. However, slow execution occurs if I want to implement for NCHW format as C is the second dimension, this makes the execution time on Batch Normalization overwhelms all other ops such as convolutions. This is surely a nightmare for training and evaluation.\r\n\r\nI hope it will be fixed if it is a bug. Or, if the reason is that the oprations are just not implemented for GPU by now, I wonder if there is a way to walk around.", "comments": ["I have GTX 1080 and get much better numbers using TF head from Sep 13\r\n\r\n(sep13) yaroslav@home:~/git0/stuff$ python reduce_sum_runs_slow.py --keep_dim 0\r\nnp time    0.20407\r\ntf time    0.01083\r\n\r\n(sep13) yaroslav@home:~/git0/stuff$ python reduce_sum_runs_slow.py --keep_dim 1\r\nnp time    0.21019\r\ntf time    0.01631\r\n\r\n(sep13) yaroslav@home:~/git0/stuff$ python reduce_sum_runs_slow.py --keep_dim 2\r\nnp time    0.20842\r\ntf time    0.02032\r\n\r\n(sep13) yaroslav@home:~/git0/stuff$ python reduce_sum_runs_slow.py --keep_dim 3\r\nnp time    0.33465\r\ntf time    0.01350", "I tested the benchmark program on four different machines (one uses 0.12 and the rest all use 1.2r) and the problem all occurs. I upgraded TensorFlow from 1.2 to 1.3 via pip on my main server and the problem remains the same.\r\n\r\nThen I compiled the latest source cloned this morning. it seems that this problem has been fixed. : )", "It's easy to test with latest version, no recompilation needed. IE, get the whl link from tensorflow github  front page, then\r\n\r\n```\r\nconda create --name sep13 --clone my_previous_env\r\nsource activate sep13\r\npip install --upgrade https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3.5,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-1.3.0-cp35-cp35m-linux_x86_64.whl\r\npython run_my_script.py\r\n```", "@yaroslavvb  I got the same problem on tf 1.3.0-rc2(installed by pip install). Does it mean that I should compile the source code?", "I am also hitting this problem. What is the suggested solution or workarounds if any.", "It's been fixed, so upgrade to latest version"]}, {"number": 13170, "title": "Updating protobuf and llvm hashes.", "body": "", "comments": ["@av8ramit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @kirilg to be potential reviewers.", "Jenkins, test this please.", "```\r\n17:09:19          -- SHA256 hash of\r\n17:09:19              C:/tf_jenkins/home/workspace/tensorflow-pr-win-cmake-py/cmake_build/downloads/a6f29d8ac48d63293f845f2253eccbf86bc28321.tar.gz\r\n17:09:19            does not match expected value\r\n17:09:19              expected: '75d40ea8e68b0d1644f052fffe8f14a410b2a73d40ccb859a95c0578d194ec26'\r\n17:09:19                actual: '1e40863d9f15dd6b15f8f18f49c500944be6118d9fe17e9dc58a1c709cadbb8a'\r\n17:09:19          -- Hash mismatch, removing...\r\n17:09:19          CMake Error at gemmlowp-stamp/download-gemmlowp.cmake:157 (message):\r\n17:09:19            Each download failed!\r\n```\r\nLet's switch cmake build to only use our bazel mirrors."]}]