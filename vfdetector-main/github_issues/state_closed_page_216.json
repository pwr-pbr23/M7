[{"number": 48145, "title": "Missing Add* function for Builtin operator FILL", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from (source or binary): Source built locally\r\n- Tensorflow version (commit SHA if source): 771c870a81c1025c4886a4fb60ca33971e98c577\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): nRF52840 DK\r\n\r\n**Describe the problem**\r\nI successfully converted my model to TF Lite for Microcontrollers and I am now trying to consume it from an nRF52840 DK target, but I get a failure when allocating the tensors:\r\n```\r\n[ERR] ./model/debug_log.cc:12: Didn't find op for builtin opcode 'FILL' version '1'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?\r\n[ERR] ./model/debug_log.cc:12: \r\n[ERR] ./model/debug_log.cc:12: Failed to get registration from op code FILL\r\n[ERR] ./model/debug_log.cc:12: \r\n[ERR] ./model/debug_log.cc:12: Failed starting model allocation.\r\n[ERR] ./model/debug_log.cc:12: \r\n[ERR] ./model/debug_log.cc:12: AllocateTensors() failed\r\n```\r\n\r\nBeing proprietary software, unfortunately, I cannot share more details about the model used.\r\n\r\nI already have a proposed resolution of the issue and I'm opening this Issue ticket in order to open a PR.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nThe first step is building TFLM locally:\r\n```\r\ncd tensorflow\r\nmake -f tensorflow/lite/micro/tools/make/Makefile \\\r\n    TARGET=cortex_m_generic \\\r\n    TARGET_ARCH=cortex-m4+fp \\\r\n    TARGET_TOOLCHAIN_ROOT=/opt/gcc-arm-none-eabi-9-2020-q2-update/bin/ \\\r\n    OPTIMIZED_KERNEL_DIR=cmsis_nn microlite\r\n```\r\nOnce `libtensorflow-microlite.a` is built I can include it as a library in the Makefile of my project.\r\n\r\nThe second step is to consume my  TFLM model from th target nRF52840. The code fails within this function call:\r\n```\r\nvoid model_init(){\r\n\t// Set up logging\r\n\tstatic tflite::MicroErrorReporter micro_error_reporter;\r\n\terror_reporter = &micro_error_reporter;\r\n\r\n\t// Map the model into a usable data structure\r\n\tmodel = ::tflite::GetModel(g_model);\r\n\tif (model->version() != TFLITE_SCHEMA_VERSION) {\r\n\t\tTF_LITE_REPORT_ERROR(error_reporter,\r\n\t\t\t\t\"Model provided is schema version %d not equal \"\r\n\t\t\t\t\"to supported version %d.\\n\",\r\n\t\t\t\tmodel->version(), TFLITE_SCHEMA_VERSION);\r\n\t}\r\n\r\n\t// This pulls in all the operation implementations we need\r\n\tstatic tflite::AllOpsResolver resolver;\r\n\tresolver.AddExpandDims();\r\n\r\n\t// Build an interpreter to run the model with.\r\n\tstatic tflite::MicroInterpreter static_interpreter(\r\n\t\t\tmodel, resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n\tinterpreter = &static_interpreter;\r\n\r\n\t// Allocate memory from the tensor_arena for the model's tensors.\r\n\tTfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n\tif (allocate_status != kTfLiteOk){\r\n\t\tTF_LITE_REPORT_ERROR(error_reporter, \"AllocateTensors() failed\");\r\n\t\treturn;\r\n\t}\r\n\r\n\t// Obtain pointers to the model's input and output tensors.\r\n\tinput = interpreter->input(0);\r\n\toutput = interpreter->output(0);\r\n}\r\n```", "comments": ["Hi,\r\n\r\nI created a PR with a fix [(48144)](https://github.com/tensorflow/tensorflow/pull/48144).\r\n\r\nApologies if did not respecting your contributing guidelines this time.\r\n\r\nCheers", "Hi, could you please tell me how big is your libtensorflow-microlite.a?", "> Hi, could you please tell me how big is your libtensorflow-microlite.a?\r\n\r\nHi @napoleonwar, my `libtensorflow-microlite.a` seems to be 1.0 MB on my host. I currently don't know the size on the target though.", "> Hi @napoleonwar, my `libtensorflow-microlite.a` seems to be 1.0 MB on my host. I currently don't know the size on the target though.\r\n\r\nOk Thanks! I am wondering how big is it after you compiling the hello world example if you did before. Because in my case, the binary file is 350KB for this simplest example, it will become bigger for more complex cases. Did you get some result whose size is under 100KB by using TFLM?"]}, {"number": 48144, "title": "Added the Add* function for the missing Builtin operator Fill", "body": "Fixes #48145\r\n\r\nAdded the Add* function for the missing Builtin operator Fill in TFLM. I can now consume Fill op in my Cortex-M micro-controller.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48144) for more info**.\n\n<!-- need_sender_cla -->", "@advaitjain coud you review this PR?", "Hi @abattery and @advaitjain,\r\n\r\nI was not aware of the contributing guidelines for TFLM. I have just opened the issue [48145](https://github.com/tensorflow/tensorflow/issues/48145) to be linked with this PR. I'm still in the process of learning your guidelines, I'll try now to link PR and Issue.\r\n\r\nThanks for your help.", "@googlebot I signed it!"]}, {"number": 48143, "title": "Fix xtensa_download.sh to appropriately return error string to Makefile.", "body": "Since we call the script via the Makefile, we need to ensure that the only logs on stdout (when the script is successful) SUCCESS. Every other log should either be directed to /dev/null or to stderr.\r\n\r\nManually confirmed that the following two commands now succeed:\r\n```\r\nrm -rf tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade run_keyword_benchmark -j8\r\n```\r\n\r\nPrior to this change, we would get the following error:\r\n```\r\ntensorflow/lite/micro/tools/make/ext_libs/xtensa.inc:5: *** Something went wrong with the xtensa download: ~/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4 ~/github/tensorflow Initialized empty Git repository in /home/advaitjain/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4/.git/ ~/github/tensorflow SUCCESS.  Stop.\r\n```\r\n\r\nAlso confirmed that:\r\n```\r\ntensorflow/lite/micro/tools/make/downloads/xa_nnlib_hifi4\r\n```\r\nis a git repo with uncommitted changes (i.e. the patch was properly applied).\r\n\r\nThe depthwise_conv kernel test is failing, likely because of https://github.com/tensorflow/tensorflow/pull/47950\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=fusion_f1 XTENSA_CORE=F1_190305_swupgrade test_kernel_depthwise_conv_test -j8\r\n```\r\n\r\ngives:\r\n```\r\nTesting Int8Input32x1Filter32x1ShouldMatchGolden\r\ntensorflow/lite/micro/kernels/xtensa/depthwise_conv.cc:338 required_scratch > 0 was not true.\r\nkTfLiteOk == tflite::testing::ValidateDepthwiseConvGoldens( golden_quantized, output_elements, &conv_params, kQuantizationTolerance, kTensorsSize, tensors) failed at tensorflow/lite/micro/kernels/depthwise_conv_:936 (0 vs 1)\r\n2/3 tests passed\r\n~~~SOME TESTS FAILED~~~\r\n```\r\n\r\nProgress towards http://b/183497550", "comments": ["Tagging @nyadla-sys"]}, {"number": 48142, "title": "tensforflow stucks at the installation", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Family\r\n- TensorFlow version:  2.4.1\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): No \r\n- GCC/Compiler version (if compiling from source): none\r\n- CUDA/cuDNN version: CUDA 11.2\r\n- GPU model and memory: GTX 1660 Ti\r\n\r\n\r\n\r\nI wanted to install tensorflow but the installation stucks.\r\n\r\nI did a pip install. tensorflow had started downloading. And then this installation got stuck...\r\n", "comments": ["@LucasColas \r\nCould you please down grade your Cuda to 11.0 and let us know if you face the same issue as Cuda 11.2 is compatible from tf-nightly [2.5].\r\nAlso please verify if you meet the system requirements:\r\n#47720,#43030,#47517", "I'm not a big to down grade Cuda. ", "@LucasColas \r\nCuda 11.2 is compatible with tf-nightly and you are using tf 2.4", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48142\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48142\">No</a>\n"]}, {"number": 48141, "title": "micro: L2_POOL_2D flatbuffer fix", "body": "Fix for shape data for tensors sometimes existing in the flatbuffer, which is supposed to be read-only.  L2_POOL_2D now modifies the tensor shape data after relocating it.\r\n\r\nAdditional fix for Issue #47814", "comments": [":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", "> Could you move the RelocateTensorDims function to kernels/kernel_util.* instead? We try to keep kernel code separate from the general micro interpreter code as much as possible, so in theory the kernels could be redeployed without the wider framework.\r\n> Also, renaming the function to CreateWritableTensorDimsCopy() or something similar might be clearer?\r\n\r\n@petewarden Fixed."]}, {"number": 48140, "title": "Global barrier in TensorFlow", "body": "\r\nIn paper https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf, there mentioned a concept **global barrier** in Tensorflow between successive iterations.\r\n _\u2013 the\r\nglobal barrier waits for all communication operations to\r\nfinish before moving on to the next iteration_\r\n\r\nBut not found any discuss on this, want to make sure that is there really a **global barrier** in Tensorflow?\r\n", "comments": ["This seems more like a stack overflow question than a bug, in the future, please send questions like to stack overflow please.\r\n\r\nI'm not 100% sure, but I assume this depends on how you're _using_ TensorFlow. \r\n\r\nFor example if you write a `train_step` `tf.function` and call it repeatedly from python, the python call may be equivalent to this \"global barrier\". But calling `train_step` multiple times within a single `tf.function` may get you around this restriction.\r\n\r\nOr if you're using `keras.Model.fit`, then the `experimental_steps_per_execution` argument to `.compile` is the workaround. [example here](https://www.tensorflow.org/guide/tpu)."]}, {"number": 48139, "title": "Update configure.py", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48139) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 48138, "title": "Error: Report to Tensorflow team, \"Input 0 of layer sequential is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1, 60)\"", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux 5.11.10-arch1-1\r\n- TensorFlow installed from (source or binary): Anaconda\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8.8\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: 3060 Ti FE\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6218233/tf_env.txt)\r\n\r\n**Describe the current behavior**\r\nWhile using tensorflow and keras to create a LSTM model I get the error attached.\r\n\r\n**Describe the expected behavior**\r\nI expect to get no error and to be able to continue finalizing the model.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n[Colab](https://colab.research.google.com/drive/1UKkZilpQj34lv6GvWOMC-g-pLarcb1dY?usp=sharing)\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n[output.log](https://github.com/tensorflow/tensorflow/files/6218216/output.log)", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48138\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48138\">No</a>\n", "I was able to fix this error by reshaping x_train_data."]}, {"number": 48137, "title": "Output size same as input size only if strides = 1", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\r\n\r\n## Description of issue (what needs changing):\r\nIn the description of the input argument \"padding\", it says:\r\n\r\n_one of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input._\r\n\r\nHowever, \"same\" would make the output have the same dimensions as the input only if strides is set to be 1. I was initially very confused by this typo.\r\n\r\n", "comments": ["Created [a CL](https://critique-ng.corp.google.com/cl/365992643) which specifies the point about `strides=1`.", "Closing since the associated PR is now merged. Thanks!"]}, {"number": 48136, "title": "equations are terribly formatted", "body": "hello, \r\nif I change the language (e.g. German) in the documentation, the equations are terribly formatted. This problem appears e.g. [here](https://www.tensorflow.org/agents/tutorials/intro_bandit). In English language, every looks fine. Nevertheless, as soon as I change the language, there seems to be a formatting issue.\r\nBest Regards\r\n", "comments": ["@FeU-aKlos \r\nCan you please check the same on different browsers and confirm as we do not face the same issue.", "> @FeU-aKlos\r\n> Can you please check the same on different browsers and confirm as we do not face the same issue.\r\n\r\n\r\n# how it should look like:\r\n![image](https://user-images.githubusercontent.com/28005881/112882327-1a5e1180-90cd-11eb-8668-58e7da984162.png)\r\n# how it looks in another language:\r\n![image](https://user-images.githubusercontent.com/28005881/112882438-44173880-90cd-11eb-83fd-a6ecccc675a6.png)\r\n\r\n\r\nI tested it in google chrome and firefox as well as on a different computer. The equations are only correct formatted in English language.", "This is an issue with the site-infrastructure.\r\n\r\nI've reported it to the internal owners.\r\n\r\nb/186121200", "Hi @FeU-aKlos ! I think this issue has been addressed now. Please let us know if we can move this issue to closed status or not? Thanks!", "@mohantym the formatting is now correct, for the selectable languages. Nevertheless, German is now **not** anymore selectable. Not sure if this is the right way of adressing the issue, by just dropping the languages in which the problem was present. But yeah, I guess so far the issue can be closed.", "Ok @FeU-aKlos ! Closing this issue as it has been resolved in latest versions(2.8). Thanks!", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 48135, "title": "Removing dependency on full TF", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: aarch64 /Ubuntu 18.04\r\n- TensorFlow installation:  Package\r\n- TensorFlow library: 2.4.1\r\n\r\n### 2. Code\r\n\r\nI have taken first step to using TF Lite by using interpreter from full Tensorflow.   This code does the inferencing and there is separate code to do the modelling:\r\n\r\n[https://gitlab.com/john---/xmit_processor/-/blob/tflite/xmit_processor/xmit_processor.py](url)\r\n\r\nThis code take around 2% less RAM (out of 4GB) than the full Tensorflow version.   However, I think tot get the full benefit I need to use the Tensorflow Lite runtime.   However, for inference I am using many tf.* functions.   A couple of them:\r\n\r\ntf.audio.decode_wav\r\ntf.signal.stft\r\n\r\nFrom what I can see in the documentation these are not available in TF Lite.   Let me know:\r\n\r\n-  To get the minimized memory footprint of TF Lite do I need to use TF Lite runtime?\r\n- Do I need to find TF Lite (tfl.\\*) alternates for all the tf.\\* functions?\r\n- Any other paths I can take to decrease memory footprint?\r\n\r\nNote: the training currently works with full Tensorflow but I can make changes to that part as needed to improve memory consumption on the inferencing side.\r\n", "comments": ["TFLite is optimized for the mainly ARM devices and inference use cases. It can reduce overall memory footprint by using only carrying inference features only.\r\n\r\n- If possible, you might find an alternative option in programming language for tf.audio.decode_wav function out of graph, which is usually needed at the beginning of the graph.\r\n- You can find alternatives in tfl dialect or you can implement your own stft op by creating a custom op. https://www.tensorflow.org/lite/guide/ops_custom\r\n- If you can quantize your model, you can easily decrease your tensor memory by half. https://www.tensorflow.org/lite/performance/model_optimization", "Thanks for the detailed response.\r\n\r\nBased on the the first two items it seems like the biggest gain would be to not import Tensorflow but to remove the dependency on it by reimplementing those functions I am currently using from  the tensorflow import.\r\n\r\nIs it fair to say the the model itself is not really what what is bringing in the overhead but the functions needed to prepare the data for inference?  Or is that not really relevant because data preparation can be moved to the model itself?", "Sorry, I don't have a good answer of your question related to the overhead expectations.\r\n\r\nIn my best guess, the overhead should be analyzed per model since each model has different characters.\r\n\r\nFor the details of the overhead analysis, please consider using the TFLite benchmark tool set to analyze overall memory overhead: https://www.tensorflow.org/lite/performance/measurement\r\n\r\n", "Thanks.   I guess I distinguished the model from the data preparation.   Sounds like the benchmark tool will give be a complete picture.", "I did some tests which indicate to me that the main contributor to memory  utilisation is the TensorFlow framework and not the model.\r\n\r\nWithout any model loaded:\r\n\r\n`\r\n/usr/bin/time -f '%M' python -c \"import tensorflow as tf\"\r\n`\r\n349296\r\n`\r\n/usr/bin/time -f '%M' python -c \"import tflite_runtime.interpreter as tflite\"\r\n`\r\n28644\r\n\r\nWith the model:\r\n\r\n`\r\n/usr/bin/time -f '%M' python -c \"import tensorflow as tf; model = tf.keras.models.load_model('./1')\"\r\n`\r\n440056\r\n`\r\n/usr/bin/time -f '%M' python -c \"import tflite_runtime.interpreter as tflite; interpreter = tflite.Interpreter('./model_1.tflite')\"\r\n`\r\n30728\r\n\r\nBased on this the full framework (no nodel loaded) takes 12 times the memory as the runtime framework.   If I don't replace the tf.\\* functions with python or tfl.\\* equivalents any changes to the model will have much smaller gains in comparison.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48135\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48135\">No</a>\n"]}, {"number": 48134, "title": "Update python package description to include python 3.9", "body": "As tf-nightly have python 3.9 available, this PR updates\r\nthe python package description to include python 3.9 entry.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Should this commit be cherry-picked to the [r2.5 branch](https://github.com/tensorflow/tensorflow/tree/r2.5)?\r\n"]}, {"number": 48133, "title": "Update sqlite to latest sqlite-amalgamation-3350300", "body": "This PR updates sqlite to the latest sqlite-amalgamation-3350300\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 48132, "title": "Changed one comment for better clarity and grammar", "body": "Small grammatical update to the api_template._init_py file!\r\n", "comments": []}, {"number": 48131, "title": "fixed a grammatical error", "body": "fixed a small grammatical mistake\r\n![recommend](https://user-images.githubusercontent.com/47449929/112759408-88d29f00-8ffb-11eb-9804-2a502e45ddf0.png)\r\n", "comments": ["We will not be encouraging one liner grammatical changes as this is expensive process, thank you for your interest.\r\nCC @mihaimaruseac ", "Please fix typos in the entire file/directory."]}, {"number": 48129, "title": "SparseCategoricalCrossentropy and sparse_categorical_crossentropy are different due to precision loss", "body": "The following code demonstrates the issue:\r\n```\r\nfrom tensorflow.keras.losses import *\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nloss_fn = SparseCategoricalCrossentropy(reduction='none')\r\ny_true = np.random.randint(10, size = (4, 8))\r\ny_pred = np.random.uniform(low=-10, high=10, size=(4,8,10))\r\n\r\nl1 = loss_fn(y_true, y_pred).numpy()\r\nl2 = sparse_categorical_crossentropy(y_true, y_pred).numpy()\r\nprint(np.array_equal(l1, l2), l1[0][0], l2[0][0])\r\n```\r\nAccording to TensorFlow documentation and expectations (see f.e https://stackoverflow.com/questions/59360263/tensorflow-2-0-what-is-the-difference-between-sparse-categorical-crossentropy-a) the two methods of computing the loss should always yield the exact same result. But that is not the case because the `SparseCategoricalCrossentropy` code path calls `compute_weighted_loss` and in that function the loss is converted to float32, losing precision. A comment in the function reads: \"TODO(psv): Handle casting here in a better way, eg. if losses is float64 # we do not want to lose precision.\"", "comments": ["@bjourne \r\nI ran the code shared on tf 2.4 and 2.5 stable version, here until five places after decimal the values are same.", "Yep, but *all* decimal places should be exactly the same. :)", "I ran the code on tf nightly and upto seven decimals the values match, please refer to the [gist here](https://colab.research.google.com/gist/Saduf2019/806ee04ea749c79669a33384b22e3e01/untitled591.ipynb)", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "But the calculations should be exactly identical, not only to *n* decimal places. The comment in the code even says there is a bug here.", "Probably related.\r\n\r\nTwo identical inputs gives different results with `categorical_crossentropy()` and `sparse_categorical_crossentropy()`:\r\n\r\n```python\r\n>>> tf.__version__\r\n'2.5.0'\r\n\r\n>>> tf.keras.losses.categorical_crossentropy([[1, 0]], [[.0, .1]]).numpy()\r\narray([16.118095], dtype=float32)\r\n\r\n>>> tf.keras.losses.sparse_categorical_crossentropy([0], [[.0, .1]]).numpy()\r\narray([13.815511], dtype=float32)\r\n```\r\n\r\nFirst result is correct. Manual check:\r\n\r\n```python\r\n>>> -np.log(1e-7)  # epsilon, fuzz factor\r\n16.11809565095832\r\n```\r\n\r\nShould I open new issue for this?", "@bjourne I tried to replicate the issue on colab using tf-nightly(2.9.0.dev20220309) and didn't face the issue reported here.Please have a look at the gist [here](https://colab.research.google.com/gist/sushreebarsa/b24f8177f9d10339dbf4a12e97b9a6c6/gist48129.ipynb) and let us know if it helps?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48129\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48129\">No</a>\n", "> @bjourne I tried to replicate the issue on colab using tf-nightly(2.9.0.dev20220309) and didn't face the issue reported here.Please have a look at the gist [here](https://colab.research.google.com/gist/sushreebarsa/b24f8177f9d10339dbf4a12e97b9a6c6/gist48129.ipynb) and let us know if it helps?Thanks!\r\n\r\nLooks like it has been fixed in tf 2.9. \r\n", "@bjourne Thank you for the confirmation!\r\nGlad it is working fine for you.\r\nThanks!"]}, {"number": 48128, "title": "name parameter not working in tf operations (colab)", "body": "System information\r\n-google colab, standard free account, didn't touch any config, no accelerator selected\r\n\r\nI was using the functional API because I have to write a network with two outputs, and I noticed that the the name parameter doesn't work in most of the tf operations. In this simple code I wrote below **b** should be the equal to **a** with the name changed, but it doesn't work, it simply ignores the parameter.\r\n\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION)\r\na=tf.keras.Input((1,))\r\nb=tf.identity(a, name=\"my_name\")\r\nprint(b.name)\r\n\r\nOUTPUT:\r\nv2.4.1-0-g85c8b2a817f 2.4.1\r\ntf.identity/Identity:0\r\n\r\n\r\nI also tried different operations, in al of them the name= parameter was ignored\r\nhere is the link to my colab notebook\r\nhttps://colab.research.google.com/drive/1C9TR3sTgydES39LV8oPf5gz1zW9NgvJT?usp=sharing\r\n\r\nIt wouldn't be such a big deal, but when writing a network with multiple outputs (and custom losses) the documentation is not that good and only explains how to do it calling the layers by name, so now I'm pretty much stuck, cause the auto-assigned name changes based on the exectution.", "comments": ["I tried downgrading tensorflow, and apparently before 2.4.0 the parameter worked with identity and others, but not all tf operations (I noticed that for example tf.abs ignored the parameter)", "@S99amuele,\r\nLooks like this is simliar to issue #45401, #43844 and #43840.\r\n\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/45401#issuecomment-738957954) from a member of the TensorFlow team for more information. Thanks!", "@amahendrakar,\r\nthanks, looks like it's for that reason. I will try using lambda layers with no operation.\r\nIt kinda seems a bit unpolished though, someone should consider implementing a tf.rename funtion or something like that\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48128\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48128\">No</a>\n"]}, {"number": 48127, "title": "Conv2D is 20x slower using mixed_float16 in training process", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `v2.4.0-49-g85c8b2a817f 2.4.1`\r\n- Python version: `3.7.6`\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: `CUDA Version: 11.2`\r\n- GPU model and memory: `Quadro RTX 8000, compute capability 7.5` and also `GeForce RTX 2080 Ti, compute capability 7.5`\r\n\r\n**Describe the current behavior**\r\n- `Conv2D` in training process is `~20x` slower if set `mixed_precission` `mixed_float16`. In my test, this happens in some conditions. In a short result, it's:\r\n  | input_shape | batch_size | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |\r\n  | ----------- | ---------- | -------- | --------------------- | ---------------------- |\r\n  | 112         | 512        | False    | 83                    | 67                     |\r\n  | 112         | 512        | True     | 1969                  | 1857                   |\r\n\r\n  Seems like `mixed_float16` is `~20x` times slower than `float32`.\r\n\r\n**Describe the expected behavior**\r\n- Expect `mixed_float16` performs better than `float32`.\r\n\r\n**Standalone code to reproduce the issue**\r\n- I just used a minimal python script to test this scenario, which seems easier for me:\r\n  ```py\r\n  import tensorflow as tf\r\n  from tensorflow import keras\r\n\r\n  gpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\n  for gpu in gpus:\r\n      tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n  def test_resnet(input_shape=None, classes=1000):\r\n      img_input = keras.layers.Input(shape=input_shape)\r\n      nn = img_input\r\n\r\n      # nn = keras.layers.ZeroPadding2D(padding=((2, 2), (2, 2)), name='conv1_pad')(nn) # NOT helping\r\n      # nn = keras.layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(nn) # NOT helping\r\n      nn = keras.layers.Conv2D(64, 7, strides=2, use_bias=False, padding='VALID', name=\"conv1_conv\")(nn)\r\n      # nn = keras.layers.Conv2D(64, 7, strides=2, use_bias=False, padding='VALID', name=\"conv1_conv\", data_format=\"channels_first\")(nn)\r\n\r\n      nn = keras.layers.Flatten()(nn)\r\n      nn = keras.layers.Dense(classes, name=\"predictions\")(nn)\r\n      nn = keras.layers.Activation('softmax', dtype='float32')(nn)\r\n      return keras.models.Model(img_input, nn)\r\n\r\n  def load_cifar10(batch_size=1024, image_shape=(32, 32)):\r\n      import tensorflow_datasets as tfds\r\n      AUTOTUNE = tf.data.experimental.AUTOTUNE\r\n\r\n      if image_shape[:2] == (32, 32):\r\n          preprocessing = lambda data: (tf.cast(data[\"image\"], tf.float32) / 255.0, data[\"label\"])\r\n      else:\r\n          preprocessing = lambda data: (tf.image.resize(data[\"image\"], image_shape[:2]) / 255.0, data[\"label\"])\r\n          # preprocessing = lambda data: (tf.transpose(tf.image.resize(data[\"image\"], image_shape[1:]) / 255.0, [2, 0, 1]), data[\"label\"])\r\n      dataset = tfds.load(\"cifar10\", split=\"train\").map(preprocessing, num_parallel_calls=AUTOTUNE)\r\n      dataset = dataset.cache().batch(batch_size).prefetch(buffer_size=AUTOTUNE)\r\n      return dataset\r\n\r\n  def run_test(input_shape=(32, 32, 3), batch_size=512, use_fp16=False, epochs=2):\r\n      if use_fp16:\r\n          keras.mixed_precision.set_global_policy('mixed_float16')\r\n          # tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\r\n\r\n      dataset = load_cifar10(batch_size=batch_size, image_shape=input_shape)\r\n\r\n      # model = keras.applications.ResNet50(input_shape=input_shape, classes=10, weights=None)\r\n      # model = keras.models.Model(model.inputs[0], keras.layers.Activation(\"linear\", dtype=\"float32\")(model.outputs[0]))\r\n      model = test_resnet(classes=10, input_shape=input_shape)\r\n\r\n      # optimizer = keras.mixed_precision.LossScaleOptimizer(keras.optimizers.Adam())\r\n      optimizer = keras.optimizers.Adam()\r\n      model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\r\n      history = model.fit(dataset, epochs=epochs)\r\n\r\n  if __name__ == \"__main__\":\r\n      import sys\r\n      import argparse\r\n\r\n      parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n      parser.add_argument(\"-b\", \"--batch_size\", type=int, default=512, help=\"Batch size\")\r\n      parser.add_argument(\"-i\", \"--input_shape\", type=int, default=32, help=\"Input shape\")\r\n      parser.add_argument(\"-f\", \"--use_fp16\", action=\"store_true\", help=\"Use fp16\")\r\n      parser.add_argument(\"-e\", \"--epochs\", type=int, default=2, help=\"Epochs\")\r\n\r\n      args = parser.parse_known_args(sys.argv[1:])[0]\r\n      run_test((args.input_shape, args.input_shape, 3), args.batch_size, args.use_fp16, args.epochs)\r\n      # keras.backend.set_image_data_format('channels_first') # NOT helping\r\n      # run_test((3, args.input_shape, args.input_shape), args.batch_size, args.use_fp16, args.epochs)\r\n  ```\r\n\r\n**Other info / logs**\r\n- That `Conv2D` layer in `test_resnet` is actually the head layer in our `keras.applications.ResNet50`, and I found it's this layer slowing down the model training speed in `mixed_float16`.\r\n- By changing the `input_shape` and `batch_size`, it seems `mixed_float16` working good in `input_shape == 224, batch_size == 512` or `input_shape == 112, batch_size == 256`.\r\n  | input_shape | batch_size | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |\r\n  | ----------- | ---------- | -------- | --------------------- | ---------------------- |\r\n  | 112         | 512        | False    | 83                    | 67                     |\r\n  | 112         | 512        | True     | 1969                  | 1857                   |\r\n  | 128         | 512        | False    | 96                    | 85                     |\r\n  | 128         | 512        | True     | 96                    | 97                     |\r\n  | 112         | 256        | False    | 43                    | 38                     |\r\n  | 112         | 256        | True     | 46                    | 41                     |\r\n  | 224         | 512        | False    | 233                   | 212                    |\r\n  | 224         | 512        | True     | 228                   | 210                    |\r\n  | 32          | 512        | False    | 8                     | 5                      |\r\n  | 32          | 512        | True     | 38                    | 37                     |\r\n\r\n- In some further tests in my environment:\r\n  - First set `input_shape == 112`, and use different `batch_size`, seems this slowing happens if `batch_size` **>** `384`.\r\n  - Then set `batch_size == 512`, and vary `input_shape`, seems it's happening if `input_shape` **<** `121`.\r\n\r\n  | input_shape | batch_size | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |\r\n  | ----------- | ---------- | -------- | --------------------- | ---------------------- |\r\n  | 112         | 384        | True     | 63                    | 45                     |\r\n  | 112         | 385        | True     | 1407                  | 1353                   |\r\n  | 120         | 512        | True     | 2306                  | 2183                   |\r\n  | 121         | 512        | True     | 90                    | 84                     |\r\n  | 122         | 512        | True     | 88                    | 66                     |\r\n\r\n  Also, if we add a `nn = keras.layers.ZeroPadding2D(padding=((3, 3), (3, 3)), name='conv1_pad')(nn)` layer before `Conv2D`, it will be `input_shape < 115 == 121 - 6`.\r\n- Then I set `input_shape == 112, batch_size == 512`, and change parameters in `Conv2D`, seems `padding=\"SAME\"` working good in `mixed_float16`:\r\n\r\n  | kernel_size | padding | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |\r\n  | ----------- | ------- | -------- | --------------------- | ---------------------- |\r\n  | 7           | SAME    | True     | 77                    | 60                     |\r\n  | 7           | VALID   | True     | 1969                  | 1857                   |\r\n  | 5           | SAME    | True     | 1132                  | 1061                   |\r\n  | 5           | VALID   | True     | 953                   | 948                    |\r\n  | 3           | SAME    | True     | 351                   | 342                    |\r\n  | 3           | VALID   | True     | 339                   | 330                    |\r\n\r\n- In `ResNet50` case, using `padding=\"VALID\"` is `~4times slower` in `mixed_float16` training, but if we set `padding=\"SAME\"` only in the first `Conv2D` layer, it's faster:\r\n\r\n  | padding | float16 | first epoch (ms/step) | second epoch (ms/step) |\r\n  | ------- | ------- | --------------------- | ---------------------- |\r\n  | VALID   | False   | 608                   | 570                    |\r\n  | VALID   | True    | 2622                  | 2397                   |\r\n  | SAME    | False   | 691                   | 656                    |\r\n  | SAME    | True    | 438                   | 396                    |\r\n\r\n- I also tested the forward time only, but it's not a difference for me:\r\n  ```py\r\n  from tensorflow import keras\r\n\r\n  def test_conv(use_fp16=True, padding=\"VALID\"):\r\n      if use_fp16:\r\n          dtype = keras.mixed_precision.Policy('mixed_float16')\r\n      else:\r\n          dtype = \"float32\"\r\n      inputs = tf.ones([512, 112, 112, 3])\r\n      nn = keras.layers.Conv2D(64, 7, strides=2, use_bias=False, padding=padding, name=\"conv1_conv\", dtype=dtype)\r\n      print(nn(inputs).dtype)\r\n\r\n      %timeit nn(inputs)\r\n\r\n  test_conv(use_fp16=True, padding=\"VALID\")  # 5.62 ms \u00b1 82.2 \u00b5s\r\n  test_conv(use_fp16=True, padding=\"SAME\")   # 6.61 ms \u00b1 109 \u00b5s\r\n  test_conv(use_fp16=False, padding=\"VALID\") # 4.77 ms \u00b1 36.6 \u00b5s\r\n  test_conv(use_fp16=False, padding=\"SAME\")  # 5.14 ms \u00b1 53.9 \u00b5s\r\n  ```\r\n- So what did I do wrong here?\r\n\r\n**Related issue**\r\n- I think this may related with issue [#39556](https://github.com/tensorflow/tensorflow/issues/39556) and [#41715](https://github.com/tensorflow/tensorflow/issues/41715). Here is my test results of `float32` / `float16` with `NHWC` / `NCHW`:\r\n  ```py\r\n  time for float32 NHWC: 0.11824584007263184\r\n  time for float32 NCHW: 0.06493830680847168\r\n  time for float16 NHWC: 0.1193842887878418\r\n  time for float16 NCHW: 0.09624576568603516\r\n  ```", "comments": ["I am able to replicate this issue on yf 2.4 and nightly , please find the [gist here](https://colab.research.google.com/gist/Saduf2019/fc475839fb6be70958d587a665f676c4/untitled580.ipynb)", "So does anyone checking this? Also tested `tf-nightly 2.6.0.dev20210401` with `python 3.8.5`, same situation.", "@reedwm Any insights on the root-cause of the issue? Thanks!", "I cannot reproduce, either on TF 2.4 or the latest tf-nightly. I did use different GPUs (I tried both a Titan V and an A100), which could explain it.\r\n\r\nI tried running the first program in the original post with the following commands on TF 2.4.1:\r\n\r\n```python\r\npython conv2d_slow.py --input_shape=112 --batch_size=512\r\npython conv2d_slow.py --input_shape=112 --batch_size=512 --use_fp16\r\n```\r\n\r\nOn a Titan V: In the former case (fp32), I get 54ms/step for the second epoch. In the latter case (fp16), I get 53ms/step.\r\n\r\n@leondgarse, can you confirm I'm running the correct commands? Also, you mentioned you are using CUDA 11.2, but I believe TF 2.4 requires CUDA 11.0. `tf-nightly` currently requires CUDA 11.2 though (and has required it on the version you used), so that shouldn't have caused problems when you ran `tf-nightly`", "Yes, yes, the commands are right, and I just noticed `TF 2.4.1` is actually loading `CUDA 11.0`, but my `nvidia-smi` is reporting `CUDA 11.2`. It's also `CUDA 11.0` I actually installed...\r\n```sh\r\n$ apt list --installed | grep cuda-cudart\r\ncuda-cudart-dev-11-0/now 11.0.221-1 amd64 [installed,local]\r\n\r\n$ locate libcudart.so.11\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudart.so.11.0.221\r\n```\r\n\r\nI just noticed when the lag happens, the CPU usage is very low `<50%`, and GPU usage is very high `>90%`.\r\n```py\r\n# tensorflow 2.4.1\r\nCUDA_VISIBLE_DEVICES='1' python conv2d_slow.py -i 112 -b 512 -f\r\n```\r\n![Selection_340](https://user-images.githubusercontent.com/5744524/115483738-ee751c80-a283-11eb-8cf6-de83a9fc6a22.png)\r\n\r\nWhen normal, CPU usage is high at `600~1000%`, and GPU `~20%`\r\n```py\r\n# tensorflow 2.4.1\r\nCUDA_VISIBLE_DEVICES='1' python conv2d_slow.py -i 112 -b 512\r\nCUDA_VISIBLE_DEVICES='1' python conv2d_slow.py -i 112 -b 256 -f\r\n```\r\n![Selection_341](https://user-images.githubusercontent.com/5744524/115483883-398f2f80-a284-11eb-8efd-5e2b1f533dbf.png)\r\n\r\nActually I used to have an issue running `tf-nightly`, `Could not load dynamic library 'libcusolver.so.11'`. I just made it work by `sudo ln -s libcusolver.so libcusolver.so.11`. Maybe my `tf-nightly` is not running correctly.\r\n\r\nIt's good to know this is not the common case, means I may find a solution. :)", "Updated and tested using `tf-nightly 2.6.0-dev20210420` and `tensorflow 2.5.0-rc1` with `cuda 11.2.2-1` + `libcudart.so.11.2.152` + `cudnn 8.1.1.33` + `nvidia-driver 465.19.01`. No luck, same situation...", "@nluehr can you take a look? I do not have access to any cards with compute capability 7.5\r\n\r\n> Actually I used to have an issue running tf-nightly, Could not load dynamic library 'libcusolver.so.11'. I just made it work by sudo ln -s libcusolver.so libcusolver.so.11. Maybe my tf-nightly is not running correctly.\r\n\r\nUnrelated to this issue, but the problem is you need CUDA 11.2. @sanjoy can we improve the error message if the wrong version of CUDA is used?", "Tested with `pytorch==1.8.1+cu111`, and the result seems fine under the same environment:\r\n- **Result**\r\n\r\n  | input_shape | batch_size | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |\r\n  | ----------- | ---------- | -------- | --------------------- | ---------------------- |\r\n  | 112         | 512        | False    | 84.5125               | 78.9935                |\r\n  | 112         | 512        | True     | 75.3213               | 76.9712                |\r\n  | 224         | 512        | False    | 228.2185              | 215.2186               |\r\n  | 224         | 512        | True     | 228.9982              | 254.9154               |\r\n\r\n- **Script**\r\n  ```py\r\n  import torch\r\n  import time\r\n  import torch.nn as nn\r\n  import numpy as np\r\n  from torchvision import datasets, transforms\r\n  from tqdm import tqdm\r\n  \r\n  \r\n  class TestResNet(nn.Module):\r\n      def __init__(self, input=32, num_classes=10):\r\n          super(TestResNet, self).__init__()\r\n          self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=0, bias=False)\r\n          self.flatten = nn.Flatten()\r\n          conv_out_shape = int(np.ceil(input / 2)) - 3\r\n          self.linear = nn.Linear(64 * conv_out_shape * conv_out_shape, num_classes)\r\n          self.softmax = nn.Softmax(dim=-1)\r\n  \r\n      def forward(self, x):\r\n          out = self.conv1(x)\r\n          out = self.flatten(out)\r\n          out = self.linear(out)\r\n          out = self.softmax(out)\r\n          return out\r\n  \r\n  \r\n  def load_cifar10(batch_size=512, image_shape=(32, 32)):\r\n      train_transforms = transforms.Compose([transforms.Resize(image_shape), transforms.ToTensor()])\r\n      trainset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transforms)\r\n      train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\r\n      total = trainset.data.shape[0]\r\n      steps_per_epoch = int(np.ceil(total / batch_size))\r\n      return train_loader, steps_per_epoch\r\n  \r\n  \r\n  def run_test(input_shape=(32, 32), batch_size=512, use_fp16=False, epochs=2):\r\n      train_loader, steps_per_epoch = load_cifar10(batch_size=batch_size, image_shape=input_shape)\r\n  \r\n      device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n      model = TestResNet(input=input_shape[0], num_classes=10).to(device)\r\n      if use_fp16:\r\n          model = model.half()\r\n  \r\n      optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\r\n      criterion = nn.CrossEntropyLoss()\r\n      for epoch in range(epochs):\r\n          since = time.time()\r\n          for inputs, labels in tqdm(train_loader, \"Epoch {}/{}\".format(epoch + 1, epochs), total=steps_per_epoch):\r\n              inputs = inputs.to(device).half() if use_fp16 else inputs.to(device)\r\n              labels = labels.to(device)\r\n              optimizer.zero_grad()\r\n              outputs = model(inputs)\r\n              loss = criterion(outputs, labels)\r\n              loss.backward()\r\n              optimizer.step()\r\n  \r\n          total_time = time.time() - since\r\n          print(\">>>> Total time: %.4fs, mean: %.4fms\" % (total_time, total_time * 1000 / steps_per_epoch))\r\n  \r\n  \r\n  if __name__ == \"__main__\":\r\n      import sys\r\n      import argparse\r\n  \r\n      parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n      parser.add_argument(\"-b\", \"--batch_size\", type=int, default=512, help=\"Batch size\")\r\n      parser.add_argument(\"-i\", \"--input_shape\", type=int, default=32, help=\"Input shape\")\r\n      parser.add_argument(\"-f\", \"--use_fp16\", action=\"store_true\", help=\"Use fp16\")\r\n      parser.add_argument(\"-e\", \"--epochs\", type=int, default=2, help=\"Epochs\")\r\n  \r\n      args = parser.parse_known_args(sys.argv[1:])[0]\r\n      run_test((args.input_shape, args.input_shape), args.batch_size, args.use_fp16, args.epochs)\r\n  ```", "I just updated my `nvidia-driver` to `465.27`, as well as `cuda-driver==11.2.152-1` `libcudnn8==8.2.1.32-1`. This issue is gone on both my `Quadro RTX 8000` and `GeForce RTX 2080 Ti` now.\r\n\r\n| input_shape | batch_size | use_fp16 | first epoch (ms/step) | second epoch (ms/step) |\r\n| ----------- | ---------- | -------- | --------------------- | ---------------------- |\r\n| 112         | 512        | False    | 71                    | 59                     |\r\n| 112         | 512        | True     | 65                    | 54                     |"]}, {"number": 48126, "title": "tflite model is not running on webcam feed", "body": "**System information**\r\n- OS Platform and Distribution (Linux)\r\n- TensorFlow version (2.4.1):\r\n\r\n\r\nI used TensorFlow pre-trained faster rcnn inception v2 coco model and used my own images and trained them on that model. I exported the frozen inference graph successfully and check it on my webcam , I was working fine and using the code below , I converted the frozen inference graph into tflite model....\r\n\r\nimport logging\r\nlogging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nimport numpy as np\r\nimport pathlib\r\n\r\n#conversion of faster_rcnn_inception_v2_coco_model frozen_inference_graph.pbin to tflite model\r\n\r\nimport tensorflow as tf\r\nimport os \r\n\r\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\r\n\r\n\r\n\r\nconverter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\"frozen_inference_graph.pb\", [\"image_tensor\"], [\"detection_classes\", \"detection_scores\", \"detection_boxes\", \"num_detections\"])\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nmodel = converter.convert()\r\n\r\ntflite_models_dir = pathlib.Path(\"model/\")\r\ntflite_models_dir.mkdir(exist_ok=True, parents=True)\r\ntflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\r\ntflite_model_file.write_bytes(model)\r\n\r\n\r\n#To Quantize the model on Export, Set the optimizations flag to optimize for size\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_quant_model = converter.convert()\r\ntflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\r\ntflite_model_quant_file.write_bytes(tflite_quant_model)\r\n\r\n\r\n But I am not able to load my .tflite model in python for checking if my tflite model is working and using my webcam as input to the model ...For loading the tflite model ,I am using the code below : \r\n\r\n######## Webcam Object Detection Using Tensorflow-trained Classifier #########\r\n#\r\n# Author: Evan Juras\r\n# Date: 10/27/19\r\n# Description: \r\n# This program uses a TensorFlow Lite model to perform object detection on a live webcam\r\n# feed. It draws boxes and scores around the objects of interest in each frame from the\r\n# webcam. To improve FPS, the webcam object runs in a separate thread from the main program.\r\n# This script will work with either a Picamera or regular USB webcam.\r\n#\r\n# This code is based off the TensorFlow Lite image classification example at:\r\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/python/label_image.py\r\n#\r\n# I added my own method of drawing boxes and labels using OpenCV.\r\n\r\n# Import packages\r\nimport os\r\nimport argparse\r\nimport cv2\r\nimport numpy as np\r\nimport sys\r\nimport time\r\nfrom threading import Thread\r\nimport importlib.util\r\n\r\n# Define VideoStream class to handle streaming of video from webcam in separate processing thread\r\n# Source - Adrian Rosebrock, PyImageSearch: https://www.pyimagesearch.com/2015/12/28/increasing-raspberry-pi-fps-with-python-and-opencv/\r\nclass VideoStream:\r\n    \"\"\"Camera object that controls video streaming from the Picamera\"\"\"\r\n    def __init__(self,resolution=(640,480),framerate=30):\r\n        # Initialize the PiCamera and the camera image stream\r\n        self.stream = cv2.VideoCapture(0)\r\n        ret = self.stream.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))\r\n        ret = self.stream.set(3,resolution[0])\r\n        ret = self.stream.set(4,resolution[1])\r\n            \r\n        # Read first frame from the stream\r\n        (self.grabbed, self.frame) = self.stream.read()\r\n\r\n\t# Variable to control when the camera is stopped\r\n        self.stopped = False\r\n\r\n    def start(self):\r\n\t# Start the thread that reads frames from the video stream\r\n        Thread(target=self.update,args=()).start()\r\n        return self\r\n\r\n    def update(self):\r\n        # Keep looping indefinitely until the thread is stopped\r\n        while True:\r\n            # If the camera is stopped, stop the thread\r\n            if self.stopped:\r\n                # Close camera resources\r\n                self.stream.release()\r\n                return\r\n\r\n            # Otherwise, grab the next frame from the stream\r\n            (self.grabbed, self.frame) = self.stream.read()\r\n\r\n    def read(self):\r\n\t# Return the most recent frame\r\n        return self.frame\r\n\r\n    def stop(self):\r\n\t# Indicate that the camera and thread should be stopped\r\n        self.stopped = True\r\n\r\n# Define and parse input arguments\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--modeldir', help='Folder the .tflite file is located in',\r\n                    required=True)\r\nparser.add_argument('--graph', help='Name of the .tflite file, if different than detect.tflite',\r\n                    default='mnist_model_quant.tflite')\r\nparser.add_argument('--labels', help='Name of the labelmap file, if different than labelmap.txt',\r\n                    default='labelmap.txt')\r\nparser.add_argument('--threshold', help='Minimum confidence threshold for displaying detected objects',\r\n                    default=0.5)\r\nparser.add_argument('--resolution', help='Desired webcam resolution in WxH. If the webcam does not support the resolution entered, errors may occur.',\r\n                    default='1280x720')\r\nparser.add_argument('--edgetpu', help='Use Coral Edge TPU Accelerator to speed up detection',\r\n                    action='store_true')\r\n\r\nargs = parser.parse_args()\r\n\r\nMODEL_NAME = args.modeldir\r\nGRAPH_NAME = args.graph\r\nLABELMAP_NAME = args.labels\r\nmin_conf_threshold = float(args.threshold)\r\nresW, resH = args.resolution.split('x')\r\nimW, imH = int(resW), int(resH)\r\nuse_TPU = args.edgetpu\r\n\r\n# Import TensorFlow libraries\r\n# If tensorflow is not installed, import interpreter from tflite_runtime, else import from regular tensorflow\r\n# If using Coral Edge TPU, import the load_delegate library\r\npkg = importlib.util.find_spec('tensorflow')\r\nif pkg is None:\r\n    from tflite_runtime.interpreter import Interpreter\r\n    if use_TPU:\r\n        from tflite_runtime.interpreter import load_delegate\r\nelse:\r\n    from tensorflow.lite.python.interpreter import Interpreter\r\n    if use_TPU:\r\n        from tensorflow.lite.python.interpreter import load_delegate\r\n\r\n# If using Edge TPU, assign filename for Edge TPU model\r\nif use_TPU:\r\n    # If user has specified the name of the .tflite file, use that name, otherwise use default 'edgetpu.tflite'\r\n    if (GRAPH_NAME == 'mnist_model_quant.tflite'):\r\n        GRAPH_NAME = 'mnist_model_quant.tflite'       \r\n\r\n# Get path to current working directory\r\nCWD_PATH = os.getcwd()\r\n\r\n# Path to .tflite file, which contains the model that is used for object detection\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,GRAPH_NAME)\r\n\r\n# Path to label map file\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,MODEL_NAME,LABELMAP_NAME)\r\n\r\n# Load the label map\r\nwith open(PATH_TO_LABELS, 'r') as f:\r\n    labels = [line.strip() for line in f.readlines()]\r\n\r\n# Have to do a weird fix for label map if using the COCO \"starter model\" from\r\n# https://www.tensorflow.org/lite/models/object_detection/overview\r\n# First label is '???', which has to be removed.\r\nif labels[0] == '???':\r\n    del(labels[0])\r\n\r\n# Load the Tensorflow Lite model.\r\n# If using Edge TPU, use special load_delegate argument\r\nif use_TPU:\r\n    interpreter = Interpreter(model_path=PATH_TO_CKPT,\r\n                              experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\r\n    print(PATH_TO_CKPT)\r\nelse:\r\n    interpreter = Interpreter(model_path=PATH_TO_CKPT)\r\n\r\ninterpreter.allocate_tensors()\r\n\r\n# Get model details\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\nheight = input_details[0]['shape'][1]\r\nwidth = input_details[0]['shape'][2]\r\n\r\nfloating_model = (input_details[0]['dtype'] == np.float32)\r\n\r\ninput_mean = 127.5\r\ninput_std = 127.5\r\n\r\n# Initialize frame rate calculation\r\nframe_rate_calc = 1\r\nfreq = cv2.getTickFrequency()\r\n\r\n# Initialize video stream\r\nvideostream = VideoStream(resolution=(imW,imH),framerate=30).start()\r\ntime.sleep(1)\r\n\r\n#for frame1 in camera.capture_continuous(rawCapture, format=\"bgr\",use_video_port=True):\r\nwhile True:\r\n\r\n    # Start timer (for calculating frame rate)\r\n    t1 = cv2.getTickCount()\r\n\r\n    # Grab frame from video stream\r\n    frame1 = videostream.read()\r\n\r\n    # Acquire frame and resize to expected shape [1xHxWx3]\r\n    frame = frame1.copy()\r\n    print(frame.shape)\r\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n    print(frame_rgb.shape)\r\n    frame_resized = cv2.resize(frame_rgb, (width, height))\r\n    print(frame_resized.shape)\r\n    input_data = np.expand_dims(frame_resized, axis=0)\r\n    print(input_data.dtype)\r\n\r\n    # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\r\n    if floating_model:\r\n        input_data = (np.float32(input_data) - input_mean) / input_std\r\n\r\n    # Perform the actual detection by running the model with the image as input\r\n    interpreter.set_tensor(input_details[0]['index'],input_data)\r\n    interpreter.invoke()\r\n\r\n    # Retrieve detection results\r\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0] # Bounding box coordinates of detected objects\r\n    classes = interpreter.get_tensor(output_details[1]['index'])[0] # Class index of detected objects\r\n    scores = interpreter.get_tensor(output_details[2]['index'])[0] # Confidence of detected objects\r\n    #num = interpreter.get_tensor(output_details[3]['index'])[0]  # Total number of detected objects (inaccurate and not needed)\r\n\r\n    # Loop over all detections and draw detection box if confidence is above minimum threshold\r\n    for i in range(len(scores)):\r\n        if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\r\n\r\n            # Get bounding box coordinates and draw box\r\n            # Interpreter can return coordinates that are outside of image dimensions, need to force them to be within image using max() and min()\r\n            ymin = int(max(1,(boxes[i][0] * imH)))\r\n            xmin = int(max(1,(boxes[i][1] * imW)))\r\n            ymax = int(min(imH,(boxes[i][2] * imH)))\r\n            xmax = int(min(imW,(boxes[i][3] * imW)))\r\n            \r\n            cv2.rectangle(frame, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\r\n\r\n            # Draw label\r\n            object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\r\n            label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\r\n            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\r\n            label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\r\n            cv2.rectangle(frame, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\r\n            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\r\n\r\n    # Draw framerate in corner of frame\r\n    cv2.putText(frame,'FPS: {0:.2f}'.format(frame_rate_calc),(30,50),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),2,cv2.LINE_AA)\r\n\r\n    # All the results have been drawn on the frame, so it's time to display it.\r\n    cv2.imshow('Object detector', frame)\r\n\r\n    # Calculate framerate\r\n    t2 = cv2.getTickCount()\r\n    time1 = (t2-t1)/freq\r\n    frame_rate_calc= 1/time1\r\n\r\n    # Press 'q' to quit\r\n    if cv2.waitKey(1) == ord('q'):\r\n        break\r\n\r\n# Clean up\r\ncv2.destroyAllWindows()\r\nvideostream.stop()\r\n\r\n\r\nthe Error I am getting After running the code : \r\n\r\nTraceback (most recent call last):\r\n  File \"TFLite_detection_webcam.py\", line 186, in <module>\r\n    interpreter.invoke()\r\n  File \"/root/anaconda3/envs/tf_env/lib/python3.7/site-packages/tensorflow/lite/python/interpreter.py\", line 540, in invoke\r\n    self._interpreter.Invoke()\r\nRuntimeError: Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysTensorArrayV3_0)\r\n\t (while executing 'TensorArrayScatterV3' via Eager)Node number 416 (TfLiteFlexDelegate) failed to invoke.\r\n\r\n^CException ignored in: <module 'threading' from '/root/anaconda3/envs/tf_env/lib/python3.7/threading.py'>\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/tf_env/lib/python3.7/threading.py\", line 1307, in _shutdown\r\n    lock.acquire()\r\nKeyboardInterrupt\r\nFATAL: exception not rethrown\r\nAborted\r\n\r\nthe error is in the screenshot attached \r\n\r\n![Screenshot at 2021-03-27 18-47-57](https://user-images.githubusercontent.com/65131911/112736937-8403e180-8f4e-11eb-9d6a-86f55af0de47.png)\r\n\r\n\r\n", "comments": ["To use TensorArray ops in your model, please consider using tf-nightly or TF 2.5. TensorArray is supported only in TFLite after TF 2.5", "> To use TensorArray ops in your model, please consider using tf-nightly or TF 2.5. TensorArray is supported only in TFLite after TF 2.5\n\nCan You send me a snippet how to use my tflite in TF 2.5 .... and finding my tflite's tensorArray", "You can install a newer tf version by using the pip command:\r\n```pip install tf-nightly```\r\n\r\nTo use the TensorArray support, the TFLite model should be regenerated with the new TF version.", "> You can install a newer tf version by using the pip command:\n> \n> ```pip install tf-nightly```\n> \n> \n> \n> To use the TensorArray support, the TFLite model should be regenerated with the new TF version.\n\nI regenerated my Tflite model with tf-nightly and then using TFLite_detection_webcam.py and\n Webcam is not showing the output results.\ngot the following error: screenshoot attached\n![image](https://user-images.githubusercontent.com/65131911/112750493-6cf9d980-8fb8-11eb-9849-e39711e550c0.jpeg)", "And when I am loading my mnist_model_quant.tflite model in the code below ,\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Load the TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model_path=\"model/mnist_model_quant.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\nprint(interpreter.get_input_details())\r\n\r\n# Get input and output tensors.\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n# Test the model on random input data.\r\ninput_shape = input_details[0]['shape']\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.uint8)\r\ninterpreter.set_tensor(input_details[0]['index'], input_data)\r\n\r\ninterpreter.invoke()\r\n\r\n# The function `get_tensor()` returns a copy of the tensor data.\r\n# Use `tensor()` in order to get a pointer to the tensor.\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\nI am getting the following ... This may help finding my solution\r\n\r\n![1](https://user-images.githubusercontent.com/65131911/112754461-bef82a80-8fcb-11eb-8cc8-067652d8ee5f.png)\r\n", "@abattery Jae sung Chung ", "Looks like the converted tflite is working. For the TFLite_detection_webcam.py's behavior, please ask questions at the script owner at https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi.\r\n\r\n", "> Looks like the converted tflite is working. For the TFLite_detection_webcam.py's behavior, please ask questions at the script owner at https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi.\r\n\r\n\r\nhttps://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi#step-1a-download-and-extract-quantized-ssd-mobilenet-model\r\n\r\nThe link you sent that says that TensorFlow Lite does NOT support RCNN models such as Faster-RCNN! It only supports SSD models.\r\n", "@Zakria96,\r\n\r\nCan you take a look at this [similar issue](https://github.com/tensorflow/tensorflow/issues/37401) for tflite conversion of `FasterRCNN` and also take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/34845#issuecomment-573156336) to modify `pipeline.config`. Let us know if it helps. Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48126\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48126\">No</a>\n"]}, {"number": 48125, "title": "Custom os.path.join that is aware of TF filesystems", "body": "Possibly needed for #48086", "comments": ["@bhack  can you kick off tests here for me? For now just making sure this doesn't break anything", "We could see if this will solve https://github.com/tensorflow/tensorflow/issues/36453", "It looks like some sort of internal review is required because I edited imports. Could one of you look into that please? Thanks\r\n\r\nEdit: nvm, seems like a push got things going", "@bhack  any idea what this failure means? https://source.cloud.google.com/results/invocations/c285665c-289b-4bef-b8ca-3f87efb37c5f/targets/%2F%2Ftensorflow%2Ftools%2Fapi%2Ftests:api_compatibility_test/tests;group=__main__.ApiCompatibilityTest;test=testAPIBackwardsCompatibility;row=1\r\n\r\nThe sanity/CPU failures I looked at, I'm waiting to push a fix until all testing finishes", "If you change the API you need to run https://github.com/tensorflow/tensorflow/issues/44485#issuecomment-787589186", "So I'm supposed to run:\r\n\r\n```\r\nbazel run tensorflow/tools/api/tests:api_compatibility_test -- --update_goldens True\r\n```\r\n\r\nAnd commit whatever changes that makes?", "Yes double check the diff but generally are related to your changes.", "yeah I imagine this is because I am adding `tf.io.gfile.join`.", "@bhack  can you kick off another CI run? I think at least the API and sanity issues should be resolved", "And in the meantime: who do we need for API change approval? Might as well get that ball rolling", "For the API review practices see https://github.com/tensorflow/tensorflow/pull/48161", "@bhack another request for a CI run please. Thank you.", "Thank you. I fixed the 2 sanity linting issues.\r\n\r\nThere's no link to see the Windows GPU results, I have no idea why that is failing.\r\n\r\nFor the plain \"Windows\" results, I don't see it running `ram_file_system_test`. I mirrored your build changes in #48124 to get it to run.", "@tensorflow/api-owners could someone please review this? It mitigates #48086  and unblocks #39609 ", "I see the github hints/autocomplete for /cc @tensorflow/api-owners. The tag is used by a reviewer that currently is also a TF member until we will have an official community reviewer status/team (/cc @theadactyl).", "> For the plain \"Windows\" results, I don't see it running ram_file_system_test. I mirrored your build changes in #48124 to get it to run.\r\n\r\nIt is not only the build change you need to move the test itself as you can see in my PR.", "Oh I thought I did that :facepalm:\n\nOn Fri, Apr 2, 2021, 1:57 PM bhack ***@***.***> wrote:\n\n> For the plain \"Windows\" results, I don't see it running\n> ram_file_system_test. I mirrored your build changes in #48124\n> <https://github.com/tensorflow/tensorflow/pull/48124> to get it to run.\n>\n> It is not only the build change you need to move the test itself as you\n> can see in my PR.\n>\n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/48125#issuecomment-812666232>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANMPP2VWNXL6OCT7ONS663TGYHSXANCNFSM4Z5KRZAQ>\n> .\n>\n", "Actually @gbaned it looks like I _did_ move it in ee7a51028fb04dae61936e702449d493997741e8. Can you kick off CI again so we can see if it's working? Thanks!", "I can't see the results for `Windows Bazel GPU` (there is no `Details` link, at least not for non-googlers) so I don't know what (if anything) is going wrong there. But other than that all tests seem to be passing, including the moved `ram_filesystem_test` under `Windows Bazel`.", "We could be ok with this solution  but  I don't know if passing only current tests It is enought. \nWe could be exposed on other `os.path.join` in the source code that currently have no specific test with the ramfs. Right? \nNeed we to search this string in the whole source code?", "> We could be exposed on other os.path.join in the source code that currently have no specific test with the ramfs. Right?\r\n\r\nI'm not sure I understood. You are saying that `os.path.join` is used somewhere, and in that usage it interacts with the ramfs?\r\n\r\nIf you mean \"how do we know this solves the issue faced in #39609, the answer is \"there is no test specifically for that, but I tested it locally and it does fix the issue. To keep this PR simple, I added only unit tests.\". But It is ~ easy to show via a hacky `os.path.join = <new function>` that the issue in #39609  is resolved, I can clean up https://github.com/adriangb/tensorflow-test to show this.\r\n\r\n> Need we to search this string in the whole source code?\r\n\r\nThat is what I did. I searched all of the source for `os.path.join` and replaced it.", "I meant that many `os.path.join` cases could not have a specific test case with ramfs but if you have already extensively searched all the `os.path.join` occurrences it could be enough. \r\nWe need also to advise to not introduce `os.path.join` in many cases in future commits.", "Just a note, with this we are not strictly aligned with c++ cause we don't have access to filesystem own separator https://github.com/tensorflow/tensorflow/blob/773a5e411689b04437eb6f431dcdbcbbcec49297/tensorflow/core/platform/file_system.h#L399\r\n\r\nIn the Python wrapper https://github.com/samikama/tensorflow/blob/master/tensorflow/python/lib/io/file_io_wrapper.cc", "This assumes that for native/disk filesystems, `os.path.join` \"knows\" the right separator to use and that all other filesystems (`ram`, `gc`) `/` is the right separator. I am not sure when this would not be true (which would mean we should reference the C++ defined separator, like you mention above)?\r\n\r\nAnd _if_ a filesystem was introduced that required this, it would just be a question of (1) exposing that file separator to Python and (2) modifying this custom `join` to account for that. With this in place, this has 0 user facing changes, and would not require any edits beyond the `join` method. Without this in place, well you would basically have to make this PR just for that.", "> This assumes that for native/disk filesystems, os.path.join \"knows\" the right separator to use and that all other filesystems (ram, gc) / is the right separator. I am not sure when this would not be true (which would mean we should reference the C++ defined separator, like you mention above)?\r\n\r\nI meant only that people that are working on c++ need to be aware that every new \"://\" fs that ovveride the parent separator will need python specific changes or it will be ignored. \r\n\r\nBut If we are not going to introduce other \"://\" fs with its own separator in c++ it is not strictly a problem.\r\n\r\n", "> people that are working on c++ need to be aware that every new \"://\" fs that ovveride the parent separator will need python specific changes or it will be ignored\r\n\r\nIsn't this the current status quo though? It's not like `os.path.join` knows about TFs C++ path seps.\r\n\r\n> But If we are not going to introduce other \"://\" fs with its own separator in c++ it is not strictly a problem.\r\n\r\nYep. But at least now the required Python changes will be centralized in a single place and can be done without any impact on end users.\r\n\r\n> But If we are not going to introduce other \"://\" fs with its own separator in c++ it is not strictly a problem\r\n\r\nAgreed, and maybe I'm missing something, but I'm not sure why you'd introduce a filesystem that uses `\\` or `\u03c0` or some other character since `/` is so ubiquitous (outside of Windows).", "Based on https://github.com/tensorflow/tensorflow/pull/39609#issuecomment-818999615 it sounds like the plan is to move forward with this PR. Please let me know if there's anything else needed here.", "@mihaimaruseac is there anything I can do to move this forward and unblock #39609 ", "There's already some merge conflicts showing up, and given the number of random places this is introducing changes into, I'd expect that they will snowball until this branch is unusable. I'll resolve these conflicts, but I'd appreciate a timely review of the changes.", "@adriangb I think there is a little bit of overhead with the 2.5 release, so you need to wait a little bit for the review.", "Understood, I'll try to keep resolving conflicts for now.", "I looked at the CI failures, I think none of them are related to this PR, please correct me if I'm wrong and I will hunt down the issue.", "@bhack it looks like [last run only a single doctest target was failing](https://source.cloud.google.com/results/invocations/ce4b6da3-7984-4055-a504-a0c546c50b12/targets). I attempted to fix it. Could you approve so CI runs again? Thank you", "We still have a doctest issue.", "> We still have a doctest issue.\r\n\r\nYup, sorry for taking a bit to work on it, I was vacationing/traveling the last couple of weeks. I'll take another stab at it today, I might need to simplify the multi-statement docstring.\r\n\r\nIf you have any suggestions for something simpler that will work with tf-doctest that might still get the point across, I'm all ears.", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48125) for more info**.\n\n<!-- need_author_cla -->", "@bhack I think I fixed the syntax issue. I can't really test locally (requires compiling TF), so could you please kick off CI again to check? Thanks", "Doctest Is failing again.", "Ah yeah I see the issue, should be fixed now (I tested with vanilla doctest and w/o the TF specific stuff and it works). Sorry for the back and forth.", "Yes we are increasing the carbon footprint with this commits/ping/CI loops (as we rebuild all every time). :grimacing: \r\n\r\nI hope we could find soon a more accessible local testing solution for contributors /cc @mihaimaruseac @theadactyl ", "Yeah, in this specific case, I realized I could test most of the issues by throwing the test into a `test.py` and running stdlib doctest on it (i.e. I was just being lazy / not coming up with alternatives).\r\n\r\nBut in general, that's not possible (eg. with #39609 there's no way to test it). I do see all of the work you're all putting into making it easier, very much appreciated! FWIW I think the [move of Keras to its own repo](https://twitter.com/fchollet/status/1404967230048149506) will help a ton.", "> But in general, that's not possible (eg. with #39609 there's no way to test it). I do see all of the work you're all putting into making it easier, very much appreciated! \r\n\r\nYou can still theoretically execute a test subset with something like https://github.com/tensorflow/build/issues/23 as it is python only.\r\n\r\n> FWIW I think the move of Keras to its own repo will help a ton.\r\n\r\nIn Keras only PRs yes but In cases like this one it will be more complex as you are touching Keras and non Keras files in a single PR right?\r\n\r\nSo you will need to have two separate reviews \"in a dependency flow\" and someone that has the full vision on what you are contributing and so why you are introducing a change in TF to feed something in Keras. See also [this thread in the forum](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999/4?u=bhack).\r\n\r\n\r\n\r\n", "Yeah, there are definitely other complexities that will arise from that move. I'll check out the links, thanks for those!\r\n\r\nBack to this PR: looks like Ubuntu CPU (and hence doctests) are now passing. As per https://github.com/tensorflow/tensorflow/pull/48125#discussion_r654588863, I need to fix a `BUILD` issue now.", "@mihaimaruseac looks like the `MacOS CPU Python 3` and `Windows Bazel GPU` failures are the same as on `master` (i.e. unrelated to this PR), so everything here is working. Could you chime in on https://github.com/tensorflow/tensorflow/pull/48125#discussion_r654591774? Thanks", "@mihaimaruseac I don't think any of the failures are related to these changes, please correct me if this is not the case and something needs to be fixed, or if you want me to rebase and re-run with the latest master changes.", "Copybara import is failing. We need to wait for an internal feedback.", "> We'll have to split the keras parts to the Keras repo now that it got split.\r\n\r\nSo just revert any changes to `tensorflow/python/keras/*` files?", "> > We'll have to split the keras parts to the Keras repo now that it got split.\r\n> \r\n> So just revert any changes to `tensorflow/python/keras/*` files?\r\n\r\nYes, they'll have to move to the Keras repo isntead", "Ok, reverted all Keras changes.", "@mihaimaruseac failures looked like build / compilation failures to me, guessing they're unrelated to these changes. I merged in master to see if it helps.", "@mihaimaruseac only failure is Windows Bazel GPU, and the logs for that aren't public so I can't diagnose issues. Let me know if there's anything you need from me. Thanks.", "The issue is some internal builds are failing and will need some time to look into those"]}, {"number": 48124, "title": "Test ramfs walk on win", "body": "Explore fixes https://github.com/tensorflow/tensorflow/issues/48086", "comments": ["@mihaimaruseac @frankchn  This is almost working. If we want to go ahead with `ram://` + native sep we need to just see what we want to do for `DeleteRecurively` cause `JoinPath` is not virtual and the parent `JoinPathImpl` has the `/` separator hardcoded.", "@bhack Thanks for doing this. I think we can just `#define` the implementation of `FileSystem::Separator()` like what we are doing here, then get `FileSystem::JoinPathImpl` to use `FileSystem::Separator`.", "@frankchn Bazel cache was invalidated again.  So It will need hours of compile time as usual to test it locally again.\r\n\r\nI've added a black box commit in the case you can test there with RBE or with any other faster resource.\r\n\r\n", "@frankchn Do you have a Windows host to test this? It is very slow to iterate over Windows CI passes", "I don't unfortunately :( ", "Can you mention anyone in the team who can review it on a window host?", "@frankchn I suppose that the Windows error it is unrelated to this PR. What do you think?\r\n\r\n```\r\nC:/tools/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/bin/tensorflow/python/keras/api/keras_python_api_gen_compat_v1.genrule_script.sh\r\nExecution platform: @local_execution_config_platform//:platform\r\nTraceback (most recent call last):\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\tools\\api\\generator\\create_python_api.py\", line 26, in <module>\r\n    from tensorflow.python.tools.api.generator import doc_srcs\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\__init__.py\", line 46, in <module>\r\n    from tensorflow.python import data\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\data\\__init__.py\", line 25, in <module>\r\n    from tensorflow.python.data import experimental\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\data\\experimental\\__init__.py\", line 99, in <module>\r\n    from tensorflow.python.data.experimental import service\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\data\\experimental\\service\\__init__.py\", line 140, in <module>\r\n    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py\", line 25, in <module>\r\n    from tensorflow.python.data.experimental.ops import compression_ops\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py\", line 20, in <module>\r\n    from tensorflow.python.data.util import structure\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\data\\util\\structure.py\", line 33, in <module>\r\n    from tensorflow.python.ops import tensor_array_ops\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\ops\\tensor_array_ops.py\", line 43, in <module>\r\n    from tensorflow.python.ops import math_ops\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\ops\\math_ops.py\", line 235, in <module>\r\n    tf_export(v1=[\"arg_max\"])(dispatch.add_dispatch_support(arg_max))\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\util\\tf_export.py\", line 340, in __call__\r\n    self.set_attr(undecorated_func, api_names_attr, self._names)\r\n  File \"\\\\?\\T:\\tmp\\Bazel.runfiles_jok_ffv_\\runfiles\\org_tensorflow\\tensorflow\\python\\util\\tf_export.py\", line 357, in set_attr\r\n    (func.__name__, getattr(func, api_names_attr)))  # pylint: disable=protected-access\r\ntensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol arg_max is already exposed as ('arg_max',).\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "@bhack Yeah that looks unrelated.\r\n\r\n@gbaned Can you check?", "@frankchn @bhack if I understand correctly, this is formally defining that the `ram://` filesystem will now support Windows path separators. Is that the intention?", "Any idea?", "@mihaimaruseac What do you think?", "> @frankchn @bhack if I understand correctly, this is formally defining that the `ram://` filesystem will now support Windows path separators. Is that the intention?\r\n\r\nHi, sorry to bump this, I never got a response. Thanks.", "Yeah, on Windows this will use Windows style `\\` separators", "> Yeah, on Windows this will use Windows style \\ separators\r\n\r\nAs far as I can tell, supporting `\\` isn't strictly necessary (although it also doesn't really hurt anything). And isn't that technically adding new features that would then need to be maintained?", "Yeah this isn't adding anything new, so support load shouldn't be too high.", "Currently, support for `\\` on Windows is \"undefined\". I'd think that by adding this you are now establishing it as \"defined\".\r\n\r\nBut in any case, the need for this fix came from #39609 failing. Which is caused by `os.path.join` introducing `\\`s in Windows. I just want to postulate that an alternative to implementing support for `\\` is to keep TF itself from producing `\\` on Win by replacing `os.path.join` with something that is aware the of the multiple TF filesystems. It can then collect other scattered workarounds like [this one](https://github.com/tensorflow/tensorflow/blob/e55f6d2dda8bd3381ff3070e928af5ad698e8867/tensorflow/python/lib/io/file_io.py#L818). But if you want to implement and support `\\` regardless, that's totally okay, it won't hurt anything. I just want to make it clear that it is not the only way to resolve the original issue.", "@adriangb Yeah, that's fair, but I think in general people expect `os.path` stuff to work with file system accesses rather than using TF specific functions so I think in general perhaps we should have the principle of least surprise here and support this use case for the RAM file system?", "> @adriangb Yeah, that's fair, but I think in general people expect `os.path` stuff to work with file system accesses rather than using TF specific functions so I think in general perhaps we should have the principle of least surprise here and support this use case for the RAM file system?\n\nI still prefer to enforce portable \"/\" only for host like cases ", "I wouldn't be so sure. GCS is clearly a URL, I wouldn't expect `os.path.join` to work for URLs, nor do I expect GCS to support `\\`. I don't see how `ram://` is any different. \r\n\r\nAnd users are also already going to be using Tensorflow specific implementations of `listdir`, `walk`, etc\r\n\r\nSo I would almost lean towards explicitly not supporting `\\`, for consistency.", "@mihaimaruseac Any thoughts on whether we should support `\\` as a directory separator in file systems generally or should we provide a `join_path` or similar thing and not support `\\` at all?", "GCS Is an URL so your  https://github.com/tensorflow/tensorflow/pull/48125 It will useful for GCS and other host/URL like cases.\r\nI think ramfs Is still in the OS perimeter so It Is ok to have native os sep. \r\nAlso many of these fs APIs are not wrapped in python so It Is better to be in Sync with python os when possibile..", "I think we should support `\\` only for local Windows filesystem and everywhere else mandate `/`.", "> I think we should support `\\` only for local Windows filesystem and everywhere else mandate `/`.\n\nWhat do you mean with local? Do you mean on a local physical disk storage?\n\nCause I think that RAM is local :)", "I mean the `C:\\\\path\\\\to\\\\file` / `path\\\\to\\\\file` filesystem, without an URI. Also the `file://C:\\\\path\\\\to\\file`, maybe, as it shares the same code, but ideally any filesystem that has a non-empty URI scheme should be interpreted as a valid URL and use `/`.\r\n\r\nThough I'm not completely sold on this, if there are arguments to allow `\\` anywhere on Windows OS, then we can go that way, at the risk of complicating some implementations.", "Just echoing back to make sure I understood:\r\n\r\n```python\r\ngfile.GFile(\"path\\file.csv\")  # okay, relative path that is backed by Windows native disk FS\r\ngfile.GFile(\"C:\\\\path\\file.csv\")  # okay, absolute path that is backed by Windows native disk FS\r\ngfile.GFile(\"C:\\\\path/file.csv\")  # okay with TF, but / is disallowed by Windows native disk FS, failure happens outside of TF\r\ngfile.GFile(\"file://C:\\\\path\\file.csv\")  # okay, absolute path that is backed by Windows native disk FS\r\ngfile.GFile(\"ram://path\\file.csv\")  # not okay: \\ is not allowed in `ram`. Should this fail or just make a file called \"path\\file.csv\"?\r\ngfile.GFile(\"gc://path\\file.csv\")  # not okay: \\ is not allowed in `gc`. Should this fail or just make a file called \"path\\file.csv\"?\r\ngfile.GFile(\"ram://path/file.csv\")  # okay on all platforms, including Windows\r\n```", "> Just echoing back to make sure I understood:\r\n> \r\n> ```python\r\n> gfile.GFile(\"path\\file.csv\")  # okay, relative path that is backed by Windows native disk FS\r\n> gfile.GFile(\"C:\\\\path\\file.csv\")  # okay, absolute path that is backed by Windows native disk FS\r\n> gfile.GFile(\"C:\\\\path/file.csv\")  # okay with TF, but / is disallowed by Windows native disk FS, failure happens outside of TF\r\n> gfile.GFile(\"ram://path\\file.csv\")  # not okay: \\ is not allowed in `ram`. Should this fail or just make a file called \"path\\file.csv\"?\r\n> gfile.GFile(\"gc://path\\file.csv\")  # not okay: \\ is not allowed in `gc`. Should this fail or just make a file called \"path\\file.csv\"?\r\n> gfile.GFile(\"ram://path/file.csv\")  # okay on all platforms, including Windows\r\n> ```\r\n\r\nYes, those two not okay paths should be disallowed.\r\n\r\nAlternatively, we could make a change in `TranslateName` (or somewhere similar) to canonicalize paths on Windows to replace all `\\\\` with `/` as this won't require extensive changes in the rest of the code. However, it would harm cornercases of paths that contain these separators in the file name itself.", "> Yes, those two not okay paths should be disallowed\r\n\r\nFor what it's worth, I think this is probably the way to go. Even though I am not very familiar with your codebase, I think that generally it is easier to have only one way to do things (which in this case would be `/`) and either explicitly (likely more work and fragility, but better user experience because the errors are clear) or implicitly (just don't test for or hint at supporting) disallowing other cases. This is doubly true with a case like this where you end up with a lot of rough edges, like the question above of \"should `folder\\file.txt` be an error or a file called `folder\\file.txt`\"."]}, {"number": 48123, "title": "Tutorial Redirect for Time Series Forecasting gives 404. ", "body": "\r\n### Describe the problem\r\nIn the [Machine Learning Tutorial ](https://www.tensorflow.org/guide/data) the Time-series example hyperlink redirects to a [404 page](https://www.tensorflow.org/tutorials/text/time_series) instead of the proper tutorial: https://www.tensorflow.org/tutorials/structured_data/time_series This needs to be updated, but I'm uncomfortable making the changes and then creating a pull request. \r\n\r\nPlease nuke this comment when you can. I dont know another way to point this issue out, so I did this.\r\n", "comments": ["@12o13,\r\nLooks like the broken link has been fixed in PR [#1858](https://github.com/tensorflow/docs/pull/1858). The example now redirects to the time series forecasting tutorial.\r\n\r\nPlease feel free to close the issue if resolved. Thanks!"]}, {"number": 48122, "title": "[WIP] Initial support for sparse labels on confusion-matrix metrics", "body": "This PR addresses https://github.com/tensorflow/tensorflow/issues/37104 by supporting sparse labels in confusion-matrix metrics (`Precision`, `Recall`, `SensitivityAtSpecificity`, `SpecificityAtSensitivity`, etc).\r\n\r\nCurrently, the implementation simply infers whether the `y_true` argument to `update_confusion_matrix_variables` is sparse (and only when passing `class_id`). However, I think I'd personally prefer something more explicit since this enables users to call metrics like\r\n\r\n```\r\nm = tf.keras.metrics.Precision(class_id=3)\r\nm.update([3, 0, 0], [[0, 0, 0, 1], [1, 0, 0, 0], [0, 0, 1, 0]])\r\n```\r\n\r\nwhere `y_true` (first arg) and `y_pred` (second arg) have different dimensions (but the same length) which is something other metrics don't allow, to my knowledge.\r\n\r\nTwo ideas:\r\n1. Pass an arg to the metric constructor to indicate that `y_true` is sparse like `tf.keras.metrics.Precision(class_id=3, sparse_labels=True)`\r\n2. Create Sparse variants of metrics like `SparsePrecision`, `SparseRecall`, etc. This is the current pattern used for accuracy (`SparseCategoricalAccuracy`, `SparseCategoricalCrossentropy`, etc.)\r\n\r\nI think I slightly prefer 1. as it is a little more lightweight compared to proliferating classes/test cases. However perhaps others don't think we need either of these options and can just simply infer whether the `y_true` argument is sparse as in the current iplementation.", "comments": ["@dwyatte This PR is in draft, any update on this? Please. Thanks!", "@gbaned I opened this as a draft as I was hoping to get some guidance from core contributors/maintainers on whether implicitly inferring sparse labels based on the size of the `y_true`/`y_pred` args to a metric is sufficient or whether one of the alternate designs mentioned in https://github.com/tensorflow/tensorflow/pull/48122#issue-602111321 is preferred. \r\n\r\nWhat do you think? I can convert to non-draft if easier to iterate that way. Just let me know.\r\n\r\nCertainly I think this is worth addressing as it frequently comes up here (e.g., https://github.com/tensorflow/tensorflow/issues/37104, https://github.com/tensorflow/tensorflow/issues/42383) and on StackOverflow (e.g., [1](https://stackoverflow.com/questions/61742556/valueerror-shapes-none-1-and-none-2-are-incompatible), [2](https://datascience.stackexchange.com/questions/73827/valueerror-shapes-are-incompatible-when-fitting-using-imagedatagenerator)). ", "@qlzh727 Can you please assist on above comments from @dwyatte. Thanks!", "Sorry for the late reply. Looking at the issue, I would prefer to have separate metric class like SparsePrecision/SparseRecall to handle the sparse label case. The semantic between the SparsePrecision and Precision is actually a bit different, eg the Precision has a param threshold to control when a prediction is considered true/false. In the sparse case, the index of argmax will become the result and compare against the label.", "@dwyatte Can you please check @qlzh727's comments and keep us posted ? Thanks!", "After some thought, I agree with @qlzh727's suggestion and in general, it feels like trying to mix and match sparse and dense metrics should be an indicator that this won't work as expected.\r\n\r\nLooking at the PR, it will probably require a little refactoring so that the sparse metrics don't trigger `y_pred.shape.assert_is_compatible_with(y_true.shape)` but I'm not sure when I'll have time to get to this since I ended up using the dense metrics for my use case.\r\n\r\n@gbaned I think we can close the draft for now unless you think it makes sense to leave open.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 48120, "title": "tf.estimator.BoostedTreesEstimator center_bias=True breaks when label dimension > 1", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macos Big Sur && Google Colab**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **na**\r\n- TensorFlow installed from (source or binary): **pip (python 3.8)**\r\n- TensorFlow version (use command below): **v2.4.0-49-g85c8b2a817f 2.4.1**\r\n- Python version: **3.8**\r\n- Bazel version (if compiling from source): **na**\r\n- GCC/Compiler version (if compiling from source): **na**\r\n- CUDA/cuDNN version: **na**\r\n- GPU model and memory: **na**\r\n\r\n**Describe the current behavior**\r\nA BoostedTrees**Regressor** with center_bias=True  and label_dimension > 1, correctly raises `ValueError: center_bias not yet support with label_dimension > 1.`\r\nWhen creating a BoostedTrees**Estimator** with center_bias=True and a custom head with label dimension > 1, this issue is not caught in time and only leads to a later more cryptic error.\r\n\r\n\r\n**Describe the expected behavior**\r\nExpected to raise `ValueError: center_bias not yet support with label_dimension > 1.`\r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab gist](https://colab.research.google.com/gist/valkenburg/5404d404aca36d836659d292d2d811d2/boostedbiaslabdims.ipynb)\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nLABEL_DIM = 2\r\nINPUT_DIMS = [1]\r\n\r\nfeatures = [\r\n    tf.feature_column.numeric_column(f\"random_input_{i}\",\r\n                                     shape = (sh,),\r\n                                     dtype=tf.float32\r\n                                    ) \r\n    for i, sh in enumerate(INPUT_DIMS)\r\n]\r\n\r\ndef input_fn(num_samples = 1000):\r\n    inputs = {f.key : np.random.rand(num_samples, s) for f, s in zip(features, INPUT_DIMS)}\r\n    targets = np.random.rand(num_samples, LABEL_DIM)\r\n    ds = tf.data.Dataset.from_tensor_slices((inputs, targets))\r\n    ds = ds.shuffle(num_samples).repeat(10).batch(10) \r\n    return ds\r\n\r\ndef less_input_fn():\r\n    return input_fn(10)\r\n\r\n\r\nest = tf.estimator.BoostedTreesEstimator(\r\n    feature_columns = features,\r\n    head = tf.estimator.RegressionHead(\r\n        label_dimension = LABEL_DIM, \r\n    ),\r\n    n_batches_per_layer = 1,\r\n    center_bias = True,\r\n)\r\n\r\nest.train(input_fn)\r\n```\r\n", "comments": ["I am able to replicate this issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e7e65026b102e70c0dfaa434a17baee8/untitled578.ipynb)", "Was able to replicate the issue in TF 2.6.0-dev20210603 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/fab7ca9eac4c39bbf540725c96da5587/untitled207.ipynb)..Thanks !", "@valkenburg Estimators are not recommended for new code .Could you please refer to this [link ](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesEstimator)  and try to follow the [migration](https://www.tensorflow.org/guide/migrate) document for more details?Thanks!", "Ha, thanks for the reference. Previously I did not find BoostedTrees in TFDF, but I see they are just there. Thanks, will migrate.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48120\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48120\">No</a>\n"]}, {"number": 48119, "title": "tf.estimator.BoostedTreesEstimator returns all zero predictions when using custom loss_fn involving linear loss (zero second derivative)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macos Big Sur && Google Colab**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **na**\r\n- TensorFlow installed from (source or binary): **pip (python 3.8)**\r\n- TensorFlow version (use command below): **v2.4.0-49-g85c8b2a817f 2.4.1**\r\n- Python version: **3.8**\r\n- Bazel version (if compiling from source): **na**\r\n- GCC/Compiler version (if compiling from source): **na**\r\n- CUDA/cuDNN version: **na**\r\n- GPU model and memory: **na**\r\n\r\n**Describe the current behavior**\r\nWhen creating a BoostedTreesEstimator with a custom head in order to use various loss functions, the predictions are always zero if the loss function involves tf.math.abs (I have only tried tf.math.abs and tf.math.pow, tf.math.pow works as expected). \r\n\r\n**Describe the expected behavior**\r\nnon-zero predictions. See the example and use custom_loss_mse instead of custom_loss_mae. In this example of uniform data in [0,1], both mse and mae should return predictions close to 0.5. \r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab gist](https://colab.research.google.com/gist/valkenburg/e35ce97d89523413a3db378174db9a6b/bootsedzeros.ipynb)\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nLABEL_DIM = 1\r\nINPUT_DIMS = [1]\r\n\r\nfeatures = [\r\n    tf.feature_column.numeric_column(f\"random_input_{i}\",\r\n                                     shape = (sh,),\r\n                                     dtype=tf.float32\r\n                                    ) \r\n    for i, sh in enumerate(INPUT_DIMS)\r\n]\r\n\r\ndef input_fn(num_samples = 1000):\r\n    inputs = {f.key : np.random.rand(num_samples, s) for f, s in zip(features, INPUT_DIMS)}\r\n    targets = np.random.rand(num_samples, LABEL_DIM)\r\n    ds = tf.data.Dataset.from_tensor_slices((inputs, targets))\r\n    ds = ds.shuffle(num_samples).repeat(10).batch(10) \r\n    return ds\r\n\r\ndef less_input_fn():\r\n    return input_fn(10)\r\n\r\ndef custom_loss_mse(labels, logits):\r\n    return tf.math.pow(labels - logits, 2)\r\n\r\ndef custom_loss_mae(labels, logits):\r\n    return tf.math.abs(labels - logits)\r\n\r\nest = tf.estimator.BoostedTreesEstimator(\r\n    feature_columns = features,\r\n    head = tf.estimator.RegressionHead(\r\n        label_dimension = LABEL_DIM, \r\n#         loss_fn = custom_loss_mse,\r\n        loss_fn = custom_loss_mae,\r\n    ),\r\n    n_batches_per_layer = 1,\r\n    center_bias = True,\r\n)\r\n\r\nest.train(input_fn)\r\n\r\nlist(est.predict(less_input_fn))\r\n```\r\n\r\n*I posted [this question](https://stackoverflow.com/questions/66831261/tf-estimator-boostedtreesestimator-returns-all-zero-predictions-when-using-custo) in stackoverflow, just in case the behaviour is expected and I'm misunderstanding things.*\r\n", "comments": ["@rmothukuru,\r\nI was able to reproduce the issue with TF v2.4, TF v2.5.0rc0 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/55466ea74b2fc652dced96fbde778ca4/48119.ipynb). Thanks!", "Reading up on this, https://blog.tensorflow.org/2019/03/how-to-train-boosted-trees-models-in-tensorflow.html mentions \"Users can also choose to use any twice differentiable custom loss (by providing it to BoostedTreesEstimator).\"\r\n\r\nI think this is the culprit. The loss in my gist is twice differentiable most of the time, albeit that the second derivative is always zero, except at the origin `labels - logits == 0`.\r\n\r\nSo I think the issue could either be resolved by clearer documentation of the API (explaining the exact conditions on the loss), or a runtime detection of this invalid use with a non-cryptic exception.\r\n\r\nHowever, I do not have the level of understanding about the implementation to say if the use in the gist is actually valid or invalid, and if there still is a bug to be fixed.\r\n\r\n@wvzytlyn\r\n", "@valkenburg Estimators are deprecated.Could you please refer to this [link ](https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesEstimator)  and try to follow the [migration](https://www.tensorflow.org/guide/migrate) document for more details ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48119\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48119\">No</a>\n"]}, {"number": 48117, "title": "OP_REQUIRES failed at conv_ops.cc:1106 : Not found: No algorithm worked!", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n-  Linux Ubuntu 18.04.2\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 2.4.1\r\n- Keras version: 2.4.3\r\n- Python version: 3.8.8\r\n- Installed using: conda\r\n- CUDA version: 11.1\r\n- cuDNN version: 8.1.0\r\n- GPU model and memory: two GeForce RTX 3080 10018MiB\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen i train the model from here [https://github.com/keras-team/keras-docs-zh/blob/master/sources/examples/mnist_cnn.md](https://github.com/keras-team/keras-docs-zh/blob/master/sources/examples/mnist_cnn.md) with GPU,  I get the following build error:\r\n```text\r\n2021-03-27 15:20:38.027337: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops.cc:1106 : Not found: No algorithm worked!\r\nTraceback (most recent call last):\r\n  File \"mnist.py\", line 61, in <module>\r\n    model.fit(x_train, y_train,\r\n  File \"/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n  File \"/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 888, in _call\r\n    return self._stateless_fn(*args, **kwds)\r\n  File \"/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2942, in __call__\r\n    return graph_function._call_flat(\r\n  File \"/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1918, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 555, in call\r\n    outputs = execute.execute(\r\n  File \"/home/mm/anaconda3/envs/ld/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!\r\n         [[node sequential/conv2d/Conv2D (defined at mnist.py:61) ]] [Op:__inference_train_function_790]\r\n\r\nFunction call stack:\r\ntrain_function\r\n```\r\nI don't think the code has any issues. It works fine when training with CPU.\r\n\r\n**Any other info / logs**\r\nI see #45044 but i still can't resolve this problem\r\n\r\nI also tried to run it in `conda tensorflow`, But my code stopped at the following message.\r\n```\r\n2021-03-27 15:36:42.238717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2021-03-27 15:36:42.238747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n\r\n**stop about 10 minutes**\r\n```\r\nhere is the full log\r\n```$ python mnist.py \r\n2021-03-27 15:36:40.615111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\n2021-03-27 15:36:41.948740: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-27 15:36:41.949591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-03-27 15:36:42.018520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:1a:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2021-03-27 15:36:42.019197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:3d:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2021-03-27 15:36:42.019216: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-27 15:36:42.020691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2021-03-27 15:36:42.020739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2021-03-27 15:36:42.022260: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-27 15:36:42.022487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-27 15:36:42.023973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-27 15:36:42.024853: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-27 15:36:42.028187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-27 15:36:42.030707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2021-03-27 15:36:42.031200: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-27 15:36:42.235638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \r\npciBusID: 0000:1a:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2021-03-27 15:36:42.236273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \r\npciBusID: 0000:3d:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2021-03-27 15:36:42.236294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n2021-03-27 15:36:42.236318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n2021-03-27 15:36:42.236328: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\r\n2021-03-27 15:36:42.236337: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2021-03-27 15:36:42.236347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2021-03-27 15:36:42.236356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2021-03-27 15:36:42.236365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\r\n2021-03-27 15:36:42.236385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n2021-03-27 15:36:42.238717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2021-03-27 15:36:42.238747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\n\r\n\r\n**here stop about 10 minutes**\r\n\r\n\r\n2021-03-27 15:42:38.462326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-03-27 15:42:38.462363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \r\n2021-03-27 15:42:38.462369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N \r\n2021-03-27 15:42:38.462373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N \r\n2021-03-27 15:42:38.465253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9071 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3080, pci bus id: 0000:1a:00.0, compute capability: 8.6)\r\n2021-03-27 15:42:38.466920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 9071 MB memory) -> physical GPU (device: 1, name: GeForce RTX 3080, pci bus id: 0000:3d:00.0, compute capability: 8.6)\r\n2021-03-27 15:42:38.467230: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-03-27 15:42:38.888017: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-03-27 15:42:38.909002: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\r\nEpoch 1/12\r\n2021-03-27 15:42:39.232928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\r\n\r\n**here stop about 2 minutes**\r\n\r\n2021-03-27 15:44:14.199317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\r\n\r\n469/469 [==============================] - 1054s 5ms/step - loss: 2.3028 - accuracy: 0.1104 - val_loss: 2.3026 - val_accuracy: 0.1135\r\nEpoch 2/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1120 - val_loss: 2.3026 - val_accuracy: 0.1135\r\nEpoch 3/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1110 - val_loss: 2.3026 - val_accuracy: 0.1135\r\nEpoch 4/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1108 - val_loss: 2.3026 - val_accuracy: 0.1135\r\nEpoch 5/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1115 - val_loss: 2.3026 - val_accuracy: 0.1135\r\nEpoch 6/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1125 - val_loss: 2.3026 - val_accuracy: 0.1135\r\nEpoch 7/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3026 - accuracy: 0.1121 - val_loss: 2.3025 - val_accuracy: 0.1135\r\nEpoch 8/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1123 - val_loss: 2.3025 - val_accuracy: 0.1135\r\nEpoch 9/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1124 - val_loss: 2.3025 - val_accuracy: 0.1135\r\nEpoch 10/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1117 - val_loss: 2.3025 - val_accuracy: 0.1135\r\nEpoch 11/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1128 - val_loss: 2.3025 - val_accuracy: 0.1135\r\nEpoch 12/12\r\n469/469 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1122 - val_loss: 2.3025 - val_accuracy: 0.1135\r\nTest loss: 2.3025200366973877\r\nTest accuracy: 0.11349999904632568\r\n```\r\nIt started training after waiting about 10 minutes\r\n\r\nI think the problem may lie in the version of cuDNN\r\n\r\n**nvidia-smi**\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 3080    Off  | 00000000:1A:00.0 Off |                  N/A |\r\n|  0%   35C    P8    19W / 320W |      0MiB / 10018MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 3080    Off  | 00000000:3D:00.0 Off |                  N/A |\r\n|  0%   30C    P8    19W / 320W |      0MiB / 10018MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n**nvcc -V**\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Tue_Sep_15_19:10:02_PDT_2020\r\nCuda compilation tools, release 11.1, V11.1.74\r\nBuild cuda_11.1.TC455_06.29069683_0\r\n```\r\n\r\n**cuDnn test**\r\n```$ ./mnistCUDNN \r\nExecuting: mnistCUDNN\r\ncudnnGetVersion() : 8100 , CUDNN_VERSION from cudnn.h : 8100 (8.1.0)\r\nHost compiler version : GCC 7.5.0\r\n\r\nThere are 2 CUDA capable devices on your machine :\r\ndevice 0 : sms 68  Capabilities 8.6, SmClock 1740.0 Mhz, MemSize (Mb) 10018, MemClock 9501.0 Mhz, Ecc=0, boardGroupID=0\r\ndevice 1 : sms 68  Capabilities 8.6, SmClock 1740.0 Mhz, MemSize (Mb) 10018, MemClock 9501.0 Mhz, Ecc=0, boardGroupID=1\r\nUsing device 0\r\n\r\nTesting single precision\r\nLoading binary file data/conv1.bin\r\nLoading binary file data/conv1.bias.bin\r\nLoading binary file data/conv2.bin\r\nLoading binary file data/conv2.bias.bin\r\nLoading binary file data/ip1.bin\r\nLoading binary file data/ip1.bias.bin\r\nLoading binary file data/ip2.bin\r\nLoading binary file data/ip2.bias.bin\r\nLoading image data/one_28x28.pgm\r\nPerforming forward propagation ...\r\nTesting cudnnGetConvolutionForwardAlgorithm_v7 ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 66640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 57600 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.015264 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.027648 time requiring 57600 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.057344 time requiring 178432 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.160736 time requiring 66640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.179200 time requiring 184784 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.307200 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnGetConvolutionForwardAlgorithm_v7 ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 128000 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.032768 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.032768 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.054272 time requiring 128000 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.060416 time requiring 4656640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.074752 time requiring 2450080 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.086016 time requiring 1433120 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nResulting weights from Softmax:\r\n0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 \r\nLoading image data/three_28x28.pgm\r\nPerforming forward propagation ...\r\nTesting cudnnGetConvolutionForwardAlgorithm_v7 ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 66640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 57600 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.013312 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.020480 time requiring 57600 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.036864 time requiring 178432 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.037888 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.044032 time requiring 184784 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.112640 time requiring 66640 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnGetConvolutionForwardAlgorithm_v7 ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 128000 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.032768 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.032768 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.044032 time requiring 2450080 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.052224 time requiring 4656640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.052224 time requiring 1433120 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.054272 time requiring 128000 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nResulting weights from Softmax:\r\n0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 \r\nLoading image data/five_28x28.pgm\r\nPerforming forward propagation ...\r\nResulting weights from Softmax:\r\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 \r\n\r\nResult of classification: 1 3 5\r\n\r\nTest passed!\r\n\r\nTesting half precision (math in single precision)\r\nLoading binary file data/conv1.bin\r\nLoading binary file data/conv1.bias.bin\r\nLoading binary file data/conv2.bin\r\nLoading binary file data/conv2.bias.bin\r\nLoading binary file data/ip1.bin\r\nLoading binary file data/ip1.bias.bin\r\nLoading binary file data/ip2.bin\r\nLoading binary file data/ip2.bias.bin\r\nLoading image data/one_28x28.pgm\r\nPerforming forward propagation ...\r\nTesting cudnnGetConvolutionForwardAlgorithm_v7 ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 28800 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 100 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.015360 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.027648 time requiring 28800 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.032768 time requiring 100 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.040960 time requiring 178432 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.048128 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.063392 time requiring 184784 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnGetConvolutionForwardAlgorithm_v7 ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 2000 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 64000 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.056352 time requiring 1433120 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.058304 time requiring 4656640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.060416 time requiring 2450080 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.064512 time requiring 2000 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.075776 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.092064 time requiring 64000 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nResulting weights from Softmax:\r\n0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001 \r\nLoading image data/three_28x28.pgm\r\nPerforming forward propagation ...\r\nTesting cudnnGetConvolutionForwardAlgorithm_v7 ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 28800 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 100 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 178432 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 184784 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.014336 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.020480 time requiring 28800 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.023552 time requiring 100 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.035840 time requiring 178432 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.037888 time requiring 2057744 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.040960 time requiring 184784 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnGetConvolutionForwardAlgorithm_v7 ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: -1.000000 time requiring 4656640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: -1.000000 time requiring 1433120 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: -1.000000 time requiring 2450080 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: -1.000000 time requiring 2000 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: -1.000000 time requiring 64000 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nTesting cudnnFindConvolutionForwardAlgorithm ...\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.044032 time requiring 2450080 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.055296 time requiring 1433120 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.056320 time requiring 4656640 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.061440 time requiring 2000 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.075776 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.091136 time requiring 64000 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 6: -1.000000 time requiring 0 memory\r\n^^^^ CUDNN_STATUS_NOT_SUPPORTED for Algo 3: -1.000000 time requiring 0 memory\r\nResulting weights from Softmax:\r\n0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000 \r\nLoading image data/five_28x28.pgm\r\nPerforming forward propagation ...\r\nResulting weights from Softmax:\r\n0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 \r\n\r\nResult of classification: 1 3 5\r\n\r\nTest passed!\r\n\r\n```", "comments": ["@notnotype \r\nI ran the code shared on tf 2.4 and did not see any error, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/60ea301148c77093902e3e8eafee43a1/untitled578.ipynb).", "My RTX3060 throw out this same error while my 3070 don't while running the same code,\r\nI'm using TF2.4.1 built from source, cuda11.2.2+cudnn8.1.1, compatibility issue?", "@ShuttleDDD \r\nCould you please downgrade your cuda to 11.0 and see if you still face the issue.", "> @ShuttleDDD\r\n> Could you please downgrade your cuda to 11.0 and see if you still face the issue.\r\n\r\nnah, RTX3060 only supports cuda11.1 and later versions.", "@ShuttleDDD \r\nCan you upgrade tensorflow version to 2.5 and let us know if you still face issues.", "> @ShuttleDDD\r\n> Can you upgrade tensorflow version to 2.5 and let us know if you still face issues.\r\n\r\nI switch to RTX3070 and this issue didn't came out any more, so this is a 3060 stand alone issue maybe?\r\nby the way, this can be solved by adding \r\nphysical_devices = tf.config.list_physical_devices('GPU') \r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\nafter importing everything in the code.", "@ShuttleDDD \r\nIf adding the code resolves the issue can we move this to closed status.", "> @ShuttleDDD\r\n> If adding the code resolves the issue can we move this to closed status.\r\n\r\nThis solution only works for me and I can't guarantee this is working for others, after all I'm not the one who open this issue.", "> @ShuttleDDD\r\n> Can you upgrade tensorflow version to 2.5 and let us know if you still face issues.\r\n\r\nThank you, I upgrade tensorflow version to 2.5 and resolves the issue. ", "@notnotype \r\nGlad the issue is resolved for you, please move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48117\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48117\">No</a>\n"]}, {"number": 48116, "title": "TransformedDistribution Documentation is not readable", "body": "### Describe the problem\r\nThe documentation of tfp.TransformedDistribution is not readable in between, seen the attachement\r\n\r\n### Source code / logs\r\nPlease see attachment\r\n![Capture](https://user-images.githubusercontent.com/19791881/112711538-30ae7680-8eef-11eb-86e5-008263c64d6b.PNG)\r\n\r\n", "comments": ["@ramakrse  Seems the documentation issue is more related to TF - Probability. Please submit a new issue [here](https://github.com/tensorflow/probability/issues). Thanks!", "Ok. Thanks. Done.", "@ramakrse  Please go ahead and close the issue.Thanks!"]}, {"number": 48115, "title": "Re-enable the 5d fused batch norm", "body": "This PR tries to reenable the fused batch norm for 5d tensors.\r\n\r\ncc. @nluehr ", "comments": ["CC @reedwm "]}, {"number": 48114, "title": "Integrate CUDNN v8 frontend API for fused convolution", "body": "This PR integrates the new cuDNN Frontend API (https://github.com/NVIDIA/cudnn-frontend) for the fused convolution backends (i.e. Conv2D + BiasAdd + Relu).\r\n\r\ncc. @nluehr", "comments": ["@timshen91 Can you help review this one as well since it is related to the previous cudnn v8 frontend api PR?"]}, {"number": 48112, "title": "Fused map and batch autotune", "body": "When auto-tuning is active and the batch size is 1, fused map and batch schedules ctx->runner_threadpool_size() parallel applications of the map. For instance, on a DGX-1, 80 parallel calls of the map are invoked (vs. 2 for a batch size of 2), which can result in Out Of Memory Segfaults.\r\n\r\nThe cause of this behavior can be traced to the comparison in auto busy() of the unsigned int return value of batch_results_.size() to the signed integer max_batch_results_, which is -1 under these conditions. The proposed resolution casts the return value to a signed integer.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48112) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "Thank you for your PR.\r\n\r\nThe fact that `max_batch_results_` is -1 is unexpected. In particular, the value is initialized [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/experimental/map_and_batch_dataset_op.cc#L203-L209). It should be `min(ceil(num_parallel_calls / batch_size), 16)` if `num_parallel_calls` is set and `min(ceil(num_cores / batch_size), 16)` otherwise. Your change might be masking an issue as opposed to fixing it.\r\n\r\nHave you confirmed that `max_batch_results_` is indeed -1 when `busy` is being executed? If so, it would be good to understand how could that happen. In particular, I wonder if it is because `port::NumSchedulableCPUs()` returns a negative value. If that was the case, I would expect `max_batch_results_` to be `-1` when autotuning is enabled irrespective of the value of `batch_size`. All in all, this should be investigated further before we make any code changes.\r\n\r\nLast but not least, your change made me realize that the `busy` predicate can allow more than `max_batch_results_` batches being buffered (in scenarios where `num_parallel_calls >> batch_size`, such as the one you are describing). This is because we can schedule up to `num_parallel_calls` in a \"burst\" which can result in as many \"batches\" being buffered if the batch size is 1. I would be open to you updating the `busy` logic to approximate the number of batches the outstanding calls will create to better respect the `max_batch_results_` limit.", "Hi, many thanks for your review.\r\n\r\nYou're right that the updated logic in TF2 prevents `max_batch_results_` from becoming negative. The issue I had identified only applies to TF1. I'm closing this PR."]}]