[{"number": 17819, "title": "i not understand what matter is my code about beamsearchdecoder", "body": "decoder_cell=[]\r\nfor _ in range(n_layers):\r\n    decoder_c=tf.nn.rnn_cell.BasicLSTMCell(num_units)\r\n    if train_state:\r\n        decoder_c=tf.contrib.rnn.DropoutWrapper(decoder_c,input_keep_prob)\r\n    decoder_cell.append(decoder_c)\r\ndecoder_cell=tf.nn.rnn_cell.MultiRNNCell(decoder_cell,state_is_tuple=True)\r\n\r\nattention_mechanism=tf.contrib.seq2seq.BahdanauAttention(num_units,encoder_outputs)\r\nattention_cell=tf.contrib.seq2seq.AttentionWrapper(decoder_cell,attention_mechanism)\r\nhelper=tf.contrib.seq2seq.TrainingHelper(decoder_embedded,decoder_length)\r\ninitial_state=attention_cell.zero_state(dtype=tf.float32,batch_size=50)\r\ninitial_state=initial_state.clone(cell_state=encoder_state)\r\ntraining_decoder=tf.contrib.seq2seq.BasicDecoder(attention_cell,helper,\r\n                                                     initial_state,\r\n                                                     output_layer=None)\r\ntrain_decoder_outputs,train_decoder_state,_=tf.contrib.seq2seq.dynamic_decode(training_decoder)\r\n\r\nstart_tokens=tf.placeholder(dtype=tf.int32,shape=[None])\r\nend_token=tf.placeholder(dtype=tf.int32,shape=[])\r\ntiled_encoder_outputs=tf.contrib.seq2seq.tile_batch(encoder_outputs,multiplier=beam_width)\r\ntiled_encoder_state=tf.contrib.seq2seq.tile_batch(encoder_state,multiplier=beam_width)\r\ntiled_sequence_length=tf.contrib.seq2seq.tile_batch(encoder_length,multiplier=beam_width)\r\nattention_mechanism=tf.contrib.seq2seq.BahdanauAttention(num_units,tiled_encoder_state[-1][0])\r\nattention_cell=tf.contrib.seq2seq.AttentionWrapper(decoder_cell,attention_mechanism)\r\ninitial_state=attention_cell.zero_state(dtype=tf.float32,batch_size=50*beam_width)\r\ninitial_state=initial_state.clone(cell_state=tiled_encoder_state)\r\npredicting_decoder=tf.contrib.seq2seq.BeamSearchDecoder(attention_cell,\r\n                                                        embeddings,\r\n                                                        start_tokens,\r\n                                                        end_token,\r\n                                                        initial_state=initial_state,\r\n                                                        beam_width=beam_width,\r\n                                                        output_layer=None)\r\n\r\npredict_decoder_ouputs,_,_=tf.contrib.seq2seq.dynamic_decode(predicting_decoder)\r\n\r\n------------------------------------------------------------------------------------------\r\n\r\nndexError                                Traceback (most recent call last)\r\n<ipython-input-43-08de1520bc78> in <module>()\r\n----> 1 predict_decoder_ouputs,_,_=tf.contrib.seq2seq.dynamic_decode(predicting_decoder)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)\r\n    284         ],\r\n    285         parallel_iterations=parallel_iterations,\r\n--> 286         swap_memory=swap_memory)\r\n    287 \r\n    288     final_outputs_ta = res[1]\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name\r\n   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2817     return result\r\n   2818 \r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2638       self.Enter()\r\n   2639       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2641     finally:\r\n   2642       self.Exit()\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2588         structure=original_loop_vars,\r\n   2589         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2590     body_result = body(*packed_vars_for_body)\r\n   2591     if not nest.is_sequence(body_result):\r\n   2592       body_result = [body_result]\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)\r\n    232       \"\"\"\r\n    233       (next_outputs, decoder_state, next_inputs,\r\n--> 234        decoder_finished) = decoder.step(time, inputs, state)\r\n    235       next_finished = math_ops.logical_or(decoder_finished, finished)\r\n    236       if maximum_iterations is not None:\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py in step(self, time, inputs, state, name)\r\n    456           self._maybe_merge_batch_beams,\r\n    457           cell_state, self._cell.state_size)\r\n--> 458       cell_outputs, next_cell_state = self._cell(inputs, cell_state)\r\n    459       cell_outputs = nest.map_structure(\r\n    460           lambda out: self._split_batch_beams(out, out.shape[1:]), cell_outputs)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    181       with vs.variable_scope(vs.get_variable_scope(),\r\n    182                              custom_getter=self._rnn_get_variable):\r\n--> 183         return super(RNNCell, self).__call__(inputs, state)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)\r\n   1322       attention, alignments = _compute_attention(\r\n   1323           attention_mechanism, cell_output, previous_alignments[i],\r\n-> 1324           self._attention_layers[i] if self._attention_layers else None)\r\n   1325       alignment_history = previous_alignment_history[i].write(\r\n   1326           state.time, alignments) if self._alignment_history else ()\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _compute_attention(attention_mechanism, cell_output, previous_alignments, attention_layer)\r\n    971   \"\"\"Computes the attention and alignments for a given attention_mechanism.\"\"\"\r\n    972   alignments = attention_mechanism(\r\n--> 973       cell_output, previous_alignments=previous_alignments)\r\n    974 \r\n    975   # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in __call__(self, query, previous_alignments)\r\n    531     with variable_scope.variable_scope(None, \"bahdanau_attention\", [query]):\r\n    532       processed_query = self.query_layer(query) if self.query_layer else query\r\n--> 533       score = _bahdanau_score(processed_query, self._keys, self._normalize)\r\n    534     alignments = self._probability_fn(score, previous_alignments)\r\n    535     return alignments\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _bahdanau_score(processed_query, keys, normalize)\r\n    425   dtype = processed_query.dtype\r\n    426   # Get the number of hidden units from the trailing dimension of keys\r\n--> 427   num_units = keys.shape[2].value or array_ops.shape(keys)[2]\r\n    428   # Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting.\r\n    429   processed_query = array_ops.expand_dims(processed_query, 1)\r\n\r\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py in __getitem__(self, key)\r\n    519         return TensorShape(self._dims[key])\r\n    520       else:\r\n--> 521         return self._dims[key]\r\n    522     else:\r\n    523       if isinstance(key, slice):\r\n\r\nIndexError: list index out of range\r\n\r\n", "comments": []}, {"number": 17818, "title": "Variadic arguments for tf.control_dependencies", "body": "# Disclaimer: this is a UX proposal for the Python API but it should be discussed first!\r\n\r\n## Proposal\r\nI see a lot of people get tripped up on `tf.control_dependencies` as they often try to pass a single tensor/op to it and forget to explicitly wrap it in a Python list. Perhaps it would be better to support both usages, as that is more in line with how `tf.keras` does things.\r\n\r\n**TL;DR: This commit changes `tf.control_dependencies` so that it takes a list as before, but also works with variadic arguments as well.**\r\n\r\n```py\r\nwith tf.control_dependencies(tf.assert_positive(x)):\r\n  x = tf.identity(x)\r\n```", "comments": ["API owner comment: Should use python.util.nest.flatten() to accept arbitrary nesting of lists, tuples, etc. Don't worry about collections.Iterable, flatten() will handle the common sequence types. Otherwise this looks good.", "Cool. `nest.flatten` looks very clean. \ud83c\udf89 ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@carlthome can you update the PR to address @asimshankar's comments?", "@martinwicke thanks for the bump! Fixed @asimshankar's comments now.", "@asimshankar another round of review?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17817, "title": "Feature request: batch image input for exported model (image retraining)", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04\r\n- **TensorFlow installed from (source or binary)**:  binary\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: - \r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: 7.5/6\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nFeature request:\r\nI want to use the image retraining on my own image classes and then use the exported model as part of my data pipeline. But I noticed that the exported model only accepts input tensors of size (1, 299, 299, 3). It would be really great if it could accept batch images for example (32, 299, 299, 3) tensors. If you could advise me on how best to modify the code to do this that would be great too.\r\n\r\n### Source code / logs\r\nHow I trained my model:\r\n```\r\npython3 tensorflow/examples/image_retraining/retrain.py --image_dir $DATA_DIR\r\n```\r\nHere I tried feeding (2, 299, 299, 3) tensor as input to the model\r\n```\r\nTraceback (most recent call last):\r\n  File \"tensorflow/examples/label_image/label_image.py\", line 133, in <module>\r\n    input_operation.outputs[0]: t\r\n  File \"/home/beo/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/home/beo/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1113, in _run\r\n    str(subfeed_t.get_shape())))\r\nValueError: Cannot feed value of shape (2, 299, 299, 3) for Tensor u'import/Mul:0', which has shape '(1, 299, 299, 3)'\r\n```\r\n", "comments": ["How are you saving/loading your saved model? What tags are you using?\r\n\r\nDo you have a code snippet?", "I am using the model generated by the retrain.py code. \r\n```\r\npython tensorflow/examples/label_image/label_image.py --graph=/tmp/output_graph.pb --labels=/tmp/output_labels.txt --input_layer=Mul --output_layer=final_result --input_mean=128 --input_std=128\r\n```\r\nAnd i replaced the input in label_image.py to use a (2, 299, 299, 3) shape array.\r\n```\r\ngraph = load_graph(model_file)\r\n# t = read_tensor_from_image_file(\r\n#     file_name,\r\n#     input_height=input_height,\r\n#     input_width=input_width,\r\n#     input_mean=input_mean,\r\n#     input_std=input_std)\r\n\r\nt = np.zeros((2, 299, 299, 3))\r\n\r\ninput_name = \"import/\" + input_layer\r\noutput_name = \"import/\" + output_layer\r\ninput_operation = graph.get_operation_by_name(input_name)\r\noutput_operation = graph.get_operation_by_name(output_name)\r\n\r\nwith tf.Session(graph=graph) as sess:\r\n  results = sess.run(output_operation.outputs[0], {\r\n      input_operation.outputs[0]: t\r\n  })\r\n```", "This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 17816, "title": "MKL_DNN is built during non-mkl builds.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nNo\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nmaster after 1.7\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n0.11\r\n- **GCC/Compiler version (if compiling from source)**:\r\ndoes not matter\r\n- **CUDA/cuDNN version**:\r\nn/a\r\n- **GPU model and memory**:\r\nn/a\r\n- **Exact command to reproduce**:\r\n```\r\nbazel test --test_tag_filters=-no_oss,-oss_serial,-gpu,-benchmark-test -k --copt='-march=skylake-avx512' - //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/...\r\n```\r\n\r\n### Describe the problem\r\nI expected to see some test failures, or some other build failures. no MKL code should be compiled with the above command.\r\n\r\n### Source code / logs\r\n```\r\nERROR: /home/kbuilder/.cache/bazel/_bazel_kbuilder/9a46bec5c8e82ceb5b4b229201d44d70/external/mkl_dnn/BUILD:3:1: Couldn't build file external/mkl_dnn/_objs/mkl_dnn/external/mkl_dnn/src/cpu/ref_lrn.pic.o: C++ compilation of rule '@mkl_dnn//:mkl_dnn' failed (Exit 1): clang failed: error executing command \r\n  (cd /home/kbuilder/.cache/bazel/_bazel_kbuilder/9a46bec5c8e82ceb5b4b229201d44d70/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/bin:/usr/bin \\\r\n    PORTSERVER_ADDRESS=@unittest-portserver \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n    TF_PYTHON_CONFIG_REPO=@org_tensorflow//third_party/toolchains/cpus/py \\\r\n  /usr/local/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/local/bin -B/usr/bin -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=skylake-avx512' '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/external/mkl_dnn/src/cpu/ref_lrn.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/external/mkl_dnn/src/cpu/ref_lrn.pic.o' -fPIC -iquote external/mkl_dnn -iquote bazel-out/k8-opt/genfiles/external/mkl_dnn -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -isystem external/mkl_dnn/include -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/k8-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem external/bazel_tools/tools/cpp/gcc3 -fexceptions -fopenmp -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/mkl_dnn/src/cpu/ref_lrn.cpp -o bazel-out/k8-opt/bin/external/mkl_dnn/_objs/mkl_dnn/external/mkl_dnn/src/cpu/ref_lrn.pic.o)\r\nexternal/mkl_dnn/src/cpu/ref_lrn.cpp:20:10: fatal error: 'c_types_map.hpp' file not found\r\n#include \"c_types_map.hpp\"\r\n```\r\n\r\n@tatianashp Could you add our Intel contacts to this issue?", "comments": ["Adding @agramesh1 ", "@tatianashp thanks for letting us know, will look at it.\r\n", "@gunan I am trying to reproduce this issue. Do you think it is related to clang or it can be reproduced by gcc?", "I think the problem is mostly about mkl-DNN being included in BUILD rules when it is disabled. So it should not be compiler specific.\r\nThe compilation failure is just a symptom of the problem, I think.", "But I think gcc should reproduce the issue.", "Ok, I think I am able to reproduce it with gcc.", "@gunan I created this PR #17934 which I think will fix this issue.", "Nagging Assignee @jart: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The issue have been resolved.", "Hi,\r\n\r\nI'm seeing this failure with Bazel 0.12.0rc2 (probably Bazel head as well). Here is my build info:\r\n\r\nSystem information\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nDebian 8\r\nTensorFlow installed from (source or binary):\r\nSource\r\nTensorFlow version (use command below):\r\ncommit 7535f6beb7ba95bf54e1513b0c2c51b844a7a49f\r\nPython version:\r\n2.7.9\r\nBazel version (if compiling from source):\r\n0.12.0rc2 (https://releases.bazel.build/0.12.0/rc2/index.html)\r\nGCC/Compiler version (if compiling from source):\r\ndoes not matter\r\nCUDA/cuDNN version:\r\nn/a\r\nGPU model and memory:\r\nn/a\r\nExact command to reproduce:\r\nbazel test -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/... -//tensorflow/core:platform_setround_test -//tensorflow/go:test -//tensorflow/java:pom -//tensorflow/python/estimator:export_test\r\n\r\nDescribe the problem\r\nBazel tried to build @mkl_dnn and source code is not found.\r\n\r\nSource code / logs\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/mkl_dnn/BUILD:11:1: C++ compilation of rule '@mkl_dnn//:mkl_dnn' failed (Exit 1)\r\nIn file included from external/mkl_dnn/src/cpu/jit_avx2_convolution.cpp:20:\r\nIn file included from external/mkl_dnn/src/cpu/jit_avx2_convolution.hpp:23:\r\nIn file included from external/mkl_dnn/src/cpu/cpu_reducer.hpp:23:\r\nexternal/mkl_dnn/src/common/mkldnn_thread.hpp:23:10: fatal error: 'omp.h' file not found\r\n#include <omp.h>\r\n         ^~~~~~~\r\n1 error generated.\r\nINFO: Elapsed time: 4.468s, Critical Path: 1.82s\r\nFAILED: Build did NOT complete successfully\r\n\r\nCan someone try this with Bazel 0.12.0rc2?", "The PR just went through. Please try again in a day or so. If you still see error messages please reopen this issue.", "I just executed the same command with Tensorflow head at b874783ccdf4cc36cb3546e6b6a998cb8f3470bb. I'm still seeing the same error with Bazel 0.12.0rc2:\r\n\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/mkl_dnn/BUILD:11:1: C++ compilation of rule '@mkl_dnn//:mkl_dnn' failed (Exit 1)\r\nIn file included from external/mkl_dnn/src/cpu/jit_uni_batch_normalization.cpp:22:\r\nexternal/mkl_dnn/src/common/mkldnn_thread.hpp:23:10: fatal error: 'omp.h' file not found\r\n#include <omp.h>\r\n         ^~~~~~~\r\n1 error generated.\r\n\r\nI don't see to have permission to reopen the issue.", "@xingao267 Does the problem happen with Bazel 0.11.0?", "Is `@mkl_dnn` supposed to be used for some targets?", "Just tried the same command with Bazel 0.11.1, it also failed. The log is \r\n\r\nERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/mkl_dnn/BUILD:11:1: C++ compilation of rule '@mkl_dnn//:mkl_dnn' failed (Exit 1)\r\nIn file included from external/mkl_dnn/src/cpu/cpu_reorder.cpp:23:\r\nIn file included from external/mkl_dnn/src/cpu/jit_reorder.hpp:27:\r\nIn file included from external/mkl_dnn/src/cpu/simple_reorder.hpp:25:\r\nexternal/mkl_dnn/src/common/mkldnn_thread.hpp:23:10: fatal error: 'omp.h' file not found\r\n#include <omp.h>\r\n         ^~~~~~~\r\n1 error generated.\r\nINFO: Elapsed time: 266.949s, Critical Path: 52.28s\r\n\r\nIs some of the targets run trying to use @mkl_dnn?", "@xingao267 , So this isn't a regression in Bazel 0.12.0, we should fix the dependency somewhere in TensorFlow", "I am looking at this issue..", "@xingao267 can you test again with this PR #18401 ?", "@mahmoud-abuzaina I tested it with Bazel 0.12.0rc3 and Bazel 0.11.1, and it builds."]}, {"number": 17815, "title": "contrib: minor spelling tweaks", "body": "", "comments": ["@brettkoonce can you resolve the merge conflicts.", "@sb2nov rebased, thanks!", "@sb2nov rolled back, let me know if you need anything else!", "@sb2nov rebased, please try the tests again!", "@benoitsteiner just rebased to master/HEAD again, for what it's worth!", "@sb2nov one more try!", "@sb2nov is there anything i can do to help fix things?", "These are almost certainly infra failures."]}, {"number": 17814, "title": "Add resize_image_aspect_with_pad method", "body": "This method adds the ability to resize the image to specified size with padding without distortions by preserving the aspect ratio.", "comments": ["(good for API review with name change)", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->", "@martinwicke Updated, let me know if there are other changes you'd like to see.", "Nagging Assignee @martinwicke: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@martinwicke Sorry for the delay, the tests pass now.\r\n\r\nI addressed all issues expect for https://github.com/tensorflow/tensorflow/pull/17814#discussion_r184870802 - can you elaborate? I checked `_CheckAtLeast3DImage` but its not clear to me how it can be used to replace assertions... ", "Please fix the lint errors:\r\n```\r\nFAIL: Found 8 non-whitelited pylint errors:\r\ntensorflow/python/ops/image_ops_impl.py:1047: [C0330(bad-continuation), ] Wrong continued indentation (remove 7 spaces).\r\ntensorflow/python/ops/image_ops_impl.py:1114: [C0301(line-too-long), ] Line too long (92/80)\r\ntensorflow/python/ops/image_ops_impl.py:1115: [C0301(line-too-long), ] Line too long (90/80)\r\ntensorflow/python/ops/image_ops_impl.py:1117: [C0301(line-too-long), ] Line too long (83/80)\r\ntensorflow/python/ops/image_ops_test.py:2606: [C0330(bad-continuation), ] Wrong continued indentation (remove 6 spaces).\r\ntensorflow/python/ops/image_ops_test.py:2617: [C0330(bad-continuation), ] Wrong continued indentation (remove 8 spaces).\r\ntensorflow/python/ops/image_ops_test.py:2637: [C0330(bad-continuation), ] Wrong continued indentation (remove 6 spaces).\r\ntensorflow/python/ops/image_ops_test.py:2653: [C0330(bad-continuation), ] Wrong continued indentation (remove 6 spaces).\r\n```\r\nand update the goldens:\r\n```\r\n    $ bazel build tensorflow/tools/api/tests:api_compatibility_test\r\n    $ bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n          --update_goldens True\r\n```\r\nAfter that it should be ready to go!", "@drpngx Lint should be good, and I ran the goldens steps", "Hmm, going to try to rerun the api compatibility steps", "Running update steps doesn't seem to change the repository - should it?\r\n\r\n```\r\n ~/w/t/tensorflow \ue0b0 \ue0a0 danosipov/feature/resize_image_aspect_with_pad \u2026 \ue0b0 git status                                                 Wed Jun  6 17:02:44 2018\r\nOn branch danosipov/feature/resize_image_aspect_with_pad\r\nnothing added to commit but untracked files present (use \"git add\" to track)\r\n ~/w/t/tensorflow \ue0b0 \ue0a0 danosipov/feature/resize_image_aspect_with_pad \u2026 \ue0b0 bazel-bin/tensorflow/tools/api/tests/api_compatibility_test --update_goldens True\r\nsWARNING:tensorflow:VARIABLES collection name is deprecated, please use GLOBAL_VARIABLES instead; VARIABLES will be removed after 2017-03-02.\r\n..\r\n----------------------------------------------------------------------\r\nRan 3 tests in 0.279s\r\n\r\nOK (skipped=1)\r\n ~/w/t/tensorflow \ue0b0 \ue0a0 danosipov/feature/resize_image_aspect_with_pad \u2026 \ue0b0 git status                                        1601ms \ue0b3 Wed Jun  6 17:02:58 2018\r\nOn branch danosipov/feature/resize_image_aspect_with_pad\r\nnothing added to commit but untracked files present (use \"git add\" to track)\r\n```", "hmm... it should. Can you run this on Linux? I think it should now work for both Py2 and Py3, but to be sure, can you try with Py2?", "Python 3 is a problem, but I get c++ compilation errors when I switch virtualenv to python 2:\r\n\r\n```\r\nERROR: /Users/danosipov/workspace/tensorflow/tensorflow/tensorflow/python/BUILD:387:1: C++ compilation of rule '//tensorflow/python:py_util' failed (Exit 1): wrapped_clang failed: error executing command\r\n  (cd /private/var/tmp/_bazel_danosipov/6485f105ce913a72c808d415e09f8e49/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM='' \\\r\n    APPLE_SDK_VERSION_OVERRIDE='' \\\r\n    PATH=/Users/danosipov/workspace/tensorflow/tensorflow/tf/bin:/Users/danosipov/.rbenv/shims:/opt/chefdk/bin:/Users/danosipov/.chefdk/gem/ruby/2.3.0/bin:/opt/chefdk/embedded/bin:/usr/local/opt/coreutils/libexec/gnubin:/Users/danosipov/workspace/datadog/devtools/bin:/usr/local/opt/llvm/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/chefdk/gitbin \\\r\n    XCODE_VERSION_OVERRIDE=9.4.0 \\\r\n  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -g0 '-std=c++11' -g0 -iquote . -iquote bazel-out/host/genfiles -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote external/local_config_python -iquote bazel-out/host/genfiles/external/local_config_python -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem external/local_config_python/python_include -isystem bazel-out/host/genfiles/external/local_config_python/python_include -MD -MF bazel-out/host/bin/tensorflow/python/_objs/py_util/tensorflow/python/lib/core/py_util.d '-frandom-seed=bazel-out/host/bin/tensorflow/python/_objs/py_util/tensorflow/python/lib/core/py_util.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY '-isysroot __BAZEL_XCODE_SDKROOT__' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/python/lib/core/py_util.cc -o bazel-out/host/bin/tensorflow/python/_objs/py_util/tensorflow/python/lib/core/py_util.o)\r\nIn file included from tensorflow/python/lib/core/py_util.cc:20:\r\nIn file included from ./tensorflow/core/lib/core/errors.h:19:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/sstream:174:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ostream:138:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:492:15: error: C++ requires a type specifier for all declarations\r\n    char_type toupper(char_type __c) const\r\n              ^\r\nbazel-out/host/genfiles/external/local_config_python/python_include/pyport.h:731:29: note: expanded from macro 'toupper'\r\n#define toupper(c) towupper(btowc(c))\r\n                            ^\r\nIn file included from tensorflow/python/lib/core/py_util.cc:20:\r\nIn file included from ./tensorflow/core/lib/core/errors.h:19:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/sstream:174:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ostream:138:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:498:48: error: too many arguments provided to function-like macro invocation\r\n    const char_type* toupper(char_type* __low, const char_type* __high) const\r\n                                               ^\r\nbazel-out/host/genfiles/external/local_config_python/python_include/pyport.h:731:9: note: macro 'toupper' defined here\r\n#define toupper(c) towupper(btowc(c))\r\n        ^\r\nIn file included from tensorflow/python/lib/core/py_util.cc:20:\r\nIn file included from ./tensorflow/core/lib/core/errors.h:19:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/sstream:174:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ostream:138:\r\nIn file included from /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/ios:216:\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:498:29: error: expected ';' at end of declaration list\r\n    const char_type* toupper(char_type* __low, const char_type* __high) const\r\n                            ^\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__locale:510:48: error: too many arguments provided to function-like macro invocation\r\n    const char_type* tolower(char_type* __low, const char_type* __high) const\r\n....\r\n```\r\n\r\nI'll try to compile it on Linux next", "Updating from master finally resolved the issue, and I was able to update goldens:\r\nhttps://github.com/tensorflow/tensorflow/pull/17814/commits/e2617ac25490b33c87b8e792eee0670b09a7305f\r\n\r\nLMK if anything else is needed", "unrelated flake", "\ud83c\udf86 Thanks for your patience @martinwicke !", "Thanks for your persistence!\n"]}, {"number": 17813, "title": "Add resize_image_aspect_with_pad method", "body": "This method adds the ability to resize the image to specified size with padding without distortions by preserving the aspect ratio.", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n<!-- need_author_cla -->", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17812, "title": "Fix two external anchor link in kernel method tutorial", "body": "This PR is to fix:\r\n - The link of tf.contrib.learn.Estimator. As we can see in the note section of [kernel method tutorial](https://www.tensorflow.org/tutorials/kernel_methods), the two below highlighted link was broken and I can see from the latest source code the second one was fixed while the first one wasn't pointed to the correct place;\r\n    > Note: This document uses a deprecated version of ${tf.estimator}, which has a **${tf.contrib.learn.estimator$different interface}**. It also uses other contrib methods whose **${$version_compat#not_covered$API may not be stable}**.\r\n\r\n- The accurate link of input function section. It should be @{$get_started/premade_estimators#create_input_functions$this section on input functions} instead of #input_fn.\r\n", "comments": []}, {"number": 17811, "title": "Make feature column input_layer compatible with sequential data", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian Buster/Sid\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0-rc0\r\n- **Python version**: 3.6.5rc1\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See source code below\r\n\r\n### Describe the problem\r\n\r\nFeature columns offer easy and reproducible feature encodings. It would be nice if they could be made compatible with sequential data as well. I currently would like to use them for processing inputs in a custom RNN estimator and sadly discovered that the current [`input_layer`](https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer) API doesn't support sequential inputs. It always returns a Tensor shaped `(batch_size, first_layer_dimension)`, which makes it unusable in combination with the[ `dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) wrapper which expects inputs of shape `(batch_size, max_time, first_layer_dimension)`\r\n\r\n### Source code / logs\r\nMy first attempt at a workaround for this shortcoming, was mapping the `input_layer` function across the sequences of the inputs. \r\n```python\r\ndef model_fn(features, labels, mode, params, config):\r\n    sequence_lengths = features.pop('session_length')\r\n    rnn_cell, state_size, initializer = params['rnn_cell'].values()\r\n\r\n    encoded_features = tf.map_fn(lambda input_features: tf.feature_column.input_layer(input_features, params['feature_columns']), features, dtype=tf.float32)\r\n    encoded_labels = tf.tile(tf.map_fn(lambda seqeunce_labels: tf.feature_column.input_layer(seqeunce_labels, params['label_columns']), labels, dtype=tf.float32), sequence_lengths)\r\n\r\n    cell = rnn_cell(num_units=state_size, initializer=initializer())\r\n    outputs, state = tf.nn.dynamic_rnn(cell=cell, inputs=encoded_features, dtype=tf.float32, sequence_length=sequence_lengths)\r\n    logits = tf.contrib.layers.fully_connected(outputs, num_outputs=2, activation_fn=None)\r\n    predictions = tf.nn.softmax(logits)\r\n    \r\n    loss = tf.losses.softmax_cross_entropy(encoded_labels, logits)\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=params[\"learning_rate\"])\r\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n    \r\n    return tf.estimator.EstimatorSpec(mode, predictions=predictions, train_op=train_op, loss=loss)\r\n```\r\nDue to some kind of frame error this sadly is also not possible. Maybe I am missing something here and any tips on why this is failing would be highly appreciated.\r\n```bash\r\nInvalidArgumentError: The node 'group_deps_1' has inputs from different frames. \r\nThe input 'map/while/input_layer/url_indicator/url_lookup/hash_table/table_init' is in frame 'map/while/while_context'. \r\nThe input 'map_1/while/input_layer/label_indicator/label_lookup/hash_table/table_init' is in frame 'map_1/while/while_context'.`\r\n```\r\n**In my opinion a native (higher-level) solution to this problem is needed nonetheless!**", "comments": ["@ebrevdo Given your deep involvement in the development of RNN features, maybe you can provide some valuable input on this issue... ", "I found a way to apply the feature columns to sequences by reshaping them into a non-sequential batch, applying the `input_layer` function and unpacking it into sequences afterwards.\r\n```python\r\ndef unpack_batch(packed_batch, batch_size, max_len):\r\n    return {feature_name: tf.reshape(feature_tensor, [batch_size * max_len]) for feature_name, feature_tensor in packed_batch.items()}\r\n  \r\ndef pack_batch(unpacked_batch, batch_size, max_len, first_layer_dimension):\r\n    return tf.reshape(unpacked_batch, [batch_size, max_len, first_layer_dimension])\r\n\r\ndef apply_feature_columns_to_sequences(feature_dict, feature_columns, batch_size, max_len, first_layer_dimension):\r\n    unpacked_feature_dict = unpack_batch(feature_dict, batch_size, max_len)\r\n    packed_features = pack_batch(tf.feature_column.input_layer(unpacked_feature_dict, feature_columns), batch_size, max_len, first_layer_dimension)\r\n    return packed_features\r\n```\r\nEven though this works for now, I still see the need for a higher level solution to this problem.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@philippnormann can you share the full code for your solution?", "You may be interested in sequential feature columns which were recently added:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py\r\n\r\nThese can be used with sequence_input_layer (also in that file).", "@Corbalt any chance there is a working example using sequence_feature_column?  not clear from comments in code how to connect it to a data source of some sort.\r\n\r\nthanks!", "I don't think there's anything besides the test code and the [regular FeatureColumn guide](https://www.tensorflow.org/get_started/feature_columns). But even that guide is missing some good end to end examples... [this StackOverflow question ](https://stackoverflow.com/questions/47523374/feature-columns-embedding-lookup)has a good example of using the FeatureColumn API to construct an input layer. The sequence case would be quite similar except swapping input_layer for sequence_input_layer and using the sequence FeatureColumns.\r\n\r\nYou might also find the code and tests for the relatively new [RNNEstimator](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/estimator/python/estimator/rnn.py) to be helpful. ", "thanks @Corbalt \r\n\r\n\r\n**edit**:  rnn_test has some good patterns to follow....everything is clear now.... thanks for your help!\r\n\r\nBen\r\n\r\n\r\nI am familiar with regular feature columns.  i think what is confusing me is how to connect up the graph to data.  In the case of a regular estimator i create the input_layer as follows:\r\n\r\ninput_layer = tf.feature_column.input_layer(features, get_feature_columns())\r\n\r\nwhere 'features'  is provided by an input_fn and via the dataset api are attached in graph to a de-queueing operation. \r\n\r\nin the case of the sequence_feature_column the input_layer is created as follows:\r\n\r\ninput_layer, sequence_length = sequence_input_layer(features, columns)\r\n\r\nWhere the features are created by tf.parse_example:\r\n\r\nfeatures = tf.parse_example(..., features=make_parse_example_spec(columns))\r\n\r\n where '....'  refers to 'a number of serialized protos'.  \r\n\r\nSo\u2026.if this were to work in the same way as a regular estimator, the call to tf.parse_example would need to occur in an estimator input_fn\u2026.but the fact that parse_example contains a reference to the columns suggests it belongs in the model function.\r\n\r\nMaybe i am reading much into the code snippet in the comments and i should just send raw data from the input_fn in a suitable shape and ignore the tf.parse_example.\r\n\r\n\r\n", "That part of the API is actually the same. FeatureColumns can be used in two places: specifying how to parse features (in the input_fn) and how to construct the input layer (done after parsing to combine the dictionary of raw features into a single Tensor with transformed features). The parse features part is relevant if you have tf.Examples in some file that you're parsing, but if you have data in a pandas DataFrame or something like that you can ignore it and produce the features dictionary some other way. Remember, FeatureColumns don't contain any data/Tensors and should be thought of as configuration, so using them in two places doesn't cause any dataflow problems.\r\n\r\nGlad the tests were helpful!", "Nagging Assignee @ebrevdo: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee @ebrevdo: It has been 40 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Thanks @Corbalt! The `sequence_feature_column` solves this issue!", "Guys,\r\nI am still struggling to integrate the Estimator with Sequence data.\r\nCan anyone provide a working demo code which I can refer which is using the sequence_feature_column ?\r\n\r\nMy doubt to be precise is that\r\nin the sequence_input layer, it doesn't take sequence_numeric_column as is. It throws error that Input must be a SparseTensor.\r\nPoint to note is that it still accepts the sequence_categorical_column_with_vocabulary when I wrap it around normal feature_column.indicator_column\r\n\r\nHelp is appreciated !\r\n\r\nThanks", "@tpatel0409,\r\nfrom [sequence_feature_column_test.py](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column_test.py) it seems that the only way to use `sequence_numeric_column` is with `SparseTensor`. Looking at the [implementation](https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/feature_column/python/feature_column/sequence_feature_column.py#L449), `_SequenceNumericColumn` can only handle `SparseTensor`.\r\n\r\nHowever, it seems that with the upcoming rework of the feature columns [feature_column_v2.py](https://github.com/tensorflow/tensorflow/blob/0ef639bf1d7cc540c382513c19ff3565becac537/tensorflow/python/feature_column/feature_column_v2.py) there will be a column called `SequenceDenseColumn`. Not sure if it is intended for this case.\r\n\r\nOverall it seems to me that using feature columns for sequence data is currently not advisable. \r\nSadly communication or any clarification on this issue seems to be missing.\r\n\r\n**Suggestion for improvement**:\r\nI think to prevent the overall discussion about this restriction of `sequence_numeric_column`, in the future we should add some clarification to the [documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/feature_column/sequence_numeric_column).\r\nI think it should clearly state that it does only support `SparseTensor` objects.\r\n"]}, {"number": 17810, "title": "Dataset.list_files is impractical for large number of files", "body": "I would like to use [`Dataset.list_files`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files) on a large dataset. The dataset is similar to ImageNet in that it's a big list of images nested in subdirectories.\r\n\r\nI originally guessed that under the hood the api would use queues to start training early, but that isn't the case - the implementation walks the entire directory tree and loads the filenames into memory before the next operation starts.\r\n\r\nThis means you can wait tens of minutes before training starts.\r\n\r\nTwo things would help here:\r\n1) Use queues under the hood so downstream ops can start immediately\r\n2) Allow specifying a limit, eg. `Dataset.list_files(\"**/*.jpg\", limit=1000)`\r\n\r\nThe latter is mostly useful for quicker iteration. The former needs more work due to the different backends for `GetMatchingPaths`.\r\n\r\nIs this something being worked on already?\r\n\r\nI am using [`os.scandir()`](https://www.python.org/dev/peps/pep-0471/) as a workaround but the `Dataset.list_files()` is a more natural api for this task and should be faster due to not needing to pass the file names via `feed_dict`.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I'm running TF commit 90df8ab4ab127fac239597ce0708577a335613bf (master ~~as of yesterday~~ from a few days ago). On Ubuntu 17.10. Built with bazel 0.11.1, gcc 6.4, nvcc 9.1 and cuda 9.1. Relevant bazel build options are `-c opt --config=cuda` (ie. -march=native).\r\n\r\nHowever the behaviour I'm talking about is just part of the [`Dataset.list_files()` kernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/matching_files_op.cc#L46).\r\n\r\nI hacked together a [little script](https://gist.github.com/darrengarvey/ff05fbe28ab2061c101fe64353b467ff) to demonstrate the behaviour. Some results on different disks I have here. All run with `--width 100000`:\r\n\r\n1) tmpfs:\r\n```\r\nload data: time: 4566.47 ms\r\nprep data: time: 14.50 ms\r\nread first filename: time: 14691.77 ms\r\nread second filename: time: 0.36 ms\r\nread 99998 more filenames: time: 7742.66 ms (0.08 ms per iteration)\r\n```\r\n2) SSD (Samsung SM951 256GB SSD):\r\n```\r\nload data: time: 9190.51 ms\r\nprep data: time: 19.00 ms\r\nread first filename: time: 20781.54 ms\r\nread second filename: time: 0.28 ms\r\nread 99998 more filenames: time: 7919.55 ms (0.08 ms per iteration)\r\n```\r\n\r\n3) External HDD (Samsung D3 Station, 4TB):\r\n```\r\nload data: time: 64532.58 ms\r\nprep data: time: 18.13 ms\r\nread first filename: time: 86195.52 ms\r\nread second filename: time: 0.36 ms\r\nread 99998 more filenames: time: 7742.11 ms (0.08 ms per iteration)\r\n```\r\n\r\n4) External HDD (Seagate Expansion Desk 5TB):\r\n```\r\nload data: time: 119427.41 ms\r\nprep data: time: 14.98 ms\r\nread first filename: time: 77205.09 ms\r\nread second filename: time: 0.33 ms\r\nread 99998 more filenames: time: 7723.63 ms (0.08 ms per iteration)\r\n```\r\n\r\nNote I'm not flushing caches at any point. Ok so the external drives are terrible, but as a realistic example, consider 1m files sitting on tmpfs:\r\n\r\n```\r\nload data: time: 48649.91 ms\r\nprep data: time: 14.79 ms\r\nread first filename: time: 315815.76 ms\r\nread second filename: time: 0.43 ms\r\nread 999998 more filenames: time: 86427.00 ms (0.09 ms per iteration)\r\n```\r\nThat's still 5 minutes waiting before training starts.\r\n\r\nI suppose I've got a feature request rather than a bug. It might be something I'll fix if no-one else is currently looking at it and the options suggested in the OP seem reasonable.", "This would be a great feature to have! You're right - currently we wait till the MatchingFilesOp succeeds and we get all the files. One potential idea would be to create an iterator over the output of GetMatchingFiles and stream that into the downstream ops. @mrry for more ideas.", "+1. A `MatchingFilesDataset` would be very useful in this case, and it could replace the current implementation of `Dataset.list_files()`. For bonus points, it would be great if it had the option to yield results in sorted order, since the underlying OS might not guarantee this.", "@mrry and @rohan100jain  Do you mean to add a new op `MatchingFilesDataset`, which yields results in sorted order? ", "Yes, that's right.", "@mrry If nobody else is working on it, may I submit a PR for this new op?", "Sure, that'd be welcome!", "@mrry and @rohan100jain I implemented a `MatchingFilesDatasetOp`, which could yield the matching files in a sorted order.\r\n\r\nThe performance experiments run using [the script](https://gist.github.com/darrengarvey/ff05fbe28ab2061c101fe64353b467ff) posted by @darrengarvey , where `width` is the number of dirs at each level, and `depth` is the number of nested dirs.\r\n\r\nHere are the initial experiment results. What do you think of them?\r\n\r\nwhen `width = 1000` and `depth = 2`:\r\n-- MatchingFilesDataset:\r\n```\r\nread first filename: time: 16.06 ms\r\nread second filename: time: 0.52 ms\r\nread 998 more filenames: time: 336.51 ms (0.34 ms per iteration)\r\n```\r\n-- Dataset.list_files():\r\n```\r\nread first filename: time: 253.91 ms\r\nread second filename: time: 0.27 ms\r\nread 998 more filenames: time: 135.77 ms (0.14 ms per iteration)\r\n```\r\n\r\nwhen `width = 1000` and `depth = 20`:\r\n-- MatchingFilesDataset:\r\n```\r\nread first filename: time: 22.16 ms\r\nread second filename: time: 3.09 ms\r\nread 998 more filenames: time: 2269.14 ms (2.27 ms per iteration)\r\n```\r\n\r\n-- Dataset.list_files():\r\n```\r\nread first filename: time: 1860.96 ms\r\nread second filename: time: 0.38 ms\r\nread 998 more filenames: time: 127.23 ms (0.13 ms per iteration)\r\n```", "Ah, isn't open source fantastic?\r\n\r\n@feihugis I hadn't gotten around to implementing this yet so thanks! The results look very promising. So I'm guessing that at each level, you're finding all {files,dirs} in a directory, sorting, then moving to the next level? This sounds great and the results are a big improvement.\r\n\r\nIs the sorting optional? In many cases the order isn't interesting so if it could be turned off for extra speed that would be worth a small fortune in virtual bonus points. Sorting by default does seem sensible though.\r\n\r\nKudos!", "@darrengarvey Yeah, I implemented it in a similar way. Basically, I use `DFS` to search the files instead of `BFS` used in `MatchingFilesOp`. The files and dirs at each level are stored in a `priority_queue`, then the output for each `get_next` could be sorted. \r\n\r\nSorting is not optional yet. I could make some change to make it optional. Will do some tests to see the performance difference.\r\n  ", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "This is fixed now #22429 is merged. Thanks @feihugis."]}, {"number": 17809, "title": "Wrong object and switch ", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["This looks like this was submitted in error. I don't see useful information."]}, {"number": 17808, "title": "Fix broken internal anchor link in rnn quickdraw tutorial", "body": "This PR is to fix the two broken internal anchor link in the rnn quick draw tutorial as highlighted below:\r\n> 3. Download the data in TFRecord format from here and unzip it. More details about **how to obtain the original Quick, Draw! data** and **how to convert that to TFRecord files** is available below.\r\n\r\nThis PR is to fix the above two internal anchor link with \"_\" separated instead of \"-\".\r\n", "comments": []}, {"number": 17807, "title": "Failed to load the native tensorflow runtime", "body": "this is the full stack trace\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#1>\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 906, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Skinet\\AppData\\Local\\Programs\\Python\\Python35\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Thanks you for your response. this shows up when I type in \"import tensorflow as tf\" in python. I used the command \"pip3 install --upgrade tensorflow\" as specified in www.tensorflow.org/install/install_windows. I am under the assumption that this version is the CPU version of tensorflow.\r\nOS : windows 7 64 bit.\r\ntensorflow version: 1.6.0\r\nGPU: Nvidia gt 430 (1 gb)", "I have the same problem.\r\nStack Trace:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", lin\r\ne 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", lin\r\ne 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\__\r\ninit__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", lin\r\ne 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routin\r\ne failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\py\r\nthon\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\Rayan\\Anaconda3\\envs\\tensorflow\\lib\\importlib\\__init__.py\", lin\r\ne 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_probl\r\nems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\ufffd\r\n\r\nOS Platform and Distribution:\r\nWindows 8.1 \r\nTensorFlow installed from\r\nAnaconda(using: https://www.tensorflow.org/install/install_windows )\r\nTensorFlow version\r\nCan't figure out. (Installed yesterday)\r\nBazel version\r\nWasn't a source installation\r\nCUDA/cuDNN version:\r\nNot a GPU installation\r\nGPU model and memory:\r\nNo GPU\r\nExact command to reproduce:\r\nactivate tensorflow\r\npython\r\nimport tensorflow \r\n\r\nNote:\r\n\r\nShould I downgrade python as mentioned in:\r\nhttps://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso  \r\n\r\nI added .DLL to PATHTEXT system variable,\r\nstill same error. Should I undo that?                            ", "It looks like TensorFlow cannot find a required DLL. Issues #5949 and #10033 discuss similar problems. Please check if any of the solutions described there help you.  \r\n\r\npip3 install --upgrade tensorflow installs CPU-only version. \r\npip3 install --upgrade tensorflow-gpu installs GPU version (https://www.tensorflow.org/install/install_windows)\r\n\r\n@Rayan47 - I apologize but Anaconda installations are supported by the community not us. ", "I first faced the same issue as #5949 and fixed it, Issue #10033 seems to be for the GPU version and I downloaded the CPU only version of tensorflow. The issue has still not been resolved.", "Hi All, \r\n\r\nI have the same problem, fighting with it for couple of evenings now, I wanted to learn some ML, but got stuck right at the beginning. \r\n\r\nMy issues are with tensorflow for CPU.\r\n\r\nI already tried:\r\n\r\n- installing Visual C++ Redistributable 2015 x64/MSVCP140.DLL - didn't help\r\n- uninstalling all Python versions, installations, environments,  to start everything clean and go step by step - didn't help\r\n\r\nHere is my traceback:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: Procedura inicjowania biblioteki do\u0142\u0105czanej dynamicznie (DLL) nie powiod\u0142a si\u0119.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 658, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 922, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\nImportError: DLL load failed: Procedura inicjowania biblioteki do\u0142\u0105czanej dynamicznie (DLL) nie powiod\u0142a si\u0119.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"C:\\Users\\jakub\\AppData\\Local\\Programs\\Python\\Python36\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nModuleNotFoundError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n------------------------------------------------------------------------------------\r\n\r\nI've also tried Miniconda and after installing both Python and then Tensorflow on new environment I encounter the same issue. My temporary workaround is to install Tensorflow 1.5 - works without problems. \r\n\r\n", "@gunan - Do you have any thoughts on this Windows installation problem? @J4K8B reports that it occurs with TensorFlow 1.6, while TensorFlow 1.5 works just fine.", "if 1.6 fails and 1.5 works fine, you have a CPU that does not support AVX instruction sets.", "It fails with an error \"ImportError: DLL load failed with error code\". Can this be due to AVX?\r\n\r\n@aoptomus, @J4K8B - Can you try building TensorFlow from source on your machine and see if it fixes your problem?", "Yes, since DLL is compiled with AVX instructions built in,  loading fails with unknown instructions.", "The only workaround then is to build TensorFlow from source. ", "For me, the issue was with protobuf. That was not installed properly, I reinstalled it and then it works.", "> \r\n> \r\n> For me, the issue was with protobuf. That was not installed properly, I reinstalled it and then it works.\r\n\r\nThat was the solution in my case, i updated to protobuf 3.6.1 through conda and it worked.", "@ahmed1186 I have protobuf installed and it still gives the same error, do you have anything else installed that helps?\r\npython 3.6\r\ntensorflow 1.12\r\nprotobuf 3.6.1\r\nWindows 8.1 64-bit\r\nCPU: i7-4510U", "> \r\n> \r\n> @ahmed1186 I have protobuf installed and it still gives the same error, do you have anything else installed that helps?\r\n> python 3.6\r\n> tensorflow 1.12\r\n> protobuf 3.6.1\r\n> Windows 8.1 64-bit\r\n> CPU: i7-4510U\r\n\r\nI actually don't recall using anything else other than uninstalling tensorflow then using `conda install tensorflow-gpu`,\r\nwhich reinstalled all tensorflow 1.12 dependencies that I already had (CUDA 9.0, Cudnn 3.7.1 and others), the only thing that was updated was \"protobuf\" and then it worked.", "ImportError                               Traceback (most recent call last)\r\nc:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nc:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nc:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nc:\\users\\tapashetti-sr\\tf_gpu\\lib\\imp.py in load_module(name, file, filename, details)\r\n    242         else:\r\n--> 243             return load_dynamic(name, filename, file)\r\n    244     elif type_ == PKG_DIRECTORY:\r\n\r\nc:\\users\\tapashetti-sr\\tf_gpu\\lib\\imp.py in load_dynamic(name, path, file)\r\n    342             name=name, loader=loader, origin=path)\r\n--> 343         return _load(spec)\r\n    344 \r\n\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-2-07e0f991eb97> in <module>\r\n----> 1 import tensorflow as tf\r\n      2 \r\n\r\nc:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     22 \r\n     23 # pylint: disable=g-bad-import-order\r\n---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     25 \r\n     26 from tensorflow._api.v1 import app\r\n\r\nc:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nc:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"c:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"c:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"c:\\users\\tapashetti-sr\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"c:\\users\\tapashetti-sr\\tf_gpu\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"c:\\users\\tapashetti-sr\\tf_gpu\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nGetting this traceback while using tensorflow-gpu with virtualenv. tensorflow cpu works fine.", "> I actually don't recall using anything else other than uninstalling tensorflow then using `conda install tensorflow-gpu`,\r\n> which reinstalled all tensorflow 1.12 dependencies that I already had (CUDA 9.0, Cudnn 3.7.1 and others), the only thing that was updated was \"protobuf\" and then it worked.\r\n\r\nI didn't have to uninstall tensorflow at all. I simply ran `conda install tensorflow-gpu` and it upgraded some packages and downloaded some (including `protobuf`). I realised the download size of tensorflow, tensorflow-gpu were each < 2MB, indicating that it saw my currently installed version. \r\nI am on Windows 10, Python 3.7 and Conda 4.6. Using Tensorflow, -gpu, tensorboard 1.13\r\n\r\nThanks, @ahmed1186 \r\n", "Traceback (most recent call last):\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/MMKUMAR/PycharmProjects/JARVIS/Tensorflow.py\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 18, in swig_import_helper\r\n    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\imp.py\", line 296, in find_module\r\n    raise ImportError(_ERR_MSG.format(name), name=name)\r\nImportError: No module named '_pywrap_tensorflow'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Users\\MMKUMAR\\Desktop\\AdityaJarvis\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nModuleNotFoundError: No module named '_pywrap_tensorflow'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n", "Same error\r\nWindows 7 64bits\r\ntensorrflow 2.1.0\r\n`ImportError: Traceback (most recent call last):\r\n  File \"C:\\User\\User95\\Python-3.7.3-64bits\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\User\\User95\\Python-3.7.3-64bits\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\User\\User95\\Python-3.7.3-64bits\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\User\\User95\\Python-3.7.3-64bits\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\User\\User95\\Python-3.7.3-64bits\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.`\r\n\r\n**I installed tensorflow 1.15 and the problem is solved.**\r\nMy CPU is Intel i5-5200U 2.20 GHZ and should be compatible with AVX instructions ?\r\nInstruction Set Extensions\r\nIntel\u00ae SSE4.1, Intel\u00ae SSE4.2, Intel\u00ae AVX2\r\nhttps://ark.intel.com/content/www/us/en/ark/products/85212/intel-core-i5-5200u-processor-3m-cache-up-to-2-70-ghz.html#tab-blade-1-0-7\r\n", "Approach 1\r\n\r\nSo, according to the information you provided, follow these steps :\r\n\r\nInstall Visual Studio C++ Redistributable 2015 Update 3.\r\nSince your CPU does not support AVX, go to this repository. It contains TensorFlow builds supporting SSE, which is compatible with Intel Pentium.\r\nFollow this path in the repo -> tensorflow-windows-wheel/1.12.0/py36/CPU/sse2\r\nDownload the .whl file ( or click here ).\r\nNow, we need to install this file. Use,\r\n\r\npip install tensorflow-1.12.0-cp36-cp36m-win_amd64.whl\r\n\r\nIf it shows any error, try changing the relative path to an absolute one.\r\n\r\nTensorFlow is installed successfully.\r\n\r\nApproach 2\r\n\r\nInstall Anaconda. It uses a package manager named conda which is similar to pip.\r\n\r\nCreate a new Conda environment.\r\n\r\nconda create --name mytfenv\r\n\r\nInstall TensorFlow ( version=1.10.0 )\r\n\r\nconda install tensorflow\r\n\r\nOne of the methods would work for you. I personally followed both the approachs as my CPU specifications match yours.", "I had Visual Studio 2017, i installed Visual Studio 2019 and it is ok now thanks"]}, {"number": 17806, "title": "Simplify `rejection_resample` test to remove unnecessary iterator initialization", "body": "Tested:\r\n- bazel test :resample_test", "comments": []}, {"number": 17805, "title": "minor formatting fix in random_uniform documentation", "body": "", "comments": []}, {"number": 17804, "title": "How to read `feature_lists` from tfrecord.", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 17.10\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: 3.6.2\r\n### Describe the problem\r\nI save idex of chars to tfrecords, but i cant know how to read it.\r\n\r\n### Source code / logs\r\n```python\r\ndef make_example(words, chars, labels):\r\n    # print(chars)\r\n    ex = tf.train.SequenceExample()\r\n    ex.context.feature[\"len\"].int64_list.value.append(len(words))\r\n\r\n    for w, char_of_a_word, l in zip(words, chars, labels):\r\n        ex.feature_lists.feature_list[\"words\"].feature.add().int64_list.value.append(w)\r\n        ex.feature_lists.feature_list[\"label\"].feature.add().int64_list.value.append(l)\r\n\r\n        chars_feature = ex.feature_lists.feature_list[\"chars\"].feature.add()\r\n        for c in char_of_a_word:\r\n            chars_feature.int64_list.value.append(c)\r\n\r\n    return ex\r\n```\r\n`words` is a list of indexing, example: `[1, 2, 3]`\r\n`chars` is list of character indexing, example: `[[1,2,3], [2], [2,3,4]]`\r\n\r\nParsing function:\r\n```python\r\ndef _parse_function(example_proto):\r\n    context_features = {\r\n        \"len\": tf.FixedLenFeature((), dtype=tf.int64),\r\n    }\r\n    sequence_features = {\r\n        \"words\": tf.FixedLenSequenceFeature((), dtype=tf.int64),\r\n        \"chars\": tf.FixedLenSequenceFeature((1,35), dtype=tf.int64),\r\n        \"label\": tf.FixedLenSequenceFeature((), dtype=tf.int64),\r\n    }\r\n\r\n    context_parsed, sequence_parsed = tf.parse_single_sequence_example(\r\n        serialized=example_proto,\r\n        context_features=context_features,\r\n        sequence_features=sequence_features\r\n    )\r\n\r\n    len_ = tf.cast(context_parsed['len'], dtype=tf.int32)\r\n    word = tf.cast(sequence_parsed['words'], dtype=tf.int32)\r\n    chars = tf.cast(sequence_parsed['chars'], dtype=tf.int32)\r\n    print(\"word.shape\", word.shape)\r\n    print(\"chars.shape\", chars.shape)\r\n    label = tf.cast(sequence_parsed['label'], dtype=tf.int32)\r\n\r\n    return {\"words\": word, \"chars\": chars, \"len\": len_}, label\r\n```\r\n\r\nInput function:\r\n```python\r\ndef inputs(file_names, batch_size, num_epochs):\r\n    dataset = tf.contrib.data.TFRecordDataset(file_names)\r\n    dataset = dataset.map(_parse_function)\r\n    dataset = dataset.padded_batch(batch_size=batch_size,\r\n                                   padded_shapes=({\"word\": [None], \"chars\": [None], \"len\": []}, [None]),\r\n                                   padding_values=(\r\n                                       {\"word\": word_lookup.idx_of_pad,\r\n                                        \"chars\": 0,\r\n                                        \"len\": 0}, 0))\r\n    # dataset = dataset.filter(lambda f, l: tf.equal(tf.shape(l)[0], batch_size))\r\n    dataset = dataset.repeat(num_epochs)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    features, label = iterator.get_next()\r\n    return features, label\r\n```\r\n\r\nError log:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/binhnq/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/173.4301.16/helpers/pydev/pydev_run_in_console.py\", line 53, in run_file\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/home/binhnq/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/173.4301.16/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/binhnq/hitelli/pyvltk2/tfvltk/data_set.py\", line 62, in <module>\r\n    _result = inputs_test(['/home/binhnq/hitelli/WordSegmentation/data/test-ws.tfrecords', ])\r\n  File \"/home/binhnq/hitelli/pyvltk2/tfvltk/data_set.py\", line 55, in inputs_test\r\n    \"features\": features}, n=1)\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 128, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 644, in run_n\r\n    restore_checkpoint_path=restore_checkpoint_path)\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 128, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 702, in run_feeds\r\n    return list(run_feeds_iter(*args, **kwargs))\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.py\", line 692, in run_feeds_iter\r\n    yield session.run(output_dict, f)\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/binhnq/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Name: , Key: chars, Index: 1.  Number of int64 values != expected.  values size: 2 but output shape: [1]\r\n\t [[Node: ParseSingleSequenceExample/ParseSingleSequenceExample = ParseSingleSequenceExample[Ncontext_dense=1, Ncontext_sparse=0, Nfeature_list_dense=3, Nfeature_list_sparse=0, Tcontext_dense=[DT_INT64], context_dense_shapes=[[]], context_sparse_types=[], feature_list_dense_shapes=[[1], [], []], feature_list_dense_types=[DT_INT64, DT_INT64, DT_INT64], feature_list_sparse_types=[]](arg0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_missing_assumed_empty, ParseSingleSequenceExample/ParseSingleSequenceExample/context_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_0, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_1, ParseSingleSequenceExample/ParseSingleSequenceExample/feature_list_dense_keys_2, ParseSingleSequenceExample/Const, ParseSingleSequenceExample/ParseSingleSequenceExample/debug_name)]]\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nTensorFlow installed from\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This is a usage question which is best asked on Stack Overflow.\r\nMaybe @ebrevdo knows of some other helpful resource."]}, {"number": 17803, "title": "tf throws some errors in estimator.train function when I droped additional column in input_fn", "body": "\r\n`sample_weight` is instance-based weight, which i wanted to pass to `tf.losses.sparse_softmax_cross_entropy`,  so I droped this column in the features. However, I still got some errors. If i removed the `sample_weight` column in `data_train`, it's OK !\r\n\r\n\r\n`\r\ntrain_input_fn = tf.estimator.inputs.pandas_input_fn(x=data_train, y=data_train_click, batch_size = 1024, num_epochs=1, shuffle=True)\r\n\r\nclassifier.train(input_fn=train_input_fn)  #, steps=500)\r\n\r\n   def my_model(features, labels, mode, params):\r\n\r\n    if 'order_weight' in features.keys():\r\n        sample_weight = features.pop('sample_weight')\r\n\r\n    net = tf.feature_column.input_layer(features, params['feature_columns'])\r\n\r\n    for units in params['hidden_units']:\r\n        net = tf.layers.dense(net, units=units, activation=params[\"activation\"])\r\n\r\n    logits = tf.layers.dense(net, params['n_classes'], activation=None)\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\r\n            'probabilities': tf.nn.softmax(logits),\r\n            'logits': logits\r\n        }\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    metrics_auc = tf.metrics.auc(labels=labels, predictions=tf.nn.softmax(logits)[:,1])\r\n    metrics = {'auc': metrics_auc}          #tf.summary.scalar('auc', metrics_auc)\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    # Create training op.\r\n    assert mode == tf.estimator.ModeKeys.TRAIN\r\n    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step()) \r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n`", "comments": ["Here is logs:\r\n\r\n2018-03-18 14:28:01.368845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla M60, pci bus id: 0000:00:15.0, compute capability: 5.2)\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, Unsupported feed type\r\n2018-03-18 14:28:08.345478: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_6_enqueue_input/fifo_queue' is closed and has insufficient elements (requested 1024, current size 0)\r\n\t [[Node: fifo_queue_DequeueUpTo = QueueDequeueUpToV2[component_types=[DT_INT64, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_INT64, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_INT64, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](enqueue_input/fifo_queue, fifo_queue_DequeueUpTo/n)]]\r\n2018-03-18 14:28:08.345610: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_6_enqueue_input/fifo_queue' is closed and has insufficient elements (requested 1024, current size 0)\r\n\t [[Node: fifo_queue_DequeueUpTo = QueueDequeueUpToV2[component_types=[DT_INT64, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_INT64, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_DOUBLE, DT_INT64, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](enqueue_input/fifo_queue, fifo_queue_DequeueUpTo/n)]]\r\nINFO:tensorflow:Saving checkpoints for 0 into /data/kai.zhang/dnn/model/model.ckpt.\r\n2018-03-18 14:28:11.073924: E tensorflow/core/util/events_writer.cc:162] The events file /data/kai.zhang/dnn/model/events.out.tfevents.1521348849.xh-mlplatform-job-service01.gh.sankuai.com has disappeared.\r\n2018-03-18 14:28:11.073981: E tensorflow/core/util/events_writer.cc:131] Failed to flush 30 events to /data/kai.zhang/dnn/model/events.out.tfevents.1521348849.xh-mlplatform-job-service01.gh.sankuai.com\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-38-00bd15f0669f> in <module>()\r\n      1 shutil.rmtree(\"/data/kai.zhang/dnn/model/\", ignore_errors=True)\r\n----> 2 classifier.train(input_fn=train_input_fn, steps=500)\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)\r\n    300\r\n    301     saving_listeners = _check_listeners_type(saving_listeners)\r\n--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)\r\n    303     logging.info('Loss for final step: %s.', loss)\r\n    304     return self\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)\r\n    781         loss = None\r\n    782         while not mon_sess.should_stop():\r\n--> 783           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\r\n    784       return loss\r\n    785\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in __exit__(self, exception_type, exception_value, traceback)\r\n    535     if exception_type in [errors.OutOfRangeError, StopIteration]:\r\n    536       exception_type = None\r\n--> 537     self._close_internal(exception_type)\r\n    538     # __exit__ should return True to suppress an exception.\r\n    539     return exception_type is None\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in _close_internal(self, exception_type)\r\n    572         if self._sess is None:\r\n    573           raise RuntimeError('Session is already closed.')\r\n--> 574         self._sess.close()\r\n    575       finally:\r\n    576         self._sess = None\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in close(self)\r\n    818     if self._sess:\r\n    819       try:\r\n--> 820         self._sess.close()\r\n    821       except _PREEMPTION_ERRORS:\r\n    822         pass\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.pyc in close(self)\r\n    939       self._coord.join(\r\n    940           stop_grace_period_secs=self._stop_grace_period_secs,\r\n--> 941           ignore_live_threads=True)\r\n    942     finally:\r\n    943       try:\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.pyc in join(self, threads, stop_grace_period_secs, ignore_live_threads)\r\n    387       self._registered_threads = set()\r\n    388       if self._exc_info_to_raise:\r\n--> 389         six.reraise(*self._exc_info_to_raise)\r\n    390       elif stragglers:\r\n    391         if ignore_live_threads:\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.pyc in _run(self, sess, enqueue_op, feed_fn, coord)\r\n     92         try:\r\n     93           feed_dict = None if feed_fn is None else feed_fn()\r\n---> 94           sess.run(enqueue_op, feed_dict=feed_dict)\r\n     95         except (errors.OutOfRangeError, errors.CancelledError):\r\n     96           # This exception indicates that a queue was closed.\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)\r\n    887     try:\r\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 889                          run_metadata_ptr)\r\n    890       if run_metadata:\r\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):\r\n   1119       results = self._do_run(handle, final_targets, final_fetches,\r\n-> 1120                              feed_dict_tensor, options, run_metadata)\r\n   1121     else:\r\n   1122       results = []\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\r\n   1315     if handle is None:\r\n   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\r\n-> 1317                            options, run_metadata)\r\n   1318     else:\r\n   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)\r\n\r\n/data/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)\r\n   1334         except KeyError:\r\n   1335           pass\r\n-> 1336       raise type(e)(node_def, op, message)\r\n   1337\r\n   1338   def _extend_graph(self):\r\n\r\nInternalError: Unsupported feed type\r\n", "```\r\nif 'order_weight' in features.keys():\r\n    sample_weight = features.pop('sample_weight')\r\n\r\nnet = tf.feature_column.input_layer(features, params['feature_columns'])\r\n```\r\n\r\nHow does your input_fn return 'sample_weight'? \r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17802, "title": "Importing a meta graph which contains a SummaryWriter doesn't work", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: binary via pip\r\n- **TensorFlow version (use command below)**: v1.6.0-0-gd2e24b6039 1.6.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**: GTX 1080ti 11G\r\n- **Exact command to reproduce**:\r\n\r\nFirst run this:\r\n\r\n```python\r\nimport tensorflow as tf\r\nv1 = tf.placeholder(tf.float32, name=\"v1\")\r\nv2 = tf.placeholder(tf.float32, name=\"v2\")\r\nv3 = v1 * v2\r\nvx = tf.Variable(10.0, name=\"vx\")\r\nv4 = v3 * vx\r\nwriter = tf.contrib.summary.create_file_writer(\"foo\")\r\nsaver = tf.train.Saver([vx])\r\nsess = tf.Session()\r\nsess.run(tf.initialize_all_variables())\r\nsess.run(vx.assign(tf.add(vx, vx)))\r\nresult = sess.run(v4, feed_dict={v1:12.0, v2:3.3})\r\nprint(result)\r\nsaver.save(sess, \"./model_ex1\")\r\n```\r\n\r\n**Then in a different Python instance** (it works if done right after the first snippet within the same instance)\r\n\r\n```python\r\nimport tensorflow as tf\r\nsaver = tf.train.import_meta_graph(\"./model_ex1.meta\")\r\nsess = tf.Session()\r\nsaver.restore(sess, \"./model_ex1\")\r\n```\r\n\r\n\r\n### Describe the problem\r\n\r\nTrying to restore the meta graph via `import_meta_graph` does not work if the graph contains a SummaryWriter as shown in the example above. The example works if `import_meta_graph` is called within the same instance of Python, or if the `tf.contrib.summary.create_file_writer(\"foo\")` call is removed from the graph.\r\n\r\n\r\n### Source code / logs\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-1-1661c33bc0e5> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 saver = tf.train.import_meta_graph(\"./model_ex1.meta\")\r\n      3 sess = tf.Session()\r\n      4 saver.restore(sess, \"./model_ex1\")\r\n\r\n~/.miniconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)\r\n   1907                                       clear_devices=clear_devices,\r\n   1908                                       import_scope=import_scope,\r\n-> 1909                                       **kwargs)\r\n   1910   if meta_graph_def.HasField(\"saver_def\"):\r\n   1911     return Saver(saver_def=meta_graph_def.saver_def, name=import_scope)\r\n\r\n~/.miniconda/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\r\n    735     importer.import_graph_def(\r\n    736         input_graph_def, name=(import_scope or \"\"), input_map=input_map,\r\n--> 737         producer_op_list=producer_op_list)\r\n    738\r\n    739     # Restores all the other collections.\r\n\r\n~/.miniconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    430                 'in a future version' if date is None else ('after %s' % date),\r\n    431                 instructions)\r\n--> 432       return func(*args, **kwargs)\r\n    433     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    434                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\n~/.miniconda/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\r\n    429   if producer_op_list is not None:\r\n    430     # TODO(skyewm): make a copy of graph_def so we're not mutating the argument?\r\n--> 431     _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n    432\r\n    433   graph = ops.get_default_graph()\r\n\r\n~/.miniconda/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n    209     # Remove any default attr values that aren't in op_def.\r\n    210     if node.op in producer_op_dict:\r\n--> 211       op_def = op_dict[node.op]\r\n    212       producer_op_def = producer_op_dict[node.op]\r\n    213       # We make a copy of node.attr to iterate through since we may modify\r\n\r\nKeyError: 'SummaryWriter'\r\n```", "comments": ["Thanks for the report.\r\n@alextp : Do we need to recreate the resource ops on import from the `_SUMMARY_WRITER_INIT_COLLECTION_NAME` or something?", "This looks like the metagraph is saved from a newer version of tf than it is loaded in. It's failing to find the SummaryWriter op's registration, which doesn't make sense to me.\r\n\r\n@darthdeus is this reproducible if you use tf-nightly?", "Ah I just checked and I think I know what's going on. The python code for the generated ops is not getting imported unless you use tf.contrib.summary (because of lazy contrib imports) so we're not registering these summary ops on the separate process which loads the metagraph.", "I'm submitting a fix, but a temporary workaround is to have a line like \"tf.contrib.summary\" somewhere on your program before you try to import the metagraph."]}, {"number": 17801, "title": "Cannot find libdevice.10.bc under /usr/local/cuda-8.0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS (GNU/Linux 4.4.38-rt49-tegra aarch64)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.7.0-rc0, r1.7, master\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0-6ubuntu1~16.04.9\r\n- **CUDA/cuDNN version**: 8.0/6\r\n- **GPU model and memory**: name: GP106 major: 6 minor: 1 totalMemory: 3.75GiB (as reported by some previous version of Tensorflow on the other part of the same Drive PX2)\r\n- **Exact command to reproduce**:\r\n`git checkout v1.7.0-rc0`\r\n`./configure`\r\n`bazel build //tensorflow:libtensorflow_cc.so`\r\n\r\n### Describe the problem\r\nSame problem for all branches listed under **TensorFlow version**.\r\nBuild fails immediately with\r\n`Cuda Configuration Error: Cannot find libdevice.10.bc under /usr/local/cuda-8.0`\r\nIt's strange because I explicitly answered 8.0 versions and libdevice.10.bc exists only in 9.0 AFAIK.\r\n\r\n### Source code / logs\r\n[error.log](https://github.com/tensorflow/tensorflow/files/1822159/error.log)\r\n\r\n[configure.log](https://github.com/tensorflow/tensorflow/files/1822155/configure.log)\r\n\r\n[tf_configure.txt](https://github.com/tensorflow/tensorflow/files/1822157/tf_configure.txt)\r\n", "comments": ["I met the same problem, hope someone can help me. Thanks so much!\r\nBazel version (if compiling from source): 0.10.0", "I met the same problem, finally, i solved it. \r\nwe make a file /usr/local/cuda&cuda-8.0/nvvm/libdevice ( libdevice.compute_50.10.bc) to (libdevice.10.bc).\r\nIt solved the problem.\r\ntip:if you want change name easily, you should use this code:sudo nautilus\r\n\r\n\r\n", "sorry,I don't understand.\r\nShall we make a new file under /usr/local/cuda-8.0 named libdevice.10.bc\r\nOr we move the libdevice.compute_50.10.bc under /usr/local/cuda&cuda-8.0/nvvm/libdevice to /usr/local/cuda-8.0 and rename it  libdevice.10.bc", "In my case, I solved, just change the file name(libdevice.compute_50.10.bc--->libdevice.10.bc) in /usr/local/cuda&cuda-8.0/nvvm/libdevice.\r\nI use gtx 1070.", "@HwangIkHwan yeah, it is successful, thank you so much!", "@HwangIkHwan  it works, thanks!", "Sounds like good documentation was created here on how to workaround this peculiar problem. Thanks everyone in the community for supporting.", "@jart That's not \"peculiar problem\". Current configurator will not work on any device with Cuda 8.0 and Tensorflow 1.7, as I understand.\r\nHad the same problem on PC with Ubuntu 16.04.4 x64, GTX 1060.", "Please note the [install documentation](https://www.tensorflow.org/install/install_linux) says CUDA 9 is required. If you have CUDA 8, please try upgrading CUDA, or downgrading TF.", "@jart Sorry, I missed that 8.0 support is dropped since v1.6. Also some of TF current sources still contain code to deal with CUDA 8.0.", "@roman-orekhov @jart doesn't the CUDA 9 requirement only apply to the precompiled versions (Docker, wheel etc.)? In the documentation about building it from source it mentions Cuda >= 7.0 as hard requirement: https://www.tensorflow.org/install/install_sources#optional_install_tensorflow_for_gpu_prerequisites", "thanks very much\r\nmv libdevice.compute_50.10.bc--->libdevice.10.bc\r\nhelp me a lot!", "@jart \r\nIn \"install from source\" documentation (https://www.tensorflow.org/install/install_sources) stated Cuda >= 7.0. You speak about precomiled versions, we speak about build from source.\r\n\r\nI insist that this is a bug that requires from user manual fixing code or renaming library. It is not ok.", "Same problem here and I believe this is a bug.\r\nEither state CUDA>=9.0 requirement in docs or fix this issue.", "Also hit this problem....", "I ran into the same problem trying to compile 1.8 with CUDA 8", "Also just ran into the same problem trying to compile 1.8 with CUDA 8.0.\r\n\r\nDuplicating the above mentioned problem file with the suggested truncated name also fixed the problem. ", "It does appear to be the case that we document CUDA 7+ for source builds and CUDA 9+ for binary releases, per links above. Reopening and assigning to @gunan who has greater familiarity with CUDA support in TensorFlow.", "@reedwm @chsigg may know more about this.", "Is there a workaround without using root privileges? I am working on a cluster and I cannot move these files.", "has anyone successfully build the lastest version of tensorflow with **cuda8** and may share the python package file?", "@han-qiu, I did (cuDNN 6, on MacOS 10.11.6 using CLT 8.2), but then I realized it was this may be the last release to support CUDA 8 so I upgraded to 10.13 and CUDA 9.1 (cuDNN 7.0.5, CLT 8.3.2) so I don't have the package anymore. Likely wouldn't work anyway unless it was compiled using -march=core2\r\n\r\nI played around with using LLVM (non-Apple Clang) and kinda wanna circle back to making that work. ", "@pjox try to find the line where it searches for libdevice.10:\r\n`grep 'libdevice.10' */*/*`\r\nand replace this occurrence with the according name of the file that is present on your system (e.g. libdevice.compute_50.10.bc). then run the building process.\r\n\r\nI tested it for myself, worked.", "I do not know what the issue is. But using `git bisect`, I found the issue is caused by c79c9512486daa119d3cda9c00bb36acb3933a5b.\r\n\r\n@pwnall, can you fix this?", "@dominikzietlow Thank you, I think it is in [here](https://github.com/tensorflow/tensorflow/blob/851c28951b7345f303f4ec1d6490e3fadaa0a40e/third_party/toolchains/gpus/cuda/BUILD) line 1199. I'll have to test it.", "@pjox That is not the file I meant, I was referring to the file\r\n`third_party/gpus/cuda_configure.bzl`\r\nin line _722_\r\nsee [here](https://github.com/tensorflow/tensorflow/blob/851c28951b7345f303f4ec1d6490e3fadaa0a40e/third_party/gpus/cuda_configure.bzl#L722) \r\n\r\nthis solved it for me and i built in on a cluster yesterday. you might have to install bazel in your home directory in case the cluster installation lacks a recent version of it.", "@reedwm Sorry, I haven't tested on CUDA 8.0.\r\n\r\nI can try to find some time to look at this late next week. Sorry for the high lag, I am currently adjusting to an expanded role at work.", "@dominikzietlow Thanks! your solution worked perfectly. :smile: ", "My solution was:\r\n```\r\ncd /usr/local/cuda-8.0/nvvm/libdevice\r\nsudo ln -s libdevice.compute_50.10.bc libdevice.10.bc\r\n```\r\n\r\n", "@HwangIkHwan \r\nThanks.", "Nagging Assignee @gunan: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A change equivalent to https://github.com/pwnall/tensorflow/commit/6a15fb400af3dd013284e9f6150256797e742afe has been committed to the internal repository. This issue should be fixed soon."]}, {"number": 17800, "title": "Enhancement with deprecated_argument_lookup for argmax/argmin", "body": "This fix makes some enhancement for argmax/argmin, using deprecated_argument_lookup instread of customerized logic.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 17799, "title": "Fix typo in `dataset_ops.py`: `datset` -> `dataset`.", "body": "Tested:\r\nThis is a noop.", "comments": []}, {"number": 17798, "title": "Count total number of nodes within a name scope", "body": "Is there a way to count the total number of nodes within a name scope? In tensorboard, when you select a name scope, it automatically display the total number of nodes in that scope. However I can't find anything online that describes what is been used to count the total number of nodes in a scope.\r\n", "comments": ["`len([x for x in tf.get_default_graph().get_operations() if x.name.startswith('name/')])`"]}, {"number": 17797, "title": "R1.7", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->"]}, {"number": 17796, "title": "contrib/lite: avoid building benchmark_model in build_ios_universal_lib", "body": "It is not needed part of this script. In addition, since f0633ec benchmark_model\r\nnow depends on //tensorflow/core library which is not taken into account by the\r\nMakefile from contrib/lite, and thus causes failure:\r\n\r\n    $ ./build_ios_universal_lib.sh\r\n    ...\r\n    In file included from tensorflow/contrib/lite/tools/benchmark_model.cc:29:\r\n    In file included from ./tensorflow/core/platform/env.h:24:\r\n    In file included from ./tensorflow/core/lib/core/errors.h:21:\r\n    ./tensorflow/core/lib/core/status.h:23:10: fatal error: 'tensorflow/core/lib/core/error_codes.pb.h' file not found\r\n    #include \"tensorflow/core/lib/core/error_codes.pb.h\"\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n    1 error generated.\r\n", "comments": []}, {"number": 17795, "title": "Fix nested scope issue", "body": "If you have a `myApp::tensorflow` namespace and you are using TF's logging, then missing global scope specifiers `::` are cause a compile error.\r\n\r\n", "comments": []}, {"number": 17794, "title": "how to write the objectives function roc_auc_score in tflearn by keras", "body": "Dear everyone,\r\n                     I found the roc_auc_score function in  https://github.com/tflearn/tflearn/blob/master/tflearn/objectives.py. Now I want to write this function by keras. But failed. The following is the code:\r\n```\r\ndef roc_auc_score(y_pred, y_true):\r\n    \"\"\" ROC AUC Score.\r\n    Approximates the Area Under Curve score, using approximation based on\r\n    the Wilcoxon-Mann-Whitney U statistic.\r\n    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\r\n    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\r\n    Measures overall performance for a full range of threshold levels.\r\n    Arguments:\r\n        y_pred: `Tensor`. Predicted values.\r\n        y_true: `Tensor` . Targets (labels), a probability distribution.\r\n    \"\"\"\r\n    with tf.name_scope(\"RocAucScore\"):\r\n        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\r\n        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\r\n        pos = tf.expand_dims(pos, 0)\r\n        neg = tf.expand_dims(neg, 1)\r\n        # original paper suggests performance is robust to exact parameter choice\r\n        gamma = 0.2\r\n        p     = 3\r\n        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\r\n        masked = tf.boolean_mask(difference, difference < 0.0)\r\n        return tf.reduce_sum(tf.pow(-masked, p))\r\n```\r\n\r\n\r\n**The new code was changed to the following:**\r\n\r\n```\r\ndef roc_auc_score(y_pred, y_true):\r\n    \"\"\" ROC AUC Score.\r\n    Approximates the Area Under Curve score, using approximation based on\r\n    the Wilcoxon-Mann-Whitney U statistic.\r\n    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\r\n    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\r\n    Measures overall performance for a full range of threshold levels.\r\n    Arguments:\r\n        y_pred: `Tensor`. Predicted values.\r\n        y_true: `Tensor` . Targets (labels), a probability distribution.\r\n    \"\"\"\r\n    pos = tf.boolean_mask(y_pred, K.cast(y_true, tf.bool))\r\n    neg = tf.boolean_mask(y_pred, ~K.cast(y_true, tf.bool))\r\n    pos = K.expand_dims(pos, 0)\r\n    neg = K.expand_dims(neg, 1)\r\n    # original paper suggests performance is robust to exact parameter choice\r\n    gamma = 0.2\r\n    p     = 3\r\n    difference = K.zeros_like(pos * neg) + pos - neg - gamma\r\n    masked = tf.boolean_mask(difference, difference < 0.0)\r\n    return K.sum(K.pow(-masked, p))\r\n```\r\n\r\n**All the code are the following, it doesn't work,** \r\n **The data in** \r\nhttps://pan.baidu.com/s/12yJCWdfvVW1tEKUfEU34RQ ,  password: nu98\r\n```\r\n# -*- coding: UTF-8 -*-\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\nimport numpy as np\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Activation, Dropout, Flatten, Dense\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\r\nfrom keras import optimizers\r\nfrom keras import applications\r\nfrom keras.models import Model\r\nimport keras\r\nimport numpy as np\r\nfrom keras import backend as K\r\nimport tensorflow as tf\r\n\r\n\r\nprint(\"---------------------------AUC----------------------------------------\")\r\n\r\n\r\n# def roc_auc_score(y_pred, y_true):\r\n#     \"\"\" ROC AUC Score.\r\n#     Approximates the Area Under Curve score, using approximation based on\r\n#     the Wilcoxon-Mann-Whitney U statistic.\r\n#     Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\r\n#     Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\r\n#     Measures overall performance for a full range of threshold levels.\r\n#     Arguments:\r\n#         y_pred: `Tensor`. Predicted values.\r\n#         y_true: `Tensor` . Targets (labels), a probability distribution.\r\n#     \"\"\"\r\n#     with tf.name_scope(\"RocAucScore\"):\r\n\r\n#         pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\r\n#         neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\r\n\r\n#         pos = tf.expand_dims(pos, 0)\r\n#         neg = tf.expand_dims(neg, 1)\r\n\r\n#         # original paper suggests performance is robust to exact parameter choice\r\n#         gamma = 0.2\r\n#         p     = 3\r\n\r\n#         difference = tf.zeros_like(pos * neg) + pos - neg - gamma\r\n\r\n#         masked = tf.boolean_mask(difference, difference < 0.0)\r\n\r\n#         return tf.reduce_sum(tf.pow(-masked, p))\r\n\r\n\r\n\r\ndef roc_auc_score(y_pred, y_true):\r\n    \"\"\" ROC AUC Score.\r\n    Approximates the Area Under Curve score, using approximation based on\r\n    the Wilcoxon-Mann-Whitney U statistic.\r\n    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\r\n    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\r\n    Measures overall performance for a full range of threshold levels.\r\n    Arguments:\r\n        y_pred: `Tensor`. Predicted values.\r\n        y_true: `Tensor` . Targets (labels), a probability distribution.\r\n    \"\"\"\r\n    pos = tf.boolean_mask(y_pred, K.cast(y_true, tf.bool))\r\n    neg = tf.boolean_mask(y_pred, ~K.cast(y_true, tf.bool))\r\n    pos = K.expand_dims(pos, 0)\r\n    neg = K.expand_dims(neg, 1)\r\n    # original paper suggests performance is robust to exact parameter choice\r\n    gamma = 0.2\r\n    p     = 3\r\n    difference = K.zeros_like(pos * neg) + pos - neg - gamma\r\n    masked = tf.boolean_mask(difference, difference < 0.0)\r\n    return K.sum(K.pow(-masked, p))\r\n\r\ndef roc_auc_score_loss(y_true, y_pred):\r\n    return roc_auc_score(y_true, y_pred)\r\nprint(\"---------------------------AUC----------------------------------------\")\r\n\r\n\r\n# dimensions of our images.\r\nimg_width, img_height = 512, 512\r\n\r\ntrain_data_dir = 'data/train/'\r\nvalidation_data_dir = 'data/validation/'\r\n\r\n\r\n##preprocessing\r\n# used to rescale the pixel values from [0, 255] to [0, 1] interval\r\ndatagen = ImageDataGenerator(rescale=1./255)\r\nbatch_size = 32\r\n\r\n# automagically retrieve images and their classes for train and validation sets\r\ntrain_generator = datagen.flow_from_directory(\r\n        train_data_dir,\r\n        target_size=(img_width, img_height),\r\n        batch_size=batch_size,\r\n        class_mode='binary')\r\n\r\nvalidation_generator = datagen.flow_from_directory(\r\n        validation_data_dir,\r\n        target_size=(img_width, img_height),\r\n        batch_size=batch_size,\r\n        class_mode='binary',shuffle=False)\r\n\r\n\r\n\r\n# a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers.\r\nmodel = Sequential()\r\nmodel.add(Convolution2D(32, (3, 3), input_shape=(img_width, img_height,3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Convolution2D(32, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Convolution2D(64, (3, 3)))\r\nmodel.add(Activation('relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\n\r\nmodel.add(Flatten())\r\nmodel.add(Dense(64))\r\nmodel.add(Activation('relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(1))\r\nmodel.add(Activation('sigmoid'))\r\n\r\n# model.compile(loss='binary_crossentropy',\r\n#               optimizer='rmsprop',\r\n#               metrics=['accuracy','mae'])\r\n\r\nmodel.compile(loss=roc_auc_score_loss,\r\n              optimizer='rmsprop',\r\n              metrics=['accuracy','mae',roc_auc_score])\r\nepochs = 5\r\n\r\ntrain_samples = 2048\r\nvalidation_samples = 832\r\n\r\n\r\nmodel.fit_generator(\r\n        train_generator,\r\n        steps_per_epoch=train_samples // batch_size,\r\n        epochs=epochs,\r\n        validation_data=validation_generator,\r\n        validation_steps=validation_samples// batch_size)\r\n\r\nmodel.save_weights('models/basic_cnn_30_epochs.h5')\r\nprint(model.summary())\r\n\r\n\r\n```\r\n**The problem is the following :**+1: \r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-cc3e34fe5d20> in <module>()\r\n    149         epochs=epochs,\r\n    150         validation_data=validation_generator,\r\n--> 151         validation_steps=validation_samples// batch_size)\r\n    152 \r\n    153 model.save_weights('models/basic_cnn_30_epochs.h5')\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)\r\n     85                 warnings.warn('Update your `' + object_name +\r\n     86                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\r\n---> 87             return func(*args, **kwargs)\r\n     88         wrapper._original_function = func\r\n     89         return wrapper\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/models.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\r\n   1119                                         workers=workers,\r\n   1120                                         use_multiprocessing=use_multiprocessing,\r\n-> 1121                                         initial_epoch=initial_epoch)\r\n   1122 \r\n   1123     @interfaces.legacy_generator_methods_support\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)\r\n     85                 warnings.warn('Update your `' + object_name +\r\n     86                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\r\n---> 87             return func(*args, **kwargs)\r\n     88         wrapper._original_function = func\r\n     89         return wrapper\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   1924 \r\n   1925         do_validation = bool(validation_data)\r\n-> 1926         self._make_train_function()\r\n   1927         if do_validation:\r\n   1928             self._make_test_function()\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc in _make_train_function(self)\r\n    958                     training_updates = self.optimizer.get_updates(\r\n    959                         params=self._collected_trainable_weights,\r\n--> 960                         loss=self.total_loss)\r\n    961                 updates = self.updates + training_updates\r\n    962                 # Gets loss and metrics. Updates weights at each call.\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc in wrapper(*args, **kwargs)\r\n     85                 warnings.warn('Update your `' + object_name +\r\n     86                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\r\n---> 87             return func(*args, **kwargs)\r\n     88         wrapper._original_function = func\r\n     89         return wrapper\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/optimizers.pyc in get_updates(self, loss, params)\r\n    235         for p, g, a in zip(params, grads, accumulators):\r\n    236             # update accumulator\r\n--> 237             new_a = self.rho * a + (1. - self.rho) * K.square(g)\r\n    238             self.updates.append(K.update(a, new_a))\r\n    239             new_p = p - lr * g / (K.sqrt(new_a) + self.epsilon)\r\n\r\n/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc in square(x)\r\n   1356         A tensor.\r\n   1357     \"\"\"\r\n-> 1358     return tf.square(x)\r\n   1359 \r\n   1360 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc in square(x, name)\r\n    447           indices=x.indices, values=x_square, dense_shape=x.dense_shape)\r\n    448     else:\r\n--> 449       return gen_math_ops.square(x, name=name)\r\n    450 \r\n    451 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc in square(x, name)\r\n   4565   if _ctx.in_graph_mode():\r\n   4566     _, _, _op = _op_def_lib._apply_op_helper(\r\n-> 4567         \"Square\", x=x, name=name)\r\n   4568     _result = _op.outputs[:]\r\n   4569     _inputs_flat = _op.inputs\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    526               raise ValueError(\r\n    527                   \"Tried to convert '%s' to a tensor and failed. Error: %s\" %\r\n--> 528                   (input_name, err))\r\n    529             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\r\n    530                       (input_name, op_type_name, observed))\r\n\r\nValueError: Tried to convert 'x' to a tensor and failed. Error: None values not supported.\r\n\r\nFinally, Anyone can check this problem. Looking forward to reply. Thanks advanced!\r\n### System information\r\nOperating System: Ubuntu 16.04 LTS\r\nGraphics card: Tesla K40\r\nInstalled version of CUDA: 8.0 \r\nInstalled version of cuDNN: v5 , for CUDA 8.0 \r\npip --version 9.0.1\r\npip 9.0.1 from /usr/local/lib/python2.7/dist-packages (python 2.7)\r\npip install tensorflow-gpu\r\nName: tensorflow-gpu, Version: 1.4.1", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "### System information\r\nOperating System: Ubuntu 16.04 LTS\r\nGraphics card: Tesla K40\r\nInstalled version of CUDA: 8.0 \r\nInstalled version of cuDNN: v5 , for CUDA 8.0 \r\npip --version 9.0.1\r\npip 9.0.1 from /usr/local/lib/python2.7/dist-packages (python 2.7)\r\npip install tensorflow-gpu\r\nName: tensorflow-gpu, Version: 1.4.1\r\n", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "The problem here seems to be with the differentiability of the loss function. In your custom loss function, you use `K.cast` which is not differentiable and hence produces `None` values at the output.\r\n\r\nYou may want to replace that with a different function or operation in order to use `roc_auc_score` as a loss function.\r\n\r\nIf you can make sure that the values of `y_true` are either 1 or 0 before they are used in this function (Which I believe should be possible since it is the ground truth labels) here's a small hack to perform the boolean_mask operation without using `K.cast`.\r\n\r\n> pos = tf.boolean_mask(y_pred, K.cast(y_true, tf.bool))\r\n    neg = tf.boolean_mask(y_pred, ~K.cast(y_true, tf.bool))\r\n\r\nreplace with\r\n\r\n> pos = tf.boolean_mask(y_pred, y_true)\r\n   neg = tf.boolean_mask(y_pred, (y_true-1)*(-1))\r\n\r\nI have used a similar technique in the past for numpy arrays and so should also work for tensors!\r\n\r\nGood luck!\r\n\r\nSP"]}, {"number": 17793, "title": "unable to find libcuda.so", "body": "I can use  tensorflow to program, but have a problem: \r\n\r\n***\r\n2018-03-17 17:33:18.848935: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: **was unable to find libcuda.so DSO loaded into this program**\r\n***\r\n\r\nI can see my GPU by using `nvidia-smi`.\r\n\r\nAnyone can help me to solve this ?\r\nThanks !\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17792, "title": "optimizer.apply_gradients fails inside tfe.defun", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: colab\r\n- **TensorFlow installed from (source or binary)**: colab\r\n- **TensorFlow version (use command below)**: `unknown 1.6.0`\r\n- **Python version**: 3.6.3\r\n- **Exact command to reproduce**: Colab: https://drive.google.com/file/d/1zj9IMWKF58TuEQRKt9DpETr24Z1sy9z0/view?usp=sharing\r\n\r\n### Describe the problem\r\n`optimizer.apply_gradients` doesn't seem to work inside eager mode's `tfe.defun`.  The colab contains full details, but the relevant part of the exception is\r\n\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py in _get_processor(v)\r\n        180   if context.in_eager_mode():\r\n        181     return _DenseResourceVariableProcessor(v)\r\n    --> 182   if v.op.type == \"VarHandleOp\":\r\n        183     return _DenseResourceVariableProcessor(v)\r\n        184   if isinstance(v, variables.Variable):\r\n    \r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in op(self)\r\n        601   def op(self):\r\n        602     \"\"\"The op for this variable.\"\"\"\r\n    --> 603     return self._handle.op\r\n        604 \r\n        605   def eval(self, session=None):\r\n    \r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in op(self)\r\n        836   @property\r\n        837   def op(self):\r\n    --> 838     raise AttributeError(\"op not supported for Eager Tensors.\")\r\n        839 \r\n        840   @property\r\n    \r\n    AttributeError: op not supported for Eager Tensors.\r\n\r\nThe problem is presumably that `optimizer.py`'s eager mode logic is disabled by defun's graph mode.\r\n\r\n### Source code / logs\r\nReproducing colab: https://drive.google.com/file/d/1zj9IMWKF58TuEQRKt9DpETr24Z1sy9z0/view?usp=sharing", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version\nCUDA/cuDNN version\nGPU model and memory", "Bazel version: via colab\r\nCUDA/cuDNN version: via colab\r\nGPU model and memory: via colab", "@akshayka can you take a look?", "Sure thing, currently investigating ...", "Thank you!"]}, {"number": 17791, "title": "TensorFlowInferenceInterface.feed() fails to accept multi-dimensional input", "body": "### Describe the problem\r\n\r\nMy understanding is that `TensorFlowInferenceInterface.feed()` takes a 1D float array `float[]` as input.\r\n\r\n### Source code / logs\r\n\r\n```java\r\nfloat[] floatValues;\r\nTensorFlowInferenceInterface inferenceInterface;\r\nint inputSize;\r\nString inputName;\r\n...\r\ninferenceInterface.feed(inputName, floatValues, 1, inputSize, inputSize, 3);\r\n```\r\n", "comments": ["could you tell us the solution, please?"]}, {"number": 17790, "title": "Raspbian 9 (Stretch): Failed to load native TensorFlow runtime", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No. Simply calling \"import tensorflow\" already crashes.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian GNU/Linux 9 (Stretch)\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: TensorFlow 1.6.0\r\n- **Python version**: Python 3.5.3\r\n- **Bazel version (if compiling from source)**: Bazel 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: GCC 6.3.0 20170124\r\n- **CUDA/cuDNN version**: I didn't install with CUDA support\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: \"import tensorflow\" in the Python environment\r\n\r\n### Describe the problem\r\nI built TensorFlow on Raspbian Linux for the Raspberry Pi 3 Model B. The problem is that when I try to import it in Python, the program instantly crashes, saying: \"Failed to load native TensorFlow runtime.\" The full traceback is below.\r\n\r\n### Source code / logs\r\nOnly need to call \"import tensorflow\" on Python3 to reproduce the traceback below:\r\n\r\n```\r\nTraceback (most recent call last): File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in _pywrap_tensorflow_internal = swig_import_helper() File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"/usr/lib/python3.5/imp.py\", line 242, in load_module return load_dynamic(name, filename, file) File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic return _load(spec) ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE\r\n```\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n```\r\nTraceback (most recent call last): File \"idex.py\", line 1, in import gui File \"/home/pi/Desktop/IDEX/scripts/gui.py\", line 10, in import fun_util File \"signlang/fun_util.py\", line 3, in import tensorflow as tf File \"/usr/local/lib/python3.5/dist-packages/tensorflow/init.py\", line 24, in from tensorflow.python import * # pylint: disable=redefined-builtin File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/init.py\", line 49, in from tensorflow.python import pywrap_tensorflow File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in raise ImportError(msg) ImportError: Traceback (most recent call last): File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in from tensorflow.python.pywrap_tensorflow_internal import * File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in _pywrap_tensorflow_internal = swig_import_helper() File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description) File \"/usr/lib/python3.5/imp.py\", line 242, in load_module return load_dynamic(name, filename, file) File \"/usr/lib/python3.5/imp.py\", line 342, in load_dynamic return _load(spec) ImportError: /usr/local/lib/python3.5/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions. Include the entire stack trace above this error message when asking for help.\r\n```", "comments": ["Hello :) May I ask if there's any progress on this issue? Thanks.", "The error is that you're missing the following symbol:\r\n```\r\nvoid tensorflow::ConcatCPU<tensorflow::bfloat16>(tensorflow::DeviceBase*, std::vector<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> > > > const&, tensorflow::TTypes<tensorflow::bfloat16, 2, int>::Matrix*)\r\n```\r\nIt was probably built incorrectly. How did you build?", "I built using this command:\r\n\r\nbazel build -c opt --copt=\"-mfpu=neon-vfpv4\" --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package\r\n\r\nSpecifically, I followed the tutorial here:\r\n\r\nhttps://github.com/samjabrahams/tensorflow-on-raspberry-pi/blob/master/GUIDE.md", "Hello. I tried building and installing again following the instructions on the tensorflow website for building from source and I'm getting the same error.\r\n\r\nFirst I cloned the repo\r\n`git clone https://github.com/tensorflow/tensorflow `\r\n\r\nThen I configured it using\r\n`./configure`\r\nI selected\r\n`/usr/bin/python3`\r\nand its respective default dist-packages location. For the other options, I said yes to using jemalloc and then no to all the others. I left the -march=native part as default.\r\n\r\nI then built using:\r\n`bazel build --config=opt --local_resources 512,.5,1.0 //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nThen, I built the pip package using\r\n`bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`\r\n\r\nThen, I installed the pip package via:\r\n`sudo pip3 install /tmp/tensorflow_pkg/tensorflow-1.7.0rc1-cp35-cp35m-linux_armv7l.whl`\r\n\r\nAfterwards, when I opened the python terminal in a separate directory and tried to import tensorflow, I got the same error.\r\n\r\n", "Can you try with `alwayslink = 1` [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L155)?", "I changed alwayslink = 1 and tried rebuilding but it fails with the following error (I added the --verbose_failures option):\r\n`ERROR: /home/pi/Desktop/tensorflow/tensorflow/contrib/lite/toco/BUILD:391:1: Linking of rule '//tensorflow/contrib/lite/toco:toco' failed (Exit 1): gcc failed: error executing command\r\n  (cd /home/pi/.cache/bazel/_bazel_pi/0a9cb2a29ff2847422358ab74afe8d8d/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games:/usr/games \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.5/dist-packages \\\r\n    TF_DOWNLOAD_CLANG=0 \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL_SYCL=0 \\\r\n  /usr/bin/gcc -o bazel-out/arm-py3-opt/bin/tensorflow/contrib/lite/toco/toco '-Wl,-rpath,$ORIGIN/../../../../_solib_arm/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow' -Lbazel-out/arm-py3-opt/bin/_solib_arm/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..,-rpath,$ORIGIN/../../..' -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -Wl,-z,notext -pthread '-fuse-ld=gold' -Wl,-no-as-needed -Wl,-z,relro,-z,now -B/usr/bin -B/usr/bin -pass-exit-codes -Wl,--gc-sections -Wl,@bazel-out/arm-py3-opt/bin/tensorflow/contrib/lite/toco/toco-2.params)\r\nbazel-out/arm-py3-opt/bin/tensorflow/core/kernels/_objs/list_kernels/tensorflow/core/kernels/list_kernels.o:list_kernels.cc:function tensorflow::TensorListStack<Eigen::ThreadPoolDevice, tensorflow::bfloat16>::Compute(tensorflow::OpKernelContext*): error: undefined reference to 'void tensorflow::ConcatCPU<tensorflow::bfloat16>(tensorflow::DeviceBase*, std::vector<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> >, std::allocator<std::unique_ptr<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix, std::default_delete<tensorflow::TTypes<tensorflow::bfloat16, 2, int>::ConstMatrix> > > > const&, tensorflow::TTypes<tensorflow::bfloat16, 2, int>::Matrix*)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1766.182s, Critical Path: 275.26s\r\nFAILED: Build did NOT complete successfully`", "Oh, that's good, thank you. It means that the definition is actually missing, which is the root cause. Now we need to find out why.", "So, there are two options: 1) you don't actually want the definition in raspbian, or 2) you want it, so you need to define it.\r\n\r\nFor 1), then you can try to track down the usage in `list_kernels.cc` and figure out how to pull it out, or why it is inconsistent. For 2), you need to read the templates and macro definitions carefully (the lazy way out is to build it on a platform where it works, then define it in other file. The linker should tell you where the duplicate definition is)", "I'm having trouble finding where it's actually being used. I tried searching the file itself and there are no results for \"ConcatCPU\". I thought it might have something to do with `#include \"tensorflow/core/kernels/concat_lib.h\"` but that's gonna affect a lot of other things if I change it. To clarify, I'm looking at `tensorflow/core/kernels/list_kernels.cc`\r\n[UPDATE]\r\nI thought about commenting out `#include \"tensorflow/core/kernels/concat_lib.h` as it doesn't seem to have any other uses in `tensorflow/core/kernels/list_kernels.cc`. I'm recompiling right now. I'll give another update once it's done.", "After recompiling, with the include statement commented out, it still throws the same error saying there's an undefined refernce to the ConcatCPU function. Nowhere in the file is there a mention of the said function though. It says that the error is in the `tensorflow::TensorListStack` function but I can't find that definition either.\r\n[UPDATE]\r\nI found the reference to ConcatCPU. It wasn't in `list_kernels.cc`, it was in `list_kernels.h`. I commented out the ConcatCPU reference and recompiling. Will provide updates.", "Ok it now compiles fine and I can run tensorflow now. Thank you!\r\nTo clarify, what I did was comment out this [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/list_kernels.h#L126)", "Glad it worked for you!", "I'm having the same issue, isn't this line needed for this compute function? Is it ok to close the issue?", "Hello! I'm not really sure but I was able to run Tensorflow applications normally despite commenting out that line so it doesn't seem to be a problem.", "Hello, I've been fiddling around this for a while, there's actually a clean method without commenting code lines or any ad-hoc solutions, for native building on ARM (I used Orange Pi PC Plus, ARMBIAN 5.38 stable Ubuntu 16.04.4 LTS 4.14.18-sunxi), build bazel version 0.11.1, then I checked out to the [revision with latest successful nightly build](http://ci.tensorflow.org/view/Nightly/job/nightly-pi/238/), luckily it's version 1.7.0, then you should pass one of the following macros while building (**ARM_NON_MOBILE** or **RASPBERRY_PI**), the building command becomes as follows:\r\n`git checkout -f d82b2f71b60d5fff48884c20c7b85e517330e91f`\r\n`bazel build -c opt --copt=-DRASPBERRY_PI --copt=-mfpu=neon-vfpv4 --copt=-std=gnu11 --copt=-DS_IREAD=S_IRUSR --copt=-DS_IWRITE=S_IWUSR --copt=-O3 --config=monolithic --copt=-funsafe-math-optimizations --copt=-ftree-vectorize --copt=-fomit-frame-pointer --local_resources 400,2.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package 2>&1 | tee ~/tf_nativebuild.log`\r\nI found the hint about the macros in this file `tensorflow/core/platform/platform.h`\r\n> // Require an outside macro to tell us if we're building for Raspberry Pi or\r\n// another ARM device that's not a mobile platform.\r\n\r\nAnother very convenient alternative is to use the cross-building script that tensorflow maintainers use for nightly ci builds, it's easier and also much faster, it creates a docker container, installs all required dependencies inside it, then cross-builds tensorflow for arm, you also need to checkout to the stable revision\r\n`git checkout -f d82b2f71b60d5fff48884c20c7b85e517330e91f`\r\n`tensorflow/tools/ci_build/ci_build.sh PI tensorflow/tools/ci_build/pi/build_raspberry_pi.sh 2>&1 | tee ~/tf_crossbuild.log`", "I used the same command, as mbahaa.\r\nWhat I found is that --copt=IS_RASPBERRY is not passed into build, so IS_MOBILE_PLATFORM is defined in the build. To prove this, I added #error at where IS_MOBILE_PLATFORM is defined, so that the build failed very early.\r\nThe error message also shows the actual build option, IS_RASPBERRY is not defined there and std=c++0x shows up instead of std=c++11 or std=gnu11.\r\nTherefore, the root cause is that somehow bazel build c options are not passed in."]}]