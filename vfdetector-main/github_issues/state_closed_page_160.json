[{"number": 50010, "title": "CherryPick:2.1 Fix  bugs", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50010) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50010) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 50009, "title": "CherryPick2.4:Fix heap-buffer-overflow issue with tf.raw_ops.SparseFillEmptyRows.", "body": null, "comments": []}, {"number": 50008, "title": "CherryPick2.3:Fix heap-buffer-overflow issue with tf.raw_ops.SparseFillEmptyRows.", "body": null, "comments": []}, {"number": 50007, "title": "CherryPick2.2:Fix heap-buffer-overflow issue with tf.raw_ops.SparseFillEmptyRows.", "body": null, "comments": []}, {"number": 50006, "title": "CherryPick2.1: Fix heap-buffer-overflow issue with tf.raw_ops.SparseFillEmptyRows", "body": null, "comments": []}, {"number": 50005, "title": "support SyncBatchNormalization gradient in loaded saved_model", "body": "**System information**\r\n- TensorFlow version (you are using): 2.4.1\r\n- Are you willing to contribute it (Yes/No):\r\nAbsolutely.  Just need some code pointers/guidance.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nLoading the saved_model included with the [SimCLR](https://github.com/google-research/simclr/tree/master/tf2) repository using `tf.saved_model.load` yields the error message:\r\n```\r\nW0602 17:01:50.030123 3770209 function_deserialization.py:573] Importing a function (__inference_sync_batch_normalization_2_layer_call_\r\nand_return_conditional_losses_29971) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\r\n```\r\n\r\nThis is caused by tf.keras.layers.experimental.SyncBatchNormalization. \r\n\r\n**Will this change the current api? How?**\r\nWon't\r\n\r\n**Who will benefit with this feature?**\r\nAnyone attempting to use tf.keras.layers.experimental.SyncBatchNormalization from a saved_model.\r\n\r\n**Any Other info.**\r\nI'm interested in contributing this as it would help me with some research I am performing.   I am attempting to implement some functions that measure the robustness of a given saved_model for [Neural Structured Learning](https://www.tensorflow.org/neural_structured_learning).  In order to do this, I'd like to perform Projected Gradient Descent on a given SavedModel, which requires the gradient to be included.", "comments": ["@LukeWood Can you please share a standalone code to demonstrate what is not possible with the current saved_model? \r\n\r\nGenerally, when you use `keras` layers, it is better to save the model with `tf.keras.models.save` and load with `tf.keras.models.load_model`. Did you try saving it as a `keras` model and load. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 50004, "title": "ImportError: cannot import name 'LayerNormalization' ", "body": "Description:\r\n----------------\r\nCannot import tensorflow 2.5.0 or 2.4.0 due to this error (started only recently and abruptly)\r\n\r\n\r\nTrace:\r\n-----------\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py\", line 48, in <module>\r\n    from tensorflow.python import keras\r\n  File \"/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/__init__.py\", line 27, in <module>\r\n    from tensorflow.python.keras import models\r\n  File \"/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/models.py\", line 27, in <module>\r\n    from tensorflow.python.keras.engine import sequential\r\n  File \"/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\", line 28, in <module>\r\n    from tensorflow.python.keras import layers as layer_module\r\n  File \"/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/__init__.py\", line 177, in <module>\r\n    from tensorflow.python.keras.layers.normalization import LayerNormalization\r\nImportError: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization/__init__.py)\r\n\r\n\r\nEnvironment:\r\n--------------\r\nPython Version: 3.7.8\r\n\r\nattaching tf_env_collect.sh script results as well, the env results are for 2.4.0 but at first, it began in tf 2.5.0. (downgrading did not work too)\r\n\r\n[old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/6587992/old-tf_env.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6587994/tf_env.txt)\r\n\r\n\r\n", "comments": ["just to add, I found the only resource regarding this at stackoverflow [https://stackoverflow.com/questions/60953779/error-when-importing-layernormalization-from-keras-layers](url) however the answer is not apt", "@sidphbot \r\nCould you please share code snippet/colab gist to reproduce the issue reported and which help us to analyze the error .Thanks", "Hi, its an import error, the code is simply \r\n\r\nimport tensorflow as tf\r\n\r\nIt seems to be an environment issue but is way wierd considering fresh installations are also not working after thr issue starts appearing, and it is hard to replicate, please check the stackoverflow link above to see others with the issue with layer normalization. There are more links but mostly point to how we should import tf.keras.something instead of keras.something. The problem started appearing during simple mnist import from tensorflow-datasets (suddenly as the code was working seconds earlier)\r\n", "@sidphbot Strange error. I think the stackoverflow error you referenced is mainly due to incompatibility (between keras and TF) which is not related to your issue. We don't have control on Anaconda based TensorFlow builds. \r\n\r\nCan you please check whether you get same error when you run `import tensorflow as tf` in your command prompt? Can you try using TF `pip` builds instead of Anaconda's TF builds? Thanks!", "Hi, Yes I had verified earlier itself the problem persists even in an\ninteractive python shell for both versions.\n\nOn Fri, Jun 4, 2021 at 2:18 AM Vishnuvardhan Janapati <\n***@***.***> wrote:\n\n> @sidphbot <https://github.com/sidphbot> Strange error. I think the\n> stackoverflow error you referenced is mainly due to incompatibility\n> (between keras and TF) which is not related to your issue. We don't have\n> control on Anaconda based TensorFlow builds.\n>\n> Can you please check whether you get same error when you run import\n> tensorflow as tf in your command prompt? Can you try using TF pip builds\n> instead of Anaconda's TF builds? Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/50004#issuecomment-854266470>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AFAIMS2EPFFLMRBMRP3HSRLTRALUNANCNFSM457WKZRQ>\n> .\n>\n", "I will try un-installing anaconda build and installing pip builds but it is\nquite strange.\n\nOn Fri, Jun 4, 2021 at 4:30 AM Sidharth Pal ***@***.***>\nwrote:\n\n> Hi, Yes I had verified earlier itself the problem persists even in an\n> interactive python shell for both versions.\n>\n> On Fri, Jun 4, 2021 at 2:18 AM Vishnuvardhan Janapati <\n> ***@***.***> wrote:\n>\n>> @sidphbot <https://github.com/sidphbot> Strange error. I think the\n>> stackoverflow error you referenced is mainly due to incompatibility\n>> (between keras and TF) which is not related to your issue. We don't have\n>> control on Anaconda based TensorFlow builds.\n>>\n>> Can you please check whether you get same error when you run import\n>> tensorflow as tf in your command prompt? Can you try using TF pip builds\n>> instead of Anaconda's TF builds? Thanks!\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/50004#issuecomment-854266470>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AFAIMS2EPFFLMRBMRP3HSRLTRALUNANCNFSM457WKZRQ>\n>> .\n>>\n>\n", "@sidphbot Any progress? When you uninstall, look for `tensorflow` folders and delete them completely, then restart and install `pip` [build](https://www.tensorflow.org/install) and finally test it. Also, can you check whether you have any `TF1.x` version residing somewhere in your system? Thanks", "Hi, simple uninstall and install worked, I was hesitant because i did not want to disturb the dependencies, but anyways its back to normal :)", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50004\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50004\">No</a>\n", "install and uninstall of what?", "> install and uninstall of what?\r\n\r\nFrom [this](https://stackoverflow.com/questions/68080345/importerror-cannot-import-name-layernormalization-from-tensorflow-python-ker) stack overflow uninstalling and reinstalling tensorflow worked for them.\r\n\r\nhttps://stackoverflow.com/questions/68080345/importerror-cannot-import-name-layernormalization-from-tensorflow-python-ker", "It was because of module name conflicts. Until v2.5, there were `tensorflow/python/keras/layers/normalization.py` and  a directory `tensorflow/python/keras/normalization`. The author used `normalization` in two places. It caused the error.\r\n\r\nv2.6 solved it.\r\nHowever, v2.6 is not officially released, so you should install 2.6.0rc0 by `pip install -U tensorflow==2.6.0rc0`"]}, {"number": 50003, "title": "CherryPick2.1:Define TfLiteFloatArrayGetSizeInBytes even when TF_LITE_STATIC_MEMORY\u2026", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50003) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 50001, "title": "CherryPick2.4:Add depth_to_space TFLite op", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50001) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 50000, "title": "CherryPick2.3:Add depth_to_space TFLite op", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50000) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49999, "title": "CherryPick2.1:Add depth_to_space TFLite op", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49999) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49998, "title": "CherryPick2.2:Add depth_to_space TFLite op", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49998) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49997, "title": "Cherry pick 2.4 TFLite: Error out when the graph has a recurion.", "body": null, "comments": []}, {"number": 49996, "title": "Cherrypick2.3 TFLite: Error out when the graph has a recurion.", "body": null, "comments": []}, {"number": 49995, "title": "Cherry pick 2.2 TFLite: Error out when the graph has a recurion.", "body": null, "comments": []}, {"number": 49994, "title": "Cherry pick 2.1 TFLite: Error out when the graph has a recurion.", "body": null, "comments": []}, {"number": 49992, "title": "Cherry pick 2.3 Add missing valuidation to FusedBatchNorm", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49992) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49991, "title": "Cherry pick 2.2 Add missing valuidation to FusedBatchNorm", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49991) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49990, "title": "Cherry pick 2.1 Add missing valuidation to FusedBatchNorm", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F49990) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 49989, "title": "Fix r2.4 branch after cherrypicks", "body": "", "comments": []}, {"number": 49988, "title": "Fix r2.3 branch after cherrypicks", "body": "", "comments": []}, {"number": 49987, "title": "Fix r2.2 branch after cherrypicks", "body": "", "comments": []}, {"number": 49986, "title": "Fix the 2.1 branch after cherrypicks", "body": "", "comments": []}, {"number": 49985, "title": "[XLA] Change the optimized dump filename to specify the target", "body": "Change the dump filename to know which backend generated it and which SM.\r\n\r\nXLA optimizations specialize the HLO to the target, so knowing what was the target is very useful.\r\n\r\nIdeally, we should also add the cublas/cudnn version used by the autotuner. But I do not need that now, so I let this for later.\r\n\r\n@sanjoy ", "comments": []}, {"number": 49984, "title": "Update Release.md with 2.5.0 notes.", "body": "", "comments": []}, {"number": 49983, "title": "TF 2.5.0 with CUDA 10.2 (in container) Build error extract_volume_patches_op_gpu failed: crosstool_wrapper_driver_is_not_gcc", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04 (base container, see below) \r\n- Docker base container: nvidia/cuda:10.2-cudnn8-devel-ubuntu18.04\r\n- TensorFlow installed from:source\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8\r\n- Installed using: building for pip install\r\n- Bazel version: 3.7.2\r\n- GCC/Compiler version: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: 10.2 / 8.2\r\n- GPU model and memory: RTX 3090 / 24GB\r\n\r\n**Describe the problem**\r\n\r\nWhen trying to build TF 2.5.0 with Cuda 10.2, after applying the patch proposed in https://github.com/tensorflow/tensorflow/pull/48393 I get the following build error:\r\n\r\n```\r\n- Environment variables set:\r\nTF_CUDA_CLANG=0\r\nTF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5\r\nTF_CUDA_VERSION=10.2\r\nTF_CUDNN_VERSION=8\r\nTF_DOWNLOAD_CLANG=0\r\nTF_DOWNLOAD_MKL=0\r\nTF_ENABLE_XLA=0\r\nTF_NCCL_VERSION=2\r\nTF_NEED_AWS=0\r\nTF_NEED_COMPUTECPP=0\r\nTF_NEED_CUDA=1\r\nTF_NEED_GCP=0\r\nTF_NEED_GDR=0\r\nTF_NEED_HDFS=0\r\nTF_NEED_JEMALLOC=1\r\nTF_NEED_KAFKA=0\r\nTF_NEED_MKL=0\r\nTF_NEED_MPI=0\r\nTF_NEED_OPENCL=0\r\nTF_NEED_OPENCL_SYCL=0\r\nTF_NEED_ROCM=0\r\nTF_NEED_S3=0\r\nTF_NEED_TENSORRT=0\r\nTF_NEED_VERBS=0\r\nTF_SET_ANDROID_WORKSPACE=0\r\nGCC_HOST_COMPILER_PATH=/usr/bin/gcc\r\nCC_OPT_FLAGS=\r\nPYTHON_BIN_PATH=/usr/local/bin/python\r\nPYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages\r\n-- ./configure output:\r\nYou have bazel 3.7.2 installed.\r\nFound CUDA 10.2 in:\r\n    /usr/local/cuda-10.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-10.2/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\nFound NCCL 2 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n```\r\n\r\nBuilt using `bazel build --verbose_failures --config=opt --config=v2 --config=cuda //tensorflow/tools/pip_package:build_pip_package`\r\n\r\nError log\r\n```\r\n[25,186 / 36,992] Compiling tensorflow/lite/toco/tflite/operator.cc; 13s local ... (32 actions, 31 running)\r\nERROR: /usr/local/src/tensorflow/tensorflow/core/kernels/image/BUILD:241:18: C++ compilation of rule '//tensorflow/core/kernels/image:extract_volume_patches_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \\\r\n    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \\\r\n    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5 \\\r\n    TF_CUDA_VERSION=10.2 \\\r\n    TF_CUDNN_VERSION=8 \\\r\n    TF_NCCL_VERSION=2 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/mkl_dnn_v1 -iquote bazel-out/k8-opt/bin/external/mkl_dnn_v1 -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/mkl_dnn_v1/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_60' '--cuda-gpu-arch=sm_60' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' '--cuda-include-ptx=sm_70' '--cuda-gpu-arch=sm_70' '--cuda-include-ptx=sm_75' '--cuda-gpu-arch=sm_75' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/image/extract_volume_patches_op_gpu.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal/com_google_absl/absl/functional/function_ref.h:100:29: error: parameter packs not expanded with \u2018...\u2019:\r\n   template <typename F, typename = EnableIfCompatible<const F&>>\r\n                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                \r\nexternal/com_google_absl/absl/functional/function_ref.h:100:29: note:         \u2018Args\u2019\r\nexternal/com_google_absl/absl/functional/function_ref.h:114:13: error: parameter packs not expanded with \u2018...\u2019:\r\n       typename F, typename = EnableIfCompatible<F*>,\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                \r\nexternal/com_google_absl/absl/functional/function_ref.h:114:13: note:         \u2018Args\u2019\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nERROR: /usr/local/src/tensorflow/tensorflow/lite/toco/python/BUILD:89:10 C++ compilation of rule '//tensorflow/core/kernels/image:extract_volume_patches_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \\\r\n    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \\\r\n    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5 \\\r\n    TF_CUDA_VERSION=10.2 \\\r\n    TF_CUDNN_VERSION=8 \\\r\n    TF_NCCL_VERSION=2 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/mkl_dnn_v1 -iquote bazel-out/k8-opt/bin/external/mkl_dnn_v1 -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/mkl_dnn_v1/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_60' '--cuda-gpu-arch=sm_60' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' '--cuda-include-ptx=sm_70' '--cuda-gpu-arch=sm_70' '--cuda-include-ptx=sm_75' '--cuda-gpu-arch=sm_75' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/image/extract_volume_patches_op_gpu.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 1137.195s, Critical Path: 92.10s\r\nINFO: 25995 processes: 11503 internal, 14492 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\nCommand exited with non-zero status 1\r\n```\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\nDiff from known functional build for 10.2 with 2.4.1\r\nhttps://github.com/datamachines/cuda_tensorflow_opencv/compare/20210601\r\n\r\nSteps to reproduce: \r\n```\r\n% git clone https://github.com/datamachines/cuda_tensorflow_opencv.git\r\n% cd cuda_tensorflow_opencv\r\n% git checkout 737f860\r\n% make make cudnn_tensorflow_opencv-10.2_2.5.0_3.4.14\r\n```\r\n\r\nAttaching full build log.\r\n[cudnn_tensorflow_opencv-10.2_2.5.0_3.4.14-20210601.log.txt](https://github.com/tensorflow/tensorflow/files/6586712/cudnn_tensorflow_opencv-10.2_2.5.0_3.4.14-20210601.log.txt)\r\n", "comments": ["I will add compilation of non GPU version works, and CUDA 11.3 build works as well ", "@mmartial \r\nCould you please try with the correct Cuda,cudnn version, tf2.5 is tested with 11.2/8.1\r\n\r\nThe tested build configurations for linux machine is\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.5.0 | 3.6-3.9 | GCC 7.3.1 | Bazel 3.7.2 | 8.1 | 11.2\r\ntensorflow-2.4.0 | 3.6-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 8.0 | 11.0\r\ntensorflow-2.3.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 7.6 | 10.1\r\ntensorflow-2.2.0 | 3.5-3.8 | GCC 7.3.1 | Bazel 2.0.0 | 7.6 | 10.1\r\n\r\nfor more info please refer this [linuxconfig](https://www.tensorflow.org/install/source#gpu).let me know if it helps.\r\nThanks\r\n\r\n\r\n", "Thank you, I saw the offiiclal build list.\r\n\r\nI am able to build with CUDA 11.3, so I expect will be able to build with 11.2 as well (I will try to confirm).\r\n\r\nTF 2.4.1 which was listed as not working with 10.2 compiled fine in the past, I was hoping to keep the pattern in builds, in particular because the \"Fix cub BUILD\" seemed to indicate it was possible.\r\n\r\nI usually am able to find the errors I encounter on the Github Issues and work around them using those but this recent one did not return a valid match per my search.", "If the solution is to assume TF 2.5.0 is not 10.2 compatible (or tested), it is a valid solution to my issue.", "I have the container image from the error, so I am able to go in it and try some things:\r\n```\r\n% docker run --rm -it ded1a6be0390 /bin/bash\r\n# cd /tmp\r\n# curl -s -Lo /usr/local/bin/bazel https://github.com/bazelbuild/bazelisk/releases/download/v1.9.0/bazelisk-linux-amd64\r\n# chmod +x /usr/local/bin/bazel   && mkdir -p /usr/local/src/tensorflow   && cd /usr/local/src   && wget -q --no-check-certificate -c https://github.com/tensorflow/tensorflow/archive/v2.5.0.tar.gz -O - | tar --strip-components=1 -xz -C /usr/local/src/tensorflow   && cd /usr/local/src/tensorflow\r\n# echo 3.7.2  > .bazelversion\r\n# bazel clean\r\n# chmod +x /tmp/tf_build.sh\r\n# time /tmp/tf_build.sh yes v2\r\n```\r\n\r\nTo the `compute capabilities` question: `6.0,6.1,7.0,7.5`\r\n\r\nAfter the  crash I am able to create a small `fixit.sh` script with the following content (copied from the error message)\r\n\r\n```\r\n  (cd /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \\\r\n    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \\\r\n    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5 \\\r\n    TF_CUDA_VERSION=10.2 \\\r\n    TF_CUDNN_VERSION=8 \\\r\n    TF_NCCL_VERSION=2 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/fake_quant_ops_gpu/fake_quant_ops_gpu.cu.pic.d \\\r\n'-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/fake_quant_ops_gpu/fake_quant_ops_gpu.cu.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' \\\r\n-iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl \\\r\n-iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif \\\r\n-iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf \\\r\n-iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 \\\r\n-iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d \\\r\n-iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib \\\r\n-iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_rocm \\\r\n-iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt \\\r\n-Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers \\\r\n-isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include \\\r\n-isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src \\\r\n-isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib \\\r\n-isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include \\\r\n-isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm \\\r\n-isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand \\\r\n-isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer \\\r\n-isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' \\\r\n-fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 \\\r\n-ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -std=c++14' \\\r\n-x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_60' '--cuda-gpu-arch=sm_60' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' '--cuda-include-ptx=$\r\n```\r\n\r\nI am able to test options to fix it:\r\n\r\n```\r\n# ./fixit.sh \r\nexternal/com_google_absl/absl/functional/function_ref.h:100:29: error: parameter packs not expanded with \u2018...\u2019:\r\n   template <typename F, typename = EnableIfCompatible<const F&>>\r\n                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                \r\nexternal/com_google_absl/absl/functional/function_ref.h:100:29: note:         \u2018Args\u2019\r\nexternal/com_google_absl/absl/functional/function_ref.h:114:13: error: parameter packs not expanded with \u2018...\u2019:\r\n       typename F, typename = EnableIfCompatible<F*>,\r\n             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                \r\nexternal/com_google_absl/absl/functional/function_ref.h:114:13: note:         \u2018Args\u2019\r\n```\r\n\r\nI welcome options to test, in the long run this method ought to be added to the `export CC_OPT_FLAGS=\"\"` line in the `tf_build.sh` script", "Out of curiosity, I checked with `docker run --rm -it datamachines/cudnn_tensorflow_opencv:10.2_2.4.1_3.4.14-20210420 /bin/bash` and its `gcc --version` appears to be the same `gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0` as in my current build", "And trying gcc-8 did not help either\r\n\r\n```\r\napt install gcc-8 g++-8\r\nupdate-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 700 --slave /usr/bin/g++ g++ /usr/bin/g++-7\r\nupdate-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 800 --slave /usr/bin/g++ g++ /usr/bin/g++-8\r\n```", "To answer your question, I managed a build with CUDA 11.2 without this error.\r\n\r\n```\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 4978.271s, Critical Path: 1390.04s\r\nINFO: 37252 processes: 12104 internal, 25148 local.\r\nINFO: Build completed successfully, 37252 total actions\r\nINFO: Build completed successfully, 37252 total actions\r\n-- TensorFlow building time (in seconds): 4979\r\n```\r\n\r\n\r\nWhat seems strange is that another user was encountering it using CUDA 10.2 (and TensorFlow 2.4.1 which I was able to build without issues) and it appears to have been in this case not enough memory available and that the user needed to add swap.\r\n\r\nIn my case this would be rather strange given that I have 128GB on my build system and do not even touch swap.\r\n\r\n\r\n", "Doing a search on the issues using \"parameter packs not expanded with\", I see a potential similar error in https://github.com/tensorflow/tensorflow/issues/48468 that I should investigate further.", "@mmartial \r\nAs cuda 11.2 works as expected, is there any particular reason to use with 10.2.", "Just to follow up: I have been able to build TF 2.5.0 with CUDA 10.2\r\n\r\n```\r\nTarget //tensorflow/tools/pip_package:build_pip_package up-to-date:\r\n  bazel-bin/tensorflow/tools/pip_package/build_pip_package\r\nINFO: Elapsed time: 4446.444s, Critical Path: 1000.31s\r\nINFO: 37032 processes: 11884 internal, 25148 local.\r\nINFO: Build completed successfully, 37032 total actions\r\nINFO: Build completed successfully, 37032 total actions\r\n-- TensorFlow building time (in seconds): 4446\r\n```\r\n\r\nFor people trying to reproduce this:\r\n- I followed both set of instructions listed in https://github.com/tensorflow/tensorflow/issues/48468 and more specifically https://github.com/tensorflow/tensorflow/commit/07665aa311d6a157f18d36489de67f4a258811a0\r\n- as well as https://github.com/tensorflow/tensorflow/pull/48393\r\n\r\nThose are the lines/files modified:\r\n```\r\n[**] Patching third_party/cub.BUILD\r\n--- third_party/cub.BUILD.bak\t2021-05-12 13:26:41.000000000 +0000\r\n+++ third_party/cub.BUILD\t2021-06-04 16:03:27.882398799 +0000\r\n@@ -11,5 +11,5 @@\r\n cc_library(\r\n     name = \"cub\",\r\n     hdrs = glob([\"cub/**\"]),\r\n-    deps = [\"@local_cuda//:cuda_headers\"],\r\n+    deps = [\"@local_config_cuda//cuda:cuda_headers\"],\r\n )\r\n[**] Patching third_party/absl/workspace.bzl\r\n--- third_party/absl/workspace.bzl.bak\t2021-05-12 13:26:41.000000000 +0000\r\n+++ third_party/absl/workspace.bzl\t2021-06-04 16:03:27.886398821 +0000\r\n@@ -14,7 +14,7 @@\r\n         sha256 = ABSL_SHA256,\r\n         build_file = \"//third_party/absl:com_google_absl.BUILD\",\r\n         # TODO(mihaimaruseac): Remove the patch when https://github.com/abseil/abseil-cpp/issues/326 is resolved\r\n-        patch_file = \"//third_party/absl:com_google_absl_fix_mac_and_nvcc_build.patch\",\r\n+        #patch_file = \"//third_party/absl:com_google_absl_fix_mac_and_nvcc_build.patch\",\r\n         strip_prefix = \"abseil-cpp-{commit}\".format(commit = ABSL_COMMIT),\r\n         urls = [\r\n             \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/abseil/abseil-cpp/archive/{commit}.tar.gz\".format(commit = ABSL_COMMIT),\r\n[**] Patching tensorflow/core/platform/default/cord.h\r\n--- tensorflow/core/platform/default/cord.h.bak\t2021-05-12 13:26:41.000000000 +0000\r\n+++ tensorflow/core/platform/default/cord.h\t2021-06-04 16:03:27.886398821 +0000\r\n@@ -16,7 +16,9 @@\r\n #ifndef TENSORFLOW_CORE_PLATFORM_DEFAULT_CORD_H_\r\n #define TENSORFLOW_CORE_PLATFORM_DEFAULT_CORD_H_\r\n \r\n+#if !defined(__CUDACC__)\r\n #include \"absl/strings/cord.h\"\r\n #define TF_CORD_SUPPORT 1\r\n+#endif\r\n \r\n #endif  // TENSORFLOW_CORE_PLATFORM_DEFAULT_CORD_H_\r\n```\r\n\r\nI will incorporate the logic into https://github.com/datamachines/cuda_tensorflow_opencv/blob/master/tools/tf_build.sh\r\n", "@Saduf2019 re: 10.2, mostly a continuation of previous builds but I will likely focus on CPU only and only CUDA 11 versions in the future", "Also, because of https://github.com/tensorflow/tensorflow/issues/49983#issuecomment-854888309 I am going to close this issue as resolved.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49983\">No</a>\n", "@mmartial \r\nThank you for your update, glad it is resolved."]}, {"number": 49982, "title": "test build and install issue", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49982\">No</a>\n"]}, {"number": 49981, "title": "Fix error message for using numpy-related methods", "body": "When attempting to use numpy-related methods, you encounter the following error:\r\n\r\n```python\r\nx = tf.constant([[1, 2, 3], [4, 5, 6]])\r\nx.T\r\n```\r\n\r\n```\r\nAttributeError: \r\n        'EagerTensor' object has no attribute 'T'.\r\n        If you are looking for numpy-related methods, please run the following:\r\n        import tensorflow.python.ops.numpy_ops.np_config\r\n        np_config.enable_numpy_behavior()\r\n```\r\n\r\nAttempting the fix in that error message results in:\r\n\r\n```\r\nNameError: name 'np_config' is not defined\r\n```\r\n\r\nThis updates the error message to be consistent with usage here: https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/python/ops/numpy_ops/integration_test/np_config_test.py#L22\r\n\r\nwhich works as expected", "comments": []}, {"number": 49980, "title": "MirroredStrategy throws warning during startup", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0 and 2.6.0-dev20210602\r\n- Python version: 3.7 and 3.8\r\n\r\n**Describe the current behavior**\r\nWhen instantiating a `MirroredStrategy` at the top of a program using:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.distribute.MirroredStrategy()\r\n```\r\nIt throws the following warning suggesting users that they might run into performance problems:\r\n```\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\n```\r\nThis warning is thrown in\r\nhttps://github.com/tensorflow/tensorflow/blob/280d27a99d5057f3ef421dd684fbb327cd1ff20a/tensorflow/python/distribute/mirrored_strategy.py#L375-L380\r\nand it will be thrown as well even if the code is running directly after importing TensorFlow.\r\n\r\n**Describe the expected behavior**\r\n\r\nNo warning should be thrown by default or the warning should include actionable items for the user to resolve this issue.\r\n\r\n**Standalone code to reproduce the issue**\r\nCheckout [this notebook for full reproduction](https://colab.research.google.com/drive/1h5b4Ve71n0e-VDngHRDMUL4CHwGqxvPE?usp=sharing).", "comments": ["@ymodak ,\r\nI was able to reproduce the issue in tf v2.5, nightly and in v2.4, code executed with warning.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/7c5d5f7f1abfc90d0756e2fa184e693d/49980.ipynb).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49980\">No</a>\n", "So what's the solution for this warning?\r\n\r\nAny updates?", "> So what's the solution for this warning?\r\n\r\nThe warning was removed in a27e3951d6a1f2cc4be53204e4d9e0da02e11830 which will be part of the 2.6 release.\r\n\r\nFor TF 2.5 it looks like it is save to ignore this warning for now (if I read @crccw's commit message correctly)", "Hi @lgeiger , thanks for the info!\r\n\r\nMay I ask you another question about tf distributed training?\r\n\r\nIn my practice, I found that it takes a long time to start training with MirroredStrategy.\r\n\r\nI am using keras.model.fit() to train my model, and the program stucked after printing `Epoch 1/xx` for a LONG time."]}, {"number": 49979, "title": "RuntimeError: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (3 != 4)Node number 21 (PAD) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version 2.4\r\n\r\nI have converted the mobilebert model to tflite and now I want to perform inference on that, for a single value, it works fine but for a batch, I am reshaping the input size of the tensor. I am running the following lines of code but it gives me the dimension error.\r\n\r\nCode I am running: \r\n```\r\ninput_data10 = np.expand_dims(input_text[1:1001], axis=1)\r\ninterpreter.resize_tensor_input(input_details[0]['index'], [1000, 1, 100])\r\ninterpreter.allocate_tensors()\r\ninterpreter.set_tensor(input_details[0]['index'], input_data10)\r\ninterpreter.allocate_tensors()\r\ninterpreter.invoke()\r\n```\r\n\r\nError I am receiving\r\n\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-65-7d35ed1dfe14> in <module>\r\n----> 1 interpreter.invoke()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)\r\n    538     \"\"\"\r\n    539     self._ensure_safe()\r\n--> 540     self._interpreter.Invoke()\r\n    541 \r\n    542   def reset_all_variables(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (3 != 4)Node number 21 (PAD) failed to prepare.\r\n\r\n``", "comments": ["Please make sure that the above TFLite model is capable of handling multiple batch inputs. Looks like the above model is designed and converted to handle only one input. However, due to a lack of information about the model details, it is really hard to give any other suggestions. If possible, please share a minimal and reproducible step for the above issue or the above TFLite model for debugging purpose.", "It is possible to see the input tensor specs by printing the input tensor details. E.g., `interpreter.get_input_details()`", "Hey @abattery, So when I go interpreter.get_input_details(), I get the following results.\r\n\r\n[{'name': 'input_ids',\r\n  'index': 0,\r\n  'shape': array([1000,    1,  100], dtype=int32),\r\n  'shape_signature': array([ -1, 100], dtype=int32),\r\n  'dtype': numpy.int32,\r\n  'quantization': (0.0, 0),\r\n  'quantization_parameters': {'scales': array([], dtype=float32),\r\n   'zero_points': array([], dtype=int32),\r\n   'quantized_dimension': 0},\r\n  'sparsity_parameters': {}}]\r\n\r\n\r\nHere the shape is [1000, 1,100] since I resized it to pass it 1000 inputs at once where each input is of the shape (1,100) but I dont know why on interpreter.invoke, it gives the following error:\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n<ipython-input-20-7d35ed1dfe14> in <module>\r\n----> 1 interpreter.invoke()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)\r\n    831     \"\"\"\r\n    832     self._ensure_safe()\r\n--> 833     self._interpreter.Invoke()\r\n    834 \r\n    835   def reset_all_variables(self):\r\n\r\nRuntimeError: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (3 != 4)Node number 21 (PAD) failed to prepare.\r\n\r\n\r\n\r\n\r\n", "Also to add one more thing, I used TFMobileBertModel for this text classification model and then I converted that model using tflite and then I am performing inferece on that, for a single input its working fine but when i resize the input tensor to pass 1000 inputs, it gives the error.\r\n", "Looks like the above model requires the following shape restrictions:\r\n\r\n```'shape_signature': array([ -1, 100], dtype=int32),```\r\n\r\nProbably, the right input shape is [1000, 100] for the batch case.", "So how should i resize the input tensor then?\r\nCurrently I am doing it as: \r\n\r\ninterpreter.resize_tensor_input(input_details[0]['index'], [1000, 1, 100])\r\n\r\nWhat would be the right way of doing it?\r\nAlso is this model to support a batch of input to be inferenced?\r\n", "According to the shape_signature field, the batched input size should be resized as follows:\r\n\r\n`interpreter.resize_tensor_input(input_details[0]['index'], [1000, 100])`\r\n\r\nThe batched input should be a [1000, 100]-shaped tensor.\r\n\r\n", "Okay I entered the line that you mentioned and I got the following error:\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-23-ce0005dcf216> in <module>\r\n----> 1 interpreter.set_tensor(input_details[0]['index'], input_data10)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in set_tensor(self, tensor_index, value)\r\n    570       ValueError: If the interpreter could not set the tensor.\r\n    571     \"\"\"\r\n--> 572     self._interpreter.SetTensor(tensor_index, value)\r\n    573 \r\n    574   def resize_tensor_input(self, input_index, tensor_size, strict=False):\r\n\r\nValueError: Cannot set tensor: Dimension mismatch. Got 3 but expected 2 for input 0.\r\n\r\nThe reason is because the input is (1,100) initially for 1 input i.e. in other words, 1 row and 100 columns but I needed to pass 1000 inputs at once.\r\n\r\nLet me be a little more detailed here:\r\n\r\nThis was the initial code and it gave me the shape as:\r\ninput_data = np.expand_dims(input_text[0], axis=0)\r\n(1,100)\r\n\r\nbut I wanted to pass 1000 values and changed the above code to \r\ninput_data = np.expand_dims(input_text[0:1000], axis=0)\r\nwhich gave me the following shape:\r\n\r\nfor axis =0: (1, 1000, 100) \r\nand gave me \r\nfor axis =1: (1000, 1, 100)\r\n\r\nNow I understood your point where you said to pass [1000,100] to resize the tensor because obviously intially it was [1,100] for 1 input but for 1000 inputs, I had to change it to [1000,100] but the problem arise from the input_data where I assign 1000 values and expand them using np.expands.\r\n\r\n\r\n\r\n", "This is an intended behavior. You need to provide the input with [1000, 100] not [1000, 1, 100]. As the error described, the set_tensor method invocation needs 2 dimension but the given tensor data has 3 dimension.", "Perfect! I got it working from that cell, some how, I tried to change the shape to [1000, 100] and now when I run interpreter.invoke(), It did show me result, but now I tried it and it killed the kernel. I closed all the kernels and ran this one again and on interpreter.invoke(), it kills the kernel again, Idk why it is happening now.", "Great to hear that the interpreter works for you. Please upload a new issue to keep each issue focused.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/49979\">No</a>\n"]}]