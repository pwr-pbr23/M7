[{"number": 44809, "title": "Move cudnn library preloading into Get*Algorithms", "body": "Allows only required libs to be loaded into device memory while still\r\npreloading libs to avoid impacting autotuning kernel timings.\r\n\r\nAttn: @sanjoy ", "comments": []}, {"number": 44808, "title": "Questions regarding tensorflow code (trying to learn and understand)", "body": "Hi everyone,\r\n\r\nSorry if this is not the right place, if it's not please remove.\r\n\r\nBut I have some questions regarding the source code of TensorFlow. I am trying to get a better understanding of the way TF is build up.\r\n\r\n- What is the difference between `@keras_export('keras.layers.LSTMCell', v1=[]) `and `@keras_export(v1=['keras.layers.LSTMCell'])`. In the second on it's pointing to version 1, so if you using v1 of keras you will use that class?\r\n\r\n- I am also curious about what the best way is to import things from tensorflow. I want to import stuff from the `tensorflow.python.keras.layers.recurrent` I need to import for example `ops, tensor_shape`, but these functions are imported in this recurrent.py file: `from tensorflow.python.framework import ops`, and `from tensorflow.python.framework import tensor_shape`. So what is the best approach, import them straight from the corresponding locations, so `tensorflow.python.framework`, or just import them from `tensorflow.python.keras.layers.recurrent`? \r\n\r\nImporting them from `tensorflow.python.keras.layers.recurrent` really simplifies the code, because you are importing it from one location, but I do not know what the standard approach is.\r\n\r\n\r\n\r\n\r\n", "comments": ["@ion-elgreco \r\nThis question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.", "> @ion-elgreco\r\n> This question is better asked on [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nAlright! Thank you!", "moving this to closed status with confirmation."]}, {"number": 44807, "title": "[curl_http_request] fix wrong option for starttransfer_time_status", "body": "This PR addresses the bug in `curl_http_request` implementation by modifying the option from `CURLINFO_PRETRANSFER_TIME` to `CURLINFO_STARTTRANSFER_TIME` while defining `const auto starttransfer_time_status`", "comments": ["cc: @saeta ", "I no longer work on TensorFlow; unassigning self so it can be reassigned to someone who is still working on TensorFlow. Thanks!", "@gbaned, can you request a review from another code owner, please? "]}, {"number": 44803, "title": "correctly handle nccl for nvidia_gpu_device", "body": "This is an ongoing effort of https://github.com/google/jax/pull/4843\r\n\r\nnccl is not available on windows, `if_cuda` is not reliable.", "comments": ["@hawkinsp I think you should be mentioned here.", "@cloudhan can you please check sanity build failures ?", "@rthadur done", "Why it is blocked?", "@cloudhan Looks like that did the trick!"]}, {"number": 44801, "title": "Tutorial code freezes indefinitely on TF 2.4 with tf.function", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Education 1909 64 bit\r\n\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not used\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.4.0rc1\r\n- TensorFlow version (use command below): 2.4.0-rc1(v2.4.0-rc0-30-gef82f4c66c)\r\n- Python version: 3.7.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 11.0.3/cuDNN 8.0.2\r\n- GPU model and memory: RTX 2080ti 11 GB\r\n\r\n\r\n**Describe the current behavior**\r\nWhen running the [cyclegan tutorial](https://www.tensorflow.org/tutorials/generative/cyclegan) on a local machine the program freezes during the training loop. It successfully executes two _train_step_ before freezing indefinitely during the third _train_step_. This does not happen with TF 2.3.1 and if @tf.function is removed from the function _train_step_ it also does not happen.\r\n\r\n\r\n**Describe the expected behavior**\r\nThe program should not freeze.\r\n\r\n**Standalone code to reproduce the issue**\r\nDownload the notebook from https://www.tensorflow.org/tutorials/generative/cyclegan and run it on jupyter notebook with TF 2.4 on Windows. \r\n\r\n**Other info / logs** \r\nI also tried using python versions 3.6 and 3.8 as well as cuDNN version 8.0.5 and tensorflow versions 2.4.0rc0 and 2.5.0-dev20201029 with the same results.\r\n\r\nNo errors are printed when the program halts, this is the complete log:\r\n`\r\n2020-11-12 13:23:14.333154: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-12 13:23:22.692189: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-12 13:23:22.695520: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2020-11-12 13:23:22.764684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:21:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-11-12 13:23:22.770470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:4a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-11-12 13:23:22.776596: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-12 13:23:23.156041: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-12 13:23:23.159089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-12 13:23:23.197670: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-12 13:23:23.224469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-12 13:23:23.423940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-12 13:23:23.606110: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-12 13:23:24.736751: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-12 13:23:24.739994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2020-11-12 13:23:24.742445: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-12 13:23:25.086293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:21:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-11-12 13:23:25.091969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties:\r\npciBusID: 0000:4a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.545GHz coreCount: 68 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-11-12 13:23:25.098150: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2020-11-12 13:23:25.101109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-12 13:23:25.104096: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2020-11-12 13:23:25.107660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2020-11-12 13:23:25.110613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2020-11-12 13:23:25.114070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\r\n2020-11-12 13:23:25.117107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2020-11-12 13:23:25.120321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-12 13:23:25.124087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\r\n2020-11-12 13:23:25.855306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-12 13:23:25.858740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1\r\n2020-11-12 13:23:25.860826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N\r\n2020-11-12 13:23:25.862739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N\r\n2020-11-12 13:23:25.865414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8581 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:21:00.0, compute capability: 7.5)\r\n2020-11-12 13:23:25.871513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8581 MB memory) -> physical GPU (device: 1, name: GeForce RTX 2080 Ti, pci bus id: 0000:4a:00.0, compute capability: 7.5)\r\n2020-11-12 13:23:25.877802: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-12 13:23:26.305762: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2020-11-12 13:23:26.553589: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead.\r\n2020-11-12 13:23:26.563266: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead.\r\n2020-11-12 13:23:26.940273: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead.\r\n2020-11-12 13:23:26.951226: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to dataset.cache().take(k).repeat(). You should use dataset.take(k).cache().repeat() instead.\r\n2020-11-12 13:23:30.401325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2020-11-12 13:23:33.790176: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n2020-11-12 13:23:33.839252: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\r\n2020-11-12 13:23:33.852673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2020-11-12 13:23:34.345317: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n`", "comments": ["I was trying with `tf-nightly` on Colab but it seems to freeze also without `@tf.function`. Can you try locally with `tf-nighlty`?", "I ran it localy with `tf-nightly(2.5.0-dev20201111)` and it froze with `@tf.function` but ran fine without. It is not possible to reproduce this issue in Colab at the moment because tensorflow 2.4 and tf-nightly does not have access to GPUs due to [this issue](https://github.com/googlecolab/colabtools/issues/1574). I don't think it froze in your case it just ran really slowly due to it running on the CPU. I also tried `tf-nightly` on Colab and it worked with or without `@tf.function`. \r\n", "Yes I know that colab issue but I supposed It was already solved.\nI will try again on Colab moving the debug prints.", "Ok now I am printing every `train_step` on Colab (CPU) with `@tf.function` and I've already printed 15 `train_step`. \r\nCan you run locally on CPU with `tf-nighlty` so we could check if it is a GPU only issue.", "I ran it locally on `tf-nightly` for 20 `train_step` on the CPU with `@tf.function` without any problems so it has to be related to a GPU issue. I have noticed when running it on the GPU that it sometimes completes only one `train_step` and other times completes up to 5 `train_step` before freezing. ", "@Saduf2019 Temp I don't have a local GPU for tf-nightly/2.4. Please route on your side.", "I'd like to also mention that I've been having issues with Tensorflow freezing the same way on Windows (I've also attempted using and building my own nightly (2.5.0-dev20201111) along with building my own 2.4 r1 on Python 3.8). I've also been trying to run cycle gans and gans, however I'm unsure if the architecture of the model is what's causing it. \r\n\r\nA few tidbits I've noticed:\r\n- A lower batch size helps the program run longer, but it'll still eventually freeze.\r\n- When TF freezes, the CUDA usage in task manager fluctuates, and while when it's running normally its solid.\r\n- Removing dividing by STD in normalization layers seems to help Tensorflow run longer without freezing (although it still does it eventually) .\r\n- Sometimes when it \"freezes\" and I close the Python console, the memory in the GPU won't be released and task manager still shows usage for the CUDA cores.\r\n\r\nI'm also curious about OP's hardware, as I think this might be an issue with Amphere as I'm using a 3080. I've been using CUDA 11.1 and cudnn 8.0.5.39", "@aovokaitys \r\nNice to see that I'm not alone. \r\nI'm using two 2080 Ti so I don't think this has anything to do with Ampere.\r\nI launched an AWS g4dn.xlarge instance with Ubuntu 18.04 to see if I could reproduce this issue on Linux.\r\nUsing CUDA 11.0 and cuDNN 8.0.5 combined with TF 2.4 and tf-nightly there was no problems so it appears that the issue is related to Windows.", "Now I've also tried Windows Server 2019 on AWS and there were no problems running TF 2.4 or tf-nightly.\r\nI have no idea what the problem might be now.", "@zetez I've tried setting the 'CUDA_LAUNCH_BLOCKING' environment variable to '1' and that seems to have resolved/helped the issue. The model has so far ran for ~5k batches with it set, and 2 batches without. I'll have to run the model overnight though to confirm that it's stable, as I've also had another GAN model running without instance normalization for a few hours before freezing. Try setting that environment variable on your end to see if it helps you. Here's the code needed to be appended to the top of your Python script, if you don't know how to do it:\r\n\r\n```\r\nimport os\r\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\r\n```\r\n\r\nHave a good day o/", "@aovokaitys Thank you. Enabling `CUDA_LAUNCH_BLOCKING` seems to have resolved the freezing issue.\r\nUnfortunately it also doubles the training time compared to TF 2.3.1(~47% GPU utilization vs 95%) which is not ideal.\r\nSince it works on Ubuntu 18.04 and Windows Server 2019 without any problems there has to be a way to make it work on our configurations as well. ", "Anyone have any idea what the problem might be or what steps to take next? I have really tried everything I can think of.\r\nI have successfully run it on remote computers with both Windows and Linux so it is not a problem with the operating system.\r\nI have successfully run it on remote computers with 2080 Ti so it is not a problem with 2080 Ti.\r\nI have multiple graphics cards on my local machine and it doesn't work on any of them so it is very unlikely to be a hardware issue.\r\nI have tried multiple graphics drivers with a clean uninstall([DDU](https://www.guru3d.com/files-details/display-driver-uninstaller-download.html)) and it doesn't work on any of them.\r\nI have tried [cyclegan](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) in pytorch 1.7(Also uses CUDA 11.0 and cuDNN 8.0.X) on my local machine and it works without any problems so the issue appears to be related to tensorflow.", "Fixed this issue by switching to pytorch.\r\nClosing the issue since I have no interest in finding the solution anymore.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44801\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44801\">No</a>\n", "> \r\n> \r\n> @zetez I've tried setting the 'CUDA_LAUNCH_BLOCKING' environment variable to '1' and that seems to have resolved/helped the issue. The model has so far ran for ~5k batches with it set, and 2 batches without. I'll have to run the model overnight though to confirm that it's stable, as I've also had another GAN model running without instance normalization for a few hours before freezing. Try setting that environment variable on your end to see if it helps you. Here's the code needed to be appended to the top of your Python script, if you don't know how to do it:\r\n> \r\n> ```\r\n> import os\r\n> os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\r\n> ```\r\n> \r\n> Have a good day o/\r\n\r\nthis appears to have fixed the problem for me using 2x 3090 RTX on Windows. thanks a lot!"]}, {"number": 44800, "title": "Undefined symbols for architecture arm64 _TFE_*", "body": "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catilina 10.15.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iOS 14\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: TF 2.0.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nHi,\r\n\r\nI am not used to build tensorflow from source for iOS so I'm probably doing something wrong. I used the instructions in tensorflow/contrib/makefile to build a static library for iOS architectures. But when I import it into Xcode I get link errors like:\r\n\r\n`Undefined symbols for architecture arm64: _TFE_TensorHandleResolve`\r\n\r\nWhat am I missing?\r\n\r\nMoreover, I saw that for newer versions of tensorflow there is no contrib folder: how can I build a static library with newer tensorflow, for example 2.3, for iOS?\r\n\r\n", "comments": ["Hi @SestoAle. As you've noticed, the `contrib` directory is deprecated as of TF 2, and no longer being actively maintained.\r\nThe latest instruction on how to build TensorFlow Lite iOS static framework can be accessed here:\r\nhttps://www.tensorflow.org/lite/guide/build_ios\r\n\r\nI'm closing the issue for now, but feel free to reopen if you have more specific questions or having trouble following the guide.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44800\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44800\">No</a>\n", "Hi @yyoon, thanks for the reply. I tried to build static library with tf lite but it doesn't work. When I import it to XCode it doesn't recognize any of the TF operations. I also tried to use `tensorflow/lite/tools/make/build_ios_universal_lib.sh` but with the same result.\r\n\r\nAnyway, I think I am fine also with TF 2.0 that still has the `contrib` directory and it recognizes most of the TF operations, expect the TFE ones. How can I include the TFE operations in the static library with TF 2.0?", "Sounds like you're trying to call regular TensorFlow ops from an iOS app side, correct? This is not a supported use case, as far as I can tell. Is there any specific reason to use regular TF ops on iOS side? What does your sample code look like that you wrote in Xcode?\r\n\r\nNormally, you'd train your model outside of the mobile environment, export the trained TF model into one of the supported format (e.g., TF 2.0 saved model), convert it to TFLite's flatbuffer format, and import it into the app to do inference on device.", "I'm trying to use Tensorflow.NET (TF binding library for C#) into a Unity project. I compile the Tensorflow.NET library as Unity docs say: https://docs.unity3d.com/2019.3/Documentation/Manual/NativePlugins.html \r\n\r\nbut when I compile the Unity project for iOS, Unity requires that ` all extern static methods be resolved a link time. If they are not found by the native linker, errors will occur.`\r\n\r\nI previously used TensorFlowSharp with TF 1.x (similar to Tensorflow.Net) in the same way I try to use TensorFow.Net (I compiled the C# library and Tensorflow 1.x with `contrib` folder) and it worked fine. However, TensorflowSharp is not maintained anymore and it does not support TF 2.X ", "Are there established ways to build a static library for TensorFlow.NET for platforms other than iOS? This doesn't seem to be related to TensorFlow Lite at all, and it probably requires a static library of regular TensorFlow, which I'm not too familiar with.", "Yes, I built static libraries for TensorFlow.NET for MacOS and Ubuntu and they work correctly. However I don't have any other platform to test it right now (I think should be useful trying it with Android).", "When building for those platforms, did you use the contrib directory as mentioned above?\r\n\r\nAnyways, this issue is definitely not related to TFLite, and I'd also suggest asking the TF.NET community to see if other people had successfully built for iOS. This [README from the TF.NET site](https://github.com/SciSharp/TensorFlow.NET/blob/master/tensorflowlib/README.md) might also help.\r\n\r\n@mihaimaruseac do you happen to know any ways to build TF static library for iOS, not TFLite?", "There is the `--config=monolithic` flag which might help.", "@SestoAle Can you please refer to the [comment ](https://github.com/tensorflow/tensorflow/issues/44800#issuecomment-730502054) and try using latest TF v2.6.0 ? Please let us know if it helps? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44800\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44800\">No</a>\n"]}, {"number": 44798, "title": "ambiguity of data type of weight", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04.4 LTS\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):v2.3.1\r\n- Bazel version (if compiling from source):3.1.0\r\n\r\n\r\n**Current and expected  behavior**\r\nI am supporting our delegete for tensorflow Lite. when I do the uint test, I found some cases(e.g.,SimpleTestHybridUint8) in\r\ntensorflow/lite/kernel/conv_test.cc. they describe their weight type as Uint8, but quantize and calculate as Int8. This kind of behavior doesn't make sense. we can't handle this behavior because it makes ambiguity to our delegate.\r\nSo I hope you guys to delete this kind of cases or unify their data type.\r\n\r\n\r\n", "comments": ["Hi!@multiverse-tf, I want to know the state of this issue. Do you fix it now or is it in progress\uff1f", "@liyuenan2333  \r\nCould you please try on latest stable version of TF 2.6.0  and let us know if this is still an issue?Could you  please have a look at the [TF delegate](https://www.tensorflow.org/lite/performance/delegates) and [Implementing Custom delegate](https://www.tensorflow.org/lite/performance/implementing_delegate) , do let us know if it helps ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44798\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44798\">No</a>\n"]}, {"number": 44797, "title": "set_visible_devices with MirroredStrategy causes NCCL warnings", "body": "**System information**\r\n- I have written custom code\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed `FROM tensorflow/tensorflow:2.3.0-gpu`\r\n- TensorFlow version 2.3.0 \r\n- Python version 3.6.9\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: nvidia dgx, 8xV100, volta architecture, 8x16GB\r\n\r\n**Describe the current behavior**\r\n\r\nIf `tf.config.set_visible_devices` is used presumably \"Virtual GPUs\" are created, which when combined with default `MirroredStrategy` leads to the following warning:\r\n\r\n> WARNING:tensorflow:NCCL is not supported when using virtual GPUs, falling back to reduction to one device\r\n\r\nI started setting the `CUDA_VISIBLE_DEVICES` env variable to go around the need for `set_visible_devices` but that is not ideal:\r\n- it defeats the purpose of the `set_visible_devices`  \r\n- it is not convenient\r\n- my tensorboard profiler plugin started to crash with segfault after the `keras.fit` training session is over but before profiling data buffer is flushed (perhaps this is fair game as it seems that CUDA 10.2 is needed for that to officially work for multi-GPU)\r\n\r\n**Describe the expected behavior**\r\n\r\nI would like to use `set_visible_devices` without causing any problems listed above.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI will provide this at a later stage if necessary, but I believe just using a combination of 2 APIs above should reproduce the issue.", "comments": ["@Antymon,\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet to reproduce the issue reported here. Thanks!", "Assuming a multi-GPU node run docker container:\r\n`docker run -it --rm tensorflow/tensorflow:2.3.0-gpu`\r\n\r\nCreate `limit_devices_tf.py`:\r\n```\r\nimport os\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, models\r\n\r\ndef set_visible_devices(device_idx: str, device_type: str, use_tf: bool):\r\n    \"\"\"\r\n    Limit visibility of devices available in the cluster for the current application.\r\n\r\n    @param device_idx: comma-delimited indexes or \"ALL\"\r\n    @param device_type: [XLA_](CPU|GPU), e.g., \"GPU\"\r\n    @param use_tf: set visibility of devices through tensorflow APIs or no\r\n\r\n    @return: list of selected devices\r\n    \"\"\"\r\n\r\n    if use_tf:\r\n      filtered_type_devices = tf.config.list_physical_devices(device_type=device_type)\r\n      visible_devices = select_devices_by_index(device_idx, filtered_type_devices)\r\n\r\n      # always add CPU\r\n      has_cpu = device_type[-3:] == \"CPU\"\r\n      if not has_cpu:\r\n          visible_devices += tf.config.list_physical_devices(\r\n              device_type=\"{}CPU\".format(device_type[:-3]).upper()\r\n          )\r\n\r\n      tf.config.set_visible_devices(visible_devices)\r\n    elif device_type == \"GPU\":\r\n      os.environ[\"CUDA_VISIBLE_DEVICES\"]=device_idx\r\n    else:\r\n      raise NotImplementedError()\r\n\r\ndef select_devices_by_index(device_idx: str, devices: list):\r\n    \"\"\"\r\n    @param device_idx: comma-delimited indexes or \"ALL\"\r\n    @param devices: list of PhysicalDevices properties\r\n    @return: subset of a list corresponding to indexes\r\n    \"\"\"\r\n    assert len(devices) > 0\r\n    filtered_devices = []\r\n    if device_idx.upper() == \"ALL\":\r\n        filtered_devices = devices\r\n    else:\r\n        device_idx = device_idx.split(\",\")\r\n        assert len(devices) >= len(device_idx)\r\n        for i in device_idx:\r\n            for d in devices:\r\n                if i == d.name.split(\":\")[-1]:\r\n                    filtered_devices.append(d)\r\n\r\n    print(\"{} devices selected:\".format(len(filtered_devices)))\r\n    for d in filtered_devices:\r\n        print(d.name)\r\n\r\n    return filtered_devices\r\n\r\ndef build_model():\r\n    # build dummy classification model\r\n    model = models.Sequential()\r\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\r\n    model.add(layers.MaxPooling2D((2, 2)))\r\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n    model.add(layers.MaxPooling2D((2, 2)))\r\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n    model.add(layers.Flatten())\r\n    model.add(layers.Dense(64, activation='relu'))\r\n    model.add(layers.Dense(10))\r\n    model.compile(optimizer='adam',\r\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n    return model\r\n\r\nif __name__ == \"__main__\":\r\n  use_tf = sys.argv[1].upper() == \"TRUE\"\r\n  set_visible_devices(device_idx=\"0,1\", device_type=\"GPU\", use_tf = use_tf)\r\n  strategy = tf.distribute.MirroredStrategy()\r\n  with strategy.scope():\r\n      model = build_model()\r\n```\r\nRun following in bash;\r\n```\r\npython limit_devices_tf.py true &> tf_true.txt\r\npython limit_devices_tf.py false &> tf_false.txt\r\n```\r\nOnly the former out of 2 output files is expected to contain:\r\n> WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\r\n", "Hi @Antymon, I was able to reproduce this using 4 GPUs on GCP. This does seem odd that `set_visible_devices` should result in  virtual GPUs. As a workaround while this issue is being investigated further, have you tried passing in the devices argument to the strategy? eg `tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])`", "Hi, it's speculation. I didn't look inside the source to see what exactly is going on. \r\n\r\nAs for the device parameter you pointed at, I had some other problems with it. I didn't know where I should take those names from. I ended up mapping filtered logical devices to their names (since names of physical ones caused problems), but there was something not right with it as well (can't recall details, sorry). I gave up and used `CUDA_VISIBLE_DEVICES` set as in example above. It does the main work, so I moved on.\r\n\r\nEDIT: this is what I get if I attempt to limit visibility via strategy arguments. You can clearly see by mem which ones I assigned. What does a half gig do on 0 that I do not know. And all this with adaptive memory allowed, otherwise, all memory would be mapped. But this is materials for yet another issue, separate from the original one.\r\n![image](https://user-images.githubusercontent.com/641005/100727005-f7914a00-33c5-11eb-99e7-e0e90158841d.png)\r\n", "@Antymon  It looks like you are using an older Version of Tensorflow . Many bugs have been fixed in the latest version. Could you please execute your code using Latest Version 2.6 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44797\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44797\">No</a>\n"]}, {"number": 44796, "title": "Issue in building tensorflow r2.3 from source", "body": "Hi, I am having issue compiling tensorflow r2.3 for CPU from the source on ubuntu I am using following flags \r\n\r\n```--config=opt --copt=-march=native --copt=-Wno-sign-compare --copt=-mfpmath=both --copt=-mavx --copt=-mavx2 --copt=-mfma\r\n --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n```\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No - I git clone the code\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 20.04 docker image\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: No\r\n-   **TensorFlow installed from (source or binary)**:  Source\r\n-   **TensorFlow version (use command below)**: 2.3\r\n-   **Python version**: 3.8.5\r\n-   **Bazel version (if compiling from source)**: 3.1.0\r\n-   **GCC/Compiler version (if compiling from source)**: 9.3.0\r\n-   **CUDA/cuDNN version**: Not applicable\r\n-   **GPU model and memory**: Not applicable\r\n-   **Exact command to reproduce**:\r\n\r\n`bazel build --config=opt --copt=-march=native --copt=-Wno-sign-compare --copt=-mfpmath=both --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package --verbose_failures`\r\n\r\nAttaching tf_env.txt for the reference.\r\n\r\n\r\n### Describe the problem\r\nWhile building the tensorflow from the source, build fails with error\r\nERROR: /tensorflow_src/tensorflow/python/BUILD:501:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/.local/bin \\\r\n    PWD=/proc/self/cwd \\\r\n\r\n<long string>\r\n\r\nERROR: /tensorflow_src/tensorflow/python/tools/BUILD:99:1 C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1): gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/43801f1e35f242fb634ebbc6079cf6c5/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/.local/bin \\\r\n    PWD=/proc/self/cwd \\\r\n\r\n<long string>\r\nINFO: Elapsed time: 2497.566s, Critical Path: 187.54s\r\nINFO: 10577 processes: 10577 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nAttaching error.txt for detailed error log\r\n\r\n### Source code / logs\r\nattaching error.txt for detailed error log\r\n[error.txt](https://github.com/tensorflow/tensorflow/files/5529073/error.txt)\r\n[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5529075/tf_env.txt)\r\n\r\n", "comments": ["@kunalvshah,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/40688#issuecomment-647846011) from a similar issue and let us know if it helps. Thanks!", "Yes, it was success. Dropping numpy to 1.19.0 fixes the issue. Now I am able to compile it successfully.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44796\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44796\">No</a>\n"]}, {"number": 44795, "title": "Python returns multiple errors on using gTTS module", "body": "`from gtts import gTTS`\r\n`test=gTTS(text=\"Hello World\", lang='en')`\r\n`test.save('sample.mp3')`\r\n\r\n### **Error:**\r\nC:\\Program Files\\Python37\\lib\\http\\cookiejar.py:1623: UserWarning: http.cookiejar bug!\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1621, in make_cookies\r\n    parse_ns_headers(ns_hdrs), request)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1577, in _cookies_from_attrs_set\r\n    cookie = self._cookie_from_cookie_tuple(tup, request)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1570, in _cookie_from_cookie_tuple\r\n    rest)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 793, in __init__\r\n    self._rest = copy.copy(rest)\r\nAttributeError: module 'copy' has no attribute 'copy'\r\n\r\n  _warn_unhandled_exception()\r\nC:\\Program Files\\Python37\\lib\\http\\cookiejar.py:1623: UserWarning: http.cookiejar bug!\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1621, in make_cookies\r\n    parse_ns_headers(ns_hdrs), request)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1577, in _cookies_from_attrs_set\r\n    cookie = self._cookie_from_cookie_tuple(tup, request)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1570, in _cookie_from_cookie_tuple\r\n    rest)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 793, in __init__\r\n    self._rest = copy.copy(rest)\r\nAttributeError: module 'copy' has no attribute 'copy'\r\n\r\n  _warn_unhandled_exception()\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\Benison\\OneDrive\\Programs\\Python\\XII\\test.py\", line 5, in <module>\r\n    tts.save(\"synthesized.mp3\")\r\n    prepared_requests = self._prepare_requests()                                        e 295, in save\r\n  File \"C:\\Users\\Benison\\AppData\\Roaming\\Python\\Python37\\site-packages\\gtts\\tts.py\", line 194, in _prepare_requests                                                             e 251, in write_to_fp\r\n    part_tk = self.token.calculate_token(part)\r\n  File \"C:\\Users\\Benison\\AppData\\Roaming\\Python\\Python37\\site-packages\\gtts_token\\gtts_te 194, in _prepare_requestsoken.py\", line 28, in calculate_token\r\n    seed = self._get_token_key()                                                        oken.py\", line 28, in calculate_token  \r\n  File \"C:\\Users\\Benison\\AppData\\Roaming\\Python\\Python37\\site-packages\\gtts_token\\gtts_token.py\", line 59, in _get_token_key                                                    oken.py\", line 59, in _get_token_key   \r\n    \"Unable to find token seed! Did https://translate.google.com change?\"\r\nValueError: Unable to find token seed! Did https://translate.google.com change?\r\nPS C:\\Users\\Benison\\OneDrive\\Programs\\Python>  cd 'c:\\Users\\Benison\\OneDrive\\Programs\\Python'; & 'C:\\Program Files\\Python37\\python.exe' 'c:\\Users\\Benison\\.vscode\\extensions\\ms-python.python-2020.11.358366026\\pythonFiles\\lib\\python\\debugpy\\launcher' '59434' '--' 'c:\\Users\\Benison\\OneDrive\\Programs\\Python\\XII\\test.py' \r\nC:\\Program Files\\Python37\\lib\\http\\cookiejar.py:1623: UserWarning: http.cookiejar bug!\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1621, in make_cookies    \r\n    parse_ns_headers(ns_hdrs), request)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1577, in _cookies_from_attrs_set\r\n    cookie = self._cookie_from_cookie_tuple(tup, request)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1570, in _cookie_from_cookie_tuple\r\n    rest)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 793, in __init__\r\n    self._rest = copy.copy(rest)\r\nAttributeError: module 'copy' has no attribute 'copy'\r\n\r\n  _warn_unhandled_exception()\r\nC:\\Program Files\\Python37\\lib\\http\\cookiejar.py:1623: UserWarning: http.cookiejar bug!\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1621, in make_cookies    \r\n    parse_ns_headers(ns_hdrs), request)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1577, in _cookies_from_attrs_set\r\n    cookie = self._cookie_from_cookie_tuple(tup, request)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 1570, in _cookie_from_cookie_tuple\r\n    rest)\r\n  File \"C:\\Program Files\\Python37\\lib\\http\\cookiejar.py\", line 793, in __init__\r\n    self._rest = copy.copy(rest)\r\nAttributeError: module 'copy' has no attribute 'copy'\r\n\r\n  _warn_unhandled_exception()", "comments": ["@GOdOfTHuNdeR007 \r\nThis does not seem like a tensorflow related issue, please create the issue in the correct repo and move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44795\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44795\">No</a>\n"]}, {"number": 44794, "title": "[CostModel] Generating CostGraphDef with Global Index", "body": "This is a PR from JIZHI, the AI platform in Tencent.\r\n\r\nWhen generating CostGraphDef from multi CostModels without a global index, the nodes from different CostModels have the same index. It is a conflict. This PR resolves this conflict. ", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44794) for more info**.\n\n<!-- need_author_cla -->", "@allenlavoie, I did not find any ut about cost-model in current files. Could I add a new file costmodel_test.cc?", "Sure, if none of the existing test suites in that directory make sense feel free to add a new one.", "@xinan-jiang  Any update on this PR? Please. Thanks!", "I need more time to finish the UT.", "@xinan-jiang Any update on this PR? Please. Thanks!", "@xinan-jiang can you please check sanity build failures ?", "> @xinan-jiang can you please check sanity build failures ?\r\n\r\nI think it is not broken by this change.", "Looks like the status return from AddToCostGraphDef is ignored (costmodel_test.cc:89+90). Please TF_ASSERT_OK or equivalent.", "> Looks like the status return from AddToCostGraphDef is ignored (costmodel_test.cc:89+90). Please TF_ASSERT_OK or equivalent.\r\n\r\nOK"]}, {"number": 44793, "title": "Fix a typo", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44793) for more info**.\n\n<!-- need_author_cla -->"]}, {"number": 44792, "title": "How to fastly convert numpy array into TensorFlow tensor in TF 2.x", "body": "**Describe the current behavior**\r\n\r\nI want to train TF DNN model, and the input data is from numpy array.\r\n\r\nWhen inputting data from numpy to TensorFlow, converting to tensor will be triggered no matter which ways I used.\r\nSpecifically, I tried these 4 methods:\r\n1) tf.constant(numpy_value)\r\n2) tf.convert_to_tensor(numpy_value)\r\n3) create a tf.Variable, then Variable.assign\r\n4) tf.keras.backend.set_value(variable, numpy_value)\r\n\r\nwhen profiling, there will be `TF_NewTensor` triggered, which might convert numpy value into tensor format. And it is too slow.\r\n![image](https://user-images.githubusercontent.com/69858819/98893834-602f8a00-24de-11eb-807d-ae5b07aa4ca2.png)\r\n\r\n\r\n**Describe the expected behavior**\r\n1) how to fastly converting numpy array to TF tensor?\r\n2) Or how can I convert numpy array to TF tensor in advance, then I just need give that tensor to TF ops rather than waiting for converting is done then begin forward propagation.\r\n\r\nBTW, because batchsize is too large, so `tf.data.TFRecordDataset` is not fast enough to do the profiling, therefore I need to directly input binary value to TF ops.\r\n", "comments": ["@Jianbing-D \r\n\r\nPlease, refer this [tutorial](https://www.tensorflow.org/guide/tf_numpy)  and see if it helps you.Thanks!", "> @Jianbing-D\r\n> \r\n> Please, refer this [tutorial](https://www.tensorflow.org/guide/tf_numpy) and see if it helps you.Thanks!\r\n\r\nThanks for your reply. I will try it. \r\nBTW, I used tf.data.Dataset.from_tensor_slice to prefetch datas, so that numpy will be converted into tensor format at advance.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44791, "title": "Dynamic Batching for Text Vectorization", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow-2.3.0, tf-nightly\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently tensorflow.keras.layers.experimental.preprocessing.Text Vectorization automatically pad each sentence vector to max_tokens. I was wondering if it is possible to add an dynamic batching option so that the layer only pad the each batch to longest vector in each batch. This could improve the performance of TensorFlow RNNs. Currently, the only way to do this is to clip batches after this layer, wasting computational resources.\r\n\r\n**Will this change the current api? How?**\r\nYes, an additional parameter should be passed. However, existing users would not be affected. \r\n**Who will benefit with this feature?**\r\nAnyone who use dynamic RNN. \r\n**Any Other info.**\r\n", "comments": ["Sorry,  just realized dynamic batching is already possible with current implementation. "]}, {"number": 44790, "title": "Exception: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Both Linux 20.04 and Windows 10\r\n- TensorFlow installed from (source or binary): Binary (pip install tf-nightly)\r\n- TensorFlow version (or github SHA if from source): 2.5.0-dev20201111\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nCommand\r\n```\r\n(tf25) ubuntu@ubuntu-TFG257XS:~/yolo_tf/tensorflow-yolov4-tflite$ python convert_tflite.py --weights ./checkpoints/yolov4-416-tflite --output ./checkpoints/yolov4-416.tflite\r\n```\r\nCode\r\n```\r\nimport tensorflow as tf\r\nfrom absl import app, flags, logging\r\nfrom absl.flags import FLAGS\r\nimport numpy as np\r\nimport cv2\r\nfrom core.yolov4 import YOLOv4, YOLOv3, YOLOv3_tiny, decode\r\nimport core.utils as utils\r\nimport os\r\nfrom core.config import cfg\r\n\r\nflags.DEFINE_string('weights', './checkpoints/yolov4-416', 'path to weights file')\r\nflags.DEFINE_string('output', './checkpoints/yolov4-416-fp32.tflite', 'path to output')\r\nflags.DEFINE_integer('input_size', 416, 'path to output')\r\nflags.DEFINE_string('quantize_mode', 'float32', 'quantize mode (int8, float16, float32)')\r\nflags.DEFINE_string('dataset', \"/Volumes/Elements/data/coco_dataset/coco/5k.txt\", 'path to dataset')\r\n\r\ndef representative_data_gen():\r\n  fimage = open(FLAGS.dataset).read().split()\r\n  for input_value in range(10):\r\n    if os.path.exists(fimage[input_value]):\r\n      original_image=cv2.imread(fimage[input_value])\r\n      original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\r\n      image_data = utils.image_preprocess(np.copy(original_image), [FLAGS.input_size, FLAGS.input_size])\r\n      img_in = image_data[np.newaxis, ...].astype(np.float32)\r\n      print(\"calibration image {}\".format(fimage[input_value]))\r\n      yield [img_in]\r\n    else:\r\n      continue\r\n\r\ndef save_tflite():\r\n  converter = tf.lite.TFLiteConverter.from_saved_model(FLAGS.weights)\r\n\r\n  if FLAGS.quantize_mode == 'float16':\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_types = [tf.compat.v1.lite.constants.FLOAT16]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.allow_custom_ops = True\r\n  elif FLAGS.quantize_mode == 'int8':\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n    converter.allow_custom_ops = True\r\n    converter.representative_dataset = representative_data_gen\r\n\r\n  tflite_model = converter.convert()\r\n  open(FLAGS.output, 'wb').write(tflite_model)\r\n\r\n  logging.info(\"model saved to: {}\".format(FLAGS.output))\r\n\r\ndef demo():\r\n  interpreter = tf.lite.Interpreter(model_path=FLAGS.output)\r\n  interpreter.allocate_tensors()\r\n  logging.info('tflite model loaded')\r\n\r\n  input_details = interpreter.get_input_details()\r\n  print(input_details)\r\n  output_details = interpreter.get_output_details()\r\n  print(output_details)\r\n\r\n  input_shape = input_details[0]['shape']\r\n\r\n  input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\n\r\n  interpreter.set_tensor(input_details[0]['index'], input_data)\r\n  interpreter.invoke()\r\n  output_data = [interpreter.get_tensor(output_details[i]['index']) for i in range(len(output_details))]\r\n\r\n  print(output_data)\r\n\r\ndef main(_argv):\r\n  save_tflite()\r\n  demo()\r\n\r\nif __name__ == '__main__':\r\n    try:\r\n        app.run(main)\r\n    except SystemExit:\r\n        pass\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n(tf25) ubuntu@ubuntu-TFG257XS:~/yolo_tf/tensorflow-yolov4-tflite$ python convert_tflite.py --weights ./checkpoints/yolov4-416-tflite --output ./checkpoints/yolov4-416.tflite\r\n2020-11-12 10:51:14.591438: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-12 10:51:14.591714: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-12 10:51:21.071335: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored output_format.\r\n2020-11-12 10:51:21.071397: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:322] Ignored drop_control_dependency.\r\n2020-11-12 10:51:21.071417: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:328] Ignored change_concat_input_ranges.\r\n2020-11-12 10:51:21.072230: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: ./checkpoints/yolov4-416-tflite\r\n2020-11-12 10:51:21.126090: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\r\n2020-11-12 10:51:21.126129: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: ./checkpoints/yolov4-416-tflite\r\n2020-11-12 10:51:21.270211: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:251] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2020-11-12 10:51:21.307128: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n2020-11-12 10:51:21.354451: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299965000 Hz\r\n2020-11-12 10:51:21.867525: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: ./checkpoints/yolov4-416-tflite\r\n2020-11-12 10:51:22.030647: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 958418 microseconds.\r\n2020-11-12 10:51:22.617404: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nloc(\"batch_normalization/moving_mean\"): error: is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/tf25/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 213, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"/home/ubuntu/anaconda3/envs/tf25/lib/python3.7/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_tflite.py\", line 76, in <module>\r\n    app.run(main)\r\n  File \"/home/ubuntu/anaconda3/envs/tf25/lib/python3.7/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/home/ubuntu/anaconda3/envs/tf25/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"convert_tflite.py\", line 71, in main\r\n    save_tflite()\r\n  File \"convert_tflite.py\", line 45, in save_tflite\r\n    tflite_model = converter.convert()\r\n  File \"/home/ubuntu/anaconda3/envs/tf25/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 745, in convert\r\n    result = _convert_saved_model(**converter_kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/tf25/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 637, in convert_saved_model\r\n    enable_mlir_converter=True)\r\n  File \"/home/ubuntu/anaconda3/envs/tf25/lib/python3.7/site-packages/tensorflow/lite/python/convert.py\", line 216, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nSaved Model : yolov4-416-tflite.zip\r\n```\r\nhttps://drive.google.com/file/d/1JXAUuRRfkpp5h-6NA7SXduKMAeMQBssG/view?usp=sharing\r\n```\r\n\r\n**Failure details**\r\nWhen I tried to get .tflite file from saved model(tflite), failed to generate .tflite file with Error message\r\nThis problem happens regardless of quantization (FP32, FP16, INT8)\r\n\r\n\r\n**Any other info / logs**\r\nWhen I tried to get .tflite with Tensorflow 2.3.0rc0, it worked well (could generate .tflite file and execute it with tflite interpreter)\r\nBut I failed to generate .tflite file with tf-nightly 2.5.0-dev2020111\r\nHow can I solve this problem?", "comments": ["@kukbyung \r\nI ran the code shared and face [this error](https://colab.research.google.com/gist/Saduf2019/9ad91e681d3506c343830b00fd74ebdb/untitled460.ipynb), could you please share complete code such that we could replicate the issue faced or if possible share a colab gist with the error reported.", "@Saduf2019\r\n \r\nI uploaded the complete code including saved model for tflite and  .weight files on Google Drive (File name : YOLO_tf2.5.zip)\r\n[https://drive.google.com/file/d/1u7SqINdHkgThkd4xySTbjFToCqV9Z6OT/view?usp=sharing]\r\n\r\nthis whole code also can be obtained from [https://github.com/theAIGuysCode/tensorflow-yolov4-tflite]\r\nBut it needs to generate saved Model for tflite from .weight file \r\n(yolov4.weight file need to be downloaded from https://drive.google.com/open?id=1cewMfusmPjYWbrnuJRuKhPMwRe_b9PaT) \r\n`python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416-tflite --input_size 416 --model yolov4 --framework tflite`\r\n\r\nAnd then convert it to .tflite file (<- Problem)\r\n`python convert_tflite.py --weights ./checkpoints/yolov4-416 --output ./checkpoints/yolov4-416.tflite`\r\n\r\nThanks. ", "yeah im facing same error \r\n\r\nloc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable", "Experiencing the same issue, while trying to convert `SavedModel` into `tf.lite` model.\r\n\r\nThe original model was trained with `tensorflow 2.0.0` on Windows. \r\n\r\nI am trying to convert with `tensorflow 2.4.0` @ Colab.\r\n\r\n`ConverterError: <unknown>:0: error: loc(\"lstm_36/bias\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable`", "Possible solution is here:\r\n\r\nhttps://github.com/hunglc007/tensorflow-yolov4-tflite/issues/285#issuecomment-759416899", "I resolved the problem by following the above mentioned thread. \r\nSolution: In google colab I had this issue if I used the default TF version, which was 2.4.0. \r\nRunning ` !pip install tensorflow==2.3.0` and restarting the runtime, then converting corrected the issue.", "@snehitvaddi \r\n\r\nI did the same and it also worked for me. The current version of TF on Colab also worked correctly (2.4.1 if I remeber correctly)", "Thanks for the report everyone.\r\nWorking on a fix. \r\n\r\nThanks", "I got the same issue with a similar model with the latest TF 2.4.1\r\n```python\r\n    from tensorflow.keras import layers, models\r\n    gru = models.Sequential()\r\n    gru.add(layers.GRU(units=n_hidden, activation='tanh', recurrent_activation='hard_sigmoid',\r\n                       return_sequences=False, implementation=1,\r\n                       input_shape=(n_input, 2)))\r\n    gru.add(layers.Dense(units=np.unique(train_labels).size, activation='softmax'))\r\n    tf.saved_model.save(gru, str(model_path))\r\n```\r\nThen I followed the [official guideline](https://www.tensorflow.org/lite/convert/#convert_a_savedmodel_recommended_) on how to convert a SavedModel.", "Got the error :  \r\n\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"batch_normalization/moving_mean\"): is not immutable, try running tf-saved-model-optimize-global-tensors to prove tensors are immutable\r\n\r\n", "Hi,  @karimnosseir\r\nJust test the new tensorflow version, 2.5, also got that error.\r\n```\r\nloc(\"Conv_1_bn/moving_mean\"): error: is not immutable, try removing mutable variables in your model since mutable variables are currently not supported through this converter\r\nTraceback (most recent call last):\r\n  File \"/home/hood/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 294, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"/home/hood/.local/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(\"Conv_1_bn/moving_mean\"): is not immutable, try removing mutable variables in your model since mutable variables are currently not supported through this converter\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"python/tools/tflite_convert.py\", line 50, in <module>\r\n    convert(parser())\r\n  File \"python/tools/tflite_convert.py\", line 39, in convert\r\n    tflite_model = converter.convert()\r\n  File \"/home/hood/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 913, in convert\r\n    result = _convert_saved_model(**converter_kwargs)\r\n  File \"/home/hood/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 727, in convert_saved_model\r\n    enable_mlir_converter=True)\r\n  File \"/home/hood/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 297, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(\"Conv_1_bn/moving_mean\"): is not immutable, try removing mutable variables in your model since mutable variables are currently not supported through this converter\r\n\r\n```", "Sorry for the delay everyone. The fix is not ready yet - was working on other feature and out of office for some time.\r\nI am expecting to look in this in few weeks.\r\n\r\nThanks and apologies for the delay.", "Hi Everyone,\r\n\r\nMutable variable support is now available in the nightly when converting using from_saved_model.\r\nYou need to set this flag to True for now.\r\n\r\n```\r\nconverter.experimental_enable_resource_variables = True\r\n```\r\n\r\nSetting this should resolve the issue. Please give it a try and let us know if you're having any issues.\r\n\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44790\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44790\">No</a>\n"]}, {"number": 44788, "title": "bad shape of index returned by UniqueWithCountsV2 in graph mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe index returned by `tf.raw_ops.UniqueWithCountsV2` does not have the right shape when used in graph mode.\r\n\r\n**Describe the expected behavior**\r\nThe index returned by `tf.raw_ops.UniqueWithCountsV2` should have the right shape when used iin graph mode.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n\r\nHere's some simple code to illustrate the problem:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n# The same function: decorated with tf.function (will be executed in graph mode) and \r\n# not decorated (will be executed in eager mode)\r\n\r\n@tf.function\r\ndef graph_func(x):\r\n    unique_input_ids, idx, counts = tf.raw_ops.UniqueWithCountsV2(x=x, axis=[0])\r\n    tf.print('idx shape', tf.shape(idx), 'idx', idx)\r\n    return x\r\n\r\ndef eager_func(x):\r\n    unique_input_ids, idx, counts = tf.raw_ops.UniqueWithCountsV2(x=x, axis=[0])\r\n    tf.print('idx shape', tf.shape(idx), 'idx', idx)\r\n    return x\r\n\r\nc = tf.constant([[0,0,1], \r\n                 [0,0,1], \r\n                 [0,0,2], \r\n                 [0,0,1]])\r\n_ = graph_func(c)\r\n_ = eager_func(c)\r\n```\r\nPrints this:\r\n```\r\nidx shape [4 3] idx [0 0 1 0]\r\nidx shape [4] idx [0 0 1 0]\r\n```\r\nThe first output doesn\u2019t make sense. The shape doesn\u2019t even match the printed tensor.\r\n\r\nInvestigating further with a dataset:\r\n\r\n```python\r\nds = tf.data.Dataset.from_tensor_slices(c).repeat(40).batch(4)\r\n_ = graph_func(next(iter(ds)))\r\n_ = eager_func(next(iter(ds)))\r\n```\r\nPrints this:\r\n```\r\nidx shape [4 3] idx [0 0 1 0]\r\nidx shape [4] idx [0 0 1 0]\r\n```\r\n`graph_func` is executed in graph mode. The behavior is still unexpect (shape `[4 3]` instead of `[4]`).\r\n\r\nBut if we the data comes from the dataset through a `map` function for example (both functions are executed in graph mode), it works as expected:\r\n```\r\n_ = next(iter(ds.map(graph_func)))\r\n_ = next(iter(ds.map(eager_func)))\r\n```\r\n\r\nThis time, prints the expected result:\r\n```\r\nidx shape [4] idx [0 0 1 0]\r\nidx shape [4] idx [0 0 1 0]\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["The following is a possible workaround:\r\n\r\n```python\r\n@tf.function\r\ndef graph_func(x):\r\n    unique_input_ids, idx, counts = tf.raw_ops.UniqueWithCountsV2(x=x, axis=[0])\r\n    idx = tf.reshape(idx, [-1])   # <<<<<<<<<<<<<<<<<<<<<\r\n    idx = idx[:tf.shape(x)[0]]    # <<<<<<<<<<<<<<<<<<<<<\r\n    tf.print('idx shape', tf.shape(idx), 'idx', idx)\r\n    return x\r\n\r\n_ = graph_func(c)\r\n```\r\n```\r\nidx shape [4] idx [0 0 1 0]\r\n```", "Was able to reproduce the issue with TF v2.2, 2.3 and nightly versions. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/8868ab137c4c82e5ca07c08ee4802198/44788.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44788\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44788\">No</a>\n"]}, {"number": 44787, "title": "Newer version of tf nightly has changed the index order for the tensor output array", "body": "when reading in the TFlite model for EfficientDet D0 there are 8 values to the 1st index and their meaning is defined. Older version had the below order and for the newer tfnighly version,the order has changed and couldn't find documentation for the newer order, and using the tflite converted with newer version of tf nightly breaks the android code\r\n\r\nPlease refer the image below\r\n![image](https://user-images.githubusercontent.com/47469211/98880127-e8a91e80-243b-11eb-8a34-86aa403aaf53.png)\r\n\r\nColab that used older version of tfnightly (tf-nightly==2.4.0-dev20200929) for tflite conversion: \r\nhttps://colab.research.google.com/drive/1Ss4ZaO52gOBvhr-5DRXiiuPQ7I_vIt-Y?usp=sharing\r\nhttps://github.com/grewe/covidID_mask/blob/master/maskDetect/MaskDetectTflite.ipynb\r\n\r\n\tINPUT DETAILS: \r\n\t\t [{'name': 'serving_default_input_tensor:0', 'index': 0, 'shape': array([  1, 512, 512,   3], dtype=int32), 'shape_signature': array([  1, 512, 512,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\t\tOUTPUT DETAILS: \r\n\t\t [{'name': 'StatefulPartitionedCall:4', 'index': 66563, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:6', 'index': 66384, 'shape': array([    1, 49104,     4], dtype=int32), 'shape_signature': array([    1, 49104,     4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:5', 'index': 66546, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:1', 'index': 66642, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 66620, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:7', 'index': 66405, 'shape': array([    1, 49104,     3], dtype=int32), 'shape_signature': array([    1, 49104,     3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:3', 'index': 66603, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:0', 'index': 66581, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\n\r\n**Output array:**\r\n\r\n(1, 512, 512, 3)\r\n[[0.9609589  0.0873408  0.06809524 0.05999154 0.05568287 0.03616852\r\n  0.03515935 0.03388214 0.02964088 0.02917778 0.02844885 0.02752271\r\n  0.02532643 0.02329737 0.02278286 0.0213418  0.0210638  0.02019128\r\n  0.01916748 0.01763159 0.01730308 0.0164015  0.01632011 0.01615366\r\n  0.01609802 0.01606014 0.01520342 0.01478559 0.01420614 0.0132328\r\n  0.01292685 0.01276404 0.01265842 0.01264325 0.01245722 0.01230013\r\n  0.01227069 0.01221645 0.01210919 0.01204768 0.01181906 0.01178762\r\n  0.01154467 0.01148716 0.01128137 0.01100454 0.01090339 0.01090035\r\n  0.01056659 0.01046374 0.01039562 0.01035801 0.0101763  0.01016143\r\n  0.01015401 0.0101212  0.00995958 0.00986671 0.00980252 0.00979534\r\n  0.00976944 0.00947464 0.00944296 0.00930616 0.00917202 0.00895733\r\n  0.00894633 0.00891778 0.00886655 0.00885886 0.00849381 0.00849113\r\n  0.00848216 0.00847018 0.00844333 0.00835618 0.00833991 0.00832459\r\n  0.00819856 0.0081273  0.00803497 0.00802234 0.00790223 0.0078285\r\n  0.0078209  0.00782064 0.00778884 0.00771174 0.00768062 0.00766596\r\n  0.0076198  0.00758222 0.00756696 0.00753024 0.00749895 0.00749457\r\n  0.00748292 0.00745744 0.00744084 0.00738394]]\r\noutput array length: 100\r\n\r\nColab that used later version of tfnightly(tf-nightly==2.4.0-dev20201019) for tflite conversion:\r\nhttps://colab.research.google.com/drive/1LVkk8aclpoxwmfdYeanpu6YKSEMoWxRJ?usp=sharing\r\n\r\n\r\nINPUT DETAILS: \r\n [{'name': 'serving_default_input_tensor:0', 'index': 0, 'shape': array([  1, 512, 512,   3], dtype=int32), 'shape_signature': array([  1, 512, 512,   3], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\nOUTPUT DETAILS: \r\n [{'name': 'StatefulPartitionedCall:5', 'index': 66546, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:1', 'index': 66642, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 66620, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:7', 'index': 66405, 'shape': array([    1, 49104,     3], dtype=int32), 'shape_signature': array([    1, 49104,     3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:4', 'index': 66563, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:6', 'index': 66384, 'shape': array([    1, 49104,     4], dtype=int32), 'shape_signature': array([    1, 49104,     4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:3', 'index': 66603, 'shape': array([1, 1, 1], dtype=int32), 'shape_signature': array([ 1, -1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:0', 'index': 66581, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([ 1, -1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\r\n\r\n[  1 512 512   3]\r\n[100.]", "comments": ["@Maithri-CH \r\nI ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6d39700afa98a5635187dcc1433733b0/untitled460.ipynb).", "> @Maithri-CH\r\n> I ran the code shared and face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/6d39700afa98a5635187dcc1433733b0/untitled460.ipynb).\r\n\r\nThe above colab is to convert a saved model to tflite file. The error you got in your colab is because you're trying to access the saved model in my drive! You will have to upload a saved model to the mentioned path or change the path to where your saved model is \r\n\r\nIn case you want to use my saved model:\r\nhere is the link:  https://drive.google.com/drive/folders/1JnaG61kW98NvB9XHt8Cz0PJQ7vi0Nt_I?usp=sharing", "Your model url throws 404 error. TF 2.4 final has released perhaps you can try testing it? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44787\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44787\">No</a>\n"]}, {"number": 44786, "title": "Fix slim record_writer compression type", "body": "The current code does not compile if the corresponding ifdef branch corresponding to SLIM is used.", "comments": ["@mkuchnik  Can you please check @rohan100jain's comments and keep us posted ? Thanks!"]}, {"number": 44785, "title": "H", "body": "", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44785) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 44784, "title": "gradient_transformers throw autograph errors when trying to maintain state in tensorarray", "body": "I'm trying to implement automatic gradient clipping, where the norm of the gradient is stored on each train step, and the gradient is clipped based off the accumulated distribution of gradient norms. However, I seem to be running into some autograph issue.\r\n\r\nHere's the code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\n\r\nclass AutoClipper:\r\n    def __init__(self, clip_percentile):\r\n        self.clip_percentile = clip_percentile\r\n        self.grad_history = None\r\n        self.i = None\r\n\r\n    def __call__(self, grads_and_vars):\r\n        if self.grad_history is None:\r\n            self.grad_history = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n            self.i = 0\r\n\r\n        grad_norms = [_get_grad_norm(g) for g, _ in grads_and_vars]\r\n        total_norm = tf.norm(grad_norms)\r\n        self.grad_history = self.grad_history.write(self.i, total_norm)\r\n        self.i += 1\r\n        clip_value = tfp.stats.percentile(self.grad_history.stack(), q=self.clip_percentile)\r\n        return [(tf.clip_by_norm(g, clip_value), v) for g, v in grads_and_vars]\r\n\r\ndef _get_grad_norm(t, axes=None, name=None):\r\n    values = tf.convert_to_tensor(t.values if isinstance(t, tf.IndexedSlices) else t, name=\"t\")\r\n\r\n    # Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm\r\n    l2sum = tf.math.reduce_sum(values * values, axes, keepdims=True)\r\n    pred = l2sum > 0\r\n    # Two-tap tf.where trick to bypass NaN gradients\r\n    l2sum_safe = tf.where(pred, l2sum, tf.ones_like(l2sum))\r\n    return tf.squeeze(tf.where(pred, tf.math.sqrt(l2sum_safe), l2sum))\r\n\r\n\r\nmodel = tf.keras.models.Sequential(\r\n        [\r\n            tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n            tf.keras.layers.Dense(128, activation=\"relu\"),\r\n            tf.keras.layers.Dropout(0.2),\r\n            tf.keras.layers.Dense(10),\r\n        ]\r\n    )\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.Adam(\r\n            learning_rate=0.001,\r\n            gradient_transformers=[AutoClipper(10)],\r\n        ),\r\n        loss=\"mean_absolute_error\",\r\n        metrics=[\"accuracy\"],\r\n    )\r\n\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n    x_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\n    model.fit(x_train, y_train)\r\n```\r\n\r\nIt seems to fail when trying to update the tensorarray, but I thought tensorarrays could be created outside `tf.function`:\r\n```python\r\nop_name = '__inference_train_function_13337', num_outputs = 4\r\ninputs = [<tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: shape=(), dtype=variant, numpy=<unprintable>>...ensor: shape=(), dtype=resource, numpy=<unprintable>>, <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>>, ...]\r\nattrs = ('executor_type', '', 'config_proto', b'\\n\\x07\\n\\x03CPU\\x10\\x01\\n\\x07\\n\\x03GPU\\x10\\x002\\x02J\\x008\\x01\\x82\\x01\\x00')\r\nctx = <tensorflow.python.eager.context.Context object at 0x7fa34a9b2f50>\r\nname = None\r\n\r\n    def quick_execute(op_name, num_outputs, inputs, attrs, ctx, name=None):\r\n      \"\"\"Execute a TensorFlow operation.\r\n    \r\n      Args:\r\n        op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to\r\n          execute.\r\n        num_outputs: The number of outputs of the operation to fetch.\r\n                     (Explicitly provided instead of being inferred for performance\r\n                     reasons).\r\n        inputs: A list of inputs to the operation. Each entry should be a Tensor, or\r\n          a value which can be passed to the Tensor constructor to create one.\r\n        attrs: A tuple with alternating string attr names and attr values for this\r\n          operation.\r\n        ctx: The value of context.context().\r\n        name: Customized name for the operation.\r\n    \r\n      Returns:\r\n        List of output Tensor objects. The list is empty if there are no outputs\r\n    \r\n      Raises:\r\n        An exception on error.\r\n      \"\"\"\r\n      device_name = ctx.device_name\r\n      # pylint: disable=protected-access\r\n      try:\r\n        ctx.ensure_initialized()\r\n        tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n>                                           inputs, attrs, num_outputs)\r\nE                                           TypeError: An op outside of the function building code is being passed\r\nE                                           a \"Graph\" tensor. It is possible to have Graph tensors\r\nE                                           leak out of the function building context by including a\r\nE                                           tf.init_scope in your function building code.\r\nE                                           For example, the following function will fail:\r\nE                                             @tf.function\r\nE                                             def has_init_scope():\r\nE                                               my_constant = tf.constant(1.)\r\nE                                               with tf.init_scope():\r\nE                                                 added = my_constant * 2\r\nE                                           The graph tensor has name: Adam/TensorArrayV2Write/TensorListSetItem:0\r\n\r\n./lib/python3.7/site-packages/tensorflow/python/eager/execute.py:60: TypeError\r\n```\r\n\r\n**System information**\r\n- tensorflow 2.4.0-rc1\r\n- python 3.5\r\n- ubuntu 20.04", "comments": ["I have tried in colab with TF version 2.4-rc1 and nightly version(`2.5.0-dev20201111`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/258ea3c0234e12f0c82dbd9e9f1e3b7d/untitled516.ipynb#scrollTo=07fbRaD5Zolp). Thanks!", "@lminer Thanks for the issue!\r\n\r\nI think it'd be better to do this with `tf.Variables`. There's a few things I think won't work with the current approach:\r\n\r\n(1) Access to a TensorArray element created in one tf.function call and accessed in another\r\n(2) Python control flow via `self.i += 1`\r\n\r\n(1) is the issue you're hitting now, (2) will mean only the first TensorArray element ever gets updated:\r\n\r\n```\r\ngrads_and_vars = [(tf.convert_to_tensor(1.), tf.Variable(1.))]\r\nautoclipper = AutoClipper(10)\r\n\r\n@tf.function\r\ndef my_fn(gv):\r\n  return autoclipper(gv)\r\n\r\nmy_fn(grads_and_vars)\r\nmy_fn(grads_and_vars)\r\n\r\nprint(autoclipper.i)\r\n```\r\n\r\nInstead I'd recommend creating a `tf.Variable` of shape `N` and doing running statistics over the last `N` batches for a similar result. Closing because I don't think this is an issue with the `gradient_transformers` but feel free to re-open if I've misunderstood", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44784\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44784\">No</a>\n", "Thanks @omalleyt12 I think that fixed it."]}, {"number": 44783, "title": "boringssl: cc1plus: error: command line option '-Wmissing-prototypes' is valid for C/ObjC but not for C++ [-Werror]", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.4\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: venv\r\n- Bazel version (if compiling from source): 3.5.0\r\n- GCC/Compiler version (if compiling from source): 9.1\r\n- CUDA/cuDNN version: 7\r\n- GPU model and memory: N/A (build issue)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCompiling the r2.4 branch from source on CentOS 8 using gcc-toolset-9, the build fails like so:\r\n\r\n```\r\n/opt/rh/gcc-toolset-9/root/usr/bin/g++ -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/external/boringssl/_objs/crypto/socket.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/boringssl/_objs/crypto/socket.pic.o' -fPIC -iquote external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=broadwell' -Wno-sign-compare -fpermissive -Wa,--noexecstack '-D_XOPEN_SOURCE=700' -Wall -Werror '-Wformat=2' -Wsign-compare -Wmissing-field-initializers -Wwrite-strings -Wshadow -fno-common '-std=c11' -Wmissing-prototypes -Wold-style-definition -Wstrict-prototypes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/boringssl/src/crypto/bio/socket.c -o bazel-out/k8-opt/bin/external/boringssl/_objs/crypto/socket.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\ncc1plus: error: command line option '-Wmissing-prototypes' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: error: command line option '-Wold-style-definition' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: error: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: all warnings being treated as errors\r\n```\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nscl enable gcc-toolset-9 -- bash\r\nbazel build --action_env=CC=$(which g++) //tensorflow/tools/pip_package:build_pip_package\r\n\r\n**Any other info / logs**\r\n\r\nThe third-party boringssl compilation passes options that are not valid for C++, and GCC 9 calls them out with a warning. Since the compilation also passes -Werror, it fails.", "comments": ["@plopresti,\r\nCould you please provide all the command which you have executed before running into this issue.\r\n\r\nAlso, please follow the instructions as per the [official guide](https://www.tensorflow.org/install/source#tested_build_configurations) while building TensorFlow from source. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44783\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44783\">No</a>\n"]}, {"number": 44782, "title": "Starting to consolidate the xtensa kernels.", "body": "First steps towards http://b/173043817\r\n\r\nCopied the xtensa_hifimini implementations into a new xtensa directory and added an appropriate makefile.\r\n\r\nTested the following commands:\r\n\r\nOlder build command:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa_hifimini TAGS=xtensa_hifimini XTENSA_CORE=<xtensa_core> test_keyword_benchmark\r\n```\r\n\r\nOutput:\r\n```\r\nInitializeKeywordRunner() took 1388393 ticks (1388 ms)\r\nKeywordRunNIerations(1) took 88408 ticks (88 ms)\r\nKeywordRunNIerations(10) took 883639 ticks (883 ms)\r\n```\r\n\r\nConsolidated makefile invocation:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_keyword_benchmark\r\n```\r\n\r\nOutput:\r\n```\r\nInitializeKeywordRunner() took 1388465 ticks (1388 ms)\r\nKeywordRunNIerations(1) took 88408 ticks (88 ms)\r\nKeywordRunNIerations(10) took 883639 ticks (883 ms)\r\n```\r\n\r\nxt-size (note the different location of the two output binaries)\r\n\r\nBuild type     |   text        |   data\t|    bss    |    dec      |\tfilename |\r\n|-----------       | ----------    |  --------   |   --------  | --------     |   -----------|\r\nOld                |  54864\t|  48040\t|  25032  | 127936  | tensorflow/lite/micro/tools/make/gen/xtensa_hifimini_xtensa_hifimini/bin/keyword_benchmark\r\nConsolidated |  54864\t|  48024\t|  25032 | 127920   |  tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/bin/keyword_benchmark", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "> Looks good! Is the plan to have alternative implementations for xtensa (for example, 3z, hifi4, etc) live in these files as well guarded by ifdefs?\r\n\r\nYes, more details are in the internal bug, linked from the PR description.", "tagging @pnikam-cad @nyadla-sys @kpraving"]}, {"number": 44781, "title": "Bump libjpeg-turbo from 2.0.4 to 2.0.5", "body": "It looks like the latest libjpeg-turbo is 2.0.5 so this PR\r\nbumps the version (currently on 2.0.4).\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Thanks @mihaimaruseac for the help. I have created a PR #44817 for cherry-pick the commit to R2.4."]}, {"number": 44780, "title": "Fix constructor of CommunicationOptions", "body": "Fix #44768\r\n\r\nPiperOrigin-RevId: 341888489\r\nChange-Id: I625f2e5412a217a95e3306fc6345f378b932071b\n\n<!-- Reviewable:start -->\n---\nThis change is\u2002[<img src=\"https://reviewable.io/review_button.svg\" height=\"34\" align=\"absmiddle\" alt=\"Reviewable\"/>](https://reviewable.io/reviews/tensorflow/tensorflow/44780)\n<!-- Reviewable:end -->\n", "comments": []}, {"number": 44779, "title": "Defect in code example", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/guide/data\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe code example has problems:\r\n\r\n```\r\nTo allow some overlap between the features of one batch and the labels of another, use Dataset.zip:\r\n\r\nfeature_length = 10\r\nlabel_length = 5\r\n\r\nfeatures = range_ds.batch(feature_length, drop_remainder=True)\r\nlabels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:-5])\r\n\r\npredict_5_steps = tf.data.Dataset.zip((features, labels))\r\n\r\nfor features, label in predict_5_steps.take(3):\r\n  print(features.numpy(), \" => \", label.numpy())\r\n```\r\n\r\n1. The ``label_length`` variable is never used.\r\n2. The slicing ``labels[:-5]`` is wrong. The only reason it works is because 10-5 is 5.\r\n\r\n### Clear description\r\n\r\nLet's illustrate the problem by slightly modifying the defective code.\r\n\r\n```python\r\nfeature_length = 10\r\nlabel_length = 3\r\n\r\nfeatures = range_ds.batch(feature_length, drop_remainder=True)\r\nlabels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:-label_length])\r\n\r\npredict_5_steps = tf.data.Dataset.zip((features, labels))\r\n\r\nfor features, label in predict_5_steps.take(3):\r\n  print(features.numpy(), \" => \", label.numpy())\r\n```\r\n\r\nThis prints:\r\n\r\n```\r\n[0 1 2 3 4 5 6 7 8 9]  =>  [10 11 12 13 14 15 16]\r\n[10 11 12 13 14 15 16 17 18 19]  =>  [20 21 22 23 24 25 26]\r\n[20 21 22 23 24 25 26 27 28 29]  =>  [30 31 32 33 34 35 36]\r\n```\r\n\r\nThis is wrong because the label sequence length is not 3.\r\n\r\nThe correct code is below. This will work as long as ``label_length <= feature_length``.\r\n\r\n```python\r\nfeature_length = 10\r\nlabel_length = 3\r\n\r\nfeatures = range_ds.batch(feature_length, drop_remainder=True)\r\nlabels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:label_length])\r\n\r\npredict_5_steps = tf.data.Dataset.zip((features, labels))\r\n\r\nfor features, label in predict_5_steps.take(3):\r\n  print(features.numpy(), \" => \", label.numpy())\r\n```\r\n\r\nThis will correctly print:\r\n\r\n```\r\n[0 1 2 3 4 5 6 7 8 9]  =>  [10 11 12]\r\n[10 11 12 13 14 15 16 17 18 19]  =>  [20 21 22]\r\n[20 21 22 23 24 25 26 27 28 29]  =>  [30 31 32]\r\n```\r\n\r\nA better and more generic code should avoid using names like ``predict_5_steps``:\r\n\r\n```python\r\nfeature_length = 10\r\nlabel_length = 3\r\n\r\nfeatures = range_ds.batch(feature_length, drop_remainder=True)\r\nlabels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:label_length])\r\n\r\ninput_ds = tf.data.Dataset.zip((features, labels))\r\n\r\nfor features, label in input_ds.take(3):\r\n  print(features.numpy(), \" => \", label.numpy())\r\n```\r\n\r\n\r\n\r\n### Submit a pull request?\r\n\r\n", "comments": ["Can you open a small PR?", "PR filed.\r\n\r\nhttps://github.com/tensorflow/docs/pull/1744", "This issue still exists in the doc. can i be assigned for this task?", "> This issue still exists in the doc. can i be assigned for this task?\r\n\r\nCause @bibhas2 Didn't sign the CLA with the PR at https://github.com/tensorflow/docs/pull/1744", "Sorry. I don't feel comfortable signing anything.", "@zyberg2091 If you want you can submit a Doc PR", "ok thanks!\r\n", "Submitted the PR\r\n\r\ntensorflow/docs#1797", "Closing this issue since the associated PR has been merged. Thanks!"]}, {"number": 44778, "title": "Fix TFRecord uncompressed test cases", "body": "Two of the test cases were testing the same code path. A TFRecord test case was testing the GZIP (compressed) path twice (case 2 and 3) rather than using one test case (case 3) to test the uncompressed path. This PR changes the latter test to test the uncompressed code path, as was intended.", "comments": []}, {"number": 44777, "title": "Could not load dynamic library 'libcusolver.so.10' - TF-2.4.0RC, Cuda,CudNN, RTX 3080", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 2.4.0-rc1\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1 & 8.0.5\r\n- GPU model and memory: RTX 3080\r\n\r\n**Describe the problem**\r\nDownloading everything per instructions, all GPU libs are being read except the one in the title. No idea why. \r\n\r\n`2020-11-11 14:48:06.269458: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-11 14:48:06.269897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2020-11-11 14:48:06.303715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-11 14:48:06.304046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1724] Found device 0 with properties: \r\npciBusID: 0000:08:00.0 name: GeForce RTX 3080 computeCapability: 8.6\r\ncoreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.78GiB deviceMemoryBandwidth: 707.88GiB/s\r\n2020-11-11 14:48:06.304061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-11 14:48:06.305025: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-11 14:48:06.305052: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-11 14:48:06.305398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-11 14:48:06.305501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-11 14:48:06.305583: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:/usr/lib/cuda/include:/usr/lib/cuda/lib64:\r\n2020-11-11 14:48:06.305805: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-11 14:48:06.305877: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-11 14:48:06.305883: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1761] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-11-11 14:48:06.306061: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-11-11 14:48:06.306326: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-11-11 14:48:06.306336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1265] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-11-11 14:48:06.306340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1271]      `\r\n", "comments": ["I built Tensorflow with CUDA 11.1.1 and CuDNN 8.0.5 myself today and works just fine, when comparing the loaded libraries there are some differences:\r\n\r\nOn my system;\r\nSuccessfully opened dynamic library libcudart.so.11.0\r\nSuccessfully opened dynamic library libcublas.so.11\r\nSuccessfully opened dynamic library libcublasLt.so.11\r\nSuccessfully opened dynamic library libcufft.so.10\r\nSuccessfully opened dynamic library libcurand.so.10\r\nSuccessfully opened dynamic library libcusolver.so.11\r\nSuccessfully opened dynamic library libcusparse.so.11\r\nSuccessfully opened dynamic library libcudnn.so.8\r\n\r\nIn your case we find: Could not load dynamic library 'libcusolver.so.10', while it should be looking for libcusolver.so.11. Maybe you accidentally built Tensorflow against some leftover files from an old CUDA installation? Try to completely delete CUDA first (delete /usr/local/cuda*), then do a fresh CUDA 11.1.1 install, then install CuDNN 8.0.5, then install TensorRT. Then during ./configure, ensure that the libraries it finds are indeed the correct ones, and ensure you enter CUDA 11, CuDNN 8 and TensorRT 7, rather than the defaults 10, 7, and 6.\r\n\r\nHope this helps! :)\r\n\r\nAlso, even though it should not be the root cause of this problem, clean up your LD_LIBRARY_PATH, it should not contain the same path so many times.\r\n\r\n**EDIT** Oh my bad, I did not notice you didn't build from source. I am unsure why the tf-nightly build is built against libcusolver.so.10. You could try to compile it yourself. Takes a bit more time, but works for me.", "Do you encounter the same issues with 2.4.0 RC0 or RC1?", "Yup @ion-elgreco \r\non 2.4.0Rc0", "@Amokstakov \r\n\r\nLook for the library in cuda 10.1 or cuda 10.2 folders in your installation directory and then add it to the path.\r\nPlease, check this [SO comment](https://stackoverflow.com/a/64472380) for more information. Thanks!", "I have the same problem with tf-nightly, CUDA 11.1.1 and cudnn 8.0.5 on Ubuntu. \r\n\r\nEdit:\r\nhttps://github.com/tensorflow/tensorflow/issues/43947#issuecomment-715295153\r\nThat solution worked for me.", "@nilskk I clicked the link to the comment you added, and I am not quite sure what to do?\r\nI can match the tf, python and driver version but what do I do with the sudo command he added? Any help would be great!", "> I have the same problem with tf-nightly, CUDA 11.1.1 and cudnn 8.0.5 on Ubuntu.\r\n> \r\n> Edit:\r\n> [#43947 (comment)](https://github.com/tensorflow/tensorflow/issues/43947#issuecomment-715295153)\r\n> That solution worked for me.\r\n\r\nCan you post your set up also? with dependencies for CUDA version and CUdNN version ?", "> > I have the same problem with tf-nightly, CUDA 11.1.1 and cudnn 8.0.5 on Ubuntu.\r\n> > Edit:\r\n> > [#43947 (comment)](https://github.com/tensorflow/tensorflow/issues/43947#issuecomment-715295153)\r\n> > That solution worked for me.\r\n> \r\n> Can you post your set up also? with dependencies for CUDA version and CUdNN version ?\r\n\r\nSystem information\r\n\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n    TensorFlow installed from (source or binary): pip\r\n    TensorFlow version: tf_nightly\r\n    Python version: 3.8.5\r\n    Installed using virtualenv? pip? conda?: conda\r\n    Bazel version (if compiling from source):\r\n    GCC/Compiler version (if compiling from source):\r\n    CUDA/cuDNN version: 11.1.1 & 8.0.5\r\n    GPU model and memory: RTX 3080\r\n\r\nI think sudo ln creates a link from libcusolver.so.11 (which is not yet supported) to libcusolver.so.10, so that tensorflow finds it. But I don't have much knowledge on that part. Just guessing and trying. For me it worked and my RTX3080 now runs Tensorflow.\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I do a soft link for libcusolver.so.10 to libcusolver.so.11. Now it seems to work but I don't know it really works and doesn't do wrong later on.", "2020-11-24 15:54:49.479877: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\r\n2020-11-24 15:54:49.479930: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\r\n2020-11-24 15:54:49.479954: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\r\n2020-11-24 15:54:49.479974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\r\n2020-11-24 15:54:49.479993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\r\n2020-11-24 15:54:49.480024: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\r\n2020-11-24 15:54:49.480048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\r\n2020-11-24 15:54:49.480072: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\r\n2020-11-24 15:54:49.480183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-24 15:54:49.480605: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-11-24 15:54:49.480897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\r\n2020-11-24 15:54:49.4809", "previously, it reports the error:Could not load dynamic library 'libcusolver.so.10'", "+1 experiencing this issue on tf-nightly 11/30/2020 and tensorflow 2.4 RC3\r\n\r\nCould not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n\r\nWill CUDA 11.1 be supported in TF 2.4?", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44777\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44777\">No</a>\n", "So what was the solution? I am having the same issue and can't seem to figure out what to do", "I am also having this issue with a brand new installation of TF2.4 - was this left hanging somehow?\r\n\r\nI think a solution might be to install cuda 10 and add libcusolver to your path..\r\n\r\nSymlinking libcusolver.so.11 to .so.10 does NOT work, though this issue gives it as a solution:\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/43947\r\n\r\nWhat finally worked for me was installing CUDA11.0 - seems any higher versions are not supported.\r\n\r\n", "Hmm thanks! I ended up moving back to 2.3.1 and everything works like a breeze...", "The only workaround which is consistent with 2.4 seems to be to uninstall 11.2 and install 11.0. Nothing else resolves the \"Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\"\r\n\r\nThis is not a tensorflow bug -- it is an NVIDIA bug.", "One workaround is to install libcusolver-11-0 side by side with cuda-11-1 or cuda-11-2 and tweak your LD_LIBRARY_PATH. For example, to get TensorFlow 2.4 to work with CUDA 11.2:\r\n\r\n```\r\n$ apt-get install cuda-11-2 libcusolver-11-0\r\n$ export LD_LIBRARY_PATH=/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.0/lib64\r\n```", "```\r\ncd $LD_LIBRARY_PATH\r\nsudo ln libcusolver.so.11 libcusolver.so.10  # hard link\r\n```\r\nI was able to solve the issue with missing hard linking `libcusolver.so.11` with `libcusolver.so.10`. (CUDA 11.2, RTX3070s)", "> ```\r\n> cd $LD_LIBRARY_PATH\r\n> sudo ln libcusolver.so.11 libcusolver.so.10  # hard link\r\n> ```\r\n> \r\n> I was able to solve the issue with hard linking missing `libcusolver.so.11` with `libcusolver.so.10`. (CUDA 11.2, RTX3070s)\r\n\r\nWorked for me on Debian Testing.  \r\n\r\ncd /usr/lib/x86_64-linux-gnu\r\nsudo ln libcusolver.so.11 libcusolver.so.10  # hard link", "> > ```\r\n> > cd $LD_LIBRARY_PATH\r\n> > sudo ln libcusolver.so.11 libcusolver.so.10  # hard link\r\n> > ```\r\n> > \r\n> > \r\n> > I was able to solve the issue with hard linking missing `libcusolver.so.11` with `libcusolver.so.10`. (CUDA 11.2, RTX3070s)\r\n> \r\n> Worked for me on Debian Testing.\r\n> \r\n> cd /usr/lib/x86_64-linux-gnu\r\n> sudo ln libcusolver.so.11 libcusolver.so.10 # hard link\r\n\r\nthis also worked for me... is this dangerous in some way? i mean, it's loading a different verson than it expects, no?", "This appears to still be an issue, but maybe everyone is finding different ways around the problem.\r\n\r\nRunning an strace showed me that tensorflow is trying to find \"libcusolver.so.10\" under \"~/.local\", and not in its (probably expected) location, since none of the other shared libs are here.  Could have something to do with the fact that I installed tensorflow via a \"local\" install not system-level install.\r\n\r\nAfter I put soft link here everything worked fine:\r\n~/.local/lib/python3.7/site-packages/tensorflow/python/libcusolver.so.10\r\n\r\nWhile troubleshooting I found a different, but maybe already reported issue.  keras will complain if I try to run tf-nightly.  It doesn't seem to recognize that the tf-nightly version is above its required version.", "Is this because tf 2.4 still bases on  libcusolver.so.10? With newer version of tf, this problem will go away?", "> ```\r\n> cd $LD_LIBRARY_PATH\r\n> sudo ln libcusolver.so.11 libcusolver.so.10  # hard link\r\n> ```\r\n> \r\n> I was able to solve the issue with missing hard linking `libcusolver.so.11` with `libcusolver.so.10`. (CUDA 11.2, GetForce 2060)\r\n\r\nThis works perfectly,just make sure  you have set LD_library path exactly where libcusolver.so.11 is present and make the hard link... check your LD_library_path\r\n\r\nThanks", "> ```\r\n> cd $LD_LIBRARY_PATH\r\n> sudo ln libcusolver.so.11 libcusolver.so.10  # hard link\r\n> ```\r\n> \r\n> I was able to solve the issue with missing hard linking `libcusolver.so.11` with `libcusolver.so.10`. (CUDA 11.2, RTX3070s)\r\n\r\nFor me, this does not work. Making a link to the tensorflow library instead of $LD_LIBRARY_PATH solves my problem.\r\n```\r\nln -s /usr/local/cuda/lib64/libcusolver.so.11 (venv dir)/lib/python3.6/site-packages/tensorflow/python/libcusolver.so.10\r\n```\r\nIn my case\r\n```\r\nln -s /usr/local/cuda/lib64/libcusolver.so.11 ~/.local/share/virtualenvs/my-project/lib/python3.7/site-packages/tensorflow/python/libcusolver.so.10\r\n```\r\nEither soft or hard link works.\r\n\r\n(CUDA 11.2, GTX970, debian 10)", "> ```\r\n> cd $LD_LIBRARY_PATH\r\n> sudo ln libcusolver.so.11 libcusolver.so.10  # hard link\r\n> ```\r\n> \r\n> I was able to solve the issue with missing hard linking `libcusolver.so.11` with `libcusolver.so.10`. (CUDA 11.2, RTX3070s)\r\n\r\nit's work for me.", "@cameronjacobson good catch!\r\nThe hard link didn't work for me but your approach gave me the hint to add the soft link in my virtual environment.\r\n\r\n```\r\ncd /.virtualenvs/myvenvname/lib/python3.8/site-packages/tensorflow/python\r\nln -s /path/to/my/cuda/lib/libcusolver.so.11 libcusolver.so.10\r\n```\r\nSystem: cuda 11.2, cuDNN 8.2, GTX1080, pop_OS 20.04 (ubuntu 20.04)", "`apt install nvidia-cuda-toolkit` solved the issue for me", "I have cuda 11.1 and cudnn 8.0.5 installed, while as suggested by [TensorFlow Tested build configurations](https://www.tensorflow.org/install/source), we should have cuda 11.0 when installing TF 2.4. This should be that reason. If you are using Conda, a workaround is to:\r\n```\r\nconda install cudatoolkit=11.0.221 cudnn=8.0.4 -c nvidia\r\n```"]}, {"number": 44775, "title": "unable to install 2.4.0rc1 from pip", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version: 2.4.0rc1\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow-gpu-estimator<2.5.0,>=2.4.0rc0 (from tensorflow-gpu==2.4.0rc1) (from versions: 2.1.0,2.2.0, 2.3.0)\r\nERROR: No matching distribution found for tensorflow-gpu-estimator<2.5.0,>=2.4.0rc0 (from tensorflow-gpu==2.4.0rc1)\r\n```", "comments": ["You are missing `-` in your command.\r\nTry `pip install==2.4.0-rc1`\r\nAlso did you try upgrading your `pip` version?", "@iperov \r\nIt is not necessary for you to install tensorflow-estimator first. You may try Installing tensorflow-gpu, which internally will install all the dependencies like tensorflow-estimator, tensorboard. Please check the attached [gist](https://colab.research.google.com/gist/Saduf2019/922280a616618fbe0473e43c66df07e1/untitled460.ipynb) for more. Thanks!", "pip is last\r\n\r\n`pip install tensorflow-gpu==2.4.0-rc1` produces the same error with estimator\r\n\r\n`pip install tensorflow-gpu` installs 2.3.1, but I need 2.4.0rc1", "```\r\npip install tensorflow-gpu\r\npip install tensorflow-gpu==2.4.0-rc1\r\n```\r\nsame error.\r\n", "did you even read the error?\r\n\r\ntensorflow-gpu==2.4.0-rc1\r\nREQUIRES\r\ntensorflow-gpu-estimator<2.5.0,>=2.4.0rc0\r\n\r\nbut\r\nlast version of `tensorflow-gpu-estimator` in pip is 2.3.0", "This is a bug in the release process due to the single pip package renaming. I'll make a new `tensorflow-gpu-estimator` release and then this should be fixed.", "The `tensorflow-gpu-estimator` wheel should be up now.", "This should be solved now. Closing, but please reopen if this is not the case.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44775\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44775\">No</a>\n"]}, {"number": 44774, "title": "Restored SavedModel + saved_model_cli raise exception when the object is deleted", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.6\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: cpu\r\n\r\n**Current and expected behavior**\r\n\r\nI'm exporting using a `tf.Module` two graphs created by decorating two method with `@tf.function`. I expect the SavedModel to be correctly exported and to not have a crash. Instead\r\n\r\n- I guess the SavedModel is not correctly created, since in the \"serving_default\" I can find only the information of one method and I don't know how to call the other method I'm exporting.\r\n- When I use `saved_model_cli show --all --dir at` I got an exception (see below)\r\n- I can get the same exception if I re-load (using `tf.saved_model.load(\"at\")`) the model and I delete it (the exception is when the object goes out of scope, and not when the load method is invoked).\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nimport tensorflow.keras as k\r\n\r\n\r\nclass ActivityTracker(tf.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.num_classes = 6  # activities in the training set\r\n        self.mapping = tf.lookup.StaticHashTable(\r\n            tf.lookup.KeyValueTensorInitializer(\r\n                keys=tf.range(self.num_classes, dtype=tf.int32),\r\n                values=[\r\n                    \"Walking\",\r\n                    \"Jogging\",\r\n                    \"Upstairs\",\r\n                    \"Downstairs\",\r\n                    \"Sitting\",\r\n                    \"Standing\",\r\n                ],\r\n            ),\r\n            \"Unknown\",\r\n        )\r\n\r\n        self.num_features = 3  # sensor (x,y,z)\r\n        self.batch_size = 32\r\n\r\n        # 33,Jogging,49106062271000,5.012288,11.264028,0.95342433;\r\n        self._model = k.Sequential(\r\n            [\r\n                k.layers.Input(\r\n                    shape=(1, self.num_features), batch_size=self.batch_size\r\n                ),\r\n                # Note the stateful=True\r\n                k.layers.LSTM(64, stateful=True),\r\n                k.layers.Dense(self.num_classes),\r\n            ]\r\n        )\r\n\r\n        self._global_step = tf.Variable(0, dtype=tf.int32, trainable=False)\r\n        self._optimizer = k.optimizers.SGD(learning_rate=1e-4)\r\n        # Sparse, so we can feed the scalar and get the one hot representation\r\n        # From logits so we can feed the unscaled (linear activation fn)\r\n        # directly to the loss\r\n        self._loss = k.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\n        self._last_tracked_activity = tf.Variable(-1, dtype=tf.int32, trainable=False)\r\n\r\n    @tf.function(\r\n        input_signature=[\r\n            tf.TensorSpec(shape=(None, 1, 3), dtype=tf.float32),\r\n            tf.TensorSpec(shape=(None,), dtype=tf.int32),\r\n        ]\r\n    )\r\n    def learn(self, sensor_data, labels):\r\n        # All the sensor data should be about the same activity\r\n        tf.assert_equal(labels, tf.zeros_like(labels) + labels[0])\r\n\r\n        # If the activity changes, we must reset the RNN state since the last update\r\n        # and the current update are not related.\r\n\r\n        if tf.not_equal(self._last_tracked_activity, labels[0]):\r\n            tf.print(\r\n                \"Resetting states. Was: \",\r\n                self._last_tracked_activity,\r\n                \" is \",\r\n                labels[0],\r\n            )\r\n            self._last_tracked_activity.assign_sub(labels[0])\r\n            self._model.reset_states()\r\n\r\n        self._global_step.assign_add(1)\r\n        with tf.GradientTape() as tape:\r\n            loss = self._loss(labels, self._model(sensor_data))\r\n            tf.print(self._global_step, \": loss: \", loss)\r\n\r\n        gradient = tape.gradient(loss, self._model.trainable_variables)\r\n        self._optimizer.apply_gradients(zip(gradient, self._model.trainable_variables))\r\n        return loss\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 1, 3), dtype=tf.float32)])\r\n    def predict(self, sensor_data):\r\n        predictions = self._model(sensor_data)\r\n        predicted = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\r\n        tf.print(self.mapping.lookup(predicted))\r\n        return predicted\r\n\r\n\r\ndef main() -> int:\r\n    at = ActivityTracker()\r\n\r\n    # Executing an invocation of every graph we want to export is mandatory\r\n    at.learn(\r\n        tf.zeros((at.batch_size, 1, 3), dtype=tf.float32),\r\n        tf.zeros((at.batch_size), dtype=tf.int32),\r\n    )\r\n    at.predict(tf.zeros((at.batch_size, 1, 3), dtype=tf.float32))\r\n\r\n    tf.saved_model.save(at, \"at\")\r\n\r\n    restored = tf.saved_model.load(\"at\")\r\n    return 0\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    sys.exit(main())\r\n```\r\n\r\n**Exception and SavedModel** (that I guess is wrong)\r\n\r\n```\r\nsaved_model_cli show --all --dir at/           \r\n2020-11-11 18:16:54.735051: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\r\n2020-11-11 18:16:54.735089: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\n\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n2020-11-11 18:16:57.323153: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 20 outputs. Output shapes may be inaccurate.\r\n2020-11-11 18:16:57.817962: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\r\n2020-11-11 18:16:57.817991: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\r\n2020-11-11 18:16:57.818017: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (i3): /proc/driver/nvidia/version does not exist\r\n\r\nDefined Functions:\r\n  Function Name: 'learn'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          sensor_data: TensorSpec(shape=(None, 1, 3), dtype=tf.float32, name='sensor_data')\r\n        Argument #2\r\n          labels: TensorSpec(shape=(None,), dtype=tf.int32, name='labels')\r\n\r\n  Function Name: 'predict'\r\n    Option #1\r\n      Callable with:\r\n        Argument #1\r\n          sensor_data: TensorSpec(shape=(None, 1, 3), dtype=tf.float32, name='sensor_data')\r\nException ignored in: <function CapturableResourceDeleter.__del__ at 0x7f7b9f8af700>\r\nTraceback (most recent call last):\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py\", line 202, in __del__\r\n    self._destroy_resource()\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 780, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 823, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 696, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3213, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3065, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 986, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 600, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 237, in restored_function_body\r\n    return _call_concrete_function(function, inputs)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 74, in _call_concrete_function\r\n    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py\", line 105, in _call_flat\r\n    return super(_WrapperFunction, self)._call_flat(args, captured_inputs,\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 1938, in _call_flat\r\n    flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 573, in call\r\n    outputs = functional_ops.partitioned_call(\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/ops/functional_ops.py\", line 1192, in partitioned_call\r\n    f.add_to_graph(graph)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 495, in add_to_graph\r\n    g._add_function(self)\r\n  File \"/home/paolo/projects/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3344, in _add_function\r\n    pywrap_tf_session.TF_GraphCopyFunction(self._c_graph, function._c_func.func,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).mapping._initializer\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n```", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.5.0-dev20201111`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/2b86d526f1a967b4c42b49254157ee26/untitled517.ipynb).Thanks!", "Thanks for reporting.  This one was a fun one to investigate.  I have a fix out that should make it into 2.5.  I'll update this issue with the commit ID once it makes it over to github.", "> Thanks for reporting. This one was a fun one to investigate. I have a fix out that should make it into 2.5. I'll update this issue with the commit ID once it makes it over to github.\r\n\r\nThank you @dellis23 :smile: ", "Fixed in c29e9f25e761df06e1d401868fbc0ff2071a0ed2", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44774\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44774\">No</a>\n"]}, {"number": 44772, "title": "Missing Windows binaries", "body": "I was wondering why `tensorflow_framework.dll` and `tensorflow_framework.lib` are missing from the released binaries for windows [here](https://www.tensorflow.org/install/lang_c). They are included for the other platforms. I'm not sure what to change to include them in a PR, but can investigate if needed. cc @alextp \r\n", "comments": ["I'm no longer working on TF.\r\n\r\n@aselle can you take a look?", "@eaplatanios \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "@Saduf2019 I selected \"Other Issues\" for which there was no template. Also, the relevant TF version is the latest release version, 2.3.1, and I linked to the releases page where the binaries are missing in my original post. There are no steps to reproduce because there is not really an error I ran into, but rather just missing binaries from the released zip files.", "@eaplatanios, are you seeing a concrete error of missing symbols. I'm not sure if the windows build process is different and coalesces everything into tensorflow.dll.", "@aselle you are right. I'm sorry I missed this because in my CMake config I was explicitly looking for the tensorflow_framework lib in addition to tensorflow. It turns out that `tensorflow.dll` does seem to include all symbols. Thank you! I'll close the issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44772\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44772\">No</a>\n", "@eaplatanios glad you got it working. Thanks for asking!"]}]