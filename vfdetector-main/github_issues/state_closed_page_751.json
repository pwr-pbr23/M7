[{"number": 31035, "title": "microcontroller ops", "body": "HI, i just want to know in all_ops_resolver.cc \r\nit only has these ops for microcontroller.\r\n\r\n> TfLiteRegistration* Register_DEPTHWISE_CONV_2D();\r\n> TfLiteRegistration* Register_FULLY_CONNECTED();\r\n> TfLiteRegistration* Register_SOFTMAX();\r\n> TfLiteRegistration* Register_CONV_2D();\r\n> TfLiteRegistration* Register_AVERAGE_POOL_2D();\r\n> TfLiteRegistration* Register_MAX_POOL_2D();\r\n> TfLiteRegistration* Register_ABS();\r\n> TfLiteRegistration* Register_PRELU();\r\n>\r\n\r\n[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/micro/kernels/all_ops_resolver.cc](url)\r\n\r\nAnd in hello world test, the keras frame can be used, so i want to know if it is possible to use the model with tf.keras.layers.Conv2D and tf.keras.layers.MaxPool2D?\r\n\r\nAnd if is possible to use tf.keras.layers.Convolution1D and tf.keras.layers.MaxPooling1D?\r\n\r\n\r\nBesides, if the model can be converted to .tflite means that the model can be deploy to the sparkfun board?\r\n\r\nthank you\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "I use the Linux Ubuntu 16.04 in VMware with Tensorflow 1.14 with CPU. I install Tensorflow with pip.\r\nBuild label: 0.27.1\r\nPython 3.5.2\r\n[GCC 5.4.0 20160609] on linux\r\ntf.version\r\n'1.14.0'", "@wendy518 Could you please elaborate issue with context. Will it be possible to provide the minimal reproducible code. Thanks!", "`model = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Convolution2D(30, (1,1), activation='relu',input_shape = (X_train.shape[1], X_train.shape[2],1)))\r\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), padding='same'))\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(tf.keras.layers.Dense(units =20,kernel_initializer = 'uniform', activation = 'relu'))\r\nmodel.add(tf.keras.layers.Dense(units = 5,kernel_initializer = 'uniform', activation = 'softmax'))`\r\n\r\nI want to know if this model can be used in the microcontroller, since the ops file does not represent clearly, if the high API can be used.\r\nThank you\r\n", "I think we support all the ops you need for the model you posted.  The best way to evaluate this is to train your model, convert to a micro-compatible TFLite model using the [TFLite converter](https://www.tensorflow.org/lite/convert/python_api), then follow the instructions [here](https://www.tensorflow.org/lite/microcontrollers/build_convert) to run the model on a microcontroller platform like Sparkfun Edge.  If you run into any bugs along the way, please file a bug and we will look into it."]}, {"number": 31034, "title": "Quantization Aware Training On Eager Mode", "body": "I am developing my model in the eager mode by using Keras models. \r\nI read a couple of examples and guidelines in quantization aware training. \r\nMost of them are not using gradient tape but uses the inbuilt training function\r\nto train and it looks straight forward. But let's say, someone, is calculating the gradient and have written the training model in a custom way. \r\nHow to use quantization aware training in such a case?\r\nI am asking this question because the function call takes input from a created tf.Graph() instance. If someone has tf.GradientTape() instance, how to deal with this?\r\n\r\nI am new to Tensorflow concepts, maybe I am not following it correctly.  It will be great if someone can explain to me how to use quantization aware training in general. ", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\nIf you think we've misinterpreted a bug, please comment again with a clear explanation, as well as all of the information requested in the [issue template](https://github.com/tensorflow/tensorflow/issues/new). Thanks!\r\n"]}, {"number": 31033, "title": "Failed to load the native TensorFlow runtime - Tensorflow 1.14.0 (CPU) - Python 3.7.3", "body": "System information:\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Windows Server 2008 R2 Standard\r\nTensorFlow installed from: pip install\r\nTensorFlow version: 1.14.0 CPU \r\nPython Version: 3.7.3\r\nAnaconda 2019.07 for Windows\r\nCPU Intel Xeon X5650 @ 2.67GHz \r\n\r\nThe installation seems to work but when I try to import tensorflow I get the following error:\r\n\r\nImportError                               Traceback (most recent call last)\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     57 \r\n---> 58   from tensorflow.python.pywrap_tensorflow_internal import *\r\n     59   from tensorflow.python.pywrap_tensorflow_internal import __version__\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in <module>\r\n     27             return _mod\r\n---> 28     _pywrap_tensorflow_internal = swig_import_helper()\r\n     29     del swig_import_helper\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py in swig_import_helper()\r\n     23             try:\r\n---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n     25             finally:\r\n\r\nC:\\Anaconda3\\lib\\imp.py in load_module(name, file, filename, details)\r\n    241         else:\r\n--> 242             return load_dynamic(name, filename, file)\r\n    243     elif type_ == PKG_DIRECTORY:\r\n\r\nC:\\Anaconda3\\lib\\imp.py in load_dynamic(name, path, file)\r\n    341             name=name, loader=loader, origin=path)\r\n--> 342         return _load(spec)\r\n    343 \r\n\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-d6579f534729> in <module>\r\n----> 1 import tensorflow\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py in <module>\r\n     26 \r\n     27 # pylint: disable=g-bad-import-order\r\n---> 28 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n     29 from tensorflow.python.tools import module_util as _module_util\r\n     30 \r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py in <module>\r\n     47 import numpy as np\r\n     48 \r\n---> 49 from tensorflow.python import pywrap_tensorflow\r\n     50 \r\n     51 # Protocol buffers\r\n\r\nC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py in <module>\r\n     72 for some common reasons and solutions.  Include the entire stack trace\r\n     73 above this error message when asking for help.\"\"\" % traceback.format_exc()\r\n---> 74   raise ImportError(msg)\r\n     75 \r\n     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Anaconda3\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nAny ideas?\r\nThank you-", "comments": ["From the template it looks like you are installing TF prebuilt binaries.\r\n\r\n **TensorFlow release binaries version 1.6 and above are prebuilt with AVX instruction sets.**\r\n Therefore on any CPU that does not have these instruction sets either CPU or GPU version of TF will fail to load.\r\n\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n\r\n  * Try Google Colab to use TensorFlow.\r\n     * The easiest way to use TF will be to switch to [google colab.](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true)You get pre-installed latest stable TF version. Also you can use pip install to install any other preferred TF version.\r\n     * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\r\n      * All you need is a good internet connection and you are all set.\r\n  * Try to build TF from sources by changing CPU optimization flags.\r\n\r\nPlease let us know if this helps.", "Thanks a lot! The problem is because of the AVX.\r\n", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 31032, "title": "timestamps in logs please", "body": "in TF 1.12 using the TF API i get nice timing info in the logs:\r\n\r\n```\r\nINFO:tensorflow:Elapsed 5976.983817, Step #10825: rate 0.000200, accuracy 96.9%, cross entropy 0.098157\r\n```\r\n\r\nthe `Elapsed ...` number therein is in seconds.\r\n\r\nin TF 2.0-beta using the keras API i sadly don't:\r\n\r\n```\r\n710/710 - 0s - loss: 0.3377 - val_loss: 0.5081\r\n```\r\n\r\nthe `0s` there seems to be the time for that step, not the cumulative elapsed time, as it never changes over the course of an hour long training run.\r\n\r\nat the very least it would be nice to have more significant digits on that `0s`.  better yet, would be elapsed time, or just a simple time stamp prefixed to each log entry.\r\n\r\ni've searched the docs, but haven't found a way to do this.  sorry if i missed it.\r\n\r\nthanks!", "comments": ["@bjarthur In the timestamp you could see `ETA` while running the code and after completing an epoch it shows how many seconds it took to complete that epoch and how much time it took per sample. For the example shown below, epoch 1 took 11 seconds to train and 176usec/sample. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/e752e51174143c5412389736696c222a/tf_31032.ipynb)\r\n\r\n```\r\n\r\nEpoch 1/5\r\n60000/60000 [==============================] - 11s 176us/sample - loss: 0.2202 - accuracy: 0.9352\r\nEpoch 2/5\r\n60000/60000 [==============================] - 10s 171us/sample - loss: 0.0969 - accuracy: 0.9699\r\n```\r\n\r\nIf you want overall elapsed time, please import time and get elapsed time as follows.\r\n\r\n```\r\nimport time\r\nst_time=time.time()\r\n## your model here\r\n## ops\r\ntime_elapsed=time.time()-st_time # this gives elapsed time in sec\r\n```\r\nI am closing this issue. Please feel free to open a new issue if you are looking any other feature. Thanks!", "except that as you can see in my example above, my epoch times are so short that they show up as `0s`.  how about some more significant digits?  surely that can't be hard to do."]}, {"number": 31031, "title": "Building tensorflow 1.14 (bazel 0.24.1) on Jetson Xavier with the trouble", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution \r\nLinux Ubuntu 18.04\r\n\r\n- TensorFlow installed from (source or binary):\r\nsource\r\n\r\n- TensorFlow version:\r\n1.14\r\n\r\n- Python version:\r\n3.6.8\r\n\r\n- Installed using virtualenv? pip? conda?:\r\npip3\r\n\r\n- Bazel version (if compiling from source):\r\nBuild label: 0.24.1- (@non-git)\r\nBuild target: bazel-out/aarch64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Jul 25 13:20:03 2019 (1564060803)\r\nBuild timestamp: 1564060803\r\nBuild timestamp as int: 1564060803\r\n\r\n- GCC/Compiler version (if compiling from source):\r\ngcc 5.5.0\r\n\r\n- CUDA/cuDNN version: \r\nCUDA 10.0\r\ncuDNN 7.5\r\n\r\n- GPU model and memory:\r\nNvidia Jetson AGX Xavier\r\n\r\n\r\n**Describe the problem**\r\nI try to install tensorflow 1.14 with next config:\r\n`bazel build --config=monolithic --jobs 8 -c opt --linkopt='-lrt' //tensorflow:libtensorflow_cc.so`\r\n\r\nBut i get an error\r\n```\r\nStarting local Bazel server and connecting to it...\r\nERROR: /root/tensorflow-1.14/WORKSPACE:94:1: Traceback (most recent call last):\r\n\tFile \"/root/tensorflow-1.14/WORKSPACE\", line 94\r\n\t\ttf_workspace()\r\n\tFile \"/root/tensorflow-1.14/tensorflow/workspace.bzl\", line 708, in tf_workspace\r\n\t\tnative.new_http_archive(name = \"double_conversion\", urls =...\"], <3 more arguments>)\r\ntype 'struct' has no method new_http_archive()\r\nERROR: Error evaluating WORKSPACE file\r\nERROR: Skipping '//tensorflow:libtensorflow_cc.so': error loading package 'external': Package 'external' contains errors\r\nWARNING: Target pattern parsing failed.\r\nERROR: error loading package 'external': Package 'external' contains errors\r\nINFO: Elapsed time: 5.360s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n```\r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem\r\n\r\n", "comments": ["Just to verify did you get chance to follow instructions from [TensorFlow](https://www.tensorflow.org/install/source) website .Please, let us know. Thanks!", "@suffererofautomation NVIDIA provides validated and optimized binaries for Jetson devices. Although currently, it is for TF 1.13.1. Take a look at: https://elinux.org/Jetson_Zoo#TensorFlow\r\n\r\nIf you want to build from source (assume you have a specific requirement for 1.14 and you cannot wait), check out [this thread](https://devtalk.nvidia.com/default/topic/1055131/jetson-agx-xavier/building-tensorflow-1-13-on-jetson-xavier/). Please consult the thread for a working procedure to build TF from source for Jetson Xavier.\r\n", "Redirecting to tf mobile team. ", "What version of bazel do you have installed?\r\n", "> What version of bazel do you have installed?\r\n\r\nNow i use:\r\nbazel 19.2\r\ntf 1.13\r\n\r\nThe configuration works perfectly.\r\nThe topic may be closed.", "I was able to get the c++ api compiled by following the link left by @tlkh. However the pip package will not build for tensorflow 1.14.0 and i get the same error as in [issues/28277](https://github.com/tensorflow/tensorflow/issues/28277#issuecomment-504783335)", "@suffererofautomation \r\nCan we close this issue since the query is been resolved. Thanks!"]}, {"number": 31030, "title": "tf.while loop behaviour in multigpu setting", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip binary?\r\n- TensorFlow version (use command below): 1.12\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\nI would like to verify if tf.while loop in multiple gpu setting is still an issue. I know that using while loop with variables in multigpu will give wrong result due to the while loop mixing up variables from different gpu under different name scope but same variable scope. I am not sure if this is still the case. Also if this concerns variables only or would similar behaviour happens to non-variables.", "comments": ["@colmantse Will it be possible to provide minimal code to reproduce the issue. Thanks!", "hi @gadagashwini , will work on it right away.", "@colmantse Any update on the code snippet. Thanks!", "hi sorry, I am still trying to reproduce the issue. maybe I should close for now and reopen when I manage.", "@gadagashwini  I managed to reproduce the problem, the code snippet is attached below\r\nIts a py file changed to txt.\r\n\r\n[whileTest.txt](https://github.com/tensorflow/tensorflow/files/3480801/whileTest.txt)\r\n", "Revision on the test script shows that it is instead an input problem and non-variable tf.while loop in multi gpu setting remains totally functional."]}, {"number": 31029, "title": "Will TF2 include implementations of constrained optimizers?", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.0.0-beta1\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI would like to use an optimizer with optional problem constraints. I am wondering if something like https://www.tensorflow.org/api_docs/python/tf/contrib/constrained_optimization/AdditiveExternalRegretOptimizer will be available in TF2?\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": ["TF 2.0 does not contain this feature. However, you can raise a request to add this feature in [tensorflow/addons](https://github.com/tensorflow/addons) repository.\r\nSee https://github.com/tensorflow/addons/tree/master/tensorflow_addons/optimizers to know more."]}, {"number": 31028, "title": "tensorflow.keras.layers.TimeDistributed seems to reset batch-size", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip\r\n- TensorFlow version (use command below):1.14.0\r\n- Python version:3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nTimeDistributed resets the batch_shape given in the Input layer.\r\nCould be that this is intended. If so, could you give me a hint how to run the example below? Would be really appreciated.\r\n**Describe the expected behavior**\r\nWhen using keras (not tensorflow.keras) the example below works fine\r\nSo, I expect that wen given the batchsize, TimeDistributed keeps the batchsize.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow.keras as keras\r\nkeras.backend.set_image_data_format('channels_first')\r\nresnet = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')\r\n\r\ninputShape = (1,None,3,224,224)\r\ninput = keras.layers.Input(batch_shape = inputShape)\r\nx = keras.layers.TimeDistributed(resnet)(input)\r\nx = keras.layers.TimeDistributed(keras.layers.Flatten())(x)\r\nx = keras.layers.LSTM(256, stateful=True)(x)\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nValueError: If a RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \r\n- If using a Sequential model, specify the batch size by passing a `batch_input_shape` argument to your first layer.\r\n- If using the functional API, specify the batch size by passing a `batch_shape` argument to your Input layer.\r\n```", "comments": ["I have tried on colab with TF version 1.14.0 was able to reproduce the issue.Please, find the [gist](https://colab.research.google.com/drive/1vNktBX2U5T6-KilL3zbBDz67FGA3dTnt) here.Thanks!", "@constiDisch TimeDistributed internally combines the batch and 2nd dimension into one dimension, then computes the computation of the time-distributed layer, that's why shape info is getting lost here. We can probably do better shape inference. As a workaround in the meantime, you can do a `set_shape` after the last TimeDistributed layer:\r\n\r\n```\r\ninputShape = (1,None,3,224,224)\r\ninput = keras.layers.Input(batch_shape = inputShape)\r\nx = keras.layers.TimeDistributed(resnet)(input)\r\nx = keras.layers.TimeDistributed(keras.layers.Flatten())(x)\r\nx.set_shape(<the shape with batch defined here>)\r\nx = keras.layers.LSTM(256, stateful=True)(x)\r\n```\r\n\r\n", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31028\">No</a>\n"]}, {"number": 31027, "title": "BatchNormalization axis=0 fails on batch dim changes.", "body": "Will close if this ends up to not be a bug. Posted on stackoverflow now and will follow up later.\r\n\r\nhttps://stackoverflow.com/questions/57202668/keras-batchnormalization-only-works-for-constant-batch-dim-when-axis-0", "comments": ["Probably axis meaning is the confusing bit and intention is not what axis to average over but what axis *not* to average over. Not sure if broadcast across other axes."]}, {"number": 31026, "title": "How to release keras compiled model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):1.14.0\r\n- Python version:3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nIt seems that tensorflow will increase memory usage when I compiled keras model serveral time.\r\nThere is some way to release unused compiled model \r\n**Describe the expected behavior**\r\nRelease compiled model when I don't need it anymore\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n``` python\r\nmodel.compile()\r\n\"\"\"freeze training\"\"\"\r\n\r\nmodel.compile()\r\n\"\"\"fine-tuning training\"\"\"\r\n```\r\nReplicate code: https://colab.research.google.com/drive/1GafCUt6BXjKY0LA3lc8fgGhDPwZ6Jgs3\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@fsx950223 Will it be possible to share the minimal code which replicates the reported issue. Thanks!", "> @fsx950223 Will it be possible to share the minimal code which replicates the reported issue. Thanks!\r\n\r\nI provided code above", "@fsx950223 Can you try `tf.reset_default_graph()` after importing tensorflow. I have noticed that memory was not increasing after adding `tf.reset_default_graph()`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/fa7eb1e000b42ac27f8a56a286d29f84/tf_31026_memory.ipynb). Thanks! ", "> @fsx950223 Can you try `tf.reset_default_graph()` after importing tensorflow. I have noticed that memory was not increasing after adding `tf.reset_default_graph()`. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/fa7eb1e000b42ac27f8a56a286d29f84/tf_31026_memory.ipynb). Thanks!\r\n\r\nI ran the gist and memory was still increasing after keras model was compiled and I can't release it ", "@fsx950223 Could throw little more details on the issue? How (sequence) did you ran the code. \r\n\r\nThe way i ran the model the gist is, first reset_graph, ran create model (0.6 GB of ram), then compiled, then ran training and found 0.85 GB was using.\r\n\r\nThen ran entire gist (reset_graph, create_model, compile, training) again and it was taking only 0.85 GB of ram. \r\n\r\nPlease share the sequence and any other details. Thanks!", "> @fsx950223 Could throw little more details on the issue? How (sequence) did you ran the code.\r\n> \r\n> The way i ran the model the gist is, first reset_graph, ran create model (0.6 GB of ram), then compiled, then ran training and found 0.85 GB was using.\r\n> \r\n> Then ran entire gist (reset_graph, create_model, compile, training) again and it was taking only 0.85 GB of ram.\r\n> \r\n> Please share the sequence and any other details. Thanks!\r\n\r\n``` python\r\nfor i in range(10):\r\n  model.compile(optimizer='adam',\r\n                loss='sparse_categorical_crossentropy',\r\n                metrics=['accuracy'])\r\n  #model.release() # Release memory\r\n  print(psutil.Process(os.getpid()).memory_info().rss) # You can see memory increase \r\n```\r\nThe problem is that memory will increase after I compiled keras model every time. I need some method to release compiled model and release memory.\r\n", "@fsx950223 I agree that memory is increasing. What is the reason behind compiling multiple times with same loss and optimizer? Thanks!", "> @fsx950223 I agree that memory is increasing. What is the reason behind compiling multiple times with same loss and optimizer? Thanks!\r\n\r\nMaybe I want to change trainable layers or distribute strategy that I need to compile it again. Although I can solve the problem by run the python script several times, it will cost more time to train the model.", "@fsx950223 When you change anything related to what you listed or anything related model, optimizer, and loss function, model need to be compiled and need to train the model again. If I am correct, it was adding only 1MB when you recompile the given example. Thanks!", "It's a example. In fact, it increased 8g memory in my project\r\n\r\nVishnuvardhan Janapati <notifications@github.com<mailto:notifications@github.com>> \u4e8e 2019\u5e748\u670823\u65e5\u5468\u4e94 \u4e0a\u53481:56\u5199\u9053\uff1a\r\n\r\n@fsx950223<https://github.com/fsx950223> When you change anything related to what you listed or anything related model, optimizer, and loss function, model need to be compiled and need to train the model again. If I am correct, it was adding only 1MB when you recompile the given example. Thanks!\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/31026?email_source=notifications&email_token=AEGHB47F2RRLA7I7BDFU4M3QF3HLDA5CNFSM4IGY4QRKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD453VLA#issuecomment-524008108>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AEGHB42HLKJP6HXCWDQZGVDQF3HLDANCNFSM4IGY4QRA>.\r\n", "@fsx950223 Can you provide an example with the more pathological behavior? As @jvishnuvardhan said, a small increase in memory consumption on recompile is expected.", "> @fsx950223 Can you provide an example with the more pathological behavior? As @jvishnuvardhan said, a small increase in memory consumption on recompile is expected.\r\n\r\nI have updated notebook. I just tested it with a small dataset, and large dataset will significantly improve memory usage.", "I am closing this issue as the increase is very minimal. Please feel free to reopen with more concrete example and a standalone code to reproduce the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31026\">No</a>\n"]}, {"number": 31025, "title": "Fix inconsistent default learning rate in tensorflow.keras.optimizers.Adadelta()", "body": "Fixing [#31024 issue](https://github.com/tensorflow/tensorflow/issues/31024) of inconsistencies between tensorflow.keras.optimizers.Adadelta() and keras.optimizers.Adadelta().", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31025) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31025) for more info**.\n\n<!-- ok -->", "Thanks for the PR! We have made a decision to make 0.001 as default learning rate for most of our optimizers, and put release notes regarding the changes.", "@tanzhenyu does that mean Keras is the one that needs to change their learning rate?", "Yeah I had a PR https://github.com/keras-team/keras/pull/12841 but was reverted due to some weird issues, still working on it.", "@tanzhenyu  Ah well, okay then. Just let me know if I can help you with anything. :)", "@tanzhenyu Does that mean the learning rate for Adadelta() on [this one](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizers.py) need to be changed to 0.001?"]}, {"number": 31024, "title": "tensorflow.keras.optimizers.Adadelta() Inconsistencies With keras.optimizers.Adadelta()", "body": "**System information**\r\n- OS Platform and Distribution: Linux-4.9.0-8-amd64-x86_64-with-debian-9.9 . \r\n- TensorFlow version: v1.14.0-9-gc407b045b8 1.14.0 . \r\n- Python version: 3.6.6 . \r\n\r\n**Describe the current behavior**  \r\nTake a look at the following two Kaggle kernel:  \r\n1. https://www.kaggle.com/ilhamfp31/keras-mnist  \r\n2. https://www.kaggle.com/ilhamfp31/tensorflow-keras-mnist\r\n\r\nBoth of them run the same code with the same OS, python version and TensorFlow version. The only difference is the first one importing from keras and the second one from tensorflow.keras. Optimizer Adadelta produces a different result because of the default configuration is not the same. The difference lies in the learning rate.\r\n\r\n`>> print(keras.optimizers.Adadelta().get_config())`\r\n`{'lr': 1.0, 'rho': 0.95, 'decay': 0.0, 'epsilon': 1e-07}`\r\n\r\n`>> print(tensorflow.keras.optimizers.Adadelta().get_config())`\r\n`{'name': 'Adadelta', 'learning_rate': 0.001, 'decay': 0.0, 'rho': 0.95, 'epsilon': 1e-07}`\r\n\r\n\r\n**Describe the expected behavior**\r\n`keras.optimizers.Adadelta()` and `tensorflow.keras.optimizers.Adadelta()` should produce the same result as shown by optimizer Adam. Looking at the source code, both Adam optimizer code in [optimizer.py](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizers.py) and [optimizer_v2/adam.py](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizer_v2/adam.py) default parameter is consistent with [keras optimizer code](https://github.com/keras-team/keras/blob/master/keras/optimizers.py). This is not true with Adadelta. While the code in [optimizer.py](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizers.py) is consistent, the code in [optimizer_v2/adadelta.py](https://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/keras/optimizer_v2/adadelta.py) is not. \r\n\r\n**Code to reproduce the issue**\r\nCode needed to reproduce the issue is available by downloading it from the given Kaggle kernel link. Simply click three grey dot on the upper right corner and click `Download code`.\r\n", "comments": ["I have created a [pull request](https://github.com/tensorflow/tensorflow/pull/31025) for this issue.", "Discussion on the [PR thread](https://github.com/tensorflow/tensorflow/pull/31025) addresses this issue. Hence closing it. Feel free to reopen if necessary. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31024\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31024\">No</a>\n"]}, {"number": 31023, "title": "patch-callbacks_test", "body": "The PR is minor to modify: locate a statement to exactly place with exception raised, remove `del` variable, and add message. ", "comments": ["Can one of the admins verify this patch?", "Thanks @rchao, it may be good to locate the exact statement where the exception is raiased; to limit the number of statements in exception raising test blocks to 1. ", "@rchao Can you please take a look on this PR? Thanks!", "@autoih Could you please check reviewer comments and keep us posted. Thanks!", "@autoih Can you please resolve conflicts? Thanks!"]}, {"number": 31022, "title": "Invalid result on some GPUs, probably einsum", "body": "**System information**\r\n- I have written custom code.\r\n- Linux Ubuntu 19.04, Debian 9.9\r\n- TensorFlow installed from binary\r\n- TensorFlow version 2.0.0-b1, 1.14, 1.15.0\r\n- Python version: 3.7.1, 3.7.3\r\n- CUDA/cuDNN version: 10.0, 10.1\r\n- GPU model and memory: Tesla T4, Tesla P4, P100, does not exist on k80\r\n\r\n**Describe the current behavior**\r\n\r\nThe result on the GPU is significantly different from CPU. It is correct on CPU, but wrong on GPU. The difference on GPU is much bigger than zero.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe print out of the difference should be zero.\r\n\r\n**Code to reproduce the issue**\r\n\r\n**tf 2.0.0-b1**\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\n\r\n@tf.function\r\ndef sample_y_nn_tf(w, X_sample, config):\r\n\r\n        layers = config['layers']\r\n        current_w_index = 0\r\n        h = X_sample\r\n        w_shape = tf.shape(input=w)\r\n\r\n        l = layers[0]\r\n        w_current = tf.slice(w, begin=[0,0,0,0],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])\r\n        w_current = tf.reshape(w_current, [config['batch_size'], w_shape[1], w_shape[2]] + list(l))\r\n        b_current = tf.slice(w, begin=[0,0,0,l[0] * l[1]-1],size=[w_shape[0], w_shape[1], w_shape[2], l[1]])\r\n        h = tf.einsum('lm,ikjmn->likjn', h, w_current) + b_current\r\n        current_w_index = current_w_index + (l[0] + 1) * l[1]\r\n\r\n        for l in layers[1:]:\r\n            h = tf.nn.elu(h)\r\n            w_current = tf.slice(w, begin=[0,0,0,current_w_index],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])\r\n            w_current = tf.reshape(w_current, [config['batch_size'],w_shape[1], w_shape[2]]+list(l))\r\n            b_current = tf.slice(w, begin=[0, 0, 0,current_w_index + l[0] * l[1] - 1], size=[w_shape[0], w_shape[1], w_shape[2], l[1]])\r\n            h = tf.einsum('likjm,ikjmn->likjn',h,w_current)+b_current\r\n            current_w_index = current_w_index+(l[0]+1)*l[1]\r\n\r\n        h = tf.squeeze(h, axis=4)\r\n\r\n        return h\r\n\r\nconfig = {'batch_size': 8, 'layers': [[2,2],[2,1]]}\r\n\r\nnp.random.seed(111)\r\n\r\nw = np.random.normal(size=(config['batch_size'],64,512,13)).astype(dtype=np.float32)\r\nx_sample = np.random.normal(size=(100,2)).astype(dtype=np.float32)\r\n\r\npermutation = tf.random.shuffle(tf.range(config['batch_size'], dtype=tf.int32), seed=111)\r\n\r\nprint('permutation', permutation)\r\n\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)\r\n    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)\r\n    print('difference on cpu', np.sum(np.abs(tf.gather(means,permutation, axis=1)-means_p)))\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)\r\n    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)\r\n    print('difference on gpu', np.sum(np.abs(tf.gather(means,permutation, axis=1)-means_p)))\r\n\r\n\r\n```\r\n\r\nOutput:\r\n\r\n> difference on cpu 0.0\r\n> difference on gpu 37953784.0\r\n\r\n**tf.1.14**\r\n\r\n```\r\nfrom tensorflow.python.framework.versions import VERSION\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\nprint(tf.__version__)\r\n\r\n@tf.function\r\ndef sample_y_nn_tf(w, X_sample, config):\r\n    \r\n        layers = config['layers']\r\n        current_w_index = 0\r\n        h = X_sample\r\n        w_shape = tf.shape(input=w)\r\n        \r\n        l = layers[0]\r\n        w_current = tf.slice(w, begin=[0,0,0,0],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])\r\n        w_current = tf.reshape(w_current, [config['batch_size'], w_shape[1], w_shape[2]] + list(l))\r\n        b_current = tf.slice(w, begin=[0,0,0,l[0] * l[1]-1],size=[w_shape[0], w_shape[1], w_shape[2], l[1]])\r\n        h = tf.einsum('lm,ikjmn->likjn', h, w_current) + b_current\r\n        current_w_index = current_w_index + (l[0] + 1) * l[1]\r\n        \r\n        for l in layers[1:]:\r\n            h = tf.nn.elu(h)\r\n            w_current = tf.slice(w, begin=[0,0,0,current_w_index],size=[w_shape[0], w_shape[1], w_shape[2], l[0] * l[1]])\r\n            w_current = tf.reshape(w_current, [config['batch_size'],w_shape[1], w_shape[2]]+list(l))\r\n            b_current = tf.slice(w, begin=[0, 0, 0,current_w_index + l[0] * l[1] - 1], size=[w_shape[0], w_shape[1], w_shape[2], l[1]])\r\n            h = tf.einsum('likjm,ikjmn->likjn',h,w_current)+b_current\r\n            current_w_index = current_w_index+(l[0]+1)*l[1]\r\n    \r\n        h = tf.squeeze(h, axis=4)\r\n\r\n        return h\r\n\r\nconfig = {'batch_size': 8, 'layers': [[2,2],[2,1]]}\r\n\r\nnp.random.seed(111)\r\n\r\nw = np.random.normal(size=(config['batch_size'],64,512,13)).astype(dtype=np.float32)\r\nprecision = np.exp(np.random.normal(size=(config['batch_size'],64,512))).astype(dtype=np.float32)\r\nx_sample = np.random.normal(size=(100,2)).astype(dtype=np.float32)\r\n\r\npermutation = tf.random.shuffle(tf.range(config['batch_size'], dtype=tf.int32), seed=111)\r\n\r\nconf = tf.ConfigProto()\r\nconf.gpu_options.allow_growth = True\r\nsess = tf.Session(config=conf)\r\n\r\n\r\nprint('permutation', sess.run(permutation))\r\n\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)\r\n    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)\r\n    net_cpu = tf.gather(means,permutation, axis=1)-means_p\r\n\r\nwith tf.device(\"/device:GPU:0\"):\r\n    means = sample_y_nn_tf(w=w, X_sample=x_sample, config=config)\r\n    means_p = sample_y_nn_tf( w = tf.gather(w, permutation, axis=0), X_sample=x_sample, config=config)\r\n    net_gpu = tf.gather(means,permutation, axis=1)-means_p\r\n\r\nnet_cpu_result = sess.run(net_cpu)\r\nnet_gpu_result = sess.run(net_gpu)\r\nprint(\"tensorflow version: {}\".format(VERSION))\r\nprint('difference on cpu', np.sum(np.abs(net_cpu_result)))\r\nprint('difference on gpu', np.sum(np.abs(net_gpu_result)))\r\n\r\n```\r\nOutput:\r\n\r\n> difference on cpu 0.0\r\n> difference on gpu 39042744.0\r\n\r\n**Other info / logs**\r\n\r\nI did not check the tf2 version on K80, P100.\r\nThe bug probably persists when using Pythonic slicing operations.", "comments": ["I have tried on colab with TF version 2.0 beta1,1.14 and was able to reproduce the issue.Please, find the [gist ](https://colab.research.google.com/drive/117O0l7XLQeFAIf1cr4PbNOzSsjUtkKHY)here.Thanks!", "Thanks. I investigated a bit more. The bug does not occur when using only one layer 'layers': [[2,1]]. I think the minimal configuration is config = {'batch_size': 2, 'layers': [[1,1],[1,1]]}.\r\n  ", "i also reproduced the bug on geforce gtx 1050 ti ", "I can reproduce the problem with TF 2.0.\r\n@sanjoy, could you help to take a look? ", "Duplicate of https://github.com/tensorflow/tensorflow/issues/31166\r\n\r\nDoes not reproduce with tf-nightly that uses CUDA 10.1", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31022\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31022\">No</a>\n"]}, {"number": 31021, "title": "estimator.predcit load model every time", "body": "estimator.predict() works to slowly. I want to predict some text in my model in every 2 seconds. Everytime I call the estimator.predict() function, it loads the model all over again. I want to load the model just once and after that use estimator.predict() every 2 seconds on this same model to get the faster prediction.\r\n", "comments": ["same problem, but using cache session and session.run to predict could solve it. ", "Perhaps multi-threaded prediction can help you achieve this.\r\nSee https://medium.com/element-ai-research-lab/multithreaded-predictions-with-tensorflow-estimators-eb041861da07"]}, {"number": 31020, "title": "Symbolic Links Aren't Created When Needed Using TF_SYSTEM_LIBS", "body": "**System information**\r\n- OS: FreeBSD\r\n- Building from source \r\n- TensorFlow version: V1.14.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):  0.28\r\n- GCC/Compiler version (if compiling from source): Clang 6.0.1\r\n\r\n**Describe the problem**\r\n\r\nWhen using _TF_SYSTEM_LIBS_ to build Tensorflow from source. It tries to include files which it has yet to create. In the file third_party/systemlibs/protobuf.BUILD. It creates symbolic links to the header files when building with _TF_SYSTEM_LIBS_. However it tries to compile code which relies on these headers before these headers have been created. If I continue the build without cleaning the build continues as normal. \r\n\r\nSetting jobs to 1 even causes this problem. As the files either haven't been created or haven't finished creating them. I can't tell if this is an issue with Bazel or Tensorflow, but it's either not waiting for the sub command to complete and create these files, or when using _TF_SYSTEM_LIBS_ it is compiling code which relies on these before they're created.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n**Compile Tensorflow with the following _TF_SYSTEM_LIBS:_**\r\n\r\n```\r\nabsl_py astor_archive \r\nboringssl\r\ncom_github_googleapis_googleapis\r\ncom_github_googlecloudplatform_google_cloud_cpp\r\ncom_google_protobuf\r\ncom_google_protobuf_cc\r\ncom_googlesource_code_re2\r\ncurl\r\ncython\r\ndouble_conversion\r\nenum34_archive\r\nflatbuffers\r\ngast_archive\r\ngif_archive grpc\r\nhwloc\r\nicu\r\njpeg\r\njsoncpp_git\r\nkeras_applications_archive\r\nlmdb\r\nnasm\r\nnsync org_sqlite\r\npasta\r\npcre\r\npng_archive\r\nprotobuf_archive\r\nsix_archive\r\nsnappy\r\nswig\r\ntermcolor_archive\r\nwrapt\r\nzlib_archive\"\r\n```\r\n\r\n**Additional enviroment variables for building:**\r\n\r\n```\r\nPYTHON_BIN_PATH=${PYTHON_CMD}\"\r\nPYTHON_LIB_PATH=\"${PYTHON_SITELIBDIR}\"\r\nTF_NEED_JEMALLOC=0\r\nTF_NEED_KAFKA=0\r\nTF_NEED_OPENCL_SYCL=0\r\nTF_NEED_AWS=0\r\nTF_NEED_GCP=0\r\nTF_NEED_HDFS=0\r\nTF_NEED_S3=0\r\nTF_ENABLE_XLA=0\r\nTF_NEED_GDR=0\r\nTF_NEED_VERBS=0\r\nTF_NEED_OPENCL=0\r\nTF_NEED_MPI=0\r\nTF_NEED_TENSORRT=0\r\nTF_NEED_NGRAPH=0\r\nTF_NEED_IGNITE=0\r\nTF_NEED_ROCM=0\r\nTF_NEED_CUDA=0\r\nTF_SET_ANDROID_WORKSPACE=0\r\nTF_DOWNLOAD_CLANG=0\r\nTF_NEED_NCCL=0\r\nTF_NEED_OPENCL=0\r\nTF_IGNORE_MAX_BAZEL_VERSION=1\r\nCC_OPT_FLAGS=\"-march=${CPU_TARGET}\"\r\nPREFIX=\"${LOCALBASE}\"\r\n```\r\n\r\n**Using the follow bazelrc file:**\r\n\r\n```\r\nstartup --batch\r\n\r\nbuild --strip=never\r\nbuild --verbose_failures --noshow_loading_progress\r\ntest --verbose_test_summary --verbose_failures --noshow_loading_progress\r\n\r\nbuild --spawn_strategy=local --genrule_strategy=local\r\ntest --spawn_strategy=local --genrule_strategy=local\r\n\r\nfetch --repository_cache=\"%%BAZEL_DIR%%/bazel-cache/\" --distdir=\"%%BAZEL_DIST%%/bazel-distdir/\"\r\nbuild --repository_cache=\"%%BAZEL_DIR%%/bazel-cache/\" --distdir=\"%%BAZEL_DIST%%/bazel-distdir/\"\r\n\r\nbuild --define=PREFIX=%%LOCALBASE%%\r\nbuild --define=LIBDIR=%%LOCALBASE%%/lib\r\n\r\nbuild --config=noaws --config=nohdfs --config=noignite --config=nokafka\r\n```\r\n\r\n** Build Command Used**\r\n\r\n```\r\nbazel --bazelrc=\"${WRKDIR}/bazelrc\" ${BAZEL_BOOT} build ${BAZEL_COPT} --host_copt=\"-I${LOCALBASE}/include\" \\\r\n\t\t--host_linkopt=\"-L${LOCALBASE}/lib\" --linkopt=\"-L${LOCALBASE}/lib\" --config=opt \\\r\n\t\t--incompatible_no_support_tools_in_action_inputs=false \\\r\n\t\t--verbose_failures -s \\\r\n\t\t//tensorflow:libtensorflow.so \\\r\n\t\t//tensorflow:libtensorflow_cc.so \\\r\n\t\t//tensorflow:install_headers \\\r\n\t\t//tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n**Any other info / logs**\r\n\r\nCompilation log:\r\n\r\n\r\n```\r\nSUBCOMMAND: # //tensorflow/core:conv_autotuning_proto_cc_genproto [action 'ProtoCompile tensorflow/core/protobuf/conv_autotuning\r\n.pb.cc [for host]']\r\n(cd /usr/home/Amzo/FreeBSD-Tensorflow/science/py-tensorflow/work-py36/bazel_out/f19d4b0a99d0cb4c4a90e3adec2fef45/execroot/org_te\r\nnsorflow && \\\r\n  exec env - \\\r\n    PATH=/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/sbin:/usr/local/bin:/home/Amzo/bin \\\r\n  bazel-out/host/bin/external/protobuf_archive/protoc.bin '--cpp_out=bazel-out/host/bin/' -I. -I. -Iexternal/protobuf_archive -I\r\nbazel-out/host/bin/external/protobuf_archive -Iexternal/protobuf_archive -Ibazel-out/host/bin/external/protobuf_archive tensorfl\r\now/core/protobuf/conv_autotuning.proto)\r\nSUBCOMMAND: # //tensorflow/core:autotuning_proto_cc_genproto [action 'ProtoCompile tensorflow/core/protobuf/autotuning.pb.cc [fo\r\nr host]']\r\n(cd /usr/home/Amzo/FreeBSD-Tensorflow/science/py-tensorflow/work-py36/bazel_out/f19d4b0a99d0cb4c4a90e3adec2fef45/execroot/org_te\r\nnsorflow && \\\r\n  exec env - \\\r\n    PATH=/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/sbin:/usr/local/bin:/home/Amzo/bin \\\r\n  bazel-out/host/bin/external/protobuf_archive/protoc.bin '--cpp_out=bazel-out/host/bin/' -I. -Iexternal/protobuf_archive -Ibaze\r\nl-out/host/bin/external/protobuf_archive tensorflow/core/protobuf/autotuning.proto)\r\nERROR: /usr/home/Amzo/FreeBSD-Tensorflow/science/py-tensorflow/work-py36/tensorflow-1.14.0/tensorflow/core/BUILD:2255:1: ProtoCo\r\nmpile tensorflow/core/protobuf/autotuning.pb.cc failed (Exit 1)\r\ngoogle/protobuf/any.proto: File not found.\r\ngoogle/protobuf/duration.proto: File not found.\r\ntensorflow/core/protobuf/autotuning.proto:10:1: Import \"google/protobuf/any.proto\" was not found or had errors.\r\ntensorflow/core/protobuf/autotuning.proto:11:1: Import \"google/protobuf/duration.proto\" was not found or had errors.\r\ntensorflow/core/protobuf/autotuning.proto:52:3: \"google.protobuf.Duration\" is not defined.\r\n```\r\n\r\nWhile I know FreeBSD isn't a supported platform, I just need help finding the culprit to this issue. As the problem only occasionally occurs. If I create a new jail and build, then sometimes it will succeed and sometimes it will fail due to trying to compile source before the proto headers have been linked. This is just based on luck as to whether or not another job has created them.\r\n\r\n", "comments": ["@perfinion as he helped set up `TF_SYSTEM_LIBS`", "I've just worked around the issue for now and patched the bzl file to just use the include files directly from ${LOCALBASE}/includes instead of just creating symlinks to the build directory. ", "I just ran into the same issue: `TF_SYSTEM_LIBS=com_google_protobuf` set but compile throws a \"dangling symbolic link\" to the protobuf headers. I was using the 2.0.0 tag", "Actually it is not the same issue. The link is indeed dangling because TF uses `$(INCLUDEDIR)`. We have protobuf installed to a different prefix as other software (think `/opt/protobuf-x.y.z`) to isolate it properly. So what we need is something like `TF_PROTOBUF_PREFIX` to set this to the folder containing the `include`, `bin` etc folders.", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31020\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31020\">No</a>\n", "@Flamefire was this ever fixed? I have it on Tensorflow 2.5.0", "@surak What exactly do you have? TF now uses another variable for the protobuf headers: https://github.com/tensorflow/tensorflow/blob/c313c925f4d525aef40914413299706074fb51f8/third_party/systemlibs/protobuf.BUILD#L57\r\n\r\nThis works so far in EasyBuild"]}, {"number": 31019, "title": "Are tf.TFRecord(num_parallel_reads) and interleave(num_parallel_calls) duplicated?", "body": "Are tf.TFRecord(num_parallel_reads) and interleave(num_parallel_calls) duplicated? what is the behavior if they are both set > 1?  And the difference of shuffle before repeat and shuffle after repeat? I think the design of Dataset is too complicated, we need a more clear approach.", "comments": ["From documentation,  ```tf.TFRecord``` and ```interleave``` do same operation. However ```interleave``` provides more options (map_function, block_length) to optimize the performance. ", "no, not at the moment `TFRecord` uses a legacy version of parallel interleaving that does not support autotuning", "So is it `interleave` the recommended way to parallelize (instead of `TFRecordDataset`)?", "After https://github.com/tensorflow/tensorflow/commit/a4030f232072199046888efa95ae82c51959a07b#diff-887ec34979932479f014a211bf634614 `TFRecordDataset` supports autotuning as well.\r\n\r\nMy recommendation would be to use `TFRecordDataset` and fall back to using `interleave` for advanced use cases."]}, {"number": 31018, "title": "did 2.0beta1 version support cuda 10.1", "body": "i installed it by\r\n`C:\\Python37\\Scripts>pip install tensorflow-gpu==2.0.0-beta1`\r\nit is done ok\r\nbut when i run `import tensorflow as tf` in python,\r\nit reports\r\n`ImportError: Could not find 'cudart64_100.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 10.0 from this URL: https://developer.nvidia.com/cuda`\r\nindeed i have installed cuda 10.1\r\ni found the file `cudart64_101.dll` at `C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1\\bin`, and it is in PATH\r\n", "comments": ["No, it does not (see the [documentation](https://www.tensorflow.org/install/gpu)). You therefore need to use CUDA 10.0.\r\n\r\nNo offence, but this question has been repeatedly asked before - please search closed issues before posting (although it is for sure better to ask than to remain without an answer) :-)", "Closing since its resolved. Thanks!"]}, {"number": 31017, "title": "Fix incorrect usage of execution plan in GPU delegate", "body": "When checking supported ops, instead of using the node id values from the execution plan, the delegate was just using node ids 0..execution_plan.size. In a case where your graph has 20 nodes, and your execution plan covers nodes 5-20, this would instead build a subgraph out of nodes 0-15.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31017) for more info**.\n\n<!-- need_sender_cla -->", "Er... need to make the commit with a different email."]}, {"number": 31016, "title": "when using tf.train.exponential_decay , how can i  stop learning_rate changing during test", "body": "is there a simple example?", "comments": ["This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!", "I am closing issue as stackoverflow is better place for this kind of support questions. thanks!"]}, {"number": 31015, "title": "Failed to convert an SSD mobilenet model to a frozen graph with 4 outputs by using export_tflite_ssd_graph.py", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary; with conda environment\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7.6.0\r\n- GPU model and memory: GeForce GTX 970, AMD64\r\n\r\n**Describe the current behavior**\r\nI get a frozen model with only two outputs (raw_outputs/box_encodings, raw_outputs/class_predictions) when using export_tflite_ssd_graph.py with the extra attribute `add_postprocessing_op True`. I tried this script on these models from model zoo: ssdlite_mobilenet_v2_coco_2018_05_09, ssd_mobilenet_v2_coco_2018_03_29, ssd_mobilenet_v2_oid_v4_2018_12_12\r\n\r\n**Describe the expected behavior**\r\nI should get a frozen model with four outputs (detection_boxes, detection_classes, detection_scores, num_boxes) when using `add_postprocessing_op True`.\r\n\r\n**Code to reproduce the issue**\r\n`Cd .\\ssdlite_mobilenet_v2_coco_2018_05_0`\r\n`Python .\\models\\research\\object_detection\\export_tflite_ssd_graph.py --pipeline_config_path pipeline.config --trained_checkpoint_prefix model.ckpt --output_directory my_frozen_graph --add_postprocessing_op True`\r\n\r\n**Other info / logs**\r\nI know the outputs of the frozen graphs because I visualized them and tflite_convert worked successfully with this code: `tflite_convert --graph_def_file=my_frozen_graph/tflite_graph.pb --output_file=detect_raw.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor  --output_arrays=raw_outputs/box_encodings,raw_outputs/class_predictions`\r\n\r\nMy goal in general is to convert an ssd model from model zoo to a tflite float model with the expected four outputs. If there is another way to achieve this, I'm happy to hear about it. I've already tried to convert the frozen_inference_graph.pb (provided by model zoo) with tflite_convert but it didn't work. I got errors like:\r\n `Check failed: other_op->type == OperatorType::kMerge Found Sub as non-selected output from Switch, but only Merge supported.` (ssdlite_mobilenet_v2_coco_2018_05_09) or \r\n`Check failed: dim_size >= 1 (0 vs. 1)` (ssd_mobilenet_v2_coco_2018_03_29, ssd_mobilenet_v2_oid_v4_2018_12_12).\r\nSearching for solving this problem, I read that some ops are missing for tflite and it is recommended to freeze the graph with export_tflite_ssd_graph.py instead and then convert it to tflite. This was successful but only if you want to work with the raw outputs.\r\n\r\nThanks for helping!", "comments": ["So the model converted using `export_tflite_ssd_graph.py` is not intended to be the final model to be used by TFLite.\r\nHeres how it works:\r\n\r\n1. The model zoo `.pb` files are the original TF graphs. They contain control flow ops at the end(`Merge`, `Enter`, etc), to perform NMS. TFLite doesn't have full fledged support for control flow ops yet. (Hence you can't directly call `tflite_convert` on the original version).\r\n\r\n2. The  `export_tflite_ssd_graph.py` script is used to take this original TF graph, and convert it into an 'intermediate' form that removes all the control-flow logic, and leaves the graph with raw outputs that can be directly fed to a custom TFLite op (thats specifically designed for NMS). However, this graph is an intermediate, and not really the final TFLite version.\r\n\r\n3. Then, you use `tflite_convert` to convert this intermediate representation into the final TFLite graph, which should succeed. What we do at this stage, is convert the intermediate graph, and append the post-processing op to it. This model should have the 4 outputs you want.\r\n\r\nWithout running `tflite_convert`, the output of  `export_tflite_ssd_graph.py` is meaningless for on-device inference. I guess the parameters on the export script weren't named very well, which misled you into thinking that the intermediate graph (after step 2) should contain 4 inputs.", "Thanks for the explanation! Now I understand.\r\n\r\nBecause I\u2019ve struggled a little bit finding out how to convert a ssd model. This is one way to do it from the command line:\r\n1.\tYou have TF and TF object detection installed\r\n2.\tDownloading and unpacking the file (the model) from model zoo -> cd into the folder (e.g., `cd \u2026/ssdlite_mobilenet_v2_coco_2018_05_09`)\r\n3.\t`python .\\models\\research\\object_detection\\export_tflite_ssd_graph.py --pipeline_config_path pipeline.config --trained_checkpoint_prefix model.ckpt --output_directory my_frozen_graph --add_postprocessing_op True`\r\n4.\t`tflite_convert --graph_def_file=my_frozen_graph/tflite_graph.pb --output_file=detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor  --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --allow_custom_ops`\r\n\r\nYou get a tflite model (float) with 4 outputs. The output_arrays correspond to \u201cdetection_boxes, detection_classes, detection_scores, num_boxes\u201d.\r\n", "Yup, that is it. Is it failing for you at any step?\r\nOr do you want some help figuring out what the outputs contain?", "Everything is working, thanks for asking!\r\nI just wanted to share my gained knowledge, so other people can get to the solution faster.", "> Thanks for the explanation! Now I understand.\r\n> \r\n> Because I\u2019ve struggled a little bit finding out how to convert a ssd model. This is one way to do it from the command line:\r\n> \r\n> 1. You have TF and TF object detection installed\r\n> 2. Downloading and unpacking the file (the model) from model zoo -> cd into the folder (e.g., `cd \u2026/ssdlite_mobilenet_v2_coco_2018_05_09`)\r\n> 3. `python .\\models\\research\\object_detection\\export_tflite_ssd_graph.py --pipeline_config_path pipeline.config --trained_checkpoint_prefix model.ckpt --output_directory my_frozen_graph --add_postprocessing_op True`\r\n> 4. `tflite_convert --graph_def_file=my_frozen_graph/tflite_graph.pb --output_file=detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor  --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --allow_custom_ops`\r\n> \r\n> You get a tflite model (float) with 4 outputs. The output_arrays correspond to \u201cdetection_boxes, detection_classes, detection_scores, num_boxes\u201d.\r\n\r\nsolved.", "> Thanks for the explanation! Now I understand.\r\n> \r\n> Because I\u2019ve struggled a little bit finding out how to convert a ssd model. This is one way to do it from the command line:\r\n> \r\n> 1. You have TF and TF object detection installed\r\n> 2. Downloading and unpacking the file (the model) from model zoo -> cd into the folder (e.g., `cd \u2026/ssdlite_mobilenet_v2_coco_2018_05_09`)\r\n> 3. `python .\\models\\research\\object_detection\\export_tflite_ssd_graph.py --pipeline_config_path pipeline.config --trained_checkpoint_prefix model.ckpt --output_directory my_frozen_graph --add_postprocessing_op True`\r\n> 4. `tflite_convert --graph_def_file=my_frozen_graph/tflite_graph.pb --output_file=detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor  --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --allow_custom_ops`\r\n> \r\n> You get a tflite model (float) with 4 outputs. The output_arrays correspond to \u201cdetection_boxes, detection_classes, detection_scores, num_boxes\u201d.\r\n\r\n@val9299 When I am running this model which is converted like this in android, I am getting array of 10 elements with 4 inner elements each, i.e zzasu[0][0][0] = [0.4893098, 0.83000225,0.5201194,0.8918484]; Now I don't understand how these values are corresponding to \"\u201cdetection_boxes, detection_classes, detection_scores, num_boxes\u201d\". Kindly help me with some links to understand.", "> > Thanks for the explanation! Now I understand.\r\n> > Because I\u2019ve struggled a little bit finding out how to convert a ssd model. This is one way to do it from the command line:\r\n> > \r\n> > 1. You have TF and TF object detection installed\r\n> > 2. Downloading and unpacking the file (the model) from model zoo -> cd into the folder (e.g., `cd \u2026/ssdlite_mobilenet_v2_coco_2018_05_09`)\r\n> > 3. `python .\\models\\research\\object_detection\\export_tflite_ssd_graph.py --pipeline_config_path pipeline.config --trained_checkpoint_prefix model.ckpt --output_directory my_frozen_graph --add_postprocessing_op True`\r\n> > 4. `tflite_convert --graph_def_file=my_frozen_graph/tflite_graph.pb --output_file=detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor  --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 --allow_custom_ops`\r\n> > \r\n> > You get a tflite model (float) with 4 outputs. The output_arrays correspond to \u201cdetection_boxes, detection_classes, detection_scores, num_boxes\u201d.\r\n> \r\n> @val9299 When I am running this model which is converted like this in android, I am getting array of 10 elements with 4 inner elements each, i.e zzasu[0][0][0] = [0.4893098, 0.83000225,0.5201194,0.8918484]; Now I don't understand how these values are corresponding to \"\u201cdetection_boxes, detection_classes, detection_scores, num_boxes\u201d\". Kindly help me with some links to understand.\r\n\r\n@rohittayal595: I'm not longer working on it. The output is explained here: https://www.tensorflow.org/lite/models/object_detection/overview#output "]}, {"number": 31014, "title": "ConvLSTM2D hidden states aren't updated with Session.run", "body": "Hey guys,\r\n\r\nI have a problem when I use a Keras ConvLSTM2D layer in my TensorFlow graph. When I compute the output of the graph using `session.run`, the hidden states of the ConvLSTM2D won't be updated. Here is an minimal example to reproduce the behavior:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import ConvLSTM2D\r\n\r\n# create input data\r\nshape = [1, 1, 3, 3, 2]\r\ninput_tensor = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\r\n\r\n# create and build layer\r\nconv_lstm = ConvLSTM2D(filters=1,\r\n                       kernel_size=(2, 2),\r\n                       padding='same',\r\n                       return_sequences=True,\r\n                       stateful=True)\r\nconv_lstm.build(shape)\r\n\r\n# deploy layer and get its hidden states\r\nconv_lstm_out = conv_lstm(input_tensor)\r\nhidden_states = conv_lstm.states\r\n\r\n# create and initialise session\r\nsess = tf.compat.v1.Session()\r\nsess.run(tf.compat.v1.global_variables_initializer())\r\n\r\n# run op and print results multiple times\r\n# Remark: output should change, because hidden states should be updated, but they won't\r\nfor _ in range(3):\r\n    output, states = sess.run([conv_lstm_out, hidden_states])\r\n    print(output.shape, output)\r\n    print(states[0].shape, states[0])\r\n    print(states[1].shape, states[1])\r\n    print()\r\n```\r\n\r\nOS Platform and Distribution: Windows 10\r\nTensorFlow installed from: pip\r\nTensorFlow version: 1.14.0\r\nPython version: 3.6\r\n\r\nDoes anyone knows a fix to this? Is this a bug in TensorFlow/Keras itself or in my code?\r\n", "comments": ["@baiztencale \r\n\r\nI tried on cloab with TF 1.14.0 and able to see different results after multiple executions.Can you cross check again and confirm.Thanks!", "I checked it again and I'm still getting the same outputs during the execution loop. Furthermore are all states of the layer filled with zeros.\r\n\r\nBy the way, I'm using the CPU version of TensorFlow without any GPU.", "@baiztencale I cannot reproduce the issue. Please check the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/b2220efcb17b4e40f8cb91f7d4753937/tf_31014_lstm.ipynb). This might have been resolved recently in `tf-nightly`. Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31014\">No</a>\n", "I finally found the reason for this behaviour. The Keras API handels update ops as part of its fit, evaluate and predict functions. Because of this, it won't update and evaluate the `tf.compat.v1.GraphKeys.UPDATE_OPS` collection without them. So in order to make it work, we need to update and evaluate  it manually.\r\n\r\nThe updated code looks as follows:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import ConvLSTM2D\r\n\r\n# create input data\r\nshape = [1, 1, 3, 3, 2]\r\ninput_tensor = np.arange(np.prod(shape), dtype=np.float32).reshape(shape)\r\n\r\n# create and build layer\r\nconv_lstm = ConvLSTM2D(filters=1,\r\n                       kernel_size=(2, 2),\r\n                       padding='same',\r\n                       return_sequences=True,\r\n                       stateful=True)\r\n#conv_lstm.build(shape)  # unnecessary\r\n\r\n# deploy layer\r\nconv_lstm_out = conv_lstm(input_tensor)\r\n    \r\n# update collection and ensure by using control dependencies that lstm states are updated\r\nfor op in conv_lstm.updates:\r\n    tf.compat.v1.add_to_collection(tf.compat.v1.GraphKeys.UPDATE_OPS, op)\r\nupdate_ops = tf.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    conv_lstm_out = tf.identity(conv_lstm_out)\r\n    hidden_states = conv_lstm.states\r\n\r\n# create and initialise session\r\nsess = tf.compat.v1.Session()\r\nsess.run(tf.compat.v1.global_variables_initializer())\r\n\r\n# run op and print results multiple times\r\nfor _ in range(3):\r\n    output, states = sess.run([conv_lstm_out, hidden_states])\r\n    print(output.shape, output)\r\n    print(states[0].shape, states[0])\r\n    print(states[1].shape, states[1])\r\n    print()\r\n```"]}, {"number": 31013, "title": "[TF 2.0 Docs] Include @tf.function in site/en/r2/tutorials/generative/cvae.ipynb", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nsite/en/r2/tutorials/generative/cvae.ipynb\r\n\r\n## Description of issue (what needs changing): implement @tf.function decorators in the computation to improve performance, and highlight one of the tf 2.0 features.\r\n\r\n### Clear description\r\nWhen implementing in colab the performance improves from 30s / epoch average to 3.6s /epoch which is a huge benefit, and well worth highlighting/recommending by adding only 4 lines of code.\r\n\r\n### Submit a pull request?\r\nI can do that, yes\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Pull request submitted", "Pull request is submitted, approved, and merged. Website docs just need to be updated to include recent code merge.", "Automatically closing this out since I understand it to be resolved by the PR https://github.com/tensorflow/docs/pull/871. Thanks for the contribution."]}, {"number": 31012, "title": "Opt out DEVICE_GPU_XLA_JIT and DEVICE_XLA_GPU from ResizeNearestNeigh\u2026", "body": "@sanjoy, this is the follow-up of our discussion.  This PR is mainly for discussion purpose at this moment.  I am also holding merging for PR (https://github.com/tensorflow/tensorflow/pull/30336), as it may not be necessary, if we decide to opt out GPU devices. \r\n \r\nDilation-based resizing may create huge convolution for CuDNN.  For example: nearest neighbor resizing from 16x256x256x16 to 16x512x512x16.  The sizes are reasonable.\r\nThe following transforms create giant convolutions:\r\n1. TF2XLA elaborates nearest neighbor resizing into two 1-D convolutions with window size 1021x1, but requires dilation on feature by 511 (not dilation on kernel), i.e.\r\n{ conv(16x256x256x16, 1x1021x1x16, dilation on feature by 511 in dim0) -> conv(16x512x256x16, 1x1x1021x16, dilation on feature by 511 in dim1) }\r\n2. cudnn_conv_padding_legalization sees the feature dilation.  It removes the feature dilation from the conv and adds physical padding to the input features, because cudnn only has kernel, but not feature dilation, i.e.\r\n{ pad(16x256x256x16, pad 510 zeros before, between, and after pixels in dim0) -> conv(16x131326x256x16, 1x1021x1x16) -> pad(16x512x256x16, pad 510 zeros before, between, and after pixels in dim1)->conv(16x512x131326x16, 1x1x1021x16) }\r\nIt causes an error because cudnn doesn't allow tensor to have more than 2G elements, while the first padded tensor has 8G and the second one 16G.\r\n\r\nA possible solution is to use a GEMM-based interpolation approach to replace the dilation-based convolution one for some cases during Op::compile().  Before alternative resizing algorithm is available, how about opting out GPU from these OPs to avoid failing entire XLA?  For U-net using multiple resizing ops, leaving nearest neighbor resizing out of XLA doesn't cause observable slow down.", "comments": ["I didn't know about RecursiveCompilabilityChecker::OperationFilter::allow_slow_ops.  It is better way.  Please review the updated changes."]}, {"number": 31011, "title": "What is HalfPixelScalerForNN in nearest interpolation?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ArchLinux\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.1-dev20190526\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): **tf_env_collect.sh just crashes here**\r\n\r\n**Describe the current behavior**\r\nhttps://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/core/kernels/resize_nearest_neighbor_op.cc#L111\r\n\r\nI am implementing resize op for ONNX. https://github.com/onnx/onnx/pull/2057. I don't know why there is not \"-0.5f\" like vanilla HalfPixelScaler. The comments don't solve my concern. For example, if `(static_cast<float>(x) + 0.5f) * scale` equals 1.4, minus 0.5f or not will brings completely different result, no matter there is or is not `std::floor`.\r\n\r\n**Describe the expected behavior**\r\nI think `-0.5f` is more reasonable\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31011\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31011\">No</a>\n"]}, {"number": 31010, "title": "Detail Design doc of tf.Dataset not available", "body": "Want the Detail Design doc of tf.Dataset not available. The current design include quite a lot of tuning parameters such as num_threads, and we users have little about its functionality. It is impossible for use to debug the performance bottleneck, it just looks like a black box.  I think it is necessary for the dataset developers to publish the design doc, and give more details about the data pipeline.\r\n", "comments": ["@mrry ", "tf.data performance is discussed in https://www.tensorflow.org/beta/guide/data_performance\r\n\r\nAs you can imagine, the original tf.data design doc created in 2017 is outdated and we do not have any plans to create a new one."]}, {"number": 31009, "title": "Union of string sets cannot be converted to dense tensor (raises TypeError)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMacOSX 10.13.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\nbinary\r\n- TensorFlow version (use command below):\r\nVERSION=2.0.0-dev20190724\r\nGIT_VERSION=v1.12.1-6931-g2b5ece29d3\r\n- Python version:\r\n3.6.8\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\nN/A\r\n- GPU model and memory:\r\nN/A\r\n\r\n**Describe the current behavior**\r\nI get an exception when computing the union of two string sets and trying to convert the output to a dense tensor (`TypeError: Cannot convert 0 to EagerTensor of dtype string`).\r\n\r\n**Describe the expected behavior**\r\nI should get no exception.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\na = tf.constant([[\"a\", \"b\", \"c\", \"\", \"\"], [\"d\", \"e\", \"\", \"\", \"\"]])\r\nb = tf.constant([[\"c\", \"e\", \"g\", \"h\", \"\"], [\"d\", \"\", \"\", \"\", \"\"]])\r\ntf.sparse.to_dense(tf.sets.union(a, b))\r\n```\r\n\r\n**Other info / logs**\r\nHere's the stacktrace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-e742e5c0ff6f> in <module>\r\n      2 a = tf.constant([[\"a\", \"b\", \"c\", \"\", \"\"], [\"d\", \"e\", \"\", \"\", \"\"]])\r\n      3 b = tf.constant([[\"c\", \"e\", \"g\", \"h\", \"\"], [\"d\", \"\", \"\", \"\", \"\"]])\r\n----> 4 tf.sparse.to_dense(tf.sets.union(a, b))\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/sparse_ops.py in sparse_tensor_to_dense(sp_input, default_value, validate_indices, name)\r\n   1478       default_value=default_value,\r\n   1479       validate_indices=validate_indices,\r\n-> 1480       name=name)\r\n   1481\r\n   1482\r\n\r\n~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_sparse_ops.py in sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value, validate_indices, name)\r\n   3158         \"SparseToDense\", name, _ctx._post_execution_callbacks, sparse_indices,\r\n   3159         output_shape, sparse_values, default_value, \"validate_indices\",\r\n-> 3160         validate_indices)\r\n   3161       return _result\r\n   3162     except _core._FallbackException:\r\n\r\nTypeError: Cannot convert 0 to EagerTensor of dtype string\r\n```", "comments": ["Ok, the problem seems to have nothing to do with sets. It's just that the default value in `to_dense()` is 0, which cannot be converted to a string, so the following code works:\r\n\r\n```python\r\nimport tensorflow as tf\r\na = tf.constant([[\"a\", \"b\", \"c\", \"\", \"\"], [\"d\", \"e\", \"\", \"\", \"\"]])\r\nb = tf.constant([[\"c\", \"e\", \"g\", \"h\", \"\"], [\"d\", \"\", \"\", \"\", \"\"]])\r\ntf.sparse.to_dense(tf.sets.union(a, b), default_value=b'')\r\n```\r\n\r\nI think the default value for string sparse tensors should be the empty string. At the very least, the error message should be clearer (it should say that it's talking about the `default_value`).", "@ageron I think this is the same issue as #30750 and will be fixed by PR #30781. The PR has been approved so likely it will be merged soon.", "Indeed, it is, thanks @yongtang.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31009\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31009\">No</a>\n"]}, {"number": 31008, "title": "throw std::out_of_range while convert Lite quantization model with representative_dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0\r\n- Python version: 3.6.3\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nWhen convert to Lite quantized model with representative_dataset, it throws `std::out_of_range`, when model includes `tf.split()`\r\nMy model implement Pixel Shuffle with some split and concat operation,\r\nI narrow down to tf.split() which is causing the issue\r\n\r\n**Describe the expected behavior**\r\nConvert sucessfully\r\n\r\n**Code to reproduce the issue**\r\n```\r\ninputs_raw = tf.placeholder(tf.float32, shape=[1, 32, 32, 1], name='inputs_raw')\r\noutputs = tf.split(value = inputs_raw, num_or_size_splits = 2, axis = 1)[0]\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    converter = lite.TFLiteConverter.from_session(sess, [inputs_raw], [outputs])\r\n\r\n    converter.optimizations = [lite.Optimize.DEFAULT]\r\n    def representative_data_gen():\r\n        for i in range(1):\r\n            yield [np.random.random_sample((1, 32, 32, 1)).astype(np.float32)]\r\n            #yield [sess.run(tf.random.normal(inputs_raw.get_shape(), dtype = tf.float32, seed = 1))] # both have issue\r\n    converter.representative_dataset = representative_data_gen\r\n    tflite_model = converter.convert()\r\n```\r\n**Other info / logs**\r\nW0725 10:03:24.455737 140039335962368 deprecation_wrapper.py:119] From main.py:419: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nTensor(\"split:0\", shape=(1, 16, 32, 1), dtype=float32)\r\nW0725 10:03:24.459982 140039335962368 deprecation_wrapper.py:119] From main.py:431: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\r\n\r\n2019-07-25 10:03:24.464880: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /xxxoss/Python3/3.6.3_gpu_tf1131_cuda10-ubuntu16/x86_64/lib:/xxxtools/LSF/10/10.1/linux2.6-glibc2.3-x86_64/lib:/xxxoss/tcl/8.4.19/x86_64/lib:/xxxoss/tcl/8.4.19/x86_64/lib/tcl8.4:/xxxoss/tcl/8.4.19/x86_64/lib/tclx8.4:/lib64:/usr/lib64:/lib:\r\n2019-07-25 10:03:24.464949: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\r\n2019-07-25 10:03:24.465006: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a07ws012123): /proc/driver/nvidia/version does not exist\r\n2019-07-25 10:03:24.465369: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-07-25 10:03:24.508462: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394230000 Hz\r\n2019-07-25 10:03:24.508764: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4283070 executing computations on platform Host. Devices:\r\n2019-07-25 10:03:24.508788: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\nW0725 10:03:24.521992 140039335962368 deprecation_wrapper.py:119] From main.py:432: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\r\n\r\n2019-07-25 10:03:24.525090: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\r\n2019-07-25 10:03:24.526496: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-07-25 10:03:24.526607: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-07-25 10:03:24.528117: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-07-25 10:03:24.528143: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-07-25 10:03:24.528153: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\r\n2019-07-25 10:03:24.530117: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\r\n2019-07-25 10:03:24.530193: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2019-07-25 10:03:24.532220: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2019-07-25 10:03:24.532249: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 3 nodes (-2), 2 edges (0), time = 0.465ms.\r\nINFO: Initialized TensorFlow Lite runtime.\r\nterminate called after throwing an instance of 'std::out_of_range'\r\n  what():  _Map_base::at\r\nAbort (core dumped)\r\n", "comments": ["I seems to find a fix for this, this is probably a current limitation for lite now,\r\nas stated in https://www.tensorflow.org/lite/guide/ops_compatibility\r\n\"num_or_size_split contains number of splits as a 0D tensor\"\r\n\r\nInput shape x=[1, 32, 32, 1]\r\nIf neet tf.split(x,  2, axis = 1), use tf.split(x, tf.const([16, 16]), axis = 1)", "is there any way around to do this \r\n", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31008\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31008\">No</a>\n"]}, {"number": 31007, "title": "Remove the max_batch_size in tf-trt create_inference_graph  api", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.13.1\r\n- Are you willing to contribute it (Yes/No):  Sure, if I had the wherewithal . \r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe max_batch_size batch size limit  can  wreak havoc on codes that  can  increase the batch size dynamically (e.g. going from 25 to 50). \r\n\r\n**Will this change the current api? How?**\r\n trt.create_inference_graph would no longer need max_batch_size \r\n\r\n**Who will benefit with this feature?**\r\nAll\r\n\r\n**Any Other info.**\r\n", "comments": ["@sgambient the max_batch_size is the limit from [TensorRT library](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_builder.html#a7285560854aec37979363e1d71709bfe) since each TensorRT engine needs to be built with a specific max_batch_size.\r\n\r\nBut if you set is_dynamic_op to True, TRT engines will be built dynamically at runtime and each engine will have max_batch_size set to the batch size that causes it to be built, so you can have multiple engines with different batch sizes. This seems to be what you need, and the drawback is it'll require more resource (e.g. cpu/gpu mem), and building engine on the fly can increase the serving latency.\r\n\r\nTRT 6.0 will support dynamic shapes, so by then we don't need the max_batch_size in either mode.", "Hi, does not seem to work for me. Getting the following error.  \r\n\r\ntensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:264] Engine buffer is full. buffer limit=1, current entries=1, requested batch=25\r\n\r\nCan you please provide an example code that works ?", "@sgambient would you please set maximum_cached_engines to something larger than 1, e.g. the number of different input shapes? ", "I see, so it has to be set to maximum possible.  In my case that could be\n1000 or more.\n\n\nOn Tue, Jul 30, 2019, 8:34 PM Guangda Lai <notifications@github.com> wrote:\n\n> @sgambient <https://github.com/sgambient> would you please set\n> maximum_cached_engines to something larger than 1, e.g. the number of\n> different input shapes?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/31007?email_source=notifications&email_token=AKBUG62PEZPLIZBI5AAHQ2TQCEB3NA5CNFSM4IGWHQE2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3F6LDA#issuecomment-516679052>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AKBUG66BGZW24KUAJRANQZLQCEB3NANCNFSM4IGWHQEQ>\n> .\n>\n", "What is  the consequence of setting the batch_size to a large number ? ", "The maximum batch size specifies the batch size for which TensorRT will optimize. So running with smaller batches may result in sub-optimal performance.", "That is what I figured.  The batch size can be variable for some DL models , specially in  ROI (Regions of Interest)  type of applications, where one batch slice can trigger  50x  slices down the pipeline.  So, if  input batch size if 25,  some where in the middle of the  pipeline, batch size can become 25*50 = 1250.  Looks like TensorRT  does not handle that use case transparently  at this point - would that be a correct conclusion to have ? \r\n", "@sgambient yes that is the case. But future version of TensorRT may address this issue by supporting dynamic shapes (even on batch dimension), and we'll update the TF-TRT bridge correspondingly when that comes out."]}, {"number": 31006, "title": "Contradicting bazel versions suggested", "body": "### URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/source_windows#install_bazel\r\n\r\n## Description of issue (what needs changing):\r\nIt should be Bazel <0.23.0, not 0.24.1\r\n### Clear description\r\nFirst and last sentence contradict each other:\r\nInstall Bazel 0.24.1,\r\nEnsure you install Bazel 0.23.0 or lower.\r\n\r\n### Correct links\r\n\r\n### Parameters defined\r\n\r\n### Returns defined\r\n\r\n### Raises listed and defined\r\n\r\n### Usage example\r\n\r\n### Request visuals, if applicable\r\n\r\n### Submit a pull request?\r\nNo", "comments": ["@gadagashwini This issue seems to have been fixed and can be closed. Bazel 0.24.1 or greater is required and is correctly mentioned on the site.\r\nCould I be added as a collaborator on this project? Then I'll be able to self assign tasks and do much more. Thanks.", "@boraca  Proceeding to close this issue as its resolved", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31006\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31006\">No</a>\n"]}]