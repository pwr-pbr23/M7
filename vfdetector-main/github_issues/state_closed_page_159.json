[{"number": 50041, "title": "[INTEL MKL] Allow input permutations for binary commutative ops in pattern matcher.", "body": "This PR is a small enhancement of pattern matcher for grappler remapper graph optimization.\r\n\r\nInputs to commutative binary ops can be in different order than the pattern specified for binary commutative ops (e.g., Add, Mul etc.). While checking all possible permutations has exponential time complexity, we keep it linear by allowing permutation only in a limited manner by looking ahead one level in the graph to decide whether visiting right child before left is likely to match the given pattern.\r\n ", "comments": ["@penpornk /@ezhulenev Can you please review this PR ? Thanks!", "This PR fails to compile internally with:\r\n\r\n```(10:36:33) ERROR: third_party/tensorflow/core/grappler/utils/BUILD:395:11: Compiling third_party/tensorflow/core/grappler/utils/pattern_utils.cc failed: (Exit 1) wrapped_clang failed: error executing command third_party/crosstool/v18/stable/toolchain/bin/wrapped_clang '-frandom-seed=blaze-out/k8-opt/bin/third_party/tensorflow/core/grappler/utils/_objs/pattern_utils/pattern_utils.pic.o' ... (remaining 330 arguments skipped). \r\nthird_party/tensorflow/core/grappler/utils/pattern_utils.cc:20:10: error: module //third_party/tensorflow/core/grappler/utils:pattern_utils does not depend on a module exporting 'third_party/absl/container/flat_hash_set.h'\r\n#include \"third_party/absl/container/flat_hash_set.h\"\r\n         ^\r\nthird_party/tensorflow/core/grappler/utils/pattern_utils.cc:124:26: error: unused variable 'graph_child1_node_view' [-Werror,-Wunused-variable]\r\n        MutableNodeView* graph_child1_node_view =\r\n```", "@ezhulenev How do I get build_cleaner?", "We fixed the flat_hash_set.h dependency internally. This PR has been merged so I'm closing it now. Thank you again for the PR!"]}, {"number": 50040, "title": "Different outputs for same input & model in Colab vs Laptop", "body": "\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab and Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.5.0\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): na\r\n- GCC/Compiler version (if compiling from source): na\r\n- CUDA/cuDNN version:na\r\n- GPU model and memory:na\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am getting different output for the same model.\r\nThe model is here: https://drive.google.com/file/d/1yUjCG5rMeVCKTSPbST2F9BrRRlkDPzEA/view\r\nMy test script is:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport PIL\r\nimport numpy as np\r\n\r\nm = tf.keras.models.load_model('./model.h5')\r\nar = np.expand_dims(np.asarray(PIL.Image.open('./test.jpeg')), axis=0)\r\nar1 = (ar-127.5) / 127.5\r\ne = m.predict(ar1)\r\n```\r\n**here e is different on my laptop/mobile(tflite) vs my server/Colab. i.e. output in my laptop matches tflite output on my mobile but is different from Colab/My Server.**\r\n\r\n**Describe the expected behavior**\r\nembeddings should be exactly same up to 4-5 decimal points!\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n\r\n![sample](https://user-images.githubusercontent.com/29163297/120754851-d4895380-c52a-11eb-9e3b-816db34af347.jpeg)", "comments": ["@vizakshat \r\n\r\n In order to expedite the trouble-shooting process, please provide colab gist along with all the dependencies to reproduce the issue reported here. Thanks!\r\n", "> @vizakshat\r\n> \r\n> In order to expedite the trouble-shooting process, please provide colab gist along with all the dependencies to reproduce the issue reported here. Thanks!\r\n\r\nHow can I get the gist? I am using normal colab. Haven't changed any settings!", "@vizakshat \r\n\r\nCopy and Paste the colab link you worked on and give me the access or Save the copy as github gist you will get a copy of the gist then copy and paste the link here.Thanks", "https://colab.research.google.com/gist/vizakshat/7dc03a6278ab840e2a4a5e79639cd45c/emb-issue-tf.ipynb\r\nhere.", "@vizakshat Interesting. did you compare the model weights after loading? Can you please share the model `keras_mobilenet.h5`? Thanks!", "https://drive.google.com/file/d/1yUjCG5rMeVCKTSPbST2F9BrRRlkDPzEA/view\r\nhere", "Any update on this?", "@vizakshat Need the image also. But, this is more of a support issue (not a bug). Debugging your model may be required to find the root-cause. If you think this is a bug, can you use some public data and model to verify and find root-cause of the issue. Thanks!\r\n\r\nThis repo is mainly for bugs and performance related issues. For support issues, we recommend to post them in Stackoverflow where there is a big community to support and resolve issues faster. Thanks!", "I think this is a bug. I have used tensorflow pre-built libraries everywhere nothing else. Plus image you can use anything. Just do a random image and you can check for the outputs. ex **np.random.random((1,112,112,3))** check for outputs using same array (use np.save and np.load)\r\n\r\nThis is a bug because same model output is different for different servers vs laptop vs colab vs tflite on mobile. (as shown in image above). I was expecting outputs to be **same atleast for 4-5 decimal points!!**\r\n\r\nThis model is also based on tensorflow-keras. You can check the code in the given repo: https://github.com/leondgarse/Keras_insightface", "> @vizakshat Interesting. did you compare the model weights after loading? Can you please share the model `keras_mobilenet.h5`? Thanks!\r\n\r\nHow can I compare the weights while doing on different devices? Saving the model and then loading on a different device is resulting in the same difference in outputs!!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@vizakshat I ran [`Image_classification_model`](https://www.tensorflow.org/tutorials/images/classification) from TF tutorial page, saved it and loaded it on colab, local (mac OS), and local2 (Windows10 OS). I could see exactly same numbers. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/92918286d3195d11c07f530d6b739481/untitled1003.ipynb).\r\n\r\nYou could check `model.weights` on different systems. If the weights are same, then the results should be exactly same. If the weights are same but the results are not same, then there may be some uncertainty coming from the input. Thanks! \r\n\r\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50040\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50040\">No</a>\n"]}, {"number": 50039, "title": "Bump curl", "body": "", "comments": []}, {"number": 50038, "title": "Bump curl", "body": "", "comments": []}, {"number": 50037, "title": "Bump curl", "body": "", "comments": []}, {"number": 50036, "title": "Bump curl", "body": "", "comments": []}, {"number": 50035, "title": "Ensure validation sticks in banded_triangular_solve_op", "body": "PiperOrigin-RevId: 373275480\r\nChange-Id: Id7717cf275b2d6fdb9441fbbe166d555182d2e79", "comments": []}, {"number": 50034, "title": "Ensure validation sticks in banded_triangular_solve_op", "body": "PiperOrigin-RevId: 373275480\r\nChange-Id: Id7717cf275b2d6fdb9441fbbe166d555182d2e79", "comments": []}, {"number": 50033, "title": "Validate that a and b are proper sparse tensors", "body": "PiperOrigin-RevId: 373274848\r\nChange-Id: I3a665ac3a29dee9fb69bdf408a939330cb93ea75", "comments": []}, {"number": 50032, "title": "Validate that a and b are proper sparse tensors", "body": "PiperOrigin-RevId: 373274848\r\nChange-Id: I3a665ac3a29dee9fb69bdf408a939330cb93ea75", "comments": []}, {"number": 50031, "title": "Validate that a and b are proper sparse tensors", "body": "PiperOrigin-RevId: 373274848\r\nChange-Id: I3a665ac3a29dee9fb69bdf408a939330cb93ea75", "comments": []}, {"number": 50030, "title": "Validate that a and b are proper sparse tensors", "body": "PiperOrigin-RevId: 373274848\r\nChange-Id: I3a665ac3a29dee9fb69bdf408a939330cb93ea75", "comments": []}, {"number": 50029, "title": "Build tensorflow-lite for armv7-a (embedded linux) failed", "body": "\r\n**System information**\r\n- Host platform - Ubuntu 20.04.1 LTS:\r\n- target platform - embedded linux - OS Openwrt, CPU - IMX6ULL;\r\n- TensorFlow installed from source:\r\n- TensorFlow version: 2.6.0\r\n- Cross compilation using C++ and CMake. CMake version - 3.16.0\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nTrying to cross-compile Tensorflow-lite minimal c++ example using CMake.\r\n\r\n```\r\n-DCMAKE_SYSTEM_PROCESSOR=armv7-a\r\n```\r\n\r\nChecked my target hardware as explained here: \r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm#check_your_target_environment\r\nHere my target device /proc/cpuinfo output:\r\n```\r\nroot@OpenWrt:/# cat /proc/cpuinfo                                                                                                                                                        \r\nprocessor       : 0                                                                                                                                                                      \r\nmodel name      : ARMv7 Processor rev 5 (v7l)                                                                                                                                            \r\nBogoMIPS        : 109.09                                                                                                                                                                 \r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt vfpd32 lpae                                                                                              \r\nCPU implementer : 0x41                                                                                                                                                                   \r\nCPU architecture: 7                                                                                                                                                                      \r\nCPU variant     : 0x0                                                                                                                                                                    \r\nCPU part        : 0xc07                                                                                                                                                                  \r\nCPU revision    : 5                                                                                                                                                                      \r\n                                                                                                                                                                                         \r\nHardware        : Freescale i.MX6 Ultralite (Device Tree)                                                                                                                                \r\nRevision        : 0000                                                                                                                                                                   \r\nSerial          : 0000000000000000         \r\n```                                                                                                                                              \r\n\r\nBuild fails with following error:\r\n\r\n```\r\n[ 85%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundz-neonv8.c.o\r\n[ 85%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-neondot.c.o\r\narm-openwrt-linux-muslgnueabi-gcc: error: unrecognized argument in option '-march=armv8.2-a+dotprod'\r\narm-openwrt-linux-muslgnueabi-gcc: note: valid arguments to '-march=' are: armv2 armv2a armv3 armv3m armv4 armv4t armv5 armv5e armv5t armv5te armv5tej armv6 armv6-m armv6j armv6k armv6kz armv6s-m armv6t2 armv6z armv6zk armv7 armv7-a armv7-m armv7-r armv7e-m armv7ve armv8-a armv8-a+crc armv8-m.base armv8-m.main armv8-m.main+dsp armv8.1-a armv8.2-a armv8.2-a+fp16 iwmmxt iwmmxt2 native; did you mean 'armv8.2-a+fp16'?\r\nmake[5]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:14766: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-neondot.c.o] Error 1\r\nmake[5]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt/build_dir/target-arm_cortex-a7+neon-vfpv4_musl_eabi/tensorflow/tensorflow-sdk-build'\r\nmake[4]: *** [CMakeFiles/Makefile2:4062: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/all] Error 2\r\nmake[4]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt/build_dir/target-arm_cortex-a7+neon-vfpv4_musl_eabi/tensorflow/tensorflow-sdk-build'\r\nmake[3]: *** [Makefile:152: all] Error 2\r\nmake[3]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt/build_dir/target-arm_cortex-a7+neon-vfpv4_musl_eabi/tensorflow/tensorflow-sdk-build'\r\nmake[2]: *** [Makefile:67: /home/al/imx6ull-openwrt2/imx6ull-openwrt/build_dir/target-arm_cortex-a7+neon-vfpv4_musl_eabi/tensorflow/.built] Error 2\r\nmake[2]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt/package/tensorflow'\r\ntime: package/tensorflow/compile#211.83#30.92#311.67\r\nmake[1]: *** [package/Makefile:113: package/tensorflow/compile] Error 2\r\nmake[1]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt'\r\nmake: *** [/home/al/imx6ull-openwrt2/imx6ull-openwrt/include/toplevel.mk:227: package/tensorflow/compile] Error 2\r\n```\r\n\r\nseems that error produced by following lines in `xnnpack/CMakeLists.txt` file:\r\n\r\n```\r\nIF(CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv[5-8]\" OR IOS_ARCH MATCHES \"^armv7\")\r\n  SET_PROPERTY(SOURCE ${XNNPACK_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -marm \")\r\n  SET_PROPERTY(SOURCE ${XNNPACK_NEON_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -march=armv7-a -mfpu=neon \")\r\n  SET_PROPERTY(SOURCE ${XNNPACK_NEONFMA_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -march=armv7-a -mfpu=neon-vfpv4 \")\r\n  IF(IOS)\r\n    SET_PROPERTY(SOURCE ${XNNPACK_NEONV8_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -mcpu=cyclone -mtune=generic \")\r\n    SET_PROPERTY(SOURCE ${XNNPACK_AARCH32_ASM_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -arch ${IOS_ARCH} \")\r\n  ELSE()\r\n    SET_PROPERTY(SOURCE ${XNNPACK_NEONV8_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -march=armv8-a -mfpu=neon-fp-armv8 \")\r\n    SET_PROPERTY(SOURCE ${XNNPACK_NEONDOT_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS \" -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8 \")\r\n  ENDIF()\r\n```\r\nTried to edit `xnnpack/CMakeLists.txt` manually by replacing `armv8.2-a+dotprod` to `armv7-a` and `neon-fp-armv8` to `neon-vfpv4` without luck. New errors happens like `incompatible types when assigning to type 'int32x4_t' from type 'int'`.\r\n\r\nCould you please point me to some solution of that issue?\r\nAs I understand target hardware CPU supports tensorflow lite.", "comments": ["@terryheo could you take a look?", "Was able to cross compile and run on target hardware using flag `-DTFLITE_ENABLE_XNNPACK=OFF`.\r\nBut not with XNNPACK unfortunately. I believe it is possible. Could you please help me?", "What's the build commands and toolchains you used? I think the current XNNPACK build only works with `-march=armv7-a`, not `armv8.2-a+dotprod`.\r\nIs there any issue on using `-march=armv7-a`?\r\n\r\nhttps://www.tensorflow.org/lite/guide/build_cmake_arm#run_cmake_2", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@terryheo I have this issue too,  my cmd is \r\n```bash\r\ncmake -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON -DCMAKE_SYSTEM_NAME=Linux -DCMAKE_SYSTEM_PROCESSOR=armv7 -DCMAKE_CXX_FLAGS=\"-march=armv7\" ../tensorflow/tensorflow/lite/\r\n```\r\n\r\nwhy add **-march=armv8.2-a+dotprod** when compile\r\n```\r\n[ 85%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c.o\r\ncd /opt/module_src/tflite_build/_deps/xnnpack-build && /opt/build_env/toolchain-sunxi-musl/toolchain/bin/arm-openwrt-linux-muslgnueabi-gcc -DCPUINFO_SUPPORTED_PLATFORM=1 -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_SPARSE=1 -DXNN_LOG_LEVEL=0 -I/opt/module_src/tflite_build/xnnpack/include -I/opt/module_src/tflite_build/xnnpack/src -I/opt/module_src/tflite_build/clog-source/deps/clog/include -I/opt/module_src/tflite_build/cpuinfo-source/include -I/opt/module_src/tflite_build/pthreadpool-source/include -I/opt/module_src/tflite_build/FXdiv-source/include -I/opt/module_src/tflite_build/FP16-source/include -O2 -pipe -g -feliminate-unused-debug-types -fdebug-prefix-map=/mfs/mtkslt0207/mtk71589/mt8516_rel/build/tmp/work/x86_64-nativesdk-oesdk-linux/meta-environment-aud8516-ali-slc/1.0-r8=/usr/src/debug/meta-environment-aud8516-ali-slc/1.0-r8 -fdebug-prefix-map=/mfs/mtkslt0207/mtk71589/mt8516_rel/build/tmp/sysroots/x86_64-linux= -fdebug-prefix-map=/mfs/mtkslt0207/mtk71589/mt8516_rel/build/tmp/sysroots/x86_64-nativesdk-oesdk-linux= -O3 -DNDEBUG -fPIC -Wno-psabi -pthread -std=gnu99  -marm  -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8  -O2  -MD -MT _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c.o -MF CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c.o.d -o CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c.o -c /opt/module_src/tflite_build/xnnpack/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c\r\n```", "Finally was able to build `tensorflow-lite` for `armv-7` by applying following patch:\r\n\r\n```\r\nIndex: tensorflow/tensorflow-sdk-build/xnnpack/CMakeLists.txt\r\n===================================================================\r\n--- tensorflow.orig/tensorflow-sdk-build/xnnpack/CMakeLists.txt\r\n+++ tensorflow/tensorflow-sdk-build/xnnpack/CMakeLists.txt\r\n@@ -34,6 +34,8 @@ IF(XNNPACK_BUILD_TESTS)\r\n   ENABLE_TESTING()\r\n ENDIF()\r\n\r\n+ADD_DEFINITIONS(-DXNN_NO_QS8_OPERATORS)\r\n+\r\n IF(XNNPACK_ENABLE_ASSEMBLY)\r\n   ADD_DEFINITIONS(-DXNN_ENABLE_ASSEMBLY=1)\r\n ELSE()\r\n@@ -2916,7 +2918,7 @@ IF(CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv\r\n   LIST(APPEND XNNPACK_MICROKERNEL_SRCS ${XNNPACK_NEON_MICROKERNEL_SRCS})\r\n   LIST(APPEND XNNPACK_MICROKERNEL_SRCS ${XNNPACK_NEONFMA_MICROKERNEL_SRCS})\r\n   LIST(APPEND XNNPACK_MICROKERNEL_SRCS ${XNNPACK_NEONV8_MICROKERNEL_SRCS})\r\n-  IF(NOT IOS)\r\n+  IF(NOT IOS AND NOT CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv[5-8]\")\r\n     LIST(APPEND XNNPACK_MICROKERNEL_SRCS ${XNNPACK_NEONDOT_MICROKERNEL_SRCS})\r\n   ENDIF()\r\n   IF(XNNPACK_ENABLE_ASSEMBLY)\r\n```\r\n\r\nplus that:\r\n\r\n```\r\nCMAKE_OPTIONS=-DCMAKE_SYSTEM_PROCESSOR=armv7\r\n```\r\n\r\nI guess that issue might be closed. Thank you.", "My problem is very similar to yours. I didn't perform the build after cmake", "Is this still an issue.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50029\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50029\">No</a>\n"]}, {"number": 50028, "title": "tf.contrib.framework.assign_from_variables in Tensorflow 2", "body": "I'm running an inference model that used tf1, and I've changed the code to tf2 but I can't find a proper tf2 alternative to contrib.framework.assign_from_variables anywhere. I've tried tf.compat.v1 but still the same issue. \r\n", "comments": ["Just found out the solution ; just use tf_slim", "@ahbe ,\r\n\r\nPlease refer this link for more information on contrib.[Link](https://www.tensorflow.org/guide/migrate),[Link2](https://www.tensorflow.org/guide/upgrade).It helps.Thanks", "@ahbe ,\r\n\r\nPlease feel free to move this issue to closed status if the issue has resolved.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50028\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50028\">No</a>\n"]}, {"number": 50027, "title": "Tensorflow 1.14.0 C++ can't get GPU device", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux centos 7.7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 1.14\r\n- Python version:  3.6\r\n- Installed using virtualenv? pip? conda?: None\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): 4.85\r\n- CUDA/cuDNN version: CUDA 10.0, cudnn 7.4\r\n- GPU model and memory: Tesla T4\r\n\r\n**Describe the problem**\r\n - run service can't get GPU device by use libtensorflow_cc.so and libtensorflow_framework.so.\r\n    bazel build tensorflow_cc\r\n- python3.6:\r\n    - print(device_lib.list_local_devices())\r\n        - [name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 7969440054434969841\r\n, name: \"/device:XLA_GPU:0\"\r\ndevice_type: \"XLA_GPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 13181855577668801818\r\nphysical_device_desc: \"device: XLA_GPU device\"\r\n, name: \"/device:XLA_CPU:0\"\r\ndevice_type: \"XLA_CPU\"\r\nmemory_limit: 17179869184\r\nlocality {\r\n}\r\nincarnation: 13243497781981531688\r\nphysical_device_desc: \"device: XLA_CPU device\"\r\n, name: \"/device:GPU:0\"\r\ndevice_type: \"GPU\"\r\nmemory_limit: 14892338381\r\nlocality {\r\n  bus_id: 1\r\n  links {\r\n  }\r\n}\r\nincarnation: 8775195624210653350\r\nphysical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5\"\r\n]\r\n\r\n- C++:\r\n     - std::vector<tensorflow::DeviceAttributes> resp;\r\n     - TF_CHECK_OK(session_->ListDevices(&resp));\r\n     - for (const auto& dev : resp)   {std::cout<< \"dev type = \"<<dev.device_type()<<std::endl;}\r\n         - dev type = CPU\r\ndev type = XLA_CPU\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\nNone\r\n", "comments": ["@silingtong123 \r\nKindly upgrade to 2.x as there is no active support for 1.x, and let us know if the issue persist.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50027\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50027\">No</a>\n"]}, {"number": 50026, "title": "Issue converting full scale BERT Model to TFLite Model.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ***Ubuntu 18.04.5 LTS*** (Colab)\r\n- TensorFlow installation (pip package or built from source): ***pip package*** (Colab)\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): ***Tensorflow 2.5.0*** (Colab)\r\n\r\n### 2. Context\r\nI was following tutorial on Classification of text with BERT from [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) notebook I am using the same code from that notebook without any changes.\r\n\r\nTo convert  the model to TFLite I am following the documentation from [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert) on converting a model to TFLite model with the recommended method i.e. ```from_saved_model()```.\r\n\r\nI have also tried with other method ```from_keras_model()``` but this also shows error.\r\n\r\nP.S. I am aware that lite version of BERT is also available ([ALBERT](https://tfhub.dev/tensorflow/albert_en_base/3), [TF Lite Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_text_classification) ) but I want to know why TFLiteConverter is not able to convert the full scale TF model.\r\nI have also tried with tf-nightly too and the issue still persists.\r\nI want to know can this issue be solved or is there any workaround right now or I should wait for future support from Tensorflow regarding this.\r\n\r\n### 3. Code\r\n\r\n### Please find the Google Colab Notebook link: [Colab Notebook](https://colab.research.google.com/drive/1q7ShcS0sk7-ZMH2Nwc3n5khaOzCExysk)\r\n\r\n## Code Block where error occurs ```from_saved_model``` [Colab Notebook block](https://colab.research.google.com/drive/1q7ShcS0sk7-ZMH2Nwc3n5khaOzCExysk#scrollTo=-UPELrKGRdoj):\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\ntflite_model = converter.convert()\r\nlite_model_name = \"BERTLite.tflite\"\r\nwith open(f'{lite_model_name}', 'wb') as f:\r\n\tf.write(tflite_model)\r\n```\r\n\r\n## Code Block where error occurs ```from_keras_model``` [Colab Notebook block](https://colab.research.google.com/drive/1q7ShcS0sk7-ZMH2Nwc3n5khaOzCExysk#scrollTo=lXzszo1EiGkl):\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(classifier_model)\r\ntflite_model = converter.convert()\r\n```\r\n\r\n### 4. Error Traceback:\r\n\r\n### When trying with ```from_saved_model()```.\r\n```\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    293                                                  debug_info_str,\r\n--> 294                                                  enable_mlir_converter)\r\n    295       return model_str\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n     37       debug_info_str,\r\n---> 38       enable_mlir_converter)\r\n     39 \r\n\r\nException: <unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(\"map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_63086\" at \"bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_63100\") at \"StatefulPartitionedCall@__inference_model_layer_call_fn_63122\") at \"StatefulPartitionedCall@__inference_restored_function_body_151700\") at \"model/preprocessing/StatefulPartitionedCall@__inference__wrapped_model_152562\") at \"StatefulPartitionedCall@__inference_signature_wrapper_158612\") at \"StatefulPartitionedCall_2\")): 'tf.TensorListReserve' op requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall_2\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(\"map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_63086\" at \"bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_63100\") at \"StatefulPartitionedCall@__inference_model_layer_call_fn_63122\") at \"StatefulPartitionedCall@__inference_restored_function_body_151700\") at \"model/preprocessing/StatefulPartitionedCall@__inference__wrapped_model_152562\") at \"StatefulPartitionedCall@__inference_signature_wrapper_158612\") at \"StatefulPartitionedCall_2\")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall_2\"): called from\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n<ipython-input-25-8d1ce648fcd0> in <module>()\r\n      1 converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\r\n----> 2 tflite_model = converter.convert()\r\n      3 lite_model_name = \"BERTLite.tflite\"\r\n      4 with open(f'{lite_model_name}', 'wb') as f:\r\n      5         f.write(tflite_model)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n    911     converter_kwargs.update(quant_mode.converter_flags())\r\n    912 \r\n--> 913     result = _convert_saved_model(**converter_kwargs)\r\n    914     if self.experimental_new_quantizer:\r\n    915       calibrate_and_quantize, flags = quant_mode.quantizer_flags(\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in convert_saved_model(saved_model_dir, saved_model_version, saved_model_tags, saved_model_exported_names, **kwargs)\r\n    725       None,  # input_data, unused\r\n    726       None,  # debug_info_str, unused\r\n--> 727       enable_mlir_converter=True)\r\n    728   return data\r\n    729 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    295       return model_str\r\n    296     except Exception as e:\r\n--> 297       raise ConverterError(str(e))\r\n    298 \r\n    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(\"map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_63086\" at \"bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_63100\") at \"StatefulPartitionedCall@__inference_model_layer_call_fn_63122\") at \"StatefulPartitionedCall@__inference_restored_function_body_151700\") at \"model/preprocessing/StatefulPartitionedCall@__inference__wrapped_model_152562\") at \"StatefulPartitionedCall@__inference_signature_wrapper_158612\") at \"StatefulPartitionedCall_2\")): 'tf.TensorListReserve' op requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall_2\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(\"map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_63086\" at \"bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_63100\") at \"StatefulPartitionedCall@__inference_model_layer_call_fn_63122\") at \"StatefulPartitionedCall@__inference_restored_function_body_151700\") at \"model/preprocessing/StatefulPartitionedCall@__inference__wrapped_model_152562\") at \"StatefulPartitionedCall@__inference_signature_wrapper_158612\") at \"StatefulPartitionedCall_2\")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall_2\"): called from\r\n```\r\n\r\n### When trying with ```from_keras_model()```.\r\n\r\n```\r\nWARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 910). These functions will not be directly callable after loading.\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-26-d96cda79a162> in <module>()\r\n      1 converter = tf.lite.TFLiteConverter.from_keras_model(classifier_model)\r\n----> 2 tflite_model = converter.convert()\r\n\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py in convert(self)\r\n   1034     frozen_func, graph_def = (\r\n   1035         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\r\n-> 1036             self._funcs[0], lower_control_flow=False))\r\n   1037 \r\n   1038     input_tensors = [\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)\r\n   1110       func=func,\r\n   1111       lower_control_flow=lower_control_flow,\r\n-> 1112       aggressive_inlining=aggressive_inlining)\r\n   1113 \r\n   1114   output_graph_def, converted_input_indices = _replace_variables_by_constants(\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/convert_to_constants.py in __init__(self, func, lower_control_flow, aggressive_inlining, variable_names_allowlist, variable_names_denylist)\r\n    805         variable_names_allowlist=variable_names_allowlist,\r\n    806         variable_names_denylist=variable_names_denylist)\r\n--> 807     self._build_tensor_data()\r\n    808 \r\n    809   def _build_tensor_data(self):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/convert_to_constants.py in _build_tensor_data(self)\r\n    824         data = map_index_to_variable[idx].numpy()\r\n    825       else:\r\n--> 826         data = np.array(val_tensor.numpy())\r\n    827       self._tensor_data[tensor_name] = _TensorData(\r\n    828           numpy=data,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1092     \"\"\"\r\n   1093     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1094     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1095     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1096 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1060       return self._numpy_internal()\r\n   1061     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1062       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1063 \r\n   1064   @property\r\n\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.\r\n```\r\n\r\n\r\nAny Suggestion? Is there something I am doing wrong?\r\nThanks.", "comments": ["Please try out the saved model conversion with the Select TF option.\r\n\r\nhttps://www.tensorflow.org/lite/guide/ops_select", "@abattery Thank you so much for the link. It seems that some of the ops are unsupported which are required by the TF Model of BERT, although the TFLiteConverter just manages to convert, it does give warning saying it might not work. Anyways thanks for the help.\r\n\r\nThis Issue can be closed now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50026\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50026\">No</a>\n", "Actually, the above warning message is just for your information. No doubt that the converted model will work. This is just a matter of the required operators fully supported through builtin operator set or not.", "Ya that's what the HashTableV2 ops needs explicit initialization so that's why it was giving the warning. It did convert it to TFLite model but it just crashes when trying to predict the value."]}, {"number": 50024, "title": "Datosmoviles", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@Georgeiva \r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nThanks\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50024\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50024\">No</a>\n"]}, {"number": 50023, "title": "problem with tf.image.ssim_multiscale 's filter size", "body": "when i use this function(with filter size=11) to compare two batches of images' (width:100,height:100)multiscale ssim score, i got error like 'Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: 10, 7, 7, 3 11'. I think this is due to the wrong filter size choosing, since 100 > 11, i think there is something wrong when you implement this function \r\n", "comments": ["ghgch ", "@poseidonchan ,\r\n\r\nWe see that the [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced] or if possible share a colab gist with the issue reported.\r\n\r\nThanks!\r\n", "Can you please go through the issues with similar references.It helps.[Link1](https://github.com/tensorflow/tensorflow/issues/33840#issuecomment-633715778),[Link2](https://stackoverflow.com/questions/65484160/invalidargumenterror-expected-tf-tensorfalse-shape-dtype-bool-to-be-tr/65486274).\r\nThanks!"]}, {"number": 50022, "title": "Convert tflite output location to screen location", "body": "Hi everyone. I have an android app that use a detection model. the model is working fine. I just have problem with converting the output location to screen location that user can see. I used the tflite android example and extract some code out of it. but the conversion is not correct. here is my function to converting:\r\n\r\n```\r\n`private fun mapOutputCoordinates(\r\n        location: RectF,\r\n        rotationDegrees: Int,\r\n        width: Int,\r\n        height: Int\r\n    ): RectF {\r\n        val frameToCropTransform = getTransformationMatrix(\r\n            width,\r\n            height,\r\n            ModelInputSize,\r\n            ModelInputSize,\r\n            rotationDegrees,\r\n            MAINTAIN_ASPECT\r\n        )\r\n\r\n        val cropToFrameTransform = Matrix()\r\n        frameToCropTransform.invert(cropToFrameTransform)\r\n\r\n        cropToFrameTransform.mapRect(location)\r\n        val rotated = rotationDegrees % 180 == 90\r\n\r\n        val multiplier = min(\r\n            camera.height / (if (rotated) width else height).toFloat(),\r\n            camera.width / (if (rotated) height else width).toFloat()\r\n        )\r\n        val frameToCanvasMatrix = getTransformationMatrix(\r\n            width,\r\n            height,\r\n            (multiplier * if (rotated) height else width).toInt(),\r\n            (multiplier * if (rotated) width else height).toInt(),\r\n            rotationDegrees,\r\n            false\r\n        )\r\n        //frameToCanvasMatrix.mapRect(location)\r\n        val rgbFrameToScreen = Matrix(frameToCanvasMatrix)\r\n\r\n        val detectionFrameRect = RectF(location)\r\n\r\n        val detectionScreenRect = RectF()\r\n        rgbFrameToScreen.mapRect(detectionScreenRect, detectionFrameRect)\r\n        return detectionScreenRect\r\n    }\r\n}`\r\n```\r\n\r\nI don't know if this place is the right place for asking this question but i'm straggling with this for over a weak. ", "comments": ["@shadmanadman ,\r\n\r\nPlease fill the issue template and  in order to expedite the trouble-shooting process, could you please provide a minimal code snippet and the TensorFlow version you are using.\r\n\r\nThanks!\r\n", "It's also relevant with how the detection model encodes bounding boxes (absolute coordinates, or relative ones), and how you preprocess the image (how you resize - any cropping? resizing maintaining/not maintaining aspect?)\r\n\r\nYou can also try `ImageProcessor` in TFLite Support library, which has API to transfer the location back to original coordinate system. (link: https://github.com/tensorflow/tflite-support/blob/cf5d79051856f4b06e29bb2a2e3361bfe748fc2e/tensorflow_lite_support/java/src/java/org/tensorflow/lite/support/image/ImageProcessor.java#L71)\r\n\r\nExamples using `ImageProcessor`: https://github.com/tensorflow/examples/blob/a228a3460f3fdd8edee9e8b061a08ffc92629907/lite/examples/image_classification/android/lib_support/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L308", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50021, "title": "U-Net is not converging in RTX3090", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.9\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0(both tensorflow and tensorflow-gpu)\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA-11.0, cudatoolkit-10.0, cudnn-7.6.5\r\n- GPU model and memory: GTX3090(24G mem)\r\n\r\n**Describe the current behavior**\r\npackages(tensorflow, cudnn, cudatoolkit) installed using anaconda; \r\nTyring to replicate result from a U-Net project(https://github.com/hongweilibran/wmh_ibbmTum) by running train_leave_one_out.py; \r\nThe model cannot converge, loss not drop from the beginning. Same code worked as expected on Google colab and other remote runtime environment.\r\n\r\nOther TF2.X and pytorch toy project works fine on this runtime.\r\nAny input is appreaciated!\r\n", "comments": ["@yilei-wu \r\n\r\nI tried running the code on colab didn't face any errors as you mentioned,here is the [gist](https://colab.research.google.com/gist/UsharaniPagadala/357a13728f737da6b337be8f87ddedca/untitled76.ipynb) as you mentioned\r\nCould you please check the Tensorflow and cuda and cuDNN versions  compatibility of your env as the latest tested build configurations are these.\r\n\r\nVersion | Python version | Compiler | Build tools | cuDNN | CUDA\r\n-- | -- | -- | -- | -- | --\r\ntensorflow-2.5.0 | 3.6-3.9 | GCC 7.3.1 | Bazel 3.7.2 | 8.1 | 11.2\r\ntensorflow-2.4.0 | 3.6-3.8 | GCC 7.3.1 | Bazel 3.1.0 | 8.0 | 11.0\r\n\r\n please upgrade to tf2.x  as we don't support for tf1.x,let me know if It helps\r\nFor Anaconda env related issues  post [here](https://github.com/ContinuumIO/anaconda-issues/issues) for better response.\r\nThanks\r\n", "Thank you for your reponse!\r\n\r\nYes, I did not observe errors on the google colab as well and the colab dependency is not exactly same as it used in the original repo.\r\n\r\nThe project is written in TF1.X therefore I have to rewirte the some part to make it runnable on TF2.X.\r\nAnother problem is I cannot choose the offcial-compatible CUDA version since that is not installed on the HPC(the problematic runtime).\r\n\r\nMy question is is there possible explanations casuing the model not converge using TF1.14 with GTX3090 and corresponding cuda? Based on your knowledge, would the compatbility issue cause this problem?\r\n", "@yilei-wu \r\nCould you please refer [this](https://github.com/tensorflow/tensorflow/issues/44200),and let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50020, "title": "[MLIR][DISC] support fusion on buffer", "body": "This pass implements the logic to group kLoop/kInput fusion patterns on\r\nbuffer level. The reason for this is that we can avoid a lot of\r\nheadaches to handle `shape-only` consumers specially (e.g. memref.dim,\r\nshape.shapeOf) since shapes are already resolved in buffer world. It may\r\nbe better to move this pass to tensor level after more shape\r\ninference/constraint infras are ready on mhlo level.", "comments": ["@joker-eph Could you help to review this PR?", "Sorry I was out for a few days when this PR was sent and I missed, will look now!", "@joker-eph Sorry, I add a more commit to fix the sanity check failure. BTW, is there any tool like clang-format I can use outside the google to check the BUILD file format? Thanks!", "@joker-eph Hi, could you help to have a look again?", "Formatter for BUILD file: https://github.com/bazelbuild/buildtools", "You need to rebase and fix the conflict I think before the import will go through", "@joker-eph Thanks! I have fixed the conflict."]}, {"number": 50019, "title": "How to merge results from distribute worker in tf.distribute.experimental.MultiWorkerMirroredStrategy", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@LeslieGe ,\r\n\r\nPlease refer the links mentioned that provide the information on MultiWorkerMirroredStrategy.[Link1](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy),[Link2](https://www.tensorflow.org/tutorials/distribute/input),[Link3](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext).\r\n\r\nThanks!", "Thanks!!!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50019\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50019\">No</a>\n"]}, {"number": 50018, "title": "[tensorflow/python/ops/init_ops.py] RandomUniform docstring says `maxval` should default to 1\u2026 now it does", "body": "", "comments": ["It does default to 1.0 for floats:\r\n\r\n```\r\n>>> tf.compat.v1.initializers.random_uniform()(shape=[])\r\n<tf.Tensor: shape=(), dtype=float32, numpy=0.14846969>\r\n```\r\n\r\nThe None for `maxval` matches `tf.random.uniform`, and is important for throwing an error when `dtype` is integer and `maxval` isn't specified. You could maybe argue that initializers are more likely to be float-only so integers aren't very important, but given this is a 1.x-only API I'd lean toward leaving it as-is since we don't plan on further 1.x releases.", "@allenlavoie In that case the docstring needs to be updated. Because it isn't defaulting to `1.0`."]}, {"number": 50017, "title": "tf.linalg.eigh yields invalid eigensystem when used inside a decorated tf.function with jitcompile=True flag", "body": "\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 20.04 host using docker image tensorflow/tensorflow:2.5.0-gpu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN/A\r\n- TensorFlow installed from (source or binary):\r\ndocker image tag 2.5.0-gpu\r\n- TensorFlow version (use command below):\r\nv2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n\r\n- Python version:\r\n3.6.9\r\n- Bazel version (if compiling from source):\r\nN/A\r\n- GCC/Compiler version (if compiling from source):\r\nN/A\r\n- CUDA/cuDNN version:\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudart.so.11.2.152\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n- GPU model and memory:\r\nNVIDIA RTX 2070 8 GB\r\n\r\n**Describe the current behavior**\r\nUsing the jitcompile=True flag for a decorated function that uses tf.linalg.eigh returns an incorrect solution. Removing the flag yields a correct solution. Note that this did work in TF 2.4.1 using the same code with experimentalcompile=True. It also fails for when CUDA_VISIBLE_DEVICES=-1 is to disable GPU evaluation.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code snippet pasted below, which tests that the eigen system is a valid solution, should not raise an error. \r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you\r\nwant to contribute a PR? (yes/no): - Briefly describe your candidate solution\r\n(if contributing):\r\n\r\nPossibly, if I can get some guidance on where the issue might be. As it stands I wouldn't know where to begin.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n# fails both with and without this environment variable set to disable GPU evaluation\r\n# import os\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\na_mat = np.array([\r\n    [0., -1., -1.,  1.],\r\n    [-1.,  0.,  1., -1.],\r\n    [-1.,  1.,  0., -1.],\r\n    [1., -1., -1.,  0.]], dtype=np.float32)\r\n\r\n\r\n@tf.function(input_signature=[\r\n    tf.TensorSpec(None, dtype=tf.float32)],\r\n    jit_compile=False)\r\ndef eigh_uncompiled(arg):\r\n    return tf.linalg.eigh(arg)\r\n\r\n\r\n@tf.function(input_signature=[\r\n    tf.TensorSpec(None, dtype=tf.float32)],\r\n    jit_compile=True)\r\ndef eigh_compiled(arg):\r\n    return tf.linalg.eigh(arg)\r\n\r\n\r\nfor eigh in [np.linalg.eigh, eigh_uncompiled, eigh_compiled]:\r\n    val, vec = eigh(a_mat)\r\n    # assert A.x = lambda x for eigen system\r\n    print(tf.linalg.matmul(a_mat, vec) - val[tf.newaxis] * vec)\r\n    if not np.allclose(tf.linalg.matmul(a_mat, vec), val[tf.newaxis] * vec, atol=1e-6):\r\n        raise AssertionError(f'Test fails for function {eigh.__name__}')\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nN/A. Should be easy to reproduce with the above code snippet.\r\n", "comments": ["\r\nI was able to reproduce the issue in tf v2.5,v2.4 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/00bde45762a0c845b374e59752ef0530/50017.ipynb)", "As a followup, this particular test matrix seems to be a corner case but one that is actually encountered in my application. The test actually passes when the matrix under test is a randomly generated self-adjoint matrix.\r\n\r\nFor example,\r\n```\r\ng_mat = np.random.normal(0, 1, (4, 4)).astype(np.float32)\r\na_mat = g_mat*g_mat.T\r\n```\r\n\r\nIt fails again, however, if the diagonal of the random test matrix is set to all zeros, which might be a clue:\r\n```\r\ng_mat = np.random.normal(0, 1, (4, 4)).astype(np.float32)\r\na_mat = g_mat*g_mat.T\r\nfor n in range(4):\r\n    a_mat[n, n] = 0.\r\n```", "Hi everyone,\r\n\r\nJust jumping in to point out this bug is not present in TF 2.4 (`jit_compile` is not a valid option for `tf.function` in TF 2.4):\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\na_mat = np.array([\r\n    [0., -1., -1.,  1.],\r\n    [-1.,  0.,  1., -1.],\r\n    [-1.,  1.,  0., -1.],\r\n    [1., -1., -1.,  0.]], dtype=np.float32)\r\n\r\n\r\n@tf.function(input_signature=[\r\n    tf.TensorSpec(None, dtype=tf.float32)],\r\n    experimental_compile=False)\r\ndef eigh_uncompiled(arg):\r\n    return tf.linalg.eigh(arg)\r\n\r\n\r\n@tf.function(input_signature=[\r\n    tf.TensorSpec(None, dtype=tf.float32)],\r\n    experimental_compile=True)\r\ndef eigh_compiled(arg):\r\n    return tf.linalg.eigh(arg)\r\n\r\n\r\nfor eigh in [np.linalg.eigh, eigh_uncompiled, eigh_compiled]:\r\n    val, vec = eigh(a_mat)\r\n    # assert A.x = lambda x for eigen system\r\n    print(tf.linalg.matmul(a_mat, vec) - val[tf.newaxis] * vec)\r\n    if not np.allclose(tf.linalg.matmul(a_mat, vec), val[tf.newaxis] * vec, atol=1e-6):\r\n        raise AssertionError(f'Test fails for function {eigh.__name__}')\r\n```\r\n\r\n```\r\ntf.Tensor(\r\n[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\r\n [-2.9802322e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00]\r\n [-5.9604645e-08  0.0000000e+00  0.0000000e+00  0.0000000e+00]\r\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]], shape=(4, 4), dtype=float32)\r\ntf.Tensor(\r\n[[ 1.7881393e-07 -1.4901161e-08  0.0000000e+00 -2.3841858e-07]\r\n [-1.7881393e-07  0.0000000e+00 -1.4901161e-08  2.3841858e-07]\r\n [-1.4901161e-07  0.0000000e+00  0.0000000e+00  2.3841858e-07]\r\n [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  4.7683716e-07]], shape=(4, 4), dtype=float32)\r\ntf.Tensor(\r\n[[ 8.9406967e-08 -1.1920929e-07 -1.1920929e-07  2.3841858e-07]\r\n [-8.9406967e-08  1.1920929e-07 -1.1920929e-07 -2.3841858e-07]\r\n [-1.1920929e-07 -5.9604645e-08  0.0000000e+00 -1.1920929e-07]\r\n [ 1.1920929e-07 -5.9604645e-08  0.0000000e+00  1.1920929e-07]], shape=(4, 4), dtype=float32)\r\n```\r\n\r\nBased on the fact that `jit_compile` is an XLA thing, the issue probably somewhere in the `eigh` kernel generated by XLA? Though, the issue is not present in JAX:\r\n\r\n```python\r\nfrom jax import jit\r\nimport jax.numpy as jnp\r\nimport numpy as np\r\n\r\na_mat = np.array([\r\n    [0., -1., -1.,  1.],\r\n    [-1.,  0.,  1., -1.],\r\n    [-1.,  1.,  0., -1.],\r\n    [1., -1., -1.,  0.]], dtype=np.float32)\r\n\r\n@jit\r\ndef eigh_jit(arg):\r\n  return jnp.linalg.eigh(arg)\r\n\r\nval, vec = eigh_jit(a_mat)\r\nprint(jnp.matmul(a_mat, vec) - jnp.expand_dims(val, axis=0) * vec)\r\nif not np.allclose(jnp.matmul(a_mat, vec), jnp.expand_dims(val, axis=0) * vec, atol=1e-6):\r\n  raise AssertionError(f'Test fails for JAX')\r\n```\r\n\r\nBut I believe JAX uses a Custom Call for it's `eigh` implementation rather than something else.\r\n\r\nIs it possible this issue is related to the addition of `eigh_expander.h` by @hawkinsp in March (https://github.com/tensorflow/tensorflow/commits/master/tensorflow/compiler/xla/service/eigh_expander.h)?", "I agree, this is a bug. I will look into it. Something looks off about the 2x2 Jacobi rotations generated for this special case.\r\n\r\nNote that this code path isn't a particularly good one to be using on CPU or GPU, so even if I fix this issue, you should not expect good performance from it on those platforms. TF needs to do the same thing JAX does here (call into an optimized kernel).", "This should now be fixed at head.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50017\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50017\">No</a>\n"]}, {"number": 50016, "title": "TensorFlow Profiler stuck loading data", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): N/A\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Tesla T4, 16 GB\r\n\r\n**Describe the current behavior**\r\nWhen trying to use the TensorFlow Profiler on a GPU-enabled Colab notebook, the TensorBoard page displays the message \"Loading data\" and appears to be stuck there.\r\n<img width=\"1008\" alt=\"Screen Shot 2021-06-02 at 9 26 44 PM\" src=\"https://user-images.githubusercontent.com/12498403/120582805-bc161d80-c3f2-11eb-9367-68ec7f9795c5.png\">\r\n\r\n**Describe the expected behavior**\r\nThe TensorFlow Profiler should appear as a TensorBoard page.\r\n<img width=\"850\" alt=\"Screen Shot 2021-06-02 at 10 38 13 PM\" src=\"https://user-images.githubusercontent.com/12498403/120583129-4494be00-c3f3-11eb-9a70-a6720eb74de7.png\">\r\n\r\n**Standalone code to reproduce the issue**\r\nThis issue can be reproduced by running the TensorFlow Profiler tutorial notebook.\r\nhttps://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb\r\n", "comments": ["@aecelaya \r\nI did not face any loading issue when I reproduced the colab shared.Please find the [gist](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb#scrollTo=ZlRwCDoVinHV) .Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50016\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50016\">No</a>\n"]}, {"number": 50015, "title": "[TF-TRT] Argument bugfix", "body": "TF-TRT supports lowercase arguments.\r\nhttps://github.com/tensorflow/tensorflow/blob/9460c57fb5fb112e1e6d03a0221799d71b042926/tensorflow/python/compiler/tensorrt/trt_convert.py#L100-L110\r\n\r\nFrom official documentation API instruction,\r\nhttps://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#int8-quantization\r\n\r\n<br>\r\n\r\n```precision_mode```\r\nDefault value is TrtPrecisionMode.FP32. This is one of TrtPrecisionMode.supported_precision_modes(), in other words, FP32, FP16 or INT8 (lowercase is also supported).\r\n\r\nBut INT8 calibration with ```precision_mode='int8'``` didn't work before.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50015) for more info**.\n\n<!-- need_sender_cla -->", "> Thanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\r\n> \r\n> \ud83d\udcdd **Please visit https://cla.developers.google.com/ to sign.**\r\n> \r\n> Once you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\r\n> \r\n> #### What to do if you already signed the CLA\r\n> ##### Individual signers\r\n> * It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> \r\n> ##### Corporate signers\r\n> * Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\r\n> * The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\r\n> * The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50015) for more info**.\r\n\r\n@googlebot I signed it!"]}, {"number": 50014, "title": "ValueError: inputs must be an iterable of at least one Tensor/IndexedSlices with the same dtype and shape", "body": "\r\n```\r\n>>> for epoch in range(3):\r\n...         for inputs in train2.batch(1):\r\n...             inputs=[tf.squeeze(k) for k in inputs]\r\n...             with tf.GradientTape() as tape:\r\n...                 loss=model.train_step(inputs)\r\n```\r\n\r\ncodes similar as the [issue](https://github.com/tensorflow/tensorflow/issues/49907), but the bug is not\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 5, in <module>\r\n  File \"/data/logs/xulm1/debiasing_rush/tf2_dist_data2.py\", line 145, in train_step\r\n    loss=self.loss_function(next_item,logits)\r\n  File \"/data/logs/xulm1/debiasing_rush/tf2_dist_data2.py\", line 132, in loss_function\r\n    self.loss_train = self.loss + tf.add_n([tf.nn.l2_loss(v) for v in tf.compat.v1.trainable_variables()]) * self.l2#\u6700\u540e\u4e00\u9879\u5f71\u54cd\u8f83\u5927\r\n  File \"/data/logs/xulm1/myconda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/data/logs/xulm1/myconda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py\", line 3497, in add_n\r\n    raise ValueError(\"inputs must be an iterable of at least one \"\r\nValueError: inputs must be an iterable of at least one Tensor/IndexedSlices with the same dtype and shape\r\n```\r\n\r\ncould you please help me ?\r\nthx\r\n", "comments": ["```\r\n>>> inputs\r\n[<tf.Tensor: shape=(512, 19, 19), dtype=float32, numpy=...>, <tf.Tensor: shape=(512, 19, 19), dtype=float32, numpy=..., dtype=float32)>, <tf.Tensor: shape=(512, 19), dtype=int32, numpy=..., dtype=int32)>, <tf.Tensor: shape=(512,), dtype=int32, numpy=..., dtype=int32)>, <tf.Tensor: shape=(512,), dtype=int32, numpy=...>\r\n```\r\nso what's the problem ?\r\nthx\r\n", "Now I find the problem ,\r\n`tf.add_n([tf.nn.l2_loss(v) for v in tf.compat.v1.trainable_variables()]) * self.l2`\r\nbut how to modify the up code in tf2 version ? \r\nI want the all trainable variables !\r\nplease help me ?\r\nthx\r\n", "and how to change the code down, same as up\r\n```\r\ngradients = tape.gradient(loss, tf.compat.v1.trainable_variables())\r\nself.optimizer.apply_gradients(zip(gradients, tf.compat.v1.trainable_variables()))\r\n```\r\n", "@ucasiggcas ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal code snippet or gist and the TensorFlow version you are using.\r\n\r\nThanks!", "hi\uff0cdear\r\nthe problem is how to translate the code to tf2,\r\n`tf.compat.v1.trainable_variables()`", "@ucasiggcas \r\nplease refer to the example in [this link](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)which explains everything from scratch in detail.\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50014\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50014\">No</a>\n"]}, {"number": 50013, "title": "CherryPick:2.3 Fix tf.io.decode_raw bugs", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50013) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50013) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 50012, "title": "CherryPick:2.4 Fix tf.io.decode_raw bugs", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50012) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50012) for more info**.\n\n<!-- need_author_consent -->"]}, {"number": 50011, "title": "CherryPick:2.2 Fix tf.io.decode_raw bugs", "body": null, "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50011) for more info**.\n\n<!-- need_author_consent -->", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F50011) for more info**.\n\n<!-- need_author_consent -->"]}]