[{"number": 2473, "title": "Dynamic use of tensorflow.python.ops.array_ops.space_to_batch", "body": "When calling \"tensorflow.python.ops.array_ops.space_to_batch\" in Python I need to specify the padding at compile time. In my use case I calculate the padding in the computation graph. Therefore it fails when I use a tensor as padding. I attached the stacktrace:\n\n```\n/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc in space_to_batch(input, paddings, block_size, name)\n   1636   result = _op_def_lib.apply_op(\"SpaceToBatch\", input=input,\n   1637                                 paddings=paddings, block_size=block_size,\n-> 1638                                 name=name)\n   1639   return result\n   1640 \n\n/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    691           op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    692                            input_types=input_types, attrs=attr_protos,\n--> 693                            op_def=op_def)\n    694           outputs = op.outputs\n    695           return _Restructure(ops.convert_n_to_tensor(outputs),\n\n/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   2186                     original_op=self._default_original_op, op_def=op_def)\n   2187     if compute_shapes:\n-> 2188       set_shapes_for_outputs(ret)\n   2189     self._add_op(ret)\n   2190     self._record_op_seen_by_control_dependencies(ret)\n\n/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)\n   1640       raise RuntimeError(\"No shape function registered for standard op: %s\"\n   1641                          % op.type)\n-> 1642   shapes = shape_func(op)\n   1643   if len(op.outputs) != len(shapes):\n   1644     raise RuntimeError(\n\n/home/user/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc in _SpaceToBatchShape(op)\n   1630 \n   1631   paddings = tensor_util.constant_value(op.inputs[1])\n-> 1632   if (paddings[0, 0] < 0 or paddings[0, 1] < 0 or\n   1633       paddings[1, 0] < 0 or paddings[1, 1] < 0):\n   1634     raise ValueError(\"paddings cannot be negative.\")\nTypeError: 'NoneType' object has no attribute '__getitem__'\n```\n\nI think it has something to do with the Python shape op. If so how can I override the existing shape op to test a new one with dynamic calculation?\n", "comments": ["It's a bug indeed.\n\nOn Mon, May 23, 2016 at 11:03 AM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Assigned #2473 https://github.com/tensorflow/tensorflow/issues/2473 to\n> @touts https://github.com/touts.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2473#event-669223214\n", "Any Idea how to fix it? If it's just the python shape function I can submit a patch. I want to test it first, so how to set a different shape function temporary?\n", "Alas TensorFlow does not let you replace a shape function by another one,\nso you cannot set one temporarily.\n\nYou'll have to edit the current shape function in python/ops/array_ops.py\nand either run from the python source or rebuild tensorflow and run from\nthe compiled binary.\n\nOn Wed, May 25, 2016 at 12:26 AM, ThomasWollmann notifications@github.com\nwrote:\n\n> Any Idea how to fix it? If it's just the python shape function I can\n> submit a patch. I want to test it first, so how to set a different shape\n> function temporary?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2473#issuecomment-221493918\n", "I just sent a fix out for review.\n"]}, {"number": 2472, "title": "distributed tensorflow mnist example hang at prepare_or_wait_for_session() ", "body": "I tried both Distributed [tensorflow](https://www.tensorflow.org/versions/r0.8/how_tos/distributed/index.html) and [minist_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt). They both hang at the prepare_or_wait_for_session() function with non-chief worker tasks.\n\ntask_index == 0(chief worker ) can run as expected.\nI can not find out the reasion why it behaves like this. \nCan anyone please give some advice?\n", "comments": ["We'll need some more information to debug this. How are you invoking the programs when they hang? What logs do you see from each process, if any?\n", "as for the [minis_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt),  on my machine I name the source code file 'distributed_test.py'. The code are the same as mnist_softmax.py.txt,  I run the ps server on one machine using command: <br><pre><code>_python ./distribute_test.py --ps_hosts=100.193.128.27:2222 --worker_hosts=100.193.128.20:2223,100.193.128.20:2224 --job_name=ps --task_index=0_  </code></pre>  then on  100.193.128.20, I run two processes using commands: <br><pre><code>_python ./distribute_test.py --ps_hosts=100.193.128.27:2222 --worker_hosts=100.193.128.20:2223,100.193.128.20:2224 --job_name=worker --task_index=0_</code></pre>    and<br><pre><code> _python ./distribute_test.py --ps_hosts=100.193.128.27:2222 --worker_hosts=100.193.128.20:2223,100.193.128.20:2224 --job_name=worker --task_index=1_ </code></pre>  the job 0 runs as expected, the job 1 hang,  logs look like this:<br><pre>I0524 09:30:49.453812186   25095 socket_utils_common_posix.c:139] Disabling AF_INET6 sockets because socket() failed.\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {100.193.128.27:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {100.193.128.20:2223, localhost:2224}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2224  </pre>  **After job 0 ends, job 1 's logs looks like this:**<br><pre> E0524 09:35:43.322394911   25302 tcp_client_posix.c:173]     failed to connect to 'ipv4:100.193.128.20:2223': socket error: connection refused</pre>\n", "Are there some requirements for installing and building tensorflow, I installed on centOS 7.2 using pip install following the instructions on tensorflow.org .  Is it necessary for me to build it from source in order to run the distributed tensor flow ? Or is it necessary to build from the newest source in order to run distributed tensor flow?\n", "I added some print() calls at supervisor.py and session_manager.py in the source code of tensorflow,  then I found the reason why it hangs at prepare_or_wait_for_session():<br>\n\nprepare_or_wait_for_session calls wait_for_session, The wait_for_session function in session_manager.py calls _mode_not_ready() function, and the _mode_not_ready() function returns a Exception, Here it is:<br>\n\n<pre><code>Attempting to use uninitialized value global_step\n     [[Node: global_step/read = Identity[T=DT_FLOAT, _class=[\"loc:@global_step\"], _device=\"/job:ps/replica:0/task:0/cpu:0\"](global_step)]]\n     [[Node: Rank_2_S1 = _HostRecv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/gpu:0\", send_device=\"/job:ps/replica:0/task:0/cpu:0\", send_device_incarnation=6943251664905551597, tensor_name=\"edge_12_Rank_2\", tensor_type=DT_INT32, _device=\"/job:worker/replica:0/task:1/gpu:0\"]()]]\n     [[Node: pack_G3 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device=\"/job:worker/replica:0/task:1/gpu:0\", send_device_incarnation=-7346167572165274731, tensor_name=\"edge_5_pack\", tensor_type=DT_INT32, _device=\"/job:worker/replica:0/task:1/cpu:0\"]()]]\nCaused by op u'global_step/read', defined at:\n  File \"./distribute_test.py\", line 96, in <module>\n    tf.app.run()\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"./distribute_test.py\", line 93, in main\n    run_training(server, cluster_spec, num_workers)\n  File \"./distribute_test.py\", line 32, in run_training\n    initializer = tf.constant_initializer(0), trainable = False)\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 339, in get_variable\n    collections=collections)\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 262, in get_variable\n    collections=collections, caching_device=caching_device)\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 158, in get_variable\n    dtype=variable_dtype)\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 209, in __init__\n    dtype=dtype)\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/ops/variables.py\", line 318, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 609, in identity\n    return _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/data0/guohongyan/tools/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack() </code></pre>\n\nthe source code I run is:  [mnist_softmax.py.txt](https://github.com/tensorflow/tensorflow/files/271770/mnist_softmax.py.txt) <br>\nIt seems like the ops in non-chief tasks are not initialized as expected!  \n", "@s0okiym @mrry  Is there any news on how to fix this? I am experiencing the exact same problem.\n", "I had the same problem as well, can somebody tell how to fix this? \n", "Found the problem. All variables in non-chief worker should be initialized by chief worker.\n", "Awesome! @perhapszzy any chance you could give a working code example of your solution? \n", "I'm putting all my codes into this repo:\n\nhttps://github.com/caicloud/tensorflow-demo/tree/master/distributed\n\nmnist_dnn.py and mnist_cnn.py should be working.\n", "I was using timestamp as name_scope to avoid re-use of parameters between two different runs, that doesn't work and now I' using tf.reset_default_graph(), this works well.\n", "@perhapszzy I have similar problem to yours. Basicaly, I'm trying to set different files as data source in each worker. I am therefore setting up worker specific variable scope for my dataset. What does reseting default graph in chief worker do? Would you like to share your experience with this. \r\n\r\n    with tf.variable_scope('data_source_task_index_%d' % FLAGS.task_index):\r\n        batch_generator = MyCustomDataSet(FLAGS.batch_size)"]}, {"number": 2471, "title": "per_process_gpu_memory_fraction didn`t work", "body": "per_process_gpu_memory_fracion Option didn't work in follow code.\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\nsess = sv.prepare_or_wait_for_session(server.target,                                            config=tf.ConfigProto(gpu_options=gpu_options))\n", "comments": ["@KyungJunAn Could you please clarify how it doesn't work?\n", "for example my total GPU Memory Size is 4G\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n\n=> tensorflow use 2G GPU memory only \nbut,\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4)\nwith sv.prepare_or_wait_for_session(server.target, config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n\n=> tensorflow use 4G GPU memory\n", "@zffchen78: Could you take a look at this since Sherry is out? \n", "Hi, I am experiencing the same thing:\n\nUbuntu 16 / tf built from master Jun 14th / CUDA 8 / GTX 1080\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3, allow_growth=True)\nconfig = tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement=True)\n\n+------------------------------------------------------+  \n| NVIDIA-SMI 367.18     Driver Version: 367.18         |  \n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Graphics Device     Off  | 0000:01:00.0      On |                  N/A |\n| 55%   82C    P0   137W / 180W |   7835MiB /  8109MiB |     90%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n", "#3057 worked for me. TL;DR: pass the config object when you initialize `tf.Server`.\n", "Closing due to inactivity. I'm assuming you did the same as @daeyun .", "#8021\r\nCheck if you are running `pywrap_tensorflow.list_devices()` before setting the limitation for your process.", "HI @daeyun, i have met the same problem. But I didn't define any `tf.train.Server` object. And my code is like `sess = sv.prepare_or_wait_for_session(config=tf.ConfigProto(gpu_options=gpu_options))` directly. So where should I pass the `config` object to??? Thanks!", "Hi,\r\n\r\nThe instrucction per_process_gpu_memory_fraction seems not working on my computer. \r\n\r\nI have Quadro K1200, 425/4039 MiB aprox, driver 387.12, cuda 8, cudnn 6,  tensorflow 1.4 install from anaconda. python 3.5.2 (64 bits).\r\n\r\nmy code goes like this:\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\r\nwith tf.Graph().as_default():\r\n        with tf.Session(config=config) as sess:\r\n            ...\r\n\r\n\r\nAnd I get:\r\n\r\n2017-11-04 13:55:44.926218: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 3.94G (4235198464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-11-04 13:55:44.933243: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 3.55G (3811678464 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n\r\nhow can i check if I am running pywrap_tensorflow.list_devices()?", "Meet the same issue, hope somebody could help", "This is an old issue but I hope it is still helpful to someone. \r\n\r\nI believe this is not a problem with tensorflow api. \r\n\r\n@zendelcrow \r\nFrom what I am seeing, your training hits OOM since you only allowed 0.4 of your total gpu memory --- which is much much smaller than what your model demands. Please set `allow_growth` or use 1.0 fraction to fully utilize your gpu, and possibly a smaller batch_size would also reduce memory requirement. \r\n@cshanbo hope it helps"]}, {"number": 2470, "title": "per_process_gpu_memory_fraction didn'", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": []}, {"number": 2469, "title": "Add x permission to parameterized_docker_build.sh", "body": "", "comments": []}, {"number": 2468, "title": "Learning rates based on Karpathy CS231n lecture 6", "body": "Continuation of #1512 \n@vrv seems like I can test now.  Updated the pull request as you suggested.  Please let me know if you have any comments.\n", "comments": ["Can one of the admins verify this patch?\n", "I find it helpful to phrase commit titles in the imperative, with a verb that describes what merging that commit will do. Basically I try to write commit messages that can complete this sentence: Adding this commit will --do something useful--. Using this commit as an example, it would be helpful for me to know what's being done with the learning rates -- is a new library method being added, or is it becoming the new default learning rate, etc.\n\nI'm a new guy and I submit this as a humble comment. Just doing research on Tensor Flow because I'd like to make some contributions this summer.\n", "@vrv Is this what you mean?\n", "Great!\n\n@tensorflow-jenkins test this please\n"]}, {"number": 2467, "title": "conv3d_backprop_input", "body": "I'm using `conv3d_backprop_input` to implement a 3d version of `tf.nn.conv2d_transpose`. Unlike `conv2d_backprop_input`, `conv3d_backprop_input` requires an `input` tensor (rather than just `input_shape`), and this seems a little inconsistent. I don't have the input tensor because it is actually the deconvolution output in this case. My current solution is to create a dummy tensor like the following:\n\n``` python\ndef conv3d_transpose(value, filter, output_shape, strides, padding='SAME', name=None):\n  dummy_tensor = tf.zeros(shape=output_shape, dtype=value.dtype)\n\n  deconv = tf.nn.conv3d_backprop_input(input=dummy_tensor, filter=filter,\n      out_backprop=value, strides=strides, padding=padding, name=name)\n\n  return deconv\n```\n\nSo my question is, in `conv3d_backprop_input`, do the values of `input` actually matter or is it only used to infer the shape of the output `deconv`?\n", "comments": ["From the [implementation](https://github.com/tensorflow/tensorflow/blob/7b4d733593842361d066d7e33a03a07da5dca465/tensorflow/core/kernels/conv_grad_ops_3d.cc#L356) it looks like the `input` tensor is only used for shape information, so your dummy tensor approach should work fine. \n", "We should aim for parity with conv2d then.  \n", "Cc @mjanusz\n", "Also, from my limited experience, it seems that a gradient needs to be defined for conv3d_backprop_input. \n\nI am getting this error:\nLookupError: No gradient defined for operation 'Conv3DBackpropInput' (op type: Conv3DBackpropInput)\n", "Looks like this was fixed with Conv3DBackpropInputV2?", "@daeyun did @vincentvanhoucke 's suggestion work?", "Closing due to inactivity. "]}, {"number": 2466, "title": "Feature request: tensordot", "body": "I found one useful tensor operation \"tensordot\" is missing in tensorflow. [Theano has an implementation of it](http://deeplearning.net/software/theano/library/tensor/basic.html). It would be great to have tensordot also in tensorflow.\n", "comments": ["Duplicate of #216 \n", "@vrv tensordot is related to but not exactly the same as a broadcasting matmul -- with tensordot, there is an explicit axes argument.\n", "`tensordot` can contract multiple indices at the same time which is different with what mentioned in #216 \n", "Agreed, I just wanted to have one issue that tracks them both, since they are closely related and we already have a billion issues.\n"]}, {"number": 2465, "title": "Multidimennsionnal input for LSTM Issues", "body": "I have a series of multdimensionnal time series as follow:\n\nInput Xi   i=1..N samples\nXi=[y1,..yk..., yT] K=1..T , yk is a vector, T= 50 (sequence length) with yk= [yk1, ... ykm] m=3 with ykj : Float or Int\n\nUsing LSTM Tensor Flow,would like to predict the next step (T+1), given the training of samples like :\n\nXi=[y1,...., yT]`\nThe current code gives input form for m=1 (unidimensionnal Y),\n\ninput_data = tf.placeholder(tf.float32, [batchSize, numSteps, numInputs])\ntargets = tf.placeholder(tf.float32, [batchSize, numSteps, numInputs])\nWondering if there is a way to input data if m=2, 3 in the LSTM tensor flow ?\n\nIf not, is there a workaround to input multdimensionnal time series ?\n\nOr new devs needs to be done in RNN ?\n", "comments": ["This is a question more appropriate to StackOverflow, unless there is a very concrete feature request here.\n", "It means this is not possible.\n\nCan you add this feature in tensor flow ?\n\nThanks, regards\n\nOn 24 May 2016, at 04:05, Vijay Vasudevan notifications@github.com wrote:\n\nThis is a question more appropriate to StackOverflow, unless there is a very concrete feature request here.\n\n\u2015\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n", "@ebrevdo, @yuanbyu  to let me know if this is a feasible feature request\n", "What is the rank and size of the states that you plan to pass around?\n", "Hello,\nFor example\nInput Xi i=1..N samples\n     Xi=[y1,..yk..., yT]                      T= 50 (sequence length) \n     with yk= [yk1, ... ykm]      \n\nSo, we have \nXi size :   (m,  1toK_char_size, NsequenceSize)\n   i=1... Nsample\n   sequenceSize= 50 or 100\n   1toK_char_size=  50 \n   m= 1 or 2 or 3 (max)\n\nTensor Ranks is  3  (4 if we account with the samples).\nNow, size of the tensor is just:\n    m=1,        (1toK_char_size, NsequenceSize)\n\nSo m>1 , it would be very helpful for new architectures.\nThanks for replying.\n", "You're talking about the input shapes, not the state shapes. rnns accept inputs and emit outputs and state. I'm asking about the shape of the state.\n", "Thank you for your question.\nIt would depend if this Grid LSTM or standard LSTM....\n\nThere is the initial transformation:\n    S= W . Input\n         Input= [Xt, ht]\n\n```\n(ht_o, mt_o) = LSTM(ht, mt, W)\n```\n\nMy questions is :  \n 1) Can TensorFlow RNN accept this Input Tensor rank (3 to 5) ?\n                   (Nsample, m, 1toK_char_size, NsequenceSize)\n\n 2) The input can be transformed into sub-space through W.....\n      But, in order to keep the initial input rank, state dimension would be:\n                      ht:  (m2,  d2)     hidden state\n                      mt:  (m2, d2)     memory cell\n\n```\n   (Usually, ht is  vector  of size d   for standard LSTM).\n\n   Is this feasible ?\n```\n"]}, {"number": 2464, "title": "Inconsistent tensor shapes in documentation for tf.nn.sampled_softmax_loss", "body": "Hi,\n\nIn the documentation for tf.nn.sampled_softmax_loss (as viewed [here](https://www.tensorflow.org/versions/master/api_docs/python/nn.html#sampled_softmax_loss)) it says the equivalent full softmax probabilities can be computed using `tf.nn.softmax(tf.matmul(inputs, weights) + biases)`. However, under 'Args' it also states that `inputs` has shape `batch_size, dim` and `weights` has shape `[num_classes, dim]` \u2014 The dimensions of these tensors are incompatible for matrix multiplication.\n\nI suspect that the _actual_ equivalent operation would be something like `tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases)` but I'm not 100% sure.\n", "comments": ["@girving is this just a doc bug?\n", "Yep, that's a doc bug.  It should be `tf.nn.softmax(tf.matmul(inputs, tf.transpose(weights)) + biases)` as suggested.  Fixing.\n"]}, {"number": 2463, "title": "Using `tf.nn.rnn_cell.LSTMCell` with `tf.nn.rnn` breaks when `state_is_tuple` is True", "body": "### Environment info\n\nOperating System: mac\n\nIf installed from sources, provide the commit hash: 7d9ab3eb485e6eb1778bad4ef01a1cd95b2d22d9\n### Steps to reproduce\n\nCreating an rnn based on a list of cell specified by `MultiRNNCell` breaks at the `zero_state` function:\n\n``` python\nnum_units = 3\ninput_size = 5\nnum_proj = 4\nmax_length = 8\nsequence_length = [4, 6]\ninitializer = tf.random_uniform_initializer(-0.01, 0.01)\ninputs = max_length * [tf.placeholder(tf.float32, shape=(None, input_size))]\ninputs_c = tf.pack(inputs)\ncell = tf.nn.rnn_cell.LSTMCell(num_units, input_size, use_peepholes=True,\n                               num_proj=num_proj, initializer=initializer, \n                               state_is_tuple=True)\ncells = tf.nn.rnn_cell.MultiRNNCell([cell] * 2, state_is_tuple=True)\noutputs_static, state_static = tf.nn.rnn(cells, inputs, dtype=tf.float32, \n                                         sequence_length=sequence_length)\n```\n\nThe same model definition would work if `state_is_tuple=False`, the code break at this [line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L107)\n\nThe same code would work if we pass the cell directly to `tf.nn.rnn` i.e. without using `tf.nn.rnn_cell.MultiRNNCell`, the reason is that `tf.nn.rnn_cell.MultiRNNCell` would have a `state_size` value as tuple of tuples, in my example the value of the `state_size` is `<type 'tuple'>: ((3, 4), (3, 4))`\n### What have you tried?\n\n.1. Tried the same model definition without `state_is_tuple=False`, which works but shows a deprication warning.\n\n``` python\nnum_units = 3\ninput_size = 5\nnum_proj = 4\nmax_length = 8\nsequence_length = [4, 6]\ninitializer = tf.random_uniform_initializer(-0.01, 0.01)\ninputs = max_length * [tf.placeholder(tf.float32, shape=(None, input_size))]\ninputs_c = tf.pack(inputs)\ncell = tf.nn.rnn_cell.LSTMCell(num_units, input_size, use_peepholes=True,\n                               num_proj=num_proj, initializer=initializer)\ncells = tf.nn.rnn_cell.MultiRNNCell([cell])\noutputs_static, state_static = tf.nn.rnn(cells, inputs, dtype=tf.float32, \n                                         sequence_length=sequence_length)\n```\n\n.2. Tried the same model definition with `state_is_tuple=True` and without `tf.nn.rnn_cell.MultiRNNCell`, which works.\n\n``` python\nnum_units = 3\ninput_size = 5\nnum_proj = 4\nmax_length = 8\nsequence_length = [4, 6]\ninitializer = tf.random_uniform_initializer(-0.01, 0.01)\ninputs = max_length * [tf.placeholder(tf.float32, shape=(None, input_size))]\ninputs_c = tf.pack(inputs)\ncell = tf.nn.rnn_cell.LSTMCell(num_units, input_size, use_peepholes=True,\n                               num_proj=num_proj, initializer=initializer, \n                               state_is_tuple=True)\noutputs_static, state_static = tf.nn.rnn(cell, inputs, dtype=tf.float32, \n                                         sequence_length=sequence_length)\n```\n### Logs or other output that would be helpful\n\n```\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py\", line 123, in rnn\n    state = cell.zero_state(batch_size, dtype)\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 109, in zero_state\n    for s in state_size)\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py\", line 109, in <genexpr>\n    for s in state_size)\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 249, in pack\n    return gen_array_ops._pack(values, name=name)\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1126, in _pack\n    result = _op_def_lib.apply_op(\"Pack\", values=values, name=name)\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2242, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1696, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 439, in _PackShape\n    input_shape = input_shape.merge_with(inp.get_shape())\n  File \"/Users/mourad/.virtualenvs/ca/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 554, in merge_with\n    (self, other))\nValueError: Shapes () and (2,) are not compatible\n```\n", "comments": ["I am experiencing the same issue.\nAs far as I explored, it occurs because of the current implementation of `zero_state`.\nWhen `state_is_tuple` flag is set to false, the cell state and the hidden state of a recurrent layer is concatenated, and the shape is `(2 * num_layers,)`.\nHowever, when the flag is on, the cell state and the hidden state is passed separately, and the shape is `(num_layers, num_layers)` (one for the cell state, another for the hidden state).\n1. Without `MultiRNNCell`\n   In this case, `state_size` would be `(num_layers, num_layers)`.\n   So line 107-109 of the current implementation would make a length-2 tuple of `(batch_size, num_layers)`-shape zero tensors.\n2. With `MultiRNNCell`\n   In this case, `state_size` would be `((num_layers, num_layers), ..., (num_layers, num_layers))` (whose length is `num_layers`).\n   So, line 107-109 of the current implementation causes error, because `s` in line 109 is a tuple `(num_layers, num_layers)`, where a single number is expected.\n\nIf you configure that setting the flag of a single `LSTMCell` to `False` and that of a `MultiRNNCell` to `True`, it would work well, although warnings would occur and there would be some degradation on performance. (`num_layer + 1` warnings if you set both flags to `False`, and (`num_layer` warnings if you turn on only `MultiRNNCell`'s flag.)\n\nI think it can be fixed by fixing as follows: if the type of `state_size` is tuple of tuple, initialize for all single numbers in the tuple of tuple and return with the same form.\n", "After a bit more exploration, I found that it works in case that both flags are on, if I apply `zero_state` for each layer manually and then make a tuple of the results.\nHowever, I think `zero_state` should be modified, since it is specified in the abstract class `RNNCell` and `MultipleRNNCell` is a subclass of that abstract class.\nI made a PR modifying this issue. Since I started studying TensorFlow yesterday, my codes might be awkward!\n", "This has been fixed internally and the next public push should have the fix.\nOn May 24, 2016 8:33 AM, \"Jihun Choi\" notifications@github.com wrote:\n\n> After a bit more exploration, I found that it works in case that both\n> flags are on, if I apply zero_state for each layer manually and then make\n> a tuple of the results.\n> However, I think zero_state should be modified, since it is specified in\n> the abstract class RNNCell and MultipleRNNCell is a subclass of that\n> abstract class.\n> I made a PR modifying this issue. Since I started studying TensorFlow\n> yesterday, my codes might be awkward!\n> \n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2463#issuecomment-221309662\n", "@ebrevdo: Is this fixed?\n", "Seems this is fixed.  @jihunchoi feel free to reopen if you're still having issues.\n", "@ebrevdo  I am facing a similar issue , how do I migrate my code to v1.1 from 0.9 , \r\n```\r\n        #cell = tf.nn.rnn_cell.LSTMCell(self.num_input, self.num_input)\r\n        cell = tf.contrib.rnn.BasicLSTMCell(self.num_input, self.num_input)\r\n        #print(_X.shape)\r\n        state = _istate\r\n        for step in range(self.num_steps):\r\n            outputs, state = tf.nn.rnn(cell, [_X[step]], state)\r\n            #outputs, state = tf.contrib.rnn.BasicRNNCell(cell, _X[step], state)\r\n            tf.get_variable_scope().reuse_variables()\r\n        return outputs\r\n```\r\nbut I am getting the following error : \r\n```\r\n    outputs, state = tf.nn.rnn(cell, [_X[step]], state)\r\nAttributeError: 'module' object has no attribute 'rnn'\r\n*** Error in `python': free(): invalid pointer: 0x0000000000e032f0 ***\r\nAborted (core dumped)\r\n```"]}, {"number": 2462, "title": "Why is there no support for directly computing cross entropy?", "body": "I see that we have methods for computing softmax and sigmoid cross entropy, which involve taking the softmax or sigmoid of the logit vector and then computing cross entropy with the target, and the weighted and sparse implementations of these. But what if I simply want to compute the cross entropy between 2 vectors?\n", "comments": ["We provide optimized cross-entropy implementations that are fused with the softmax/sigmoid implementations because their performance and numerical stability are critical to efficient training.\n\nIf however you are just interested in the cross entropy itself, you can compute it directly using code from the [beginners tutorial](https://www.tensorflow.org/versions/r0.8/tutorials/mnist/beginners/index.html):\n\n``` python\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n```\n\n**N.B.** DO NOT use this code for training. Use [`tf.nn.softmax_cross_entropy_with_logits()`](https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#softmax_cross_entropy_with_logits) instead.\n", "It would be great if the \"numerical unstability\" with the naive implementation was explained somewhere - I am trying to learn this stuff, and I wonder what that implies.", "@stolsvik Take a look at [this post](http://www.nowozin.net/sebastian/blog/streaming-log-sum-exp-computation.html) to get some insight", "I found this implementation from the [Keras code](https://github.com/fchollet/keras/blob/master/keras/backend/tensorflow_backend.py#L2717). As far as I could get, they just clip the `output` (i.e. the softmax) between `EPSILON` and `1-EPSILON`, perform a `log` operation on it and us the result as the logits. Could make sense?  \r\n  \r\n```python\r\ndef sparse_categorical_crossentropy(output, target, from_logits=False):\r\n    \"\"\"Categorical crossentropy with integer targets.\r\n    # Arguments\r\n        output: A tensor resulting from a softmax\r\n            (unless `from_logits` is True, in which\r\n            case `output` is expected to be the logits).\r\n        target: An integer tensor.\r\n        from_logits: Boolean, whether `output` is the\r\n            result of a softmax, or is a tensor of logits.\r\n    # Returns\r\n        Output tensor.\r\n    \"\"\"\r\n    # Note: tf.nn.softmax_cross_entropy_with_logits\r\n    # expects logits, Keras expects probabilities.\r\n    if not from_logits:\r\n        epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\r\n        output = tf.clip_by_value(output, epsilon, 1 - epsilon)\r\n        output = tf.log(output)\r\n\r\n    output_shape = output.get_shape()\r\n    targets = cast(flatten(target), 'int64')\r\n    logits = tf.reshape(output, [-1, int(output_shape[-1])])\r\n    res = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n        labels=targets,\r\n        logits=logits)\r\n    if len(output_shape) == 3:\r\n        # if our output includes timesteps we need to reshape\r\n        return tf.reshape(res, tf.shape(output)[:-1])\r\n    else:\r\n        return res\r\n```\r\n  \r\n*EDIT*: of course it makes sense, my question should be read as: could this solution make sense from the point of view of the numerical stability and efficiency?\r\n", "Then in case that my output of the last layer is softmax activated, I have to manually do the cross entropy?\r\nThen I don't understand why it is not supported because many models pick the output based on probabilities from softmax activation. \r\nAlso, if you compute the loss from softmax output with sparse_softmax_cross_entropy_with_logits it will be inaccurate.", "Is it possible to add the `cross_entropy_with_logits()`? `tf.nn.softmax_cross_entropy_with_logits()` can work for the performance, but it makes codes difficult to read and implement. Thank you in advance. @mrry  ", "`cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))`\r\n\r\nreturns nan when predict class has probability zero ", "Is clipping with `eps` still not make solution stable?\r\n\r\nExample:\r\n\r\n~~~\r\n# Array of samples\r\ny_pred_arr = np.array([[0.4,0.3,0.3], [1.0,0.0,0.0], [0.0,0.0,1.0]], np.float32)\r\ny_true_arr = np.array([[1,0,0], [1,0,0], [0,0,1]], np.float32)\r\n\r\nprint('y_pred_arr.shape', y_pred_arr.shape)\r\nprint('y_true_arr.shape', y_true_arr.shape)\r\n\r\n# 1 : Implemented in numpy\r\ndef cross_entropy_loss(y_pred_arr, y_true_arr, eps=1e-6):\r\n    p_arr = np.clip(y_pred_arr, eps, 1 - eps)\r\n    n_samples = p_arr.shape[0]\r\n    n_classes = p_arr.shape[1]\r\n    loss_arr = np.zeros((n_samples,), np.float32)\r\n    for i in range(n_samples):\r\n        for c in range(n_classes):\r\n            loss_arr[i] += -y_true_arr[i][c] * np.log(p_arr[i][c])\r\n    loss = np.mean(loss_arr)\r\n    return loss\r\nloss = cross_entropy_loss(y_pred_arr, y_true_arr)\r\nprint(loss)\r\n\r\n# 2 : Implemented in sklearn\r\nfrom sklearn.metrics import log_loss\r\nloss = log_loss(np.argmax(y_true_arr,axis=1), y_pred_arr, labels=[0,1,2], eps=1e-6)\r\nprint(loss)\r\n\r\n# 3 : Implemented in tensorflow\r\ny_pred_tf = tf.convert_to_tensor(y_pred_arr, np.float32)\r\ny_true_tf = tf.convert_to_tensor(y_true_arr, np.float32)\r\neps = 1e-6\r\ncliped_y_pref_tf = tf.clip_by_value(y_pred_tf, eps, 1-eps)\r\nloss_tf = tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(cliped_y_pref_tf), axis=1))\r\nwith tf.Session() as sess:\r\n    loss = sess.run(loss_tf)\r\n    print(loss)\r\n~~~\r\n\r\nOutput:\r\n~~~\r\ny_pred_arr.shape (3, 3)\r\ny_true_arr.shape (3, 3)\r\n0.30543092\r\n0.30543154478209544\r\n0.30543092\r\n~~~", "Just wanted to add that a better implementation might be\r\n\r\n```python\r\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(tf.math.xlogy(y_,  y), reduction_indices=[1]))\r\n```\r\n\r\nwhich will give `0` instead of `nans` where both `y_` and `y` are zero.\r\n\r\n\r\n"]}, {"number": 2461, "title": "Server restarts silently on kubernetes and variables reset after restart", "body": "### Environment info\n\nOperating System:\nKubernetes with default images\n### Steps to reproduce\n\nHard to reproduce due to lack of log\n### Logs or other output that would be helpful\n\nThe kubernetes status:\n\n```\nNAME               READY     STATUS    RESTARTS   AGE\ntf-ps2-l2ka8       1/1       Running   0          3h\ntf-ps4-2jmuf       1/1       Running   1          3h\ntf-worker1-fm1t8   1/1       Running   1          3h\ntf-worker3-ujd7h   1/1       Running   0          3h\ntf-worker5-ymx71   1/1       Running   0          3h\n```\n\nThe log regarding restarted parameter server:\n\n```\nzeyu@perhaps:~$ kubectl logs -p tf-ps4-2jmuf\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {tf-worker1:2222, tf-worker3:2222, tf-worker5:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {tf-ps2:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\n```\n\nThe log of current running server: (almost the same)\n\n```\nzeyu@perhaps:~$ kubectl logs  tf-ps4-2jmuf\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {tf-worker1:2222, tf-worker3:2222, tf-worker5:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {tf-ps2:2222, localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2222\n```\n\nAs the result of the restart, the global count  reset to 0 and the running log looks like:\n\n```\nTraining step 269, global step 802\nTraining step 270, global step 805\nTraining step 271, global step 808\nTraining step 272, global step 810\nTraining step 273, global step 3\nTraining step 274, global step 6\nTraining step 275, global step 12\nTraining step 276, global step 19\nTraining step 277, global step 22\nTraining step 278, global step 27\nTraining step 279, global step 32\n```\n\nIs there any way that I can materialize the variables (like hostpath or persistent disk?) so that they won't be affected by restarts?\n", "comments": ["We typically use a `tf.train.Saver` (which is implicitly created by the `tf.train.Supervisor`) to handle periodic checkpointing and restoring variables from a checkpoint on restart. Are you using those in your program?\n", "Yes, I did use the `tf.train.Supervisor`, but the job may not be running for 600 seconds. Are the checkpoints stored on the server side? I didn't see any checkpoints stored on my local machine (where I did see the summary files) without explicitly call `sv.saver.save`\n", "The checkpoints are written on the server side. The current code assumes that you have a shared filesystem between the client and the servers, such as a shared NFS mount (or gcsfuse, if you're running on GCE).\n\nNote that you can set the supervisor to checkpoint more frequently by overriding the `save_model_secs` argument to the constructor.\n", "Thanks! I'll try this\n", "Closing due to inactivity. Feel free to reopen if you still have concerns.\n"]}, {"number": 2460, "title": "set_random_seed with a value too large for an int fails during later call to random()", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN:  No CUDA\n\nIf installed from sources, provide the commit hash: 7d9ab3eb485e6eb1778bad4ef01a1cd95b2d22d9\n### Steps to reproduce\n\n``` python\nimport tensorflow as tf\ntf.set_random_seed(2 ** 70)\nsess = tf.InteractiveSession()\ntf.random_normal((1,)).eval()\n```\n\nFails with \"Value out of range\" exception on eval. \n\nI'd would prefer that it would either fail on `tf.set_random_seed` or just truncate the seed to fit into the field (as this would be semantically correct behavior). Which one should be a correct one? I'd like to make a PR to fix it.\n", "comments": ["Thanks for noticing this and we'd welcome a PR!\n\n@zheng-xq: Since you wrote this code, do you have a preference for the behavior here?\n", " Then truncation is the correct behavior. Thanks for noticing this!\n"]}, {"number": 2459, "title": "\u6bb5\u9519\u8bef", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System:\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\n1.\n2.\n3.\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nSystem-XPS-L321X:~$ python\nPython 2.7.11+ (default, Apr 17 2016, 14:00:29) \n[GCC 5.3.1 20160413] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n> > > import tensorflow\n> > > \u6bb5\u9519\u8bef (\u6838\u5fc3\u5df2\u8f6c\u50a8)\n", "comments": ["I  use  ubuntu  16.04 and py2.7\n", "Not sure what the bug report is.\n"]}, {"number": 2458, "title": "Graph \"Training/Test\" flag & global_flags", "body": "I am not sure if this has been proposed yet, but I would like the ability in the .RUN() op to flag \"test\" rather than \"train\".  The motivation is around \"Dropout\".  When I am training a batch I would like the dropout in my graph, but when I am running a \"test\" I would like the Dropout to simply pass the tensor (0% dropout if you will).  I am sure I can pass the \"dropout\" as a param through the feeder but they need to be set and reset each time and could get cumbersome for larger and complex graphs.  It would be nice to have an internal feature in \"Dropout\" and other Training specific features such that they will adjust their behavior if \"Not-Training\".\n\nRight now I have a graph that has 2 paths, one for training with the Dropouts Ops in-line and one that does note and I call the proper 'Output' based upon my Test/Training desire.  This complicated the graph.\n\nMaybe this function can be extended to pass \"Flags\" into the graph so other features can be added that are based upon run-time demand.  This \"flag_dict\" could be global to the entire graph so Ops' could check their status upon a run.\n\n`sess.run([cost,optimizer,merged], \n                                                                 feed_dict={X: Xbatch,\n                                                                 Y: YDesired}, flag_dict={Training: True})`\n\nI can think of another request where this would be useful.  As I search solution space, I would like to setup a Genetic Algorithm to assist.  One of the things I would like to change in each graph iteration might be the \"activation\", \"Cost\" and/or \"optimizer\" functions.  So a generic Activation Op or Optimizer op could be written (are maybe a simple \"Case\" Op based upon the flag)?\n\neg. `flag_dict={Training: True, Activation: Relu, Optimizer: AdagradOptimizer}`\n\nNot sure how to deal with the parameters if they differ from Op to Op.  But maybe \"custom global flags\" could be passed as well. \n\neg.  `layer2out    = tf.nn.genericActivation(layer1out,ActivationFlag=Activation)   #Activation=Relu from above`\n\nJust a thought to help explore during training/testing.\n", "comments": ["@Mazecreator: The way to express this in TensorFlow is to make two different pieces of graph that share variables (either explicitly or using variable scopes).  You then run one part of the graph for training and one part for testing. \n"]}, {"number": 2457, "title": "external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed: bash failed", "body": "### Environment info\n\nOperating System:\nRed Hat 4.8.3-9\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n/usr/local/cuda-7.5/ \ncudnn-7.5-linux-x64-v5.0-ga-tgz\nlibcudnn.so.5\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n1. Getting bazel from https://github.com/bazelbuild/bazel/archive/0.2.3.tar.gz\n2. On 18, May , I Get tensor flow via git clone --recurse-submodules https://github.com/tensorflow/tensorflow\n3. Running command /home/admin/bazel/bazel_source/bazel-0.2.3/output/bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures, I got error\n   external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed\n### What have you tried?\n1.  After I modified external/png_archive/BUILD:30 by adding CPPFLAGS=\\\"-I /usr/local/zlib/usr /include\\\"; LDFLAGS=\\\"-L /usr/local/zlib/lib64\\\" in cmd, it would get the error \"external/png_archive/BUILD:33:1: C++ compilation of rule '@png_archive//:png' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n   (cd /home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/tensorflow && \\\n   exec env - \\\n     PATH=/sbin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/X11R6/bin:/home/zy80232/.local/bin:/home/zy80232/bin \\\n   third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -g0 -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/host/genfiles/external/png_archive/libpng-1.2.53 -isystem external/bazel_tools/tools/cpp/gcc3 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/host/bin/external/png_archive/_objs/png/external/png_archive/libpng-1.2.53/pngwutil.o' -MD -MF bazel-out/host/bin/external/png_archive/_objs/png/external/png_archive/libpng-1.2.53/pngwutil.d -c external/png_archive/libpng-1.2.53/pngwutil.c -o bazel-out/host/bin/external/png_archive/_objs/png/external/png_archive/libpng-1.2.53/pngwutil.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n   In file included from external/png_archive/libpng-1.2.53/pngwutil.c:16:0:\n   external/png_archive/libpng-1.2.53/png.h:548:18: fatal error: zlib.h\"\n2. Then I added one extra line in png_archive/BUILD:38 by copts = [\"-Iexternal/zlib/usr/include\"], and I get the error  \"/home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/external/png_archive/BUILD:33:1: undeclared inclusion(s) in rule '@png_archive//:png':\n   this rule is missing dependency declarations for the following files included by 'external/png_archive/libpng-1.2.53/pngwio.c':\n   'external/zlib/usr/include/zlib.h'\n   'external/zlib/usr/include/zconf.h'\"\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Before your changes, what were the full output logs from bazel ? (add `--verbose_failures`)\n", "Hi, Vrv, thanks for your quick response, just paste the whole log as belows:\nbazel build --sandbox_debug -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer --verbose_failures \nWARNING: Sandboxed execution is not supported on your system and thus hermeticity of actions cannot be guaranteed. See http://bazel.io/docs/bazel-user-manual.html#sandboxing for more information. You can turn off this warning via --ignore_unsupported_sandboxing.\nINFO: Found 1 target...\nERROR: /home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/external/png_archive/BUILD:23:1: Executing genrule @png_archive//:configure failed: bash failed: error executing command \n  (cd /home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/tensorflow && \\\n  exec env - \\\n    PATH=/sbin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/X11R6/bin:/home/zy80232/.local/bin:/home/zy80232/bin:/home/admin/bazel/bazel_source/bazel-0.2.2b/output \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; pushd external/png_archive/libpng-1.2.53; workdir=$(mktemp -d -t tmp.XXXXXXXXXX); cp -a \\* $workdir; pushd $workdir; ./configure --enable-shared=no --with-pic=no; popd; popd; cp $workdir/config.h bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53; rm -rf $workdir;'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n/home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/tensorflow/external/png_archive/libpng-1.2.53 /home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/tensorflow\n/tmp/tmp.l1NdEzzkxa /home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/tensorflow/external/png_archive/libpng-1.2.53 /home/admin/.cache/bazel/_bazel_admin/b33b4c76bea799d489291f7bcf5433ae/tensorflow\nchecking for a BSD-compatible install... /usr/bin/install -c\nchecking whether build environment is sane... yes\nchecking for a thread-safe mkdir -p... /usr/bin/mkdir -p\nchecking for gawk... gawk\nchecking whether make sets $(MAKE)... yes\nchecking whether make supports nested variables... yes\nchecking whether to enable maintainer-specific portions of Makefiles... no\nchecking for gcc... gcc\nchecking whether the C compiler works... yes\nchecking for C compiler default output file name... a.out\nchecking for suffix of executables... \nchecking whether we are cross compiling... no\nchecking for suffix of object files... o\nchecking whether we are using the GNU C compiler... yes\nchecking whether gcc accepts -g... yes\nchecking for gcc option to accept ISO C89... none needed\nchecking whether gcc understands -c and -o together... yes\nchecking for style of include used by make... GNU\nchecking dependency style of gcc... gcc3\nchecking build system type... x86_64-unknown-linux-gnu\nchecking host system type... x86_64-unknown-linux-gnu\nchecking for a sed that does not truncate output... /usr/bin/sed\nchecking for grep that handles long lines and -e... /usr/bin/grep\nchecking for egrep... /usr/bin/grep -E\nchecking for fgrep... /usr/bin/grep -F\nchecking how to print strings... printf\nchecking for ld used by gcc... /usr/bin/ld\nchecking if the linker (/usr/bin/ld) is GNU ld... yes\nchecking how to run the C preprocessor... gcc -E\nchecking for sed... /usr/bin/sed\nchecking whether ln -s works... yes\nchecking whether make sets $(MAKE)... (cached) yes\nchecking for BSD- or MS-compatible name lister (nm)... /usr/bin/nm -B\nchecking the name lister (/usr/bin/nm -B) interface... BSD nm\nchecking the maximum length of command line arguments... 1572864\nchecking how to convert x86_64-unknown-linux-gnu file names to x86_64-unknown-linux-gnu format... func_convert_file_noop\nchecking how to convert x86_64-unknown-linux-gnu file names to toolchain format... func_convert_file_noop\nchecking for /usr/bin/ld option to reload object files... -r\nchecking for objdump... objdump\nchecking how to recognize dependent libraries... pass_all\nchecking for dlltool... dlltool\nchecking how to associate runtime and link libraries... printf %s\\n\nchecking for ar... ar\nchecking for archiver @FILE support... @\nchecking for strip... strip\nchecking for ranlib... ranlib\nchecking command to parse /usr/bin/nm -B output from gcc object... ok\nchecking for sysroot... no\nchecking for a working dd... /usr/bin/dd\nchecking how to truncate binary pipes... /usr/bin/dd bs=4096 count=1\nchecking for mt... no\nchecking if : is a manifest tool... no\nchecking for ANSI C header files... yes\nchecking for sys/types.h... yes\nchecking for sys/stat.h... yes\nchecking for stdlib.h... yes\nchecking for string.h... yes\nchecking for memory.h... yes\nchecking for strings.h... yes\nchecking for inttypes.h... yes\nchecking for stdint.h... yes\nchecking for unistd.h... yes\nchecking for dlfcn.h... yes\nchecking for objdir... .libs\nchecking if gcc supports -fno-rtti -fno-exceptions... no\nchecking for gcc option to produce PIC... -fPIC -DPIC\nchecking if gcc PIC flag -fPIC -DPIC works... yes\nchecking if gcc static flag -static works... no\nchecking if gcc supports -c -o file.o... yes\nchecking if gcc supports -c -o file.o... (cached) yes\nchecking whether the gcc linker (/usr/bin/ld -m elf_x86_64) supports shared libraries... yes\nchecking dynamic linker characteristics... GNU/Linux ld.so\nchecking how to hardcode library paths into programs... immediate\nchecking whether stripping libraries is possible... yes\nchecking if libtool supports shared libraries... yes\nchecking whether to build shared libraries... no\nchecking whether to build static libraries... yes\nchecking for ANSI C header files... (cached) yes\nchecking malloc.h usability... yes\nchecking malloc.h presence... yes\nchecking for malloc.h... yes\nchecking for stdlib.h... (cached) yes\nchecking for string.h... (cached) yes\nchecking for strings.h... (cached) yes\nchecking for an ANSI C-conforming const... yes\nchecking for size_t... yes\nchecking whether struct tm is in sys/time.h or time.h... time.h\nchecking for working strtod... yes\nchecking for memset... yes\nchecking for pow... no\nchecking for pow in -lm... yes\nchecking for zlibVersion in -lz... no\nconfigure: error: zlib not installed\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nINFO: Elapsed time: 7.337s, Critical Path: 6.10s\n", "You probably need to install the zlib development libraries, I guess.\n"]}, {"number": 2456, "title": "Problem in distributed parameter server", "body": "### Environment info\n\nRunning the distributed tensorflow on GCE kubernetes cluster using the [demo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/scripts/dist_mnist_test.sh) with the default images.\n### Steps to reproduce\n1. Run the demo with [hidden_units](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py#L69) = 100\n2. Run the demo with  [hidden_units](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dist_test/python/mnist_replica.py#L69) = 500\n### What have you tried?\n1. Restart all the pods (this worked)\n### Logs or other output that would be helpful\n\n```\nTraceback (most recent call last):\n  File \"/home/zeyu/gocode/src/github.com/caicloud/BigData/distributed-tensorflow/runner/demo/mnist_replica.py\", line 250, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zeyu/gocode/src/github.com/caicloud/BigData/distributed-tensorflow/runner/demo/mnist_replica.py\", line 203, in main\n    with sv.prepare_or_wait_for_session(FLAGS.worker_grpc_url, config=sess_config) as sess:\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 685, in prepare_or_wait_for_session\n    config=config, init_feed_dict=self._init_feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 163, in prepare_session\n    sess.run(init_op, feed_dict=init_feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [100,10] rhs shape= [500,10]\n     [[Node: layer2/weights/Variable/Assign = Assign[T=DT_FLOAT, _class=[\"loc:@layer2/weights/Variable\"], use_locking=true, validate_shape=true, _device=\"/job:ps/replica:0/task:1/cpu:0\"](layer2/weights/Variable, layer2/weights/truncated_normal_S19)]]\nCaused by op u'layer2/weights/Variable/Assign', defined at:\n  File \"/home/zeyu/gocode/src/github.com/caicloud/BigData/distributed-tensorflow/runner/demo/mnist_replica.py\", line 250, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/zeyu/gocode/src/github.com/caicloud/BigData/distributed-tensorflow/runner/demo/mnist_replica.py\", line 157, in main\n    y = nn_layer(hidden1, FLAGS.hidden_units, 10, 'layer2', act=tf.nn.softmax)\n  File \"/home/zeyu/gocode/src/github.com/caicloud/BigData/distributed-tensorflow/runner/demo/mnist_replica.py\", line 115, in nn_layer\n    weights = weight_variable([input_dim, output_dim])\n  File \"/home/zeyu/gocode/src/github.com/caicloud/BigData/distributed-tensorflow/runner/demo/mnist_replica.py\", line 85, in weight_variable\n    return tf.Variable(initial)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 209, in __init__\n    dtype=dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 308, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 40, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n```\n\nThe error happened at the init time, and it seems that the log suggests the parameter server didn't clean up the information from previous job:\n\n```\ntensorflow.python.framework.errors.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [100,10] rhs shape= [500,10]\n```\n", "comments": ["It seems to be a general problem cross multiple runs:\n\nI got the following error about \"global_step\" when running different jobs using the same cluster (this can also be fixed by delete (and restart) the parameter server in kubernetes\n\n```\nTraceback (most recent call last):\n  File \"performance_test.py\", line 207, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"performance_test.py\", line 183, in main\n    _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.UnavailableError\n```\n", "I think this is working as intended: variables are globally shared by default.\n\ncc @mrry in case I'm misunderstanding something.  You probably want to reset the cluster and/or scope the variable names if you intend to reuse variable names across multiple runs.\n", "how can I reset the cluster?\n", "Just in case someone else lands here, try clearing all old data from train_dir and then restart your distributed training."]}, {"number": 2455, "title": "Adding a New Op Error:  can not load the customed file", "body": "### Environment info\n\nOperating System:  Ubuntu 14.04 LTS\n\nInstalled version of CUDA and cuDNN: None \n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   I use the command below to install tensorflow.\n   sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.8.0\n### What have you tried?\n\nI have tried to create my new operation, but there was something wrong that made me fail to load the self-defined_tensorflow_op.so. Here are steps which are exactly the same as those in the official tutorial and what I have tried. \n1. Copy the whole content from the official tutorial to create a  file named tensorflow/core/user_ops/zero_out.cc . What is the file is as following.\n   \n   ```\n   #include \"tensorflow/core/framework/op.h\"\n   \n   REGISTER_OP(\"ZeroOut\")\n       .Input(\"to_zero: int32\")\n       .Output(\"zeroed: int32\");\n   \n   \\#include \"tensorflow/core/framework/op_kernel.h\"\n   \n   using namespace tensorflow;\n   \n   class ZeroOutOp : public OpKernel {\n    public:\n     explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\n   \n     void Compute(OpKernelContext* context) override {\n       // Grab the input tensor\n       const Tensor& input_tensor = context->input(0);\n       auto input = input_tensor.flat<int32>();\n   \n       // Create an output tensor\n       Tensor* output_tensor = NULL;\n       OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\n                                                        &output_tensor));\n       auto output = output_tensor->template flat<int32>();\n   \n       // Set all but the first element of the output tensor to 0.\n       const int N = input.size();\n       for (int i = 1; i < N; i++) {\n         output(i) = 0;\n       }\n   \n       // Preserve the first input value if possible.\n       if (N > 0) output(0) = input(0);\n     }\n   };\n   \n   REGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\n   ```\n2. Then I compile the zero_out.cc with following commands:\n   \n   ```\n   TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())')\n   \n   g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC -I $TF_INC\n   ```\n3. Trying not to miss anything, I also create the BUILD file and run the corresponding commands.\n   \n   ```\n   \\# Here is the BUILD file\n   load(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\n   \n   tf_custom_op_library(\n       name = \"zero_out.so\",\n       srcs = [\"zero_out.cc\"],\n   )\n   \n   \\# Then run this command\n   $ bazel build -c opt //tensorflow/core/user_ops:zero_out.so\n   ```\n4. When I try to use the new op, the error comes out. \n       # Run in python 2.7.11, with following command\n       import tensorflow as tf\n       zero_out_module = tf.load_op_library('zero_out.so')\n### Here is the error message\n\n```\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n        File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/load_library.py\", line 71, in load_op_library\n            raise errors._make_specific_exception(None, None, error_msg, error_code)\n            tensorflow.python.framework.errors.NotFoundError: zero_out.so: cannot open shared object file: No such file or directory\n```\n\nPlease help me out with this. I have spent lots of time trying to figure out what is wrong and searching for the answer, but nothing works. Any advice is also will welcome. Thanks \n", "comments": ["`tf.load_op_library` relies on the system provided mechanism to locate the `.so` file, especially if you don't provide the full path to the file. On linux, the directories present in the `LD_LIBRARY_PATH` environment variable are looked up. In your case, when compiling with `g++`, can you try providing the full path to `tf.load_op_library`, most probably `./zero_out.so`?\nIf you are compiling using bazel, there is an utility function to give you the full path to the `.so`. Look [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/how_tos/adding_an_op/zero_out_1_test.py#L36) :\n\n```\ntf.load_op_library(os.path.join(\n          tf.resource_loader.get_data_files_path(), 'zero_out_op_kernel_1.so'))\n```\n", "Ping. @StrivingMax Does my suggestion work for you?\n", "@keveman. Yes, I have successfully loaded my costumed ops to the python interpreter with the code below\n\n```\nmy_module = tf.load_op_library(\n    '/home/my_name/Tools/tensorflow/tensorflow/core/user_ops/my_ops.so'\n)\n```\n\nThank you very much. I already realized that I could not locate the file and did not know what was wrong. Your advice is exactly the answer.\n\nHowever, another problem occurs when I run `my_mudule.my_ops` in `with tf.Sessin():`.\n\n```\nE tensorflow/core/framework/op_kernel.cc:867] OpKernel ('op: \"Fact\" device_type: \"GPU\"     host_memory_arg: \"fact\"') for unknown op: Fact\nE tensorflow/core/framework/op_kernel.cc:867] OpKernel ('op: \"Fact\" device_type: \"CPU\" label:  \"Sergey\"') for unknown op: Fact\nE tensorflow/core/framework/op_kernel.cc:867] OpKernel ('op: \"Fact\" device_type: \"CPU\" label: \"sergey\"') for unknown op: Fact\n```\n\nI did not do anything with _Fact.cc_ or _Fact.so_. How can I do to resolve this error?\n\nAgain, thank  you very muck for giving me advice.\n", "The error seems to indicate that you have the `HostMemory` attribute set for an output arg , and a label called `Sergey` in your kernel specification. However, the code you provided doesn't seem to contain those. Can you please provide the exact code for your custom op?\n", "Closing the issue as there seems to be no outstanding action items. @StrivingMax feel free to open again if you feel that this has not been addressed sufficiently.\n", "Hi, I got a similar problem.\n\nOperating System: macOS 10.12\n\nInstalled version of CUDA and cuDNN: None\n\nTensorFlow 0.11.0 installed from source.\n\nI followed the [add new op](https://www.tensorflow.org/versions/r0.11/how_tos/adding_an_op/index.html) tutorial, adding zero_out.cc file:\n\n```\n#include \"tensorflow/core/framework/op.h\"\n\nREGISTER_OP(\"ZeroOut\")\n    .Input(\"to_zero: int32\")\n    .Output(\"zeroed: int32\");\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n\nusing namespace tensorflow;\n\nclass ZeroOutOp : public OpKernel {\n public:\n  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto input = input_tensor.flat<int32>();\n\n    // Create an output tensor\n    Tensor* output_tensor = NULL;\n    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),\n                                                     &output_tensor));\n    auto output = output_tensor->flat<int32>();\n\n    // Set all but the first element of the output tensor to 0.\n    const int N = input.size();\n    for (int i = 1; i < N; i++) {\n      output(i) = 0;\n    }\n\n    // Preserve the first input value if possible.\n    if (N > 0) output(0) = input(0);\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp);\n```\n\nand bazel BUILD file:\n\n```\nload(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")\n\ntf_custom_op_library(\n    name = \"zero_out.so\",\n    srcs = [\"zero_out.cc\"]\n)\n```\n\nthen I run:\n\n```\nbazel build -c opt //tensorflow/core/user_ops:zero_out.so\n```\n\noutput:\n\n```\nINFO: Waiting for response from Bazel server (pid 28589)...\nINFO: Found 1 target...\nTarget //tensorflow/core/user_ops:zero_out.so up-to-date:\n  bazel-bin/tensorflow/core/user_ops/zero_out.so\nINFO: Elapsed time: 5.115s, Critical Path: 0.00s\n```\n\nThe generated shared library located in bazel-bin. When I tried to load it like this:\n\n```\ntf.load_op_library('/Users/dtong/code/data/tensorflow/bazel-bin/tensorflow/core/user_ops/zero_out.so')\n```\n\nthe result:\n\n```\npython(41716,0x7fffb7e123c0) malloc: *** error for object 0x7f9e9cd2de18: pointer being freed was not allocated\n*** set a breakpoint in malloc_error_break to debug\n```\n\n@keveman could you please help take a look at this problem?\n", "@JakeRenn Hi Jake, I saw similar error message as you did:\r\n```\r\nE tensorflow/core/framework/op_kernel.cc:943] OpKernel ('op: \"Fact\" device_type: \"CPU\" label: \"sergey\"') for unknown op: Fact\r\nE tensorflow/core/framework/op_kernel.cc:943] OpKernel ('op: \"Fact\" device_type: \"CPU\" label: \"Sergey\"') for unknown op: Fact\r\nE tensorflow/core/framework/op_kernel.cc:943] OpKernel ('op: \"Fact\" device_type: \"GPU\" host_memory_arg: \"fact\"') for unknown op: Fact\r\n```\r\nDo you have any thoughts on that? Thanks in advance! :-)"]}, {"number": 2454, "title": "Suggestion: adding Tags or descriptions to the run log", "body": "Hi!\n\nSo far, I I have to say that TensorBoard is a really great tool to analyze the training procedure.\nHowever, I often find myself comparing various runs and wondering \"wait, which version of my code was that and which parameters did I use?\".\nCurrently, the only way to \"document\" a Run is the name of the log directory - so I find myself encoding all kinds of cryptic acronyms into the directory name.\n\nWhat would be really awesome is some sort of feature that allows the training scripts to pass Data to the TensorBoard logs. This could be in any form, maybe as sort of \"tags\" or as key-value pairs, or simply as a plain old string called \"description\" or something.\n\nIs this something that sounds useful to others as well? Is this feasible?\n", "comments": ["@danmane: Can you comment on this?  It seems like a reasonable thing to mark \"contributions welcome\", but we should get your input first on how best to express it if so. \n", "This is on my roadmap, actually. I want to make a new field in the event.proto that will let you record arbitrary info about the run, including key value pairs like \"code version\", \"vx.x.x\", hyperparameters, etc.\nI'd like to get this in in-time for the next release, no promises.\n\nIt's probably not a great candidate for contributions welcome because I already have a plan to do it, and it requires some API changes.\n", "@danmane Thanks, that's great to hear.\n\nBy the way, #2510 seems like a duplicate of this one.\n", "Hyperparameter & run description for tensorboard is now under active development.\nI'm closing this issue to make for easier issue tracking.\n", "Hi, this is great news! Is there a tracking bug or anything to get notified, when you have implemented this feature?\n"]}, {"number": 2453, "title": "Building from local docker File", "body": "### Environment info\n\nOperating System:Docker / Win7 64bits\n\nFollowing instructions here:\nhttps://docs.docker.com/windows/step_four/\n\nUsing this docker file:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile\n\nWe build the tensorflow file for installation:\n\nWe had this error message:\nStep 7 : COPY jupyter_notebook_config.py /root/.jupyter/\nlstat jupyter_notebook_config.py: no such file or directory\n\nHow to solve this issue ?\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["I met the same problem,how did you solve it ?\n", "@arita37  I made it ,just copy  the whole git code to the same folder with dockerfile.\n", "Ok, good,\nWhat is the docker command line ?\nRUN git ???\n\nThanks\n\nOn 24 May 2016, at 13:33, ChenZhaobin notifications@github.com wrote:\n\n@arita37 I made it ,just copy the whole git code to the same folder with dockerfile.\n\n\u2015\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n", "@jendap: Want to comment? \n", "Switching to @caisq.\n", "Answer the damn question ? Hello LOL\n\" I made it ,just copy the whole git code to the same folder with dockerfile. \"   what the BLIP does that mean \n", "Use this file: https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/run_jupyter.sh#L1\r\n\r\nEither copy in your directory, or run the docker build directory from there."]}, {"number": 2452, "title": "ImportError: No module named 'tensorflow'", "body": "I downloaded tensorflow using Anacaonda environment installation (I use python 3.5). When I enter `import tensorflow as tf` it always shows the message: \"ImportError: No module named 'tensorflow' Can someone please help me resolve this issue? \n", "comments": ["Can you paste detailed log on the tensorflow installation and execution?\n", "Same question here, @MichaelLi0913 have you solved it?\n", "Automatically closing due to lack of recent activity. Please reopen when further information becomes available.\n"]}, {"number": 2451, "title": "Compile Issue about Eigen", "body": "I want to compile tensorflow-r0.8 while meet an issue about Eigen.\nThis issue happened in both CPU and GPU version. \n\nThe error log is \n\n```\nERROR: /home/wangbing/Git/tensorflow/tensorflow/core/kernels/BUILD:274:1: C++ compilation of rule '//tensorflow/core/kernels:mirror_pad_op' failed: gcc failed: error executing command \n  (cd /home/wangbing/.cache/bazel/_bazel_wangbing/ab8decc7da56ae5392500261af1cc855/tensorflow && \\\n  exec env - \\\n    PATH=/home/wangbing/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/wangbing/bin \\\n  /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-50812b426b7c -isystem bazel-out/local-opt/genfiles/external/eigen_archive/eigen-eigen-50812b426b7c -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -DTENSORFLOW_USE_EIGEN_THREADPOOL -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.o' -MD -MF bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.d -c tensorflow/core/kernels/mirror_pad_op.cc -o bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\n```\n\nThen I run that command by myself,and get the error as below\n\n```\nangbing@wangbing-MS-7996:~/Git/tensorflow$ /usr/bin/gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wl,-z,-relro,-z,now -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -iquote . -iquote bazel-out/local-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local-opt/genfiles/external/bazel_tools -iquote external/jpeg_archive -iquote bazel-out/local-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-50812b426b7c -isystem bazel-out/local-opt/genfiles/external/eigen_archive/eigen-eigen-50812b426b7c -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -DTENSORFLOW_USE_EIGEN_THREADPOOL -pthread -no-canonical-prefixes -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' '-frandom-seed=bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.o' -MD -MF bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.d -c tensorflow/core/kernels/mirror_pad_op.cc -o bazel-out/local-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.o\nIn file included from ./tensorflow/core/kernels/mirror_pad_op.h:19:0,\n                 from tensorflow/core/kernels/mirror_pad_op.cc:20:\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:67: fatal error: eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"eigen-eigen-50812b426b7c/unsupported/Eigen/CXX11/Tensor\"\n```\n\nIt seems that a eigen model can not be found.  How can I fix it?\n", "comments": ["my system is ubuntu 14.04\n\n```\nwangbing@wangbing-MS-7996:~/Git/tensorflow$ uname -a\nLinux wangbing-MS-7996 4.2.0-27-generic #32~14.04.1-Ubuntu SMP Fri Jan 22 15:32:26 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\nwangbing@wangbing-MS-7996:~/Git/tensorflow$ \n\n```\n", "I found the root cause is the command `exec env -` , this will cause the terminal exit normally in ubuntu. \n\nCurrently I have to compile some modules manually to work around it.\n\n```\nERROR: /home/wangbing/Git/tensorflow/tensorflow/core/kernels/BUILD:297:1: C++ compilation of rule '//tensorflow/core/kernels:mirror_pad_op' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command \n  (cd /home/wangbing/.cache/bazel/_bazel_wangbing/ab8decc7da56ae5392500261af1cc855/tensorflow && \\\n  exec env - \\\n    PATH=/home/wangbing/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/wangbing/bin \\\n  third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -iquote . -iquote bazel-out/local_linux-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/local_linux-opt/genfiles/external/bazel_tools -iquote external/farmhash_archive -iquote bazel-out/local_linux-opt/genfiles/external/farmhash_archive -iquote external/jpeg_archive -iquote bazel-out/local_linux-opt/genfiles/external/jpeg_archive -iquote external/png_archive -iquote bazel-out/local_linux-opt/genfiles/external/png_archive -iquote external/re2 -iquote bazel-out/local_linux-opt/genfiles/external/re2 -iquote external/eigen_archive -iquote bazel-out/local_linux-opt/genfiles/external/eigen_archive -isystem google/protobuf/src -isystem bazel-out/local_linux-opt/genfiles/google/protobuf/src -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem bazel-out/local_linux-opt/genfiles/external/farmhash_archive/farmhash-34c13ddfab0e35422f4c3979f360635a8c050260 -isystem external/jpeg_archive/jpeg-9a -isystem bazel-out/local_linux-opt/genfiles/external/jpeg_archive/jpeg-9a -isystem external/png_archive/libpng-1.2.53 -isystem bazel-out/local_linux-opt/genfiles/external/png_archive/libpng-1.2.53 -isystem external/re2 -isystem bazel-out/local_linux-opt/genfiles/external/re2 -isystem third_party/eigen3 -isystem bazel-out/local_linux-opt/genfiles/third_party/eigen3 -isystem external/eigen_archive/eigen-eigen-a5e9085a94e8 -isystem bazel-out/local_linux-opt/genfiles/external/eigen_archive/eigen-eigen-a5e9085a94e8 -isystem third_party/gpus/cuda/include -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda/include -isystem third_party/gpus/cuda -isystem bazel-out/local_linux-opt/genfiles/third_party/gpus/cuda -fno-exceptions -DEIGEN_AVOID_STL_ARRAY -DTENSORFLOW_USE_EIGEN_THREADPOOL '-DGOOGLE_CUDA=1' -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -fno-canonical-system-headers '-frandom-seed=bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.o' -MD -MF bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.d -c tensorflow/core/kernels/mirror_pad_op.cc -o bazel-out/local_linux-opt/bin/tensorflow/core/kernels/_objs/mirror_pad_op/tensorflow/core/kernels/mirror_pad_op.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 4.\ngcc: internal compiler error: Killed (program cc1plus)\nPlease submit a full bug report,\nwith preprocessed source if appropriate.\nSee <file:///usr/share/doc/gcc-4.8/README.Bugs> for instructions.\n\n```\n", "This issue is caused by OOM, I added a larger swap file and fixed this issue, thanks.\n"]}, {"number": 2450, "title": "gradient check", "body": "Hi I have some problem to write new tensorflow op.<br>\nI use tf.test.compute_gradient_error to write the test case. <br>\nAnd get the error <br>\nThe error message <br>\n\n> FAIL: test_grad (**main**.ZeroOut2Test) Traceback (most recent call\n> last):   File \"test_case.py\", line 42, in test_grad\n>     self.assertLess(err, 1e-4) AssertionError: 0.0072760284 not less than 0.0001\n\nI already differential on f(data,truthdata).\nHow to write correct gradient calculate?<br>\n\nThe op have two inputs and two output.<br>\nInput1 data<br>\nInput2 truthdata<br>\noutput1 loss<br>\noutput2 delta(gradient)<br>\n<br>\nHere is the op code\n\n```\n#include \"tensorflow/core/framework/op.h\"\n\nREGISTER_OP(\"DetectionOut\")\n    .Attr(\"T: {float}\")\n    .Input(\"detect: T\")\n    .Input(\"truthdata: T\")\n    .Output(\"loss: T\")\n    .Output(\"delta: T\");\n\n\n#include \"tensorflow/core/framework/op_kernel.h\"\n\nusing namespace tensorflow;\n\ntypedef Eigen::ThreadPoolDevice CPUDevice;\ntypedef Eigen::GpuDevice GPUDevice;\n\ntemplate <typename Device, typename T>\nclass DetectionOutOp : public OpKernel {\n public:\n  explicit DetectionOutOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* context) override {\n    // Grab the input tensor\n    const Tensor& input_tensor = context->input(0);\n    auto PreDetection = input_tensor.flat<T>();\n\n    const Tensor& input_tensor1 = context->input(1);\n    auto TruthData = input_tensor1.flat<T>();\n\n    // Create an output tensor\n    Tensor* output_tensor = NULL;\n    OP_REQUIRES_OK(context, context->allocate_output(0, TensorShape({input_tensor.dim_size(0)}),\n                                                     &output_tensor));\n    auto loss = output_tensor->template flat<T>().setZero();\n\n    Tensor* output_tensor1 = NULL;\n    OP_REQUIRES_OK(context, context->allocate_output(1, input_tensor.shape(),\n                                                     &output_tensor1));\n    auto back_detl = output_tensor1->template flat<T>().setZero();\n\n    const int N = PreDetection.size();\n\n    const int64 batch_size_ =  input_tensor.dim_size(0);\n    const int64 bottom_count_ = input_tensor.dim_size(1);\n    int b,i;\n    for (b = 0; b < batch_size_ ; ++b){\n        int index = b * bottom_count_;\n        for (i = 0; i < bottom_count_; ++i) {\n            loss(b) +=  pow(PreDetection(index+i)-TruthData(index+i), 2);\n            back_detl(index+i) = 2 *(PreDetection(index+i) - TruthData(index+i));\n        }\n    }\n  }\n};\n\n#define REGISTER_KERNEL(T)                                      \\\n  REGISTER_KERNEL_BUILDER(                                      \\\n      Name(\"DetectionOut\").Device(DEVICE_CPU).TypeConstraint<T>(\"T\"), \\\n      DetectionOutOp<CPUDevice, T>);\n\nREGISTER_KERNEL(float);\n#undef REGISTER_KERNEL\n```\n\nHere is TestCase.py<br>\n    import tensorflow as tf\n    import numpy as np\n\n```\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import sparse_ops\n\nD2Test = np.zeros((2,7*7*(15)),float) \nD2Truth = np.zeros((2,7*7*(15)),float) \n\nitems = 1\ncoord = 4\nnum = 2\nclass_num = 5\nlocalsize = 49\n\nobject_index = (class_num) * localsize + items\ntobject_index = (class_num) * localsize + items\n\nD2Truth[0,tobject_index] = 1.0\nD2Test[0,object_index] = 0.5\n\n\n@ops.RegisterGradient(\"DetectionOut\")\ndef _detection_out_grad(op, grad, grad1):\n  mat = op.outputs[1]\n  vec = array_ops.expand_dims(grad, -1)\n  vec = vec * mat\n  return [vec, None]\n\ndetection_module = tf.load_op_library('detection.so')\n\nclass ZeroOut2Test(tf.test.TestCase):\n  def test_grad(self):\n    with self.test_session():\n      shape=(2,7*7*(15))\n      shape1=(2,)\n      x = tf.constant(D2Test, dtype=tf.float32)\n      y = tf.constant(D2Truth, dtype=tf.float32)\n      result = detection_module.detection_out(x,y)\n      err = tf.test.compute_gradient_error(x, shape, result[0], shape1)\n      self.assertLess(err, 1e-4)\n\nif __name__ == '__main__':\n  tf.test.main()\n```\n\nThanks for help to correct my errors\n", "comments": ["I'm going to close this issue and encourage you to submit this on Stack Overflow instead:\n\n> GitHub issues are for bugs / installation problems / feature requests.  \n> For general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\n> To make bugs and feature requests more easy to find and organize, we close issues that are deemed\n> out of scope for GitHub Issues and point people to StackOverflow.\n"]}, {"number": 2449, "title": "Branch 122885116", "body": "", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n\n<!-- need_author_cla -->\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 2448, "title": "Session got stuck after fork", "body": "Version: nightly prebuilt for Python2 w/ GPU (just now)\n\nI'm expecting the following code to print \"10.0\" 3 times, but session.run got stuck in all forked processes.\n\n``` python\nimport tensorflow as tf\nimport multiprocessing as mp\nimport os\n\nclass Worker(mp.Process):\n    def __init__(self, gid):\n        self.gid = gid\n        super(Worker, self).__init__()\n\n    def run(self):\n        G = tf.Graph()\n        with G.as_default():\n            x = tf.placeholder(tf.float32, shape=[])\n            y = x * 2\n            sess = tf.Session()\n            print sess.run(y, feed_dict={x: 5})\n\nG = tf.Graph()\nwith G.as_default():\n    sess = tf.Session()\n    with sess.as_default():\n        x = tf.placeholder(tf.float32, shape=[])\n        y = x * 2\n        print sess.run(y, feed_dict={x: 5})\n\nprocs = [Worker(k) for k in range(2)]\nfor p in procs: p.start()\nfor p in procs: p.join()\n```\n\nRemoving the graph/session in master process will solve the problem. So it seems like once there is a session, we cannot use fork?\nThe problem exists with and without GPU.\n\nNOTE:  this code doesn't terminate normally. You'll probably need to manually kill the forked processes after the master exited. \n", "comments": ["The in-process session (i.e. `tf.Session()` with no arguments) is not designed to be `fork()`-safe. If you want to share a set of devices between multiple processes, create a [`tf.train.Server`](https://www.tensorflow.org/versions/r0.8/api_docs/python/train.html#Server) in one process, and create sessions that connect to that server (with `tf.Session(\"grpc://...\")`) in the other processes.\n", "@mrry Does it mean there is a way to create `fork` safe `tf.Session` with `tf.Session(args)`?\n", "@mavenlin \nThe prototype of `tf.Session` is\n\n``` python\ntf.Session.__init__(target='', graph=None, config=None)\n```\n\nHere `target` refers to the execution engine to connect to. That is, one has to run the working session in another process, in distributed mode, and `tf.Session` with arguments is still not `fork()`-safe.\n\nSad news.\n"]}, {"number": 2447, "title": "TensorFlow segfaults on attempting to save a large variable", "body": "The checkpoint format writes the value of a variable into a Protocol Buffer, which has a 2GB limit. The saving mechanism does not validate the size of a variable&mdash;which can be much larger than 2GB&mdash;before attempting to write it into the Protocol Buffer. The following code can cause a segfault:\n\n``` python\nwith tf.Session(\"\") as sess:\n  # Declare a variable of size 4GB.                                                                                                                    \n  var = tf.Variable(tf.zeros([1024, 1024, 1024], dtype=tf.float32))\n  save = tf.train.Saver({var.op.name: var})\n  var.initializer.run()\n  save.save(sess, save_path)\n```\n", "comments": []}, {"number": 2446, "title": "[tf.learn] Fix logistic_regression() summary name conflict.", "body": "If you call logistic_regression() more than once in the same model, you get this error.\n\n> Duplicate tag logistic_regression.X found in summary inputs\n\nFixed by using the variable scope in the summary name.\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 2445, "title": "Add a new contrib/avro directory and initial schema for tf.Example.", "body": "Since Google Cloud Dataflow has native support for [Avro](https://cloud.google.com/dataflow/model/avro-io), we'd like to add Avro support to TensorFlow so that Google Cloud users can train directly on their Dataflow output.\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 2444, "title": "Reinforcement Learning can be really, really slow in tensorflow.", "body": "I was looking at applying DeepMind's [DDPG algorithm](http://arxiv.org/pdf/1509.02971.pdf) to the new OpenAI gym problem set, focusing on continuous control problems like: [the pendulum](https://gym.openai.com/evaluations/eval_I3x3s4DuS7CmIbpaTCs51w)\n\nI noticed that a TensorFlow version of the algorithm was running about 10-20x slower than a theano version of the same algorithm.\n\n[Tensorflow implementation](https://github.com/nivwusquorum/tensorflow-deepq/blob/continuous/tf_rl/controller/continuous_deepq.py)  (but with a couple bugs fixed)\n[Theano implementation](https://github.com/scroyston/rllab/blob/master/rllab/algos/ddpg.py)\n\nWhen I started profiling both implementations in detail, I noticed there were huge performance difference even when just comparing the forward pass of the actor neural network.\n\nI built a quick script to just compare a forward pass of a layered net, varying the layer depth and layer breadth (code and results below), and using a \"mini batch\" size of 64.  (For reference, in Deepmind's DDPG paper, they used a network of 2 hidden layers, sized 400 and 300, mini batch size of 64).\nUsing the handy tf.RunOptions.FULL_TRACE, it showed 99% of the cost was in running the ops themselves (vs op scheduling, startup time, etc).\n\nI then stumbled on a couple of settings that seemed to help:\n    config_proto.intra_op_parallelism_threads = 1\n    config_proto.inter_op_parallelism_threads = 1\n\nThat seemed to improve things a bunch, to where tensorflow was as fast in a couple instances, or \"only\" 2-3x slower.\n\nBefore finding the threading flags, I started looking at speeding up the individual ops. In MatMulOp I replaced the eigen matrix multiply call with a blas call.  That also seemed to help quite a bit.\n\nBelow are some chart of the performance delta vs Theano.\n\"EigenMultiThread\" is just vanilla tensorflow. \n\"EigenSingleThread\" is tensorflow with intra/inter op_parallelism = 1. \n\"BlasSingleThread is the same as \"EigenSingleThread\", with the eigen matrix multiply replaced by blas.\nThe data that makes up these charts can be [found here](https://docs.google.com/a/rgmadvisors.com/spreadsheets/d/1EU9xPFUqELDpRnFUOJUKCDNjm977sUC7AAM29iCwfgc/pubhtml).\n\n![depth2](https://cloud.githubusercontent.com/assets/620227/15438353/62156fc2-1e91-11e6-9b22-5c7cf04aef91.png)\n![depth4](https://cloud.githubusercontent.com/assets/620227/15438368/795d2698-1e91-11e6-979e-ea341f5d6720.png)\n![depth6](https://cloud.githubusercontent.com/assets/620227/15438371/7d03b064-1e91-11e6-9589-33ccf83b926f.png)\n![depth8](https://cloud.githubusercontent.com/assets/620227/15438374/80979718-1e91-11e6-8f88-ecdfeff44fef.png)\n\nI primarily ran these tests on a mac pro (with avx).  I did compile tensorflow with the avx flag.  I also spot checked these results on a linux box and got similar results.  Blas implementation was OpenBlas\n\nI guess my issue/question is: Is there any ongoing work to improve performance for Reinforcement Learning scenarios like these, especially on CPUs? \n\nI have heard DeepMind is moving completely to tensorflow, and they used/are using the new TPU chips. However, they also got a huge performance boost in runtime and error rate over GPUs by [using 16 normal cores asynchronously](http://arxiv.org/pdf/1602.01783v1.pdf) (which btw, I don't think I can set the threading options to 1 and have an [implementation](https://github.com/muupan/async-rl) still work).\n### Environment info\n\nOperating System: \nOS X 10.11.4,  3.5 GHz 6-Core Intel Xeon E5\nand\nFedora 20, 12 physical cores, Xeon X5670  @ 2.93GHz\n\nInstalled version of CUDA and cuDNN: \nNone\n\nIf installed from binary pip package, provide:\nTried both a release version:\npip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0-py2-none-any.whl\n\nand from source, commit hash:\n371d341c7ef22d38b80235bbcb5a6e230546781c\ncompile command:\nbazel build -c opt --copt=-mavx //tensorflow/tools/pip_package:build_pip_package\n(for the Xeon E5 machine)\n### Steps to reproduce\n1. I used the script below to generate the numbers\n   [tf_perf_test.py.zip](https://github.com/tensorflow/tensorflow/files/275280/tf_perf_test.py.zip)\n", "comments": ["@benoitsteiner @rmlarsen: Benoit, Rasmus, are there other configuration options for Eigen that could help improve the performance here? Would any of the post-0.8 performance improvements be likely to help here?\n", "@scroyston Haveyou tried building with the -mfma option as well ? The Mac Pro have shipped with Haswell cpus for quite some time, which support the FMA instructions. This can make a significant difference in performance.\nI'll try to take a look at you testcase next week. There are a few things that we can do to improve the scalability of the code for relatively small matrix multiplication, and some of the options we're considering might help in your case.\n", "Thanks for the suggestion, unfortunately this particular mac pro doesn't seem to have fma:\n[1]    13850 illegal hardware instruction\n\nmachdep.cpu.brand_string: Intel(R) Xeon(R) CPU E5-1650 v2 @ 3.50GHz\nmachdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX SMX EST TM2 SSSE3 CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC POPCNT AES PCID XSAVE OSXSAVE TSCTMR AVX1.0 RDRAND F16C\n", "I have a similar issue with Reinforcement Learning.  I am using the discrete_deepe.py from nivwusquorum [https://github.com/nivwusquorum/tensorflow-deepq/tree/master/tf_rl/controller](url).\n\nI have a dataset with about 50,000 samples and each observation is about 80 points.  This takes about 10 Mins per iteration to processes which seems pretty slow.  I have a GPU and haven't check to see if the individual observations are being computed in that or the CPU.\n", "The threading issue is suspicious.  @rmlarsen, @benoitsteiner: Any thoughts as to why turning off threading would make Eigen 10x faster?  \n", "The dependence on threading does seem suspicious. Could you try to test what happens if you only set one of the threading params to 1? In other words, run with\n\nconfig_proto.intra_op_parallelism_threads = $num_cores\nconfig_proto.inter_op_parallelism_threads = 1\n\nor\n\nconfig_proto.intra_op_parallelism_threads = 1\nconfig_proto.inter_op_parallelism_threads = $num_cores\n", "Using either of those gives about the same performance as without the settings (so current and these two settings are about the same).\n\nWhat is strange is I reinstalled TensorFlow R0.9 (had an earlier R0.9) without GPU support and it was about 1/2 the speed as with GPU support compiled in.  I wonder if I should open a bug report on that?  Doesn't seem to make sense as I am working with 200 x 95 batches (no 1 sample at a time) so IO shouldn't be an issue.  The newer version R0.9 is faster as I suspect it was the np.asarray() fix in the dictionary feeding.  My GPU is a 950 with compute capability of 5.2 (that is what I have compiled into TensorFlow).\n\nI am running on a dual Xeon 6 Core (X5650) 2.66 GHz system with 72GB of RAM, but still think the GPU should improve (or not change) performance.\n", "Just experienced another odd event.  Here are the steps I took\n\n1) Fresh install of \"R0.9\" from github\n2) Configured without GPU (GPU card is install in system)\n3) Built TF\n4) Ran test experiment with data set ~117 seconds/iteration  (203000 observations of 95 data points in 200/batch) \nNOTE: Experiment is compiled with GPU option and loads library when TF supports GPU\n5) Reconfigured with GPU Compute 5.2 (Bazel clean)\n6) Ran test experiment same as in # 4 above , time doubled with GPU support enabled\n7) Reconfigured with GPU Compute 3.5,5.2 -> Same results as # 6\n8) Reconfigured with CPU only again (Bazel clean)\n9) Ran experiment, I didn't get my ~117 second/iteration, I got over 350 seconds/iteration, this was a major surprise\n10) Delete TF directory, re CLONED TF from Github, compiled with CPU only, ran experiment back to 117 second without GPU support\n\nI am not sure exactly what this experiment is indicating, but seems like when the GPU option is added, something is included in the final compiled objects that slows the matrix calculations.  I assume when I went back to the CPU only without a fresh install, some of the old GPU code was not erased but found when the experiment ran.  It didn't load the GPU libraries so it was using a different bit of code than when I first install TF without GPU support.  Hope this helps, let me know if you need more information.\n", "I ran a profile based upon #1824 and got the following.  I am not sure how to read this, but the GPU iteration took over 10ms where the CPU alone is <10ms.  I am running the HEAD of TensorFlow on Ubuntu 15.10 with CUDA 7.5 and cuDNN 4.  The CPU is a dual XEON 6 core processors (24 threads total) with 72 GB or RAM (DDR3).\n\nI have a GTX 950, not the fastest, but is this related to the structure of the graph or simply the data set isn't big enough to get a benefit from the GPU given the IO overhead?\n\nThen networks runs in this case a Batch of 200 x 189 into 5 layers with Dropout() between each layer.  The layers are 140, 120, 100, 80, and 3 as the output.\n\n![screenshot 2016-07-10 15 08 49](https://cloud.githubusercontent.com/assets/18412448/16715534/dd719032-46b0-11e6-81cf-f0b22d96e336.png)\n\n![screenshot 2016-07-10 15 08 58](https://cloud.githubusercontent.com/assets/18412448/16715536/e375b508-46b0-11e6-8380-bb129c380442.png)\n", "Hi, what's the status of this?\n", "@scroyston very nice analysis! Can I ask how you enabled BLAS optimization for matmul?\n", "I did some [investigations](https://github.com/hholst80/tf_sgemm) myself on this. I have not been able to reproduce the performance speedup in `perftest.py` in a real environment. Actually, in our \"production code\" we got a slowdown and I hope I can return to investigate why later on. If anyone have some battle experience to share I am happy to hear about it!\n\nEDIT: The matrices we have are too small to benefit from the OpenBLAS call. The break even seems to be for ~2M floating point instructions for OpenBLAS sgemm to start to outperform Eigens matmul.\n\nMy machine is 75% loaded right now so the numbers should be taken with a grain of salt:\n\n```\n$ pip list | grep tensorflow\ntensorflow (0.11.0rc0)\n```\n\n```\n$ env OMP_NUM_THREADS=1 python perftest.py \nmatmul 1000\n0.11668629897758365\nsgemm 1000\n0.057338198996149004 speedup 2.0\nmatmul 2000\n0.9684892640216276\nsgemm 2000\n0.352693248889409 speedup 2.7\nmatmul 3000\n3.1248108180006966\nsgemm 3000\n1.1585902459919453 speedup 2.7\nmatmul 4000\n7.574722188990563\nsgemm 4000\n2.3656532769091427 speedup 3.2\nmatmul 5000\n14.732756665092893\nsgemm 5000\n5.034700996009633 speedup 2.9\n```\n\n`/proc/cpuinfo`\n\n```\nprocessor       : 0\nvendor_id       : GenuineIntel\ncpu family      : 6\nmodel           : 63\nmodel name      : Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz\nstepping        : 2\nmicrocode       : 0x36\ncpu MHz         : 2802.246\ncache size      : 30720 KB\nphysical id     : 0\nsiblings        : 24\ncore id         : 0\ncpu cores       : 12\napicid          : 0\ninitial apicid  : 0\nfpu             : yes\nfpu_exception   : yes\ncpuid level     : 15\nwp              : yes\nflags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts\nbugs            :\nbogomips        : 4988.57\nclflush size    : 64\ncache_alignment : 64\naddress sizes   : 46 bits physical, 48 bits virtual\npower management:\n```\n", "@rmlarsen @benoitsteiner What's the status of this issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]