[{"number": 3768, "title": "TF_OperationOpType", "body": "Hi,\n\n`TF_Node` [has been renamed](https://github.com/tensorflow/tensorflow/commit/b2c21f7e15cb61d2b1b2ca4154b1da45662f4747) to `TF_Operation`, which involved changing the names of a lot of functions. In particular, `TF_NodeOpType` has been changed to `TF_OperationOpType`. Before it\u2019s been released, I just want to confirm that `Op` in `TF_OperationOpType` is not redundant. Thanks!\n\nRegards,\nIvan\n", "comments": ["@josh11b Is `TF_OperationOpType` right?\n", "I can see what the issue is, but I think we are going to leave it as is.  I try to consistently use the word \"Op\" to talk about the type of nodes/operations.  So Node/Operation : Op as Object : Class.  We renamed \"Node\" -> \"Operation\" in the C and C++ APIs to be consistent with the Python API, though I think \"Node\" is a bit less confusing for this very reason.  Note that I am also trying only abbreviate \"operation\"  using \"oper\", never \"op\".\n", "@josh11b, all your changes have been reverted in [this pull request](https://github.com/tensorflow/tensorflow/pull/3781). Does it mean that you have changed your mind, and it\u2019s going to stay `TF_Node` in the next release?\n", "@girving, I\u2019m sorry for bothering, but maybe you could shed some light on the above question about the name to be used in the next release. The reason I ask is that I\u2019d like to be a bit more proactive and get my code ready for version 0.10 already now. Thank you.\n", "@IvanUkhov Looks like it's `TF_OperationOpType` still.\n", "@girving, I was referring to my last question about @josh11b\u2019s changes that have been reverted.\n\nOn Thu, Aug 18, 2016 at 5:27 PM Geoffrey Irving notifications@github.com\nwrote:\n\n> @IvanUkhov https://github.com/IvanUkhov Looks like it's\n> TF_OperationOpType still.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> \n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3768#issuecomment-240758874,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AADgYFEYDo7VUr597EuHIvzyVWqdFPs8ks5qhHnGgaJpZM4Ji-js\n> .\n", "Please check again.\n", "@girving, I think we\u2019re looking at different branches. I was assuming that the [r0.10](https://github.com/tensorflow/tensorflow/commits/r0.10) branch was representing to what would go into the 0.10 release, and that branch no longer contain the changes. But probably that branch is just an auxiliary one, and one should be looking at master instead. \n", "@IvanUkhov Ah.  I'm not sure what's going into 0.10.  @gunan Do you know if 0.10 will have `TF_Node` or `TF_Operation`?\n", "Ah, this was not supposed to happen. Looks like there has been a mistake.\nLooks like there was a misunderstanding.\nI will revert my PR to r0.10 to reinstate Josh's changes.\n"]}, {"number": 3767, "title": "Incorrect Hessian of the tf.nn.sigmoid function", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Linux 3.13.0-85-generic #129-Ubuntu SMP Thu Mar 17 20:50:15 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root 322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   tensorflow\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.10.0rc0\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### Steps to reproduce\n\nI am trying to evaluate hessian of a logistic function via the following program.\n\n```\n# x = 1\n# logistic(x) = 0.7310585786300049\n# gradient(x) = logistic(x) * (1 - logistic(x)) = 0.19661193324148185\n# hessian(x) = logistic(x) * (1 - logistic(x)) ^ 2 + -logistic(x) * (1 - logistic(x)) * logistic(x) = -0.09085774767294841\nimport tensorflow as tf\n\nx = tf.Variable(1, name='x', dtype=tf.float32)\n#logistic = 1. / (1 + tf.exp(-x))\nlogistic = tf.nn.sigmoid(x, name='logistic')\n\ngradient = tf.gradients(logistic, x, name='gradient')[0]\nhessian = tf.gradients(gradient, x, name='hessian')[0]\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nprint(sess.run([logistic, gradient, hessian]))\n```\n\nI directly run the program, it produce the wrong result. \n\n`TypeError: Fetch argument None of None has invalid type <type 'NoneType'>, must be a string or Tensor`\n\nIt seems that `hessian` is `None`.\n\nHowever, if I write the logistic function manually `logistic = 1. / (1 + tf.exp(-x))`, it produces the correct result.  `[0.7310586, 0.19661197, -0.090857767]`\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["The problem is that the gradient of `tf.sigmoid` is a dedicated C++ op called `SigmoidGrad`...which has no registered gradient:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L34\n", "This would be great to fix, and `TanhGrad` could be fixed at the same time.\n", "To clarify: there's no need to make new custom ops.  A few lines of Python writing the gradient of these two routines in terms of other existing ops suffices.\n", "Thanks for the clarification! So for my problem, the current solution is just replacing tf.sigmoid(x) with 1/(1+exp(-x)), right?\n", "@cjauvin Yep, that works, though registering the gradient is only a couple lines of code.\n", "@girving I didn't see tests for the other gradient functions, but I submitted a pull request for the two gradients. https://github.com/tensorflow/tensorflow/pull/3807\n", "@girving for completeness can you show how this is done?\n", "@hholst80 Take a look at #3807.\n", "@girving Have you fixed the bug? I can directly run the program and get right outputs\n", "@DjangoPeng: @chemelnucfin got part of the way through https://github.com/tensorflow/tensorflow/pull/3807 but then got pulled away.  You're welcome to take over if you want!\n", "@girving Should I create a new PR to fix the bug? \n", "Whoops, sorry: @rmlarsen just fixed this, and it should be pushed out soon.  Should have commented earlier.\n", "@girving, so should I close this PR?\n\nOn Sun, Sep 4, 2016 at 10:40 AM, Geoffrey Irving notifications@github.com\nwrote:\n\n> Whoops, sorry: @rmlarsen https://github.com/rmlarsen just fixed this,\n> and it should be pushed out soon. Should have commented earlier.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3767#issuecomment-244616989,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ADzDDPbflSqb4SvSBRNXPgH8YYB9TXy5ks5qmwKcgaJpZM4Ji98U\n> .\n", "Yep, please close the PR.  Don't close this bug, though; that should wait until @rmlarsen's change is pushed out. \n", "The fix has been pushed: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L443\n"]}, {"number": 3766, "title": "Tensorflow distributed RNN crashed on ProtoBuf when using 3*512 rnn. ", "body": "### Environment info\n\nOperating System:\nLinux XNLPEXP2 4.4.0-24-generic #43-Ubuntu SMP Wed Jun 8 19:27:37 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\nAzure VM 8 cores, 56GB memory\n\nIf installed from binary pip package, provide:\npip 8.1.2 from /home/xubixiong/.local/lib/python2.7/site-packages (python 2.7)\n0.10.0rc0\n### Steps to reproduce\n1. Create 2_PS + 2_worker, \n2. Try 3 \\* 512 RNN, (I changed some code in rnn/translate to make it distributed), worker0 will 100% crash on some code related to TensorBuf and run global_step, the stack is at the end of this post, same everytime\n3. Smaller the RNN, make it 1*128, it works fine. \n### What have you tried?\n1.  tensorflow.python.framework.errors.UnavailableError is said to not used in the document. \n2. The crashing line is TensorBuffer\\* buf = tensorflow::TensorCApi::Buffer(src); in tensorflow/core/client/tensor_c_api.cc:\n3. I try to smaller the RNN size and layers, then there is no problem. \n4. Is it a known issue, I did find some issue relating to TensorBuf, but not the exactly same case. \n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\nE tensorflow/core/client/tensor_c_api.cc:485]\nTraceback (most recent call last):\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 346, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 343, in main\n    train()\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 215, in train\n    with sv.managed_session(server.target, config=sess_config) as sess:\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in **enter**\n    return self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 942, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 768, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 357, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 931, in managed_session\n    start_standard_services=start_standard_services)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 683, in prepare_or_wait_for_session\n    self.start_standard_services(sess)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 624, in start_standard_services\n    current_step = training_util.global_step(sess, self._global_step)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_util.py\", line 50, in global_step\n    return int(sess.run(global_step_tensor))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call    raise type(e)(node_def, op, message)tensorflow.python.framework.errors.UnavailableError\n", "comments": ["@josh11b What's `UnavailableError`?  I haven't seen it before.\n", "\"Raised when the runtime is currently unavailable.\"\nSince I don't see any TensorFlow code creating that error, I suspect it is coming from GRPC.\n", "@mrry Can you take a look?\n", "Hi, guys,\n\nAlso When i used small RNN, such as 1*128, the same exception raise on worker 0 when I restore from the checkpoint, after worker run another hundreds of  steps from the ckpt. And after worker 0 is down, worker 1 can continue running until i shut down ps server manually.\nFresh running on small RNN is fine as I said in the post.\n\nI have monitored the memory usage, it is always below 10%.\n\nThank you!\n\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nOn Sat, Aug 13, 2016 at 12:43 AM +0800, \"Geoffrey Irving\" <notifications@github.com<mailto:notifications@github.com>> wrote:\n\n@mrryhttps://github.com/mrry Can you take a look?\n\n## \n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239496725, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfoEY9aRefP7sqdTSLlLS593n4b_Vks5qfKK-gaJpZM4Ji93R.\n", "Sorry for word missing in last mail.\nAfter restoring worker 0 raise unavailable error after worker 1 run hundreds of steps from ckpt. It seems like a timeout.\n\nAlso When i used small RNN, such as 1*128, the same exception raise on worker 0 when I restore from the checkpoint, after worker 1 run another hundreds of steps from the ckpt. And after worker 0 is down, worker 1 can continue running until i shut down ps server manually.\nFresh running on small RNN is fine as I said in the post.\n\nI have monitored the memory usage, it is always below 10%.\n\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nOn Sat, Aug 13, 2016 at 12:43 AM +0800, \"Geoffrey Irving\" <notifications@github.com<mailto:notifications@github.com>> wrote:\n\n@mrryhttps://github.com/mrry Can you take a look?\n\n## \n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239496725, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfoEY9aRefP7sqdTSLlLS593n4b_Vks5qfKK-gaJpZM4Ji93R.\n", "When you see an `UnavailableError`, this is most likely caused by another (remote) process crashing. Since it seems to depend on the size of your RNN, I suspect something is hitting the 2GB limit for protobuf serialization, but it's not clear what that might be. Can you capture the logs for any other crashed processes to see if there's more information in there?\n", "Of course.\nAlthough so far worker 0 is the only one who crashed. And small rnn worker 0 also crashes when it restores from ckpt.\n\nBut for distributed training , 2 GB shouldn't be enough. I saw some fix about the protonuf  2GB crash in github. It seems being done by add a check and return. Any other plans to make it fit for larger data?\n\nThank you!\n\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nOn Sat, Aug 13, 2016 at 1:22 AM +0800, \"Derek Murray\" <notifications@github.com<mailto:notifications@github.com>> wrote:\n\nWhen you see an UnavailableError, this is most likely caused by another (remote) process crashing. Since it seems to depend on the size of your RNN, I suspect something is hitting the 2GB limit for protobuf serialization, but it's not clear what that might be. Can you capture the logs for any other crashed processes to see if there's more information in there?\n\n## \n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239506773, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfiBBTlz4wMZnfK2jYPCb1mv-mLBqks5qfKvrgaJpZM4Ji93R.\n", "And, when i shut down ps, worker 1 will raise this exception, followed by a network error message. But I am sure when work 0 raise this exception, no other process is down.\n\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nOn Sat, Aug 13, 2016 at 1:22 AM +0800, \"Derek Murray\" <notifications@github.com<mailto:notifications@github.com>> wrote:\n\nWhen you see an UnavailableError, this is most likely caused by another (remote) process crashing. Since it seems to depend on the size of your RNN, I suspect something is hitting the 2GB limit for protobuf serialization, but it's not clear what that might be. Can you capture the logs for any other crashed processes to see if there's more information in there?\n\n## \n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239506773, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfiBBTlz4wMZnfK2jYPCb1mv-mLBqks5qfKvrgaJpZM4Ji93R.\n", "The current recommend workaround to the 2GB limit is to use a [variable partitioner](https://www.tensorflow.org/versions/r0.10/api_docs/python/state_ops.html#variable-partitioners-for-sharding) that splits the variable transfer into multiple smaller transfers.\n\nGetting an exception in a worker when you call a PS task that it's using is expected behavior. A typical distributed deployment using `tf.train.Supervisor` (e.g. on Kubernetes) will automatically restart any process when it fails.\n", "Ok, i will try it out. Thank you!\n\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nOn Sat, Aug 13, 2016 at 1:51 AM +0800, \"Derek Murray\" <notifications@github.com<mailto:notifications@github.com>> wrote:\n\nThe current recommend workaround to the 2GB limit is to use a variable partitionerhttps://na01.safelinks.protection.outlook.com/?url=https%3a%2f%2fwww.tensorflow.org%2fversions%2fr0.10%2fapi_docs%2fpython%2fstate_ops.html%23variable-partitioners-for-sharding&data=01%7c01%7cbix%40064d.mgd.microsoft.com%7cb294e90bda764d872a2f08d3c2d94235%7c72f988bf86f141af91ab2d7cd011db47%7c1&sdata=E%2b66PmyQw8J1Lh4hgi2aqwwFNnu7biH3E8%2bi9sQHdbY%3d that splits the variable transfer into multiple smaller transfers.\n\nGetting an exception in a worker when you call a PS task that it's using is expected behavior. A typical distributed deployment using tf.train.Supervisor (e.g. on Kubernetes) will automatically restart any process when it fails.\n\n## \n\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239514105, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfhQY9osHVqKHdYXIOA0tysLHz8bWks5qfLKSgaJpZM4Ji93R.\n", "@bixiongxu I'm trying to change the seq2seq model to distributed model. Could I ask you some question about that? My email is pjt73651@gmail.com, look forward to your reply! \n", "@DjangoPeng, my mail is peter@questmatic.commailto:peter@questmatic.com.\nWelcome to discuss.\n\nFrom: Jingtian Peng [mailto:notifications@github.com]\nSent: Monday, August 15, 2016 10:00 AM\nTo: tensorflow/tensorflow tensorflow@noreply.github.com\nCc: Bixiong Xu bix@microsoft.com; Mention mention@noreply.github.com\nSubject: Re: [tensorflow/tensorflow] Tensorflow distributed RNN crashed on ProtoBuf when using 3*512 rnn. (#3766)\n\n@bixiongxuhttps://github.com/bixiongxu I'm trying to change the seq2seq model to distributed model. Could I ask you some question about that? My email is pjt73651@gmail.commailto:pjt73651@gmail.com, look forward to your reply!\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/tensorflow/tensorflow/issues/3766#issuecomment-239714937, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AUBCfvy_7o5M7Q8ytjpbYZU8TzIWHIvEks5qf8gngaJpZM4Ji93R.\n", "@bixiongxu My problem is that my workers server hanging on forever, and I don't know what happened in the `Session.run()`. Could you have a look at [issue#3762](https://github.com/tensorflow/tensorflow/issues/3762). \n", "@mrry I have added variable partition in my code, but still got the same exception (on same line: E tensorflow/core/client/tensor_c_api.cc:485)  on Worker 0, while Worker1 can start training... \n\nThe whole model is wrapped by a variable_scope with a partitioner, so in theory it will work to solve the problem. I will keep trying and appreciate to have some suggestions from you. \n\nI extracted some code as below: \n\n```\n    with tf.variable_scope(\"soulmate\", partitioner=tf.variable_axis_size_partitioner(\n            max_shard_bytes=33554432)):\n        with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n            cluster=cluster)) as dev:\n            # Create model.\n            print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n            model = create_model_fresh(False)   \n            summary_op = tf.merge_all_summaries()\n            init_op = tf.initialize_all_variables()\n            print(model.global_step.name)\n            print(model.learning_rate.name)\n            # Create a \"supervisor\", which oversees the training process.\n            sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=FLAGS.train_dir,\n                                 init_op=init_op,\n                                 summary_op=None,\n                                 saver=None,\n                                 global_step=None,\n                                 recovery_wait_secs=10)\n\n            print(\"Global_step is placed on \" + model.global_step.device + \", will be initialized on \" + model.global_step.initializer.device)\n            if FLAGS.task_index == 0:\n                print(\"Worker %d: Initializing session...\" % FLAGS.task_index)\n            else:\n                print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.task_index)\n            time_begin = time.time()    \n            with sv.managed_session(server.target) as sess:\n                print(\"Worker %d: Session initialization complete.\" % FLAGS.task_index)\n                print(\"in %f secs.\" % (time.time()-time_begin))\n\n                # This is the training loop.\n                step_time, loss = 0.0, 0.0\n                current_step = 0\n                current_global_step = 0\n                previous_losses = []\n                while not sv.should_stop():\n                    # in [0, 1] and use the corresponding interval in train_buckets_scale.\n                    random_number_01 = np.random.random_sample()\n                    bucket_id = min([i for i in xrange(len(train_buckets_scale))\n                                   if train_buckets_scale[i] > random_number_01])\n\n                    # Get a batch and make a step.\n                    start_time = time.time()\n                    encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n                        train_set, bucket_id)\n                    _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n                                               target_weights, bucket_id, False)\n\n                    step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n                    loss += step_loss / FLAGS.steps_per_checkpoint\n                    current_step += 1\n\n            sv.stop()\n```\n\nAnd the stack: \n\nE tensorflow/core/client/tensor_c_api.cc:485]\nI tensorflow/core/distributed_runtime/master_session.cc:651] DeregisterGraph error: Aborted: Graph handle is not found: . Possibly, this worker just restarted.\nI tensorflow/core/distributed_runtime/master_session.cc:651] DeregisterGraph error: Aborted: Graph handle is not found: . Possibly, this worker just restarted.\nTraceback (most recent call last):\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 349, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 346, in main\n    train()\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/translate.py\", line 239, in train\n    target_weights, bucket_id, False)\n  File \"/home/xubixiong/soulmate/intent_trainer/rnn/translate/seq2seq_model.py\", line 228, in step\n    outputs = session.run(output_feed, input_feed)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 382, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 655, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 723, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 743, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.UnavailableError\n", "I try another partitioner:  **tf.min_max_variable_partitioner(max_partitions=50, axis=0,min_slice_size=8 << 10)** , got same crash on worker 0. \nI printed the partitioned variables, they seems to be disposed properly, as below: \nsoulmate/proj_w/part_6:0\nsoulmate/proj_w/part_7:0\nsoulmate/proj_w/part_8:0\nsoulmate/proj_w/part_9:0\nsoulmate/proj_w/part_10:0\nsoulmate/proj_w/part_11:0\nsoulmate/proj_w/part_12:0\n.......\n(These are printed after \"with sv.managed_session(server.target) as sess:\", so the session is initialized successfully. )\n\nWork 0 and Work 1 printed the params with the same name. But still only worker 1 started training and woker 0 crashed. \n\nAccording to my tryings, I doubt the root cause is not the 2G limit of protobuf, reasons: \n1. Only worker 0 crashed. If we hit 2G limit, worker1 should has the same problem. But worker 1 can run normally on big RNN while worker 0 cannot. \n2. Worker 0 always crash on the first **session.run**, if I set the saver/summary_op/global_step in the supervisor, it will crash on the **prepare_or_wait_for_session** in which session will run global_step. \n3. Smaller RNN can run fresh params with no problem, but when restoring from ckpt,  worker 0 crashes everytime with same exception, no matter big or small. \n4. Worker 0 spends 2x time to run every step ( in small RNN ). I suppose worker 0 is doing some extra job or holding a lower priority lock on the params?  \n5. The interval between worker 0 starting and crashing is always about 25 minutes.... so, is there any timeout? \n\nSo the problem might exist in the **extra job** of worker 0, not saver and summary because I have not used them. Do you have some clues? \n", "@bixiongxu Can you get a core dump from the crashed worker task? (The error message just suggests that the worker restarted, but gives no clue about _why_ this might have happened.)\n", "@mrry sorry I didn't explain right. It is not a crash but just a UnavailableError exception and Exit(code: 1), so there won't be a core dump I think. \nAnyway, today I dig into the source code and location the failing position:  \nin **tensorflow/core/distributed_runtime/rpc/grpc_session.cc** line 186: \n**TF_RETURN_IF_ERROR(RunProto(&call_options, &req, &resp));**\nI found a timeout option in the **call_options**, and I suspect big RNN will cause a timeout. I try to set a \n**run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE, timeout_in_ms=10000000)** when I call **outputs = session.run(output_feed, input_feed, options=run_options).** and now is trying to see if it can pass through. \nAppreciate to have your suggestions...\n", "@mrry sorry for some typo in last post and I have fixed in the post. \nAnd can you recommend some tool for debugging the tensorflow source? I am not so familiar with Linux coding envioronment, but I think there should be some handy tool can do breakpoint / step over / step in ....... . I did try gdb, but it cannot link to the source code, maybe I didn't build the source right?  ( I followed the [https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#installing-from-sources], it works)\n", "**Update**\nThe timeout value is passed through, but no effect :(, still raise same exception.  \n\nThen I thought maybe too many variables for only 2 ps, I adjusted the cluster to 1 worker vs 4 ps, something new is happened. \nThe worker 0 still raise the same exception, and 2 of the PS servers raise: **'unit.device' Must be non NULL** in tensorflow/core/distributed_runtime/graph_mgr.cc:55. \nNot sure if they are related. \n", "@mrry can you take another look?\n", "Without the crash dump from worker0, or some way to reproduce the bug, there's not much we can do. With `gdb` were you able to get a stack trace at least?\n", "@mrry I think there is no dump, because it is not a crash of process. \n**in tensorflow/core/distributed_runtime/rpc/grpc_session.cc line 186: \nTF_RETURN_IF_ERROR(RunProto(&call_options, &req, &resp));**\nThe process failed on this point and raise a Unavailable Error then exit. \n\nIf you want to investigate the potential problem of the grpc, I can give you the code package, which can 100% re-raise the problem, by setting up a 2 worker and 1 ps cluster, of which worker 0 will always fail if we use 3*512 or bigger size. \nThe code is modified from rnn/translate, with model unmodified and some extra cluster code. \n\nOf course there might be some other causes, such as using VMs environment, no GPU or some codes in rnn/translate are not fit for cluster. \n", "Have you had any luck solving this problem on your own @bixiongxu? I would try the following..\n1. Try tensorflow 0.10rc1 or the nightly build since 0.8 is now quite old.\n2.  If that doesn't work try to simplify your example to the simplest test that reproduces the behavior. Include the code and exact instructions on how to reproduce it.\n\nWe cannot guarantee that we will have time to look into it, but others in the community definitely would have a chance of helping you then too. However, without a reproducible test case, we will close the issue as \"cannot reproduce.\"\n", "Hi Andrew\n\nI just put it aside for now as one machine is ok for my purpose so far.  But I will definitely come back to it this month.\nI will update you when I get something new. Thank you for your help.\n\nGet Outlook for iOShttps://aka.ms/o0ukef\n\n---\n\nFrom: Andrew Selle <notifications@github.com<mailto:notifications@github.com>>\nSent: ???, ?? 13, 2016 2:00 ??\nSubject: Re: [tensorflow/tensorflow] Tensorflow distributed RNN crashed on ProtoBuf when using 3*512 rnn. (#3766)\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com<mailto:tensorflow@noreply.github.com>>\nCc: Bixiong Xu <bix@microsoft.com<mailto:bix@microsoft.com>>, Mention <mention@noreply.github.com<mailto:mention@noreply.github.com>>\n\nHave you had any luck solving this problem on your own @bixiongxuhttps://na01.safelinks.protection.outlook.com/?url=https%3a%2f%2fgithub.com%2fbixiongxu&data=02%7c01%7cbix%40064d.mgd.microsoft.com%7c6a8afe7337af40880a8e08d3db369c1c%7c72f988bf86f141af91ab2d7cd011db47%7c1%7c0%7c636093000002884346&sdata=VIRlIbQtimFps41JLFjojxP5i%2bykG5W4sukNt1z679A%3d? I would try the following..\n1.  Try tensorflow 0.10rc1 or the nightly build since 0.8 is now quite old.\n2.  If that doesn't work try to simplify your example to the simplest test that reproduces the behavior. Include the code and exact instructions on how to reproduce it.\n\nWe cannot guarantee that we will have time to look into it, but others in the community definitely would have a chance of helping you then too. However, without a reproducible test case, we will close the issue as \"cannot reproduce.\"\n\n## \n\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://na01.safelinks.protection.outlook.com/?url=https%3a%2f%2fgithub.com%2ftensorflow%2ftensorflow%2fissues%2f3766%23issuecomment-246434500&data=02%7c01%7cbix%40064d.mgd.microsoft.com%7c6a8afe7337af40880a8e08d3db369c1c%7c72f988bf86f141af91ab2d7cd011db47%7c1%7c0%7c636093000002884346&sdata=CwEvdWuPKiXpcMtNt2vKOR%2bgeLWE8R8j5D1BobIUi0U%3d, or mute the threadhttps://na01.safelinks.protection.outlook.com/?url=https%3a%2f%2fgithub.com%2fnotifications%2funsubscribe-auth%2fAUBCftLZUWuNTG5KXJI4HGddSlCVfyh2ks5qpZMdgaJpZM4Ji93R&data=02%7c01%7cbix%40064d.mgd.microsoft.com%7c6a8afe7337af40880a8e08d3db369c1c%7c72f988bf86f141af91ab2d7cd011db47%7c1%7c0%7c636093000002894359&sdata=jXsqWqziuBAQYWegv%2fSZKwsyMMQgrg7F1t6pzdQQ1y8%3d.\n", "Closing automatically due to lack of recent activity. Please reopen when further information becomes available. Thank you.\n"]}, {"number": 3765, "title": "Don't know if tensorflow supports this type of restoring a model.", "body": "I am new here so I have a basic question. I want to train a cnn for 20000 steps. In the 100th step I want to save all variables and after that I want to re-run my code restoring model and starting from the 101- step. I am trying to make it work with tensorflow documentation: https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html\nbut I don't know if I am right. When I re-run my code I don't initialize again variables because I run saver.restore and according to my accuracy it works. I achieve big accuracy from the beginning so I think model is restored. Although, I would like to start the loop from the 101 - step. So:\n1st question: Am I saving and restoring in the right way?\n2nd question: How can I start from the 101-step when re-running?\n", "comments": ["Perhaps you want something like this?\n\nhttps://www.youtube.com/watch?v=Lx8JUJROkh0\n\nhttps://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/04_Save_Restore.ipynb\n\nI found the TensorFlow API and doc very confusing when trying to save and restore the whole model instead of just its variables.\n", "I think that's not what I need. All I need is to split my training because I am gonna use a big dataset in my final project and I won't be able to train my network for a lot of hours continuously. I would possibly need to train for 2 hours, save my model and after a while restore it to continue training. I can't find a tutorial like this which explains this problem exactly as I want. \n", "Please ask questions like this on StackOverflow.  Github issues are for bug reports and feature requests.\n", "I asked this question as feature request because I haven't seen anything like this yet. Maybe I miss something. Thank you.\n", "Everything you've described is within the normal saving / restoring functionality of TensorFlow.  If you're unsure how to use that functionality, please ask a StackOverflow question.\n"]}, {"number": 3764, "title": "iOS: No OpKernel was registered to support Op 'RandomShuffleQueue' with these attrs", "body": "I am trying to load the simple trained model, but I am facing the following issue:\n\n```\nNo OpKernel was registered to support Op 'RandomShuffleQueue' with these attrs\n\n[[Node: shuffle_batch_1/random_shuffle_queue = RandomShuffleQueue[capacity=1003, \ncomponent_types=[DT_FLOAT, DT_FLOAT, DT_FLOAT], container=\"\", min_after_dequeue=1000,\nseed=0, seed2=0, shapes=[[227,227,3], [10], [18]], shared_name=\"\"]()]]\n```\n\nThanks,\n", "comments": ["The Android bit strips out a bunch of ops that are typically used only for training, in order to reduce code size.  @petewarden Is there a way to turn the stripping off?\n", "The list of supported ops is in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/makefile/tf_op_files.txt. \n\nIf you add the right cc files for the RandomShuffleQueue op there and rebuild, hopefully you'll get past that problem. We need to document the limits on the ops on mobile more clearly too.\n"]}, {"number": 3763, "title": "contrib.learn.LinearClassifier steps not working as tutorial instructs", "body": "In tutorial it is written\n\nclassifier.fit(x=x_train, y=y_train, steps=200)\nThe state of the model is preserved in the classifier, which means you can train iteratively if you like. For example, the above is equivalent to the following:\nclassifier.fit(x=x_train, y=y_train, steps=100)\nclassifier.fit(x=x_train, y=y_train, steps=100)\n\nBut whne I try on my own, the same step parameter and evaluate the model, the results are all the time the same, only when I increase the parameter, the accuracy changes. Based on the behavior I believe the steps are rememebered and only new steps are fitted/performed.\nSo the equivalent is\nclassifier.fit(x=x_train, y=y_train, steps=100)\nclassifier.fit(x=x_train, y=y_train, steps=200)\n\nCan anybody check this?\n", "comments": ["@ilblackdragon The docstrings don't seem clear on which behavior is correct.  Can you comment?\n", "We've revamped the docs. I'm not sure that the tutorial is still there. I haven't seen an new issue filed on this. Feel free to open a new github issue if the problem still persists in recent versions."]}, {"number": 3762, "title": "PS is bound in GPU:0 by default, we can't change!", "body": "I modified the seq2seq , mnist_replica and ptb model into distributed training model. But all of them have the same problem, that is when I start the server and `server.join()` the ps. The ps was bound in GPU:0 by default, and I can't change the setting. I follow the tutorial of distributed tensorflow [here](https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html#putting-it-all-together-example-trainer-program). \nThis is part of my translate.py :\n\n```\ndef train():\n\n  # set distributed configs\n  ps_hosts = [\"9.91.9.130:2222\"]\n  worker_hosts = [\"9.91.9.130:2223\", \"9.91.9.130:2224\"]\n  #worker_hosts = [\"9.91.9.130:2223\"]\n\n  cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n  server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n        server.join()\n  elif FLAGS.job_name == \"worker\":\n      # Worker server \n      is_chief = (FLAGS.task_index == 0)      \n      gpu_num = FLAGS.task_index + 1\n      #with tf.Graph().as_default():\n      with tf.device(tf.train.replica_device_setter(cluster=cluster,\n          worker_device=\"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu_num))):\n      #with tf.device(\"/gpu:%d\" % FLAGS.task_index):\n          \"\"\"Train a en->fr translation model using WMT data.\"\"\"\n```\n\nThis is the GPU info:\n\n```\nFri Aug 12 09:17:07 2016\n+------------------------------------------------------+\n| NVIDIA-SMI 352.39     Driver Version: 352.39         |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           On   | 0000:06:00.0     Off |                    0 |\n| N/A   67C    P0    63W / 149W |  11099MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           On   | 0000:07:00.0     Off |                    0 |\n| N/A   45C    P0    72W / 149W |  10986MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  Tesla K80           On   | 0000:85:00.0     Off |                    0 |\n| N/A   76C    P0    66W / 149W |  11049MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   3  Tesla K80           On   | 0000:86:00.0     Off |                    0 |\n| N/A   57C    P0    75W / 149W |    223MiB / 11519MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      9198    C   python                                       10940MiB |\n|    0     29865    C   python                                          64MiB |\n|    0     29961    C   python                                          64MiB |\n|    1      9198    C   python                                          64MiB |\n|    1     29865    C   python                                          64MiB |\n|    1     29961    C   python                                       10827MiB |\n|    2      9198    C   python                                          64MiB |\n|    2     29865    C   python                                       10891MiB |\n|    2     29961    C   python                                          64MiB |\n|    3      9198    C   python                                          64MiB |\n|    3     29865    C   python                                          64MiB |\n|    3     29961    C   python                                          64MiB |\n+-----------------------------------------------------------------------------+\n```\n\nAs you can see, the pid 9198 is the `ps server` process, cause it occupied the GPU:0, I have to bind the `worker server` in GPU:1 and GPU:2.\nCould we add a function or API to set the `ps server device` by configuration or arguments?\n", "comments": ["What happens if you set the `ps_tasks` argument to `replica_device_setter`?\n", "@girving I set a cluster in one machine with 4 GPUs, and tried the `ps_tasks=1`. Then I run 4 scripts in the machine:\n\n```\n#PS0\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=ps --task_index=0\n\n#Worker0\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=0\n\n#Worker1\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=1\n\n#Worker2\ndl@dl130:~/PJT$ python rnn/translate/translate.py --job_name=worker --task_index=2\n```\n\nAnd I changed the code:\n\n```\n  ps_hosts = [\"9.91.9.130:2222\"]\n  worker_hosts = [\"9.91.9.130:2223\", \"9.91.9.130:2224\", \"9.91.9.130:2225\"]\n  #worker_hosts = [\"9.91.9.130:2223\"]\n\n  cluster = tf.train.ClusterSpec({\"ps\":ps_hosts, \"worker\":worker_hosts})\n  server = tf.train.Server(cluster,\n                            job_name=FLAGS.job_name,\n                            task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n        server.join()\n  elif FLAGS.job_name == \"worker\":\n      # Worker server \n      is_chief = (FLAGS.task_index == 0)      \n      gpu_num = FLAGS.task_index\n      with tf.Graph().as_default():\n        with tf.device(tf.train.replica_device_setter(cluster=cluster,\n            worker_device=\"/job:worker/task:%d/gpu:%d\" % (FLAGS.task_index, gpu_num),\n            #ps_device=\"/job:ps/task:0/cpu:0\",\n            ps_tasks=1)):\n```\n\nBut the chief task(worker0) get errors `CUDA_ERROR_OUT_OF_MEMORY: failed to allocate 10.54G down to 370.68M`, cause the memory is allocated to the `ps task(ps0)`\n\n```\nCreating 1 layers of 10 units.\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nI'm in chief task!\nfinish creating model\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 10.54G (11319703296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n...\nE tensorflow/stream_executor/cuda/cuda_driver.cc:965] failed to allocate 370.68M (388683520 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n...\nE tensorflow/stream_executor/cuda/cuda_blas.cc:361] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED\n...\ntensorflow.python.framework.errors.InternalError: Blas SGEMM launch failed : a.shape=(1, 20), b.shape=(20, 10), m=1, n=10, k=20\n```\n\nWhat's more, the other worker task hanging forever with the log info:\n\n```\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7616 get requests, put_count=3642 evicted_count=1000 eviction_rate=0.274574 and unsatisfied allocation rate=0.666229\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n```\n\nAnd even though I solve the `memory problem` by putting the worker task in other GPU, the `stuck problem` is still exist. I have no idea about what happend in the hanging process.\n", "@DjangoPeng They're called parameter servers because that's where the parameters are stored.  If you have one of them, all of the variables with be divided among the 1 parameter servers, which means all of the variables will go on the same machine.\n", "@girving But I can't change the ps device, I know it's bound in /GPU:0 by default. So it may cause the `conflicting memory allocation` with worker0. And why all workers hanging even though the ps is ready, is it a bug? or something else?\n", "@DjangoPeng what do you mean \"conflicting memory allocation\"? If you want to change ps location, you could try switch the launching sequence, making ps to be the last one to launch. But I have no GPU env to test, sorry if I didn't get it. \n", "@bixiongxu `conflicting memory allocation` means both ps0 and worker0 apply memory in GPU0, while I have fixed it by shifting one #GPU for workers. The device placement is as below:\n\n```\nps0: GPU0\nworker0: GPU1\nworker1: GPU2\nworker2: GPU3\n```\n\nBut now my problem is the `stuck worker process`, .I can't start training even though the first step. The  log for each worker is the same, except the chief worker.\nChief worker has no output log, but just stuck at `session.run(output_feed, input_feed)`.\n\n```\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nstart running session\n```\n\nAll other worker:\n\n```\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\nstart running session\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7615 get requests, put_count=3642 evicted_count=1000 eviction_rate=0.274574 and unsatisfied allocation rate=0.666185\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\n```\n", "@DjangoPeng Hopefully your original issue is fixed by #4008.  \nI'm going to close this issue - please reopen the original issue is still presetn.\n\n> But now my problem is the stuck worker process,\n\nPlease can you report unrelated issues in a separate thread.\n"]}, {"number": 3761, "title": "Minor comment change.", "body": "Erroneous comment from copying the graph freezing test.\n", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n"]}, {"number": 3760, "title": "Ubuntu 16.04 / Gpu 1080 Version compile Error (Tensorflow installation)", "body": "Hello, I recently got a new machine gpu 1080 and I did follow most of instructions via http://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow.\nAnyone successfully installed Ubuntu 16.04 GPU with that version?? Let me know\n\nBut the Issue is it doesn't let me complie via bazel.\n\nThe following code...\n\nERROR: /home/ryan/git_ryan/tensorflow/tensorflow/core/kernels/BUILD:1655:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:dense_update_ops_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/dense_update_ops_gpu.cu.cc':\n  '/usr/local/cuda-8.0/include/cuda_runtime.h'\n  '/usr/local/cuda-8.0/include/host_config.h'\n  '/usr/local/cuda-8.0/include/builtin_types.h'\n  '/usr/local/cuda-8.0/include/device_types.h'\n  '/usr/local/cuda-8.0/include/host_defines.h'\n  '/usr/local/cuda-8.0/include/driver_types.h'\n  '/usr/local/cuda-8.0/include/surface_types.h'\n  '/usr/local/cuda-8.0/include/texture_types.h'\n  '/usr/local/cuda-8.0/include/vector_types.h'\n  '/usr/local/cuda-8.0/include/library_types.h'\n  '/usr/local/cuda-8.0/include/channel_descriptor.h'\n  '/usr/local/cuda-8.0/include/cuda_runtime_api.h'\n  '/usr/local/cuda-8.0/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-8.0/include/driver_functions.h'\n  '/usr/local/cuda-8.0/include/vector_functions.h'\n  '/usr/local/cuda-8.0/include/vector_functions.hpp'\n  '/usr/local/cuda-8.0/include/common_functions.h'\n  '/usr/local/cuda-8.0/include/math_functions.h'\n  '/usr/local/cuda-8.0/include/math_functions.hpp'\n  '/usr/local/cuda-8.0/include/math_functions_dbl_ptx3.h'\n  '/usr/local/cuda-8.0/include/math_functions_dbl_ptx3.hpp'\n  '/usr/local/cuda-8.0/include/cuda_surface_types.h'\n  '/usr/local/cuda-8.0/include/cuda_texture_types.h'\n  '/usr/local/cuda-8.0/include/device_functions.h'\n  '/usr/local/cuda-8.0/include/device_functions.hpp'\n  '/usr/local/cuda-8.0/include/device_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/device_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/device_double_functions.h'\n  '/usr/local/cuda-8.0/include/device_double_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_20_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_20_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_32_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_32_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_35_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_60_atomic_functions.h'\n  '/usr/local/cuda-8.0/include/sm_60_atomic_functions.hpp'\n  '/usr/local/cuda-8.0/include/sm_20_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_20_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_30_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_30_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_32_intrinsics.h'\n  '/usr/local/cuda-8.0/include/sm_32_intrinsics.hpp'\n  '/usr/local/cuda-8.0/include/sm_35_intrinsics.h'\n  '/usr/local/cuda-8.0/include/surface_functions.h'\n  '/usr/local/cuda-8.0/include/texture_fetch_functions.h'\n  '/usr/local/cuda-8.0/include/texture_indirect_functions.h'\n  '/usr/local/cuda-8.0/include/surface_indirect_functions.h'\n  '/usr/local/cuda-8.0/include/device_launch_parameters.h'\n  '/usr/local/cuda-8.0/include/cuda_fp16.h'\n  '/usr/local/cuda-8.0/include/math_constants.h'\n  '/usr/local/cuda-8.0/include/curand_kernel.h'\n  '/usr/local/cuda-8.0/include/curand.h'\n  '/usr/local/cuda-8.0/include/curand_discrete.h'\n  '/usr/local/cuda-8.0/include/curand_precalc.h'\n  '/usr/local/cuda-8.0/include/curand_mrg32k3a.h'\n  '/usr/local/cuda-8.0/include/curand_mtgp32_kernel.h'\n  '/usr/local/cuda-8.0/include/cuda.h'\n  '/usr/local/cuda-8.0/include/curand_mtgp32.h'\n  '/usr/local/cuda-8.0/include/curand_philox4x32_x.h'\n  '/usr/local/cuda-8.0/include/curand_globals.h'\n  '/usr/local/cuda-8.0/include/curand_uniform.h'\n  '/usr/local/cuda-8.0/include/curand_normal.h'\n  '/usr/local/cuda-8.0/include/curand_normal_static.h'\n  '/usr/local/cuda-8.0/include/curand_lognormal.h'\n  '/usr/local/cuda-8.0/include/curand_poisson.h'\n  '/usr/local/cuda-8.0/include/curand_discrete2.h'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nTarget //tensorflow/cc:tutorials_example_trainer failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 50.107s, Critical Path: 49.88s\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: Linux 64bit / GPU 1080\n\nInstalled version of CUDA and cuDNN:  Cuda 8 / cuDNN: 8.0\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n", "comments": ["You need to update the CROSSTOOLS file to see this CUDA includes:\ntensorflow/third_party/gpus/crosstool/CROSSTOOL\n\nAround line 65, add:\n  cxx_builtin_include_directory: \"/usr/local/cuda-8.0/include\"\n", "@Mazecreator  Thank you for your answer. But I still have pywrap error...which doesnt work for my pc.\n", "+1 \n\nhaving similar undeclared inclusion(s) error during build (Ubuntu 14.04, cudnn v5, cuda 7.5)\n", "I am not certain I know what pywrap error you are having, but I have Ubuntu 16.04 with CUDA 8.0 / cuDNN 5 running on my  system with no problems.\n\nI did manually install the latest NVIDIA 367.35.0 Video Driver (.run) and the (.run) file for CUDA 8.0 as well.  Make sure you do not ignore the gcc version error.  I did that and everything compiled properly, but python failed to execute a tensor graph.  I had to go back and create a symbolic link from gcc-4.9 to gcc for the CUDA build.\n\nA few things to try if you haven't already as I am guessing a bit since I don't have the pywrap details:\n- make sure you do a \"bazel clean\" ( you might need to use the --expunge_async option as well)\n- if you are running python from the \"tensorflow\" repository directory, it is known not to work, change to a different director (not sure why, but it is true)\n- Search for other CUDA 8.0 issues under tensorflow as there are a few and they may help\n\nIf that doesn't help, I would need more details for your current error.\n", "@sungjin712 If you want help with a pywrap error, you'll have to show us the error.\n", "solved my own problem by using the official docker image to compile a wheel against cudnn 5 and then copying it out to the host. i think adding a pointer to docker in the `building from source` section in doc will be super helpful.\n", "@Mazecreator \nI also followed all the instructions from (http://textminingonline.com/dive-into-tensorflow-part-iii-gtx-1080-ubuntu16-04-cuda8-0-cudnn5-0-tensorflow) which I assumed very similar to what you explained. There should be some compile errors and let me try again and update the error if it doesn't work.  Good to heard that there are some people actually did the installation.  Thanks for your help anyway!\n", "@girving  Yes, I will try the full procedure again and upload the error if it doesn't work.\n", "@falcondai Where can I get those sources? I guess I need to learn how to use docker well too.\n", "@sungjin712 look at the CI build folder here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build\n(i do not recommend trying the build without docker which defeats the whole purpose)\n\nThe main idea is that we run a standard CI (Continuous Integration) build in a docker image (clean, standard environment) and then copy out the pip wheel file:\n- install docker and nvidia-docker. have the docker daemon running\n- make sure you have checkout the intended tensorflow version in your cloned git repo, e.g. `git checkout v0.10.0rc0`.\n- go to the ci_build folder and copy this file from my gist: https://gist.github.com/falcondai/7e159cc3cfa1529996f567ca284b00a7#file-docker_build-sh (the original build script removes the container after each build)\n- make sure [the first line in Dockerifle.gpu](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/Dockerfile.gpu#L1) is the cuda + cudnn version you want: for cuda 8 and cudnn 5, it should be `FROM nvidia/cuda:8.0-cudnn5-devel`. (Note that this image uses ubuntu 14.04 as base, but i think the wheel file will likely work for ubuntu 16.04. if you really want to _build_ in ubuntu 16.04, you need to look into first building your own cuda image, which should be a matter of changing a line in the cuda image Dockerfile and then building, since all public nvidia/cuda images are built on 14.04: https://hub.docker.com/r/nvidia/cuda/)\n- `$ ./docker_build.sh gpu bash`. \n- now you should be inside a container at `/workspace`, execute `$ ./tensorflow/tools/ci_build/builds/pip.sh gpu` (this will take a while). **DO NOT close this session!**\n- open another session and look up the container id of your `tf_ci.gpu` by `docker ps`\n- copy the built wheel file from that container, e.g. `$ docker cp <container_id>:/workspace/pip_test/whl/tensorflow-0.10.0rc0-cp27-none-linux_x86_64.whl .`\n- pip install this wheel file `$ pip install -U <host-path>`\n", "I tried again and It seems like after compiling the file via bazel, \n\"bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\" command returns error like below. It seems like errors from my compiling issues.\n\nF tensorflow/cc/tutorials/example_trainer.cc:80] Check failed: ::tensorflow::Status::OK() == (root.ToGraphDef(&def)) (OK vs. Unimplemented: Explict cast of a non-empty tensor not implemented yet)\nF tensorflow/cc/tutorials/example_trainer.cc:80] Check failed: ::tensorflow::Status::OK() == (root.ToGraphDef(&def)) (OK vs. Unimplemented: Explict cast of a non-empty tensor not implemented yet)\nF tensorflow/cc/tutorials/example_trainer.cc:80] Check failed: ::tensorflow::Status::OK() == (root.ToGraphDef(&def)) (OK vs. Unimplemented: Explict cast of a non-empty tensor not implemented yet)\nAborted (core dumped)\n\n---\n\nINFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_atan.cu.cc:\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\n\nAlways thanks for your support\n", "@falcondai Thanks for alternative solutions. It takes little bit of time for me to understand following those instructions since I didn't use docker, but I will try that!\n", "@sungjin712 That \"Explicit cast\" issue was recently fixed; see https://github.com/tensorflow/tensorflow/issues/3752.  Are you at HEAD?\n", "@girving Yeah, I saw that comment before and just ignore and install tensorflow via pip, but it has pywrap error which dose not work for me. By the way, what do you mean by \"at HEAD\"???\n", "@sungjin712 By \"at HEAD\" I mean from the current source, as opposed to from a release such as 0.10.\n", "@girving I have tensorflow-0.10.0rc0-py2-none-any.whl (seems like 10.0rc) version complied from current source. So, do I need to change the version or sth? It seems like recent ver. since I cloned the tensorflow repository last week.\n", "@sungjin712 #3752 was fixed on Friday, so last week isn't recent enough. :)\n", "@girving Lol, thanks for your clarification. Let me re-download and try again!\n", "@girving Thank you so much, It finally works. Everything I did was right except I needed to download current version of tensorflow source.\n"]}, {"number": 3759, "title": "LSTMFusedCell ops only running on CPU", "body": "Operating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: 8.0.27 and v5\n394472 Jul 26 13:26 /usr/local/cuda/lib64/libcudart.so.8.0.27\n78065952 Jul 26 13:33 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n1. TF commit : c5f94b10bbb30e525fa3ca313e7ccb173040c90a\n2. bazel version :  0.3.0\n\nWe switched over a working Seq2Seq model to use the LSTMFusedCell (previously GRUCell).  The graph build is a lot faster, but the step times are slower.\n\nBy profiling the ops (using timeline to generate a chrome trace file) we discovered that most of the processing was occurring on the GPU, as specified, but the LSTMFusedCell ops were all running on the CPU.\n\nThere may be a known issue, as referenced by @wchan in #2002  - however that PR is closed.\n", "comments": ["@ebrevdo Is there a plan for GPU `LSTMFusedCell`?\n", "This should be fixed as of ~1 week ago (it always supported GPU, but due to a weird bug we were not compiling it for nvcc).  And today/tomorrow this cell is being renamed to LSTMBlockCell.  Please let us know if it still does not work with GPU for you.\n", "If this is fixed for you, please close the bug.\n", "(please use the tensorflow nightly to ensure you're using a fixed version of this Cell)\n", "The TF commit you're using is ~4 commits too early!  This was fixed in [this commit](https://github.com/tensorflow/tensorflow/commit/c5b3ea14c0fc93032a8d31afa4546d7add1ed25e).\n", "Yes, it is fixed with the new code!  I was looking for a mention of LSTMFused in the commits and didn't realize that commit addressed this issue.  Very nice to have a compact graph and faster step times. Thank you very much!\n", "FYI we're renaming them to LSTMBlockCell.\n"]}, {"number": 3758, "title": "Tflearn tutorial updates", "body": "Hi TF team,\n\nWould it be possible to fold these tutorial doc updates from master into the r0.10 release branch?\n\nThanks,\nSanders\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 3757, "title": "Fixes a potential thread leak in the gRPC runtime.", "body": "- Increases the number of pending CleanupGraph requests that a WorkerService will handle to avoid requests being dropped (or backpressure causing an unbounded number of cleanup closure threads being created at the master).\n- Avoids creating a thread every time the runtime wants to clean up a step.\n\nFixes #3470.\n", "comments": ["@tensorflow-jenkins: Test this please.\n", "Looks like all failures are in either tf.learn or TensorBoard, so shouldn't be affected by this change.\n", "Sounds good. Merging.\n"]}, {"number": 3756, "title": "Beam Search Wrapper", "body": "I've been working with the beam search implementation @nikitakit wrote to fix issue #654 , and found the outputs aren't exactly what they should be (unless I'm misunderstanding something). It seems the top output is correct but the rest of the beams are not reordered correctly in the computation. This is my fixed version with a more detailed test case to prove correctness, as well as an attention wrapper to be able to use beam search with attention without much effort. I think it would be great to nail down some official solution.\n", "comments": ["Can one of the admins verify this patch?\n", "`cand_symbols` and `cand_probs` are not updated? unwrap functions use these states as output but I can't see any updating code...\n", "@ebrevdo can you take a look at this?  Does this make sense for TensorFlow core, or does this maybe belong in a separate, more focused repo?\n", "@lukaszkaiser let's discuss this one on Monday.\n", "Ok, maybe around 3pm?\n\nLukasz\n\nOn Sat, Aug 13, 2016 at 7:13 PM, ebrevdo notifications@github.com wrote:\n\n> @lukaszkaiser https://github.com/lukaszkaiser let's discuss this one on\n> Monday.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3756#issuecomment-239651923,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AApzZRjQFWLrTyuPnO1UK70S-B6wEkVMks5qfnnDgaJpZM4Jim5A\n> .\n", "Unfortunately, without the original author of the beam search code having provided a clear license in their original gist/github, we cannot accept code except from that author.\n\nThat said, the attention wrapper looks like it's original (is it?  if not, do you have details about the original license?)  So if you want, you can submit that on its own with corresponding unit tests for review.  Please look at some other code and unit tests to ensure you conform to code and documentation guidelines.\n\nThanks!\n", "I want to chime in here and say that I'm happy to have my code included in contrib, just like I said in the original issue.\n\nI've been away on vacation until recently, which is why you've only been hearing from @anair13 so far.\n\n@ebrevdo I can resubmit my parts under my name if you'd like. (Though I wouldn't be comfortable doing so without first fixing the bugs and getting the tests into shape.) That said, I would find it extremely useful to hear some comments on the high-level approach (i.e. cell wrapping, as opposed to dynamic loops or custom ops) and API design before writing any more code.\n", "Thanks for looking at this everyone. As @nikitakit said, I think it would be good to discuss the approach here (personally, I think this way of implementing both attention and beam search as wrappers is simpler to understand and easier to modify) and make changes as we go.\n\nI can fix the issue mentioned by @therne as I had commented out parts I had not tested, and also contribute some new tests and a sequence to sequence model with attention and beam search.  \n", "Have you seen the `AttentionCellWrapper` we have in [contrib.rnn](https://github.com/tensorflow/tensorflow/blob/ad9e5b29aeee6638476cb47522bc880876eaab86/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L670)?\n\n`tf.nn.raw_rnn` is now safe to use; so any decoding algorithms should probably use it.\n", "@nikitakit if you could take a look at AttentionCellWrapper and `raw_rnn` and let us know whether you would like to add onto these or build on top of these, we can decide whether your work should go into core, contrib, or is better suited for its own, higly focused, \"model\" repo.\n", "@ebrevdo All attention code here is by @anair13. To be honest I'd prefer to keep the beam search and attention discussions separate, because their implementations should be orthogonal. (The beam search code can work with any attention implementation, including the built-in AttentionCellWrapper).\n\nI would love to build on top of `raw_rnn`, but unfortunately its API is still not expressive enough to support beam search. Beam search needs to have access to intermediate cell states (not just inputs/outputs), because items that fall off the beam need to have their cell state removed and replaced with that of another item. \n\nNow if `raw_rnn` could be modified to receive `cell_state` into the loop function and output a `new_cell_state` from it, then it's a different story.\n", "Thanks for the reply @ebrevdo. I hadn't seen the AttentionCellWrapper before, and thats exactly what the one included here does so I'll take it out.\n", "Nikita I'll consider adding control of state in raw_rnn.  The interface is\nstill flexible\n\nOn Aug 19, 2016 5:06 PM, \"Ashvin Nair\" notifications@github.com wrote:\n\n> Thanks for the reply @ebrevdo https://github.com/ebrevdo. I hadn't seen\n> the AttentionCellWrapper before, and thats exactly what the one included\n> here does so I'll take it out.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/3756#issuecomment-241163708,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim8hcwMPKhTJKsYrV4ljtCtQ9PXlwks5qhkTvgaJpZM4Jim5A\n> .\n", "@ebrevdo If you could add state control to raw_rnn, that would be much appreciated. Using that API should also provide performance improvements for beam search, because it will allow early termination using dynamic_rnn primitives.\n"]}, {"number": 3755, "title": "Branch 130028402", "body": "", "comments": []}, {"number": 3754, "title": "Unresolved RNN performance issue", "body": "My previous issue was closed without resolution: https://github.com/tensorflow/tensorflow/issues/3738#issuecomment-239050102\n\nThe tf RNN approach of running up to the maximum length seems to be fundamentally flawed compared to the pycnn approach. \n\nEven when only calling the LSTM cell on the relevant steps like below the performance issue persists:\n\n```\nflat_state = nest.flatten(state)\n    flat_zero_output = nest.flatten(zero_output)\n\n    def select_relevant_state(state, mask):\n        c = tf.boolean_mask(state[0], mask)\n        h = tf.boolean_mask(state[1], mask)\n        return (c, h)\n\n    # Function to Perform at Each Time Step\n    def time_step(time, output_ta, state):\n\n        mask = (time < sequence_length)\n        indices = tf.squeeze(tf.to_int32(tf.where(mask)))\n        invert_indices = tf.squeeze(tf.to_int32(tf.where(invert_mask)))\n        invert_indices = tf.to_int32(tf.where(invert_mask))\n        input_t = tuple(ta.read(time) for ta in input_ta)\n\n        # Restore Shape Information\n        for input_, shape in zip(input_t, inputs_got_shape):\n            input_.set_shape(shape[1:])\n\n        input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n\n        # Select Only Relevant at This Time Step\n        input_t = tf.boolean_mask(input_t, mask)\n        state = select_relevant_state(state, mask)\n        call_cell = lambda: cell(input_t, state)\n\n        # Call Cell\n        (output, new_state) = call_cell()\n\n        # Fill Unprocessed Steps\n        filler_output = tf.boolean_mask(zero_output, invert_mask)\n        filler_state = select_relevant_state(state, invert_mask)\n\n        output = tf.dynamic_stitch([indices, invert_indices], [output, filler_output])\n        new_state_c = tf.dynamic_stitch([indices, invert_indices], [new_state[0], filler_state[0]])\n        new_state_h = tf.dynamic_stitch([indices, invert_indices], [new_state[1], filler_state[1]])\n        new_state = tf.pack([new_state_c, new_state_h], axis=0)\n\n        # Pack State if Using State Tuples\n        output = nest.flatten(output)\n\n        output_ta = tuple(ta.write(time, out) for ta, out in zip(output_ta, output))\n\n        return (time + 1, output_ta, new_state)\n```\n\nBucketing is not a reasonable approach in this situation as a LSTM is applied to each token and then a higher level LSTM is applied across combined word and character embeddings for actual tagging. This means that the only available inputs at each training step are the tokens in the sentence and therefore bucketing is not possible.\n", "comments": ["Please do not file new issues just because I close old ones.  I am happy to reopen the old one, and am reading through it now; creating new issues just clutters up the tracker.\n"]}, {"number": 3753, "title": "Branch 130016968", "body": "", "comments": []}, {"number": 3752, "title": "Error: OK vs. Unimplemented: Explict cast of a non-empty tensor not implemented yet", "body": "```\nbazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.5 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.8.0 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.8.0 locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.367\npciBusID 0000:01:00.0\nTotal memory: 3.95GiB\nFree memory: 3.33GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:866] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\nF tensorflow/cc/tutorials/example_trainer.cc:80] Check failed: ::tensorflow::Status::OK() == (root.ToGraphDef(&def)) (OK vs. Unimplemented: Explict cast of a non-empty tensor not implemented yet)\nF tensorflow/cc/tutorials/example_trainer.cc:80] Check failed: ::tensorflow::Status::OK() == (root.ToGraphDef(&def)) (OK vs. Unimplemented: Explict cast of a non-empty tensor not implemented yet)\nAbort (core dumped)\n```\n### Environment info\n\nOperating System: Ubuntu 16.04 \nCuda Toolkit 8.0 and cudnn 5 \nDriver Version: 361.77 \n\nls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   560184 Mai 18 21:44 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Mai 18 21:47 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Mai 18 21:47 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rw-r--r-- 1 root root   394472 Mai 18 21:44 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Mai 18 21:44 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 78065952 Ago 11 06:26 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 78065952 Ago 11 06:26 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 78065952 Ago 11 06:26 /usr/local/cuda/lib64/libcudnn.so.5.0.5\n-rw-r--r-- 1 root root 68709594 Ago 11 06:26 /usr/local/cuda/lib64/libcudnn_static.a\n", "comments": ["@keveman Do you want to keep this bug open?  The C++ API is currently not fully supported, so it  may be fine to close.\n", "How I can fix this? I installed tensorflow two days ago in another machine and it worked fine.\n", "I am also having the same issue, Ubuntu 14.04 with 4x Titan X building from source with cuDNN 5.\n\nInstalled a few weeks back on a machine with the exact same config fine - looks like it is caused by a recent commit.\n", "Yeah, using `git checkout r0.10` It worked.\n", "Sending out a fix shortly. Sorry about the breakage.\n", "Just a heads up for anyone else who might be having this issue, this only affects the example trainer, and tensorflow still compiles and runs fine (at least for me). So if you're waiting for a fix - you should be good just skipping over the test example and proceeding with the installation.\n", "I was encountering this issue, grabbed the latest build and now have a different compilation failure....\n"]}, {"number": 3751, "title": "Grid3LSTMCell running out of memory ", "body": "Here are my specs:\n- NVIDIA GTX 1070\n- Tensorflow from source (bazel 0.3.0)\n- CUDA 8.0\n- Cudnn 5\n\nI am trying to implement a 3D Grid LSTM network. My network is actually a CNN-LSTM end-to-end system, but I know that the `Grid3LSTMCell` is what is causing the issue (I have tested without and it functions fine). \n\nTypically this would be an appropriate SO post, but I think I have exhausted my GPU options so maybe this is a bug in the `GridRNNCell` itself. I have included... \n\n```\n# Initializing the variables\nwith tf.name_scope(\"initialize-and-config\") as scope:\n    init = tf.initialize_all_variables()\n    saver = tf.train.Saver()\n    #gpu_options = tf.GPUOptions()\n    #config = tf.ConfigProto(gpu_options=gpu_options)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    #config.gpu_options.per_process_gpu_memory_fraction = 0.1\n\n# Launch the graph\nwith tf.Session(config=config) as sess:\n```\n\nI commented out `config.gpu_options.per_process_gpu_memory_fraction = 0.1` because I have tried including it. I have tried excluding `config.gpu_options.allow_growth` and including `config.gpu_options.per_process_gpu_memory_fraction = 0.1`. I have tried different fraction values. And I have tried including both GPU options\n\nI have also tried finalizing the graph before `sess.run()` by using `tf.get_default_graph().finalize()`\n\nI should note that the RNN initialization step alone takes roughly 10 minutes to intialize -- `outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)`.\n\nI am able to run the network on a cropped image of size 20x20, but when I train the network on the full 396x396 image I get the following...\n\n```\n totalling 86.14MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 301086720 totalling 287.14MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 6.57GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: \nLimit:                  7531433165\nInUse:                  7054675456\nMaxInUse:               7054689536\nNumAllocs:                  369387\nMaxAllocSize:            301086720\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ****************************************************************************************************\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 768.0KiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[384,512]\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=140010 evicted_count=140000 eviction_rate=0.999929 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=150010 evicted_count=150000 eviction_rate=0.999933 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=160010 evicted_count=160000 eviction_rate=0.999938 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=170010 evicted_count=170000 eviction_rate=0.999941 and unsatisfied allocation rate=0\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=180010 evicted_count=180000 eviction_rate=0.999944 and unsatisfied allocation rate=0\nTraceback (most recent call last):\n  File \"3D-CNN-LSTM-reg.py\", line 225, in <module>\n    keep_prob: dropout})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[396,128]\n     [[Node: opt/gradients/net/RNN/Grid3LSTMCell_537/MatMul_1_grad/MatMul_1 = MatMul[T=DT_FLOAT, transpose_a=true, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](net/RNN/Grid3LSTMCell_537/split, opt/gradients/net/RNN/Grid3LSTMCell_537/concat_4_grad/tuple/control_dependency)]]\nCaused by op u'opt/gradients/net/RNN/Grid3LSTMCell_537/MatMul_1_grad/MatMul_1', defined at:\n  File \"3D-CNN-LSTM-reg.py\", line 191, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 476, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py\", line 637, in _MatMulGrad\n    math_ops.matmul(op.inputs[0], grad, transpose_a=True))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1352, in matmul\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 1296, in _mat_mul\n    transpose_b=transpose_b, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'net/RNN/Grid3LSTMCell_537/MatMul_1', defined at:\n  File \"3D-CNN-LSTM-reg.py\", line 176, in <module>\n    pred = conv_net(x, weights, biases, keep_prob)\n  File \"3D-CNN-LSTM-reg.py\", line 135, in conv_net\n    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 219, in rnn\n    (output, state) = call_cell()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py\", line 206, in <lambda>\n    call_cell = lambda: cell(input_, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/grid_rnn/python/ops/grid_rnn_cell.py\", line 150, in __call__\n    c_prev[j] = math_ops.matmul(input_splits[i], input_project_c)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py\", line 1352, in matmul\n    name=name)\n```\n\nThe full code is as follows...\n\n```\n\n#Kendall Weihe\n#This is a CNN that handles 3D data\n#Adjust network parameters below, also adjust data directory\n\nimport tensorflow as tf\nimport pdb\nimport numpy as np\nfrom numpy import genfromtxt\nfrom PIL import Image\nfrom tensorflow.python.ops import rnn, rnn_cell\nfrom tensorflow.contrib.grid_rnn.python.ops import grid_rnn_cell\nfrom tensorflow.tensorflow.scroll import scroll_data\n\n# Parameters\nlearning_rate = 0.001\ntraining_iters = 1000000\nbatch_size = 3\ndisplay_step = 1\n\n# Network Parameters\nn_input_x = 396 # Input image x-dimension\nn_input_y = 396 # Input image y-dimension\nn_input_z = 5\nn_hidden = 128\nn_classes = 2 # Binary classification -- on a surface or not\nn_output = n_input_x * n_classes\n\ndropout = 0.75 # Dropout, probability to keep units\n\n# tf Graph input\nx = tf.placeholder(tf.float32, [batch_size, n_input_z, n_input_x, n_input_y])\ntemp_x = tf.placeholder(tf.float32, [1, n_input_z, n_input_x, n_input_y])\ny = tf.placeholder(tf.float32, [batch_size, n_input_z, n_input_x, n_input_y, n_classes], name=\"ground_truth\")\nkeep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n\n# This function converts the ground truth data into a\n    #2 channel classification -- n_input_x x n_input_y x 2\n    # one layer for 0's and the other for 1's\ndef convert_to_2_channel(x):\n    #assume input has dimension (batch_size,x,y)\n    #output will have dimension (batch_size,x,y,2)\n    output = np.empty((batch_size, n_input_z, n_input_x, n_input_y, n_classes))\n    for i in range(batch_size):\n        for j in range(n_input_z):\n            for k in range(n_input_x):\n                for l in range(n_input_y):\n                    for m in range(n_classes):\n                        if m == 0:\n                            output[i][j][k][l][m] = x[i][j][k][l]\n                        else:\n                            output[i][j][k][l][m] = 1 - x[i][j][k][l]\n\n    return output\n\n\n# Create some wrappers for simplicity\ndef conv3d(x, W, b, strides=1):\n    # Conv2D wrapper, with bias and relu activation\n    x = tf.nn.conv3d(x, W, strides=[1, strides, strides, strides, 1], padding='SAME')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\ndef maxpool3d(x, k=2):\n    # MaxPool2D wrapper\n    return tf.nn.max_pool3d(x, ksize=[1, k, k, k, 1], strides=[1, k, k, k, 1],\n                          padding='SAME')\n\ndef deconv3d(prev_layer, w, b, output_shape, strides):\n    # Deconv layer\n    deconv = tf.nn.conv3d_transpose(prev_layer, w, output_shape=output_shape, strides=strides, padding=\"VALID\")\n    deconv = tf.nn.bias_add(deconv, b)\n    deconv = tf.nn.relu(deconv)\n    return deconv\n\n# Create model\ndef conv_net(x, weights, biases, dropout):\n    # Reshape input picture\n    x = tf.reshape(x, shape=[batch_size, n_input_z, n_input_x, n_input_y, 1])\n\n    with tf.name_scope(\"conv1\") as scope:\n    # Convolution Layer\n        conv1 = conv3d(x, weights['wc1'], biases['bc1'])\n        # Max Pooling (down-sampling)\n        #conv1 = tf.nn.local_response_normalization(conv1)\n        conv1 = maxpool3d(conv1, k=2)\n\n    # Convolution Layer\n    with tf.name_scope(\"conv2\") as scope:\n        conv2 = conv3d(conv1, weights['wc2'], biases['bc2'])\n        # Max Pooling (down-sampling)\n        # conv2 = tf.nn.local_response_normalization(conv2)\n        conv2 = maxpool3d(conv2, k=2)\n\n    # Convolution Layer\n    with tf.name_scope(\"conv3\") as scope:\n        conv3 = conv3d(conv2, weights['wc3'], biases['bc3'])\n        # Max Pooling (down-sampling)\n        # conv3 = tf.nn.local_response_normalization(conv3)\n        conv3 = maxpool3d(conv3, k=2)\n\n    # pdb.set_trace()\n\n    temp_batch_size = tf.shape(x)[0] #batch_size shape\n    with tf.name_scope(\"deconv1\") as scope:\n        output_shape = [temp_batch_size, 2, n_input_x / 4, n_input_y / 4, 64]\n        strides = [1,2,2,2,1]\n        #conv4 = deconv3d(conv3, weights['wdc1'], biases['bdc1'], output_shape, strides)\n        # conv4 = tf.nn.local_response_normalization(conv4)\n        conv4 = tf.nn.conv3d_transpose(conv3, weights['wdc1'], output_shape=output_shape, strides=strides, padding=\"SAME\")\n        conv4 = tf.nn.bias_add(conv4, biases['bdc1'])\n        conv4 = tf.nn.relu(conv4)\n\n    with tf.name_scope(\"deconv2\") as scope:\n        output_shape = [temp_batch_size, 3, n_input_x / 2, n_input_y / 2, 32]\n        strides = [1,1,2,2,1]\n        conv5 = deconv3d(conv4, weights['wdc2'], biases['bdc2'], output_shape, strides)\n        # conv5 = tf.nn.local_response_normalization(conv5)\n\n    with tf.name_scope(\"deconv3\") as scope:\n        output_shape = [temp_batch_size, 5, n_input_x, n_input_y, 1]\n        #this time don't use ReLu -- since output layer\n        conv6 = tf.nn.conv3d_transpose(conv5, weights['wdc3'], output_shape=output_shape, strides=[1,1,2,2,1], padding=\"VALID\")\n        x = tf.nn.bias_add(conv6, biases['bdc3'])\n        x = tf.reshape(x, [batch_size, n_input_z, n_input_x, n_input_y])\n        # conv6 = tf.nn.relu(conv6)\n\n    # pdb.set_trace()\n\n    x = tf.reshape(conv6, [batch_size * n_input_y * n_input_z, n_input_x])\n    x = tf.split(0, n_input_y * n_input_z, x)\n\n    lstm_cell = grid_rnn_cell.Grid3LSTMCell(n_hidden)\n\n    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n\n    output = []\n    for i in xrange(n_input_y * n_input_z):\n        output.append(tf.matmul(outputs[i], lstm_weights[i]) + lstm_biases[i])\n\n    return output\n\nweights = {\n    # 5x5 conv, 1 input, 32 outputs\n    'wc1' : tf.Variable(tf.random_normal([5, 5, 5, 1, 32])),\n    # 5x5 conv, 32 inputs, 64 outputs\n    'wc2' : tf.Variable(tf.random_normal([3, 5, 5, 32, 64])),\n    # 5x5 conv, 32 inputs, 64 outputs\n    'wc3' : tf.Variable(tf.random_normal([2, 5, 5, 64, 128])),\n\n    'wdc1' : tf.Variable(tf.random_normal([2, 2, 2, 64, 128])),\n\n    'wdc2' : tf.Variable(tf.random_normal([2, 2, 2, 32, 64])),\n\n    'wdc3' : tf.Variable(tf.random_normal([3, 2, 2, 1, 32])),\n}\n\nbiases = {\n    'bc1': tf.Variable(tf.random_normal([32])),\n    'bc2': tf.Variable(tf.random_normal([64])),\n    'bc3': tf.Variable(tf.random_normal([128])),\n    'bdc1': tf.Variable(tf.random_normal([64])),\n    'bdc2': tf.Variable(tf.random_normal([32])),\n    'bdc3': tf.Variable(tf.random_normal([n_input_z])),\n}\n\nlstm_weights = {}\nlstm_biases = {}\n\nfor i in xrange(n_input_y * n_input_z):\n    lstm_weights[i] = tf.Variable(tf.random_normal([n_hidden, n_output]))\n    lstm_biases[i] = tf.Variable(tf.random_normal([n_output]))\n\n# Construct model\nwith tf.name_scope(\"net\") as scope:\n    pred = conv_net(x, weights, biases, keep_prob)\n    # pdb.set_trace()\n    pred = tf.transpose(tf.pack(pred),[1,0,2])\n    pred = tf.reshape(pred, [-1, n_input_z, n_input_x, n_input_y, n_classes])\n\n    # Define loss and optimizer\n    # Reshape for cost function\n    temp_pred = tf.reshape(pred, [-1, 2])\n    temp_y = tf.reshape(y, [-1, 2])\n\nwith tf.name_scope(\"loss\") as scope:\n    # cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(pred, y))\n    cost = (tf.nn.sigmoid_cross_entropy_with_logits(temp_pred, temp_y))\n\nwith tf.name_scope(\"opt\") as scope:\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Evaluate model\nwith tf.name_scope(\"acc\") as scope:\n    # accuracy is the difference between prediction and ground truth matrices\n    correct_pred = tf.equal(0,tf.cast(tf.sub(tf.nn.sigmoid(temp_pred),temp_y), tf.int32))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\nwith tf.name_scope(\"initialize-and-config\") as scope:\n    init = tf.initialize_all_variables()\n    saver = tf.train.Saver()\n    gpu_options = tf.GPUOptions()\n    config = tf.ConfigProto(gpu_options=gpu_options)\n    config.gpu_options.allow_growth = True\n    #config.gpu_options.per_process_gpu_memory_fraction = 0.1\n\n# Launch the graph\nwith tf.Session(config=config) as sess:\n    sess.run(init)\n    summary = tf.train.SummaryWriter('/tmp/logdir/', sess.graph) #initialize graph for tensorboard\n    step = 1\n1\n    # Import data\n    data = scroll_data.read_data('/home/volcart/Documents/Data/', 100, n_input_x, n_input_y)\n    # Keep training until reach max iterations\n    while step * batch_size < training_iters:\n        batch_x, batch_y = data.train.next_batch(batch_size * n_input_z)\n        # Run optimization op (backprop)\n        batch_x = batch_x.reshape((batch_size, n_input_z, n_input_x, n_input_y))\n        batch_y = batch_y.reshape((batch_size, n_input_z, n_input_x, n_input_y))\n        batch_y = convert_to_2_channel(batch_y) # Converts the 3960x3960 ground truth to a 3960x3960x2 classification\n        batch_y = batch_y.reshape(batch_size, n_input_z, n_input_x, n_input_y, n_classes)\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n                                       keep_prob: dropout})\n\n        step = step + 1\n        if step % display_step == 0:\n            batch_y = batch_y.reshape(batch_size, n_input_z, n_input_x, n_input_y, n_classes)\n            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n                                                              y: batch_y,\n                                                              keep_prob: 1.0})\n            print \"Step = \" + str(step) + \" Accuracy = \" + str(acc)\n            #print \"Loss = \" + str(loss)\n            # Save network\n            if step % 50 == 0:\n                save_path = \"/home/volcart/Documents/3D-CNN-LSTM-reg-model/3D-CNN-LSTM-seg-step-\" + str(step) + \"-model.ckpt\"\n                saver.save(sess, save_path)\n```\n", "comments": ["Can you elaborate on why you think this isn't caused by trying to use more memory than the GPU has?\n", "@girving I suppose that is an issue. I assumed Tensorflow had a way of handling overflow memory. Am I correct that it doesn't? Are there any workarounds?\n", "Unfortunately, TensorFlow is limited to using the amount of memory that the computer possesses.  There any many workarounds, but exploring them would be better in a StackOverflow question.\n"]}, {"number": 3750, "title": "TensorBoard doesn't show Events on TensorFlow v0.10 RC", "body": "After a clean install of TensorFlow v0.10 (from `master`) my TensorBoard is suddenly broken. The scalar event plots do not show upon clicking (see screenshow below). While the logs in the terminal do not show any errors, the Chrome Developer console shows the following error upon opening a figure:\n\n```\ntf-tensorboard.html:1517 Uncaught Error: tf-chart-scaffold's content doesn't implement the required interfaceinsertBefore @ VM2478 polymer-mini.html:560\n```\n\nI am running Chrome Version 52.0.2743.116 (64-bit) on Linux Mint 17.\n\n![TensorBoard](http://i.imgur.com/GnR46NL.png)\n", "comments": ["@danmane Can you take a look?  Looks like a dependency version problem.\n", "Got the same error. The server is on a ubuntu, and the client is a mac.\n", "I found a temporary fix. Replace the file `[...]/python2.7/site-packages/tensorflow/tensorboard/dist/tf-tensorboard.html` with the older version from TensorFlow v0.9. This version can be found here: https://raw.githubusercontent.com/tensorflow/tensorflow/r0.9/tensorflow/tensorboard/dist/tf-tensorboard.html. This way the plots will work again. Hopefully there will be a proper fix soon :-)\n", "How are you running TensorBoard? I can't reproduce it here.\n", "I can reproduce on the master branch, on r0.10 it is working properly.\n", "We've fixed this internally, and I've verified that a new opensource repo built from the fixed version has a working TensorBoard.\n\nOnce we sync the changes to GitHub, I'll merge the fix into the r0.10 branch.\n", "I can confirm that this bug is now fixed (https://github.com/tensorflow/tensorflow/commit/ccd431b20a82ba2ecb96a82fb6e2712f2b522a11). Just did a `git pull` of the master and rebuild, TensorBoard events are now shown properly. \n", "Great. As r0.10 was not broken, only master, I'll close this as it's been fixed.\n", "(Also, I merged TB25 into r0.10 anyway :) )\n"]}, {"number": 3749, "title": "error: can't copy 'tensorflow/python/ops/gen_sparse_ops.py': doesn't exist or not a regular file", "body": "After I compiled the source code of TF0.10 (the latest version) using command \"bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\", I tried to generate the .whl package using the following command:\n\"bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\"\nHowever, errors occured:\nThu Aug 11 20:10:14 EDT 2016 : === Using tmpdir: /tmp/tmp.r1OX0vuucW\n/tmp/tmp.r1OX0vuucW ~/bxl/tf1.0\nThu Aug 11 20:10:14 EDT 2016 : === Building wheel\nerror: can't copy 'tensorflow/python/ops/gen_sparse_ops.py': doesn't exist or not a regular file\n\nAnyone knows how to solve this? Thx\n", "comments": ["I had the same error, but for me there was already an error during the build process. Check if you have the same problem (missing `/usr/local/cuda/` dependencies) and then fix the problem by adding a line to the file `third_party/gpus/crosstool/CROSSTOOL`, see here: https://github.com/tensorflow/tensorflow/issues/3589#issuecomment-236430359\n\nThis fix works for me using both CUDA Toolkit version 7.5 and 8.0 RC.\n", "No, I don't have the cuda error. \nIs there any idea to solve the problem in generating whl package?\n", "Can you show us the full output of all your build commands?\n", "I found that the build is not successful and has the following error: @girving \n\nERROR: /home/dl/bxl/tf-0815/tensorflow/contrib/rnn/BUILD:46:1: undeclared inclusion(s) in rule '//tensorflow/contrib/rnn:python/ops/_lstm_ops_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/rnn/kernels/lstm_ops_gpu.cu.cc':\n  '/usr/local/cuda-7.5/include/cuda_runtime.h'\n  '/usr/local/cuda-7.5/include/host_config.h'\n  '/usr/local/cuda-7.5/include/builtin_types.h'\n  '/usr/local/cuda-7.5/include/device_types.h'\n  '/usr/local/cuda-7.5/include/host_defines.h'\n  '/usr/local/cuda-7.5/include/driver_types.h'\n  '/usr/local/cuda-7.5/include/surface_types.h'\n  '/usr/local/cuda-7.5/include/texture_types.h'\n  '/usr/local/cuda-7.5/include/vector_types.h'\n  '/usr/local/cuda-7.5/include/channel_descriptor.h'\n  '/usr/local/cuda-7.5/include/cuda_runtime_api.h'\n  '/usr/local/cuda-7.5/include/cuda_device_runtime_api.h'\n  '/usr/local/cuda-7.5/include/driver_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.h'\n  '/usr/local/cuda-7.5/include/vector_functions.hpp'\n  '/usr/local/cuda-7.5/include/common_functions.h'\n  '/usr/local/cuda-7.5/include/math_functions.h'\n  '/usr/local/cuda-7.5/include/math_functions.hpp'\n  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.h'\n  '/usr/local/cuda-7.5/include/math_functions_dbl_ptx3.hpp'\n  '/usr/local/cuda-7.5/include/cuda_surface_types.h'\n  '/usr/local/cuda-7.5/include/cuda_texture_types.h'\n  '/usr/local/cuda-7.5/include/device_functions.h'\n  '/usr/local/cuda-7.5/include/device_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/device_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_double_functions.h'\n  '/usr/local/cuda-7.5/include/device_double_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_20_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_20_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_32_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_32_atomic_functions.hpp'\n  '/usr/local/cuda-7.5/include/sm_35_atomic_functions.h'\n  '/usr/local/cuda-7.5/include/sm_20_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_20_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_30_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_30_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_32_intrinsics.h'\n  '/usr/local/cuda-7.5/include/sm_32_intrinsics.hpp'\n  '/usr/local/cuda-7.5/include/sm_35_intrinsics.h'\n  '/usr/local/cuda-7.5/include/surface_functions.h'\n  '/usr/local/cuda-7.5/include/surface_functions.hpp'\n  '/usr/local/cuda-7.5/include/texture_fetch_functions.h'\n  '/usr/local/cuda-7.5/include/texture_fetch_functions.hpp'\n  '/usr/local/cuda-7.5/include/texture_indirect_functions.h'\n  '/usr/local/cuda-7.5/include/texture_indirect_functions.hpp'\n  '/usr/local/cuda-7.5/include/surface_indirect_functions.h'\n  '/usr/local/cuda-7.5/include/surface_indirect_functions.hpp'\n  '/usr/local/cuda-7.5/include/device_launch_parameters.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/Core'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/DisableStupidWarnings.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/Macros.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/MKL_support.h'\n  '/usr/local/cuda-7.5/include/cuda_fp16.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/Constants.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/Meta.h'\n  '/usr/local/cuda-7.5/include/math_constants.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/ForwardDeclarations.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/StaticAssert.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/XprHelper.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/Memory.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/NumTraits.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/MathFunctions.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/GenericPacketMath.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/arch/CUDA/Half.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/arch/CUDA/PacketMathHalf.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/arch/CUDA/TypeCasting.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/arch/CUDA/PacketMath.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/arch/CUDA/MathFunctions.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/arch/Default/Settings.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/functors/TernaryFunctors.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/functors/BinaryFunctors.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/functors/UnaryFunctors.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/functors/NullaryFunctors.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/functors/StlFunctors.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/functors/AssignmentFunctors.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/DenseCoeffsBase.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/DenseBase.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/plugins/BlockMethods.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/MatrixBase.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/plugins/CommonCwiseUnaryOps.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/plugins/CommonCwiseBinaryOps.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/plugins/MatrixCwiseUnaryOps.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/plugins/MatrixCwiseBinaryOps.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/EigenBase.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Product.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/CoreEvaluators.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/AssignEvaluator.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Assign.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/ArrayBase.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/plugins/ArrayCwiseUnaryOps.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/plugins/ArrayCwiseBinaryOps.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/BlasUtil.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/DenseStorage.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/NestByValue.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/ReturnByValue.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/NoAlias.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/PlainObjectBase.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Matrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Array.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/CwiseTernaryOp.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/CwiseBinaryOp.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/CwiseUnaryOp.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/CwiseNullaryOp.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/CwiseUnaryView.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/SelfCwiseBinaryOp.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Dot.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/StableNorm.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Stride.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/MapBase.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Map.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Ref.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Block.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/VectorBlock.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Transpose.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/DiagonalMatrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Diagonal.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/DiagonalProduct.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Redux.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Visitor.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Fuzzy.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/IO.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Swap.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/CommaInitializer.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/GeneralProduct.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Solve.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Inverse.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/SolverBase.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/PermutationMatrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Transpositions.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/TriangularMatrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/SelfAdjointView.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/GeneralBlockPanelKernel.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/Parallelizer.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/ProductEvaluators.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/GeneralMatrixVector.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/GeneralMatrixMatrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/SolveTriangular.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/GeneralMatrixMatrixTriangular.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/SelfadjointMatrixVector.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/SelfadjointMatrixMatrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/SelfadjointProduct.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/SelfadjointRank2Update.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/TriangularMatrixVector.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/TriangularMatrixMatrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/TriangularSolverMatrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/products/TriangularSolverVector.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/BandMatrix.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/CoreIterators.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/ConditionEstimator.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/BooleanRedux.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Select.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/VectorwiseOp.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Random.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Replicate.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/Reverse.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/ArrayWrapper.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/GlobalFunctions.h'\n  '/home/dl/bxl/fix-0815/eigen-eigen-6f952374ef2b/Eigen/src/Core/util/ReenableStupidWarnings.h'\n  '/usr/local/cuda-7.5/include/curand_kernel.h'\n  '/usr/local/cuda-7.5/include/curand.h'\n  '/usr/local/cuda-7.5/include/curand_discrete.h'\n  '/usr/local/cuda-7.5/include/curand_precalc.h'\n  '/usr/local/cuda-7.5/include/curand_mrg32k3a.h'\n  '/usr/local/cuda-7.5/include/curand_mtgp32_kernel.h'\n  '/usr/local/cuda-7.5/include/cuda.h'\n  '/usr/local/cuda-7.5/include/curand_mtgp32.h'\n  '/usr/local/cuda-7.5/include/curand_philox4x32_x.h'\n  '/usr/local/cuda-7.5/include/curand_globals.h'\n  '/usr/local/cuda-7.5/include/curand_uniform.h'\n  '/usr/local/cuda-7.5/include/curand_normal.h'\n  '/usr/local/cuda-7.5/include/curand_normal_static.h'\n  '/usr/local/cuda-7.5/include/curand_lognormal.h'\n  '/usr/local/cuda-7.5/include/curand_poisson.h'\n  '/usr/local/cuda-7.5/include/curand_discrete2.h'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nnvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 103.576s, Critical Path: 94.31s\n", "@martinwicke Another undeclared inclusion; do you know if this is fixed?  @gclouding Can you try with the latest source to see if it's different?\n", "The problem was finally solved by adding the library path of cuda and eigen in the cxx_builtin_include_directory in the file  ./third_party/gpus/crosstool/CROSSTOOL @girving \n", "This issue looks like it is now solved?  Please reopen if the problem is still present. \n"]}, {"number": 3748, "title": "unexpected names in `NamedOutputs` tuples when using `outputs_collections` with some layers", "body": "We are getting unexpected names (they lack outer name scope) in `NamedOutputs` tuples added to collections using `tf.contrib.layers` `outputs_collections`. The following code demonstrates the issue:\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\n\n\nwith tf.name_scope(\"train\"):\n    with slim.arg_scope([slim.fully_connected, slim.flatten],\n                        outputs_collections=tf.GraphKeys.ACTIVATIONS):\n        ph = tf.placeholder(tf.float32, [2, 2])\n        fc = slim.fully_connected(ph, 10)\n        flat = slim.flatten(ph)\n\n{print(\"name in tuple: \", no.name, \", tensor name:\", no.outputs.name)\n for no in tf.get_collection(tf.GraphKeys.ACTIVATIONS)}\n```\n\nThe output is:\n\n```\nname in tuple:  fully_connected , tensor name: train/fully_connected/Relu:0\nname in tuple:  train/Flatten , tensor name: train/Flatten/Reshape:0\n```\n\nWe have tracked the cause of this down in to `tf.contrib.layers`. For layers that use internal variables (fully_connected, conv2d, ...), final outputs are added to collections based on internal variable_scope name. Please see:\n\n tensorflow/tensorflow/contrib/layers/python/layers/layers.py lines 758, 835 (version 0.10, commit 1df3fb0b4ae5915364f09e233496e98a99a4a886)\n\nIt is our understanding that activation names generally fall under name_scopes, which is consistent with actual op names in the output above.\n\nThis issue makes it impossible to retrieve items from collections filtered down with a name scope, an approach that we are trying to use for decoupling op creation and summarizing. It seems a valid use-case.\n", "comments": ["Duplicate of #3721.\n", "Never mind. Different issue.\n", "@lukaszkaiser could you look at this? Is this a scope issue or something wrong with how we use the scopes in layers?\n", "I'm not sure if It is the expected behaviour, but it might be. Are you printing variable names or just tensor names? Variable names are by design not affected by name_scope, only variable_scope. But is it this? Or is something else wrong? Could you clarify what you're printing and what you'd expect and why? Thanks!\n", "@lukaszkaiser, I am looking at tensor names, not variable names.\n\nRefering to the example above. Two tensors are created with the `train` name_scope and added to the ACTIVATIONS collection. When looking at the contents of that collection it can be seen that the `Flatten` tensor's `NamedOutput.name` becomes `train/Flatten`. This obeys name scoping and matches the name attribute of the tensor itself (`NamedOutput.outputs.name`). However, the `fully_connected` tensor's `NamedOutput.name` is just `fully_connected`, instead of `train/fully_connected`. That does not obey name scoping, and differs from the actual tensor's name attribute which does obey name scoping.\n\nI would expect that the `NamedOutputs.name` attribute for a fully_connected tensor respects name_scope and thus reflects the actual tensor's name attribute.\n\nTracking that down in TF source reference in the original description, it can be seen that tensors which use internal variables (such as fully_connect and conv2d) are using variable scopes when creating `NamedOutputs`, which seems a bug.\n\nI have modified the original example where I inappropriately named some variables, it should be less confusing now. Sorry about that.\n", "Great thanks for the explanation! It looks like a bug, probably not in scoping per se but, as you said, in assigning NamedOutputs.name based on variable_scope and not name_scope. Since you tracked it down, could you say where in TF code this appears? Would you like to contribute a fix? I assigned to Sergio who might know more, and in particular if there were any reasons to do it like this in layers.\n\nGreat thanks for pointing this out!\n", "I think it was fixed by #3635\n", "@sguada: it seems fixed to me too, by looking at the source code of the fix and the tests.\n"]}, {"number": 3747, "title": "Segmentation fault when importing TensorFlow", "body": "After installing TensorFlow 0.10.0rc0 with pip in a virtualenv\nimport tensorflow as tf\nresults in \nSegmentation fault (core dumped)\n\nImporting numpy before tensorflow results in the same segfault\n\nOS: RedHat 6, 64-bit\n(Installed glibc 2.1.4 from source)\nPython Version 3.4.5\n\npip packages (cpu versions):\ntensorflow-0.10.0rc0-cp34-cp34m-linux_x86_64.whl\nprotobuf-3.0.0b2.post2-cp34-none-linux_x86_64.whl\nsix-1.10.0-py2.py3-none-any.whl\nnumpy-1.11.1-cp34-cp34m-manylinux1_x86_64.whl\n(same problem with numpy-1.8.2-cp34-cp34m-manylinux1_x86_64.whl)\n\nDebugger Output:\ngdb python\nrun tf.py\n(tf.py contains: import tensorflow as tf)\n\n[Thread debugging using libthread_db enabled]\nMissing separate debuginfo for /home/rolf/tfenv3/lib/python3.4/site-packages/numpy/core/../.libs/libgfortran-ed201abd.so.3.0.0\n[New Thread 0x7ffff352a700 (LWP 2920)]\n[New Thread 0x7ffff2b29700 (LWP 2921)]\n[New Thread 0x7ffff0128700 (LWP 2922)]\n[New Thread 0x7fffed727700 (LWP 2923)]\n[New Thread 0x7fffead26700 (LWP 2924)]\n[New Thread 0x7fffe8325700 (LWP 2925)]\n[New Thread 0x7fffe5924700 (LWP 2926)]\n\nProgram received signal SIGSEGV, Segmentation fault.\n0x00007ffff793878b in init_one_static_tls (map=0x0) at allocatestack.c:1171\n1171      void *dest = (char *) curp - map->l_tls_offset;\nMissing separate debuginfos, use: debuginfo-install bzip2-libs-1.0.5-7.el6_0.x86_64 glibc-2.12-1.192.el6.x86_64 keyutils-libs-1.4-5.el6.x86_64 krb5-libs-1.10.3-57.el6.x86_64 libcom_err-1.41.12-22.el6.x86_64 libselinux-2.0.94-7.el6.x86_64 openssl-1.0.1e-48.el6_8.1.x86_64 zlib-1.2.3-29.el6.x86_64\n", "comments": ["I think this kind of error is caused by dueling versions of glibc.  Something linked with TensorFlow is using the wrong version, and two separate versions of glibc in the same process is bad.\n", "This seems to have been the case.\nI have replaced six and numpy with the source versions and reinstalled tensorflow with pip.\nThis removed the segfault but left me with the problem that libstdc++ was too old.\nFinally, the following way of setting up a local glibc/libstdc++ environment solved my problem:\nhttp://stackoverflow.com/questions/33655731/error-while-importing-tensorflow-in-python2-7-in-ubuntu-12-04-glibc-2-17-not-f/34897674#34897674\n", "@bardeli Glad you found away around it!\n", "I meet this question,too. then how to get the correct version of glibc?"]}, {"number": 3746, "title": "underperformed test of inception-v3 retraining result", "body": "Hello, I'am do the retraining of inceptionv3 following tensorflow official [tutorial](https://www.tensorflow.org/versions/master/how_tos/image_retraining/index.html). Everything goes fine and it gives a `output_graph.pb` finally, but when I test the `output_graph.pb` with `label_image` module, it gives me quite odd result like below. Even when I test it with the flower images provided by the tutorial, it performs oddly as well.\n\nDid anyone meet with the same problem? or what did I miss? thanks a lot for any information.\n### Environment info\n\nubuntu 1404LST\ntensorflow 0.9.0-GPU\nCUDA: 7.5\ncuDNN: 5.0\nGPU: tesla K40\n### Logs or other output that would be helpful\n\n```\nUbuntu:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image  --graph=tensorflow/examples/label_image/data/v3_retrained.pb   --output_layer=final_result  \nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nI tensorflow/examples/label_image/main.cc:207] dummy (0): 0.387557\nI tensorflow/examples/label_image/main.cc:207] Siberian husky (3): 0.255237\nI tensorflow/examples/label_image/main.cc:207] kit fox (1): 0.254475\nI tensorflow/examples/label_image/main.cc:207] Australian terrier (4): 0.059399\nI tensorflow/examples/label_image/main.cc:207] English setter (2): 0.0433322\n\nUbuntu:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image  --graph=tensorflow/examples/label_image/data/v3_retrained.pb   --output_layer=final_result  --image=rose.jpg\nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nI tensorflow/examples/label_image/main.cc:207] kit fox (1): 0.97847\nI tensorflow/examples/label_image/main.cc:207] dummy (0): 0.0214963\nI tensorflow/examples/label_image/main.cc:207] English setter (2): 1.68903e-05\nI tensorflow/examples/label_image/main.cc:207] Siberian husky (3): 1.2378e-05\nI tensorflow/examples/label_image/main.cc:207] Australian terrier (4): 4.45126e-06\n\nUbuntu:~/tensorflow$ bazel-bin/tensorflow/examples/label_image/label_image  --graph=tensorflow/examples/label_image/data/v3_retrained.pb   --output_layer=final_result  --image=daisy.jpg\nW tensorflow/core/framework/op_def_util.cc:332] Op BatchNormWithGlobalNormalization is deprecated. It will cease to work in GraphDef version 9. Use tf.nn.batch_normalization().\nI tensorflow/examples/label_image/main.cc:207] English setter (2): 0.903881\nI tensorflow/examples/label_image/main.cc:207] Siberian husky (3): 0.0819594\nI tensorflow/examples/label_image/main.cc:207] dummy (0): 0.00714857\nI tensorflow/examples/label_image/main.cc:207] Australian terrier (4): 0.00495803\nI tensorflow/examples/label_image/main.cc:207] kit fox (1): 0.00205291\n```\n\nFYI: the `v3_retrained.pb` is the `output_graph.pb` produced by retraining. and `rose, daisy` are two images I picked from flower_photos provided by the tutorial.\n", "comments": ["This is a better fit for StackOverflow.  Github issues are for bugs and feature requests for the TensorFlow codebase, not requests for help using TensorFlow.\n"]}, {"number": 3745, "title": "Tensorflow and import IPython errors", "body": "When I use a thread-based data queue in Tensorflow and also include `import IPython`, I get various errors thrown at the very end of the Tensorflow session. Since I am offering a \"solution\" (do not import IPython), I am fine closing this immediately and re-opening if necessary. I am filing this mostly to have these errors and the cause on file in case it happens to someone else.\n### Environment info\n\n64-bit CentOS / cuda 7.5.18 / cudnn 5.0.5 / bazel 0.2.1 / python 3.4.4 / ipython 4.0.3\n\nTensorflow installed from source: 83fe2a5b7328e1b754b53cbbcf9b313450a2f863\n### Steps to reproduce\n1. Create file `files.txt` (lines omitted): \n   \n   ```\n   file0\n   file1\n   ...\n   file10\n   ```\n2. Create file `bug.py`\n   \n   ```\n   import tensorflow as tf\n   import IPython  # <-- remove and errors stop happening\n   \n   with tf.Session() as sess:\n       filename_queue = tf.train.string_input_producer(['files.txt'], shuffle=True)\n       reader = tf.TextLineReader()\n       key, value = reader.read(filename_queue)\n       batch_size = 3\n       min_after_dequeue = 10\n       capacity = min_after_dequeue + 3 * batch_size\n       batch_fn = tf.train.shuffle_batch(\n               [value], batch_size=batch_size, capacity=capacity,\n               min_after_dequeue=min_after_dequeue)\n   \n       sess.run(tf.initialize_all_variables())\n       coord = tf.train.Coordinator()\n       threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n       try:\n           while not coord.should_stop():\n               print(sess.run(batch_fn))\n               break\n       except tf.errors.OutOfRangeError:\n           pass\n       finally:\n           coord.request_stop()\n       coord.join(threads)\n   ```\n3. Run `python bug.py` (repeatedly if no error occurs) \n### What have you tried?\n\nRemoving `import IPython` fixes it. I have only tested it on one platform, so I do not know if this is a universal problem.\n\nOf course, this might also happen if packages that import IPython are imported. It originally happened to me when I imported ipdb.\n### Errors\n\nIn the tradition of thread-related bugs, the error message is not deterministic and 100 runs breaks down as follows (I am being thorough here to make these searchable):\n\n| Occurrences | Error |\n| --- | --- |\n| 35 | No error |\n| 34 | Error 1 |\n| 29 | Error 2 |\n| 2 | Error 3 |\n\nInvestigating these errors further leads nowhere, since it is clear that Python is behaving erratically and variables that are clearly set one line can be corrupted the next.\n#### Error 1\n\n```\nException ignored in: <bound method Session.__del__ of <tensorflow.python.client.session.Session object at 0x7f12d1d75908>>\nTraceback (most recent call last):\n  File \"/.../python3.4/site-packages/tensorflow/python/client/session.py\", line 524, in __del__\nAttributeError: 'NoneType' object has no attribute 'raise_exception_on_not_ok_status'\n```\n#### Error 2\n\n```\nException ignored in: <bound method Session.__del__ of <tensorflow.python.client.session.Session object at 0x7fde89961908>>\nTraceback (most recent call last):\n  File \"/.../python3.4/site-packages/tensorflow/python/client/session.py\", line 524, in __del__\n  File \"/share/data/vision-greg/common/anaconda3/lib/python3.4/contextlib.py\", line 126, in helper\nTypeError: 'NoneType' object is not callable\n```\n#### Error 3\n\n```\nException ignored in: <bound method Session.__del__ of <tensorflow.python.client.session.Session object at 0x7feec524b908>>\nTraceback (most recent call last):\n  File \"/.../python3.4/site-packages/tensorflow/python/client/session.py\", line 524, in __del__\n  File \"/share/data/vision-greg/common/anaconda3/lib/python3.4/contextlib.py\", line 59, in __enter__\n  File \"/.../python3.4/site-packages/tensorflow/python/framework/errors.py\", line 452, in raise_exception_on_not_ok_status\nUnboundLocalError: local variable 'status' referenced before assignment\n```\n", "comments": ["What if you call `del sess` before the end of your program? From the errors, it sounds like `__del__` is called after some modules it depends on have been unloaded\n", "Nice call, adding `del sess` resolves the problem while still importing IPython.\n", "@yaroslavvb Thanks for the catch!\n", "Another thing that worked was to add `del IPython` at the end of the file.\n"]}, {"number": 3744, "title": "extract element from list when py_func's output type is a single tensorflow type", "body": "#3464\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please\n", "LGTM\n"]}, {"number": 3743, "title": "Bazel missing dependency declaration for Eigen", "body": "Compilation error that suggests Bazel is not recognizing the Eigen header files. Occurs consistently for me on all versions starting from 21716d8f6e175cd6e8cd97a84e48497574268b0c up until the current master (00726750c9f7dedfc57685c5011e21df3b5b3706).\n### Environment info\n\n64-bit CentOS / cuda 7.5.18 / cudnn 5.0.5 / bazel 0.2.1 / java 1.8.0_91\n### Steps to reproduce\n\nRun `./configure` and set up compilation with CUDA 7.5 and CuDNN 5.0.\n\n```\nbazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package\n```\n\nCompiles fine without `--config=cuda`.\n### What have you tried?\n\nI ran a `git bisect` and traced it to 21716d8f6e175cd6e8cd97a84e48497574268b0c. More specifically, it is moving Eigen out of the `eigen-eigen-b4fa9622b809` folder that for some reason is causing this. Reverting only these changes from the master makes it compile just fine (see https://github.com/gustavla/tensorflow/commit/058a6517576171bb0c7dcf11ec1aac04bd59aedf). I have so far been unable to fix it while keeping the `strip_prefix` in.\n\nIt suspect it might be a quirk in bazel 0.2.1. It would be great if anyone else running that version can confirm.\n### Error\n\nThe error message (abridged and personal paths removed):\n\n```\nERROR: /.../tensorflow/core/kernels/BUILD:1527:1: undeclared inclusion(s) in rule '//tensorflow/core/kernels:depth_space_ops_gpu':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/kernels/spacetodepth_op_gpu.cu.cc':\n  '/.../.cache/external/eigen_archive/Eigen/Core'\n  '/.../.cache/external/eigen_archive/Eigen/src/Core/util/DisableStupidWarnings.h'\n  '/.../.cache/external/eigen_archive/Eigen/src/Core/util/Macros.h'\n  ...\n  '/.../.cache/external/eigen_archive/Eigen/src/Core/ArrayWrapper.h'\n  '/.../.cache/external/eigen_archive/Eigen/src/Core/GlobalFunctions.h'\n  '/.../.cache/external/eigen_archive/Eigen/src/Core/util/ReenableStupidWarnings.h'.\n```\n\nIssues with related error messages include #1157 and  #3589.\n", "comments": ["Bazel 0.2.1 is quite old.  Could you check if upgrading to recent bazel solves the problem?\n", "I just upgraded to Bazel 0.3.1 and tried to compile again (after fully expunging the cache). The problem still happens.\n", "@martinwicke Have you seen anything similar?\n", "The same happens for me on Ubuntu 16.04.1 with Bazel 0.3.1. Tried reconfiguring with and without GPU support - this doesn't change anything. \n", "@davidzchen We discussed this change a while ago, and it seemed to be doing ok -- did something change in bazel 0.3.1?\n", "I have tested on all versions of bazel supported by the master branch now (0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.3.0, 0.3.1) and I got the same error every time. I carefully cleaned cache and killed any lingering bazel process between builds.\n\nI also double-checked that building without GPU support is indeed fine, contrary to @flybirdx101's experience.\n", "Here is my error message, if this helps (abridged and personal paths removed):\n\n```\nERROR: /.../tensorflow/core/BUILD:925:1: undeclared inclusion(s) in rule '//tensorflow/core:framework_internal':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/core/framework/device_base.cc':\n  '/.../third_party/eigen3/unsupported/Eigen/CXX11/CustomOps'\n  '/.../third_party/eigen3/unsupported/Eigen/CXX11/src/CustomOps/CTypes.h'\n  '/.../third_party/eigen3/unsupported/Eigen/CXX11/src/CustomOps/CMatMatProduct.h'\n```\n\nUbuntu 16.04.1 LTS, Bazel 0.3.1-2016-08-12 ([e0029d8](https://github.com/bazelbuild/bazel/commit/e0029d87045712e8cbacdec2fd391c0969453fbb)), TensorFlow [a6323a7](https://github.com/tensorflow/tensorflow/commit/a6323a7b6ad8fbf6ee84c05bc991219a911c11b9), Java 1.8.0_101\n", "@martinwicke - What is the relationship between `third_party/eigen3` and the `eigen_archive` external repository? `//third_party/eigen3` depends on `@eigen_archive//:eigen`, but I'm not familiar with what the differences between these two packages are.\n\n@flybirdx101's error message seems to be complaining about headers in `//third_party/eigen3` and not `@eigen_archive`:\n\n```\nthird_party/eigen3/unsupported/Eigen/CXX11/CustomOps\nthird_party/eigen3/unsupported/Eigen/CXX11/src/CustomOps/CTypes.h\nthird_party/eigen3/unsupported/Eigen/CXX11/src/CustomOps/CMatMatProduct.h\n```\n\nThese headers do not seem to be listed in the `hdrs` for the `cc_library` rule in `third_party/eigen3`'s [BUILD file](https://github.com/tensorflow/tensorflow/blob/master/third_party/eigen3/BUILD).\n\n@flybirdx101 - can you try adding the following items to the `hdrs` glob for the `cc_library` rule in `third_party/eigen3/BUILD`?\n\n``` python\n        \"unsupported/Eigen/CXX11/CustomOps\",\n        \"unsupported/Eigen/CXX11/src/CustomOps/*.h\",\n```\n", "It looks like @flybirdx101 and my issue are probably not the same. @flybirdx101: Did you add `CustomOps` yourself? I can't find any evidence of that directory ever being in tensorflow.\n\n@davidzchen `@eigen_archive` is the external library that is downloaded and unpacked at build time. `//third_party/eigen3` is a thin wrapper that makes Eigen look like a local bazel package (I imagine for the purpose of the dependency graph, but I'm just guessing).\n", "I have after a lot of debugging understood this error and it is more appropriately filed and fixed as a bazel issue (https://github.com/bazelbuild/bazel/issues/1642).\n\nIn the meantime, the dirty fix is to name your cache directory to something really really long:\n\n```\nbazel --output_base=/var/tmp/.really-really-long-cache ...\n```\n\nIf it still doesn't work, try making it even longer.\n", "That was some impressive sleuthing! Thanks!\nOn Sun, Aug 14, 2016 at 17:44 Gustav Larsson notifications@github.com\nwrote:\n\n> Closed #3743 https://github.com/tensorflow/tensorflow/issues/3743.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/3743#event-755188390,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AAjO_WmVbY8m23NYdH8CBmP2gDg_dh4Yks5qf7ZPgaJpZM4Jhxs3\n> .\n", "Thanks, @gustavla!\n", "Acually, after further digging (catch up at https://github.com/bazelbuild/bazel/issues/1642), it turns out that this is indeed a Tensorflow bug. Instead of `gcc -M` directly, it is actually the python script `crosstool_wrapper_driver_is_not_gcc` that gets called, which in turn calls gcc (or in the case I am dealing with nvcc, which then invokes gcc). The problem is that this python script is not acknowledging when `-fno-canonical-system-headers` is added, so it does not get included in the actual call.\n\nI tried adding `-fno-canonical-system-headers` inside the `--compiler-options` of the python script if it was part of the python script's arguments. This fixed the problem and I got no build errors. What do you think of this solution? The question is, should _all_ arguments (or at least all `-f...` arguments) be propagated into the `--compiler-options` or should a special case be done for `-fno-canonical-system-headers` alone?\n", "@davidzchen I have tried this, but it didn't work, the same error.\n\n@gustavla These are my own custom Eigen ops, they used to work just fine before I merged my TensorFlow fork with the upstream repository. \n\nI have also tried the long cache directory name fix, but it is not working for me as well.\n", "Actually my bad, the error has changed. Now it is like this:\n\n```\n/tensorflow/contrib/factorization/kernels/BUILD:20:1: undeclared inclusion(s) in rule '//tensorflow/contrib/factorization/kernels:wals_solver_ops':\nthis rule is missing dependency declarations for the following files included by 'tensorflow/contrib/factorization/kernels/wals_solver_ops.cc':\n```\n\nAnd then the same list of my custom ops.\n\nI will have to dig deeper.\n", "Okey, there was my mistake and it looks like @davidzchen 's advice has fixed everything for me. \n\nI have updated to TensorFlow [6d57860](https://github.com/tensorflow/tensorflow/commit/6d57860e390f8f44580ae3a05aa350a4b6138b2b) and Bazel [936c2c2](https://github.com/bazelbuild/bazel/commit/936c2c2c815b64525bc6d3c6ac8f049655589370) and everything is working now. Thanks!\n", "I just pushed a PR with a propsed fix to this. Note, this is fixing the issue that I described and not the one @flybirdx101 was having, which turned out to be a separate issue altogether.\n", "PR has been merged. Closing this.\n"]}, {"number": 3742, "title": "solve the bug which the python include config gives out repeat results", "body": "```\n./configure \nPlease specify the location of python. [Default is /home/wenjian/anaconda3/bin/python]: \nDo you wish to build TensorFlow with Google Cloud Platform support? [y/N] N\nNo Google Cloud Platform support will be enabled for TensorFlow\nFound possible Python library paths:\n  /home/wenjian/anaconda3/lib/python3.5/site-packages\n  /home/wenjian/anaconda3/lib/python3.5/site-packages\nPlease input the desired Python library path to use.  Default is [/home/wenjian/anaconda3/lib/python3.5/site-packages]\n```\n\nit shows repeat result in config on python includes, it is because the old code show paths[0] and ret_paths at the same time,\nso I add a `else` condition and do some simple optimization by using `join`\n", "comments": ["Jenkins, test this please\n", "@vrv , should this PR be merged? I think it is a easy fix.\n"]}, {"number": 3741, "title": "Support a separate namespace for the protobuf library", "body": "Some clients are already using protobuf v2 as part of their application and would need a lot of work to move, but TensorFlow requires v3. This script patches the protobuf source code after downloading it to put it into a google::protobuf3 namespace, and alters all the necessary TensorFlow code to compile with this new namespace. This allows the v3 library to be linked into applications alongside v2 without causing linker errors.\n", "comments": ["Jenkins, test this please.\n", "The only testing failure is the unrelated dnn_test, which is a known issue, so I believe the testing for this is passed.\n", "Jenkins, test this please.\n", "@tensorflow-jenkins test this please\n"]}, {"number": 3740, "title": "Document tf.nn.conv1d", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Jenkins, test this please.\n", "Failures seem unrelated here, e.g.,\n\n```\n============== BEGINS failure log content ==============\nTraceback (most recent call last):\n  File \"/workspace/pip_test/tests/retrain_test.py\", line 24, in <module>\n    from tensorflow.examples.image_retraining import label_image\nImportError: cannot import name label_image\n```\n"]}, {"number": 3739, "title": "Cross entropy should give targets out of range error as given by tf.nn.in_top_k", "body": "I ran the tensorflow code(given below) and it gave me error(error stack is below code) targets out of range.\n\nI have figured out what was causing this error, it was due to mismatch between labels and outputs, like I'm doing 8 class sentiment classification and my labels are `(1,2,3,4,7,8,9,10)` so it was unable to match predictions`(1,2,3,4,5,6,7,8)` with my labels, so that's why it was giving out of range error. My question is, why it didn't gave me error in this line `c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)` , how it's matching labels with predictions in this case as opposed to in in_top_k? I think c_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y) should give me error because predictions and labels are not same. Why I'm not getting targets out of range error in cross entropy function? \n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nfrom nltk.tokenize import TweetTokenizer\nbatch = 500\nstart = 0\nend = batch - 1\nlearning_rate = 0.2\nnum_classes = 8\npath = \"/home/indy/Downloads/aclImdb/train/pos\"\ntime_steps = 250\nembedding = 50\n\ndef get_embedding():\n    gfile_path = os.path.join(\"/home/indy/Downloads/glove.6B\", \"glove.6B.50d.txt\")\n    f = open(gfile_path,'r')\n    embeddings = {}\n    for line in f:\n        sp_value = line.split()\n        word = sp_value[0]\n        embedding = [float(value) for value in sp_value[1:]]\n        embeddings[word] = embedding\n    return embeddings\n\nebd = get_embedding()\n\ndef get_y(file_name):\n    y_value = file_name.split('_')\n    y_value = y_value[1].split('.')\n    return y_value[0] \n\ndef get_x(path,file_name):\n    file_path = os.path.join(path,file_name)\n    x_value = open(file_path,'r')\n    for line in x_value:\n        x_value = line.replace(\"<br /><br />\",\"\") \n        x_value = x_value.lower()\n    tokeniz = TweetTokenizer()\n    x_value = tokeniz.tokenize(x_value)\n    padding = 250 - len(x_value)\n    if padding > 0:\n       p_value = ['pad' for i in range(padding)]\n       x_value = np.concatenate((x_value,p_value))\n    x_value = [ebd['value'] for value in x_value]\n\n    return x_value\n\ndef  batch_f(path):\n     directory = os.listdir(path)\n     y = [get_y(directory[i]) for i in range(len(directory))]\n     x = [get_x(path,directory[i]) for i in range(len(directory))]    \n     return x,y\n\n\nX = tf.placeholder(tf.float32, [batch,time_steps,embedding])\nY = tf.placeholder(tf.int32, [batch])\n\ndef build_nlp_model(x, _units, lstm_layers,num_classes):\n\n     x = tf.transpose(x, [1, 0, 2])\n     x = tf.reshape(x, [-1, embedding])\n     x = tf.split(0, time_steps, x)\n\n\n     lstm = tf.nn.rnn_cell.LSTMCell(num_units = _units, state_is_tuple = True)\n\n     multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\n     outputs , state = tf.nn.rnn(multi_lstm,x, dtype = tf.float32)     \n\n     weights = tf.Variable(tf.random_normal([_units,num_classes]))\n     biases  = tf.Variable(tf.random_normal([num_classes]))\n\n     logits = tf.matmul(outputs[-1], weights) + biases\n     return logits\n\nlogits = build_nlp_model(X,400,4,num_classes)\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\n\ndecayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(decayed_learning_rate)\nminimize_loss = optimizer.minimize(loss)\n\n\n\ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(25):\n         x, y = batch_f(path)\n         sess.run(minimize_loss,feed_dict={X : x, Y : y})\n         accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n         cost = sess.run(loss,feed_dict = {X: x,Y: y})\n         start = end \n         end = (start + batch)\n         print (\"Minibatch Loss = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n```\n\nThis is the error stack that is caused by tf.nn.in_top_k.\n\n```\n(500, 250, 50)\n(500,)\nTraceback (most recent call last):\n  File \"nlp.py\", line 115, in <module>\n    accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 372, in run\n    run_metadata_ptr)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 636, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 708, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 728, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: targets[0] is out of range\n     [[Node: InTopK = InTopK[T=DT_INT32, k=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add, _recv_Placeholder_1_0)]]\nCaused by op u'InTopK', defined at:\n  File \"nlp.py\", line 102, in <module>\n    correct_predict = tf.nn.in_top_k(logits, Y, 1)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 890, in in_top_k\n    targets=targets, k=k, name=name)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 704, in apply_op\n    op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2260, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1230, in __init__\n    self._traceback = _extract_stack()\n```\n\nAnd I think this type of error(targets out of range) should be given by cross entropy also, when labels don't match with predictions.\n", "comments": ["Can you reduce it to a small example? That's a lot of code to read through...\n", "```\nimport tensorflow as tf\nimport numpy as np\n\ny = [1,2,3,4,7,8,9,10]\nbatch_size = 8\nnum_classes = 8\n\nweights  = tf.Variable(tf.random_normal([batch_size,num_classes]))\nbiases = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = weights + biases\n\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,y)\ntop_k = tf.nn.in_top_k(logits,y,1)\n\ninit = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init)\n\nprint sess.run(cross_entropy)\nprint sess.run(top_k)\n```\n\n@yaroslavvb  This is the code to reproduce the result. As you will see, cross_entropy is printed while top_k is not printed giving error[5] targets out of range. I don't know why cross_entropy doesn't give this type of error, it should give this error because there is mismatch between labels(y) and predictions.\n\nHere is the output:\n\n```\n[ 3.54709339  4.26697874  2.73241401  2.7566452   3.60998082  0.          0.\n  0.        ]\nTraceback (most recent call last):\n  File \"python.py\", line 22, in <module>\n    print sess.run(top_k)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 340, in run\n    run_metadata_ptr)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 637, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 659, in _do_call\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: targets[5] is out of range\n     [[Node: InTopK = InTopK[T=DT_INT32, k=1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](add, InTopK/targets)]]\nCaused by op u'InTopK', defined at:\n  File \"python.py\", line 14, in <module>\n    top_k = tf.nn.in_top_k(logits,y,1)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 549, in in_top_k\n    targets=targets, k=k, name=name)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 655, in apply_op\n    op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2154, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/indy/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1154, in __init__\n    self._traceback = _extract_stack()\n```\n", "OK, looks like there's a TODO [to fix this](https://github.com/tensorflow/tensorflow/blob/2a10e469d70827ede21c0bb5d0a46aac97404d4c/tensorflow/python/ops/nn_ops.py#L537) added[ in april](https://github.com/tensorflow/tensorflow/commit/7fdd41b374cf6ed3528985e55ddeebe431df13eb) in python/ops/nn_ops.py\n\n`# TODO(pcmurray) Raise an error when the label is not an index in`\n", "@yaroslavvb  Yes, this is what I'm looking for. It's an issue.\n", "I believe `sparse_softmax_cross_entropy_with_logits` spits out nans now for invalid indices, and this is the best that can be feasibly done given the structure of the code (especially on GPU).  Unfortunately Eigen is designed to make returning errors nearly impossible.\n", "Please\uff0c how to solve the problem\uff1fThanks\uff01", "try to use y = [0,1,2,3,4,7,8,9]\r\nis that ok?"]}]