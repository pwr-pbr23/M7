[{"number": 48592, "title": "TensorFlow Reddit Dataset to DataFrame Throws Error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Python version: 3.6.9\r\n- TensorFlow version (use command below):\r\n\r\n```\r\n>>> tf.version.GIT_VERSION\r\n'v2.4.0-49-g85c8b2a817f'\r\n>>> tf.version.VERSION\r\n'2.4.1'\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI want to load the [tfds reddit dataset](https://www.tensorflow.org/datasets/catalog/reddit) and convert it to a dataframe. The below standalone code throws the following error:\r\n```\r\n  File \"/home/rylan/Documents/FieteLab-RCRP/exp_03_language_modeling/main.py\", line 36, in load_dataset\r\n    reddit_dataframe = tfds.as_dataframe(reddit_dataset)\r\n  File \"/home/rylan/Documents/FieteLab-RCRP/rcrp/lib/python3.6/site-packages/tensorflow_datasets/core/as_dataframe.py\", line 218, in as_dataframe\r\n    df = StyledDataFrame(rows)\r\n  File \"/home/rylan/Documents/FieteLab-RCRP/rcrp/lib/python3.6/site-packages/tensorflow_datasets/core/as_dataframe.py\", line 144, in __init__\r\n    super().__init__(*args, **kwargs)\r\nTypeError: object.__init__() takes no parameters\r\npython-BaseException\r\n```\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\nreddit_dataset = tfds.load(\r\n    'reddit',\r\n    split='train',\r\n    shuffle_files=False,\r\n    download=True,\r\n    data_dir=data_dir)\r\nassert isinstance(reddit_dataset, tf.data.Dataset)\r\nreddit_dataframe = tfds.as_dataframe(reddit_dataset)\r\n\r\n```\r\n", "comments": ["The error goes away and a sensible result is produced if I remove these two lines https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/core/as_dataframe.py#L222-L223 and add `df = pandas.DataFrame(rows)`.\r\n\r\nInterestingly, if I instead try `df = DataFrame(rows)`, the original error is thrown:\r\n\r\n```\r\n  File \"/home/rylan/Documents/FieteLab-RCRP/rcrp/lib/python3.6/site-packages/tensorflow_datasets/core/as_dataframe.py\", line 221, in as_dataframe\r\n    df = DataFrame(rows)\r\nTypeError: object() takes no parameters\r\npython-BaseException\r\n```", "Ok upon further investigation, I found the cause of the error. I'm using Tensorflow Datasets 4.2.0, which has the following at the top of the `as_dataframe.py` file:\r\n\r\n```\r\ntry:\r\n  import pandas  # pylint: disable=g-import-not-at-top\r\n  import pandas.io.formats.style  # pylint: disable=g-import-not-at-top\r\n  DataFrame = pandas.DataFrame\r\nexcept ImportError:\r\n  DataFrame = object\r\n```\r\n\r\nThe second import statement was failing, causing `DataFrame` to be set to `object`.  I noticed this import was removed in the most recent commit. \r\n\r\nhttps://github.com/tensorflow/datasets/commit/81e8ec79e9782f6eb5a93689bef9a5a076a3c2d4\r\n\r\nFor anyone else with a similar problem, the solution is to remove `import pandas.io.formats.style`.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48592\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48592\">No</a>\n"]}, {"number": 48591, "title": "Add \"contribution welcome\" in Bug template", "body": "This will add a candidate contribution claim in the Bug Template (we could add this also in other templates if you like)\r\n\r\n/cc @theadactyl @nikitamaia @joanafilipa", "comments": []}, {"number": 48590, "title": "error: 'tf.TensorScatterUpdate', 'tf.Size' op is neither a custom op nor a flex op", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Ubuntu 18.04.4 LTS\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.3.0\r\n\r\n### 2. Code\r\nI'm providing the saved model in the [google-drive](https://drive.google.com/file/d/16tkI7FWiQ8QTqNpBUAceLbS8jNljLKsL/view?usp=sharing).\r\n```\r\nimport tensorflow as tf\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model_6.2090354') # path to the SavedModel directory\r\ntflite_model = converter.convert()\r\n```\r\n\r\n### 3. Failure after conversion\r\nI'm getting following error:\r\n```\r\nloc(callsite(\"while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/Size@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): error: 'tf.Size' op is neither a custom op nor a flex op\r\nloc(callsite(\"while/NotEqual@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): error: 'tf.NotEqual' op is neither a custom op nor a flex op\r\nloc(\"while/cond@__inference_while_body_32776_19417\"): error: failed while converting: 'while/cond_then': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.NotEqual {device = \"\", incompatible_shape_error = false}\r\n\ttf.Size {device = \"\"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.TensorScatterUpdate {device = \"\"}\r\nTraceback (most recent call last):\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 199, in toco_convert_protos\r\n    enable_mlir_converter)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/wrap_toco.py\", line 38, in wrapped_toco_convert\r\n    enable_mlir_converter)\r\nException: <unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %37 = \"tf.TensorScatterUpdate\"(%arg5, %1, %36) {device = \"\"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %39 = \"tf.TensorScatterUpdate\"(%arg6, %1, %38) {device = \"\"} : (tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<?xi32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %66 = \"tf.TensorScatterUpdate\"(%arg7, %1, %65) {device = \"\"} : (tensor<?x300x4xf32>, tensor<1x1xi32>, tensor<1x?x?xf32>) -> tensor<?x300x4xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %75 = \"tf.TensorScatterUpdate\"(%arg8, %1, %74) {device = \"\"} : (tensor<?x300x?x?xf32>, tensor<1x1xi32>, tensor<1x?x?x?xf32>) -> tensor<?x300x?x?xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %80 = \"tf.TensorScatterUpdate\"(%arg9, %1, %79) {device = \"\"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): called from\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): see current operation: %12 = \"tf.TensorScatterUpdate\"(%arg5, %11, %6) {device = \"\"} : (tensor<?x4xf32>, tensor<?x1xi32>, tensor<?x4xf32>) -> tensor<?x4xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): called from\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): see current operation: %14 = \"tf.TensorScatterUpdate\"(%arg2, %11, %13) {device = \"\"} : (tensor<?x?xf32>, tensor<?x1xi32>, tensor<?x?xf32>) -> tensor<?x?xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): called from\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): see current operation: %19 = \"tf.TensorScatterUpdate\"(%arg4, %11, %18) {device = \"\"} : (tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> tensor<?xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): called from\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): see current operation: %20 = \"tf.TensorScatterUpdate\"(%arg3, %11, %15) {device = \"\"} : (tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> tensor<?xf32>\r\n<unknown>:0: error: loc(callsite(\"while/Size@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): 'tf.Size' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"]): called from\r\n<unknown>:0: note: loc(callsite(\"while/Size@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): see current operation: %28 = \"tf.Size\"(%27) {device = \"\"} : (tensor<?x?xf32>) -> tensor<i32>\r\n<unknown>:0: error: loc(callsite(\"while/NotEqual@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): 'tf.NotEqual' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"]): called from\r\n<unknown>:0: note: loc(callsite(\"while/NotEqual@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): see current operation: %29 = \"tf.NotEqual\"(%28, %cst) {device = \"\", incompatible_shape_error = false} : (tensor<i32>, tensor<i32>) -> tensor<i1>\r\n<unknown>:0: error: loc(\"while/cond@__inference_while_body_32776_19417\"): failed while converting: 'while/cond_then': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.NotEqual {device = \"\", incompatible_shape_error = false}\r\n\ttf.Size {device = \"\"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.TensorScatterUpdate {device = \"\"}\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<i32>, %arg1: tensor<?x?xf32>, %arg2: tensor<?x?xf32>, %arg3: tensor<?x?x?x64xf32>, %arg4: tensor<?x4xf32>, %arg5: tensor<?x300xf32>, %arg6: tensor<?xi32>, %arg7: tensor<?x300x4xf32>, %arg8: tensor<?x300x?x?xf32>, %arg9: tensor<?x300xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<[2, 0, 1]> : tensor<3xi32>} : () -> tensor<3xi32>\r\n  %cst_0 = \"std.constant\"() {value = dense<[1, 0]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_1 = \"std.constant\"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n  %cst_2 = \"std.constant\"() {value = dense<300> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_3 = \"std.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n  %cst_4 = \"std.constant\"() {value = dense<4> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_5 = \"std.constant\"() {value = dense<100> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_6 = \"std.constant\"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_7 = \"std.constant\"() {value = dense<1> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_8 = \"std.constant\"() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_9 = \"std.constant\"() {value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>\r\n  %cst_10 = \"std.constant\"() {value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_11 = \"std.constant\"() {value = dense<[300, 4]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_12 = \"std.constant\"() {value = dense<[0, 4]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_13 = \"std.constant\"() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_14 = \"std.constant\"() {value = dense<[0, 2]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_15 = \"std.constant\"() {value = dense<[0, 3]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_16 = \"std.constant\"() {value = dense<0> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_17 = \"std.constant\"() {value = dense<[300, -1]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_18 = \"std.constant\"() {value = dense<1> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_19 = \"std.constant\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_20 = \"std.constant\"() {value = dense<300> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_21 = \"std.constant\"() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %0 = \"tfl.pack\"(%arg0) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>\r\n  %1 = \"tfl.pack\"(%0) {axis = 0 : i32, values_count = 1 : i32} : (tensor<1xi32>) -> tensor<1x1xi32>\r\n  %2 = \"tfl.add\"(%arg0, %cst_7) {fused_activation_function = \"NONE\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %3 = \"tfl.pack\"(%2) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>\r\n  %4 = \"tfl.shape\"(%arg1) : (tensor<?x?xf32>) -> tensor<2xi32>\r\n  %5 = \"tfl.strided_slice\"(%4, %cst_21, %cst_8, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %6 = \"tfl.shape\"(%arg2) : (tensor<?x?xf32>) -> tensor<2xi32>\r\n  %7 = \"tfl.strided_slice\"(%6, %cst_21, %cst_8, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %8 = \"tfl.maximum\"(%7, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %9 = \"tfl.floor_div\"(%8, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %10 = \"tfl.floor_mod\"(%8, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %11 = \"tfl.not_equal\"(%10, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i1>\r\n  %12 = \"tfl.cast\"(%11) : (tensor<i1>) -> tensor<i32>\r\n  %13 = \"tfl.add\"(%9, %12) {fused_activation_function = \"NONE\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %14 = \"tfl.maximum\"(%13, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %15 = \"tfl.mul\"(%7, %cst_5) {fused_activation_function = \"NONE\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %16 = \"tfl.pack\"(%15, %cst_4) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %17 = \"tfl.fill\"(%16, %cst_3) : (tensor<2xi32>, tensor<f32>) -> tensor<?x4xf32>\r\n  %18 = \"tfl.pack\"(%15, %5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %19 = \"tfl.fill\"(%18, %cst_3) : (tensor<2xi32>, tensor<f32>) -> tensor<?x?xf32>\r\n  %20 = \"tfl.reshape\"(%15, %cst_10) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>\r\n  %21 = \"tfl.fill\"(%20, %cst_3) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>\r\n  %22 = \"tfl.strided_slice\"(%arg3, %0, %3, %cst_9) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<?x?x?x64xf32>, tensor<1xi32>, tensor<1xi32>, tensor<4xi32>) -> tensor<180x604x64xf32>\r\n  %23:11 = \"tfl.while\"(%cst_6, %cst_6, %19, %21, %21, %17, %8, %14, %arg2, %arg4, %arg1) ( {\r\n  ^bb0(%arg10: tensor<i32>, %arg11: tensor<i32>, %arg12: tensor<?x?xf32>, %arg13: tensor<?xf32>, %arg14: tensor<?xf32>, %arg15: tensor<?x4xf32>):  // no predecessors\r\n    %81 = \"std.call\"(%arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %8, %14, %arg2, %arg4, %arg1) {callee = @\"while/cond/while_cond\"} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> tensor<i1>\r\n    \"tfl.yield\"(%81) : (tensor<i1>) -> ()\r\n  },  {\r\n  ^bb0(%arg10: tensor<i32>, %arg11: tensor<i32>, %arg12: tensor<?x?xf32>, %arg13: tensor<?xf32>, %arg14: tensor<?xf32>, %arg15: tensor<?x4xf32>):  // no predecessors\r\n    %81:11 = \"std.call\"(%arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %8, %14, %arg2, %arg4, %arg1) {callee = @\"while/cond/while_body\"} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>)\r\n    \"tfl.yield\"(%81#0, %81#1, %81#2, %81#3, %81#4, %81#5, %81#6, %81#7, %81#8, %81#9, %81#10) : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> ()\r\n  }) {is_stateless = true} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>)\r\n  %24 = \"tfl.shape\"(%23#3) : (tensor<?xf32>) -> tensor<1xi32>\r\n  %25 = \"tfl.strided_slice\"(%24, %cst_10, %cst_19, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %values, %indices = \"tfl.topk_v2\"(%23#3, %25) : (tensor<?xf32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)\r\n  %26 = \"tfl.gather\"(%23#3, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>\r\n  %27 = \"tfl.strided_slice\"(%26, %cst_19, %cst_20, %cst_21) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>\r\n  %28 = \"tfl.gather\"(%23#5, %indices) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>\r\n  %29 = \"tfl.strided_slice\"(%28, %cst_16, %cst_11, %cst_18) {begin_mask = 3 : i32, ellipsis_mask = 0 : i32, end_mask = 2 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x4xf32>\r\n  %30 = \"tfl.shape\"(%29) : (tensor<?x4xf32>) -> tensor<2xi32>\r\n  %31 = \"tfl.strided_slice\"(%30, %cst_19, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %32 = \"tfl.sub\"(%cst_2, %31) {fused_activation_function = \"NONE\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %33 = \"tfl.reshape\"(%32, %cst_10) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>\r\n  %34 = \"tfl.fill\"(%33, %cst_3) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>\r\n  %35 = \"tfl.concatenation\"(%27, %34) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %36 = \"tfl.expand_dims\"(%35, %cst_6) : (tensor<?xf32>, tensor<i32>) -> tensor<1x?xf32>\r\n  %37 = \"tf.TensorScatterUpdate\"(%arg5, %1, %36) {device = \"\"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>\r\n  %38 = \"tfl.pack\"(%31) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>\r\n  %39 = \"tf.TensorScatterUpdate\"(%arg6, %1, %38) {device = \"\"} : (tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<?xi32>\r\n  %40 = \"tfl.pack\"(%cst_6, %32) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %41 = \"tfl.pack\"(%40, %cst_16, %cst_16) {axis = 0 : i32, values_count = 3 : i32} : (tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<3x2xi32>\r\n  %42 = \"tfl.pack\"(%40, %cst_16) {axis = 0 : i32, values_count = 2 : i32} : (tensor<2xi32>, tensor<2xi32>) -> tensor<2x2xi32>\r\n  %43 = \"tfl.strided_slice\"(%29, %cst_13, %cst_14, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>\r\n  %44 = \"tfl.strided_slice\"(%29, %cst_15, %cst_12, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>\r\n  %45 = \"tfl.maximum\"(%43, %44) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %46 = \"tfl.add\"(%45, %cst_3) {fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %47 = \"tfl.minimum\"(%46, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %48 = \"tfl.maximum\"(%47, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %49 = \"tfl.minimum\"(%43, %44) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %50 = \"tfl.sub\"(%49, %cst_3) {fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %51 = \"tfl.minimum\"(%50, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %52 = \"tfl.maximum\"(%51, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %53 = \"tfl.strided_slice\"(%29, %cst_16, %cst_13, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>\r\n  %54 = \"tfl.strided_slice\"(%29, %cst_14, %cst_15, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>\r\n  %55 = \"tfl.maximum\"(%53, %54) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %56 = \"tfl.add\"(%55, %cst_3) {fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %57 = \"tfl.minimum\"(%56, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %58 = \"tfl.maximum\"(%57, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %59 = \"tfl.minimum\"(%53, %54) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %60 = \"tfl.sub\"(%59, %cst_3) {fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %61 = \"tfl.minimum\"(%60, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %62 = \"tfl.maximum\"(%61, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %63 = \"tfl.pack\"(%62, %52, %58, %48) {axis = 1 : i32, values_count = 4 : i32} : (tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) -> tensor<?x4xf32>\r\n  %64 = \"tfl.pad\"(%63, %42) : (tensor<?x4xf32>, tensor<2x2xi32>) -> tensor<?x?xf32>\r\n  %65 = \"tfl.expand_dims\"(%64, %cst_6) : (tensor<?x?xf32>, tensor<i32>) -> tensor<1x?x?xf32>\r\n  %66 = \"tf.TensorScatterUpdate\"(%arg7, %1, %65) {device = \"\"} : (tensor<?x300x4xf32>, tensor<1x1xi32>, tensor<1x?x?xf32>) -> tensor<?x300x4xf32>\r\n  %67 = \"tfl.gather\"(%23#2, %indices) {axis = 0 : i32} : (tensor<?x?xf32>, tensor<?xi32>) -> tensor<?x?xf32>\r\n  %68 = \"tfl.strided_slice\"(%67, %cst_16, %cst_17, %cst_18) {begin_mask = 3 : i32, ellipsis_mask = 0 : i32, end_mask = 2 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?x?xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xf32>\r\n  %69 = \"tfl.transpose\"(%68, %cst_0) : (tensor<?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>\r\n  %70 = \"tfl.batch_matmul\"(%22, %69) {adj_x = false, adj_y = false} : (tensor<180x604x64xf32>, tensor<?x?xf32>) -> tensor<180x604x?xf32>\r\n  %71 = \"tfl.logistic\"(%70) : (tensor<180x604x?xf32>) -> tensor<180x604x?xf32>\r\n  %72 = \"tfl.transpose\"(%71, %cst) : (tensor<180x604x?xf32>, tensor<3xi32>) -> tensor<?x180x604xf32>\r\n  %73 = \"tfl.pad\"(%72, %41) : (tensor<?x180x604xf32>, tensor<3x2xi32>) -> tensor<?x?x?xf32>\r\n  %74 = \"tfl.expand_dims\"(%73, %cst_6) : (tensor<?x?x?xf32>, tensor<i32>) -> tensor<1x?x?x?xf32>\r\n  %75 = \"tf.TensorScatterUpdate\"(%arg8, %1, %74) {device = \"\"} : (tensor<?x300x?x?xf32>, tensor<1x1xi32>, tensor<1x?x?x?xf32>) -> tensor<?x300x?x?xf32>\r\n  %76 = \"tfl.gather\"(%23#4, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>\r\n  %77 = \"tfl.strided_slice\"(%76, %cst_19, %cst_20, %cst_21) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>\r\n  %78 = \"tfl.concatenation\"(%77, %34) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %79 = \"tfl.expand_dims\"(%78, %cst_6) : (tensor<?xf32>, tensor<i32>) -> tensor<1x?xf32>\r\n  %80 = \"tf.TensorScatterUpdate\"(%arg9, %1, %79) {device = \"\"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>\r\n  \"std.return\"(%39, %75, %37, %66, %80) : (tensor<?xi32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>, tensor<?x300x4xf32>, tensor<?x300xf32>) -> ()\r\n}) {sym_name = \"while/cond_then\", sym_visibility = \"private\", type = (tensor<i32>, tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?x?x64xf32>, tensor<?x4xf32>, tensor<?x300xf32>, tensor<?xi32>, tensor<?x300x4xf32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>) -> (tensor<?xi32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>, tensor<?x300x4xf32>, tensor<?x300xf32>)} : () -> ()\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 1076, in convert\r\n    return super(TFLiteConverterV2, self).convert()\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 900, in convert\r\n    self).convert(graph_def, input_tensors, output_tensors)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 633, in convert\r\n    **converter_kwargs)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 574, in toco_convert_impl\r\n    enable_mlir_converter=enable_mlir_converter)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/convert.py\", line 202, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: <unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate_2@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %37 = \"tf.TensorScatterUpdate\"(%arg5, %1, %36) {device = \"\"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate_4@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %39 = \"tf.TensorScatterUpdate\"(%arg6, %1, %38) {device = \"\"} : (tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<?xi32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %66 = \"tf.TensorScatterUpdate\"(%arg7, %1, %65) {device = \"\"} : (tensor<?x300x4xf32>, tensor<1x1xi32>, tensor<1x?x?xf32>) -> tensor<?x300x4xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate_3@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %75 = \"tf.TensorScatterUpdate\"(%arg8, %1, %74) {device = \"\"} : (tensor<?x300x?x?xf32>, tensor<1x1xi32>, tensor<1x?x?x?xf32>) -> tensor<?x300x?x?xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/TensorScatterUpdate_1@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): see current operation: %80 = \"tf.TensorScatterUpdate\"(%arg9, %1, %79) {device = \"\"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): called from\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/while/TensorScatterUpdate@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): see current operation: %12 = \"tf.TensorScatterUpdate\"(%arg5, %11, %6) {device = \"\"} : (tensor<?x4xf32>, tensor<?x1xi32>, tensor<?x4xf32>) -> tensor<?x4xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): called from\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/while/TensorScatterUpdate_1@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): see current operation: %14 = \"tf.TensorScatterUpdate\"(%arg2, %11, %13) {device = \"\"} : (tensor<?x?xf32>, tensor<?x1xi32>, tensor<?x?xf32>) -> tensor<?x?xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): called from\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/while/TensorScatterUpdate_2@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): see current operation: %19 = \"tf.TensorScatterUpdate\"(%arg4, %11, %18) {device = \"\"} : (tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> tensor<?xf32>\r\n<unknown>:0: error: loc(callsite(\"while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\")): called from\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): called from\r\n<unknown>:0: note: loc(callsite(\"while/cond/while/TensorScatterUpdate_3@__inference_while_cond_while_body_33002_4586\" at callsite(\"while/cond/while@__inference_while_cond_true_32942_19253\" at \"while/cond@__inference_while_body_32776_19417\"))): see current operation: %20 = \"tf.TensorScatterUpdate\"(%arg3, %11, %15) {device = \"\"} : (tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> tensor<?xf32>\r\n<unknown>:0: error: loc(callsite(\"while/Size@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): 'tf.Size' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"]): called from\r\n<unknown>:0: note: loc(callsite(\"while/Size@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): see current operation: %28 = \"tf.Size\"(%27) {device = \"\"} : (tensor<?x?xf32>) -> tensor<i32>\r\n<unknown>:0: error: loc(callsite(\"while/NotEqual@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): 'tf.NotEqual' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"]): called from\r\n<unknown>:0: note: loc(callsite(\"while/NotEqual@__inference_while_body_32776_19417\" at fused[\"while@__inference_call_40958\", \"StatefulPartitionedCall/yolact/StatefulPartitionedCall/while\"])): see current operation: %29 = \"tf.NotEqual\"(%28, %cst) {device = \"\", incompatible_shape_error = false} : (tensor<i32>, tensor<i32>) -> tensor<i1>\r\n<unknown>:0: error: loc(\"while/cond@__inference_while_body_32776_19417\"): failed while converting: 'while/cond_then': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):\r\n\ttf.NotEqual {device = \"\", incompatible_shape_error = false}\r\n\ttf.Size {device = \"\"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.TensorScatterUpdate {device = \"\"}\r\n<unknown>:0: note: loc(\"while/cond@__inference_while_body_32776_19417\"): see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<i32>, %arg1: tensor<?x?xf32>, %arg2: tensor<?x?xf32>, %arg3: tensor<?x?x?x64xf32>, %arg4: tensor<?x4xf32>, %arg5: tensor<?x300xf32>, %arg6: tensor<?xi32>, %arg7: tensor<?x300x4xf32>, %arg8: tensor<?x300x?x?xf32>, %arg9: tensor<?x300xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<[2, 0, 1]> : tensor<3xi32>} : () -> tensor<3xi32>\r\n  %cst_0 = \"std.constant\"() {value = dense<[1, 0]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_1 = \"std.constant\"() {value = dense<1.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n  %cst_2 = \"std.constant\"() {value = dense<300> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_3 = \"std.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n  %cst_4 = \"std.constant\"() {value = dense<4> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_5 = \"std.constant\"() {value = dense<100> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_6 = \"std.constant\"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_7 = \"std.constant\"() {value = dense<1> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_8 = \"std.constant\"() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_9 = \"std.constant\"() {value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>\r\n  %cst_10 = \"std.constant\"() {value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_11 = \"std.constant\"() {value = dense<[300, 4]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_12 = \"std.constant\"() {value = dense<[0, 4]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_13 = \"std.constant\"() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_14 = \"std.constant\"() {value = dense<[0, 2]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_15 = \"std.constant\"() {value = dense<[0, 3]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_16 = \"std.constant\"() {value = dense<0> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_17 = \"std.constant\"() {value = dense<[300, -1]> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_18 = \"std.constant\"() {value = dense<1> : tensor<2xi32>} : () -> tensor<2xi32>\r\n  %cst_19 = \"std.constant\"() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_20 = \"std.constant\"() {value = dense<300> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %cst_21 = \"std.constant\"() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>\r\n  %0 = \"tfl.pack\"(%arg0) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>\r\n  %1 = \"tfl.pack\"(%0) {axis = 0 : i32, values_count = 1 : i32} : (tensor<1xi32>) -> tensor<1x1xi32>\r\n  %2 = \"tfl.add\"(%arg0, %cst_7) {fused_activation_function = \"NONE\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %3 = \"tfl.pack\"(%2) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>\r\n  %4 = \"tfl.shape\"(%arg1) : (tensor<?x?xf32>) -> tensor<2xi32>\r\n  %5 = \"tfl.strided_slice\"(%4, %cst_21, %cst_8, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %6 = \"tfl.shape\"(%arg2) : (tensor<?x?xf32>) -> tensor<2xi32>\r\n  %7 = \"tfl.strided_slice\"(%6, %cst_21, %cst_8, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %8 = \"tfl.maximum\"(%7, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %9 = \"tfl.floor_div\"(%8, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %10 = \"tfl.floor_mod\"(%8, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %11 = \"tfl.not_equal\"(%10, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i1>\r\n  %12 = \"tfl.cast\"(%11) : (tensor<i1>) -> tensor<i32>\r\n  %13 = \"tfl.add\"(%9, %12) {fused_activation_function = \"NONE\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %14 = \"tfl.maximum\"(%13, %cst_6) : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %15 = \"tfl.mul\"(%7, %cst_5) {fused_activation_function = \"NONE\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %16 = \"tfl.pack\"(%15, %cst_4) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %17 = \"tfl.fill\"(%16, %cst_3) : (tensor<2xi32>, tensor<f32>) -> tensor<?x4xf32>\r\n  %18 = \"tfl.pack\"(%15, %5) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %19 = \"tfl.fill\"(%18, %cst_3) : (tensor<2xi32>, tensor<f32>) -> tensor<?x?xf32>\r\n  %20 = \"tfl.reshape\"(%15, %cst_10) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>\r\n  %21 = \"tfl.fill\"(%20, %cst_3) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>\r\n  %22 = \"tfl.strided_slice\"(%arg3, %0, %3, %cst_9) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<?x?x?x64xf32>, tensor<1xi32>, tensor<1xi32>, tensor<4xi32>) -> tensor<180x604x64xf32>\r\n  %23:11 = \"tfl.while\"(%cst_6, %cst_6, %19, %21, %21, %17, %8, %14, %arg2, %arg4, %arg1) ( {\r\n  ^bb0(%arg10: tensor<i32>, %arg11: tensor<i32>, %arg12: tensor<?x?xf32>, %arg13: tensor<?xf32>, %arg14: tensor<?xf32>, %arg15: tensor<?x4xf32>):  // no predecessors\r\n    %81 = \"std.call\"(%arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %8, %14, %arg2, %arg4, %arg1) {callee = @\"while/cond/while_cond\"} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> tensor<i1>\r\n    \"tfl.yield\"(%81) : (tensor<i1>) -> ()\r\n  },  {\r\n  ^bb0(%arg10: tensor<i32>, %arg11: tensor<i32>, %arg12: tensor<?x?xf32>, %arg13: tensor<?xf32>, %arg14: tensor<?xf32>, %arg15: tensor<?x4xf32>):  // no predecessors\r\n    %81:11 = \"std.call\"(%arg10, %arg11, %arg12, %arg13, %arg14, %arg15, %8, %14, %arg2, %arg4, %arg1) {callee = @\"while/cond/while_body\"} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>)\r\n    \"tfl.yield\"(%81#0, %81#1, %81#2, %81#3, %81#4, %81#5, %81#6, %81#7, %81#8, %81#9, %81#10) : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> ()\r\n  }) {is_stateless = true} : (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>) -> (tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?x4xf32>, tensor<i32>, tensor<i32>, tensor<?x?xf32>, tensor<?x4xf32>, tensor<?x?xf32>)\r\n  %24 = \"tfl.shape\"(%23#3) : (tensor<?xf32>) -> tensor<1xi32>\r\n  %25 = \"tfl.strided_slice\"(%24, %cst_10, %cst_19, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %values, %indices = \"tfl.topk_v2\"(%23#3, %25) : (tensor<?xf32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)\r\n  %26 = \"tfl.gather\"(%23#3, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>\r\n  %27 = \"tfl.strided_slice\"(%26, %cst_19, %cst_20, %cst_21) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>\r\n  %28 = \"tfl.gather\"(%23#5, %indices) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>\r\n  %29 = \"tfl.strided_slice\"(%28, %cst_16, %cst_11, %cst_18) {begin_mask = 3 : i32, ellipsis_mask = 0 : i32, end_mask = 2 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x4xf32>\r\n  %30 = \"tfl.shape\"(%29) : (tensor<?x4xf32>) -> tensor<2xi32>\r\n  %31 = \"tfl.strided_slice\"(%30, %cst_19, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>\r\n  %32 = \"tfl.sub\"(%cst_2, %31) {fused_activation_function = \"NONE\"} : (tensor<i32>, tensor<i32>) -> tensor<i32>\r\n  %33 = \"tfl.reshape\"(%32, %cst_10) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>\r\n  %34 = \"tfl.fill\"(%33, %cst_3) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>\r\n  %35 = \"tfl.concatenation\"(%27, %34) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %36 = \"tfl.expand_dims\"(%35, %cst_6) : (tensor<?xf32>, tensor<i32>) -> tensor<1x?xf32>\r\n  %37 = \"tf.TensorScatterUpdate\"(%arg5, %1, %36) {device = \"\"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>\r\n  %38 = \"tfl.pack\"(%31) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>\r\n  %39 = \"tf.TensorScatterUpdate\"(%arg6, %1, %38) {device = \"\"} : (tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> tensor<?xi32>\r\n  %40 = \"tfl.pack\"(%cst_6, %32) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>\r\n  %41 = \"tfl.pack\"(%40, %cst_16, %cst_16) {axis = 0 : i32, values_count = 3 : i32} : (tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<3x2xi32>\r\n  %42 = \"tfl.pack\"(%40, %cst_16) {axis = 0 : i32, values_count = 2 : i32} : (tensor<2xi32>, tensor<2xi32>) -> tensor<2x2xi32>\r\n  %43 = \"tfl.strided_slice\"(%29, %cst_13, %cst_14, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>\r\n  %44 = \"tfl.strided_slice\"(%29, %cst_15, %cst_12, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>\r\n  %45 = \"tfl.maximum\"(%43, %44) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %46 = \"tfl.add\"(%45, %cst_3) {fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %47 = \"tfl.minimum\"(%46, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %48 = \"tfl.maximum\"(%47, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %49 = \"tfl.minimum\"(%43, %44) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %50 = \"tfl.sub\"(%49, %cst_3) {fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %51 = \"tfl.minimum\"(%50, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %52 = \"tfl.maximum\"(%51, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %53 = \"tfl.strided_slice\"(%29, %cst_16, %cst_13, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>\r\n  %54 = \"tfl.strided_slice\"(%29, %cst_14, %cst_15, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?xf32>\r\n  %55 = \"tfl.maximum\"(%53, %54) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %56 = \"tfl.add\"(%55, %cst_3) {fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %57 = \"tfl.minimum\"(%56, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %58 = \"tfl.maximum\"(%57, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %59 = \"tfl.minimum\"(%53, %54) : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %60 = \"tfl.sub\"(%59, %cst_3) {fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %61 = \"tfl.minimum\"(%60, %cst_1) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %62 = \"tfl.maximum\"(%61, %cst_3) : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>\r\n  %63 = \"tfl.pack\"(%62, %52, %58, %48) {axis = 1 : i32, values_count = 4 : i32} : (tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) -> tensor<?x4xf32>\r\n  %64 = \"tfl.pad\"(%63, %42) : (tensor<?x4xf32>, tensor<2x2xi32>) -> tensor<?x?xf32>\r\n  %65 = \"tfl.expand_dims\"(%64, %cst_6) : (tensor<?x?xf32>, tensor<i32>) -> tensor<1x?x?xf32>\r\n  %66 = \"tf.TensorScatterUpdate\"(%arg7, %1, %65) {device = \"\"} : (tensor<?x300x4xf32>, tensor<1x1xi32>, tensor<1x?x?xf32>) -> tensor<?x300x4xf32>\r\n  %67 = \"tfl.gather\"(%23#2, %indices) {axis = 0 : i32} : (tensor<?x?xf32>, tensor<?xi32>) -> tensor<?x?xf32>\r\n  %68 = \"tfl.strided_slice\"(%67, %cst_16, %cst_17, %cst_18) {begin_mask = 3 : i32, ellipsis_mask = 0 : i32, end_mask = 2 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?x?xf32>, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xf32>\r\n  %69 = \"tfl.transpose\"(%68, %cst_0) : (tensor<?x?xf32>, tensor<2xi32>) -> tensor<?x?xf32>\r\n  %70 = \"tfl.batch_matmul\"(%22, %69) {adj_x = false, adj_y = false} : (tensor<180x604x64xf32>, tensor<?x?xf32>) -> tensor<180x604x?xf32>\r\n  %71 = \"tfl.logistic\"(%70) : (tensor<180x604x?xf32>) -> tensor<180x604x?xf32>\r\n  %72 = \"tfl.transpose\"(%71, %cst) : (tensor<180x604x?xf32>, tensor<3xi32>) -> tensor<?x180x604xf32>\r\n  %73 = \"tfl.pad\"(%72, %41) : (tensor<?x180x604xf32>, tensor<3x2xi32>) -> tensor<?x?x?xf32>\r\n  %74 = \"tfl.expand_dims\"(%73, %cst_6) : (tensor<?x?x?xf32>, tensor<i32>) -> tensor<1x?x?x?xf32>\r\n  %75 = \"tf.TensorScatterUpdate\"(%arg8, %1, %74) {device = \"\"} : (tensor<?x300x?x?xf32>, tensor<1x1xi32>, tensor<1x?x?x?xf32>) -> tensor<?x300x?x?xf32>\r\n  %76 = \"tfl.gather\"(%23#4, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>\r\n  %77 = \"tfl.strided_slice\"(%76, %cst_19, %cst_20, %cst_21) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 0 : i32} : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>\r\n  %78 = \"tfl.concatenation\"(%77, %34) {axis = 0 : i32, fused_activation_function = \"NONE\"} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>\r\n  %79 = \"tfl.expand_dims\"(%78, %cst_6) : (tensor<?xf32>, tensor<i32>) -> tensor<1x?xf32>\r\n  %80 = \"tf.TensorScatterUpdate\"(%arg9, %1, %79) {device = \"\"} : (tensor<?x300xf32>, tensor<1x1xi32>, tensor<1x?xf32>) -> tensor<?x300xf32>\r\n  \"std.return\"(%39, %75, %37, %66, %80) : (tensor<?xi32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>, tensor<?x300x4xf32>, tensor<?x300xf32>) -> ()\r\n}) {sym_name = \"while/cond_then\", sym_visibility = \"private\", type = (tensor<i32>, tensor<?x?xf32>, tensor<?x?xf32>, tensor<?x?x?x64xf32>, tensor<?x4xf32>, tensor<?x300xf32>, tensor<?xi32>, tensor<?x300x4xf32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>) -> (tensor<?xi32>, tensor<?x300x?x?xf32>, tensor<?x300xf32>, tensor<?x300x4xf32>, tensor<?x300xf32>)} : () -> ()\r\n```\r\n\r\nThe function where I'm using `tensor_scatter_nd_update` is as follows:\r\n```\r\ndef _traditional_nms(self, boxes, mask_coef, scores, iou_threshold=0.5, score_threshold=0.3, max_class_output_size=100, max_output_size=300, soft_nms_sigma=0.5):\r\n        num_classes = tf.shape(scores)[1]\r\n\r\n        _num_coef = tf.shape(mask_coef)[1]\r\n        _boxes = tf.zeros((max_class_output_size*num_classes, 4), tf.float32)\r\n        _coefs = tf.zeros((max_class_output_size*num_classes, _num_coef), tf.float32)\r\n        _classes = tf.zeros((max_class_output_size*num_classes), tf.float32)\r\n        _scores = tf.zeros((max_class_output_size*num_classes), tf.float32)\r\n\r\n        for _cls in range(num_classes):\r\n            cls_scores = scores[:, _cls]\r\n            selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(\r\n                boxes, \r\n                cls_scores, \r\n                max_output_size=max_class_output_size, \r\n                iou_threshold=iou_threshold, \r\n                score_threshold=score_threshold,\r\n                soft_nms_sigma=soft_nms_sigma)\r\n\r\n            _update_boxes = tf.gather(boxes, selected_indices)\r\n            _num_boxes = tf.shape(_update_boxes)[0]\r\n            _ind_boxes = tf.range(_cls*max_class_output_size, _cls*max_class_output_size+_num_boxes)\r\n\r\n            _boxes = tf.tensor_scatter_nd_update(_boxes, tf.expand_dims(_ind_boxes, axis=-1), _update_boxes)\r\n            _coefs = tf.tensor_scatter_nd_update(_coefs, tf.expand_dims(_ind_boxes, axis=-1), tf.gather(mask_coef, selected_indices))\r\n            _classes = tf.tensor_scatter_nd_update(_classes, tf.expand_dims(_ind_boxes, axis=-1), tf.gather(cls_scores, selected_indices) * 0.0 + tf.cast(_cls, dtype=tf.float32) + 1.0)\r\n            _scores = tf.tensor_scatter_nd_update(_scores, tf.expand_dims(_ind_boxes, axis=-1), tf.gather(cls_scores, selected_indices))\r\n\r\n        _ids = tf.argsort(_scores, direction='DESCENDING')\r\n        scores = tf.gather(_scores, _ids)[:max_output_size]\r\n        boxes = tf.gather(_boxes, _ids)[:max_output_size]\r\n        mask_coef = tf.gather(_coefs, _ids)[:max_output_size]\r\n        classes = tf.gather(_classes, _ids)[:max_output_size]\r\n\r\n        return boxes, mask_coef, classes, scores\r\n```\r\nalso in the following `__call__` function:\r\n```\r\ndef __call__(self, net_outs, trad_nms=True):\r\n        \"\"\"\r\n        Args:\r\n             pred_offset: (tensor) Loc preds from loc layers\r\n                Shape: [batch, num_priors, 4]\r\n            pred_cls: (tensor) Shape: Conf preds from conf layers\r\n                Shape: [batch, num_priors, num_classes]\r\n            pred_mask_coef: (tensor) Mask preds from mask layers\r\n                Shape: [batch, num_priors, mask_dim]\r\n            priors: (tensor) Prior boxes and variances from priorbox layers\r\n                Shape: [num_priors, 4]\r\n            proto_out: (tensor) If using mask_type.lincomb, the prototype masks\r\n                Shape: [batch, mask_h, mask_w, mask_dim]\r\n        \r\n        Returns:\r\n            output of shape (batch_size, top_k, 1 + 1 + 4 + mask_dim)\r\n            These outputs are in the order: class idx, confidence, bbox coords, and mask.\r\n            Note that the outputs are sorted only if cross_class_nms is False\r\n        \"\"\"\r\n\r\n        box_p = net_outs['pred_offset']  # [1, 27429, 4]\r\n        class_p = net_outs['pred_cls']  # [1, 27429, 2]\r\n        coef_p = net_outs['pred_mask_coef']  # [1, 27429, 32]\r\n        anchors = net_outs['priors']  # [27429, 4]\r\n        proto_p = net_outs['proto_out']  # [1, 90, 302, 32]\r\n        \r\n        proto_h = tf.shape(proto_p)[1]\r\n        proto_w = tf.shape(proto_p)[2]\r\n\r\n        box_decode = self._decode(box_p, anchors)  # [1, 27429, 4]\r\n        \r\n        num_class = tf.shape(class_p)[2] - 1\r\n\r\n        # Apply softmax to the prediction class\r\n        class_p = tf.nn.softmax(class_p, axis=-1)\r\n        # exclude the background class\r\n        class_p = class_p[:, :, 1:]\r\n        # get the max score class of 27429 predicted boxes\r\n        class_p_max = tf.reduce_max(class_p, axis=-1)  # [1, 27429]\r\n        batch_size = tf.shape(class_p_max)[0]\r\n\r\n        detection_boxes = tf.zeros((batch_size, self.max_output_size, 4), tf.float32)\r\n        detection_classes = tf.zeros((batch_size, self.max_output_size), tf.float32)\r\n        detection_scores = tf.zeros((batch_size, self.max_output_size), tf.float32)\r\n        detection_masks = tf.zeros((batch_size, self.max_output_size, proto_h, proto_w), tf.float32)\r\n        num_detections = tf.zeros((batch_size), tf.int32)\r\n\r\n        for b in range(batch_size):\r\n            # filter predicted boxes according the class score\r\n            class_thre = tf.boolean_mask(class_p[b], class_p_max[b] > 0.3)\r\n            box_thre = tf.boolean_mask(box_decode[b], class_p_max[b] > 0.3) \r\n            coef_thre = tf.boolean_mask(coef_p[b], class_p_max[b] > 0.3)\r\n\r\n            if tf.size(class_thre) != 0:\r\n                if not trad_nms:\r\n                    box_thre, coef_thre, class_ids, class_thre = _fast_nms(box_thre, coef_thre, class_thre)\r\n                else:\r\n                    box_thre, coef_thre, class_ids, class_thre = self._traditional_nms(box_thre, coef_thre, class_thre)\r\n\r\n                # Padding with zeroes to reach max_output_size\r\n                class_ids = tf.concat([class_ids, tf.zeros(self.max_output_size - tf.shape(box_thre)[0])], 0)\r\n                class_thre = tf.concat([class_thre, tf.zeros(self.max_output_size - tf.shape(box_thre)[0])], 0)\r\n                num_detection = [tf.shape(box_thre)[0]]\r\n                pad_num_detection = self.max_output_size - num_detection[0]\r\n\r\n                _masks_coef = tf.matmul(proto_p[b], tf.transpose(coef_thre))\r\n                _masks_coef = tf.sigmoid(_masks_coef) # [138, 138, NUM_BOX]\r\n\r\n                boxes, masks = self._sanitize(_masks_coef, box_thre)\r\n                masks = tf.transpose(masks, (2,0,1))\r\n                paddings = tf.convert_to_tensor( [[0, pad_num_detection], [0,0], [0, 0]])\r\n                masks = tf.pad(masks, paddings, \"CONSTANT\")\r\n                \r\n                paddings = tf.convert_to_tensor( [[0, pad_num_detection], [0, 0]])\r\n                boxes = tf.pad(boxes, paddings, \"CONSTANT\")\r\n\r\n                detection_boxes = tf.tensor_scatter_nd_update(detection_boxes, [[b]], tf.expand_dims(boxes, 0))\r\n                detection_classes = tf.tensor_scatter_nd_update(detection_classes, [[b]], tf.expand_dims(class_ids, 0))\r\n                detection_scores = tf.tensor_scatter_nd_update(detection_scores, [[b]], tf.expand_dims(class_thre, 0))\r\n                detection_masks = tf.tensor_scatter_nd_update(detection_masks, [[b]], tf.expand_dims(masks, 0))\r\n                num_detections = tf.tensor_scatter_nd_update(num_detections, [[b]], num_detection)\r\n        \r\n        result = {'detection_boxes': detection_boxes,'detection_classes': detection_classes, 'detection_scores': detection_scores, 'detection_masks': detection_masks, 'num_detections': num_detections}\r\n        return result\r\n```\r\n", "comments": ["In the recent TF versions those ops can be supported through the tf select option. https://www.tensorflow.org/lite/guide/ops_select", "I've updated tf version to 2.4.1. Tf conversion is working now, but when I try to use INT8 quantization I'm getting error. Here, is my code:\r\n```\r\nimport tensorflow as tf\r\nimport os\r\nimport cv2\r\n\r\nIMAGE_PATH = '/home/deploy/ved/test/'\r\n\r\ntrain_images = tf.keras.preprocessing.image_dataset_from_directory(\r\n        directory=IMAGE_PATH, labels='inferred', label_mode='int', class_names=None,\r\n        color_mode='rgb', batch_size=1)\r\n\r\ndef representative_dataset():\r\n  for i in range(100):\r\n    image = train_images[i]\r\n    image = tf.io.read_file(image)\r\n    image = tf.io.decode_jpeg(image, channels=3)\r\n    image = tf.image.resize(image, [720, 2410])\r\n    image = tf.cast(image / 255., tf.float64)\r\n    image = tf.expand_dims(image, 0)\r\n    yield [image]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_models/saved_model_0.6849702')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\nconverter.target_spec.supported_ops = [\r\n\t\t\ttf.lite.OpsSet.TFLITE_BUILTINS_INT8,\r\n\t\t\ttf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n\t\t\ttf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n\t\t\t]\r\nconverter.inference_input_type = tf.uint8  # or tf.uint8\r\nconverter.inference_output_type = tf.uint8  # or tf.uint8\r\ntflite_quant_model = converter.convert()\r\n\r\nopen(\"yolact.tflite\", \"wb\").write(tflite_quant_model)\r\n```\r\n\r\nI'm getting following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 58, in __init__\r\n    _calibration_wrapper.CalibrationWrapper(model_content))\r\nTypeError: pybind11::init(): factory function returned nullptr\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"to_tflite.py\", line 44, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 742, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 449, in _calibrate_quantize_model\r\n    calibrate_quantize = _calibrator.Calibrator(result)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 60, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n```", "Could you try the tf-nightly version or tf 2.5 since we have  improved a better support in the quantization tool?\r\n\r\nYou can try a new quantization tool by specifying the flag, `converter.experimental_new_quantizer`. Please try out both the old and new quantizers.", "I tried using `2.6.0-dev20210419` with `converter.experimental_new_converter = True`, but I'm having following error:\r\n```\r\nFlex ops: FlexNotEqual, FlexTensorScatterUpdate\r\nDetails:\r\n\ttf.NotEqual(tensor<i32>, tensor<i32>) -> (tensor<i1>) : {device = \"\", incompatible_shape_error = false}\r\n\ttf.TensorScatterUpdate(tensor<?x300x4xf32>, tensor<?x2xi32>, tensor<?x4xf32>) -> (tensor<?x300x4xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?x300x?x?xf32>, tensor<?x2xi32>, tensor<?x180x604xf32>) -> (tensor<?x300x?x?xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?x300xf32>, tensor<?x2xi32>, tensor<?xf32>) -> (tensor<?x300xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?x4xf32>, tensor<?x1xi32>, tensor<?x4xf32>) -> (tensor<?x4xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?x?xf32>, tensor<?x1xi32>, tensor<?x?xf32>) -> (tensor<?x?xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> (tensor<?xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> (tensor<?xi32>) : {device = \"\"}\r\nTraceback (most recent call last):\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 71, in __init__\r\n    custom_op_registerers_by_func))\r\nValueError: Only models with a single subgraph are supported, model had 7 subgraphs\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"to_tflite.py\", line 45, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 926, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 518, in _calibrate_quantize_model\r\n    custom_op_registerers_by_func)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 73, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: Only models with a single subgraph are supported, model had 7 subgraphs.\r\n\r\n```", "Could you also try out with the `converter.experimental_new_quantizer = False`?\r\n\r\nThanks!", "With `converter.experimental_new_converter = False`, it having following error:\r\n```\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MAX_POOL_2D, PAD, REDUCE_MAX, RELU, RESHAPE, RESIZE_BILINEAR, SOFTMAX, STRIDED_SLICE, TANH. Here is a list of operators for which you will need custom implementations: StatelessWhile.\r\nTraceback (most recent call last):\r\n  File \"/home/deploy/.local/bin/toco_from_protos\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nSome of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, MAX_POOL_2D, PAD, REDUCE_MAX, RELU, RESHAPE, RESIZE_BILINEAR, SOFTMAX, STRIDED_SLICE, TANH. Here is a list of operators for which you will need custom implementations: StatelessWhile.\r\n```\r\n\r\nWith `converter.experimental_new_converter = True` and `converter.experimental_new_quantizer = False` it's having following error:\r\n```\r\n2021-04-19 12:49:10.560641: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1801] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\r\nFlex ops: FlexNotEqual, FlexTensorScatterUpdate\r\nDetails:\r\n\ttf.NotEqual(tensor<i32>, tensor<i32>) -> (tensor<i1>) : {device = \"\", incompatible_shape_error = false}\r\n\ttf.TensorScatterUpdate(tensor<?x300x4xf32>, tensor<?x2xi32>, tensor<?x4xf32>) -> (tensor<?x300x4xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?x300x?x?xf32>, tensor<?x2xi32>, tensor<?x180x604xf32>) -> (tensor<?x300x?x?xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?x300xf32>, tensor<?x2xi32>, tensor<?xf32>) -> (tensor<?x300xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?x4xf32>, tensor<?x1xi32>, tensor<?x4xf32>) -> (tensor<?x4xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?x?xf32>, tensor<?x1xi32>, tensor<?x?xf32>) -> (tensor<?x?xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?xf32>, tensor<?x1xi32>, tensor<?xf32>) -> (tensor<?xf32>) : {device = \"\"}\r\n\ttf.TensorScatterUpdate(tensor<?xi32>, tensor<1x1xi32>, tensor<1xi32>) -> (tensor<?xi32>) : {device = \"\"}\r\nTraceback (most recent call last):\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 71, in __init__\r\n    custom_op_registerers_by_func))\r\nValueError: Only models with a single subgraph are supported, model had 7 subgraphs\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"to_tflite.py\", line 46, in <module>\r\n    tflite_quant_model = converter.convert()\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 926, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/lite.py\", line 518, in _calibrate_quantize_model\r\n    custom_op_registerers_by_func)\r\n  File \"/home/deploy/.local/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 73, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: Only models with a single subgraph are supported, model had 7 subgraphs.\r\n```", "Is it possible to share your saved model to us for debugging purpose?", "[saved_model](https://drive.google.com/file/d/1fAFdQ0vZY2xQUv1mo-Aw4VAvU-EmcMjf/view?usp=sharing).", "I confirmed that TF -> TFL legalization does work correctly. We can consider this issue as a TF MOT issue.", "@liufengdb could you triage this TFMOT issue?", "If there are If or While ops in the graph, the only weights-only quantization will work. We can consider this as a feature request for broader multiple subgraph support in the model optimization toolkit.", "There are `if` and `for` loops in the code. I've shared the code in the question above. I don't know how to replace these for loops. I can use `tf.map_fn` but I guess that won't work with tflite. Any suggestions? I need to do batch NMS.", "In tf Object detection api, the author has legalized the NMS. He uses custom [SSDModule](https://github.com/tensorflow/models/blob/14cc49853d45903875c01b75ceaf7f083828e2ee/research/object_detection/export_tflite_graph_lib_tf2.py#L58). In [_get_postprocess_fn](https://github.com/tensorflow/models/blob/94618de64fac2606082b5b1746f2053d37ae19de/research/object_detection/export_tflite_graph_lib_tf2.py#L136), they are returning empty values. How to use that functionality in my use-case. I see no documentation about it.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 48589, "title": "Conv2d didn't raise exception for invalid input argument.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): bianry\r\n- TensorFlow version (use command below): 2 .4.1\r\n- Python version: 3.7\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nWhen `Conv2D` with `kernel_size`=`2` and padding=`valid` receives an invalid input, it does not raise any exception. Instead it outputs a tensor with zero-dimension. This can lead to future crash for other APIs with 0-dim tensor as input.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfilters, kernel_size, strides, padding = 3, [2, 2], 2, 'valid'\r\ndata = np.random.rand(1, 1, 1, 1)\r\nlayer = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding)\r\nprint(layer(data).shape)\r\n```\r\n\r\nOutputs\r\n```\r\n(1, 0, 0, 3)\r\n```\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nNo exception is raised for invalid input argument.\r\n\r\n**Describe the expected behavior**\r\nExpect `ValueError` to be raised.\r\n", "comments": ["@lugalUrim \r\nWhen you make a model and compile, it gives the expected error.\r\nExample:\r\n```python\r\nmodel = tf.keras.Sequential()\r\nmodel.add(tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding))\r\nmodel.add(tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding))\r\nmodel.compile(loss = 'categorical_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\r\nmodel.fit(np.random.rand(10, 1, 1, 3))\r\n```\r\nError trace:\r\n```\r\nValueError: in user code:\r\n\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\r\n        return step_function(self, iterator)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\r\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\r\n        return self._call_for_each_replica(fn, args, kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\r\n        return fn(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\r\n        outputs = model.train_step(data)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\r\n        y_pred = self(x, training=True)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:389 call\r\n        outputs = layer(inputs, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\r\n        outputs = call_fn(inputs, *args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/convolutional.py:248 call\r\n        outputs = self._convolution_op(inputs, self.kernel)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper\r\n        return target(*args, **kwargs)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:1020 convolution_v2\r\n        name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:1150 convolution_internal\r\n        name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:2604 _conv2d_expanded_batch\r\n        name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py:973 conv2d\r\n        data_format=data_format, dilations=dilations, name=name)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:750 _apply_op_helper\r\n        attrs=attr_protos, op_def=op_def)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:592 _create_op_internal\r\n        compute_device)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:3536 _create_op_internal\r\n        op_def=op_def)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:2016 __init__\r\n        control_input_ops, op_def)\r\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1856 _create_c_op\r\n        raise ValueError(str(e))\r\n\r\n    ValueError: Negative dimension size caused by subtracting 2 from 1 for '{{node sequential_1/conv2d_5/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true](IteratorGetNext, sequential_1/conv2d_5/Conv2D/ReadVariableOp)' with input shapes: [?,1,1,3], [2,2,3,3].\r\n\r\n```\r\n\r\nPlease close this issue if the query is resolved.\r\nThanks", "Thanks @AdityaKane2001 . When I make a model, it raises a `ValueError` indeed.\r\n\r\nAs far as I am concerned, it would be better if the error can also be raised when building a layer for invalid input. \r\n\r\n - Image this case: One breaks the model into multiple blocks to debug, and every single block (consisting of one or more layers) works fine with some input, but when putting them altogether into a model, it gives some error. It is a bit confusing, right.\r\n\r\n - This bug leads to future crash when taking gradient.\r\nFirst, build a layer. (Executes successfully)\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfilters, kernel_size, strides, padding = 3, [2, 2], 2, 'valid'\r\ndata = np.random.rand(1, 1, 1, 1)\r\nlayer = tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, padding=padding)\r\nprint(layer(data).shape)\r\n```\r\nSecond, take the gradient. (**Session crashes**.)\r\n```\r\nwith tf.GradientTape() as tape:\r\n  out = layer(data)\r\n  loss = tf.reduce_sum(out)\r\n  layer_variables = layer.trainable_variables\r\n  grads = tape.gradient(loss, layer_variables)\r\n```", "@lugalUrim \r\nThe thing is, mostly one does not execute a layer like this without putting it into a model, for obvious reasons. But yes, it's a valid thing to expect, and we can perhaps implement it.", "/cc  @nikitamaia Assign/Inprogress", "Requesting to close, solved in #48610 \r\nThanks", "Yes, as a suggestion please use a valid pattern in the PR next time for autolinking https://docs.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48589\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48589\">No</a>\n", "Thank you @AdityaKane2001 @bhack closed this as the PR got merged", "> Yes, as a suggestion please use a valid pattern in the PR next time for autolinking https://docs.github.com/en/github/managing-your-work-on-github/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword\n\nSure, would do"]}, {"number": 48588, "title": "make `bazel build //tensorflow/tools/pip_package:build_pip_package` work on Jetson devices", "body": "Fixed two problems stopping `bazel build //tensorflow/tools/pip_package:build_pip_package` on Jetson devices\r\n1. CUDA 10.2 doesn't work well with absl CORD\r\n2. mlir needs AArch64 codegen\r\n\r\nFixes #48468", "comments": ["The CUDA 10.2 problem is not device specific or aarch64 specific, see https://github.com/tensorflow/tensorflow/issues/48468"]}, {"number": 48587, "title": "TFlite Converter - EfficientDet conversion fails on int quantization (IndexError: _Map_base::at)", "body": "Hello, \r\n\r\nI am trying to convert EfficientDet-D4 model to tflite, with integer quantization with CPU fallback for unsupported operations. I am encountering error same as [this issue](https://github.com/tensorflow/tensorflow/issues/43239). I have tried using different converter settings but none of them work. Interestingly, though, EfficientDet-D0 converts with this type of quantization without any issue. What I've noticed is that all EfficientDet versions from D1-D7 have this error, only D0 converts successfully.\r\n\r\n### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1\r\n\r\n### 2. Code\r\n\r\nI am using slightly modified code from [EfficientDet repo](https://github.com/google/automl/tree/master/efficientdet), file \"inference.py\". I had to enable TF Select Ops in order to convert the model to tflite.\r\n\r\nI am converting the model using guidelines on EfficientDet github repo (model_inspect.py)\r\n\r\ndef representative_dataset():\r\n        files = [f for f in os.listdir('./representative_dataset') if os.path.isfile(os.path.join(\"representative_dataset\", f))]\r\n        for file in files:\r\n           file = \"representative_dataset/\" + file\r\n           img = cv2.imread(file, cv2.IMREAD_COLOR)\r\n           resized = cv2.resize(img, (1024, 1024), interpolation = cv2.INTER_CUBIC)\r\n           rgb_resized = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\r\n           np_resized = np.array([rgb_resized])\r\n           yield [np_resized.astype(np.uint8)]\r\n\r\nif tflite_path:\r\n      height, width = utils.parse_image_size(self.params['image_size'])\r\n      input_name = signitures['image_arrays'].op.name\r\n      input_shapes = {input_name: [None, height, width, 3]}\r\n      converter = tf.lite.TFLiteConverter.from_saved_model(\r\n          output_dir,\r\n          input_arrays=[input_name],\r\n          input_shapes=input_shapes,\r\n          output_arrays=[signitures['prediction'].op.name])\r\n      converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      converter.representative_dataset = representative_dataset\r\n      converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n      tflite_model = converter.convert()\r\n\r\nThis code results in the following error\r\n\r\nTraceback (most recent call last):\r\n  File \"../automl/efficientdet/model_inspect.py\", line 520, in <module>\r\n    app.run(main)\r\n  File \"/home/fapannen/.local/lib/python3.7/site-packages/absl/app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"/home/fapannen/.local/lib/python3.7/site-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"../automl/efficientdet/model_inspect.py\", line 513, in main\r\n    trace_filename=FLAGS.trace_filename)\r\n  File \"../automl/efficientdet/model_inspect.py\", line 462, in run_model\r\n    self.export_saved_model(**config_dict)\r\n  File \"../automl/efficientdet/model_inspect.py\", line 151, in export_saved_model\r\n    driver.export(self.saved_model_dir, self.tflite_path, self.tensorrt)\r\n  File \"/mnt/c/Users/Owner/Desktop/git/Thesis-EfficientDet/automl/efficientdet/inference.py\", line 621, in export\r\n    tflite_model = converter.convert()\r\n  File \"/home/fapannen/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1947, in convert\r\n    return super(TFLiteConverter, self).convert()\r\n  File \"/home/fapannen/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 1313, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/fapannen/.local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 461, in _calibrate_quantize_model\r\n    inference_output_type, allow_float, activations_type)\r\n  File \"/home/fapannen/.local/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 100, in calibrate_and_quantize\r\n    self._calibrator.FeedTensor(sample)\r\n[tflitelog.txt](https://github.com/tensorflow/tensorflow/files/6328939/tflitelog.txt)\r\n\r\n### 5. (optional) Any other info / logs\r\nAttached full traceback log.\r\n\r\nAny help is highly appreciated.\r\nCheers,\r\nTomas\r\n", "comments": ["@teijeong could you triage this issue?", "@Fapannen ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset  or colab link to reproduce the issue reported here.\r\n\r\nThanks!", "Hello, sure.\r\n\r\n1) Clone [EfficientDet repo](https://github.com/google/automl/tree/master/efficientdet)\r\n\r\n2) mkdir <CUSTOM_DIR>\r\n    cd <CUSTOM_DIR>\r\n\r\n3) Download and unpack the model\r\n    Concrete model doesnt matter, d1-d7 all have the issue\r\n    `wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/efficientdet-d4.tar.gz`\r\n\r\n    `tar xf efficientdet-d4.tar.gz  ` \r\n\r\n4) The full conversion code used (`<path_to_efficientdet_repo>/automl/efficientdet/inference.py`, starting at line 617)\r\n```\r\nif tflite_path:\r\n      height, width = utils.parse_image_size(self.params['image_size'])\r\n      input_name = signitures['image_arrays'].op.name\r\n      input_shapes = {input_name: [None, height, width, 3]}\r\n      converter = tf.lite.TFLiteConverter.from_saved_model(\r\n          output_dir,\r\n          input_arrays=[input_name],\r\n          input_shapes=input_shapes,\r\n          output_arrays=[signitures['prediction'].op.name])\r\n      converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\n      converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n      converter.representative_dataset = representative_dataset\r\n      tflite_model = converter.convert()\r\n\r\n      tf.io.gfile.GFile(tflite_path, 'wb').write(tflite_model)\r\n      logging.info('TFLite is saved at %s', tflite_path)\r\n```\r\n\r\n4.1) `representative_dataset` function. In contrary to original post, dummy dataset can be used. I have tested and it **Still produces the error.** Please note, that ussing different efficientDet than D4 will need different numbers in the middle two brackets. Refer to EfficientDet paper or hparams_config.py in github repo\r\n```\r\ndef representative_dataset():\r\n    for _ in range(100):\r\n      data = np.random.rand(1, 1024, 1024, 3)\r\n      yield [data.astype(np.uint8)]\r\n```\r\n\r\n5) try to convert the model \r\n`python3 <path_to_efficientdet_repo>/automl/efficientdet/model_inspect.py --runmode=saved_model --model_name=efficientdet-d4 \\\r\n--ckpt_path=<CUSTOM_DIR>/efficientdet-d4 --saved_model_dir=<CUSTOM_DIR>/efficientdet-d4/saved_model \\\r\n--tflite_path=<CUSTOM_DIR>/efficientdet-d4.tflite`\r\n\r\nCheers,\r\nTomas", "I have tried converting this with tf-nightly ('2.6.0-dev20210418'), which seems to produce an tflite output. Closing the issue for now.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48587\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48587\">No</a>\n"]}, {"number": 48586, "title": "Why do we need to divide the loss in case of multi-dimensional labels?", "body": "I am following TensorFlow's tutorial on [custom training][1].\r\nThere are two things that I do not understand:\r\n\r\nFirst, when defining the losses, the authors point to dividing by the shape of one example, in the case of a multi-dimensional labels:\r\n\r\n\"If `labels` is multi-dimensional, then average the `per_example_loss` across the number of elements in each sample. For example, if the shape of predictions is `(batch_size, H, W, n_classes)` and labels is `(batch_size, H, W)`, you will need to update per_example_loss like: `per_example_loss /= tf.cast(tf.reduce_prod(tf.shape(labels)[1:]), tf.float32)`\"\r\n\r\nWhy do we need to rescale the loss this way? And does this also hold for Cycle-GANs?\r\n\r\nSecondly, a general question: I noticed that several functions use the `with strategy.scope()` annotation. If this object is available globally, then we can define a function on the global level like this\r\n```\r\nwith strategy.scope():\r\n  def example_function():\r\n    #do stuff\r\n```\r\nIf the object is not available globally, we have to structure the code like this:\r\n```\r\ndef train():\r\n  strategy = ...\r\n\r\n  with strategy.scope():\r\n    def sub_function1():\r\n      #do stuff\r\n```\r\n\r\nThis leads to one large function, `train()`, with many short sub functions. I'd like to define the sub functions on the same level as `train()`, without using a global `strategy` object. \r\nIs there a workaround to achieve this?\r\n\r\n\r\n\r\n  [1]: https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function", "comments": ["Hi @Yannik1337, for multi-dimensional labels, you still need to divide the loss by the global batch size later with `tf.reduce_sum(per_example_loss) / GLOBAL_BATCH_SIZE`\r\n```\r\nper_example_loss = loss_function(labels, predictions) # shape: [batch_size, H, W]\r\nper_example_loss /= tf.reduce_prod(labels.shape[1:])\r\nloss = tf.reduce_sum(per_example_loss) / GLOBAL_BATCH_SIZE\r\n```\r\n\r\nwhich is equivalent to if you're using a single replica\r\n```\r\nper_example_loss = loss_function(labels, predictions)\r\nloss = tf.reduce_mean(per_example_loss)\r\n```\r\n\r\nI don't think it's completely obvious from the tutorial that you still need to divide by the global batch size (like in the case where your labels are not multi-dimensional), so i hope this clears things up!\r\n\r\nAlso, you only need to wrap the creation of your model variables (including metrics and optimizers) within `strategy.scope`. You do not need to put your training code within the scope of the strategy. \r\n\r\nGithub is for bugs and performance issues, so I am closing this issue now. Stack Overflow is a better place to go if you have support questions since there's a larger community there to respond. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48586\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48586\">No</a>\n"]}, {"number": 48585, "title": "how use session in function for TF1.14?", "body": "I build a seesion,but I use it in functoin,it will call me that No default TensorFlow session found.\r\nhow I reslove the issue?", "comments": ["@TomatoBoy90 \r\nIt is recommended that you use TensorFlow 2.4.1, which is the latest stable release. Installation guide [here](https://www.tensorflow.org/install?hl=en).", "@TomatoBoy90  \r\nCould you please fill the issue template like OS ,Tensorflow version..etc \r\n\r\nCan you share more details for us to analyse, simple stand alone code to replicate the issue faced or a colab gist with the code and error. Thanks\r\n\r\n\r\n", "@TomatoBoy90 \r\n\r\nPlease confirm if the issue  still persist", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48585\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48585\">No</a>\n"]}, {"number": 48584, "title": "Models having locally connected layers with implementation = 2 or 3 cannot be saved in SavedModel format.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow installed from (source or binary): Colab, so binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7\r\n\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nThis issue is a continuation of #47689 . Whenever the user opts for `implementation=2` or `implementation=3`, the model cannot be saved as a SavedModel format.  \r\n\r\n**Describe the expected behavior**\r\nOne should be able to save it as SavedModel format.\r\n\r\n**Standalone code to reproduce the issue**\r\nPlease see the gist [here](https://colab.research.google.com/gist/AdityaKane2001/c74692d463f14517d84ee9be9443941d/tfpr_maker.ipynb).\r\n\r\n**Other info**\r\nI have played around a bit with the source code in the colab environment. I have inserted many `logging_ops.print_v2(...)` statements to make it easier to debug. Also, I have cloned my fork (it's a clean fork, no personal commits) and ran the `local_test.py` file. I have added `model_2.save('model2')` line, too check for the issue.\r\n\r\n**Some observations:**\r\n1. The `local.py`  file has no errors (none that I found). But many function calls can be updated as they are deprecated.\r\n2. The `K.reshape` or `array_ops.reshape` calls are the ones that are causing the issue. There's some bug in  `tensorflow/python/framework/op_def_library.py  : _apply_op_helper(op_type_name, name=None, **keywords)` function. \r\n3. Note that `local.py: 790` calls `reshape`, which calls `gen_array_ops.reshape`, which calls `op_def_library._apply_op_helper`. But the arguments that go to the latter **were not recorded in `**keywords` argument.**  I checked that by printing the keywords dict in the error message. \r\n\r\nPlease take a look at the error log under local_test.py execution cell in the colab notebook.\r\nThanks. \r\n", "comments": ["> Note that local.py: 790 calls reshape, which calls gen_array_ops.reshape, which calls op_def_library._apply_op_helper. But the arguments that go to the latter were not recorded in **keywords argument. I checked that by printing the keywords dict in the error message.\r\n\r\nIt is that in_dims and out_dims have shape `shape=(3,)` so then `in_size/out_size` are `shape=()`. \r\n\r\n```\r\n  in_size = math_ops.reduce_prod(in_dims)\r\n  out_size = math_ops.reduce_prod(out_dims)\r\n```", "@bhack \r\nCan you please elaborate on that?", "In `local.py` add a print yourself:\r\n\r\n```\r\n  in_size = math_ops.reduce_prod(in_dims)\r\n  out_size = math_ops.reduce_prod(out_dims)\r\n  print(\"== Shape in ==\" + str(in_size.shape))\r\n  print(\"== Shape out ==\" + str(out_size.shape))\r\n  return array_ops.reshape(tensor, (in_size, out_size))\r\n```", "Got it, will look into it\r\n", "P.s. I fixed a typo", "@bhack \r\nI understood that in `make_2d` function we are facing empty arrays. But then why in the test error logs we see that the error is on some other line, and not that one?", "> Note that local.py: 790 calls reshape, which calls gen_array_ops.reshape, which calls op_def_library._apply_op_helper. But the arguments that go to the latter were not recorded in **keywords argument. I checked that by printing the keywords dict in the error message.\r\n\r\nI was just commenting your point 3. Isn't that line?", "Yes, that's the one. \r\nMy question is, how do we pinpoint where the bug is? Where are we calling the `reshape` that has argument `shape` which has `None` in it?", "The problem is that on save the layer `def call(self, inputs):` is called with `inputs.shape` `(None, None, None, 1)` instead of `(None, 28, 28, 1)`. As in `implementation == 2` and `3` we have always have `self.compute_output_shape(inputs.shape)` we will get the error for `(None, None, None, 1)`.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@AdityaKane2001 is this a duplicate of https://github.com/tensorflow/tensorflow/issues/47689 or there is something new?", "@bhack \r\nYes, this is a duplicate, but I had added some observations.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@bhack \r\nFrankly, I don't have any idea how to proceed from here. I had read the related code and tried to understand it, but I couldn't identify what changes were required. How can we proceed from here?", "@bhack , @AdityaKane2001 - Please consider looking at the detailed description in the PR #49230 and provide your feedback.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48584\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48584\">No</a>\n"]}, {"number": 48583, "title": "[Cherry-pick] Support the input/output data type inside the mlir quantizer", "body": "This patch added an MLIR pass to modify the input and output types of the\r\nmodels. This is based on the assumption that the input models have leading\r\ntfl.quantize ops and tailing tfl.dequantize ops. The modification uses the\r\nfollowing rules:\r\n\r\n- Requested float input type: no input change\r\n- Requested int8 input type: remove the tfl.quantize op\r\n- Requested uint8 input type: change the input storage type of the tfl.quantize op from int8 to uint8\r\n- Requested float output type: no input change\r\n- Requested int8 output type: remove the tfl.dequantize op\r\n- Requested uint8 output type: replace the tfl.dequantize op by an tfl.quantize op and change the output storage type uint8\r\n\r\nPiperOrigin-RevId: 368220771\r\nChange-Id: I513da34045f11402f8e09af6004e8b7df2b923b1", "comments": []}, {"number": 48582, "title": "[ROCm] Reduction for complex types, dynamic warp setting", "body": "This PR supplies two related sets of ROCm-related changes:\r\n* Reduction for complex data types\r\n* Dynamic warp size setting (on ROCm, warp size is either 32 vs 64 depending on the GPU and the value is not known at compile time)", "comments": ["@chsigg @cheshire gentle ping", "gentle ping", "@chsigg @cheshire gentle ping", "@ekuznetsov139  Can you please check @cheshire's comments and keep us posted ? Thanks!", "@ekuznetsov139 Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 48581, "title": "[determinism] Update release notes in branch r2.5", "body": "This current pull request updates the release notes for version 2.5 of TensorFlow with respect to deterministic op functionality.\r\n\r\ncc @sanjoy", "comments": ["This should edit the existing PR for the release notes (#48090) so we don't get merge conflicts when we finally merge that and so that the release automation generates the right release notes."]}, {"number": 48580, "title": "Symmetric Fake Quantization", "body": "Resolves model optimization issue [#635, Symmetric quantization returns nonzero value for 0 on GPU](https://github.com/tensorflow/model-optimization/issues/635) by modifying the fake-quantization functors. The new formulation is device-independent and produces on both CPUs and GPUs the same output as the CPU execution of the current implementation.", "comments": ["Would be great to have this PR reviewed soon. Pinging @Xhark and @teijeong since I think this might fix #47193.", "Thanks for the review. The PR includes 6 new unit tests for symmetric fake quantization that fail with the present implementation of the functors.\r\n\r\nThe updated implementation also passes the script in model optimization issue [#635](https://github.com/tensorflow/model-optimization/issues/635):\r\n\r\n```\r\n[10.039216   9.0196085  0.        10.039216 ]   \r\n[10.         8.9763775  0.        10.       ]\r\n[10.         8.9763775  0.        10.       ]\r\n[10.039216   9.0196085  0.        10.039216 ]\r\n[10.         8.9763775  0.        10.       ]\r\n[10.         8.9763775  0.        10.       ]", "@Xhark: Can you provide some insight regarding the failure of the internal checks?", "Hi @philipphack,\r\nThis PR was rollbacked due to it breaks some existing internal test. I'm trying to remerge. (fix the internal test or fix the code on this PR.)\r\nWould you please let me know how did you check the new test is failed with original the present implementation? I can't reproduce....\r\n\r\nOne comment during fixing this issue was: We prefer to use A * (1.0 / scale) because (1.0 / scale) can be cached, and * is much faster than /. (Some compiler optimize option may changes A / scale as A * (1.0 / scale), but it seems not optimized the tensor div op internal code by default.)\r\nI've figured out just shifting the input value (as you did on this PR) of rounding function and use A * inv_scale form might be the best during this journey. WDYT?\r\n\r\n\r\n\r\n\r\n", "@Xhark, thanks for your message. You're right that the present unit tests don't demonstrate the problem with the previous implementation since they run exclusively on the CPU. I've added a set of GPU specific tests which reproduce the issue.\r\n\r\nIt's certainly possible to precompute ` const float inv_nudged_scale = 1.0f / nudged_scale`. I had only removed it from `FakeQuantWithMinMaxArgsFunctor` since it wasn't done in `FakeQuantWithMinMaxVarsFunctor` and `FakeQuantWithMinMaxVarsFunctor` either.\r\n\r\nLet me know how you wish to proceed. Should I push another commit with these changes to the original branch?", "@philipphack \r\n\r\nFor the best case:\r\nIt might be very helpful if you add `inv_nudged_scale` like things for all functions, and it also resolve the original problem, (because this PR shifted input of round function.) and adding GPU tests. \r\n\r\nWould you please update the PR? \r\nOr you can create new PR, I think that might be more clear.\r\n(We can add the link on this thread.)"]}, {"number": 48579, "title": " ai-explanations-image.ipynb - Saved model error ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\nhttps://colab.research.google.com/github/GoogleCloudPlatform/ml-on-gcp/blob/master/tutorials/explanations/ai-explanations-image.ipynb#scrollTo=Yn8hEAHiXaE7\r\n\r\nPerformance issues\r\nError encountered during saved model of iris Dataset Flower - using sample colab tutorial - ai-explanations-image.ipynb\r\n\r\n![Screen Shot 2021-04-16 at 4 47 57 PM](https://user-images.githubusercontent.com/62075076/115094774-97fa8d80-9ed3-11eb-8f60-cc93d26dad98.png)\r\n\r\n", "comments": ["@jocelynbaduria,\r\nAlthough I do not have access to a GCP project, I was able to export the model to the local storage without any issues. \r\n\r\nPlease find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/c69ed74c7b2906ca0436478cddbf4023/48579.ipynb#scrollTo=Yn8hEAHiXaE7&line=1&uniqifier=1). Thanks!", "Hi @amahendrakar \r\nI'll check if it works with my setup in GCP AI Platform. \r\nIf I found any issues again, will provide feedback to you. \r\nThanks..", "@jocelynbaduria \r\nCould you please update if this is still an issue.", "@Saduf2019 , I will update you, just crunching with projects and hw. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48579\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48579\">No</a>\n", "@jocelynbaduria\r\nCan you please confirm."]}, {"number": 48578, "title": "[INTEL MKL] [2.5 cherry pick]  Log warning message for int8 in case of native format", "body": "\r\nThis PR logs a warning message to the Tensorflow customers in the following cases when OneDNN optimization does not \r\nsupport INT8 inference/training:\r\n\r\n   (1) stock Tensorflow, with OneDNN optimizations enabled (in this case, INT8 is not supported at all.\r\n \r\n   (2) Intel Optimized Tensorflow (aka, build with --config=mkl): in this case, the warning message reminds the user\r\n        to set a proper environment variable before running INT8 inference/training. \r\n\r\nPlease help to cherry pick this small PR into RC2 branch. ", "comments": ["@penpornk Thanks for review suggestions. Code change has been committed.", "@penpornk We would like to have this PR cherry-picked into TF 2.5-rc2. Thanks!", "@gzmkl Please track the cherry-pick progress here: https://github.com/tensorflow/tensorflow/pull/48651 :)", "@penpornk  thank you for let me know the cherry pick process. I will keep an eye on the specified item."]}, {"number": 48577, "title": "Android demo GPU: updating dependencies and migration to Android X", "body": "Aligning Android demo GPU with the tutorial:\r\nhttps://www.tensorflow.org/lite/performance/gpu#android_with_android_studio\r\n\r\n- Updating dependencies to the most actual ones\r\n- Migration to AndroidX\r\n\r\nThe app is tested and working as expected.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48577) for more info**.\n\n<!-- need_sender_cla -->", "@xunkai55 could you review this PR?", "@applikationsprogramvara Can you please sign CLA. Thanks!", "@gbaned I've signed the CLA under https://cla.developers.google.com/clas", "FYI, the PR cannot be pulled for the failures, for:\r\n(1) Conflicting files: tensorflow/lite/java/demo/app/build.gradle, tensorflow/lite/java/demo/build.gradle\r\n(2) The bazel BUILD error for [//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo](https://github.com/tensorflow/tensorflow/blob/b8d2ab43ff06d6a67deba79ddaf33d76bb1064a9/tensorflow/lite/java/demo/app/src/main/BUILD#L8)", "@lintian06 \r\n\r\n> (1) Conflicting files: tensorflow/lite/java/demo/app/build.gradle, tensorflow/lite/java/demo/build.gradle\r\n\r\nConflict has been resolved. I took the version from master.\r\n\r\n> (2) The bazel BUILD error for [//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo](https://github.com/tensorflow/tensorflow/blob/b8d2ab43ff06d6a67deba79ddaf33d76bb1064a9/tensorflow/lite/java/demo/app/src/main/BUILD#L8)\r\n\r\nI don't really understand what this bazel BUILD is doing. Probably it should be also somehow tuned. The change I did is working all right when I import demo app in Android studio. The app is also building and running all right on a real device.\r\n\r\nCould you give some advice what bazel BUILD is doing and how build issue can be resolved?", "[Bazel](https://www.bazel.build/) is a build tool similar to make/maven etc. It supports cross-language compilation, and resolves dependencies. TensorFlow uses it to build all projects. The build rule is BUILD file related to the path. In this case, [`tensorflow/lite/java/demo/app/src/main/BUILD`](https://github.com/tensorflow/tensorflow/blob/de9a4335c96bec8fa69abb89618b1daa4b2459fa/tensorflow/lite/java/demo/app/src/main/BUILD#L9).\r\n\r\nYou can install [bazelisk](https://github.com/bazelbuild/bazelisk) that determines the bazel version smartly.\r\n\r\n```\r\nalias bazel='path/to/bazelisk or bazel'\r\n```\r\n\r\n```\r\nbazel build //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo  # to build this demo target.\r\n```", "@lintian06 thank you for the explanation. I used Bazel all right for building TFLite libraries for Android and iOS.\r\n\r\nBut the current usage of building (?) the Android app in Bazel is puzzling me. What is the purpose? To get a complete APK or a Bundle?\r\n\r\nActually if try to build TfLiteCameraDemo using Bazel using the command:\r\n```\r\nbazel build //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo\r\n```\r\n\r\nI get the following error:\r\n`ERROR: /Users/user/tensorflow/tensorflow/lite/java/demo/app/src/main/BUILD:8:15: no such package '@androidsdk//com.android.support': BUILD file not found in directory 'com.android.support' of external repository @androidsdk. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo'`\r\n\r\nThis can be partially understandable, because we changed `com.android.support` package to `androidx.appcompat` in https://github.com/tensorflow/tensorflow/pull/48577/commits/06ae16ee03613e285d666d7c88956780c492c826.\r\n\r\nI have pretty much the same error if I run the same command in the master branch. Which means that I am still missing some build configuration. And yes, I have my ANDROID_SDK and ANDROID_NDK set and run `./confugure` before the build. I don't think that these paths are critical, because Gradle saves `androidx.appcompat` to a separate cache directory.\r\n\r\nI am just wondering how @xunkai55 managed to build it when he made his changes recently: https://github.com/tensorflow/tensorflow/commit/0a1f70f80d1308d388ed9c1d40d246a57104ff74. Can we ask him about it?", "@lintian06  Can you please take a look on above comments from @applikationsprogramvara. Thanks!", "Hi,\r\n\r\nSorry for the late reply. That commit 0a1f70f was mainly about doc change and gradle change, so the bazel part was untouched in that CL.\r\n\r\nTo build master, you need to run `./configure` as you mentioned and correctly configure the Android part. I'd recommend \r\nSDK=23, build_tool_version=28.0.0, NDK=r19c. You might need to install them locally using Android tools.\r\n\r\nTo resolve the build failure, it's a different story, because it seems that \"androidx.compat\" is not yet a dependency of TensorFlow.\r\n\r\n", "@applikationsprogramvara Can you please check @xunkai55's comments and keep us posted ? Thanks!\r\n", "@gbaned I am confused by the comment https://github.com/tensorflow/tensorflow/pull/48577#issuecomment-856185077 , because in the current commit mainly `build.gradle` was changed and Bazel part stayed untouched.\r\n\r\nAnyway the logic behind this PR is totally different: it is not about  building the app in some testing environment, but to let the developers use the latest version of Android framework available and not update it manually after pulling the repository.\r\n\r\nI would suggest to close the PR without merging on the ground of that testing environment does not support the latest Android framework.", "@applikationsprogramvara Shall we close the PR, please confirm. Thank you."]}, {"number": 48576, "title": "Update custom_gradient.py", "body": "Added error raised when executing eagerly .\r\n\r\nFixes https://github.com/tensorflow/tensorflow/issues/29275", "comments": ["@jvishnuvardhan Can you please check @MarkDaoust's comments and keep us posted ? Thanks!", "@jvishnuvardhan  Any update on this PR? Please. Thanks!"]}, {"number": 48575, "title": "tensortflow-lite installation and packaging", "body": "Hello,\r\n\r\nI'm trying to create a standalone artifact of TFLite SDK. Running\r\n\r\n`cmake -DCMAKE_INSTALL_PREFIX=~/tmp/foo ../tensorflow/tensorflow/lite\r\nmake\r\nmake install\r\n`\r\n\r\nThe build completes, but the result of the build isn't installed. How can I assemble a standalone package?", "comments": ["@terryheo could you take a look?", "For C++ API, you should use C++ CMake project to use the libraries.\r\nFor C API, you can use the output directly.\r\nhttps://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library", "@w3sip Could you please let us know if this issue resolved or not? please check https://github.com/tensorflow/tensorflow/issues/48575#issuecomment-841909304", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48575\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48575\">No</a>\n"]}, {"number": 48574, "title": "Update of oneDNN version for mkl_aarch64 build target", "body": "This change noticeably, x1.7 for ResNet50 inference, improves the performance of CNNs on AArch64 (`--config=mkl_aarch64`).", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48574) for more info**.\n\n<!-- need_author_cla -->", "@penpornk please have a look. Also when I commit via GitHub GUI google-cla bot for some reason fails to verify the CLA."]}, {"number": 48573, "title": "Update TF/TFL -> TOSA legalization tests with SSA values associated", "body": "This is per https://github.com/tensorflow/tensorflow/pull/48193 requested.\r\n\r\nUpdating existing tests with SSA values to construct a dependency graph.\r\nThis is crucial to check complicated lowering like gather, softmax.", "comments": ["Added @stellaraccident and @rsuderman to the reviewer list", "Previous comment doesn't tag successfully... let me try again.\r\nAdd @stellaraccident and @rsuderman to reviewer list.", "What I'm running now is:\r\n`bazel test --config=v2 tensorflow/compiler/mlir/tosa/... -j 16`\r\nand this runs fine\r\n\r\nIs there any way I can reproduce the failure on my side?", "> What I'm running now is:\r\n> `bazel test --config=v2 tensorflow/compiler/mlir/tosa/... -j 16`\r\n> and this runs fine\r\n> \r\n> Is there any way I can reproduce the failure on my side?\r\n\r\nI'm not sure why but bazel on the github repo is not reproducing the issue. I patched in some changes and should be able to land.\r\n\r\nFor interest sake, it was related to test_matmul not having the correct checks. A few lines of checks were missing."]}, {"number": 48572, "title": "tf.nn.compute_average_loss documentation/code misaligned", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/api_docs/python/tf/nn/compute_average_loss\r\n\r\n## Description of issue (what needs changing):\r\nThe name of the method itself and the documentation at the URL above implies that the calculation will be an average (mean) reduction.\r\nBut in the source code (https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/ops/nn_impl.py#L391-L446), line 446 calculates a sum reduction.\r\nEither this is a mistake, and the code should be changed to do a mean reduction, or the documentation should be made more clear as to what the method actually does and why.", "comments": ["Is the average not over the `global_batch_size`?", "It does indeed divide the sum by the global batch size.  I will close as user error.  I was passing in the global batch size since I had it available, but the error was that by not naming the parameter, the scalar global batch size was being broadcast as a weight to add to each example, thus producing the summing behavior I was seeing, despite the tensorflow code being correct."]}, {"number": 48571, "title": "No allocator statistics when starting a network", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10, debain 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below):  2.5.0\r\n- Python version: 3.9.2\r\n- CUDA/cuDNN version: 11.3, tried with 10.0, 11.0, 11.1, 11.2 aswell\r\n- GPU model and memory: Rtx 2060 Super 8G\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nIf i try to use tensorflow in any way i could it throws a \r\n`RuntimeError: No allocator statistics` Error \r\nEven if i type \r\n`import tensorflow as tf; \r\ntf.test.is_gpu_available() `\r\nOr if i try to Initialise a Sequential  model \r\n\r\n**Describe the expected behavior**\r\nNormaly it worked fine until today when it just stopped working\r\n\r\n\r\n**Other info / logs** \r\nIf i try to initialise a Sequential() model it gives me the following \r\n` 2021-04-16 21:27:46.833849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n\r\n2021-04-16 21:27:49.167053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-04-16 21:27:49.193965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-04-16 21:27:49.194117: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-16 21:27:49.199998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-16 21:27:49.200139: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-16 21:27:49.203405: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-16 21:27:49.204197: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-16 21:27:49.206899: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll\r\n2021-04-16 21:27:49.208804: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-16 21:27:49.209441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-16 21:27:49.209593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n2021-04-16 21:27:49.214120: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-16 21:27:49.217270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-04-16 21:27:49.217438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-04-16 21:27:49.713688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-16 21:27:49.713844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0\r\n2021-04-16 21:27:49.715934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N\r\n2021-04-16 21:27:49.717181: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:210] Using CUDA malloc Async allocator for GPU.\r\nTraceback (most recent call last):\r\n  File \"f:\\Github\\test\\tensorflow\\word.py\", line 107, in <module>\r\n    model = Sequential()\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 522, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py\", line 114, in __init__\r\n    super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 522, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 318, in __init__\r\n    self._init_batch_counters()\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\", line 522, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 326, in _init_batch_counters\r\n    self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 262, in __call__\r\n    return cls._variable_v2_call(*args, **kwargs)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 244, in _variable_v2_call\r\n    return previous_getter(\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 237, in <lambda>\r\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2662, in default_variable_creator_v2\r\n    return resource_variable_ops.ResourceVariable(\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 264, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1584, in __init__\r\n    self._init_from_args(\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\", line 1727, in _init_from_args\r\n    initial_value = ops.convert_to_tensor(initial_value,\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\profiler\\trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1566, in convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\", line 52, in _default_conversion_function\r\n    return constant_op.constant(value, dtype, name=name)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 264, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 276, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 301, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 97, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py\", line 525, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: No allocator statistics `\r\n\r\nAs well as this if i run \r\n`--> tf.test.is_gpu_available()`\r\n\r\n` >>> import tensorflow as tf\r\n2021-04-16 21:23:14.876381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\nINFO:tensorflow:Enabling eager execution\r\nINFO:tensorflow:Enabling v2 tensorshape\r\nINFO:tensorflow:Enabling resource variables\r\nINFO:tensorflow:Enabling tensor equality\r\nINFO:tensorflow:Enabling control flow v2\r\n--> tf.test.is_gpu_available()\r\nWARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.config.list_physical_devices('GPU')` instead.\r\n2021-04-16 21:23:25.481405: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-16 21:23:25.484960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-04-16 21:23:25.520614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:09:00.0 name: NVIDIA GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 34 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2021-04-16 21:23:25.520759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-16 21:23:25.529842: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-16 21:23:25.529971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-16 21:23:25.534167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-16 21:23:25.535573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-16 21:23:25.540947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll\r\n2021-04-16 21:23:25.544673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-16 21:23:25.545533: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-16 21:23:25.545662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-04-16 21:23:26.045993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-16 21:23:26.046109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0\r\n2021-04-16 21:23:26.047907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N\r\n2021-04-16 21:23:26.048739: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:210] Using CUDA malloc Async allocator for GPU.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 337, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 1600, in is_gpu_available\r\n    for local_device in device_lib.list_local_devices():\r\n  File \"C:\\Users\\Weise\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\device_lib.py\", line 43, in list_local_devices\r\n    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\r\nRuntimeError: No allocator statistics` ", "comments": ["@weiserhase  I tried the same thing colab and i didn't face any errors. Also,  tf.test.is_gpu_available() is deprecated.\r\n\r\n Could you please try with cuda 11.2 and let us know if you are still facing the issue. Thanks!", "I think there were leftover pieces from another cuda install because after mannualy deleting all cuda files \r\nThe Installation of Cuda 11.3 crashed it again but if you use a fresh cuda 11.2 installation it worked constantly.\r\nThank you for the hint @saikumarchalla ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48571\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48571\">No</a>\n"]}, {"number": 48570, "title": "Fails building example projects for ESP micro", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.12.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 2.5.0-rc1\r\n- TensorFlow version:\r\n- Python version:3.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n1. Got tensorflow-master\r\n2. make -f tensorflow/lite/micro/tools/make/Makefile PARSE_THIRD_PARTY=true TARGET=esp generate_projects\r\n3. Results in the following\r\ntensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.\r\ntensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.\r\nTraceback (most recent call last):\r\n  File \"tensorflow/lite/micro/tools/make/generate_keil_project.py\", line 122, in <module>\r\n    parse_args()\r\n  File \"tensorflow/lite/micro/tools/make/generate_keil_project.py\", line 118, in parse_args\r\n    main(unparsed, flags)\r\n  File \"tensorflow/lite/micro/tools/make/generate_keil_project.py\", line 40, in main\r\n    six.ensure_str(flags.executable),\r\nAttributeError: 'module' object has no attribute 'ensure_str'\r\nmake: *** [tensorflow/lite/micro/examples//hello_world/Makefile.inc:36: tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32_default/prj/hello_world_test/keil/keil_project.uvprojx] Error 1\r\n\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @jruk do you still have an issue?\r\n\r\nCan you please share ESP-IDF version?\r\n\r\nI would recommend to use esp-idf `release/v4.2`.\r\n\r\nPlease follow IDF setup instructions. i.e., goto IDF directory,\r\n1.  run  `./install.sh` do install required packages including python and toolchain\r\n2.  In the same directory run `. ./export.sh` to setup the environment.\r\n3. Generate, build, flash and run the example as mentioned in example readme.\r\n", "Hi Vikram,\nThank you for the reply. I have found a branch that works with PlatformIO\nfor ESP32 (somebody took care of building it from the sources).  I was\nable to convert a model to TFLite and then to microformat.  Still very\ndifficult to find complete examples for 2D cases (including model,\ntransformation, microcontroller code).  How to map 2D arrays to the input\ntensor preserving row and column order...  In some cases C++ code is\navailable, but corresponding models are not.   I hope that there will be\nmore complete cases learn from...\n\n-Alex\n\nOn Wed, May 19, 2021 at 7:20 AM Vikram Dattu ***@***.***>\nwrote:\n\n> Hi @jruk <https://github.com/jruk> do you still have an issue?\n>\n> Can you please share ESP-IDF version?\n>\n> I would recommend to use esp-idf release/v4.2.\n>\n> Please follow IDF setup instructions. i.e., goto IDF directory,\n>\n>    1. run ./install.sh do install required packages including python and\n>    toolchain\n>    2. In the same directory run . ./export.sh to setup the environment.\n>    3. Generate, build, flash and run the example as mentioned in example\n>    readme.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/48570#issuecomment-844006161>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABCEKLTDIKBETGUWSHR45LLTOONJVANCNFSM43CCZQ2A>\n> .\n>\n", "Hello again,\r\n\r\nThe examples here are pretty good IMHO: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples \r\n\r\nYou can find exact instructions to generate and build the examples in README under each example: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world#deploy-to-esp32 \r\n\r\nCurrently supported examples for ESP are: hello_world (sign wave generator), micro_speech (YES/NO/UNKNOWN words detector) and person_detection.\r\n\r\nYou can find one and start with it.\r\n\r\n> Still very\r\n> difficult to find complete examples for 2D cases (including model,\r\n> transformation, microcontroller code).  How to map 2D arrays to the input\r\n> tensor preserving row and column order...  In some cases C++ code is\r\n> available, but corresponding models are not.\r\n\r\nThe CPP code, simply takes all the data in an array an interprets it as required. \r\nDo refer person_detection example provided by tflite for the same.\r\n\r\nThere are many resources available on training a model from scratch or re-training and conversion to quantized tflite format. \r\n\r\nI have created an example myself (cats_and_dogs). For your reference: https://github.com/vikramdattu/tensorflow/tree/master/cats_and_dogs \r\n\r\n\r\n", "Hi Vikram, Thanks for the info!   I will look closer at the references you\nhave sent.\nAlex\n\nOn Wed, May 19, 2021 at 8:46 AM Vikram Dattu ***@***.***>\nwrote:\n\n> Hello again,\n>\n> The examples here are pretty good IMHO:\n> https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples\n>\n> You can find exact instructions to generate and build the examples in\n> README under each example:\n> https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world#deploy-to-esp32\n>\n> Currently supported examples for ESP are: hello_world (sign wave\n> generator), micro_speech (YES/NO/UNKNOWN words detector) and\n> person_detection.\n>\n> You can find one and start with it.\n>\n> Still very\n> difficult to find complete examples for 2D cases (including model,\n> transformation, microcontroller code). How to map 2D arrays to the input\n> tensor preserving row and column order... In some cases C++ code is\n> available, but corresponding models are not.\n>\n> The CPP code, simply takes all the data in an array an interprets it as\n> required.\n> Do refer person_detection example provided by tflite for the same.\n>\n> There are many resources available on training a model from scratch or\n> re-training and conversion to quantized tflite format.\n>\n> I have created an example myself (cats_and_dogs). For your reference:\n> https://github.com/vikramdattu/tensorflow/tree/master/cats_and_dogs\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/48570#issuecomment-844070348>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABCEKLWFEXIRZ6RBN6AHAYLTOOXJJANCNFSM43CCZQ2A>\n> .\n>\n", "@good-point-lab Could you please let us know if your issue got resolved or not ? If it is resolved then please feel free to move this issue to close status ? check https://github.com/tensorflow/tensorflow/issues/48570#issuecomment-844070348 Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48570\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48570\">No</a>\n", "I have closed the issue, thank you Vikram\n\nOn Fri, Aug 27, 2021 at 2:26 AM kumariko ***@***.***> wrote:\n\n> @good-point-lab <https://github.com/good-point-lab> Could you please let\n> us know if your issue got resolved or not ? If it is resolved then please\n> feel free to move this issue to close status ? check #48570 (comment)\n> <https://github.com/tensorflow/tensorflow/issues/48570#issuecomment-844070348>\n> Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/48570#issuecomment-906958391>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ABCEKLVEWXCPWORJD7JAKZTT64V3BANCNFSM43CCZQ2A>\n> .\n> Triage notifications on the go with GitHub Mobile for iOS\n> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n> or Android\n> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\n>\n>\n"]}, {"number": 48569, "title": "[Cherrypick:2.5]Copy execution plan in graph partitioner. Also adds code to model_builder_test that fails without this fix.", "body": "PiperOrigin-RevId: 368744429\nChange-Id: I59378037b9e42a20581fa4c2393e2a64bbcb54ef", "comments": []}, {"number": 48568, "title": "Model datatype", "body": "Hi,\r\n\r\nI am trying print the summary of give model. I am more specifically interested in the datatype of the model.\r\n\r\nI have tried the print_model_analysis() but this doesn't print the datatype. I have also tried summarize_graph utility but had no luck.\r\n\r\nIs there something I am missing ? Are there any apis to do so ?\r\n\r\nCheers,\r\nPrabhakar", "comments": ["You can use something like\r\n```\r\nimport tensorflow as tf\r\ninput = tf.keras.Input(shape=(100,), dtype='int32', name='input')\r\nx = tf.keras.layers.Embedding(\r\n    output_dim=512, input_dim=10000, input_length=100)(input)\r\nx = tf.keras.layers.LSTM(32)(x)\r\nx = tf.keras.layers.Dense(64, activation='relu')(x)\r\nx = tf.keras.layers.Dense(64, activation='relu')(x)\r\nx = tf.keras.layers.Dense(64, activation='relu')(x)\r\noutput = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\ndot_img_file = '/tmp/model_1.png'\r\ntf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True, show_dtype=True)\r\n```\r\n\r\n![image](https://user-images.githubusercontent.com/1710528/115073556-c8085780-9ef8-11eb-94d5-b35c97a0c62c.png)\r\n", "Hi @bhack,\r\n\r\nThank you for the response, your suggestion prints the data type of each tensor.\r\n\r\nI have come with below snip code which prints the dtype of model.\r\n\r\ninterpreter = tf.lite.Interpreter(model_path=args.input_path)\r\ninput_type = interpreter.get_input_details()[0]['dtype']\r\nprint('input: ', input_type)\r\noutput_type = interpreter.get_output_details()[0]['dtype']\r\nprint('output: ', output_type)\r\n\r\nCheers,\r\nPrabhakar", "@prabhakarlad ,\r\n\r\nCould you please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status.\r\n\r\nThanks!", "Hi @tilakrayal,\r\n\r\nSimilar to above for tflite is there something similar to get dType for onnx and pb models ?\r\n\r\nCheers,\r\nPrabhakar", "When you have a Keras model you can print model input/output dtype. For onnx you need to ask in an onnx repository.\r\n\r\ne.g from https://stackoverflow.com/questions/60792775/keras-how-to-check-what-is-the-data-type-expected-in-the-input-layer\r\n```python\r\nimport tensorflow as tf \r\nx = tf.keras.layers.Input((2,), dtype=tf.float64)\r\nres = tf.keras.layers.Dense(2, dtype=tf.float64)(x)\r\nmodel = tf.keras.models.Model(x, res)\r\n\r\nprint(model.inputs[0].dtype, model.outputs[0].dtype)\r\n```", "@prabhakarlad ,\r\n\r\n Please confirm if the issue is resolved. if yes, please feel free to move this issue to closed status and for new issues please submit new issue from [this link ](https://github.com/tensorflow/models/issues/new/choose)and fill in the template, so that we can track the issue there. \r\n\r\nThanks!\r\n", "Thank you for the pointers @bhack ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48568\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48568\">No</a>\n"]}, {"number": 48567, "title": "Dilated and causal convolutions on microcontrollers", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.10\r\n- TensorFlow version: 2.4.1\r\n- Python version : 3.8\r\n\r\n**Describe the problem**\r\n\r\nI would like to run inference on a microcontroller above by using a model characterized by Conv1D layers which implement causal convolutions 1D. In particular, in main.cpp I thought I would use something like this:\r\n\r\n`// Pull in only needed operations (should match NN layers).`\r\n`// Template parameter <n> is number of ops to be added. Available ops:`\r\n`//` `tensorflow/lite/micro/kernels/micro_ops.h`\r\n\r\n`static tflite::MicroMutableOpResolver <1> micro_op_resolver;`\r\n`tflite_status = micro_op_resolver.Conv1D(); `\r\n\r\n`if (tflite_status != kTfLiteOk) {`\r\n`error_reporter->Report(\"Could not add Conv1D op\");`\r\n`while(1);`\r\n`}`\r\n\r\nHowever, the Conv1D operation shouldn't be supported by TensorFlow Lite: how can I solve this problem? Have I create a custom operator, in order to implement the op, or there is another way to fix it?\r\nIn order to create the op, I wrote this simple code:\r\n\r\n`import tensorflow as tf`\r\n`tf.config.run_functions_eagerly(True)`\r\n\r\n`input_shape = (1, 7, 1)`\r\n`x = tf.random.normal(input_shape)`\r\n\r\n`@tf.function`\r\n`def convol1d():`\r\n`y=tf.keras.layers.Conv1D(1, 3, input_shape=input_shape[1:], name=\"Conv1D\")(x)`\r\n`return y`\r\n\r\n`data = convol1d()`\r\n`print(\"\\n\\n data is:\", data)`\r\n\r\n`tflite_model_name = 'convol1d'`\r\n`converter= tf.lite.TFLiteConverter.from_concrete_functions([convol1d.get_concrete_function()])`\r\n`converter.allow_custom_ops = True`\r\n`tflite_model = converter.convert()`\r\n`open(tflite_model_name + '.tflite', 'wb').write(tflite_model)`\r\n\r\nIf I run it, it appears the following error:\r\n\r\n`ValueError: tf.function-decorated function tried to create variables on non-first call.`\r\n\r\nInstead, I would expect it:\r\n\r\n`Error: Didn't find custom operator for name 'Conv1D'`\r\n`Registration failed.`\r\n\r\nIf this latter error appeared, I would try to define the functions Prepare and Eval and construct a TfLiteRegistration in a file .cpp, then I would add an AddCustom call to register.cpp, am I right? At this point, if all went well, I would try to use the op. \r\n\r\nThanks in advance.\r\n", "comments": ["Unassigning myself from the micro issue.", "@njeffrie can you take a look?", "I've seen similar issues, and previously our suggested workaround was to create a model using the conv2d operator instead of conv1d. I don't think conv2d in TFLite or TFLM supports causal padding for convolutions, but somebody more familiar with TFLite would know better than I do.\r\n\r\nAs you suggested, it seems like your custom op approach is likely best. As you stated, you will have to create a python version for use in training, along with a TFLite or TFLM implementation registered with the same custom op name.\r\n\r\nI don't fully understand the custom op implementation you showed above (probably due to my own ignorance of the subject), but it seems odd to use the global \"x\" within your custom op declaration, rather than passing in an input tensor.", "Hi @njeffrie, thanks for you reply: I used a global \"x\" only as a test input for the custom operator. Regarding the following error:\r\n\r\n`ValueError: tf.function-decorated function tried to create variables on non-first call.`\r\n\r\nwhat could it depend on in this case?", "I'm not very familiar with this area, but it seems like [this thread](https://github.com/tensorflow/tensorflow/issues/27120) may be relevant.\r\n\r\nHave you tried changing\r\ndef convol1d(): to def convol1d(x): and renaming the global x to test_input or something? It looks like other examples pass the input into the tf-function rather than referencing it from within.", "I had tried with what you suggested; unfortunately it does not work.", "Could I try to implement a custom operator without using @tf.function?", "I have very little experience with tf functions - perhaps @jdduke can assign this to somebody more familiar.", "It should be that I get ValueError because I can't create a Keras layer in tf.function (the layer should create variables inside it), but I'm not sure how I could define the layer inside tf.function at this point.", "I tried with this code:\r\n\r\n`import tensorflow as tf`\r\n\r\n`input_shape = (1, 7, 1)`\r\n\r\n`test_input = tf.random.normal(input_shape)`\r\n`y = None`\r\n\r\n`@tf.function`\r\n`def convol1d():`\r\n`global y    `\r\n    `if y is None:`\r\n     `y=tf.keras.layers.Conv1D(1, 3, input_shape=input_shape[1:],name=\"Conv1D\")(test_input)`\r\n    `return y`\r\n\r\n`tflite_model_name = 'convol1d'`\r\n`converter=tf.lite.TFLiteConverter.from_concrete_functions([convol1d.get_concrete_function()])`\r\n`converter.allow_custom_ops = True`\r\n`tflite_model = converter.convert()`\r\n`open(tflite_model_name + '.tflite', 'wb').write(tflite_model)`\r\n\r\nHowerer, now I get this error:\r\n\r\n`AttributeError: 'Tensor' object has no attribute 'numpy'`\r\n\r\nI thought I should convert test_input (a tensor) to a numpy array, but I'm not sure on it.", "I've added `tf.config.run_functions_eagerly(True)` again and used \r\n\r\n`data = convol1d()`\r\n`print(\"\\n\\n data is:\", data)`\r\n\r\nbefore using TF converter.  I haven't got errors now, but I would expect it:\r\n\r\n`Error: Didn't find custom operator for name 'Conv1D'.\r\nRegistration failed.`\r\n\r\nInstead, it doesn't happen. What could it depend on in this case?\r\nIs it correct to use this snippet of code to implement a custom operator?\r\n", "I know that I can use Conv2D instead of Conv1D, but I'd like to measure performance of my MCU which uses the same Conv1D.", "Hey @Lucy20211, at the moment, we don't have immediate plans to natively implement Conv1D support, and instead plan to rely on the Conv2D lowering. In theory you could implement Conv1D as a custom op, if you wanted to write a dedicated kernel for it, but it's not clear that you'd see a meaningful resource/performance improvement. ", "Ok :) \r\n", "Thanks :)", "Actually, now I'm a bit puzzled as to why you're seeing a Conv1D during conversion at all. TF lowers Conv1D to Conv2D automatically [see implementation here](https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/nn_ops.py#L1867). \r\n\r\nAs for what a custom op would look like, you have to distinguish between tensors and attributes. Attributes will be embedded in the flexbuffer data for that op, and you would reference it as we do in [this custom MFCC op](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/mfcc.cc#L58). It might help if you could share the `.tflite` model that you successfully converted, which includes the Conv1D op?", "I had not thought that Conv1D is replaced by TFLite Conv2D op. It is probably for this reason that the line\r\n\r\n`Error: Didn't find custom operator for name 'Conv1D'. Registration failed.`\r\n\r\ndoesn't appear.\r\n\r\nThen, even though there is a custom Conv1D op, the TF to TFLite conversion always favors the TFLite Conv2D op, since it is a builtin op, is it right?\r\n", "At the moment I'd try with this model: https://www.programmersought.com/article/13674618779/, for which I'd add the snippet of code concerning the converter.\r\n", "Over to @advaitjain for follow-up.", "Similar to https://github.com/tensorflow/tflite-micro/issues/149#issuecomment-886855952, we do not have a direct path to fixing the issue described. Using Conv2D is likely the path of least resistance at the moment (https://github.com/tensorflow/tflite-micro/issues/149#issuecomment-858111382).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48567\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48567\">No</a>\n"]}, {"number": 48566, "title": "Fix all conv layers when filters is 0", "body": "Fix #48470. This PR raises a `ValueError` for all conv layers upon encountering `filters = 0`. @fchollet ", "comments": ["@fchollet can you review this ?", "I guess I run into some pylint errors . Solved them. Please review @bhack "]}, {"number": 48565, "title": "[tf.data] support all test combinations for `PrefetchWithSlackTest`", "body": "This PR addresses point 5 of https://github.com/tensorflow/tensorflow/pull/46761#issuecomment-770059963 by refactoring `PrefetchWithSlackTest` and enabling the test case for all the test combinations.\r\n\r\nTEST LOG\r\n```console\r\nINFO: Build completed successfully, 4070 total actions\r\n//tensorflow/python/data/experimental/kernel_tests:prefetch_with_slack_test PASSED in 2.5s\r\n```\r\n\r\ncc: @jsimsa \r\n", "comments": []}, {"number": 48564, "title": "while creating VGGSEGNET giving following error", "body": "<em>Below is VGGSEGNET code</em>\r\n\r\nThis function call- **model=VGGSegnet(n_classes=1, input_height=224, input_width=224)** is giving same error on following both versions of TF and KERAS and Python\r\n\r\n**System information**\r\n- tf version=2.4.1 & 2.2.0\r\n- keras version=2.4.3 & 2.4.3\r\n- python=3.7 and python 3.8\r\n\r\n**Following is code defining VGGSEGNET:-**\r\n\r\n`def VGGSegnet(n_classes, input_height, input_width, vgg_level=3, pretrained_weights = None):\r\n\r\n    img_input = Input(shape=(input_height, input_width,3 ))\r\n\r\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format='channels_last')(img_input)\r\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format='channels_last')(x)\r\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool1', data_format='channels_last')(x)\r\n    f1 = x\r\n\r\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format='channels_last')(x)\r\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format='channels_last')(x)\r\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format='channels_last')(x)\r\n    f2 = x\r\n\r\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format='channels_last')(x)\r\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format='channels_last')(x)\r\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format='channels_last')(x)\r\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool1', data_format='channels_last')(x)\r\n    f3 = x\r\n\r\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format='channels_last')(x)\r\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format='channels_last')(x)\r\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format='channels_last')(x)\r\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool1', data_format='channels_last')(x)\r\n    f4 = x\r\n\r\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format='channels_last')(x)\r\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format='channels_last')(x)\r\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format='channels_last')(x)\r\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool1', data_format='channels_last')(x)\r\n    f5 = x\r\n\r\n    x = Flatten(name='flatten')(x)\r\n    x = Dense(4096, activation='relu', name='fc1')(x)\r\n    x = Dense(4096, activation='relu', name='fc2')(x)\r\n    x = Dense(1000, activation='softmax', name='predictions')(x)\r\n\r\n    vgg = Model(img_input, x)\r\n    vgg.load_weights(\"image-segmentation-keras-py3-master/Models/vgg16_weights_th_dim_ordering_th_kernels.hdf5\")\r\n\r\n    levels = [f1, f2, f3, f4, f5]\r\n\r\n    o = levels[vgg_level]\r\n\r\n    o = ZeroPadding2D((1,1),data_format='channels_last')(o)\r\n    o = Conv2D(512,(3,3),padding='valid',data_format='channels_last')(o)\r\n    o = BatchNormalization()(o)\r\n\r\n    o = UpSampling2D((2,2),data_format='channels_last')(o)\r\n    o = ZeroPadding2D((1,1),data_format='channels_last')(o)\r\n    o = Conv2D(256,(3,3),padding='valid',data_format='channels_last')(o)\r\n    o = BatchNormalization()(o)\r\n\r\n    o = UpSampling2D((2,2),data_format='channels_last')(o)\r\n    o = ZeroPadding2D((1,1),data_format='channels_last')(o)\r\n    o = Conv2D(128,(3,3),padding='valid',data_format='channels_last')(o)\r\n    o = BatchNormalization()(o)\r\n\r\n    o = UpSampling2D((2, 2), data_format='channels_last')(o)\r\n    o = ZeroPadding2D((1, 1), data_format='channels_last')(o)\r\n    o = Conv2D(64, (3, 3), padding='valid', data_format='channels_last')(o)\r\n    o = BatchNormalization()(o)\r\n    \r\n    o = UpSampling2D((2, 2), data_format='channels_last')(o)\r\n    o = ZeroPadding2D((1, 1), data_format='channels_last')(o)\r\n    o = Conv2D(32, (3, 3), padding='valid', data_format='channels_last')(o)\r\n    o = BatchNormalization()(o)\r\n\r\n    #o = UpSampling2D((2, 2), data_format='channels_last')(o)\r\n    #o = ZeroPadding2D((1, 1), data_format='channels_last')(o)\r\n    o = Conv2D(n_classes,(3,3),padding='same',data_format='channels_last')(o)\r\n    #o = BatchNormalization()(o)\r\n    o_shape = Model(img_input,o).output_shape\r\n    #outputHeight = o_shape[2]\r\n    #outputWidth = o_shape[3]\r\n    outputHeight = o_shape[2]\r\n    outputWidth = o_shape[1]\r\n\r\n\r\n    #o = (Reshape((outputHeight*outputWidth, -1)))(o)\r\n    #o = (Permute((1,2)))(o)\r\n    o = (Activation('sigmoid'))(o)\r\n    model = Model(img_input,o)\r\n    model.outputWidth = outputWidth\r\n    model.outputHeight = outputHeight\r\n    if(pretrained_weights):\r\n        model.load_weights(pretrained_weights)\r\n\r\n    return  model`\r\n\r\n**Error is as below**\r\n\r\n`Traceback (most recent call last):\r\n  File \"/scratch/pkasar.dbatu/training/VGGSEGNET_224_224_working_on_20_03_21_on_augmented_images_of_size_256_by_256.py\", line 248, in <module>\r\n    model=VGGSegnet(n_classes=1, input_height=224, input_width=224)\r\n  File \"/scratch/pkasar.dbatu/training/VGGSEGNET_224_224_working_on_20_03_21_on_augmented_images_of_size_256_by_256.py\", line 56, in VGGSegnet\r\n    vgg.load_weights(\"image-segmentation-keras-py3-master/Models/vgg16_weights_th_dim_ordering_th_kernels.h5\")\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 2234, in load_weights\r\n    hdf5_format.load_weights_from_hdf5_group(f, self.layers)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\", line 710, in load_weights_from_hdf5_group\r\n    K.batch_set_value(weight_value_tuples)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\", line 3706, in batch_set_value\r\n    x.assign(np.asarray(value, dtype=dtype(x)))\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values.py\", line 781, in assign\r\n    return values_util.on_write_assign(\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values_util.py\", line 140, in on_write_assign\r\n    return var._update(  # pylint: disable=protected-access\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values.py\", line 940, in _update\r\n    return self._update_cross_replica(update_fn, value, **kwargs)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values.py\", line 893, in _update_cross_replica\r\n    return self.distribute_strategy.extended.update(\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2494, in update\r\n    return self._update(var, fn, args, kwargs, group)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 710, in _update\r\n    fn(v, *distribute_utils.select_replica_mirrored(i, args),\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 572, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/distribute/values_util.py\", line 139, in <lambda>\r\n    assign_fn = lambda var, *a, **kw: var.assign(*a, **kw)\r\n  File \"/home/pkasar.dbatu/.conda/envs/dl_new/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 888, in assign\r\n    raise ValueError(\r\nValueError: Cannot assign to variable block1_conv1/kernel:0 due to variable shape (3, 3, 3, 64) and value shape (3, 3, 64, 3) are incompatible`\r\n\r\nI am doing segmentation task using iou as performance metric.\r\nThe link for vgg16_weights_th_dim_ordering_th_kernels.h5 is this [https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5](url)\r\nHelp me out. Thank you in advance", "comments": ["@pankajkasar \r\n\r\nThis issue is more suitable for TensorFlow Models repo. Could you Please post it on Tensorflow Models repo from [here.](https://github.com/tensorflow/models/issues) Thanks!\r\n\r\n\r\n\r\n", "@UsharaniPagadala \r\nOK done I have posted on Tensorflow Models. Shall I close or shall I expect some more help from your side?", "@pankajkasar \r\nYes you can feel free to close the issue.Thanks\r\n", "OK done", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48564\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48564\">No</a>\n"]}, {"number": 48563, "title": "Could not load dynamic library amazon ec2", "body": "\r\n**System information**\r\n- OS Platform and Distribution (aws Linux 2):\r\n\r\n- TensorFlow installed from (source or binary): via pip\r\n- TensorFlow version: 2.4.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: virtual env, pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.0\r\n- GPU model and memory: Tesla t4 (g4dn.xlarge)\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI installed GPU drivers on fresh EC2 instance using commands mentioned [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html#nvidia-GRID-driver)\r\nBut it is not picking gpu. \r\n\r\n2021-04-16 13:19:22.605167: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not  creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-04-16 13:19:22.622026: I tensorflow/stream_executor/platform/default/dso_lo  ader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-04-16 13:19:23.590887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.   cc:941] successful NUMA node read from SysFS had negative value (-1), but there                         must be at least one NUMA node, so returning NUMA node zero\r\n2021-04-16 13:19:23.591452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\r\npciBusID: 0000:00:1e.0 name: Tesla T4 computeCapability: 7.5\r\ncoreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\r\n2021-04-16 13:19:23.591551: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-04-16 13:19:23.591612: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\r\n2021-04-16 13:19:23.591657: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\r\n2021-04-16 13:19:23.591698: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\r\n2021-04-16 13:19:23.591745: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\r\n2021-04-16 13:19:23.591798: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\r\n2021-04-16 13:19:23.591838: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\r\n2021-04-16 13:19:23.591883: W tensorflow/stream_executor/platform/default/dso_lo                        ader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.s.8: cannot open shared object file: No such file or directory\r\n2021-04-16 13:19:23.591894: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1                        757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the                         required libraries for your platform.", "comments": ["@talhaanwarch \r\nCould you please refer to [this link and follow it]( https://www.tensorflow.org/install/gpu)", "the os is amazon linux 2, instead of ubunto ", "@talhaanwarch \r\nplease follow the link shared, it works for all linux versions.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48563\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/48563\">No</a>\n"]}]