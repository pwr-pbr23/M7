[{"number": 55624, "title": "Build TFLite in c++ using cmake for GPU", "body": "Hello! \r\nI am struggling with building my project files in c++ using cmake. My setup is folloging:\r\n\r\n- tensorflow lite installed using bazel (created a .so file)\r\n- CMakeLists as below\r\n- building using \"make\"-commad in a build folder\r\n\r\n````\r\ncmake_minimum_required(VERSION 3.17)\r\n\r\nproject(TFLite)\r\n\r\nset(CMAKE_CXX_STANDARD 14)\r\n\r\nfind_package(OpenCV REQUIRED)\r\ninclude_directories(${OpenCV_INCLUDE_DIRS})\r\n\r\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR}/../tflite-dist/include/)\r\n\r\nset(TIME TFLiteTimeit)\r\nset(TIMEGPU TFLiteTimeitGPU)\r\n\r\nadd_executable(${TIME} tflite_timeit.cpp)\r\nadd_executable(${TIMEGPU} tflite_timeit_gpu.cpp)\r\n\r\nadd_library(tensorflowlite SHARED IMPORTED)\r\nset_property(TARGET tensorflowlite PROPERTY IMPORTED_LOCATION ${CMAKE_CURRENT_SOURCE_DIR}/../tflite-dist/libs/linux_x64/libtensorflowlite.so)\r\n\r\ntarget_link_libraries(${TIME} tensorflowlite ${OpenCV_LIBS})\r\ntarget_link_libraries(${TIMEGPU} tensorflowlite ${OpenCV_LIBS})\r\n````\r\n\r\nTFLiteTimeit works but not TFLiteTimeitGPU since it also includes following in the c++ code:\r\n````\r\nTfLiteGpuDelegateOptionsV2 gpu_options = TfLiteGpuDelegateOptionsV2Default();\r\n\r\n  auto* delegate = TfLiteGpuDelegateV2Create(&gpu_options);\r\n  if (interpreter->ModifyGraphWithDelegate(delegate) != kTfLiteOk) {\r\n    std::cout << \"Fail\" << std::endl;\r\n    return -1;\r\n  }\r\n````\r\nError i get when i try to build the file:\r\n````\r\n/usr/bin/ld: CMakeFiles/TFLiteMemoryGPY.dir/tflite_mem_gpu.cpp.o: in function `main':\r\ntflite_mem_gpu.cpp:(.text+0x31a): undefined reference to `TfLiteGpuDelegateOptionsV2Default'\r\n/usr/bin/ld: tflite_mem_gpu.cpp:(.text+0x329): undefined reference to `TfLiteGpuDelegateV2Create'\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/TFLiteMemoryGPY.dir/build.make:146: TFLiteMemoryGPY] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:93: CMakeFiles/TFLiteMemoryGPY.dir/all] Error 2\r\nmake: *** [Makefile:91: all] Error 2\r\n````\r\n\r\nMy question is: what do I need to add to be able to run the code with GPU-delegates? (if I want to have the same setup)", "comments": ["Hi @Sara980710 ! Sorry for the late response! Did you use TFLITE_ENABLE_GPU = ON flag before building your project as mentioned in this [document](https://www.tensorflow.org/lite/guide/build_cmake#opencl_gpu_delegate)?\r\n\r\n`cmake ../tensorflow_src/tensorflow/lite -DTFLITE_ENABLE_GPU=ON`", "Hello @mohantym!\r\nI have now tried to compile with adding \r\n```` \r\noption(TFLITE_ENABLE_GPU \"enable GPU\" ON)\r\n```` \r\nto my CMakeLists.txt and it still gives the error:\r\n```` \r\n[ 33%] Linking CXX executable TFLiteTimeitGPU\r\n/usr/bin/ld: CMakeFiles/TFLiteTimeitGPU.dir/tflite_timeit_gpu.cpp.o: in function `main':\r\ntflite_timeit_gpu.cpp:(.text+0x2d1): undefined reference to `TfLiteGpuDelegateOptionsV2Default'\r\n/usr/bin/ld: tflite_timeit_gpu.cpp:(.text+0x2e0): undefined reference to `TfLiteGpuDelegateV2Create'\r\n/usr/bin/ld: tflite_timeit_gpu.cpp:(.text+0x6be): undefined reference to `TfLiteGpuDelegateV2Delete'\r\ncollect2: error: ld returned 1 exit status\r\nmake[2]: *** [CMakeFiles/TFLiteTimeitGPU.dir/build.make:146: TFLiteTimeitGPU] Error 1\r\nmake[1]: *** [CMakeFiles/Makefile2:117: CMakeFiles/TFLiteTimeitGPU.dir/all] Error 2\r\nmake: *** [Makefile:91: all] Error 2\r\n```` ", "Hi @gadagashwini ! Could you please look at this issue?"]}, {"number": 55623, "title": "fit() with generator and multiprocessing leaks sockets/files", "body": "**System information**\r\n- Have I written custom code - yes\r\n- OS Platform and Distribution - Linux CentOS7\r\n- TensorFlow installed from binary\r\n- TensorFlow version 2.5.0\r\n- Python version: 3.8\r\n- \r\n**Describe the current behavior**\r\n\r\nIf I run model.fit() with a generator in a loop, for a long time, I eventually get `OSError: [Errno 24] Too many open files`.\r\n\r\n**Describe the expected behavior**\r\n\r\nRun stably forever.\r\n\r\nThe actual code is on an offline machine, but this is the basic idea:\r\n\r\n```\r\nwhile True:\r\n   model.fit(mySeqGen(xx), ..., use_multiprocessing=True, workers=8)\r\n   <stuff>\r\n   model.save_weights(fname)\r\n   gc.collect()\r\n```\r\nThis will run for a long time, but eventually it will generate the error every iteration.  The stack trace is all about \"python3.8/multiprocessing/...\" so I'm sure it's not related to actual files, but rather sockets or whatever the multiprocessor generates.  I thought the `gc.collect()` would solve the problem but does not appear to do so, or it can't keep up.  Each loop iteration takes about a minute as I have it configured now.  Note also that the seqGen is a wrapper around a simulator and does not go to the file system.", "comments": ["Hello @Mastiff37 ,\r\nIn order to expedite the trouble-shooting process, could you please provide a complete code and the dependencies you are using.Thanks!\r\n", "Also i request  that there is a high possibility that this might fixed with later TF versions. Perhaps you can use latest tf stable version2.8 for your case. Thanks!\r\n", "The actual code is on an offline machine, but I can try to gin up a toy example...", "@Mastiff37 ,\r\n Please provide the update?Thanks!"]}, {"number": 55622, "title": "tensorflow 2.8 distributed training for neural network model (designed by subclass) cannot improve the training runtime on multicore CPUs", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no (the code is too long)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu (18.04.6)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.8.0\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to train a neural network model built by tf2.8 and keras 2.8.\r\nI would like to train the model with distributed training on multiple CPUs (e.g. EC2 m4.4 with 16 cpu cores) to improve the training performance.\r\nI am using [MirroredStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy).\r\nI am following the example at https://www.tensorflow.org/tutorials/distribute/keras\r\nand https://www.tensorflow.org/tutorials/distribute/custom_training\r\nMy neural network model is built by subclass (not functional API) of TF2.8 and keras.\r\n\r\nBut, the training run time  (50 mins per epoch) is not improved significantly compared with non-distributed training (65 mins per epoch) on EC2 m4.4 (16 cpu cores). During the training process, most cores are idle or have very low utilizations (< 5%).\r\nI also got a warning at the end of each epoch:     \r\n\r\n**WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.**\r\n\r\n But, in my model, there are no \"call_for_each_replica\", \"experimental_run\" and \"run\". \r\nThere are only \r\n\r\n     with mirroredStrategy.scope():\r\n         class model(keras.model):\r\n                 def __init__(self):\r\n                      self.subclass_model1 = model1()\r\n                      self.subclass_model2 = model2() \r\n\r\n                 def  call(self, input, training=True):\r\n                      # call self.subclass_model1  and self.subclass_model2 to run the whole model\r\n      with mirroredStrategy.scope():\r\n          model.compile()\r\n          model.fit()\r\n\r\nPlease let me know how to improve the training runtime ? \r\n\r\n**Describe the expected behavior**\r\n The distributed training runtime should be improved greatly compared with non-distributed training. \r\n\r\n", "comments": ["Hello @umusa ,\r\nYou have provided multiple links for the code.Can you please confirm the reproducible code link or provide the complete here to reproduce the issue.Thanks!", "@tilakrayal , please use  https://www.tensorflow.org/tutorials/distribute/keras, my code is based on this example. "]}, {"number": 55621, "title": "Build issue r2.6 with Cuda Compatibility 3.0", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6.3\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): MSVC2019\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: Nvidia Quadro K1100M 2GB\r\n\r\n**Describe the problem**\r\n\r\nHi,\r\nI'm trying to build Tensorflow 2.6.3 from source on Windows, to support Cuda Compatibility 3.0.`\r\nI follow this procedure [53062](https://github.com/tensorflow/tensorflow/issues/53062), but after few tries i continue to get this error during the compiling:\r\n\r\n> Starting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=146\r\nINFO: Reading rc options for 'build' from c:\\users\\priva\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/priva/AppData/Local/Programs/Python/Python37/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\priva\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from c:\\users\\priva\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/priva/AppData/Local/Programs/Python/Python37/python.exe --action_env PYTHON_LIB_PATH=C:/Users/priva/AppData/Local/Programs/Python/Python37/lib/site-packages --python_path=C:/Users/priva/AppData/Local/Programs/Python/Python37/python.exe --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.0 --config=cuda --define=with_xla_support=false --action_env TF_ENABLE_XLA=0 --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctionsINFO: Found applicable config definition build:short_logs in file c:\\users\\priva\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\priva\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\priva\\tensorflow\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:windows in file c:\\users\\priva\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=falseINFO: Found applicable config definition build:monolithic in file c:\\users\\priva\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: C:/users/priva/_bazel_priva/5lyc5377/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/users/priva/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/users/priva/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  C:/users/priva/_bazel_priva/5lyc5377/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/users/priva/_bazel_priva/5lyc5377/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (441 packages loaded, 32225 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/priva/tensorflow/tensorflow/core/kernels/BUILD:1222:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): python.exe failed: error executing command\r\n  cd C:/users/priva/_bazel_priva/5lyc5377/execroot/org_tensorflow\r\n  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1\r\n    SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\lib\\x64;C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.19041.0\\um\\x64\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.8 Tools\\x64\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\WINDOWS\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/priva/AppData/Local/Programs/Python/Python37/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/priva/AppData/Local/Programs/Python/Python37/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\priva\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0\r\n    SET TF_ENABLE_XLA=0\r\n    SET TMP=C:\\Users\\priva\\AppData\\Local\\Temp\r\n  C:/Users/priva/AppData/Local/Programs/Python/Python37/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive /Iexternal/mkl_dnn_v1 /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1 /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cublas_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cufft_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/curand_headers_virtual /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/local_config_cuda/cuda/cublas/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cublas/include /Iexternal/local_config_cuda/cuda/cufft/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cufft/include /Iexternal/local_config_cuda/cuda/curand/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/curand/include /Iexternal/mkl_dnn_v1/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/include /Iexternal/mkl_dnn_v1/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_dnn_v1/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/common /Iexternal/mkl_dnn_v1/src/common/ittnotify /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/common/ittnotify /Iexternal/mkl_dnn_v1/src/cpu /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu /Iexternal/mkl_dnn_v1/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/gemm /Iexternal/mkl_dnn_v1/src/cpu/x64/xbyak /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DCURL_STATICLIB /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /std:c++14 -x cuda -DGOOGLE_CUDA=1 -Xcuda-fatbinary=--compress-all --no-cuda-include-ptx=all --cuda-include-ptx=sm_30 --cuda-gpu-arch=sm_30 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DINTEL_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY -nvcc_options=relaxed-constexpr -nvcc_options=ftz=true /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/inplace_ops_gpu/inplace_ops_functor_gpu.cu.obj /c tensorflow/core/kernels/inplace_ops_functor_gpu.cu.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : warning della riga di comando D9035 : l'opzione 'experimental:preprocessor' \u00e8 deprecata e verr\u00e0 rimossa in una futura versione\r\ncl : warning della riga di comando D9036 : utilizzare 'Zc:preprocessor' invece di 'experimental:preprocessor'\r\ncl : warning della riga di comando D9002 : l'opzione sconosciuta '--no-cuda-include-ptx=all' verr\u00e0 ignorata\r\ncl : warning della riga di comando D9002 : l'opzione sconosciuta '--cuda-include-ptx=sm_30' verr\u00e0 ignorata\r\ncl : warning della riga di comando D9002 : l'opzione sconosciuta '--cuda-gpu-arch=sm_30' verr\u00e0 ignorata\r\nC:\\users\\priva\\_bazel_priva\\5lyc5377\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(245): warning: invalid friend declaration\r\nC:\\users\\priva\\_bazel_priva\\5lyc5377\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(709): warning: invalid friend declaration\r\nexternal/com_google_absl\\absl/strings/string_view.h(337): warning: expression has no effect\r\nexternal/com_google_absl\\absl/strings/string_view.h(347): warning: expression has no effect\r\nexternal/com_google_absl\\absl/strings/string_view.h(529): warning: expression has no effect\r\nexternal/com_google_protobuf/src\\google/protobuf/map.h(1028): warning: invalid friend declaration\r\nexternal/com_google_absl\\absl/types/internal/span.h(38): error: incomplete type is not allowed\r\n          detected during instantiation of \"absl::lts_20210324::span_internal::GetDataImpl\" based on template argument <tensorflow::TensorShapeProto>\r\n.\\tensorflow/core/framework/tensor_shape.h(376): here\r\n.\\tensorflow/core/lib/gtl/flatmap.h(157): warning: invalid friend declaration\r\n.\\tensorflow/core/platform/file_system.h(574): warning: overloaded virtual function \"tensorflow::FileSystem::FilesExist\" is only partially overridden in class \"tensorflow::WrappedFileSystem\"\r\n.\\tensorflow/core/platform/file_system.h(574): warning: overloaded virtual function \"tensorflow::FileSystem::CreateDir\" is only partially overridden in class \"tensorflow::WrappedFileSystem\"\r\n.\\tensorflow/core/platform/env.h(495): warning: overloaded virtual function \"tensorflow::Env::RegisterFileSystem\" is only partially overridden in class \"tensorflow::EnvWrapper\"\r\nexternal/com_google_absl\\absl/types/optional.h(428): warning: expression has no effect\r\n          detected during instantiation of \"const T &absl::lts_20210324::optional<T>::operator*() const & [with T=stream_executor::dnn::AlgorithmDesc]\"\r\n.\\tensorflow/stream_executor/dnn.h(817): here\r\nexternal/com_google_absl\\absl/types/optional.h(428): warning: expression has no effect\r\n          detected during instantiation of \"const T &absl::lts_20210324::optional<T>::operator*() const & [with T=size_t]\"\r\n.\\tensorflow/stream_executor/dnn.h(876): here\r\n1 error detected in the compilation of \"C:/Users/priva/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmpcg3nm8rj/inplace_ops_functor_gpu.cu.cpp1.ii\".\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1302.308s, Critical Path: 224.15s\r\nINFO: 4180 processes: 2005 internal, 2175 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nIs this a problem related with the MSVC compiler or something else?\r\nThanks for the support.", "comments": ["@OrazioLombardi Could you please try to upgrade to latest TF v2.8.0 and refer this [tested build configurations](https://www.tensorflow.org/install/source_windows#tested_build_configurations) , please let us know if it helps?Thanks!", "@sushreebarsa the problem is that cuda 11.2 doesn't support compute capability 3.0.\r\nI can tra to upgrade to 2.8 but I cannot use a tested build configuration as suggest.\r\nI also try the tested build configuration of TF v2.4 but the result it's quite the same.", "@OrazioLombardi Thank you for the update!\r\n From the error log it seems to be an issue with MSVC2019 , could you please  make sure that you are using the right GCC version as mentioned in the tested build configuration ?\r\nThanks!\r\n", "Hi @sushreebarsa. I have GCC installed by Cygwin.\r\nToday I update GCC to the latest version and try to build tensorflow but nothing seems to change in the output.\r\nBy the way, in the tested build configuration for Windows, GCC is not metioned (GCC is mentioned only in Linux guide), are you sure that this could influence the building process?\r\nHow can I check that the right version of GCC is installed on my computer and that works correctly with MSVC2019?", "@OrazioLombardi,\r\nHi, Issue with CUDA compatibility. Tensorflow 2.6 doesn\u2019t support Cuda compatibility 3. \r\n\r\nYou need to add the compatibility factor.\r\n`ComputeCapabilityFromString(\"3.0\"), ComputeCapabilityFromString(\"3.5\"), ComputeCapabilityFromString(\"5.2\")};`\r\nDo changes in this [file](https://github.com/tensorflow/tensorflow/blob/r2.6/tensorflow/core/common_runtime/gpu/gpu_device.cc#L1717). Thanks!  ", "@gadagashwini\r\nHi, thank you for your answer.\r\nI already edit the `gpu_device.cc` file adding the `ComputeCapabilityFromString(\"3.0\")` statement as already mentioned in my issue description.\r\nI think that the problem is related to MSVC that has problem to compile some of the files.", "@OrazioLombardi,\r\n```\r\nERROR: C:/users/priva/tensorflow/tensorflow/core/kernels/BUILD:1222:18: C++ compilation of rule '//tensorflow/core/kernels:inplace_ops_gpu' failed (Exit 1): python.exe failed: error executing command\r\ncd C:/users/priva/_bazel_priva/5lyc5377/execroot/org_tensorflow\r\n```\r\nLooks issue with MSVC2019.\r\n\r\nMake sure you followed steps mentioned [here](https://www.tensorflow.org/install/source_windows) and install required packages. Thanks ! "]}, {"number": 55620, "title": "No module named 'tensorflow.contrib.framework.ops'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: win 10 \r\n- TensorFlow installed from (source or binary): I don't remember....\r\n- TensorFlow version: 1.15.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.0/7.4\r\n- GPU model and memory: MX330\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen i run the train_ssd_network.py in ssd training program there was an error:\r\n=======================================================\r\n\r\n from tensorflow.contrib.framework.ops import variables as contrib_variables\r\n\r\nModuleNotFoundError: No module named 'tensorflow.contrib.framework.ops'\r\n\r\n=====================================================\r\n\r\n\r\nThe codes which caused problem are in metrics.py and these are part of the codes:\r\n-----------------------------------------------------------------------------------------\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n_**from tensorflow.contrib.framework.ops import variables as contrib_variables**_\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.ops import nn\r\nfrom tensorflow.python.ops import state_ops\r\nfrom tensorflow.python.ops import variable_scope\r\nfrom tensorflow.python.ops import variables\r\n\r\nfrom tf_extended import math as tfe_math\r\n----------------------------------------------------------------------------------------------\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\n\r\n", "comments": ["Hello @0smilingpig,\r\n\r\ntensorflow.contrib is being removed in version 2.x,I request you to please refer this [document](https://www.tensorflow.org/guide/migrate) to migrate from `TensorFlow 1.x to TensorFlow 2. `\r\n\r\nAlso I request you to please try to update the latest stable version `2.8` for [here](https://www.tensorflow.org/install/pip).There is a high possibility that any bug fixes with later TF versions.Thanks!", "Thank you for your comment, but I said I am using tensorflow version 1.14.0 which doesn't remove tensorflow.contrib and now the error occurred. I wonder why there is no module named tensorflow.contrib.framework.ops in 1.14.0 if tensorflow.contrib is still exist on that version.\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba: ***@***.***&gt;; \r\n\u53d1\u9001\u65f6\u95f4: 2022\u5e744\u670814\u65e5(\u661f\u671f\u56db) \u665a\u4e0a10:30\r\n\u6536\u4ef6\u4eba: ***@***.***&gt;; \r\n\u6284\u9001: ***@***.***&gt;; ***@***.***&gt;; \r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] No module named &#39;tensorflow.contrib.framework.ops&#39; (Issue #55620)\r\n\r\n\r\n\r\n\r\n\r\n \r\nHello @0smilingpig,\r\n \r\ntensorflow.contrib is being removed in version 2.x,I request you to please refer this document to migrate from TensorFlow 1.x to TensorFlow 2. \r\n \r\nAlso I request you to please try to update the latest stable version 2.8 for here.There is a high possibility that any bug fixes with later TF versions.Thanks!\r\n \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;", "@0smilingpig ,\r\nWe see that you are using tf version 1.14, where 1.x is not actively supported.Requesting you, please update to latest v2.x and let us know if you are facing same issue.Thanks!", "Well, I have tried it on 2.8.0 and 1.15.0 before but they didn't work, then I downgrade to 1.14.0 the problem was still there.&nbsp;\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba: ***@***.***&gt;; \r\n\u53d1\u9001\u65f6\u95f4: 2022\u5e744\u670818\u65e5(\u661f\u671f\u4e00) \u665a\u4e0a6:41\r\n\u6536\u4ef6\u4eba: ***@***.***&gt;; \r\n\u6284\u9001: ***@***.***&gt;; ***@***.***&gt;; \r\n\u4e3b\u9898: Re: [tensorflow/tensorflow] No module named &#39;tensorflow.contrib.framework.ops&#39; (Issue #55620)\r\n\r\n\r\n\r\n\r\n\r\n \r\n@0smilingpig ,\r\n We see that you are using tf version 1.14, where 1.x is not actively supported.Requesting you, please update to latest v2.x and let us know if you are facing same issue.Thanks!\r\n \r\n\u2014\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***&gt;", "@0smilingpig Contrib module exists in tensorflow 1.14. Here is my [github gist](https://colab.research.google.com/gist/gowthamkpr/1a8f7137c38e1977eba0409f6aa2e35d/untitled.ipynb). \r\n\r\nYou are importing variables from the wrong path inside contrib module. \r\nInstead of using from `tensorflow.contrib.framework.ops import variables as contrib_variables`, \r\nuse `from tensorflow.contrib.framework.python.ops import variables as contrib_variables`", "TF 1.15 is out of date. We no longer support it.", "(sorry, closed by mistake)"]}, {"number": 55619, "title": "Use faster sort in TopK kernel when num_rows==1", "body": "`DeviceSegmentedRadixSort` is very slow when `num_segments=1` because it only uses 1 SM per segment. Calling the un-segmented version is much faster (3-5x) in this case (i.e., for 1D input).\r\n\r\nNo functional change besides the improved performance.\r\n\r\ncc @nluehr ", "comments": ["@jpienaar Can you please review this PR ? Thank you!"]}, {"number": 55618, "title": "Android - abort crash.", "body": "I'm having this crash on android 10.\r\n\r\n**Dependencies**\r\n\r\nimplementation 'org.tensorflow:tensorflow-lite:2.8.0'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:2.8.0'\r\n\r\nStack Trace:\r\n\r\nabort\r\n\r\npid: 0, tid: 0 >>> com.humbbles.imagenes <<<\r\n\r\nbacktrace:\r\n  #00  pc 000000000005ec46  /apex/com.android.runtime/lib/bionic/libc.so (abort+166)\r\n  #00  pc 000000000003606d  /system/lib/libc++.so (abort_message+88)\r\n  #00  pc 00000000000361e9  /system/lib/libc++.so (demangling_terminate_handler()+160)\r\n  #00  pc 0000000000044ecb  /system/lib/libc++.so (std::__terminate(void (*)())+2)\r\n  #00  pc 00000000000446cf  /system/lib/libc++.so (__cxxabiv1::failed_throw(__cxxabiv1::__cxa_exception*)+12)\r\n  #00  pc 0000000000044631  /system/lib/libc++.so (__cxa_throw+72)\r\n  #00  pc 000000000007fe5f  /system/lib/libc++.so (std::__1::__throw_system_error(int, char const*)+86)\r\n  #00  pc 000000000000e645  /system/lib/libEGL.so (android::egl_cache_t::setBlob(void const*, long, void const*, long)+228)\r\n  #00  pc 000000000000e4fd  /system/lib/libEGL.so (android::setBlob(void const*, long, void const*, long)+20)\r\n  #00  pc 0000000000141aa8  /vendor/lib/egl/libGLES_mali.so\r\n  #00  pc 0000000000141ddc  /vendor/lib/egl/libGLES_mali.so\r\n  #00  pc 00000000003cabe8  /vendor/lib/egl/libGLES_mali.so\r\n  #00  pc 0000000002bfc13b  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002bfbe99  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002bfbd59  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002bfc91d  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002bfc511  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002bfc1e1  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002bfb1a1  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002b50861  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002b51477  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002b51203  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002b51095  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002b5100d  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002b606c1  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 00000000020ca769  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 00000000020c88c7  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000001796063  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002061309  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 000000000206109d  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002060e01  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 0000000002060d1f  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 00000000020df89d  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 00000000020df0b7  /data/app/com.google.android.trichromelibrary_469209830-OMAU-ijYS8rmP_RtBvJXIg==/base.apk!libmonochrome.so (offset 0x664000)\r\n  #00  pc 00000000000a6077  /apex/com.android.runtime/lib/bionic/libc.so (__pthread_start(void*)+20)\r\n  #00  pc 0000000000060131  /apex/com.android.runtime/lib/bionic/libc.so (__start_thread+30)", "comments": ["Hi @ahmadbajwa8282 ! Could you please fill the template which will help us expedite the issue? **Updating Android webview and chrome from playstore** might resolve the issue. Attaching relevant [thread](https://android.stackexchange.com/questions/234830/com-google-android-trichromelibrary-causes-many-android-apps-to-crash) for reference.  Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55617, "title": "keep restore and save op for functaion_optimizer", "body": "Keep save and restore op in function optimizer. \r\nCurrently, function optimizer will make restore op inlined, if restore op is StatefulPartitionedCall.\r\nif we use tf_optimizer.OptimizeGraph to optimize a saved_model, the restore op will be replaced.\r\nWhen use session to restore this optimized saved model, it will report an error.", "comments": ["@ezhulenev is there anything else i need to do for ready to merge?", "It should be merged semi-automatically once it will pass all internal tests at Google, probably sometime tomorrow."]}, {"number": 55616, "title": "How does MultiWorkerMirroredStrategy works?", "body": "We are training to run a distributed training on cluster with just CPUs. After reading the tutorials we choose to use tf.distribute.MultiWorkerMirroredStrategy. But there are something confusing us. It said that I need to prepare the same code on every work and this strategy will send all the model, checkpoint and dataset to every worker. **But dose It sent the data of the sample or just the index of every sample?** Do I need to prepare model, checkpoint and whole dataset on every worker? I hope that the chef worker can load all data from itself and sent what others need to every worker so that other workers don't have to prepare training data. It's not easy for us to put all data on every worker limit to  cluster using rules of our business.\r\nWe try to only load checkpoint on chef worker and the program doesn't work. We also try to load the whole dataset on chef worker and load just a part of dataset on other workers and it doesn't work.", "comments": ["@wzzh Please refer this[ link ](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy)for more details on MultiWorkerMirroredStrategy works and for any further queries please post this issue on [TF forum ](https://discuss.tensorflow.org/)where there is a larger community to get you the right help.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55615, "title": "tf.io.encode png doesn't work when trying to write array to png", "body": "Please see here.  https://colab.research.google.com/drive/1ayBIErWSDAZGJ6B_CjeqraJyMAzEWSyv?usp=sharing\r\n\r\nI am trying to write a tensor to a png. As seen here, when I take the encoded bytes and write it a file using python, it doesn't work. Trying opening the image it gets corrupted \r\nWhen I use tf.io.write_file it works. I also tried taking a png and using tf.io_decode_png and than tf.io.encoding_png and the image string bytes differ from the original png. I am using tensorflow serving and using this png result through the rest api, where I don't have access to tf.io.wrte_file so I was wondering what encode_png does differently, than traditional png encodings.  \r\n", "comments": ["Hello @rohanmuplara ,\r\nThe error log stating that `write() takes exactly one argument (0 given)` which will be in bytes, not in str type.I request you please try to execute the code by providing the expected input.Thanks!", "@tilakrayal please try again. I was just playing around in colab and forgot to return it to proper state. I apologize for that. The error is one I was describing above.", "@tilakrayal sorry for repining but I haven't been able to get it to work for any other image formats. I have also tried other formats here and they are still not working. I have added this https://colab.research.google.com/drive/13Y4ET8jAHZDq7qwUYgg8nuBkxqniUqGu#scrollTo=kTeKBHg-kcaG.", "@rohanmuplara ,\r\nOn running the given code snippet, I am facing an error stating `FileNotFoundError: [Errno 2] No such file or directory`. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/61f8e2745d17296e957284e7d4a73adf/55615.ipynb).\r\n", "You have to upload a sample image for that cell because it reads it locally. Wasn't sure how it was supposed to work otherwise.  https://user-images.githubusercontent.com/47044724/163808624-2f435116-f8cd-4cb2-9be0-116c16fe56fe.png", "@tilakrayal I have gotten rid of having to upload a sample image to make it easier. I also added a video https://www.loom.com/share/4b32b330a0e742cab2ea3a1a9af11b22 here. I am also happy to jump on google meet or whatever is required", "@sachinprasadhs ,\r\nI was able to reproduce the issue in tf [v2.7](https://colab.research.google.com/gist/tilakrayal/cf1fe8ad48178485690bb9f9553f04c6/55615_2_7.ipynb), [v2.8](https://colab.research.google.com/gist/tilakrayal/1e8292cafef6c3312b5756750d9a615f/55615-2-8.ipynb) and [nightly](https://colab.research.google.com/gist/tilakrayal/ce2f4f1b6d881c5504ef2866eeb2b6a1/55615_nightly.ipynb).Please find the attached gist."]}, {"number": 55614, "title": "[Memory Issue]: Dst tensor is not initialized", "body": "Summary: I believe this issue is fired by Memory Issue with GPU on Tensorflow 2.8 and Tensorflow-GPU 2.8.\r\n\r\nConfiguration:\r\n- Python 3.8.12\r\n- GPU: NVIDIA Quadro P1000 Notebook 4Gb (InUse: ~ 3Gb)\r\n- RAM: 16 Gb\r\n- OS: Windows 10 Pro\r\nConfig 1: Tensorflow: 2.8 - Tensorflow-GPU: 2.8 - TF-Addons: 0.16.1 - Keras: 2.8\r\nConfig 2: Tensorflow: 2.5.x - Tensorflow-GPU: 2.5.x - TF-Addons: 0.15\r\n\r\nDescription: The following model was just a super-cheap model using only Dense, Add, and Concatenation with only 8M+ parameters, but the warning warns me about memory overflow and stop my training on PyCharm\r\n\r\nTotal behaviour: \r\n(Batch Size: 256)\r\n- I have been training this model (8M+) and two heavier versions (48M+, and 55M+) on Tensorflow 2.3.x and 2.5.x but Tensorflow did not raise to me either warning and error and the model still worked fine. The model received four input with `dtype='uint8'`, totally 14930 features per vector using 256 batch size with Adam optimizer.\r\n- But since I upgraded my Tensorflow to 2.8, this issue arised on both models, even I have pruned my model to 8M+ parameters with 13674 features per vector only (same dtype=uint8). \r\n- In the same code, I always used `from tensorflow.keras import ...` for both versions: 2.5- and 2.8\r\n\r\nThe memory log is displayed in PyCharm 2021.3.3\r\n[memory_p07-15.pdf](https://github.com/tensorflow/tensorflow/files/8485684/memory_p07-15.pdf)\r\n\r\nBehaviour Prediction: I believed this is raised in version 2.6 due to the introduction of API shifting. Please check on that. Also, the PyCharm could not track the code + autocompletion when Tensorflow >= 2.6 (It worked only with Tensorflow 2.3 - 2.5*).\r\n* I did not try installing previous version of Tensorflow ", "comments": ["@IchiruTake \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55613, "title": "Store PTX for highest compute capability only when using old x.y format", "body": "It is common practice for CUDA applications to include native cubin for all arches, and PTX for the highest arch only (for forward combability). This PR makes this desirable behavior the default when using the old x.y format for `TF_CUDA_COMPUTE_CAPABILITIES`. Currently in TensorFlow, if a user sets `TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.0,6.1,7.0,7.5,8.0,8.6`, both cubin and ptx is generated for ALL of the arches which results in a huge binary size.\r\n\r\nThis change reduces the size of the tensorflow binaries with no downsides.\r\n\r\nIf a user wanted to generate PTX for a different arch besides the highest, they can use the new format of sm_xy,compute_xy.", "comments": ["Wouldn't this break [this](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/toolchains/remote_config/rbe_config.bzl;drc=af542fe44674dd8c86159cd2f51782e75140ae8b;l=40) config running on e.g. sm_52?", "> Wouldn't this break [this](https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/tools/toolchains/remote_config/rbe_config.bzl;drc=af542fe44674dd8c86159cd2f51782e75140ae8b;l=40) config running on e.g. sm_52?\r\n\r\nHi @chsigg, thanks for reviewing. Yes, that is correct, sm_52 would no longer work with this change and config  ` TF_CUDA_COMPUTE_CAPABILITIES=\"3.5,6.0\"`.\r\nDo you happen to know the motivation behind that particular config? It would make more to sense to me if it was `3.5,5.2,6.0` which would still work with this PR."]}, {"number": 55610, "title": "[XLA] Different JIT compile behavior from TF2.7", "body": "For the customized code below, I have seen such a error at runtime when xla is turned on. This does NOT appear in TF2.7.\r\n`2022-04-13 19:49:36.873241: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:436 : INVALID_ARGUMENT: Fail to proof the equality of two dimensions at compile time: %multiply.144 = s32[] multiply(s32[] %constant.142, s32[] %add.1), metadata={op_type=\"Reshape\" op_name=\"Reshape_3\"} vs %add = s32[] add(s32[] %reduce.109, s32[] %constant.17)`\r\n\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Ubuntu 20.04.4 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version:2.8.0\r\n- Python version: 3.8\r\n- GCC/Compiler version (if compiling from source): gcc 10\r\n- CUDA/cuDNN version: 11.6\r\n- GPU model and memory: V100, 32G\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\ndisplay_id_counter = tf.Variable(0, trainable=False, dtype=tf.float64)\r\n\r\n@tf.function\r\ndef evaluation_step(x, y, predictions):\r\n    dummy_loss = 0.9\r\n    predictions = tf.reshape(predictions, [-1])\r\n    predictions = tf.cast(predictions, tf.float64)\r\n    display_ids = x\r\n    display_ids = tf.reshape(display_ids, [-1])\r\n    labels = tf.reshape(y, [-1])\r\n    sorted_ids = tf.argsort(display_ids)\r\n    display_ids = tf.gather(display_ids, indices=sorted_ids)\r\n    predictions = tf.gather(predictions, indices=sorted_ids)\r\n    labels = tf.gather(labels, indices=sorted_ids)\r\n    _, display_ids_idx, display_ids_ads_count = tf.unique_with_counts(\r\n        display_ids, out_idx=tf.int64)\r\n    pad_length = 30 - tf.reduce_max(display_ids_ads_count)\r\n    preds = tf.RaggedTensor.from_value_rowids(\r\n        predictions, display_ids_idx).to_tensor()\r\n    labels = tf.RaggedTensor.from_value_rowids(\r\n        labels, display_ids_idx).to_tensor()\r\n    labels_mask = tf.math.reduce_max(labels, 1)\r\n    preds_masked = tf.boolean_mask(preds, labels_mask)\r\n    labels_masked = tf.boolean_mask(labels, labels_mask)\r\n    labels_masked = tf.argmax(labels_masked, axis=1, output_type=tf.int32)\r\n    labels_masked = tf.reshape(labels_masked, [-1, 1])\r\n\r\n    preds_masked = tf.pad(preds_masked, [(0, 0), (0, pad_length)])\r\n    _, predictions_idx = tf.math.top_k(preds_masked, 12)\r\n    indices = tf.math.equal(predictions_idx, labels_masked)\r\n\r\n    shape = tf.cast(tf.shape(indices)[0], tf.float64)\r\n    display_id_counter.assign_add(shape)\r\n\r\nDIM = 102400\r\ntf.config.optimizer.set_jit(True)\r\nfor step in range(200):\r\n    pre = np.random.random((DIM, 1))\r\n    y_tmp = np.zeros((DIM, 1), dtype=float)\r\n\r\n    num_ones = np.random.randint(1, DIM+1, 1)\r\n    id_one = np.random.randint(0, DIM, num_ones)\r\n    for i in id_one:\r\n        y_tmp[i][0] = 1.\r\n    x_tmp = np.random.randint(0, DIM, (DIM, 1), dtype=np.int64)\r\n    evaluation_step(x_tmp, y_tmp, pre)\r\n```\r\n\r\nTracked down to commit [ac4575](https://github.com/tensorflow/tensorflow/commit/ac457560364f18f24ae506d978f2ab5f04aa501b).", "comments": ["@cheshire can you triage to check where this error is coming from?", "Right, what has changed is that `tf.where` became compilable in autoclustering, and then the compiler tries at compile time to prove equality of dimensions from two dynamic outputs and fails.\r\n\r\nDo you need a general solution, or a workaround? Switching this line (https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/xla/service/gpu/gpu_compiler.cc;l=440?q=padder%20f:gpu_compiler) to `kIgnore`  would solve the issue by ignoring the check.\r\n\r\nWe can also give you a flag to switch those without modifying TF. Or do you need a general solution which always works?", "Thanks @cheshire. Making `tf.where` XLA compilable in autoclustering may results into a perf regression. Say in a training loop, XLA will trigger compilation multiple times every time seeing a data with different size.  See the snapshot from profiler.\r\n![recompile](https://user-images.githubusercontent.com/25590028/163456405-41e50e98-69b6-4d07-9410-9867cd067fd2.PNG)\r\n Reverting this commit resolve this.  I would prefer to have a switch flag to turn it off. ", "@wenscarl OK it might make sense to not compile tf.where in autoclustering environment altogether then. I can make this change.\r\n\r\n> Say in a training loop, XLA will trigger compilation multiple times every time seeing a data with different size\r\n\r\nDo you have a repro I could try? XLA compiles up to the upper bound of the `tf.where` output, not to the concrete bound seen at runtime, so it seems bizarre it would recompile.", "@wenscarl Actually do you want to send a patch disabling tf.where autoclustering?", "@cheshire \r\n> Do you have a repro I could try?\r\nI don't have a lite repro. The regression is observed from [wideanddeep](https://github.com/NVIDIA/DeepLearningExamples/tree/master/TensorFlow2/Recommendation/WideAndDeep) model. With this commit, XLA compiled 3 times as indicated by the above snapshot. Turning off, only 1 XLA jit compile as shown below.\r\n![image](https://user-images.githubusercontent.com/25590028/163497115-a399d02c-76a6-4862-9d63-197ed97c7e51.png). The recompilation may not be directly caused by tf.where autoclustering, but turning-off is a WAR.\r\n", "For the sample code above, the compile time for cluster_1 which include tf.where, is about 10sec while for other clusters on ms level. Recompilation couldn't be reproduced by the sample code, but once happen, it would stall the cpu for 10 sec long. "]}, {"number": 55609, "title": "tensorflow 2.8 neural network model training cannot be run as multiprocess even though the training data is created by keras.utils.Sequence ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu (18.04.6)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): pip install tensorflow==2.8.0\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to train a neural network model built by tf2.8 ans keras 2.8.\r\nI would like to train the model with **\"multiprocess\".** to improve the training performance.\r\nThe following url shows that only keras.utils.Sequence is supported for multiprocess.\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\r\n\r\n\" \"use_multiprocessing\"  Boolean. **Used for generator or [keras.utils.Sequence](https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) input only**. If True, use process-based threading. If unspecified, use_multiprocessing will default to False. Note that because this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes.\"\r\n\r\n**My code (tf2.8):** \r\n\r\n------------------------------\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras.utils import Sequence\r\n\r\nclass MyDataGenerator(Sequence):\r\n    def __init__(self, train_file_paths, batch_size, train_data_size, epoch ):\r\n\r\n        self.file_paths = train_file_paths\r\n        self.batch_size = batch_size\r\n        self.index_and_filename = defaultdict(str)\r\n        self.cur_file_index = 0\r\n        self.total_file_nums = 0\r\n        self.num_parallel_calls = -1\r\n        self.set_files_indices()\r\n        self.on_epoch_end()\r\n       \r\n    def __len__(self):\r\n        return 100\r\n   \r\n    def __getitem__(self, index):\r\n        cur_train_file_path = self.index_and_filename[self.cur_file_index]\r\n        features_and_labels = self.__data_create(cur_train_file_path)\r\n        self.cur_file_index += 1\r\n        features, labels = next(iter(features_and_labels))\r\n        return features, labels\r\n   \r\n    def  on_epoch_end(self):\r\n        self.cur_file_index = 0\r\n        self.total_file_nums = 0\r\n        \r\n    def  set_files_indices(self):\r\n        for a_file in self.file_paths:\r\n            self.index_and_filename[self.total_file_nums] = a_file\r\n            self.total_file_nums += 1\r\n        \r\n    def data_generation(self, file_path):\r\n       \r\n        def process_sample(input_dataset):\r\n            features = tf.io.parse_single_example(input_dataset, [some feature names])\r\n            labels = tf.io.parse_single_example(input_dataset, LABEL_NAME)[LABEL_NAME]\r\n            return (features, labels)\r\n       \r\n        file_paths_ds = tf.data.Dataset.from_tensors(file_path)\r\n       \r\n        all_features_dataset = file_paths_ds.interleave(lambda x: tf.data.TFRecordDataset(x,  compression_type='GZIP'),\r\n                                        cycle_length=10, num_parallel_calls=-1, deterministic=False)\r\n        \r\n        all_features_dataset = all_features_dataset.map(lambda x: process_sample(x),\r\n                                                num_parallel_calls=-1,\r\n                                                deterministic=False)\r\n       \r\n        all_features_dataset = all_features_dataset.shuffle(100)\r\n        all_features_dataset = all_features_dataset.batch(self.batch_size, drop_remainder=True)\r\n        all_features_dataset = all_features_dataset.cache()\r\n        return all_features_dataset\r\n\r\ntrain_data_generator = MyDataGenerator(train_data_path, 2, 10000, 2)\r\ntest_data_generator = MyDataGenerator(train_data_path, 2, 1000, 2)\r\n\r\nmodel.fit(train_data_generator,\r\n          epochs=2\r\n          steps_per_epoch = 100,\r\n          validation_data=test_data_generator,\r\n          validation_steps=10,\r\n          **workers=1,\r\n          use_multiprocessing=False**)\r\n\r\n```\r\n\r\n\r\nIf I run model.fit as:\r\n\r\n```\r\nmodel.fit(train_data_generator,\r\n          epochs=2,\r\n          steps_per_epoch = 100 ,\r\n          validation_data=test_data_generator,\r\n          validation_steps=10),\r\n          workers=1,\r\n          use_multiprocessing=False)\r\n```\r\n\r\nI got error: \r\n\r\n```\r\nInvalidArgumentError: ValueError: Could not find callback with key=pyfunc_44 in the registry.\r\n    Traceback (most recent call last):\r\n\r\n   File \"lib/python3.7/site-packages/tensorflow/python/ops/script_ops.py\", line 259, in __call__\r\n    raise ValueError(f\"Could not find callback with key={token} in the \"\r\n\r\n    ValueError: Could not find callback with key=pyfunc_44 in the registry.\r\n\r\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext]\r\n```\r\n\r\n------------------\r\nIf I run model.fit as:\r\n\r\n ```\r\nmodel.fit(train_data_generator,\r\n          epochs=2,\r\n          steps_per_epoch = 100 ,\r\n          validation_data=test_data_generator,\r\n          validation_steps=10),\r\n          workers=2,\r\n          use_multiprocessing=True)\r\n```\r\n\r\nThe whole training process is frozen and made no progress and no errors popped.\r\nIt seems that there is a deadlock ? \r\nI have also tried tf2.9, got the same error. \r\nCould anybody let me know what I did wrong ? \r\nI missed something ? \r\n\r\nthanks\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nThe \r\n\r\n    model.fit(train_data_generator,\r\n          epochs=2,\r\n          steps_per_epoch = 100 ,\r\n          validation_data=test_data_generator,\r\n          validation_steps=10),\r\n          workers=2,\r\n          use_multiprocessing=True)\r\n\r\nshould not be frozen and all CPU cores should be busy in working on data loading and model training. \r\n\r\nAnd, if run it by single-process, \r\n\r\n```\r\nmodel.fit(train_data_generator,\r\n          epochs=2,\r\n          steps_per_epoch = 100 ,\r\n          validation_data=test_data_generator,\r\n          validation_steps=10),\r\n          workers=1,\r\n          use_multiprocessing=False)\r\n```\r\nit should also works well. No errors.\r\n\r\n", "comments": ["Hello @umusa,\r\nOn running the given code snippet, I am facing an different error.Can you please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/2316d92952b12ccfb40bf15e4a282e2c/untitled300.ipynb) and requesting to provide the complete code to debug the issue.Thanks!\r\n\r\n\r\n", "@tilakrayal , this code needs to access some data files, which are not available to access here. Can you please check the logic to confirm that I do not do anything wrong. thanks", "@umusa ,\r\nWithout the reproducible code, it would be difficult for us to debug the issue. In order to expedite the trouble-shooting process, could you please provide a complete code and the dependencies you are using.Thanks!"]}, {"number": 55608, "title": "[TF-TRT] Support resource variables (ReadVariableOp)", "body": "cc @bixia1, @tfeher, @DEKHTIARJonathan \r\n\r\nThis is a continuation of #55337. Please review after the former (all the changes are included in this branch).\r\nIt's a second building block in the effort to support non-frozen models in TF-TRT. Here, support for models with `ReadVariableOp` nodes is added, when the experimental feature to disable freezing is enabled.", "comments": ["@Nyrio This PR is in draft, any update on this? Please. Thank you!", "@gbaned This PR will be opened for review when #55337 is merged (it contains all the changes in the former)."]}, {"number": 55606, "title": "Some modification for tflite GPU delegate", "body": "I find some bug during programming. But I am not sure if these are right way to fix them, so I list them here first.\r\n\r\n**System information**\r\n- Mobile device  MTK mobile device.\r\n\r\n**Details**\r\n1. When a model has the structure that a Div followed by a Conv2D, and feature map has 3 channels. In this situation, I would get a all-nan output.\r\nfile tensorflow/lite/delegates/gpu/common/tasks/conv_powervr.cc:\r\nin line 857, \r\n-    \\-         for (int ch = 0; ch < 4; ++ch){\r\n-    \\+        for (int ch = 0; ch < (input_ch < 4 ? input_ch : 4); ++ch) {\r\nin line 1043:\r\n-    \\-         conv_core(0);\r\n-    \\+        conv_core(0, input_ch);\r\nin line 1046:\r\n-    \\-         conv_core(i * block_size.w * 4);\r\n-    \\+         input_ch -= 4;\r\n-    \\+         conv_core(i * block_size.w * 4, input_ch);\r\nWhere input_ch represents the input channel number, which is 3 in this situation.\r\nThis modification will prevent zeros in channel 4, which is padded by GPU device for alignment, getting into calculation process.\r\n\r\n2. When a conv2D op receive a dynamic weight as its second input, it will resullt in wrong padding.\r\nfile tensorflow/lite/delegates/gpu/common/model_builder.cc\r\nline 531, add this two lines after RETURN_IF_ERROR(reader->AddInput(node, 1)):\r\n-    \\+          attr.weights.shape.h = graph->FindInputs(node->id)[1]->tensor.shape.h;\r\n-    \\+          attr.weights.shape.w = graph->FindInputs(node->id)[1]->tensor.shape.w;\r\nThis modification will provide shape information for padding, which is in line 542:\r\nUpdatePadding(tf_options->padding, graph->FindInputs(node->id)[0]->tensor.shape, &attr);\r\n\r\n3. When a model has the structure that a Div followed by a Sub, and feature map has batch greater than 1, for example, feature map is in shape 64x1. This will cause a calculation error.\r\nfile tensorflow/lite/delegates/gpu/common/tasks/elementwise.cc, function CreateElementwiseTwoInput:\r\n-    \\-        const std::string x_coord = shape.w == 1 ? \"0\" : \"X_COORD\";\r\n-    \\+       const std::string x_coord = (definition.IsBatchSupported() ? shape.w * shape.b : shape.w) == 1 ? \"0\" : \"X_COORD\";\r\n\r\n", "comments": ["Hi @njuhang ! Could you also update the template with sample stand alone codes to help investigate the above bugs? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55605, "title": "Cant't make quantization for op that tflite doesn't support", "body": "I will use tf.raw_ops.CropAndResize as an example.\r\n\r\nThis is how I define the op:\r\nwith tf.compat.v1.Session() as sess:\r\n    inputs  = tf.raw_ops.Placeholder(dtype = tf.float32, shape = [1,18,18,8], name = \"input\")\r\n    boxes = tf.raw_ops.Placeholder(dtype = tf.float32, shape = [1,64,4], name = \"boxes\")\r\n    zeros = [0,0,0,0]\r\n    zeros = zeros + zeros + zeros + zeros\r\n    zeros = zeros + zeros + zeros + zeros\r\n    crop_size = [18, 18]\r\n    out = tf.raw_ops.CropAndResize(image=inputs, boxes= boxes, box_ind=zeros, crop_size=crop_size)\r\ngraph = tf.compat.v1.get_default_graph()\r\ngraph_def = graph.as_graph_def()\r\noutput_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, ['output'])\r\nwith tf.io.gfile.GFile(save_name, 'wb') as f:\r\n    f.write(graph_def.SerializeToString())\r\n\r\nThis is part of code I convert the .pb to .tflite and make quantization:\r\nmodel = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file = pb_name, input_arrays=[\"input\",\"boxes\"],output_arrays=[\"CropAndResize\"])\r\nmodel.optimizations = [tf.lite.Optimize.DEFAULT]\r\nmodel.representative_dataset = representative_dataset\r\nmodel.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nmodel.inference_input_type = tf.uint8  # or tf.uint8\r\nmodel.inference_output_type = tf.uint8  # or tf.uint8\r\nmodel.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nmodel.allow_custom_ops = True\r\ntflite_model = model.convert()\r\nwith open(tflite_name, 'wb') as f:\r\n    f.write(tflite_model)\r\n\r\nI have some questions during this process:\r\n1. Can I use SaveModel to save this single layer model?\r\n2. I don't know why I failed to quantize this model. But I could convert it from .pb to .tflite without quantization successfully. What's more, I can quantize a similar single conv layer model successfully, with similar way above.\r\n\r\n\r\n", "comments": ["Hi @njuhang ! Could you please update the template with Complete code as Colab gist? I was getting value error in [above code](https://colab.sandbox.google.com/gist/mohantym/1e2a1fe0155a3b6680111c21e625e297/github_55605.ipynb).  \r\n\r\n[Integer Quantization ](https://www.tensorflow.org/lite/performance/post_training_integer_quant)might have failed because of float32 data types in model. You can try again after yielding them as int8 datatype in representative datset.\r\n```\r\n def representative_dataset:\r\n   ....\r\n   yield [a.astype(tf.int8)]\r\n```\r\n\r\n\r\nYes! It is possible to get a single layer model by using v2 apis and [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) . Thanks!", "I have update the code here:\r\nhttps://colab.research.google.com/gist/njuhang/a9e8fe57a0d3e677093d5b84073b7d7c/github_55605.ipynb#scrollTo=rFaRvhLPEUW0\r\n\r\nI am sorry for not explaining my 1st question explicitly. The point is not single layer model, but model with layer defined in raw_op or other package rather than keras.layer. For example, tf.raw_ops.CropAndResize. I wonder if I can save them with SaveModel method.", " @njuhang ! I am going through this issue now . I was suggesting to create a class and use **tf.raw_ops.CropAndResize(image=inputs, boxes= boxes, box_ind=zeros, crop_size=crop_size) function** under a [tf.function](https://www.tensorflow.org/api_docs/python/tf/function) to call it as [concrete function](url) later. \r\n\r\nYou can use then tf.saved_model method to save the model and [tf.lite.TFLiteConverter.get_concrete_function](https://www.tensorflow.org/lite/convert#convert_concrete_functions_) to load in the converter. After that you can use select ops before converting to Tflite model. Thank you!", "Thanks, I got to know how to convert by concrete function. \r\nBut what about the question2? I still have no idea how to quantize this op.", "@njuhang ! tf.raw_ops.CropAndResize is already in supported [select ops](https://www.tensorflow.org/lite/guide/op_select_allowlist) list . It should work with saved_model in v2 api's. Can you only go with this [select ops](https://www.tensorflow.org/lite/guide/ops_select#convert_a_model) syntax (remove custom_ops = true and integer quantization) after loading interpreter from concrete function and let us know the difference? You can also visualize your Tflite model using [netron app ](https://netron.app/)for confirmation . Thanks!"]}, {"number": 55604, "title": "Handle called_computation in AbslHashValue", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): master\r\n- Are you willing to contribute it (Yes/No): yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/hlo_instruction.h#L1315\r\nOnly `kFusion` is considered in this code, modify to the following code to consider other instructions that has a called_computation.\r\n\r\n```python\r\nif (!hlo.called_computations().empty()) {\r\n  for (auto* comp : hlo.called_computations()) {\r\n    h = H::combine(std::move(h), *comp);\r\n  }\r\n}\r\n\r\nif (hlo.opcode() == HloOpcode::kFusion) {\r\n  h = H::combine(std::move(h), hlo.fusion_kind(), \r\n                 hlo.fused_instruction_count(),\r\n                 hlo.fused_parameters().size());\r\n}\r\n```\r\n\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nEveryone who is using the hash value to distinguish different computations.\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 55602, "title": "C++ Using TensorflowLiteC in iOS with Flex delegate TensorflowLiteCSelectTfOps", "body": "I try to make inference with flex delegate supported tflite model in iOS using C++ language.\r\n\r\nfollowing the instructions in:\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\n\r\nI have installed the following pods:\r\n\r\n   pod 'TensorFlowLiteC', '~> 2.5.0'\r\n\r\n   pod 'TensorFlowLiteSelectTfOps', '~> 2.5.0'\r\n\r\nand I have also added linker flag:\r\n\r\n-force_load $(SRCROOT)/Pods/TensorFlowLiteSelectTfOps/Frameworks/TensorFlowLiteSelectTfOps.framework/TensorFlowLiteSelectTfOps\r\n\r\nBut during inference I observe:\r\n\r\n **Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.**\r\n\r\nis TensorFlowLiteSelectTfOps  not suitable for C++? or what am I doing wrong here?\r\n\r\nThanks", "comments": ["Hi @fatihkiralioglu ! Please make below changes in Pod file and then follow [Bazel instructions ](https://www.tensorflow.org/lite/guide/ops_select#using_bazel_xcode) for C/C++ implementation .  Thanks!\r\n \r\n```\r\npod 'TensorFlowLiteSwift'                        # or 'TensorFlowLiteObjC' \r\npod 'TensorFlowLiteSelectTfOps', '~> 0.0.1-nightly'\r\n```\r\n", "@mohantym thank you, then will I strictly need to do bazel build for ios, right? \r\nCurrently, is there no way to use flex op models with currently published pods for C++?", "@fatihkiralioglu ! Not fully confident on default pods. You can refer to this [document](https://www.tensorflow.org/lite/guide/ops_select#cc) for Flex op models.\r\n\r\n`bazel build -c opt --config=monolithic tensorflow/lite/delegates/flex:tensorflowlite_flex.dylib (for macos)\r\n`\r\n \r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55601, "title": "Allow different inputs choice to pad to bucket boundary in tf.data.Dataset.bucket_by_sequence_length", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.8.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\ntf.data.Dataset().bucket_by_sequence_length() has boolean input variable `pad_to_bucket_boundary`. However, if one wants to pad multiple sequences (e.g. audio, length=1000) and labels (e.g. 10 categories) then there is no capability to pad one by bucket_boundary and the other by maximum length in batch.\r\n\r\nIn my instance, I'd like to pad the audio to the maximum length in the batch, but the label by buckets.\r\n\r\n**Will this change the current api? How?**\r\nBoolean pad_to_bucket_boundary can also receive a dictionary with string key and boolean value.\r\n\r\n**Who will benefit with this feature?**\r\nUsers who want to avoid unnecessary sparsity conversions/padding\r\n\r\n**Any Other info.**\r\n", "comments": ["If there's a way to achieve the above via multiple operations that would be great to know."]}, {"number": 55600, "title": "request for feature gather and matmul", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.7.1\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThis function can realize that the corresponding weight is taken from the matrix B according to the index given by the matrix C and then multiplied by the matrix A.\r\nA: [batch, h]\r\nB: [num, h, d]\r\nC: [batch, ]\r\nout: [batch, d]\r\nbatch >> num\r\n```python\r\nfor i, j in enumerate(C):\r\n   out[i] = matmul(A[i], B[j])\r\n```\r\nCurrently this function can be implemented by tf.gather then tf.matmul. However, it will waste a lot of gpu memory due to the need to define an intermediate variable W.\r\n```python\r\nW = tf.gather(B, C)\r\nout = tf.matmul(A, W)\r\n```\r\nIt can also be implemented by one hot matmul. However the time complexity of the calculation will be very high.\r\n```python\r\ntmp = tf.one_hot(C, depth=num)\r\nout = tf.enisum('bh,bn,nhd->bd', A, tmp, B)\r\n```\r\n**Will this change the current api? How?**\r\nNo\r\n**Who will benefit with this feature?**\r\nThis function can be applied to the moe model to achieve the assignment of experts strictly according to the probability of the router. Avoids the limitations of the expert capacity currently required. \r\n**Any Other info.**\r\nNo", "comments": ["To make sure I'm on the same page: you want to add a new op that does the combined gather+matmul?  So this *will* modify the API (i.e. it will extend it by introducing a `tf.gather_and_matmul(...)` op).\r\n\r\nAny new op will need to be implemented for all backends (CPU/GPU/XLA).  You will also need to demonstrate the savings are worth the extra code complexity/maintenance burden.\r\n\r\nFeel free to submit a PR, and we can take a look."]}, {"number": 55599, "title": "right after install TensorFlowLiteObjC , by pod install, cause error in Xcode", "body": "my Podfile is \r\n\r\n```\r\n# Uncomment the next line to define a global platform for your project\r\n# platform :ios, '9.0'\r\n\r\ntarget 'MyApp' do\r\n  # Comment the next line if you don't want to use dynamic frameworks\r\n  # use_frameworks!\r\n\r\n  # Pods for MyApp\r\n  pod 'TensorFlowLiteObjC'\r\n\r\nend\r\n```\r\n\r\n\r\nright after install pod file by commanding \r\narch -x86_64 pod install\r\n\r\n\r\nthen build keep failed saying \r\n\r\ndiff: /Podfile.lock: No such file or directory\r\ndiff: /Manifest.lock: No such file or directory\r\nerror: The sandbox is not in sync with the Podfile.lock. Run 'pod install' or update your CocoaPods installation.\r\n\r\nI did many things that on web says...delete pods file...clean .... I did almost everything but no worth it. \r\nit just happened when install pod. \r\n\r\nm1 Mac \r\n\r\n", "comments": ["Hi @kotran88 ! Is this issue duplicate to #55585 ?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55594, "title": "adding the enable_gpu_compatible_memory pass", "body": "The purpose for this PR is to add a data grappler pass to enable GPU compatible mode for tensors which will be staged downstream using prefetch_to_device. This is in accordance with recent efforts to parallelize the fetch and execute (commit #s: 9a8772d, ad60111, 2803dfc, 4111779, ecc3db7).\r\nNote that the purpose for this PR would be to enable the pass. There would be a subsequent PR to implement the \"UseGpuAllocator\" potentially within the following data ops: RepeatDataset, MapDataset, ParallelMapDataset, ParallelMapDatasetV2, InterleaveDataset, ParallelInterleaveDatasetV2, ParallelInterleaveDatasetV3, ParallelInterleaveDatasetV4, FilterDataset, ParallelFilterDataset, BatchDataset, BatchDatasetV2, ParallelBatchDataset, ShardDataset, PaddedBatchDataset, PaddedBatchDatasetV2, RangeDataset\r\nAttn: @changhuilin", "comments": ["/CC @changhuilin @qqfish", "@qqfish Can you please review this PR ? Thank you!"]}, {"number": 55592, "title": "Unable to Get Metric Result Using Mixed Precision and Mirrored Strategy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): 2.6.2\r\n- Python version: 3.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: cuda_11.1.TC455_06.29190527_0\r\n- GPU model and memory:  NVIDIA RTX A6000 48GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nUnable to call `tf.keras.Metric.result()` when using MixedPrecision.\r\n\r\nWe get the following error \r\n\r\n```\r\nNotImplementedError: in user code:                                                                                                                           \r\n                                                                                                                                                             \r\n    /home/zach/package/package_name/metrics.py:113 result  *                                                                                             \r\n        true_mean = self._true_sum / self._count                                                                                                             \r\n    /home/zach/conda/envs/gent/lib/python3.7/site-packages/keras/mixed_precision/autocast_variable.py:412 __truediv__                                        \r\n        return self.read_value() / o                                                                                                                         \r\n    /home/zach/conda/envs/gent/lib/python3.7/site-packages/keras/mixed_precision/autocast_variable.py:113 read_value                                         \r\n        val = self._variable.read_value()                                                                                                                    \r\n    /home/zach/conda/envs/gent/lib/python3.7/site-packages/tensorflow/python/distribute/values.py:1303 read_value                                            \r\n        \"call `variable.read_value()` inside variable_sync_on_read_context is\"                                                                               \r\n                                                                                                                                                             \r\n    NotImplementedError: call `variable.read_value()` inside variable_sync_on_read_context is not supported **Describe the\r\n``` \r\n**Expected behavior**\r\nShould be able to get metric when using `MirroredStrategy` and `MixedPrecision`\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@zanussbaum,\r\nHi, Thanks for reporting this issue.\r\nCan you provide minimal reproducible code to investigate the root cause of this issue. Thanks!", "Here is a link to a [colab notebook](https://colab.research.google.com/drive/10uaVNPi4qia119Vk3uWpDH6iH98v1jMD?usp=sharing)", "@sachinprasadhs,\r\nI could able to reproduce the issue with Tf v 2.8 and Tf -nightly version.\r\nPlease find the [gist](https://colab.research.google.com/drive/1amTlnvZFeCPuvGc1OuRTvv6WB0bZZYzb?usp=sharing).", "Ok what are the workarounds? What is the issue? Seems like something is not working properly in the `keras/mixed_precision/autocast_variable` module"]}, {"number": 55590, "title": "Link to Resource Variables", "body": "Added hyperlink to Resource Variables.\r\n\r\nFixes: https://github.com/tensorflow/tensorflow/issues/49614", "comments": []}, {"number": 55585, "title": "on xcode,  not found for architecture arm64 caused", "body": "\r\nI added on my podfile \r\npod 'TensorFlowLiteObjc' \r\n\r\nthen install, \r\nbuild then cause error on xcode. \r\n\r\nwhat may be the problem? \r\nI'm using m1 mac \r\n\r\n```\r\nld: warning: Could not find or use auto-linked framework 'TFLTensorFlowLite'\r\nUndefined symbols for architecture arm64:\r\n  \"_OBJC_CLASS_$_TFLInterpreter\", referenced from:\r\n      objc-class-ref in SmartVisionVehicle.o\r\nld: symbol(s) not found for architecture arm64\r\n```\r\n\r\n", "comments": ["Hi @kotran88 ! Sorry for the late response. Could you please fill the template with steps followed to expedite the issue ? Attaching relevant [thread](https://stackoverflow.com/a/54651151) for reference. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55583, "title": "how reduce  tensorflow-lite-select-tf-ops.aar size \uff1f", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04.4 LTS \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 2.7.0\r\n- Python version:3.8.10 \r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source):3.7.2\r\n- GCC/Compiler version (if compiling from source):  9.4.0\r\n- CUDA/cuDNN version:N\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI follow the guide [reduce_binary_size](https://www.tensorflow.org/lite/guide/reduce_binary_size#selectively_build_tensorflow_lite_with_docker)  and execute this commands: [model.tflite](https://drive.google.com/file/d/1f-BDEojrEgx0lPmaI8_x44sKOJl-1mOv/view?usp=sharing)\r\n\r\n```\r\nbash tensorflow/lite/tools/build_aar.sh  --input_models=/host_dir/model.tflite --target_archs=arm64-v8a,armeabi-v7a\r\n```\r\n\r\nand get the tensorflow-lite-select-tf-ops.aar 6.5M\r\n\r\n```\r\n6507898 Apr 12 03:31 tensorflow-lite-select-tf-ops.aar*\r\n```\r\n\r\nbut its size is still too big\uff0ccan i further reduce it size ? only build cpu or build without Delegate?\r\n\r\n\r\n\r\n**Any other info / logs**\r\n\r\n\r\nIntermediate file in tmp :`ops_list.txt`\r\n\r\n```\r\n[\"AddN\",\"BiasAddGrad\",\"BroadcastGradientArgs\",\"Cast\",\"ConcatOffset\",\"EmptyTensorList\",\"ReluGrad\",\"Restore\",\"Save\",\"ShapeN\",\"SigmoidGrad\",\"StridedSliceGrad\",\"TensorListElementShape\",\"TensorListFromTensor\",\"TensorListGetItem\",\"TensorListLength\",\"TensorListPopBack\",\"TensorListPushBack\",\"TensorListReserve\",\"TensorListSetItem\",\"TensorListStack\",\"UnsortedSegmentSum\",\"ZerosLike\"]\r\n```\r\n", "comments": ["Hi @thomaszheng ! \r\nModel binary sizes are closely correlated to the number of ops used in the model. TensorFlow Lite enables you to reduce model binary sizes by using selective builds.\r\n\r\nYou can reduce the size by using multiple tflite models built [using select ops ](https://www.tensorflow.org/lite/guide/ops_select#building_the_android_aar)or reducing no of supported target_archs (Remove armeabi-v7a if possible). Thanks!", "Thank you for your reply\u3002\r\nmy app now use only one model and armeabi-v7a also  can't remove, so i'm looking for other options.\r\n\r\nthis 23 not  built-in ops   need 6.5M aar, but  whole `tensorflow-lite.aar` only 5M with x86,x86_64,arm64-v8a,armeabi-v7a\uff0c and after reduce it only 1.28M for arm64-v8a,armeabi-v7a. \r\n\r\n```\r\n[\"AddN\",\"BiasAddGrad\",\"BroadcastGradientArgs\",\"Cast\",\"ConcatOffset\",\"EmptyTensorList\",\"ReluGrad\",\"Restore\",\"Save\",\"ShapeN\",\"SigmoidGrad\",\"StridedSliceGrad\",\"TensorListElementShape\",\"TensorListFromTensor\",\"TensorListGetItem\",\"TensorListLength\",\"TensorListPopBack\",\"TensorListPushBack\",\"TensorListReserve\",\"TensorListSetItem\",\"TensorListStack\",\"UnsortedSegmentSum\",\"ZerosLike\"]\r\n```\r\n\r\nso I feel like there is still a lot of room for optimization.", "Ok @thomaszheng ! Can you put this line during lite conversion for [optimum size](https://www.tensorflow.org/api_docs/python/tf/lite/Optimize) and let us know after rebuilding the aar files from it?\r\n\r\n```\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE\r\nconverter.convert()\r\n```\r\n\r\n", "ok @mohantym !\r\nI used this conversion  command before (tf version 2.8.0)\uff1a\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n```\r\n\r\nmodel.tflite size is 4M\r\n```\r\n1277026 Apr 12 09:34 tensorflow-lite.aar*\r\n6926947 Apr 12 09:17 tensorflow-lite-select-tf-ops.aar*\r\n```\r\n\r\nnow i use:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_DIR)\r\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\r\n```\r\nmodel_n.tflite size is 8.6M\r\n\r\nthen, reduce binary size arr:\r\n\r\n```\r\nbash tensorflow/lite/tools/build_aar.sh  --input_models=/host_dir/model_n.tflite --target_archs=arm64-v8a,armeabi-v7a\r\n```\r\n\r\n```\r\n1273647 Apr 12 08:53 tensorflow-lite.aar*\r\n6926947 Apr 12 09:17 tensorflow-lite-select-tf-ops.aar*\r\n```\r\n\r\nfrom the results, it seems that there is no decrease\r\n", "@sachinprasadhs ! Could you please look at this issue?", "@miaout17 hi\uff0cIs there any other way please \uff1f"]}, {"number": 55581, "title": "macOS wheel URLs are incorrect", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/install/pip\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe Mac wheels that the docs page purport to exist do not, in fact, exist:\r\n```\r\nNoSuchKeyThe specified key does not exist.No such object: tensorflow/mac/cpu/tensorflow-2.8.0-cp39-cp39-macosx_10_11_x86_64.whl\r\n```\r\n\r\nWhere are these wheels actually located?\r\n\r\n### Submit a pull request?\r\n\r\nI would submit a PR, but I am not aware of what the correct URLs are. The storage bucket does not have any index as far as I could tell.", "comments": ["@samuela,\r\n\r\nThank you for reporting.  \r\n\r\nFor macOS (CPU-only) : All URL's are not working\r\nFor Linux : Python 3.10 CPU-only URL is not working\r\n\r\n", "@chunduriv what are the correct URLs for those wheels?", "@samuela,\r\n\r\nI have submitted the fix internally which should reflect the updated changes.\r\n\r\nPlease refer correct path for macOS\r\n\r\nPython 3.7 | https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.8.0-cp37-cp37m-macosx_10_14_x86_64.whl\r\nPython 3.8 | https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.8.0-cp38-cp38-macosx_10_14_x86_64.whl\r\nPython 3.9 | https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.8.0-cp39-cp39-macosx_10_14_x86_64.whl\r\nPython 3.10 | https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-2.8.0-cp310-cp310-macosx_10_14_x86_64.whl\r\n\r\nThanks!", "Thanks @chunduriv! What is the correct path for linux, python 3.10, CPU only?"]}, {"number": 55579, "title": "Remove TF_CAPI_EXPORT from TF_InitGraph", "body": "`TF_InitGraph` should be implemented by the plugin, not by core TensorFlow. Removing `TF_CAPI_EXPORT` gets rid of warning `C4273`.", "comments": ["@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 55576, "title": "TFLite InterpreterBuilder Segmentation fault 11 on MacOS 11.6", "body": "I use C++ to load tflite model. Same code works fine on Linux & Android. But when running on MacOS it gives Segmentation fault 11  error.\r\n\r\nI am using tensorflow 2.8.0. \r\n\r\n```C++\r\n#include <tensorflow/lite/kernels/register.h>\r\n\r\nstd::unique_ptr<tflite::FlatBufferModel> model;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\n\r\nint main() {\r\n  model = tflite::FlatBufferModel::BuildFromFile\"model.tflite\");\r\n  tflite::ops::builtin::BuiltinOpResolver resolver;\r\n  tflite::InterpreterBuilder(*model, resolver)(&interpreter); // -> Segmentation fault 11 here\r\n  return 0;\r\n}\r\n```", "comments": ["Hi @HedgeHao ! Could you check this code snippet and let us know the output? Thanks!\r\n```\r\n#include <tensorflow/lite/kernels/register.h>\r\nusing namespace tflite;\r\nstd::unique_ptr<tflite::FlatBufferModel> model;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\n\r\nint main() {\r\n  auto model = tflite::FlatBufferModel::BuildFromFile(\"model.tflite\"); // saw a missing open bracket in original code\r\n  if (model == nullptr) {\r\n  // Return error if model is empty\r\n}\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nif (InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk) {\r\n  // Return failure.\r\n}\r\nif (interpreter->AllocateTensors() != kTfLiteOk) {\r\n  // Return failure.\r\n}\r\n  return 0;\r\n}\r\n```\r\n", "@mohantym Not working, either. Still Segmentation fault: 11 in `InterpreterBuilder`.\r\n\r\n> Hi @HedgeHao ! Could you check this code snippet and let us know the output? Thanks!\r\n> \r\n> ```\r\n> #include <tensorflow/lite/kernels/register.h>\r\n> using namespace tflite;\r\n> std::unique_ptr<tflite::FlatBufferModel> model;\r\n> std::unique_ptr<tflite::Interpreter> interpreter;\r\n> \r\n> int main() {\r\n>   auto model = tflite::FlatBufferModel::BuildFromFile(\"model.tflite\"); // saw a missing open bracket in original code\r\n>   if (model == nullptr) {\r\n>   // Return error if model is empty\r\n> }\r\n> tflite::ops::builtin::BuiltinOpResolver resolver;\r\n> if (InterpreterBuilder(*model, resolver)(&interpreter) != kTfLiteOk) {\r\n>   // Return failure.\r\n> }\r\n> if (interpreter->AllocateTensors() != kTfLiteOk) {\r\n>   // Return failure.\r\n> }\r\n>   return 0;\r\n> }\r\n> ```\r\n\r\n", "Hi @sachinprasadhs ! Could you please look at this issue?", "Anyone?", "Hi, Did you try it in the nightly version and did you find the same behavior. \r\nCould you also let us know your Mac's processor. Thanks!"]}]