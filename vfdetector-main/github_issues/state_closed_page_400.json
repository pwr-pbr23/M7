[{"number": 41944, "title": "Hadoop filesystem Part 1", "body": "@mihaimaruseac \r\nThis PR add random_access_file and writable_file", "comments": []}, {"number": 41943, "title": "tflite on Android: Didn't find op for builtin opcode 'CONV_2D' version '5'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android emulator (Android 10.0+ (Google APIs))\r\n- TensorFlow installed from (source or binary): Model was generated with TF from binary\r\n- TensorFlow version (or github SHA if from source): Model was generated with `tf-2.2.0` and converted to tflite with `tf-nightly-2.4.0.dev20200730`. Using [`tflite` v1.1.1](https://pub.dev/packages/tflite) flutter bindings.\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\nNo issues with `tflite_convert` (thanks to @amahendrakar in #41877). My issue is with using my model on the mobile side, using the [`tflite`](https://pub.dev/packages/tflite) flutter bindings.\r\n\r\nI'm getting the following error when I try to load my custom model:\r\n\r\n```\r\nUnsupported value: java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\n\r\nRegistration failed.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nSee [this gist](https://gist.github.com/mgalgs/f92d78e6c7ee09b8298cd325bf4f3ed6). The error is happening when I try to [load the model](https://gist.github.com/mgalgs/f92d78e6c7ee09b8298cd325bf4f3ed6#file-home-dart-L31-L35).\r\n\r\nIf I swap out my custom generated tflite model (see #41877 for how the model was generated) for an off-the-shelf SSD model detection works just fine.\r\n\r\n**Any other info / logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n\r\nStack trace:\r\n\r\n**Dart**\r\n```\r\nStandardMethodCodec.decodeEnvelope (message_codecs.dart:569)\r\nMethodChannel.invokeMethod (platform_channel.dart:321)\r\n<asynchronous gap>\r\nTflite.loadModel (tflite.dart:15)\r\n_HomePageState.loadModel (home.dart:30)\r\n_HomePageState.initState (home.dart:26)\r\nStatefulElement._firstBuild (framework.dart:4355)\r\nComponentElement.mount (framework.dart:4201)\r\nElement.inflateWidget (framework.dart:3194)\r\nElement.updateChild (framework.dart:2988)\r\n... snipped a bunch of framework stuff ...\r\n```\r\n\r\n**Native (Android)**\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\n\r\nRegistration failed.\r\n\r\n\tat org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n\tat org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:72)\r\n\tat org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n\tat org.tensorflow.lite.Interpreter.<init>(Interpreter.java:266)\r\n\tat sq.flutter.tflite.TflitePlugin.loadModel(TflitePlugin.java:232)\r\n\tat sq.flutter.tflite.TflitePlugin.onMethodCall(TflitePlugin.java:98)\r\n\tat io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage(MethodChannel.java:230)\r\n\tat io.flutter.embedding.engine.dart.DartMessenger.handleMessageFromDart(DartMessenger.java:85)\r\n\tat io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage(FlutterJNI.java:692)\r\n\tat android.os.MessageQueue.nativePollOnce(Native Method)\r\n\tat android.os.MessageQueue.next(MessageQueue.java:335)\r\n\tat android.os.Looper.loop(Looper.java:183)\r\n\tat android.app.ActivityThread.main(ActivityThread.java:7476)\r\n\tat java.lang.reflect.Method.invoke(Native Method)\r\n\tat com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:549)\r\n\tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:939)\r\n```", "comments": ["@mgalgs I'm having the same issue, but on ios. Did you found the solution?", "@ValentinBlokhin no \ud83d\ude2d ", "I believe this was due to some improvements made to the post-training quantization pipeline, which requires the latest TFLite runtime (2.3 or more recent).\r\n\r\nFor now, you can either downgrade to TF 2.2 for model conversion, or file an issue (or submit a PR) to upgrade the flutter bindings to use the latest TFLite 2.3 release (see https://github.com/shaqian/flutter_tflite).", "@jdduke shoot, I had to use something newer than TF 2.2 due to #41877 , so downgrading to 2.2 for model conversion isn't really an option :thinking: I'll try to get `flutter_tflite` updated and will report back with results.", "@jdduke I've updated my `flutter_tflite` to use `org.tensorflow:tensorflow-lite:2.3.0` ([commit](https://github.com/mgalgs/flutter_tflite/commit/e7356bd07129bb35dde8a2f094b20ecaf5248502)) but I'm still getting this same error.", "Here's the native-side stack trace under Android:\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\n\r\nRegistration failed.\r\n\r\n\tat org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n\tat org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:72)\r\n\tat org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n\tat org.tensorflow.lite.Interpreter.<init>(Interpreter.java:266)\r\n\tat sq.flutter.tflite.TflitePlugin.loadModel(TflitePlugin.java:232)\r\n\tat sq.flutter.tflite.TflitePlugin.onMethodCall(TflitePlugin.java:98)\r\n\tat io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage(MethodChannel.java:230)\r\n\tat io.flutter.embedding.engine.dart.DartMessenger.handleMessageFromDart(DartMessenger.java:85)\r\n\tat io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage(FlutterJNI.java:692)\r\n\tat android.os.MessageQueue.nativePollOnce(Native Method)\r\n\tat android.os.MessageQueue.next(MessageQueue.java:335)\r\n\tat android.os.Looper.loop(Looper.java:183)\r\n\tat android.app.ActivityThread.main(ActivityThread.java:7476)\r\n\tat java.lang.reflect.Method.invoke(Native Method)\r\n\tat com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:549)\r\n\tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:939)\r\n```", "If you're converting with the TF nightly, you'll need to use the nightly TFLite runtime for this particular op. That is, `org.tensorflow:tensorflow-lite:0.0.0-nightly`.", "@jdduke changing to `org.tensorflow:tensorflow-lite:0.0.0-nightly` worked, thanks!", "> @jdduke changing to `org.tensorflow:tensorflow-lite:0.0.0-nightly` worked, thanks!\r\n\r\n\r\nIn tf v2.2.0, I am getting the error:\r\nInternal error: Cannot create interpreter: Didn't find op for builtin opcode 'SUB' version '3'\r\n\r\nin v2.3.0, I am getting the error: \r\nInternal error: Cannot create interpreter: Didn't find op for builtin opcode 'CONV_2D' version '5'\r\n\r\nAlso I tried your solution @mgalgs , added implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly' , and got the error: \r\nInternal error: Cannot create interpreter: Did not get operators or tensors in subgraph 1.\r\n\r\nCould you be more specifics, did you change anything else ? ", "> Could you be more specifics, did you change anything else ?\r\n\r\n@ArtanBerisha1 this is the change I made: https://github.com/mgalgs/flutter_tflite/commit/82d62fdab9ee1826e875b11bc8c137e8d421a30e\r\n\r\nThat got me past the initial error but now I'm hitting another error preventing me from using my custom model on mobile (works fine on desktop). The stack trace is:\r\n\r\n```\r\njava.lang.ArrayIndexOutOfBoundsException: length=1; index=1\r\n\tat sq.flutter.tflite.TflitePlugin$RunSSDMobileNet.<init>(TflitePlugin.java:654)\r\n\tat sq.flutter.tflite.TflitePlugin.detectObjectOnFrame(TflitePlugin.java:634)\r\n\tat sq.flutter.tflite.TflitePlugin.onMethodCall(TflitePlugin.java:135)\r\n\tat io.flutter.plugin.common.MethodChannel$IncomingMethodCallHandler.onMessage(MethodChannel.java:233)\r\n\tat io.flutter.embedding.engine.dart.DartMessenger.handleMessageFromDart(DartMessenger.java:85)\r\n\tat io.flutter.embedding.engine.FlutterJNI.handlePlatformMessage(FlutterJNI.java:692)\r\n\tat android.os.MessageQueue.nativePollOnce(Native Method)\r\n\tat android.os.MessageQueue.next(MessageQueue.java:335)\r\n\tat android.os.Looper.loop(Looper.java:183)\r\n\tat android.app.ActivityThread.main(ActivityThread.java:7656)\r\n\tat java.lang.reflect.Method.invoke(Native Method)\r\n\tat com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592)\r\n\tat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947)\r\n```\r\n\r\nI've moved on to another component with hopes that this stuff will resolve itself in `tf-nightly` while I work on other things, but I'm pushing up against this again and might spend some more time on it soon.\r\n\r\nIf anyone else gets `flutter-tflite` working I'd love to hear about your training and model conversion pipeline, because clearly I'm missing something...", "Any progress?", "Check out the version of Tensorflow which you use while training, Use the same version in android build.gridle(app).\r\nI use tensorflow 2.4.0 (which is latest) in colab while training, so I put (implementation 'org.tensorflow:tensorflow-lite:2.4.0') in my android build.gridle(app)", "Sorry but I'm getting the same error. I have tried the above suggestions but it still doesn't work.\r\nI used TS version 2.4.1 to train, so even though I replaced the line `implementation 'org.tensorflow:tensorflow-lite:x.x.x` with xxx is different versions such as: 2.0.0 - 2.3.0 - 0.0.0 nightly, or 2.4.0\r\nBut all of them didn't work.\r\nI am following [this tutorial](https://developer.android.com/codelabs/digit-classifier-tflite#0)", "Duongbaoquy you should use this, implementation 'org.tensorflow:tensorflow-lite:2.4.1'", "I tried with 2.4.1 after reading your comment above, but then I get this error \r\n```\r\n  > Could not find org.tensorflow:tensorflow-lite:2.4.1.\r\n```\r\nMaybe I should try downgrade the TS version to a lower version and train", "2.4.1 is recent release, so that's why. Downgrade on training side may helpful", "I tried downgrading the versions from 2.4.0 to 2.0.0\r\nAnd it worked at version 2.0.0\r\nThanks", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41943\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41943\">No</a>\n", "How do you change what tensorflow_lite version tflite_flutter uses?\r\n\r\nI added the dependency in build.gradle but I still get the same error.\r\n\r\nI'm trying to implement [TensorFlow Lite Flutter Object Detection](https://github.com/am15h/object_detection_flutter) by the way."]}, {"number": 41942, "title": "Eliminate nest.flatten() call in _filtered_call", "body": "Redundant, following nest.flatten() in `_convert_numpy_outputs`.\r\nNote: PR not ready for merge yet, submitted for @sun51 and @kkimdev to check progress\r\nEdit: In review", "comments": ["Approximate benchmarks - mean & [median of means] of 10 trials with 30000 iters each:\r\ndefun_matmul: 185/184 us -> 171/167\r\ndefun_matmul_async: 93/92 us -> 89/88 us\r\ndefun_matmul_with_signature: 198/193 us -> 189/184 us", "Pending tests passing, the last step would be to fix relevant aspects of `tensorflow/python/autograph/g3doc/reference/error_handling.md`", "Changes to `tensorflow/python/autograph/g3doc/reference/error_handling.md` negligible; edits not needed after discussion with original document author"]}, {"number": 42239, "title": "[TF 2.2] tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null", "body": "System information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary): binary\r\nTensorFlow version (use command below): pip install tensorflow==2.0.0\r\nPython version:3.7.5\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\nDescribe the current behavior\r\nI am able to export the model with tf.saved_model.save but when I try to run the exported model with tf-serving, it runs to seg fault and running the saved_model_cli throws the error attached\r\n\r\nDescribe the expected behavior\r\n\r\nThe exported model from tf.saved_model.save should work correctly with tf-serving\r\n\r\nStandalone code to reproduce the issue\r\n\r\n```\r\nclass NGramTF(tf.Module):\r\n    def __init__(self):\r\n        self.oov_score = 0\r\n        self.pseudo_count = 5.0\r\n        pass\r\n\r\n    def fit_tf_lookup_table(self, dist):\r\n        keys_tensor = tf.constant(list(dist.keys()))\r\n        vals_tensor = tf.constant(list(dist.values()))\r\n        initializer = tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor)\r\n        table = tf.lookup.StaticHashTable(initializer, self.oov_score)\r\n        return table\r\n\r\n    def fit_word_casing(self, wordCasingLookup):\r\n        indices = []\r\n        values = []\r\n        tokens = []\r\n        for idx, (token, items) in enumerate(wordCasingLookup.items()):\r\n            tokens.append(token)\r\n            for j, item in enumerate(items):\r\n                indices.append([idx, j])\r\n                values.append(item)\r\n        word_casing_lookup_shape = [len(tokens), max([len(item) for item in wordCasingLookup.values()])]\r\n        word_casing_lookup_tf = tf.SparseTensor(indices=indices, values=values, dense_shape=word_casing_lookup_shape)\r\n        word_casing_indices = tf.range(0, len(tokens))\r\n        dense_word_casing_lookup_tf = tf.sparse.to_dense(word_casing_lookup_tf, default_value='')\r\n\r\n        initializer = tf.lookup.KeyValueTensorInitializer(tf.constant(tokens), word_casing_indices)\r\n\r\n        # TODO: this does ot work\r\n        table = tf.lookup.StaticHashTable(initializer, -1)\r\n        return table, dense_word_casing_lookup_tf\r\n\r\n    def fit(self, wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist):\r\n        self.word_casing, self.word_casing_lookup = self.fit_word_casing(wordCasingLookup)\r\n        self.tf_uni_dist = self.fit_tf_lookup_table(uniDist)\r\n        self.backwardBiDist = self.fit_tf_lookup_table(backwardBiDist)\r\n        self.forwardBiDist = self.fit_tf_lookup_table(forwardBiDist)\r\n        self.trigramDist = self.fit_tf_lookup_table(trigramDist)\r\n```\r\n\r\nCode to export the model:\r\n\r\n```\r\ndef load_truecasing_model(model_filename):\r\n    with open(model_filename, 'rb') as bin_file:  # from s3://workfit-models/auto-punc/\r\n        uni_dist = pickle.load(bin_file)\r\n        backward_bi_dist = pickle.load(bin_file)\r\n        forward_bi_dist = pickle.load(bin_file)\r\n        trigram_dist = pickle.load(bin_file)\r\n        word_casing_lookup = pickle.load(bin_file)\r\n        return word_casing_lookup, uni_dist, backward_bi_dist, forward_bi_dist, trigram_dist\r\n\r\ntruecaser_weights = 'en/en_truecasing_model.obj'\r\nexport_path = './truecaser_serving/1/'\r\nwordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist = load_truecasing_model(truecaser_weights)\r\ntf_model = truecaser_tf.NGramTF()\r\ntf_model.fit(wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist)\r\nsignature_def = tf_model.get_true_case.get_concrete_function(\r\n        tf.TensorSpec(shape=(None), dtype=tf.string, name=\"input_text\"))\r\ntf.saved_model.save(tf_model,export_path,signatures={'serving_default': signature_def})\r\n\r\n```\r\n\r\nOther info / logs Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\r\n\r\nsignature_def['__saved_model_init_op']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['__saved_model_init_op'] tensor_info:\r\n        dtype: DT_INVALID\r\n        shape: unknown_rank\r\n        name: NoOp\r\n  Method name is: \r\n\r\nsignature_def['serving_default']:\r\n  The given SavedModel SignatureDef contains the following input(s):\r\n    inputs['input_text'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: unknown_rank\r\n        name: serving_default_input_text:0\r\n  The given SavedModel SignatureDef contains the following output(s):\r\n    outputs['output_0'] tensor_info:\r\n        dtype: DT_STRING\r\n        shape: (-1)\r\n        name: StatefulPartitionedCall_5:0\r\n  Method name is: tensorflow/serving/predict\r\n2020-07-31 10:32:36.828182: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-07-31 10:32:36.864779: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ffd54e04ae0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-31 10:32:36.864800: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n\r\nDefined Functions:\r\n  Function Name: 'get_true_case'\r\n    Option tensorflow/serving#1\r\n      Callable with:\r\n        Argument tensorflow/serving#1\r\n          input_text: TensorSpec(shape=<unknown>, dtype=tf.string, name='input_text')\r\nException ignored in: <function CapturableResourceDeleter.__del__ at 0x12f849050>\r\nTraceback (most recent call last):\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\r\nException ignored in: <function CapturableResourceDeleter.__del__ at 0x12f849050>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py\", line 191, in __del__\r\n    self._destroy_resource()\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 627, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 506, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2446, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 441, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 241, in restored_function_body\r\n    return _call_concrete_function(function, inputs)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/function_deserialization.py\", line 72, in _call_concrete_function\r\n    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py\", line 101, in _call_flat\r\n    cancellation_manager)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1760, in _call_flat\r\n    flat_outputs = forward_function.call(ctx, args_with_tangents)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 627, in call\r\n    executor_type=executor_type)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/ops/functional_ops.py\", line 1165, in partitioned_call\r\n    f.add_to_graph(graph)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 543, in add_to_graph\r\n    g._add_function(self)\r\n  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3187, in _add_function\r\n    gradient)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).word_casing._initializer\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).tf_uni_dist._initializer\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).backwardBiDist._initializer\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).forwardBiDist._initializer\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).trigramDist._initializer\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n```\r\n\r\nThe code can be accessed here to look at the entire source code:\r\nhttps://github.com/ayushch3/truecaser_tf\r\n\r\nThe weights file can be accessed here: https://drive.google.com/file/d/1DpmsDYm-gzcwXCJT4sMCUXmtHGSxKiIR/view?usp=sharing", "comments": ["@ayushch3 This might be caused by tensorflow itself. Can you load the model and predict it without using tensorflow serving? Thanks!", "> @ayushch3 This might be caused by tensorflow itself. Can you load the model and predict it without using tensorflow serving? Thanks!\r\n\r\nthe problem is with export, so i do not think one can do a load+predict test.\r\n\r\nTF saved_model.save() API seems to be misbehaving or misused.\r\n\r\nadding @k-w-w  and @allenlavoie -- can you please take a look? thanks!\r\n", "@netfs any updates on the issue? As pointed out, the tf.saved_model.save is working incorrectly. ", "I will transfer this issue to tensorflow/tensorflow.", "@ayushch3 is it easy to verify whether the problem persists with a newer version of TensorFlow?", "the code is running tf 2.2.0 @martinwicke ", "Thanks (the template said 2.0.0, hence the question).\r\n\r\nI'll let this be picked up by triage.", "@ayushch3,\r\nOn exporting the model with TF v2.3, I did not face any error running the `saved_model_cli` command. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/1e67f1a509d0459a9b8c01993a17e00f/42239.ipynb).\r\n\r\nCould you please update TensorFlow to v2.3 and check if you are still facing the same issue? Thanks!", "@gowthamkpr @amahendrakar if you use the tag --all with show_model_cli (saved_model_cli show --dir dir_name --all) you would be able to reciprocate the issue with the following error:\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).word_casing._initializer\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).tf_uni_dist._initializer\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).backwardBiDist._initializer\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).forwardBiDist._initializer\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).trigramDist._initializer\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n\r\n", "@ayushch3,\r\nEven on adding the `--all` flag, I did not face the  `tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null` error. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/b9f6bbef876cec0c1e677f9a3d6465c0/42239.ipynb). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41941, "title": "Tensorflow 2.2.0 require scipy-1.4.1??", "body": "**System information**\r\n- OS Platform and Distribution :Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version:  Cuda10.0, cudnn 7.6.5.32\r\n- GPU model and memory: GTX1070 , 8G\r\n\r\n**Describe the problem**\r\ninstalling tensorflow removes existing scipy 1.5.2 and installs 1.4.1\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nAfter building tensorflow2.2.0 successfully, I installed it with\r\n```\r\npip install --no-cache-dir tensorflow-2.2.0-cp38-cp38-linux_x86_64.whl\r\n```\r\n(pip = pip3 in my set up)\r\n\r\nIt was installed successfully and I checked the installation with some tests, all passed. But it uninstalls my already exist scipy 1.5.2 and installed 1.4.1\r\n\r\n```\r\nAttempting uninstall: scipy\r\n    Found existing installation: scipy 1.5.2\r\n    Uninstalling scipy-1.5.2:\r\n      Successfully uninstalled scipy-1.5.2\r\n```\r\n", "comments": ["Please see #40884 and issues linked there as well as the PRs listed in the transitive set of github issues/prs from there.\r\n\r\nIn short, TF 2.3 no longer needs it and older TF will remove the dependency when a new patch release happens.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41941\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41941\">No</a>\n", "@mihaimaruseac \r\n\r\nSo it means the restriction is not necessary and upgrading scipy back to 1.5.2 after installing tensorflow-2.2.0 won't break it?\r\nPlease clarify. Thanks.", "Yes, you can upgrade."]}, {"number": 41939, "title": "[TFLite 16x8] Notebook for 16x8 post-training quantization.", "body": "This PR provides a notebook with tutorial on 16x8 post-training quantization. \r\nIt uses a simple MNIST example to demonstrate how this mode should be used.", "comments": ["Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/41939\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n Review Jupyter notebook visual diffs & provide feedback on notebooks. \n\n---\n\n <i>Powered by <a href='https://www.reviewnb.com'>ReviewNB</a></i>", "The notebook LGTM. Can you also update these pages?\r\n* https://www.tensorflow.org/lite/performance/model_optimization\r\n* https://www.tensorflow.org/lite/performance/post_training_quantization\r\n", "Hi @khanhlvg Thanks for the review! I have changes to update these pages, but I want to upstream them in a separate PR. Is it okay ? \r\n", "SGTM. Thanks @wwwind !", "@wwwind Can you please check @renjie-liu's comments and keep us posted ? Thanks!", "Hi @renjie-liu Sorry, but all output cells are empty in this notebook as far as I can see. \r\nAlso, what do you mean by \"enable python3 for the colab\" ?\r\nThanks a lot for the review!", "@renjie-liu Can you please assist on above comments from @wwwind. Thanks!", "A drive-by comment as @renjie-liu clarifies. Given that the feature is experimental, it'd be good to also mark it in the navigation side bar, like below.\r\n\r\n<img width=\"207\" alt=\"Screen Shot 2020-08-14 at 11 28 40 AM\" src=\"https://user-images.githubusercontent.com/4323109/90281388-53d55b80-de21-11ea-8490-ff2c7d4d7d7a.png\">\r\n", "@wwwind  Can you please check @alanchiao's comments and keep us posted ? Thanks!", "Hi @renjie-liu !\r\nCould you please re-approve this PR?\r\n- i think i enabled python3 correctly now in this notebook by setting \r\n`\"kernelspec\": {\r\n\"display_name\": \"Python 3\",\r\n\"name\": \"python3\"\r\n}`\r\n- i checked that all output cells are empty:\r\n`\"outputs\": [],`\r\n- added 'experimental' status as well\r\n\r\nThanks for the review!", "> \r\n> \r\n> A drive-by comment as @renjie-liu clarifies. Given that the feature is experimental, it'd be good to also mark it in the navigation side bar, like below.\r\n> \r\n> <img alt=\"Screen Shot 2020-08-14 at 11 28 40 AM\" width=\"207\" src=\"https://user-images.githubusercontent.com/4323109/90281388-53d55b80-de21-11ea-8490-ff2c7d4d7d7a.png\">\r\n\r\n@alanchiao, how is that done, could you point at the source example please?", "@alanchiao Can you please take a look on above comments from @akarmi. Thanks!", "@gbaned : Elena figured out how to add the \"experimental\" status, so Anton's comment is resolved. @renjie-liu to finalize this.", "Hi @khanhlvg Thanks for the review! I updated the notebook as suggested. Could you please re-approve ?"]}, {"number": 41938, "title": "Unresolved symbol EigenMatMulF64 when linking a compiled graph with XLA AOT runtime", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\n\r\nWhen compiling a Tensorflow graph compiled using `saved_model_cli aot_compile_cpu`, the following linking error occurs:\r\n\r\n```\r\nlibcholesky.o : error LNK2019: unresolved external symbol __xla_cpu_runtime_EigenMatMulF64 referenced in function entry_34875272679f13979ced813466bdebb0 [C:\\Projects\\aot\\aot\\tests\\mve\\build\\main.vcxproj]\r\nC:\\Projects\\aot\\aot\\tests\\mve\\build\\Release\\main.exe : fatal error LNK1120: 1 unresolved externals [C:\\Projects\\aot\\aot\\tests\\mve\\build\\main.vcxproj]\r\n```\r\n\r\nI think this is because the AOT compile references \"tensorflow/include/tensorflow/compiler/xla/service/cpu/runtime_matmul.h\" instead of \"tensorflow/include/tensorflow/compiler/xla/service/cpu/runtime_single_threaded_matmul.h\".\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect that the exported model can be compiled by referencing only the files in \"tensorflow/xla_aot_runtime_src\", as stated [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/BUILD#L193).\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI used and saved the following graph:\r\n\r\n```python\r\ndef cholesky(A, b):\r\n    \"\"\" Builds a graph \"\"\"    \r\n    A = tf.linalg.cholesky(A) \r\n    res = tf.linalg.triangular_solve(A, b)     \r\n    return res\r\n\r\nM = 4\r\npredict_fn = tf.function(cholesky,\r\n        input_signature=[tf.TensorSpec(shape=[M,M], dtype=tf.float64, name='A'), tf.TensorSpec(shape=[M,1], dtype=tf.float64, name='b')], experimental_compile=False)\r\n\r\nmodule_to_save = tf.Module()\r\nmodule_to_save.predict = predict_fn\r\ntf.saved_model.save(module_to_save, 'saved_model', signatures={'serving_default': module_to_save.predict})\r\n```\r\n\r\nThen I compiled the graph using the `saved_model_cli` script:\r\n\r\n```bash\r\n$ cd saved_model\r\n$ saved_model_cli aot_compile_cpu --checkpoint_path .\\variables\\variables --dir . --signature_def_key serving_default --target_triple x86_64-none-windows --cpp_class cholesky --output_prefix libs64/libcholesky --tag_set serve\r\n```\r\n\r\nCompiling this sample `main.cpp` and linking it against the generated library above and the [XLA AOT CPU runtime that ships with the pip package](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/BUILD#L193) produces the linking error above.\r\n\r\n```cpp\r\n#include <iostream>\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"libcholesky.h\" // generated\r\n\r\n#define M 4\r\n\r\nint main(int argc, char** argv) {  \r\n  cholesky model;  \r\n\r\n  int i, j, idx;\r\n  double test_A[] = {1.0583131 , 0.8570645 , 0.77131426, 0.9754439, 0.8570645 , 0.8788354 , 0.6582537 , 0.88981044,0.77131426, 0.6582537 , 0.6411602 , 0.7668645,\r\n       0.9754439 , 0.88981044, 0.7668645 , 1.3106735 };\r\n\r\n  double test_b[] = {1, 2, 3, 4};\r\n\r\n  std::copy(test_A, test_A+M*M, model.arg0_data());\r\n  std::copy(test_b, test_b+M, model.arg1_data());\r\n  model.Run();\r\n \r\n  std::cout << model.result0_data()[0] << std::endl;\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n**Other info / logs** \r\n", "comments": ["Hi @r4nt ,\r\n\r\nHave you been able to replicate the issue? Do you need any further information?\r\n\r\nBest Regards,\r\n\r\nMarco", "@ebrevdo Any idea what's going on?  `__xla_cpu_runtime_EigenMatMulF64` is defined in `compiler/xla/service/cpu/runtime_matmul.cc`, maybe `saved_model_cli` is not picking it up for some reason?", "Hi @sanjoy , @ebrevdo,\r\n\r\nThe strange bit is that when compiling graphs with `saved_model_cli` that for example only have `dgemm`, everything works as expected: `saved_model_cli` picks up `__xla_cpu_runtime_EigenSingleThreadedMatMulF64`, linking with mini-runtime that comes with pip succeeds.\r\n\r\nHowever, when the graph only contains `cholesky` and/or `triangular_solve` ops (like the one showed by @battuzz), it's picking `__xla_cpu_runtime_EigenMatMulF64` which is not part of the single threaded mini-runtime that ships with the pip package. Hence linking fails.\r\n\r\nIf using `bazel`, the linking succeeds (as it successfully finds `__xla_cpu_runtime_EigenMatMulF64`). But then you still have the performance issues described in #41940.\r\n\r\nEven more confusing is the fact that for larger graphs containing `cholesky` and `triangular_solve` (like for example the ones generated via GPFlow to compute the [predictive mean and variance of GP's](https://github.com/GPflow/GPflow/blob/develop/gpflow/conditionals/util.py#L103)) `saved_model_cli` picks up `__xla_cpu_runtime_EigenSingleThreadedMatMulF64` but still suffer from the limitations of #41940.\r\n\r\nBest Regards,\r\n\r\nMarco", "@d0k any ideas why this ^ would happen?", "The only thing I can think of is that something overwrites the `xla_cpu_multi_thread_eigen` flag. I don't know what could do that yet.", "[This line](https://github.com/tensorflow/tensorflow/blob/768165d2bc8a596a5274bd026295cc28e0ca1f4f/tensorflow/python/tools/saved_model_aot_compile.py#L274) may not have the intended effect in windows?", "I'm not sure we ever test large enough matmuls in our unit tests to properly test that this env var is being set.  I can try a bigger unit test.  @sanjoy  Do we have any existing saved_models containing large variables/tensors in matmul?  [This one](https://github.com/tensorflow/tensorflow/blob/e483e7f29c1994b349ae8abff3986b75abe6e2ef/tensorflow/python/tools/BUILD#L354) is small", "> @sanjoy Do we have any existing saved_models containing large variables/tensors in matmul?\r\n\r\nNot sure if we have such models checked in.  Can we just use the test case in this issue?", "Yes; absolutely.  I may not get to it this week, but it should be easy to add a genrule that creates this savedmodel, and extend the aot_compiled_test rule to run it.", "> [This line](https://github.com/tensorflow/tensorflow/blob/768165d2bc8a596a5274bd026295cc28e0ca1f4f/tensorflow/python/tools/saved_model_aot_compile.py#L274) may not have the intended effect in windows?\r\n\r\nI just tested explicitly setting the env variable on Windows directly from ps and I still run into the same issue.", "I *think* https://github.com/tensorflow/tensorflow/commit/84967b39fa98d27f5984648f9ec47a159206cfda may help.  I'm not sure it resolves the env var issue in Windows (I can't debug, lacking a windows machine); but if you run with tf nightly and use `--multithreading true`, it should emit an object file containing the multithreaded matmul code.  You may need to include headers for `libnsync` when compiling + linking, and use a [ThreadPool](https://github.com/tensorflow/tensorflow/commit/84967b39fa98d27f5984648f9ec47a159206cfda#diff-007b11909ecd62d8a55d1880a4d38e9aa1e36382f2bbadd150ba17de5a884865R71) when running the compiled code.\r\n\r\nOn the other hand, you'll get multithreaded matmuls now.\r\n\r\nLMK if this doesn't resolve your issue; we can find someone with access to windows to debug the single threaded case as well.", "Hi @ebrevdo ,\r\nI had some free time today and I tried to compile the AOT sources shipped with the last tensorflow nightly 2.4 (tf-nightly 2.4.0.dev20201023).\r\n\r\nWhen compiling for 64bit everything works fine, without additional linking, and also the `--multithreading true` option works ok using a ThreadPool. Great work! :D \r\n\r\nHowever when I compile for 32bit I keep getting this error:\r\n```\r\nC:\\VirtualEnvs\\tf_nightly\\Lib\\site-packages\\tensorflow\\include\\Eigen\\src/Core/arch/AVX/PacketMath.h(707,49): error C3861: '_mm256_extract_epi64': identifier not found \r\n```\r\n\r\nDo you know where it may come from? ", "+Rasmus Larsen <rmlarsen@google.com> does eigen support 32bit?\n\nOn Wed, Oct 28, 2020, 1:46 AM Andrea Battistello <notifications@github.com>\nwrote:\n\n> Hi @ebrevdo <https://github.com/ebrevdo> ,\n> I had some free time today and I tried to compile the AOT sources shipped\n> with the last tensorflow nightly 2.4 (tf-nightly 2.4.0.dev20201023).\n>\n> When compiling for 64bit everything works fine, without additional\n> linking, and also the --multithreading true option works ok using a\n> ThreadPool. Great work! :D\n>\n> However when I compile for 32bit I keep getting this error:\n>\n> C:\\VirtualEnvs\\tf_nightly\\Lib\\site-packages\\tensorflow\\include\\Eigen\\src/Core/arch/AVX/PacketMath.h(707,49): error C3861: '_mm256_extract_epi64': identifier not found\n>\n> Do you know where it may come from?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41938#issuecomment-717786771>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG2ZDNFPRQFPVBWW35LSM7K47ANCNFSM4PQQ252A>\n> .\n>\n", "Just to add, with the shipped pip package of Tensorflow 2.3 I didn't have any problem compiling in 32bit with the same procedure (of course using `multithreading=false`)", "+George Karpenkov <cheshire@google.com> that's unexpected. I'm out of\noffice this week but maybe George can also take a look.\n\nOn Wed, Oct 28, 2020, 10:25 AM Andrea Battistello <notifications@github.com>\nwrote:\n\n> Just to add, with the shipped pip package of Tensorflow 2.3 I didn't have\n> any problem compiling in 32bit with the same procedure (of course using\n> multithreading=false)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41938#issuecomment-718089082>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG2NGRFOCZEAFR7CH2DSNBHYRANCNFSM4PQQ252A>\n> .\n>\n", "Can you give a full compile trace?\n\nOn Wed, Oct 28, 2020, 12:27 PM Eugene Brevdo <ebrevdo@google.com> wrote:\n\n> +George Karpenkov <cheshire@google.com> that's unexpected. I'm out of\n> office this week but maybe George can also take a look.\n>\n> On Wed, Oct 28, 2020, 10:25 AM Andrea Battistello <\n> notifications@github.com> wrote:\n>\n>> Just to add, with the shipped pip package of Tensorflow 2.3 I didn't have\n>> any problem compiling in 32bit with the same procedure (of course using\n>> multithreading=false)\n>>\n>> \u2014\n>> You are receiving this because you were mentioned.\n>> Reply to this email directly, view it on GitHub\n>> <https://github.com/tensorflow/tensorflow/issues/41938#issuecomment-718089082>,\n>> or unsubscribe\n>> <https://github.com/notifications/unsubscribe-auth/AANWFG2NGRFOCZEAFR7CH2DSNBHYRANCNFSM4PQQ252A>\n>> .\n>>\n>\n", "Well the compiler output doesn't say very much. I'm using CMake with Visual Studio 2019 to compile the XLA AOT sources shipped with the package:\r\n\r\n```\r\n**********************************************************************\r\n** Visual Studio 2019 Developer Command Prompt v16.6.5\r\n** Copyright (c) 2020 Microsoft Corporation\r\n**********************************************************************\r\n[vcvarsall.bat] Environment initialized for: 'x86'\r\n\r\n\r\n    Directory: C:\\Projects\\aot\\aot\r\n\r\n\r\nMode                LastWriteTime         Length Name                                                                                                 \r\n----                -------------         ------ ----                                                                                                 \r\nd-----         27/10/20     18:56                buildtf32                                                                                            \r\n\r\n\r\n    Directory: C:\\Projects\\aot\\aot\\buildtf32\r\n\r\n\r\nMode                LastWriteTime         Length Name                                                                                                 \r\n----                -------------         ------ ----                                                                                                 \r\nd-----         27/10/20     18:56                cmake                                                                                                \r\n-- Building for: Visual Studio 16 2019\r\n-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.17763.\r\n-- The C compiler identification is MSVC 19.26.28806.0\r\n-- The CXX compiler identification is MSVC 19.26.28806.0\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.26.28801/bin/Hostx64/x86/cl.exe - skipped\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.26.28801/bin/Hostx64/x86/cl.exe - skipped\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Configuring done\r\n-- Generating done\r\n-- Build files have been written to: C:/Projects/aot/aot/buildtf32\r\nMicrosoft (R) Build Engine version 16.6.0+5ff7b0c9e for .NET Framework\r\nCopyright (C) Microsoft Corporation. All rights reserved.\r\n\r\n  Checking Build System\r\n  Building Custom Rule C:/VirtualEnvs/tf_nightly/Lib/site-packages/tensorflow/xla_aot_runtime_src/CMakeLists.txt\r\n\r\nC:\\VirtualEnvs\\tf_nightly\\Lib\\site-packages\\tensorflow\\include\\Eigen\\src/Core/arch/AVX/PacketMath.h(707,49): error C3861: '_mm256_extract_epi64': identifier not found [C:\\Projects\\aot\\aot\\buildtf32\\tf_xla_runtime_objects.vcxproj]\r\n[repeated several times]\r\n```\r\n\r\nThe compiler generates a library, but then when I link it with the lib generated with `saved_model_cli` it gives me this error:\r\n```\r\nC:\\VirtualEnvs\\tf_nightly\\Lib\\site-packages\\tensorflow\\include\\Eigen\\src/Core/arch/AVX/PacketMath.h(707,49): error C3861: '_mm256_extract_epi64': iden\r\ntifier not found\r\n```\r\n\r\nI don't know if it is something missing on my side or in the sources shipped with the pip package, but anyway this did work properly for tensorflow 2.3..", "I believe `_mm256_extract_epi64` is not available on 32bit (see another instance of the same issue: https://github.com/pytorch/pytorch/issues/40988).\r\n\r\nProbably Eigen should have some `#ifdef` logic to not use this symbol when building for 32bit.  @rmlarsen WDYT?", "Hi, \r\nWe managed to get it compiled with 32bit by disabling AVX, as described in this [issue](https://github.com/pytorch/pytorch/issues/17901)\r\n\r\nHope it helps..", "+1 this sounds like an issue in Eigen not detecting 32bit systems.", "Hi @battuzz! \r\nWe are checking to see whether you still need help in this issue . Please create a new issue if the issue is replicating in Latest version TF 2.6 . Thanks!", "Hi @mohantym , thanks for coming back to this issue!\r\n\r\nI've tried with TF 2.6 and it should work fine now... I don't have this error anymore and the compilation runs successfully, even though the performance of the compiled version of this cholesky decomposition seems quite slow on CPU..", "I suggest opening a separate bug for the performance of cholesky on CPU.  I\ndon't think anyone's bothered optimizing it, it's likely implemented as a\nseries of loops in HLO.\n\nOn Fri, Nov 5, 2021 at 8:15 AM Andrea Battistello ***@***.***>\nwrote:\n\n> Hi @mohantym <https://github.com/mohantym> , thanks for coming back to\n> this issue!\n>\n> I've tried with TF 2.6 and it should work fine now... I don't have this\n> error anymore and the compilation runs successfully, even though the\n> performance of the compiled version of this cholesky decomposition seems\n> quite slow on CPU..\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41938#issuecomment-961980695>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AANWFG76JKEJLKHEY7SJ33DUKPYI3ANCNFSM4PQQ252A>\n> .\n>\n", "@ebrevdo Ok, thanks!", "Ok @battuzz! Closing this issue as it seems to be resolved . Please create a new issue if you need further assistance . Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41938\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41938\">No</a>\n"]}, {"number": 41937, "title": "tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error", "body": "I am trying to build an image with Centos and tensorflow 1.14 GPU, I have installed in the host the Cuda 10 and tried also 11, in the image i tried 10 and also 11 , different versions and I keep getting below error\r\n\r\ntensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n\r\nFull Error:\r\n>>> import tensorflow as tf\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n/opt/rh/rh-python36/root/usr/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n2020-07-31 16:03:16.558174: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2020-07-31 16:03:16.562049: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2020-07-31 16:03:16.562109: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: c15680640844\r\n2020-07-31 16:03:16.562130: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: c15680640844\r\n2020-07-31 16:03:16.562276: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.152.0\r\n2020-07-31 16:03:16.562335: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.152.0\r\n2020-07-31 16:03:16.562359: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.152.0\r\nNum GPUs Available:  0\r\n\r\nI tried nvidia-modprobe, check /dev/nvidia script, install different cudn, nvidia-smi working fine...etc\r\n\r\nSystem Info:\r\n\r\nPython 3.6.9\r\ntensorflow-estimator (1.14.0)\r\ntensorflow-gpu (1.14.0)\r\n\r\nkmod-nvidia-latest-dkms-418.152.00-1.el7.x86_64\r\nnvidia-modprobe-latest-418.152.00-1.el7.x86_64\r\nnvidia-driver-latest-NvFBCOpenGL-418.152.00-1.el7.x86_64\r\nnvidia-driver-latest-devel-418.152.00-1.el7.x86_64\r\nnvidia-driver-latest-418.152.00-1.el7.x86_64\r\nnvidia-driver-local-repo-rhel7-418.152.00-1.0-1.x86_64\r\nnvidia-driver-latest-cuda-libs-418.152.00-1.el7.x86_64\r\nnvidia-xconfig-latest-418.152.00-1.el7.x86_64\r\n\r\nFri Jul 31 16:02:21 2020\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 418.152.00   Driver Version: 418.152.00   CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla M60           Off  | 00000000:00:1B.0 Off |                    0 |\r\n| N/A   39C    P0    42W / 150W |      0MiB /  7618MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla M60           Off  | 00000000:00:1C.0 Off |                    0 |\r\n| N/A   35C    P0    41W / 150W |      0MiB /  7618MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  Tesla M60           Off  | 00000000:00:1D.0 Off |                    0 |\r\n| N/A   36C    P0    47W / 150W |      0MiB /  7618MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  Tesla M60           Off  | 00000000:00:1E.0 Off |                    0 |\r\n| N/A   35C    P0    47W / 150W |      0MiB /  7618MiB |     53%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|  No running processes found                                                 |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n\r\n\r\nThis template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@Shikaz,\r\nTensorFlow v1.14 is not actively supported. \r\n\r\nPlease try installing TensorFlow 2.x or v1.15 and make sure you have the compatible dependency versions installed as per the [tested build configurations](https://www.tensorflow.org/install/source#gpu). Thanks!", "> @Shikaz,\r\n> TensorFlow v1.14 is not actively supported.\r\n> \r\n> Please try installing TensorFlow 2.x or v1.15 and make sure you have the compatible dependency versions installed as per the [tested build configurations](https://www.tensorflow.org/install/source#gpu). Thanks!\r\n\r\n@amahendrakar I don't think this is the issue.\r\n\r\nI tried the following and same error:\r\n\r\ngcc version 7.3.1 20180303 (Red Hat 7.3.1-5) (GCC)\r\n\r\ntensorflow-gpu                2.3.0\r\n\r\ncuda-repo-rhel7-10-1-local-10.1.243-418.87.00-1.0-1.x86_64\r\ncuda-license-10-1-10.1.243-1.x86_64\r\nnvidia-driver-latest-cuda-libs-418.152.00-1.el7.x86_64\r\ncuda-cudart-10-1-10.1.243-1.x86_64\r\n\r\n**No meaningful error , is there a way to debug as failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error does not help much**\r\n\r\n\r\n>>> import tensorflow as tf\r\n2020-07-31 19:30:35.310489: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n>>> print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n2020-07-31 19:30:39.964043: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-31 19:30:39.967983: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n2020-07-31 19:30:39.968040: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: c15680640844\r\n2020-07-31 19:30:39.968061: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: c15680640844\r\n2020-07-31 19:30:39.968189: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.152.0\r\n2020-07-31 19:30:39.968248: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.152.0\r\n2020-07-31 19:30:39.968271: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.152.0\r\nNum GPUs Available:  0\r\n\r\n", "I tried to instead of do pip install the gpu tensorflow to build using bazel and still no luck.", "@Shikaz,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/40920#issuecomment-653279389) from a similar issue and let us know if it helps. Thanks!", "> @Shikaz,\r\n> Please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/40920#issuecomment-653279389) from a similar issue and let us know if it helps. Thanks!\r\n\r\n@amahendrakar - This is different error, i am getting CUDA_ERROR_UNKNOWN: unknown error , the one you shared is CUDA_ERROR_NO_DEVICE which was fixed by updating the driver.\r\n\r\nIs installing on Centos differs than Ubuntu ? also is there a way to debug or get more insight on what CUDA_ERROR_UNKNOWN means", "@Shikaz,\r\nThe [installation steps](https://www.tensorflow.org/install/source) for all Linux distributions are the same. \r\n\r\nCould you please provide the complete code you are running. Also, check [#32623](https://github.com/tensorflow/tensorflow/issues/32623) and [#7653](https://github.com/tensorflow/tensorflow/issues/7653) and let us know if it works. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "> > @Shikaz,\r\n> > TensorFlow v1.14 is not actively supported.\r\n> > Please try installing TensorFlow 2.x or v1.15 and make sure you have the compatible dependency versions installed as per the [tested build configurations](https://www.tensorflow.org/install/source#gpu). Thanks!\r\n> \r\n> @amahendrakar I don't think this is the issue.\r\n> \r\n> I tried the following and same error:\r\n> \r\n> gcc version 7.3.1 20180303 (Red Hat 7.3.1-5) (GCC)\r\n> \r\n> tensorflow-gpu 2.3.0\r\n> \r\n> cuda-repo-rhel7-10-1-local-10.1.243-418.87.00-1.0-1.x86_64\r\n> cuda-license-10-1-10.1.243-1.x86_64\r\n> nvidia-driver-latest-cuda-libs-418.152.00-1.el7.x86_64\r\n> cuda-cudart-10-1-10.1.243-1.x86_64\r\n> \r\n> **No meaningful error , is there a way to debug as failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error does not help much**\r\n> \r\n> > > > import tensorflow as tf\r\n> > > > 2020-07-31 19:30:35.310489: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n> > > > print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n> > > > 2020-07-31 19:30:39.964043: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n> > > > 2020-07-31 19:30:39.967983: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\r\n> > > > 2020-07-31 19:30:39.968040: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: c15680640844\r\n> > > > 2020-07-31 19:30:39.968061: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: c15680640844\r\n> > > > 2020-07-31 19:30:39.968189: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.152.0\r\n> > > > 2020-07-31 19:30:39.968248: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.152.0\r\n> > > > 2020-07-31 19:30:39.968271: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.152.0\r\n> > > > Num GPUs Available:  0\r\n\r\nDear @Shikaz, How did you manage to solve your issue as i am also facing the same. and getting no solution anywhere. It really would be helpful if you could share the details. \r\nThanks, ", "@hlmhlr,\r\nCould you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!", "> @hlmhlr,\r\n> Could you please submit a new issue from [this link](https://github.com/tensorflow/tensorflow/issues/new/choose) and fill in the template, so that we can track the issue there. Thanks!\r\n\r\nDear @amahendrakar, Thanks for your response. Since i didn't get any feedback at the time of error and i restarted the machine just to give a one more try and luckily it worked without error. But it is still puzzled for me that why  this error occurred.\r\nThanks, "]}, {"number": 41936, "title": "[XLA] When LLVM doesn't know a CC that is more recent, warn only to developers.", "body": "The end user can't do anything about it and this confuse them as they think they have an not-optimal software stack that they should fix.\r\n\r\n@sanjoy @thomasjoerg ", "comments": ["I amended the commit. It is true that I didn't take as much coffee today then usually :) I'll get one more now!", "@nouiz Can you please resolve conflicts? Thanks!\r\n", "> @nouiz Can you please resolve conflicts? Thanks!\r\n\r\nJust input that Frederic is currently on vacation. Let me know if there is any urgent issues and I will help to deal with them; otherwise let's wait for his return.\r\n", "I updated this PR.", "I amended the commit for both comments."]}, {"number": 41935, "title": "Finalize porting S3", "body": "@mihaimaruseac \r\nThis PR adds all the missing parts from `core/platform/s3`. We now finish porting s3 to `c_api`.\r\n\r\nActually, We are missing a logging mechanism `LOG(INFO)`, etc. Could we use `std::cout` ?", "comments": ["> Question: Does S3 guarantee that only `/` can be used as a path separator and never `\\\\`?\r\n\r\nYes. S3 Uri will have a form of `s3://<bucket>/<object>`", "Apologies, I missed the `std::cout` question. It's preferable to use logging so that users would be able to hide the logs / integrate with custom logging solutions. But since C API does not offer logging support at the moment, I am unsure what we can do.\r\n\r\nAdding a C API wrapper around the logging macros will also be a nice addition"]}, {"number": 41934, "title": "tf.image.resize with bilinear breaks mixed precision", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04\r\n- TensorFlow installed from (source or binary):conda\r\n- TensorFlow version (use command below):2.1/2.2/2.3\r\n- Python version:3.7\r\n- CUDA/cuDNN version:10.1\r\n- GPU model and memory:v100 32GB\r\n\r\nUse tf.image.resize with BILINEAR upsampling breaks mixed precision and makes the speed much slower than fp32. Change to NEAREST_NEIGHBOR works\r\n", "comments": ["@edwardyehuang \r\nPlease update the issue template with exact tensorflow version and sample code for us to replicate the issue faced or if possible share colab gist with error to analyse.", "@Saduf2019 \r\n\r\nhttps://colab.research.google.com/drive/1G78SCjm9UVvZGHsqM6ki4LfdT-1dHDJJ?usp=sharing\r\n\r\nMake sure you are using Tesla T4 in colab to test mixed precision, or you may use a local GPU with tensor core.\r\n\r\nAdjust \"ENABLE_MIXED_PRECISION\"  and \"UPSAMPLE_METHOD\" to verify the effect.\r\n\r\nFP32 2s/batch vs FP16 4s/batch in BILINEAR", "@reedwm I see that you fixed this for tf 2.4 which is great.\r\nI was seeing the same effect in my networks and I hadn't pinned it down to upsampling.\r\n\r\nAnother problem I am noticing is that after a certain (large) number of iterations, nans start appearing in my trainig (I haven't figured out where first yet). Could this be related and should this fix the nan issue?", "@zaccharieramzi this fix does not affect NaNs or numerical stability, so unfortunately it's unlikely the NaN issue will be fixed. Unfortunately, NaN issues that only appear after a certain number of iterations are difficult to debug. Try making the loss of the model run in float32 if it doesn't already. Additionally, if the model ends in softmax, run the softmax in float32 as well. You can also try saving a checkpoint on the step before you get a NaN. Then you will be able to load the checkpoint and reproduce the NaN by just running one step."]}, {"number": 41933, "title": "Documentation fix of \"Text Classification with Movie Reviews\"", "body": "This is the same issue mentioned [here](https://github.com/tensorflow/tensorflow/issues/41413).\r\n\r\nCurrently [here](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_text_classification.ipynb#scrollTo=zXXx5Oc3pOmN) under 'Loss function and optimizer' it says:\r\n\r\n```\r\nmodel.compile(optimizer='adam',\r\n              loss=tf.losses.BinaryCrossentropy(from_logits=True),\r\n              metrics=['accuracy'])\r\n```\r\n\r\nThis needs to be corrected to:\r\n\r\n\r\n`model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[tf.metrics.BinaryAccuracy(threshold=0.0, name='accuracy')])`\r\n", "comments": []}, {"number": 41932, "title": "Only 1 subgraph is currently supported - CMSIS-NN", "body": "@tensorflow/micro\r\n\r\nSystem information\r\n\r\nHost OS Platform and Distribution (e.g., Linux Ubuntu 16.04): make with Linux Ubuntu 20.04, complied on Atmel Studio on Windows 10, Python 3.7.7\r\nTensorFlow installed from (source or binary): downloaded from master\r\nTensorflow version (commit SHA if source): 2.3.0, e544dce\r\nTarget platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Atmel SAMD51 - Atmel Studio\r\n\r\n**Describe the problem**\r\nAfter some issue to link correctly the TFLite with cmsis-nn kernels files into my Atmel Studio project I have built a static library with Atmel Studio and linked the .a file. To do that I make the project with TAGS=cmsis-nn, copy the the files from a Keil project (I used the image recognition example) from the make directory and compile with Atmel Studio to get the .a file.\r\n\r\nWhen I run inference compiling the C files (obtained without the cmsis-tag in the Tensorflow-Lite subdirectory of the image recognition example in the make folder) within the project everything works fine. When I run inference with the static library obtained from making with the cmsis-nn tag I get the following error: \"Only 1 subgraph is currently supported. Exiting with status 1\". The flatbuffer file is the same.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\nPlease refere to this Colab to see the python code: https://drive.google.com/file/d/1k-9Id6ljpf4EjlcYIy6D4-fAHwOg1hI-/view?usp=sharing\r\nPlease refere there to see the trained models: https://drive.google.com/drive/folders/1hQgInGGy3A7EEOB0xZHw5CgheXp8nl_9?usp=sharing\r\nPlease refere there for the project (in the cmsis-nn brench): https://github.com/Sixaxis9/TFLite-SAMD51\r\nReplicating the issue: copying and pasting the flatbuffer inside a working project with the cmsis-nn version of TFLite, loading the model and running inference.", "comments": ["I get the same error, \"Only 1 subgraph is currently supported. Exiting with status 1\" when trying to implement RNNs.\r\n[Issue: 36688](tensorflow/tflite-micro#907)\r\n", "Are you trying to implement it with CMSIS-NN or with the TF kernels? Are different subgraph visible via tools like Netron?", "Update on the issue. I manage to solve the problem and to compile the C files within the uC project. I run the make with the TAGS=cmsis-nn and TARGET=sparkfun_edge (since my board has a Cortex M4-F, same as Sparkfun edge) and imported the folders \"tensorflow\" and \"third_party\" in the my project directory from the Kiel folder created by the makefile. After some dependancy fixing I successfully built the project.\nI now have another problem, my output are constant zeros, but I'll open another issue for that.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41932\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41932\">No</a>\n", "> I get the same error, \"Only 1 subgraph is currently supported. Exiting with status 1\" when trying to implement RNNs.\r\n> [Issue: 36688](tensorflow/tflite-micro#907)\r\n\r\nbro did u find sloution \r\n", "Hi ossa77, are you having troubles with RNNs? \r\nI might help on CNNs"]}, {"number": 41931, "title": "Error with TFlite hello world example on ESP-EYE", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Tensorflow version (commit SHA if source): 2.4.0\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP-EYE\r\n\r\n**Describe the problem**\r\n\r\nI am trying to setup the ESP-EYE for TFlite. I have been able to set up the ESP-EYE for basic setup as per: https://docs.espressif.com/projects/esp-idf/en/stable/get-started/index.html. I am also able to use the AWS IoT example https://github.com/espressif/esp-aws-iot/tree/master/examples.\r\n\r\nFor TFlite, I am trying out the example for ESP-EYE at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/README.md\r\n\r\nWhen I try to compile this per the instructions above for ESP-EYE, I am getting a compiler error:\r\n\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: all warnings being treated as errors\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nPer the steps at the TFlite link above:\r\n\r\nGenerate the examples The example project can be generated with the following command:\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\n\r\nBuilding the example Go the the example project directory\r\ncd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf\r\n\r\nThen build with idf.py\r\n\r\nidf.py build\r\nI get this error repeatedly:\r\n\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: all warnings being treated as errors\r\n\r\nThe error does go away if I remove -Werror in the below line in components/tfmicro/CMakeLists.txt:\r\n\r\ntarget_compile_options(${COMPONENT_LIB} PRIVATE $<$<COMPILE_LANGUAGE:CXX>: -std=c++11 -DTF_LITE_STATIC_MEMORY -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -DNDEBUG -O3 -Wno-return-type -Wno-strict-aliasing -Wno-ignored-qualifiers -Wno-return-type -Wno-strict-aliasing -Wno-ignored-qualifiers -Wno-return-type -Wno-strict-aliasing -Wno-return-type -Wno-strict-aliasing >)\r\n\r\nBut the image when flashed, does not work. It keeps crashing with a register dump.\r\n\r\nAny suggestions for this issue?", "comments": ["@rajeevdm \r\nPlease provide simple stand alone code for us to replicate the issue faced or share a colab gist with the error for us to analyse the issue.", "Thanks for the response. \r\n\r\nHere are the steps to replicate. \r\n\r\nSystem information\r\n\r\nHost OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\nTensorFlow installed from (source or binary): source\r\nTensorflow version (commit SHA if source): 2.4.0\r\nTarget platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ESP-EYE (the one mentioned in https://www.tensorflow.org/lite/microcontrollers/get_started)\r\n\r\nBasic setup of ESP-EYE documented here: https://docs.espressif.com/projects/esp-idf/en/v4.0.1/get-started/index.html (I have used the v4.0.1, latest stable release). I have followed the steps exactly as listed in the link there, except that some packages had to be manually installed on the host system. Here are the steps followed:\r\n\r\nBasic python and git install steps:\r\n1. sudo apt-get install python3-pip\r\n2. sudo apt-get install git\r\n3. mkdir esp\r\n4. cd ~/esp\r\n5. git clone -b v4.0.1 --recursive https://github.com/espressif/esp-idf.git\r\n6. cd ~/esp/esp-idf\r\n7. ./install.sh\r\nThis step, for some reason, does not install all required packages automatically. I had to manually install the following:\r\n8. sudo apt-get install cmake bison flex gperf libncurses5-dev libncursesw5-dev\r\n9. . $HOME/esp/esp-idf/export.sh (this step to be done in any shell/terminal that will use ESP IDF)\r\n\r\nStart a new hello_world project\r\n10. cd ~/esp\r\n11. cp -r $IDF_PATH/examples/get-started/hello_world .\r\n12. cd ~/esp/hello_world\r\n13. idf.py menuconfig\r\n14. idf.py build\r\n15. idf.py -p PORT [-b BAUD] flash\r\n16. idf.py -p /dev/ttyUSB0 monitor\r\n\r\nThis completes basic setup of ESP-EYE IDF and sample hello_world project from ESP. \r\n\r\nNow, getting to TFlite: I am trying out the example for ESP-EYE at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/README.md\r\n\r\nSteps followed are listed here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/README.md#deploy-to-esp32\r\n\r\n1. cd esp/examples\r\n2. git clone https://github.com/tensorflow/tensorflow.git\r\n3. cd tensorflow\r\n4. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\n5. cd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf\r\n6. . $HOME/esp/esp-idf/export.sh\r\n7. idf.py build\r\n\r\nError:\r\nrajeev@ubuntu:~/esp/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf$ idf.py build\r\nChecking Python dependencies...\r\nPython requirements from /home/rajeev/esp/esp-idf/requirements.txt are satisfied.\r\nExecuting action: all (aliases: build)\r\nRunning make in directory /home/rajeev/esp/tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf/build\r\nExecuting \"make -j 6 all\"...\r\n[  0%] Built target _project_elf_src\r\nPartition table binary generated. Contents:\r\n[  0%] Built target esp32_linker_script\r\n*******************************************************************************\r\n[  0%] Built target __idf_app_trace\r\n[  0%] Built target __idf_cxx\r\n# Espressif ESP32 Partition Table\r\n# Name, Type, SubType, Offset, Size, Flags\r\nnvs,data,nvs,0x9000,24K,\r\nphy_init,data,phy,0xf000,4K,\r\nfactory,app,factory,0x10000,1M,\r\n[  1%] Built target __idf_newlib\r\n*******************************************************************************\r\n[  2%] Built target partition_table\r\n[  2%] Built target __idf_vfs\r\n[  2%] Performing build step for 'bootloader'\r\n[  4%] Built target __idf_freertos\r\n[  1%] Built target _project_elf_src\r\n[  4%] Built target __idf_log\r\n[  4%] Built target __idf_heap\r\n[  4%] Built target __idf_log\r\n[ 12%] Built target __idf_xtensa\r\n[  7%] Built target __idf_soc\r\n[ 50%] Built target __idf_soc\r\n[  7%] Built target __idf_esp_rom\r\n[ 53%] Built target __idf_micro-ecc\r\n[  8%] Built target __idf_esp_common\r\n[ 79%] Built target __idf_bootloader_support\r\n[  9%] Built target __idf_xtensa\r\n[ 87%] Built target __idf_efuse\r\n[ 90%] Built target __idf_spi_flash\r\n[ 12%] Built target __idf_esp32\r\n[ 93%] Built target __idf_main\r\n[ 98%] Built target bootloader.elf\r\n[ 13%] Built target __idf_espcoredump\r\n[100%] Built target gen_project_binary\r\n[ 14%] Built target __idf_pthread\r\n[100%] Built target app\r\n[ 15%] Built target __idf_esp_event\r\n[ 15%] No install step for 'bootloader'\r\n[ 15%] Completed 'bootloader'\r\n[ 15%] Built target __idf_tcpip_adapter\r\n[ 16%] Built target bootloader\r\n[ 25%] Built target __idf_lwip\r\n[ 26%] Built target __idf_esp_eth\r\n[ 27%] Built target __idf_esp_wifi\r\n[ 28%] Built target __idf_nvs_flash\r\n[ 30%] Built target __idf_spi_flash\r\n[ 30%] Built target __idf_app_update\r\n[ 31%] Built target __idf_bootloader_support\r\n[ 31%] Built target __idf_efuse\r\n[ 41%] Built target __idf_wpa_supplicant\r\n[ 42%] Built target mbedtls\r\n[ 50%] Built target mbedcrypto\r\n[ 52%] Built target mbedx509\r\n[ 55%] Built target __idf_driver\r\n[ 55%] Built target __idf_esp_ringbuf\r\n[ 56%] Built target __idf_esp_adc_cal\r\n[ 56%] Built target __idf_console\r\n[ 59%] Built target __idf_nghttp\r\n[ 59%] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/simple_memory_allocator.cc.obj\r\n[ 60%] Built target __idf_protobuf-c\r\n[ 61%] Built target __idf_asio\r\n[ 62%] Built target __idf_coap\r\n[ 62%] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/all_ops_resolver.cc.obj\r\n[ 62%] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_error_reporter.cc.obj\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\n[ 63%] Built target __idf_esp_gdbstub\r\n[ 63%] Built target __idf_expat\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\n[ 63%] Built target __idf_mdns\r\n[ 63%] Building CXX object esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/memory_helpers.cc.obj\r\ncc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\ncc1plus: all warnings being treated as errors\r\nesp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/build.make:94: recipe for target 'esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_error_reporter.cc.obj' failed\r\nmake[2]: *** [esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/micro_error_reporter.cc.obj] Error 1\r\nmake[2]: *** Waiting for unfinished jobs....\r\n[ 64%] Built target __idf_wear_levelling\r\n[ 64%] Built target __idf_sdmmc\r\n[ 67%] Built target __idf_freemodbus\r\n[ 68%] Built target __idf_jsmn\r\n[ 81%] Built target __idf_libsodium\r\n[ 81%] Built target __idf_json\r\n[ 82%] Built target __idf_openssl\r\n[ 83%] Built target __idf_spiffs\r\n[ 83%] Built target __idf_ulp\r\n[ 84%] Built target __idf_unity\r\n[ 85%] Built target __idf_esp_http_server\r\n[ 85%] Built target __idf_esp-tls\r\n[ 87%] Built target __idf_fatfs\r\n[ 88%] Built target __idf_protocomm\r\n[ 89%] Built target __idf_tcp_transport\r\n[ 90%] Built target __idf_wifi_provisioning\r\n[ 91%] Built target __idf_esp_local_ctrl\r\n[ 91%] Built target __idf_esp_websocket_client\r\n[ 91%] Built target __idf_esp_http_client\r\n[ 92%] Built target __idf_mqtt\r\n[ 92%] Built target __idf_esp_https_ota\r\ncc1plus: all warnings being treated as errors\r\nesp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/build.make:81: recipe for target 'esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/simple_memory_allocator.cc.obj' failed\r\nmake[2]: *** [esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/simple_memory_allocator.cc.obj] Error 1\r\ncc1plus: all warnings being treated as errors\r\nesp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/build.make:120: recipe for target 'esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/memory_helpers.cc.obj' failed\r\nmake[2]: *** [esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/memory_helpers.cc.obj] Error 1\r\ncc1plus: all warnings being treated as errors\r\nesp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/build.make:107: recipe for target 'esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/all_ops_resolver.cc.obj' failed\r\nmake[2]: *** [esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/tensorflow/lite/micro/all_ops_resolver.cc.obj] Error 1\r\nCMakeFiles/Makefile2:4097: recipe for target 'esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/all' failed\r\nmake[1]: *** [esp-idf/tfmicro/CMakeFiles/__idf_tfmicro.dir/all] Error 2\r\nMakefile:148: recipe for target 'all' failed\r\nmake: *** [all] Error 2\r\nmake failed with exit code 2\r\n\r\nThe error goes away if I remove -Werror in the below line in components/tfmicro/CMakeLists.txt:\r\n\r\ntarget_compile_options(${COMPONENT_LIB} PRIVATE $<$<COMPILE_LANGUAGE:CXX>: -std=c++11 -DTF_LITE_STATIC_MEMORY -Werror -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -DNDEBUG -O3 -Wno-return-type -Wno-strict-aliasing -Wno-ignored-qualifiers -Wno-return-type -Wno-strict-aliasing -Wno-ignored-qualifiers -Wno-return-type -Wno-strict-aliasing -Wno-return-type -Wno-strict-aliasing >)\r\n\r\nBut the image when flashed, does not work. It keeps crashing with a register dump. Pls let me know if you want to see the register dump as well.\r\n\r\nThanks a lot.\r\n", "Also have this problem with building hello_world for ESP32 board. \r\nIf I deleted \"-std=c11\" flag from CMakeList in tfmicro directory building goes well, but then I got exception:\r\n\"Guru Meditation Error: Core  0 panic'ed (StoreProhibited). Exception was unhandled.\"\r\nExeption happens when line \"input->data.f[0] = x_val;\" from main_funstions.cc is executed. Looks like I'm trying to save data out of flash memory. ", "Yes, same error if I either delete \"-std=c11\" or \"-Werror\" (to not treat warnings as errors). From the TFlite documentation, it seems like C++11 standard is quite a fundamental requirement (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro - Requirements section). So we cannot remove \"std=c++11\" I think. ", "There is currently no fix other then to reverse to the previous Tensorflow version 2.2.0. That worked for me, it seems like an error that the dev team has to fix and results from a minor change. It's probably not your fault.", "adding @petewarden for issues related to the espressif integration.", "> There is currently no fix other then to reverse to the previous Tensorflow version 2.2.0. That worked for me, it seems like an error that the dev team has to fix and results from a minor change. It's probably not your fault.\r\n\r\nI tried checking out r2.2 with this:\r\ngit clone https://github.com/tensorflow/tensorflow tensorflow -b r2.2\r\n\r\nGot this error:\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\" \"7e8191b24853d75de2af87622ad293ba\" tensorflow/lite/micro/tools/make/downloads/gemmlowp  \r\ndownloading https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\" \"02c64880acb89dbd57eebacfd67200d8\" tensorflow/lite/micro/tools/make/downloads/flatbuffers  \r\ndownloading https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.11.0.tar.gz\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip\" \"8a7d2c70325f53136faea6dde517b8cc\" tensorflow/lite/micro/tools/make/downloads/person_model_int8  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_int8_grayscale_2020_01_13.zip\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://github.com/mborgerding/kissfft/archive/v130.zip\" \"438ba1fef5783cc5f5f201395cc477ca\" tensorflow/lite/micro/tools/make/downloads/kissfft patch_kissfft \r\ndownloading https://github.com/mborgerding/kissfft/archive/v130.zip\r\nFinished patching kissfft\r\ntensorflow/lite/micro/tools/make/download_and_extract.sh \"https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip\" \"fe2934bd0788f1dcc7af3f0a954542ab\" tensorflow/lite/micro/tools/make/downloads/person_model_grayscale  \r\ndownloading https://storage.googleapis.com/download.tensorflow.org/data/tf_lite_micro_person_data_grayscale_2019_11_21.zip\r\nmake: *** No rule to make target 'tensorflow/lite/micro/examples/hello_world/esp/sdkconfig.defaults', needed by 'tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf/sdkconfig.defaults'.  Stop.\r\n\r\nI will try going back one more version to 2.1\r\n", "So, in summary:\r\n\r\nTensorflow r2.1 does not have a micro directory. So quite likely support for microcontrollers was added later. \r\n\r\nWith r2.2, I get the error during make:\r\nmake: *** No rule to make target 'tensorflow/lite/micro/examples/hello_world/esp/sdkconfig.defaults', needed by 'tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf/sdkconfig.defaults'. Stop.\r\n\r\nWith r2.3, we get the issue with \"-std=c++11\" described earlier. \r\n\r\n@umgefahren - did you face any issues with compiling r2.2? \r\n\r\n", "r2.2 worked for me.\r\n[Link](https://github.com/tensorflow/tensorflow/releases/tag/v2.2.0)", "Ok, got it working with TF r2.2.\r\n\r\nFor the error:\r\nmake: *** No rule to make target 'tensorflow/lite/micro/examples/hello_world/esp/sdkconfig.defaults', needed by 'tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf/sdkconfig.defaults'. Stop.\r\n\r\nI just did:\r\ntouch tensorflow/lite/micro/examples/hello_world/esp/sdkconfig.defaults\r\n\r\nThen\r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\ncd tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf\r\nidf.py build\r\nidf.py -p /dev/ttyUSB0 flash\r\nidf.py monitor\r\n\r\nThat seems to work, I see continuous output like this:\r\nx_value: 1.2566366*2^-2, y_value: 1.2133837*2^-2\r\nx_value: 1.5707957*2^0, y_value: 1.9224760*2^-1\r\nx_value: 1.8849551*2^0, y_value: 1.8043820*2^-1\r\nx_value: 1.995567*2^1, y_value: 1.5224055*2^-1\r\nx_value: 1.2566366*2^1, y_value: 1.1094990*2^-1\r\nx_value: 1.4137159*2^1, y_value: 1.374219*2^-2\r\nx_value: 1.5707957*2^1, y_value: -1.4249528*2^-5\r\nx_value: 1.7278753*2^1, y_value: -1.4295994*2^-2\r\nx_value: 1.8849551*2^1, y_value: -1.2867725*2^-1\r\nx_value: 1.210171*2^2, y_value: -1.7542459*2^-1\r\nx_value: 1.995567*2^2, y_value: -1.9749510*2^-1\r\n\r\nTFlite team - It will be good to update the build instructions and/or put in an errata/known issues / fixes.\r\n\r\nWill continue to other examples now. ", "Upon closer inspection, it looks like we have two errors related to build flags when I follow the repro steps.\r\n1. We are enabling strict pointer aliasing with -Werror=strict-aliasing, which fails for some of our core files (specifically tensorflow/lite/micro/micro_string.h, I will look into fixing this aliasing error). We should be able to turn off strict aliasing with a compiler flag.\r\n2. We are building with -std=c11 which is invalid for c++. This should be a relatively simple compiler flag fix as well.\r\n\r\nDo those errors match with what you're seeing? I do see micro_allocator.cc, micro_interpreter.cc, etc. failing, but I think those fail because they are cut short by the micro_string.cc error on a different thread.\r\n\r\nPete is much more familiar with the Makefile and relevant compiler flags, so he may be able to weigh in with more specific fixes when he is available to do so.", "Thanks Nat.\u00a0\r\n\r\nPlease let us know if you make progress with the issues on the latest branch. I think it will be important as more hardware start supporting Tflite and also MLPerf.org starts publishing TinyML benchmarks, which is already in the works.\u00a0\r\n\r\nFor now, I will stick to R2.2 on my ESP-EYE device. \r\n\r\nRegards\r\nRajeev", "To not clone similar bug reports...\r\n**Unable to build \"Hello World\" for ESP32**\r\nI have received a similar error just trying to compile the \"Hello World\" example.\r\n\r\n**Software environment to reproduce:**\r\n- TF sources commit hash: aee9ca5dbd\r\n- ESP-IDF v4.0.1\r\n\r\n**Steps to reproduce:**\r\n1. Clone TF sources repo\r\n2. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\n3. cd tensorflow/tensorflow/lite/micro/tools/make/gen/esp_xtensa-esp32/prj/hello_world/esp-idf\r\n4. idf.py build\r\n\r\n**Most informative part of error message:**\r\n\r\n`cc1plus: error: command line option '-std=c11' is valid for C/ObjC but not for C++ [-Werror]\r\n../components/tfmicro/tensorflow/lite/micro/micro_string.cc: In function 'char* {anonymous}::FastFloatToBufferLeft(float, char*)':\r\n../components/tfmicro/tensorflow/lite/micro/micro_string.cc:128:23: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]\r\n   const uint32_t u = *reinterpret_cast<uint32_t*>(&f);\r\n                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncc1plus: all warnings being treated as errors\r\n`", "I have tried to build Hello World with TF sources SHA 387414f.\r\nNow compilation is successful but the application always cyclically rebooting.\r\nLogs attached.\r\n[Cyclically_rebooting__Hello_World__for_ESP32.txt](https://github.com/tensorflow/tensorflow/files/5089943/Cyclically_rebooting__Hello_World__for_ESP32.txt)\r\n", "@rajeevdm   This issue has been fixed with [this PR](https://github.com/tensorflow/tensorflow/pull/42275), Please check. If the problem is solved, you may close the issue.\r\nThank you.", "@rajeevdm Could you please let us know if this issue has been fixed with  PR. If the problem is solved, you may close the issue.\r\nThank you.", "Hi team\n\nThanks a lot. Yes this can be closed now. Sorry for the delay in my\nresponse.\n\nRajeev\n\nOn Fri, Jul 16, 2021, 3:37 AM sushreebarsa ***@***.***> wrote:\n\n> @rajeevdm <https://github.com/rajeevdm> Could you please let us know if\n> this issue has been fixed with this PR. If the problem is solved, you may\n> close the issue.\n> Thank you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41931#issuecomment-880878321>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/ALTU3457VJ35LPZYOVB6E6LTX4KNNANCNFSM4PP4JZ6A>\n> .\n>\n", "@rajeevdm thank you for your response ,closing this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41931\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41931\">No</a>\n"]}, {"number": 41930, "title": "Unusable code in the Overfit and Underfit tutotial for beginners", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/tutorials/keras/overfit_and_underfit\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\n\r\nThe unbatch() method is apparently deprecated, hence TF v. 1.14.0 (used in Jupyter Notebook) complains about the following line\r\n`packed_ds = ds.batch(10000).map(pack_row).unbatch()`\r\n\r\nThe error:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-37-58db46f67dc2> in <module>\r\n----> 1 packed_ds = ds.batch(10000).map(pack_row).unbatch()\r\n\r\nAttributeError: 'DatasetV1Adapter' object has no attribute 'unbatch'\r\n```\r\nI hope this helps to improve the tutorial!", "comments": ["The issue was with my version of TF. The upgrade to 2.3.0 solved the problem."]}, {"number": 41928, "title": "Introduce the aarch64 ARM release CI", "body": "This introduces 2 stable version for releases whl packages. For now, all of them are Tensorflow CPU aarch64 packages.\r\nOne for stable 1.15.3 version, support py35, py36 and py37 packages.\r\nThe other for stable 2.1.0 version, support py35, py36, py37 and py38 packages.\r\n\r\nIn this PR, including 2 parts,\r\nThe first one is fixing the https://github.com/theopenlab/openlab/issues/559 , so we will only maintain 1 single build per day for building tensorflow master branch with py36 in Nightly build show.\r\nThe second one is introducing the aarch64 release CI, which mentioned in https://github.com/theopenlab/openlab/issues/560", "comments": ["recheck", "Hi team, @mihaimaruseac , @rthadur and @gbaned .\r\nSorry, how to escape the above CI errors? As I don't think the issue is related with the README.MD changes. Could you please tell us how to do in the next? As it's useless via `recheck` or `reopen the PR`. Thank you very much"]}, {"number": 41927, "title": "[Docs] Updated docs for tf.custom_gradient with examples", "body": "There is no proper explanation of `dy` in the docs. So, I added [that](https://stackoverflow.com/questions/63146831/what-is-the-analytic-interpretation-for-tensorflow-custom-gradient/63187019#63187019).\r\n\r\nAlso added an example for [multiple inputs](https://stackoverflow.com/a/57379955/1217998)", "comments": ["@Ghost---Shadow can you please check sanity build errors ?", "@rthadur Please trigger the pipeline. \ud83e\udd1e\ud83e\udd1e\ud83e\udd1e", "@rthadur `Linux GPU` is failing with `Not found: Container localhost does not exist. (Could not find resource: localhost/)`\r\n\r\nhttps://source.cloud.google.com/results/invocations/fdcd7455-4ae4-44c7-aabd-c2c07586bdf4/targets/%2F%2Ftensorflow%2Fc%2Feager:c_api_distributed_test_gpu/log\r\n\r\nWill re-triggering the job fix the issue?"]}, {"number": 41926, "title": "Tensorflow-2.3.0 build failed", "body": "**System information**\r\n- OS Platform and Distribution :Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version:  Cuda10.0, cudnn 7.6.5.32\r\n- GPU model and memory: GTX1070 , 8G\r\n\r\n**Describe the problem**\r\nCannot build tensorflow\r\n\r\nTensorflow configured with cuda and tensorrt enabled\r\n\r\nbuild with\r\n\r\n```\r\nbazel build --config=opt --config=cuda --copt=-mavx --copt=-mavx2 --copt=-msse4.1 --copt=-msse4.2 --copt=-mfma --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package \r\n```\r\n\r\nbuild failed with these output\r\n\r\n```\r\nINFO: From ProtoCompile tensorflow/lite/toco/toco_flags.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nERROR: /home/bernard/opt/python38/tensorflow-2.3.0/tensorflow/core/kernels/BUILD:5123:1: C++ compilation of rule '//tensorflow/core/kernels:dilation_ops_gpu' failed (Exit 1)\r\nexternal/com_google_absl/absl/time/internal/cctz/include/cctz/civil_time_detail.h: In function \u2018constexpr absl::lts_2020_02_25::time_internal::cctz::detail::civil_day absl::lts_2020_02_25::time_internal::cctz::detail::next_weekday(absl::lts_2020_02_25::time_internal::cctz::detail::civil_day, absl::lts_2020_02_25::time_internal::cctz::detail::weekday)\u2019:\r\nexternal/com_google_absl/absl/time/internal/cctz/include/cctz/civil_time_detail.h:567:20: error: call to non-constexpr function \u2018absl::lts_2020_02_25::time_internal::cctz::detail::civil_time<absl::lts_2020_02_25::time_internal::cctz::detail::day_tag> absl::lts_2020_02_25::time_internal::cctz::detail::operator+(absl::lts_2020_02_25::time_internal::cctz::detail::civil_time<absl::lts_2020_02_25::time_internal::cctz::detail::day_tag>, absl::lts_2020_02_25::time_internal::cctz::diff_t)\u2019\r\n           return cd + (j - i);\r\n                    ^\r\nexternal/com_google_absl/absl/time/internal/cctz/include/cctz/civil_time_detail.h: In function \u2018constexpr absl::lts_2020_02_25::time_internal::cctz::detail::civil_day absl::lts_2020_02_25::time_internal::cctz::detail::prev_weekday(absl::lts_2020_02_25::time_internal::cctz::detail::civil_day, absl::lts_2020_02_25::time_internal::cctz::detail::weekday)\u2019:\r\nexternal/com_google_absl/absl/time/internal/cctz/include/cctz/civil_time_detail.h:587:20: error: call to non-constexpr function \u2018absl::lts_2020_02_25::time_internal::cctz::detail::civil_time<absl::lts_2020_02_25::time_internal::cctz::detail::day_tag> absl::lts_2020_02_25::time_internal::cctz::detail::operator-(absl::lts_2020_02_25::time_internal::cctz::detail::civil_time<absl::lts_2020_02_25::time_internal::cctz::detail::day_tag>, absl::lts_2020_02_25::time_internal::cctz::diff_t)\u2019\r\n           return cd - (j - i);\r\n                    ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 1001.142s, Critical Path: 93.36s\r\nINFO: 4993 processes: 4993 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n\r\n", "comments": ["@beew \r\nPlease refer to [this issue](https://github.com/tensorflow/tensorflow/issues/22082#issuecomment-451908492) with same error and update us. [[link](https://stackoverflow.com/questions/52437344/bazel-build-failed-after-updated-command-line-tool-in-mac)]", "@Saduf2019\r\n\r\nNot work, updated setuptools to the latest version (49.2.1), failed again, similar but different error\r\n\r\n```\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/bernard/opt/python38/tensorflow-2.3.0/tensorflow/python/tools/BUILD:226:1 C++ compilation of rule '//tensorflow/core/kernels:matrix_diag_op_gpu' failed (Exit 1)\r\nINFO: Elapsed time: 2415.388s, Critical Path: 105.97s\r\nINFO: 7622 processes: 7622 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nShould add that I have the same setup with bazel 2.0 and was able to build tf 2.2.0 without a hitch.", "Facing the same issue on CI where we also use Ubuntu 16.04 + GCC 5.4.0\r\n", ">  Please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/22082#issuecomment-451908492) with same error and update us. [[link](https://stackoverflow.com/questions/52437344/bazel-build-failed-after-updated-command-line-tool-in-mac)]\r\n\r\n@Saduf2019 Your two links are completely different issues.", "According to https://github.com/HowardHinnant/date/issues/477 it might be because of GCC itself.", "@lissyx \r\n\r\nI have tried multiple versions of gcc but still not working.\r\n\r\nI tried to compile with gcc-9.3.0 installed in custom location but bazel said\r\n```\r\nERROR: /home/bernard/opt/python38/tensorflow-2.3.0/tensorflow/core/kernels/BUILD:1232:1: C++ compilation of rule '//tensorflow/core/kernels:quantize_and_dequantize_op_gpu' failed (Exit 1)\r\nIn file included from /home/bernard/opt/cuda-10.0/cuda/bin/..//include/cuda_runtime.h:83,\r\n                 from <command-line>:\r\n/home/bernard/opt/cuda-10.0/cuda/bin/..//include/crt/host_config.h:129:2: error: #error -- unsupported GNU version! gcc versions later than 7 are not supported!\r\n  129 | #error -- unsupported GNU version! gcc versions later than 7 are not supported!\r\n      |  ^~~~~\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/bernard/opt/python38/tensorflow-2.3.0/tensorflow/python/tools/BUILD:82:1 C++ compilation of rule '//tensorflow/core/kernels:quantize_and_dequantize_op_gpu' failed (Exit 1)\r\n\r\n\r\n```\r\nI then tried gcc-7.5 and it said libstdc++.so.6 is missing GLIBCXX_3.4.26 (which is supported for libstdc++.so.6 in gcc-9.3.0)\r\n", "> I have tried multiple versions of gcc but still not working.\r\n\r\nUsing backported gcc-6 from ubuntu toolchain team worked for me: https://launchpad.net/~ubuntu-toolchain-r/+archive/ubuntu/test?field.series_filter=xenial\r\n\r\nJust install gcc-6, g++-6 and either setup alternatives for gcc/g++ or `CC=gcc-6 CXX=g++-6` should do the trick.", "@beew You can also take a look at the header, there is support for disabling `constexpr` in some cases, I might use that and add a hack.", "@lissyx \r\nHi,\r\n\r\nCompiling with gcc-6.5 works, however, update alternatives is not enough,  I got the error\r\n```\r\n/usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.22' not found \r\n\r\n```\r\nSo I had to make a conf file in /etc/ld.so.conf.d with /path/to/gcc-6.5.0/lib64 followed by sudo ldconfig\r\n\r\nBuild was successful, but I am wondering if it will have other side effects by leaving this version of libstdc++.so.6 in the ld path? (tensorflow 2.3 won't run without it)\r\n\r\nThanks", "@beew \r\nPlease refer to these resolved issues for the above missing file error:\r\n[link](https://github.com/lhelontra/tensorflow-on-arm/issues/13) [link1](https://stackoverflow.com/questions/43070900/version-glibcxx-3-4-22-not-found) [link2](https://github.com/EOSIO/eosio.cdt/issues/285) [link3](https://community.rstudio.com/t/libstdc-so-6-version-glibcxx-3-4-22-not-found/63472/6) and update us.", "@Saduf2019 The issue is clearly just that this cannot build with GCC 5.4. I have been able to do so with a little nasty patch: https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-18a66e25fbcbd72730332a799ddee2ec\r\n\r\nThis does had a define `NO_CONSTEXPR_FOR_YOU` that I can set at build time and leverage the ability to build this part without `constexpr` at all.", "> Build was successful, but I am wondering if it will have other side effects by leaving this version of libstdc++.so.6 in the ld path? (tensorflow 2.3 won't run without it)\r\n\r\nThat depends on your needs.", "@lissyx\r\n> > Build was successful, but I am wondering if it will have other side effects by leaving this version of libstdc++.so.6 in the ld path? (tensorflow 2.3 won't run without it)\r\n> \r\n> That depends on your needs.\r\n\r\nHi, My question is really whether the presence of a higher  libstdc++so.6 in the library path would interfere with projects compiled with a lower version. All the links provided by @Saduf2019  involve upgrading libstdc++so.6 one way or the other basically like what I have done, by making a config file in /etc/ld.so.conf.d, except I keep the older system version as well.\r\n\r\nI could set LD_LIBRARY_PATH at runtime but it is kind of cumbersome (or by adding it to LD_LIBRARY_PATH in the script I invoke to launch python-3.8 but it has the same concern since I have compiled quite a few python packages from source) BTW setting LD_LIBRARY_PATH doesn't work for compile time since bazel couldn't find it, only adding to /etc/ld.so.conf.d works.\r\n", "> @Saduf2019 The issue is clearly just that this cannot build with GCC 5.4. I have been able to do so with a little nasty patch: [mozilla@6fad14b#diff-18a66e25fbcbd72730332a799ddee2ec](https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-18a66e25fbcbd72730332a799ddee2ec)\r\n> \r\n> This does had a define `NO_CONSTEXPR_FOR_YOU` that I can set at build time and leverage the ability to build this part without `constexpr` at all.\r\n\r\nI've been facing the same error with gcc 5.4 but I can't update my gcc as anaconda doesn't have 6.x packages for gcc/gxx. I tried above mentioned patch as well as defined the macro `NO_CONSTEXPR_FOR_YOU` in bazelrc as below, still same error -\r\n```\r\nbuild:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\nbuild --define=NO_CONSTEXPR_FOR_YOU=1\r\n```\r\nIt looks like the way I've defined it isn't right. Could someone please help me define this macro?\r\n", "> > @Saduf2019 The issue is clearly just that this cannot build with GCC 5.4. I have been able to do so with a little nasty patch: [mozilla@6fad14b#diff-18a66e25fbcbd72730332a799ddee2ec](https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-18a66e25fbcbd72730332a799ddee2ec)\r\n> > This does had a define `NO_CONSTEXPR_FOR_YOU` that I can set at build time and leverage the ability to build this part without `constexpr` at all.\r\n> \r\n> I've been facing the same error with gcc 5.4 but I can't update my gcc as anaconda doesn't have 6.x packages for gcc/gxx. I tried above mentioned patch as well as defined the macro `NO_CONSTEXPR_FOR_YOU` in bazelrc as below, still same error -\r\n> \r\n> ```\r\n> build:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> build --define=NO_CONSTEXPR_FOR_YOU=1\r\n> ```\r\n> \r\n> It looks like the way I've defined it isn't right. Could someone please help me define this macro?\r\n\r\nHave you applied the linked patch?\r\n\r\nOnce that's the case, `--copt=-DNO_CONSTEXPR_FOR_YOU=1` should be enough.", "Yes, I've it applied -\r\n```\r\ndiff --git a/third_party/com_google_absl_fix_mac_and_nvcc_build.patch b/third_party/com_google_absl_fix_mac_and_nvcc_build.patch\r\nindex 271e941..81c8e5d 100644\r\n--- a/third_party/com_google_absl_fix_mac_and_nvcc_build.patch\r\n+++ b/third_party/com_google_absl_fix_mac_and_nvcc_build.patch\r\n@@ -1,3 +1,14 @@\r\n+--- ./absl/time/internal/cctz/include/cctz/civil_time_detail.h 2020-08-06 01:33:56.005757145 +0200\r\n++++ ./absl/time/internal/cctz/include/cctz/civil_time_detail.h 2020-08-06 01:33:35.460579387 +0200\r\n+@@ -23,7 +23,7 @@\r\n+ #include \"absl/base/config.h\"\r\n+\r\n+ // Disable constexpr support unless we are in C++14 mode.\r\n+-#if __cpp_constexpr >= 201304 || (defined(_MSC_VER) && _MSC_VER >= 1910)\r\n++#if (!defined(NO_CONSTEXPR_FOR_YOU) && __cpp_constexpr >= 201304) || (defined(_MSC_VER) && _MSC_VER >= 1910)\r\n+ #define CONSTEXPR_D constexpr  // data\r\n+ #define CONSTEXPR_F constexpr  // function\r\n+ #define CONSTEXPR_M constexpr  // member\r\n --- ./absl/time/internal/cctz/BUILD.bazel      2019-09-23 13:20:52.000000000 -0700\r\n +++ ./absl/time/internal/cctz/BUILD.bazel.fixed        2019-09-23 13:20:48.000000000 -0700\r\n @@ -74,15 +74,6 @@\r\n@@ -301,4 +312,3 @@\r\n +        .internal_compressed_tuple::template Storage<CompressedTuple, I>::get();\r\n    }\r\n  };\r\n```", "Well, it's working for us, I can't do more.", "With same gcc 5.4?", "> With same gcc 5.4?\r\n\r\nYes.", "Okay. Thanks.\r\n", "@lissyx - The macro setting mentioned in bazelrc worked for cpu builds but not for gpu build. It looks like the same macro has to be defined for nvcc too. I've tried this `build:cuda --define=-DNO_CONSTEXPR_FOR_YOU=1` but it didn't work. Could you please help me set the same macro for cu.cc files?\r\n", "> @lissyx - The macro setting mentioned in bazelrc worked for cpu builds but not for gpu build. It looks like the same macro has to be defined for nvcc too. I've tried this `build:cuda --define=-DNO_CONSTEXPR_FOR_YOU=1` but it didn't work. Could you please help me set the same macro for cu.cc files?\r\n\r\nno, I can't help you. again, what I shared is what we use and it works everywhere.", "The build is also working fine for us. I cannot reproduce the issue.\r\nI would recommend reaching out to absl team for support.\r\n\r\nSince we cannot reproduce the problem, one way we can help is if you can create a docker container where we can reproduce the problem, we may be able to debug and help.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41926\">No</a>\n", "> The build is also working fine for us. I cannot reproduce the issue.\r\n\r\n@gunan Have you tried on a distro using GCC 5.4 ?", "Thanks a lot everyone. I was able to fix the problem using GCC 5.4 by adding another line in bazelrc for setting --host_copt too as below.\r\n```\r\nbuild:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\nbuild:opt --host_copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n```", "> Thanks a lot everyone. I was able to fix the problem using GCC 5.4 by adding another line in bazelrc for setting --host_copt too as below.\r\n> \r\n> ```\r\n> build:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> build:opt --host_copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> ```\r\n\r\nHi, I'm really confused about how to use the patch and where to set these two lines you mentioned. Could you please explain more details? Thanks a lot!", "The patch being talked here is `third_party/com_google_absl_fix_mac_and_nvcc_build.patch`. This patch is already being used by TF here https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/workspace.bzl#L227. But we need to update it by adding the additional stuff mentioned https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-18a66e25fbcbd72730332a799ddee2ec.  And then add following two lines in .bazelrc file which is in tensorflow source code.\r\n```\r\nbuild:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\nbuild:opt --host_copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n```", "> The patch being talked here is `third_party/com_google_absl_fix_mac_and_nvcc_build.patch`. This patch is already being used by TF here https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/workspace.bzl#L227. But we need to update it by adding the additional stuff mentioned [mozilla@6fad14b#diff-18a66e25fbcbd72730332a799ddee2ec](https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-18a66e25fbcbd72730332a799ddee2ec). And then add following two lines in .bazelrc file which is in tensorflow source code.\r\n> \r\n> ```\r\n> build:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> build:opt --host_copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> ```\r\n\r\nThanks for your reply. What's the specific command line to add the additional stuff? Could you please tell me the detailed steps? I'm not familiar with the operations of patches. Thanks again!", "@Yuze-HE \r\nTo update the existing patch, I did following -\r\n1. Download the abseil code mentioned in tensorflow/workspace.bzl at here https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/workspace.bzl#L232. \r\n`wget https://github.com/abseil/abseil-cpp/archive/df3ea785d8c30a9503321a3d35ee7d35808f190d.tar.gz`\r\n2. Extract the downloaded tar \r\n`tar -xvf df3ea785d8c30a9503321a3d35ee7d35808f190d.tar.gz`\r\n3. Create a copy of this extracted tar as abseil_orig\r\n4. Go to the extracted directory and apply the original patch which is https://github.com/tensorflow/tensorflow/blob/v2.3.0/third_party/com_google_absl_fix_mac_and_nvcc_build.patch using below command -\r\n`patch -p1 < <path_to_tensorflow>/third_party/com_google_absl_fix_mac_and_nvcc_build.patch`\r\n5. Hand edit the files mentioned here https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-18a66e25fbcbd72730332a799ddee2ec.\r\n6. Take the diff of abseil_org with this abseil directory which has got modified files because of above two steps, using below command -\r\n`diff -Naur abseil_org abseil > <path_to_tensorflow>/third_party/com_google_absl_fix_mac_and_nvcc_build.patch`\r\n7. Now, restart the TF build after updating .bazelrc with the two lines mentioned in my previous comment.\r\n", "> @Yuze-HE\r\n> To update the existing patch, I did following -\r\n> \r\n> 1. Download the abseil code mentioned in tensorflow/workspace.bzl at here https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/workspace.bzl#L232.\r\n>    `wget https://github.com/abseil/abseil-cpp/archive/df3ea785d8c30a9503321a3d35ee7d35808f190d.tar.gz`\r\n> 2. Extract the downloaded tar\r\n>    `tar -xvf df3ea785d8c30a9503321a3d35ee7d35808f190d.tar.gz`\r\n> 3. Create a copy of this extracted tar as abseil_orig\r\n> 4. Go to the extracted directory and apply the original patch which is https://github.com/tensorflow/tensorflow/blob/v2.3.0/third_party/com_google_absl_fix_mac_and_nvcc_build.patch using below command -\r\n>    `patch -p1 < <path_to_tensorflow>/third_party/com_google_absl_fix_mac_and_nvcc_build.patch`\r\n> 5. Hand edit the files mentioned here [mozilla@6fad14b#diff-18a66e25fbcbd72730332a799ddee2ec](https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-18a66e25fbcbd72730332a799ddee2ec).\r\n> 6. Take the diff of abseil_org with this abseil directory which has got modified files because of above two steps, using below command -\r\n>    `diff -Naur abseil_org abseil > <path_to_tensorflow>/third_party/com_google_absl_fix_mac_and_nvcc_build.patch`\r\n> 7. Now, restart the TF build after updating .bazelrc with the two lines mentioned in my previous comment.\r\n\r\nThank you very much!", "Is it something that will be integrated into TF build in some reasonable time period?", "@npanpaliya \"edit the files mentioned here mozilla@6fad14b#diff-18a66e25fbcbd72730332a799ddee2ec.\"\r\nWhat files do you mean? Or do I have to apply this patch as well?", "Editing the files in absl as per the changes mentioned at mozilla/tensorflow@6fad14b#diff-18a66e25fbcbd72730332a799ddee2ec.\r\nThen create a single patch of existing content of https://github.com/tensorflow/tensorflow/blob/v2.3.0/third_party/com_google_absl_fix_mac_and_nvcc_build.patch and changes in mozilla/tensorflow in that commit.\r\n\r\nRegarding TF including these changes - I think it will probably not included by TF as these changes are needed to get TF built with older gcc 5.4. But TF community can comment more on this.", "Same issue with\r\nTF 2.4.0-rc3\r\nPython 3.6.9\r\nGPU: Tesla T4 | NVIDIA driver: 455.45.01 | CUDA: 10.1.243 | CUDNN: 7.6.4\r\nBazel 3.1.0", "@npanpaliya Are you talking about this patch? https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-8757a02eb58c1cd11dda17ad77b09ded795db562ed05acd02248b00d141356e6", "> Editing the files in absl as per the changes mentioned at [mozilla/tensorflow@6fad14b](https://github.com/mozilla/tensorflow/commit/6fad14b)#diff-18a66e25fbcbd72730332a799ddee2ec.\r\n> Then create a single patch of existing content of https://github.com/tensorflow/tensorflow/blob/v2.3.0/third_party/com_google_absl_fix_mac_and_nvcc_build.patch and changes in mozilla/tensorflow in that commit.\r\n> \r\n> Regarding TF including these changes - I think it will probably not included by TF as these changes are needed to get TF built with older gcc 5.4. But TF community can comment more on this.\r\n\r\nI have 3 folder:\r\n1) Original absl in TF folder\r\n2) abseil-cpp-df3ea785d8c30a9503321a3d35ee7d35808f190d (extracted from tar.gz)\r\n3) abseil_orig (copied abseil-cpp-df3ea785d8c30a9503321a3d35ee7d35808f190d folder)\r\n\r\nI applied <path_to_tensorflow>/third_party/com_google_absl_fix_mac_and_nvcc_build.patch on 2nd folder.\r\n\r\nNow you say to hand edit files from mozilla patch.\r\nI understand that there are 2 files to be edited:\r\n1)./absl/time/internal/cctz/include/cctz/civil_time_detail.h - it will add NO_CONSTEXPR_FOR_YOU flag\r\n2) ./absl/time/internal/cctz/BUILD.bazel.fixed - I don't see it in any folder. What should I do here?", "I ended up adding \r\n\r\n```\r\n--- ./absl/time/internal/cctz/include/cctz/civil_time_detail.h\t2020-08-06 01:33:56.005757145 +0200\r\n+++ ./absl/time/internal/cctz/include/cctz/civil_time_detail.h\t2020-08-06 01:33:35.460579387 +0200\r\n@@ -23,7 +23,7 @@\r\n #include \"absl/base/config.h\"\r\n \r\n // Disable constexpr support unless we are in C++14 mode.\r\n-#if __cpp_constexpr >= 201304 || (defined(_MSC_VER) && _MSC_VER >= 1910)\r\n+#if (!defined(NO_CONSTEXPR_FOR_YOU) && __cpp_constexpr >= 201304) || (defined(_MSC_VER) && _MSC_VER >= 1910)\r\n #define CONSTEXPR_D constexpr  // data\r\n #define CONSTEXPR_F constexpr  // function\r\n #define CONSTEXPR_M constexpr  // member\r\n```\r\n\r\nat the beginning of <path_to_tensorflow>/third_party/com_google_absl_fix_mac_and_nvcc_build.patch\r\n\r\n+\r\n\r\nadding \r\n\r\n```\r\nbuild:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\nbuild:opt --host_copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n```\r\n\r\nin .bazelrc\r\n\r\nBuild completed successfully with \r\nbazel build --config=cuda --config=opt --config=v2 //tensorflow/tools/pip_package:build_pip_package", "Hello, I followed your steps to compile tensorflow2.3.0, but it got this error:\r\n```\r\n./tensorflow/stream_executor/lib/statusor_internals.h:165:76: error: invalid static_cast from type 'const Status {aka const tensorflow::Status}' to type 'stream_executor::port::Status& {aka tensorflow::Status&}'\r\n./tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of 'bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]':\r\n\r\n```\r\n\r\nMy enviroment:\r\n```\r\nubuntu16.04\r\ncuda9.0\r\ngcc5.4.0\r\ntensorflow2.3.0\r\n```\r\nIt's so wired. I have no idea about how to correct this ?\r\nwould you mind giving me some suggestion ? \r\nThanks !", "@gmt710 I think your exception is not related to this issue.\r\nProbably: https://github.com/tensorflow/tensorflow/issues/20764", "Yes, but [#20764](https://github.com/tensorflow/tensorflow/issues/20764) is not solved.", "> I ended up adding\r\n> \r\n> ```\r\n> --- ./absl/time/internal/cctz/include/cctz/civil_time_detail.h\t2020-08-06 01:33:56.005757145 +0200\r\n> +++ ./absl/time/internal/cctz/include/cctz/civil_time_detail.h\t2020-08-06 01:33:35.460579387 +0200\r\n> @@ -23,7 +23,7 @@\r\n>  #include \"absl/base/config.h\"\r\n>  \r\n>  // Disable constexpr support unless we are in C++14 mode.\r\n> -#if __cpp_constexpr >= 201304 || (defined(_MSC_VER) && _MSC_VER >= 1910)\r\n> +#if (!defined(NO_CONSTEXPR_FOR_YOU) && __cpp_constexpr >= 201304) || (defined(_MSC_VER) && _MSC_VER >= 1910)\r\n>  #define CONSTEXPR_D constexpr  // data\r\n>  #define CONSTEXPR_F constexpr  // function\r\n>  #define CONSTEXPR_M constexpr  // member\r\n> ```\r\n> \r\n> at the beginning of <path_to_tensorflow>/third_party/com_google_absl_fix_mac_and_nvcc_build.patch\r\n> \r\n> \r\n> adding\r\n> \r\n> ```\r\n> build:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> build:opt --host_copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> ```\r\n> \r\n> in .bazelrc\r\n> \r\n> Build completed successfully with\r\n> bazel build --config=cuda --config=opt --config=v2 //tensorflow/tools/pip_package:build_pip_package\r\n\r\nworked for me\r\ncuda 10.2, tensorflow 2.4.1, centos 7", "wow, it's a good idea.\r\nBut I only can use cuda9.0.", "cuda10.2\r\nI've installed tensorflow yet and do not need build tensorflow.", "> at the beginning of <path_to_tensorflow>/third_party/com_google_absl_fix_mac_and_nvcc_build.patch\r\n\r\nIn v2.5, the bug still exists, and `com_google_absl_fix_mac_and_nvcc_build.patch` has been moved to `tensorflow/third_party/absl`", "> The patch being talked here is `third_party/com_google_absl_fix_mac_and_nvcc_build.patch`. This patch is already being used by TF here https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/workspace.bzl#L227. But we need to update it by adding the additional stuff mentioned [mozilla@6fad14b#diff-18a66e25fbcbd72730332a799ddee2ec](https://github.com/mozilla/tensorflow/commit/6fad14b203093e7a19301c4ef51a87fce71e38cb#diff-18a66e25fbcbd72730332a799ddee2ec). And then add following two lines in .bazelrc file which is in tensorflow source code.\r\n> \r\n> ```\r\n> build:opt --copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> build:opt --host_copt=-DNO_CONSTEXPR_FOR_YOU=1\r\n> ```\r\n\r\nThanks, but I applied this patch and added the two lines and still got the same error.. I checked the downloaded absl code and I am sure the patch is applied.\r\n```\r\n// Copyright 2016 Google Inc. All Rights Reserved.\r\n//\r\n// Licensed under the Apache License, Version 2.0 (the \"License\");\r\n// you may not use this file except in compliance with the License.\r\n// You may obtain a copy of the License at\r\n//\r\n//   https://www.apache.org/licenses/LICENSE-2.0\r\n//\r\n//   Unless required by applicable law or agreed to in writing, software\r\n//   distributed under the License is distributed on an \"AS IS\" BASIS,\r\n//   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n//   See the License for the specific language governing permissions and\r\n//   limitations under the License.\r\n\r\n#ifndef ABSL_TIME_INTERNAL_CCTZ_CIVIL_TIME_DETAIL_H_\r\n#define ABSL_TIME_INTERNAL_CCTZ_CIVIL_TIME_DETAIL_H_\r\n\r\n#include <cstdint>\r\n#include <limits>\r\n#include <ostream>\r\n#include <type_traits>\r\n\r\n#include \"absl/base/config.h\"\r\n\r\n// Disable constexpr support unless we are in C++14 mode.\r\n#if (!defined(NO_CONSTEXPR_FOR_YOU) && __cpp_constexpr >= 201304) || (defined(_MSC_VER) && _MSC_VER >= 1910)\r\n#define CONSTEXPR_D constexpr  // data\r\n#define CONSTEXPR_F constexpr  // function\r\n#define CONSTEXPR_M constexpr  // member\r\n#else\r\n```"]}, {"number": 41925, "title": "added", "body": "add this", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41925) for more info**.\n\n<!-- need_sender_cla -->", "What does this bring to TF?"]}, {"number": 41924, "title": "Building From Source on Ubuntu TMP directory defaults to 'C:\\Windows\\Temp", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:2.3.0\r\n- Python version:3.8.0\r\n- Bazel version (if compiling from source):3.1.0\r\n\r\n**Describe the problem**\r\n\r\n$ bazel build -c opt //tensorflow/tools/lib_package:libtensorflow\r\n\r\nWarning shows \r\n\r\n`Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default`\r\n\r\nFull output\r\n\r\n```\r\nbazel build -c opt //tensorflow/tools/lib_package:libtensorflow\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=179\r\nINFO: Reading rc options for 'build' from /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/russell/git/go/src/github.com/tensorflow/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/russell/anaconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/russell/anaconda3/lib/python3.8/site-packages --python_path=/home/russell/anaconda3/bin/python3 --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=7.5 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:cuda in file /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true\r\nINFO: Found applicable config definition build:using_cuda in file /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1\r\nINFO: Found applicable config definition build:linux in file /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/russell/git/go/src/github.com/tensorflow/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /home/russell/.cache/bazel/_bazel_russell/8890081aadea2dff92679e25965ee39c/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:5: \r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\n\r\n```", "comments": ["@lacvapps \r\n\r\nThis looks like a duplicate of #40614. We can track the issue in #40614.Thanks!", "Okay, thank you for the response", "@lacvapps \r\n\r\nCan you please close this issue since it is being tracked in #40614.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41924\">No</a>\n"]}, {"number": 41923, "title": "[Wsign-compare] warning resolutions, by directory, 16", "body": "Resolution affecting directories:\r\n`tensorflow/compiler/xla/service/service.cc`\r\n\r\nsplit from\r\n#41753\r\n\r\n@mihaimaruseac", "comments": []}, {"number": 41922, "title": "[Wsign-compare] warning resolutions, by directory, 15", "body": "Resolution affecting directories:\r\n`tensorflow/compiler/xla`,\r\n`tensorflow/compiler/xla/service`\r\n\r\nsplit from\r\nhttps://github.com/tensorflow/tensorflow/pull/41753\r\n\r\npart 4 of 4\r\n\r\n@mihaimaruseac ", "comments": ["Closing as obsolete and old."]}, {"number": 41921, "title": "[Wsign-compare] warning resolutions, by directory, 14", "body": "Resolution affecting directories:\r\n`tensorflow/compiler/xla`,\r\n`tensorflow/compiler/xla/service`\r\n\r\nsplit from\r\nhttps://github.com/tensorflow/tensorflow/pull/41753\r\n\r\npart 3 of 4\r\n\r\n@mihaimaruseac ", "comments": ["Closing as obsolete and old."]}, {"number": 41920, "title": "[Wsign-compare] warning resolutions, by directory, 13", "body": "Resolution affecting directories:\r\n`tensorflow/compiler/xla`,\r\n`tensorflow/compiler/xla/service`\r\n\r\nsplit from\r\nhttps://github.com/tensorflow/tensorflow/pull/41753\r\n\r\npart 2 of 4\r\n\r\n@mihaimaruseac ", "comments": ["Closing as obsolete and old."]}, {"number": 41919, "title": "[Wsign-compare] warning resolutions, by directory, 12", "body": "Resolution affecting directories:\r\n`tensorflow/compiler/xla`,\r\n`tensorflow/compiler/xla/service`\r\n\r\nsplit from\r\nhttps://github.com/tensorflow/tensorflow/pull/41753\r\n\r\npart 1 of 4\r\n\r\n@mihaimaruseac ", "comments": ["Closing as obsolete and old."]}, {"number": 41918, "title": "pbfile output different result with checkpoint when using slim model", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n", "comments": ["@cryandme,\r\nCould you please elaborate and describe the issue you are facing. \r\n\r\nAlso, in order to expedite the trouble-shooting process, please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "I am very glad to receive your reply.\r\nThe version of TensorFlow I use is 1.13.1,\r\nLooking forword to your reply.Thanks!\r\n\r\n\r\n\r\n\r\n\r\n\r\n------------------&nbsp;\u539f\u59cb\u90ae\u4ef6&nbsp;------------------\r\n\u53d1\u4ef6\u4eba:                                                                                                                        \"tensorflow/tensorflow\"                                                                                    <notifications@github.com&gt;;\r\n\u53d1\u9001\u65f6\u95f4:&nbsp;2020\u5e747\u670831\u65e5(\u661f\u671f\u4e94) \u665a\u4e0a7:45\r\n\u6536\u4ef6\u4eba:&nbsp;\"tensorflow/tensorflow\"<tensorflow@noreply.github.com&gt;;\r\n\u6284\u9001:&nbsp;\"\u6a31\u96ea\u5982\u8587\"<1003328735@qq.com&gt;;\"Mention\"<mention@noreply.github.com&gt;;\r\n\u4e3b\u9898:&nbsp;Re: [tensorflow/tensorflow] pbfile output different result with checkpoint when using slim model (#41918)\r\n\r\n\r\n\r\n\r\n\r\n \r\n@cryandme,\r\n Could you please elaborate and describe the issue you are facing.\r\n \r\nAlso, in order to expedite the trouble-shooting process, please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!\r\n \r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub, or unsubscribe.\r\n\r\n\r\n\u4eceQQ\u90ae\u7bb1\u53d1\u6765\u7684\u8d85\u5927\u9644\u4ef6\r\n\r\ntest_data.zip (158.73M, 2020\u5e7409\u670802\u65e5 09:39 \u5230\u671f)\u8fdb\u5165\u4e0b\u8f7d\u9875\u9762\uff1ahttp://mail.qq.com/cgi-bin/ftnExs_download?k=77613335895d2392eaf5fe0e1f63054b075401000b02545518590a07584e035253001e0c5f51004956570a0d0d00525d03530205396e37105012476a5d0243051b1b5a45395e&t=exs_ftn_download&code=5a359c7d\r\n\r\n\r\n\r\n\r\ncode.zip (135.35M, 2020\u5e7409\u670802\u65e5 09:41 \u5230\u671f)\u8fdb\u5165\u4e0b\u8f7d\u9875\u9762\uff1ahttp://mail.qq.com/cgi-bin/ftnExs_download?k=04386135845ceb91e6acac0e12325349580f595652035650145c0400551f55555c0a4c570d53024b090f5907525659505c000402343a6105565c041b4e5b116604&t=exs_ftn_download&code=98a542af", "@cryandme,\r\nTo help us reproduce the issue, please provide the complete code and the dataset too.\r\n\r\nAlso, TensorFlow 1.13 is not actively supported. please update TensorFlow to 2.x or v1.15 . Thanks!", "@amahendrakar\uff0c\r\nI don't think my question has anything to do with the tensorFlow version,because I solidified the model (without calling the NETS module in TF.Slim), CKPT and PB had the same detection result,but Now the detection accuracy of PB decreases greatly when the model is solidified (nets in the called Slim).and the code used twice is exactly the same (CKPT to PB)\r\nAlso,I have sent l the complete code and the dataset to you\r\n\r\n", "@cryandme,\r\nI am not able to access the links you have given due to network restrictions. Could you please try sending the data in an alternative way? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41917, "title": "ragged.boolean_mask() gives a different result when executed in a model", "body": "```\r\nmask = tf.ones((1, 100, 300), dtype=bool)\r\n\r\ndata = tf.ragged.boolean_mask(\r\n    data=tf.ones((1,100,300,12)),\r\n    mask=mask\r\n).to_tensor()\r\n\r\nprint(data)\r\n```\r\n\r\nThis gives me `tf.Tensor(......, shape=(1, 100, 300, 12), dtype=float32)` when executed normally, but when it is executed in a model it gives me `Tensor(\"model/attention/RaggedToTensor/RaggedTensorToTensor:0\", shape=(None, None, None, 12), dtype=float32)`.\r\n\r\nWhy is this? How do I get it to return the correctly masked tensor in a model?", "comments": ["@surGeonGG \r\nWe see that you have not filled the issue template, please fill the issue template which provides us with the tf version, stand alone code the replicate the issue or is possible share a colab gist with the error faced.", "Replicated here\r\n\r\nhttps://colab.research.google.com/drive/1HaOtA209X062l7OHn3JPtI2O_uGEcc8O?usp=sharing", "@surGeonGG \r\nif Dense is first layer then you have to set shape/size of input data. You can see it even in example in[ Dense documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).\r\n\r\nPlease refer to below links for reference:\r\n#35464 #33422 #23748 [link](https://stackoverflow.com/questions/56918388/error-valueerror-the-last-dimension-of-the-inputs-to-dense-should-be-defined) \r\n", "I'm confused. Dense is not the first layer. The first layer is an lstm. The shape information becomes (none, none, none) after I call...\r\n\r\n``` \r\nmask = tf.ones((1, 200, 12), dtype=bool)\r\n    x = tf.ragged.boolean_mask(\r\n        data=x,\r\n        mask=mask\r\n    ).to_tensor()\r\n```", "Figured it out! I simply use `x = tf.ensure_shape(x, [*shape here*])` afterwards to make sure the shape is correct.\r\n\r\nThanks for the help."]}, {"number": 41916, "title": "[TF2XLA] Add EuclideanNorm kernel", "body": "Fixes #41915.", "comments": ["@WindQAQ This does not seem to work for complex numbers (actually I'm not sure why), but e.g. `EuclideanNormReductionTest.testComplex64` is failing in `//tensorflow/python/kernel_tests:reduction_ops_test_xla_gpu`. Would you be willing to investigate?", "@Cheshire will do. Can you share the full failing log? Thank you!", "@WindQAQ It produces a completely different output:\r\n\r\n```\r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\nMismatched value: a is different from b.\r\nnot close where = (array([1]),)\r\nnot close lhs = [2.236068+0.j]\r\nnot close rhs = [1.-2.j]\r\n```\r\n\r\nI'm trying to figure out what is going on, surprisingly, the HLO looks already constant folded (even before optimizations).", "@WindQAQ I have found the bug: in case of empty axis,  the base class returns output: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/tf2xla/kernels/reduction_ops_common.cc;l=52-58?q=XlaReductionOp::Compile\r\n\r\nIn your case, it might be that the finalizer still has to be applied.\r\n\r\nTo me it indicates that subclassing is not a correct approach here: euclidean norm is conceptually not a reduction, because it's not an identity for a singular element. Could you wrap it instead?", "> @WindQAQ I have found the bug: in case of empty axis, the base class returns output: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/tf2xla/kernels/reduction_ops_common.cc;l=52-58?q=XlaReductionOp::Compile\r\n> \r\n> In your case, it might be that the finalizer still has to be applied.\r\n> \r\n> To me it indicates that subclassing is not a correct approach here: euclidean norm is conceptually not a reduction, because it's not an identity for a singular element. Could you wrap it instead?\r\n\r\nThanks for pointing out this! I'll prepare a PR for it."]}, {"number": 41915, "title": "tf.math.reduce_euclidean_norm can not be in tf.function with experimental_compile=True", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.0-dev20200730\r\n- Python version: colab\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: no\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nEuclideanNorm: unsupported op: No registered 'EuclideanNorm' OpKernel for XLA_CPU_JIT devices compatible with node {{node EuclideanNorm}}\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe op can run as usual.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python3\r\nimport tensorflow as tf\r\n\r\nx = tf.complex(tf.random.uniform(shape=(5, 5)), tf.random.uniform(shape=(5, 5)))\r\n\r\n@tf.function(experimental_compile=True)\r\ndef reduce_euclidean_norm(x):\r\n  return tf.math.reduce_euclidean_norm(x)\r\n\r\nprint(reduce_euclidean_norm(x))\r\n```\r\n\r\nhttps://colab.research.google.com/drive/1YENmpGDLU6kCupseizWTO1wBiDDj8v9x?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@WindQAQ \r\n\r\nI have tried in colab with TF nightly version(2.4.0-dev20200730) and was able to reproduce the issue.However if I change \r\n`@tf.function(experimental_compile=True)`to `@tf.function`in the code I am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5167b8076f8f98b64664c351843fec89/untitled209.ipynb).Thanks!", "> @WindQAQ\r\n> \r\n> I have tried in colab with TF nightly version(2.4.0-dev20200730) and was able to reproduce the issue.However if I change\r\n> `@tf.function(experimental_compile=True)`to `@tf.function`in the code I am not seeing any issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/5167b8076f8f98b64664c351843fec89/untitled209.ipynb).Thanks!\r\n\r\nYes, but in that case, it does not utilize XLA.", "It looks like the ops is missing an XLA kernel for the gradient. @cheshire / @rmlarsen  could you triage?", "Thanks for the triage! I think it misses the XLA kernel for `EuclideanNorm` instead of its gradient as the gradient is defined with python ops [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L52-L64).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41915\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41915\">No</a>\n"]}, {"number": 41914, "title": "Keras reports \u201cValueError: initial_value must have a shape specified:Tensor(\u201ddense_4/MatMul:0\u201c,shape=(?, 1),dtype=float32)\u201d when customizing random AF", "body": "I am implementing a feed-forward neural network model with random activation function at the out layer which has only one neuron. The random activation function works as if the output of the last hidden layer is greater than a threshold, the output of the model will be 1, otherwise, the model output is zero.\r\n\r\nI used the following code using \"get_custom_objects\" to customize the activation function, but it gives me an error that \"ValueError: initial_value must have a shape specified: Tensor(\"dense_4/MatMul:0\", shape=(?, 1), dtype=float32)\"\r\n```\r\nclass Rand(Activation):                \r\n     def __init__(self, activation, **kwargs):\r\n        super(Rand, self).__init__(activation, **kwargs)\r\n        self.__name__ = 'rand'\r\n\r\n \r\ndef rand(x):\r\n    result = tf.Variable(tf.cond(tf.random.uniform(shape=[1])[0] > tf.Variable(x), 1, 0))\r\n    return result\r\n```", "comments": ["@mohamed199215,\r\nI was unable to reproduce the issue from the given code snippet. The class and methods are defined but are not being called in the script. \r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the TensorFlow verison, the complete code and the dataset you are using. Thanks!", "Tensorflow version: 1.14.0\r\nThe dataset is attached\r\n[Dataset.zip](https://github.com/tensorflow/tensorflow/files/5013092/Dataset.zip)\r\n\r\n\r\nHere is the complete code:\r\n\r\n```\r\nfrom __future__ import print_function\r\nfrom keras.layers.core import Dense, Dropout, Activation\r\nfrom keras.utils import np_utils \r\nimport pandas as pd\r\nfrom imblearn.over_sampling import ADASYN\r\nfrom imblearn.under_sampling import RandomUnderSampler\r\nimport numpy as np\r\nfrom numpy import loadtxt\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, GRU\r\nfrom keras import regularizers\r\nfrom keras.layers import Activation\r\nfrom keras.utils.generic_utils import get_custom_objects\r\nfrom keras.utils.np_utils import to_categorical\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn import metrics\r\nfrom sklearn.metrics import precision_score , recall_score\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom keras.callbacks import Callback, LearningRateScheduler\r\nimport math\r\nfrom keras.models import load_model\r\nimport csv\r\nfrom keras.callbacks import EarlyStopping\r\nfrom keras.callbacks import ModelCheckpoint\r\nfrom keras.layers import Dense, Dropout, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling1D\r\nfrom keras.layers.convolutional import Conv1D\r\nfrom keras.optimizers import adam\r\nfrom keras.callbacks import Callback, LearningRateScheduler\r\nfrom keras import backend as K\r\n\r\n#activation function\r\nclass Rand(Activation):             # Take care of Rand and rand in the following few lines\r\n    \r\n    def __init__(self, activation, **kwargs):\r\n        super(Rand, self).__init__(activation, **kwargs)\r\n        self.__name__ = 'rand'\r\n\r\n  \r\ndef rand(x):\r\n    # r = np.random.uniform(low=0.0, high=1.0, size=None)\r\n    # bool = tf.Variable(x<r)\r\n    # result = init_bias = tf.Variable(init_bias,validate_shape=False)\r\n    result = tf.Variable(tf.cond(tf.random.uniform(shape=[1])[0] > tf.Variable(x), 1, 0))\r\n    print (\"asdasdasdasD\", result)\r\n    return result\r\n    # if x is not None:\r\n        # print (\"yes\")\r\n    # if x<r:\r\n        # return int(0)\r\n    # else:\r\n        # return int(1)\r\n\r\nget_custom_objects().update({'rand': Rand(rand)})\r\n\r\npath_data_original = \"/home/tntech.edu/miibrahem42/GAN_Paper/Defense/\"\r\n\r\n# load the data\r\ndef data():\r\n    no_samples = 200000 #number of zero of one samples\r\n    last_col_indx = 100\r\n    data = pd.read_csv('Dataset.csv', sep=',', index_col=False, header=None)\r\n    \r\n    # # take n balanced samples\r\n    # s0 = data[last_col_indx][data[last_col_indx].eq(0)].sample(no_samples).index\r\n    # s1 = data[last_col_indx][data[last_col_indx].eq(1)].sample(no_samples).index \r\n    # data = data.loc[s0.union(s1)]\r\n    # # data = data.reindex(np.random.permutation(data.index))\r\n    # data = data.sample(frac=1)\r\n    # # print (len(list(data.to_numpy())[0]))\r\n    # # print ((list(data.to_numpy())))\r\n    \r\n    X_res = data.iloc[:,:data.shape[1]-1].to_numpy() #400,000\r\n    print (\"total data shape: \",X_res.shape)\r\n    Y_res = data.iloc[:,data.shape[1]-1]\r\n    print (\"total label shape: \",Y_res.shape)\r\n    \r\n    ada = RandomUnderSampler(ratio='majority',random_state=42)\r\n    x_train, y_train = ada.fit_sample(X_res,Y_res) # over-sampled data\r\n    print (\"sum of resulted labels is: \", sum(y_train))\r\n\r\n    xtr, x_valid, ytr, y_valid = train_test_split(x_train, y_train, test_size=0.3)\r\n\r\n    xtr = xtr.reshape(-1,xtr.shape[1],1)\r\n    x_valid = x_valid.reshape(-1,x_valid.shape[1],1)\r\n    \r\n    nb_classes = 2\r\n\r\n    print (\"Number of training samples is: \",xtr.shape[0])\r\n    print (\"Number of test samples is: \",x_valid.shape[0])\r\n\r\n    # ytr = to_categorical(ytr, nb_classes)\r\n    # y_valid = to_categorical(y_valid, nb_classes)\r\n\r\n    print (\"label has a shape of: \", y_valid.shape,ytr.shape)\r\n    return xtr,ytr,x_valid,y_valid\r\n\r\nlr = 0.0001 \r\n\r\ndef scheduler(epoch):\r\n  if epoch < 8:\r\n    print (lr)\r\n    return lr\r\n  else:\r\n    return lr * math.exp(0.1 * (4 - epoch))\r\nlearning_rte = LearningRateScheduler(scheduler)\r\n\r\nclass TestCallback(Callback):\r\n    def __init__(self, test_data):\r\n        self.test_data = test_data\r\n\r\n    def on_epoch_end(self, epoch, logs={}):\r\n        x, y = self.test_data\r\n        # loss, acc = self.model.evaluate(x, y, verbose=1)\r\n        # print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc*100))\r\n \r\n\r\nxtr,ytr,x_valid,y_valid=data()\r\n\r\n# input dimensions\r\n\r\n# Train a RNN model without optimization\r\n\r\nbatch_size = 400\r\nnum_classes = 2\r\nepochs = 1  #should be set\r\n\r\ninput_shape = (100,1)\r\n\r\nprint (xtr.shape)\r\nprint (x_valid.shape)\r\n\r\n# the monitoring parameter should be the same on both earlystopping and modelcheckpoint in case of classification problems\r\nstop_training = EarlyStopping( monitor='val_accuracy', mode='max', verbose=1, patience=15) #monitor='val_loss', mode='min'\r\nbest_model_save = ModelCheckpoint(path_data_original+'best_model_RNN_rand.h5',monitor='val_accuracy', mode='max', verbose=1\r\n, save_best_only=True)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv1D(150, kernel_size=50, activation='relu', input_shape=(input_shape)))\r\nmodel.add(MaxPooling1D(pool_size=4))\r\nmodel.add(GRU(200))\r\nmodel.add(Dropout(0.25))\r\n# model.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dense(35, activation='relu'))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Dense(1, activation='relu', use_bias=False))\r\nmodel.add(Dense(1, activation='rand', use_bias=False))  \r\nprint (model.summary())\r\n# model.layers[8].trainable = False  \r\n\r\n# # weights_of_last_layer = list([np.array([[1 , 1 ],\r\n       # # [0, 0]], dtype='float32'), np.array([0., 0.], dtype='float32')])\r\n\r\n# weights_of_last_layer = list([np.array([[1] ,[ 0 ]], dtype='float32'), np.array([0.], dtype='float32')])\r\n       \r\n# model.layers[8].set_weights(weights_of_last_layer)  \r\n  \r\nmodel.compile(loss='mean_squared_error', optimizer=adam(lr=lr), metrics=['accuracy'])  #categorical_crossentropy if binary classification\r\nmodel.fit(xtr, ytr,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1, \r\n         validation_split=0.1, callbacks=[TestCallback((x_valid, y_valid)), best_model_save, \r\n         stop_training, learning_rte])\r\n```\r\n\r\nplease let me know if you need further info.\r\nAll I want to do is implementing an activation function that takes the output of the last hidden layer and then generating a random number, then compares this number with the output -> if it is greater, the output will be one, otherwise, it will be zero.", "@mohamed199215,\r\nOn running the code with TF v1.15, I am facing an error while importing the `hyperas` module. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6ad617fe5497dd242a4e05b134ad2619/41914.ipynb). \r\n\r\nCould you please share the required dependency versions? Alternatively, you can also run the code on Colab and share the notebook with us. Thanks! ", "> @mohamed199215,\r\n> On running the code with TF v1.15, I am facing an error while importing the `hyperas` module. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6ad617fe5497dd242a4e05b134ad2619/41914.ipynb).\r\n> \r\n> Could you please share the required dependency versions? Alternatively, you can also run the code on Colab and share the notebook with us. Thanks!\r\n\r\nplease see the updated comment above since hyperas module is removed, it is not needed in this code.\r\nplease try again and let me know if errors found. ", "@mohamed199215,\r\nI was able to debug the `ValueError`, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d465edefcd21170b1603e43f5e035d19/41914.ipynb) and the explanation at the bottom of the notebook. \r\n\r\nBut now the code throws an error stating `IndexError: list index out of range` in the `model.compile` step. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}]