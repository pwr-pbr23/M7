[{"number": 50984, "title": "Fix segmentation fault in shape inference logic.", "body": "When running shape functions, some functions (such as `MutableHashTableShape`)\nproduce extra output information in the form of a `ShapeAndType` struct.  The\nshapes embedded in this struct are owned by an inference context that is\ncleaned up almost immediately; if the upstream code attempts to access this\nshape information, it can trigger a segfault.\n\n`ShapeRefiner` is mitigating this for normal output shapes by cloning them\n(and thus putting the newly created shape under ownership of an inference\ncontext that will not die), but we were not doing the same for shapes and\ntypes.  This commit fixes that by doing similar logic on output shapes and\ntypes.\n\nPiperOrigin-RevId: 384761124\nChange-Id: I07c0c42d29dfbb55bfa13ec1f09ef825fb0a1a1d", "comments": []}, {"number": 50983, "title": "Fix segmentation fault in shape inference logic.", "body": "When running shape functions, some functions (such as `MutableHashTableShape`)\nproduce extra output information in the form of a `ShapeAndType` struct.  The\nshapes embedded in this struct are owned by an inference context that is\ncleaned up almost immediately; if the upstream code attempts to access this\nshape information, it can trigger a segfault.\n\n`ShapeRefiner` is mitigating this for normal output shapes by cloning them\n(and thus putting the newly created shape under ownership of an inference\ncontext that will not die), but we were not doing the same for shapes and\ntypes.  This commit fixes that by doing similar logic on output shapes and\ntypes.\n\nPiperOrigin-RevId: 384761124\nChange-Id: I07c0c42d29dfbb55bfa13ec1f09ef825fb0a1a1d", "comments": []}, {"number": 50982, "title": "Why is <normalizing flow>.prob() failing for me?", "body": "I am building a Normalizing Flow (concatenation of Distribution and chain of Bijectors) in Tensorflow. Here is the code for the (single-link) chain of Bijectors:\r\n\r\n```\r\nclass Flow( tfb.Bijector ):\r\n\r\n    def __init__( self, theta, a, **kwargs ):\r\n        tfb.Bijector.__init__( self, forward_min_event_ndims = 0, **kwargs )\r\n        bijectors = [ tfb.Tanh() ]\r\n        self.chain = tfb.Chain( bijectors = bijectors )\r\n\r\n    def _forward( self, z ):\r\n        return self.chain( z )\r\n\r\n    def _inverse( self, x ):\r\n        result = self.chain.inverse( x ) \r\n        return result\r\n\r\n    def _forward_log_det_jacobian( self, z ):\r\n        return self.chain._forward_log_det_jacobian( z, event_ndims = 2 )\r\n```\r\nHere is the code I use to test the Flow:\r\n\r\n```\r\nZ = tf.convert_to_tensor( [ [ [ 0.1, 0.2 ], [ 0.3, 0.4 ], [ 0.5, 0.6 ] ], \r\n                            [ [ 0.8, 0.7 ], [ 0.6, 0.5 ], [ 0.4, 0.3 ] ],\r\n                            [ [ 0.4, 0.7 ], [ 0.2, 0.1 ], [ 0.8, 0.0 ] ] ] )\r\nprint( \"Z\", Z )\r\nnf = Flow( 1., 2. )  # ### theta, a \r\nbd = tfd.MultivariateNormalDiag( loc=[0.,0.], scale_diag=[1.,1.] )\r\ntd = tfd.TransformedDistribution( bd, nf )\r\ntd.log_prob( Z )\r\n```\r\nThe last statement fails with the following stack trace:\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-53-de138696ce92> in <module>()\r\n     24 bd = tfd.MultivariateNormalDiag( loc=[0.,0], scale_diag=[1.,1.] )\r\n     25 td = tfd.TransformedDistribution( bd, nf )\r\n---> 26 td.prob( Z )\r\n\r\n12 frames\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py in prob(self, value, name, **kwargs)\r\n   1322         values of type `self.dtype`.\r\n   1323     \"\"\"\r\n-> 1324     return self._call_prob(value, name, **kwargs)\r\n   1325 \r\n   1326   def _call_unnormalized_log_prob(self, value, name, **kwargs):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/distribution.py in _call_prob(self, value, name, **kwargs)\r\n   1304     with self._name_and_control_scope(name, value, kwargs):\r\n   1305       if hasattr(self, '_prob'):\r\n-> 1306         return self._prob(value, **kwargs)\r\n   1307       if hasattr(self, '_log_prob'):\r\n   1308         return tf.exp(self._log_prob(value, **kwargs))\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/distributions/transformed_distribution.py in _prob(self, y, **kwargs)\r\n    371         )\r\n    372     ildj = self.bijector.inverse_log_det_jacobian(\r\n--> 373         y, event_ndims=event_ndims, **bijector_kwargs)\r\n    374     if self.bijector._is_injective:  # pylint: disable=protected-access\r\n    375       base_prob = self.distribution.prob(x, **distribution_kwargs)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/bijector.py in inverse_log_det_jacobian(self, y, event_ndims, name, **kwargs)\r\n   1318       ValueError: if the value of `event_ndims` is not valid for this bijector.\r\n   1319     \"\"\"\r\n-> 1320     return self._call_inverse_log_det_jacobian(y, event_ndims, name, **kwargs)\r\n   1321 \r\n   1322   def _call_forward_log_det_jacobian(self, x, event_ndims, name, **kwargs):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/bijector.py in _call_inverse_log_det_jacobian(self, y, event_ndims, name, **kwargs)\r\n   1274               'is implemented. One or the other is required.')\r\n   1275 \r\n-> 1276         return self._reduce_jacobian_det_over_shape(ildj, reduce_shape)\r\n   1277 \r\n   1278   def inverse_log_det_jacobian(self,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow_probability/python/bijectors/bijector.py in _reduce_jacobian_det_over_shape(self, unreduced, reduce_shape)\r\n   1531     ones = tf.ones(reduce_shape, unreduced.dtype)\r\n   1532     reduce_dims = ps.range(-ps.size(reduce_shape), 0)\r\n-> 1533     return tf.reduce_sum(ones * unreduced, axis=reduce_dims)\r\n   1534 \r\n   1535   def _parameter_control_dependencies(self, is_init):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n   1232         #   r_binary_op_wrapper use different force_same_dtype values.\r\n   1233         x, y = maybe_promote_tensors(x, y, force_same_dtype=False)\r\n-> 1234         return func(x, y, name=name)\r\n   1235       except (TypeError, ValueError) as e:\r\n   1236         # Even if dispatching the op failed, the RHS may be a tensor aware\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in _mul_dispatch(x, y, name)\r\n   1573     return sparse_tensor.SparseTensor(y.indices, new_vals, y.dense_shape)\r\n   1574   else:\r\n-> 1575     return multiply(x, y, name=name)\r\n   1576 \r\n   1577 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    204     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    205     try:\r\n--> 206       return target(*args, **kwargs)\r\n    207     except (TypeError, ValueError):\r\n    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in multiply(x, y, name)\r\n    528   \"\"\"\r\n    529 \r\n--> 530   return gen_math_ops.mul(x, y, name)\r\n    531 \r\n    532 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in mul(x, y, name)\r\n   6238       return _result\r\n   6239     except _core._NotOkStatusException as e:\r\n-> 6240       _ops.raise_from_not_ok_status(e, name)\r\n   6241     except _core._FallbackException:\r\n   6242       pass\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6895   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6896   # pylint: disable=protected-access\r\n-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6898   # pylint: enable=protected-access\r\n   6899 \r\n\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: required broadcastable shapes at loc(unknown) [Op:Mul]\r\n```\r\nI do not understand the error message, nor how to correct it.   Please help", "comments": ["@markalavin ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and tensorflow version to reproduce the issue reported here.Thanks!", "Tilak, thanks! for the quick response.   The versions of code I'm using are \"tf.version 2.5.0 tfp.version 0.13.0\" and I'm running on Google Colab (for a Coursera course).   You *should* be able to reproduce the issue with the code I've supplied plus the following imports:\r\n```\r\n\r\n    \r\n    import tensorflow as tf\r\n    import tensorflow_probability as tfp\r\n    tfd = tfp.distributions\r\n    tfb = tfp.bijectors\r\n    tfpl = tfp.layers\r\n    \r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    %matplotlib inline\r\n    \r\n    import math\r\n    from math import sin, cos\r\n    import traceback\r\n    \r\n    print( \"tf.version\", tf.__version__, \"tfp.version\", tfp.__version__ )\r\n```", "@tilakrayal , I am attaching a Python notebook, zipped.   Hopefully, you can reproduce the problem with it.   Let me know if it works....\r\n[Test_Case.zip](https://github.com/tensorflow/tensorflow/files/6894612/Test_Case.zip)\r\n\r\n", "@markalavin ,\r\nPlease post TFP related issues in [TFP repository](https://github.com/tensorflow/probability/issues) so that TFP experts will respond and resolve issues. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50982\">No</a>\n"]}, {"number": 50981, "title": "XLA Optimizer Mixed Precision Support", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nI used autoclustering with optimization level 2.\r\n```\r\nexport TF_DUMP_GRAPH_PREFIX=\"./xla_hlo\"\r\nexport TF_XLA_FLAGS=\"--tf_xla_clustering_debug --tf_xla_auto_jit=2\"\r\nexport XLA_FLAGS=\"--xla_dump_hlo_as_dot --xla_dump_to=./xla_hlo\"\r\n```\r\n\r\nI found that when I have the `DynamicLossScale` in mixed-precision setting, the graph for loss scaling is appended to the original graph containing forward and backward propagations,  \r\nand the remaining part of the optimizer is emitted as a separate graph.\r\n\r\nSo, my questions is\r\n\r\nWhy does the `DynamicScaleLoss` generate separated graphs in XLA?\r\n\r\n**Will this change the current api? How?**\r\nThe API should not be changed because already full precision training is supported.\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who wants to analyze the XLA HLO graph for their accelerator.\r\n\r\n**Any Other info.**", "comments": ["The problem was when `DynamicScaleLoss` is enabled, few iterations spend some time to figure out if the gradient has underflowed value. After it converges, it emits weight update kernels properly. "]}, {"number": 50980, "title": "XLA compilation error for TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 (GCP Cloud TPU VM)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): default cloud TPU VM installation\r\n- TensorFlow version (use command below): unknown 2.6.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: TPU\r\n\r\n**Describe the current behavior**\r\nModel compilation for TPU fails with logs:\r\n```\r\n2021-07-27 14:46:12.541836: I tensorflow/core/tpu/kernels/tpu_compilation_cache_interface.cc:435] TPU host compilation cache miss: cache_key(7086181526061168380), session_name()\r\n2021-07-27 14:46:14.141350: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:66] TpuCompileOp was cancelled. Sleeping for 300 seconds to give time for TPUCompileOp to finished.\r\n2021-07-27 14:46:17.250612: I tensorflow/core/tpu/kernels/tpu_compile_op_common.cc:175] Compilation of 7086181526061168380 with session name  took 4.708620071s and failed\r\n2021-07-27 14:46:17.250711: F tensorflow/core/tpu/kernels/tpu_program_group.cc:86] Check failed: xla_tpu_programs.size() > 0 (0 vs. 0)\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe model compiles or at least a meaningful error message with a cause is printed.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThe code ran was this: https://github.com/TensorSpeech/TensorFlowASR/blob/main/examples/conformer/train.py\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFull log: https://gist.github.com/lamyiowce/2e9c0adc3c9c52b4785c63898c0a6b43", "comments": ["@lamyiowce \r\n\r\nCould you please refer similar issues [#30580](https://github.com/tensorflow/tensorflow/issues/30580) and  [#48268](https://github.com/tensorflow/tensorflow/issues/48268) ,let us know if it helps.Thanks", "Thank you for the tips. However, both of those are supposed to be fixed in TF 2.6 and none of the methods showed there helped me.", "@lamyiowce \r\n\r\nCould you please provide the colab gist with all the dependencies to analyse the issue better.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50980\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50980\">No</a>\n", "My models failed in different situations with the same log. \r\n> F tensorflow/core/tpu/kernels/tpu_program_group.cc:86] Check failed: xla_tpu_programs.size() > 0 (0 vs. 0)  \r\n\r\nIn those cases, reducing model size or reducing the batch size often helped. \r\nI suspect it could be a memory related issue. ", "@UsharaniPagadala \r\nThis gist produces the similar error https://gist.github.com/thsunkid/28731eddd4192cb10f8441e338d84d35.\r\nCan you take a look on it?\r\n"]}, {"number": 50979, "title": "FunctionalPreprocessingStage incompatible with some keras preprocessing layers", "body": "**Describe the current behavior**\r\n\r\n[FunctionalPreprocessingStage](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/preprocessing/preprocessing_stage.py#L98-L133) provides a Functional like interface for connecting up multiple keras preprocessing layers, and adapting them all with a single call. If I understand the intended use correctly, you must first call the instances of the various preprocessing layers in order to connect them up into a DAG, wrap the inputs & outputs with FunctionalPreprocessingStage and then call adapt on that. However, some of the preprocessing layers throw an error if you call them before they have been adapted (eg. [IndexLookup](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/preprocessing/index_lookup.py#L599-L602) which is also used by other related classes like StringLookup, although this was not the case in [v2.4.0](https://github.com/tensorflow/tensorflow/blob/582c8d236cb079023657287c318ff26adb239002/tensorflow/python/keras/layers/preprocessing/index_lookup.py#L370-L373)).\r\n\r\n**Describe the expected behavior**\r\n\r\nAll Keras preprocessing layers should be compatible with FunctionalPreprocessingStage.\r\n\r\nMinimal reproducible example:\r\n\r\n```python\r\n# Expected usage\r\ninput_layer = tf.keras.Input(name='scalar_string', shape=(1,), dtype=tf.string)\r\nlookup_layer = tf.keras.layers.experimental.preprocessing.StringLookup(num_oov_indices=1, name=\"scalar_string_idx\")\r\noutput_layer = lookup_layer(input_layer) # this actually raises a ValueError\r\nfps = preprocessing_stage.FunctionalPreprocessingStage(input_layer, output_layer)\r\nfps.adapt(tf.constant([['testing'], ['one'], ['two'], ['three']]))\r\n```", "comments": ["@peteboothroyd ,\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\n", "Thanks for the pointer @tilakrayal . Looking at the keras repo I can see that this has already been [fixed](https://github.com/keras-team/keras/commit/fe31a34aa3447c5a7a768a8f61dcfd022ac2467b#diff-85c5529af75b28bafb4ddf95a2f4909eb6016ebd747ee62d3308bb738a7cfc80) in 2.6.0 there, so this issue is no longer relevant", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50979\">No</a>\n"]}, {"number": 50978, "title": "RuntimeError: Attempting to capture an EagerTensor without building a function.", "body": "**tensorflow 2.5**\r\n\r\n\r\nAdding the code of \r\n```\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\n```\r\nindeed solves the problem. But it maynot be what I want. Because in the function of `dense_tensor_to_sparse_tensor` , I indeed need the eager execution of `tensor.numpy()`, which means I should not disable the v2 behaviour. So is this the internal bug of tf 2.5. Any suggestion of how to modify this piece of code?\r\n\r\n\r\n### source code\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# import tensorflow.compat.v1 as tf\r\n# tf.disable_v2_behavior()\r\n\r\n\r\nfrom tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\r\n\r\n\r\na_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\r\na_values = np.array([1.0, 5.0, -1.0, -2.0], np.float32)\r\na_dense_shape = [4, 5]\r\n\r\na_st = tf.sparse.SparseTensor(a_indices, a_values, a_dense_shape)\r\n\r\ndef dense_tensor_to_sparse_tensor(tensor):\r\n\r\n    col = tensor.shape.as_list()[0]\r\n    print(col)\r\n\r\n    x = np.arange(col)\r\n    y = np.zeros(1)\r\n    xi, yi = np.meshgrid(x, y)\r\n    sparse_tensor = tf.sparse.SparseTensor(indices=np.array(np.vstack([xi.ravel(), yi.ravel()]).T),\r\n                                           values=tensor.numpy(),\r\n                                           dense_shape=[col, 1])\r\n    return sparse_tensor\r\n\r\n\r\nb_indices = np.array([[0,0], [1,0], [2,0], [3,0],[4,0]])\r\n# b_indices = np.array([[0,0], [0,1], [0,2], [0,3],[0,4]])\r\nb_values = np.array([2.0, 7.0, 8.0,9.0,10.0], np.float32)\r\nb_dense_shape = [5,1]\r\n\r\nB = tf.constant([2.0, 7.0, 8.0,9.0,10.0])\r\nB_sparse = dense_tensor_to_sparse_tensor(B)\r\nprint(B_sparse.indices, B_sparse.values, B_sparse.dense_shape)\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    # Define (COO format) Sparse Tensors over Numpy arrays\r\n    a_st = tf.sparse.SparseTensor(a_indices, a_values, a_dense_shape)\r\n    # b_st = tf.sparse.SparseTensor(b_indices, b_values, b_dense_shape)\r\n    b_st = B_sparse\r\n\r\n    # Convert SparseTensors to CSR SparseMatrix\r\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\r\n        a_st.indices, a_st.values, a_st.dense_shape)\r\n    b_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\r\n        b_st.indices, b_st.values, b_st.dense_shape)\r\n\r\n    # Compute the CSR SparseMatrix matrix multiplication\r\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\r\n        a=a_sm, b=b_sm, type=tf.float32)\r\n\r\n    # Convert the CSR SparseMatrix product to a dense Tensor\r\n    c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\r\n        c_sm, tf.float32)\r\n    # Evaluate the dense Tensor value\r\n    c_sm_dense_value = sess.run(c_sm_dense)\r\n    print(c_sm_dense_value)\r\n    # print(sess.run(a_st))\r\n\r\n```\r\n\r\n### **error  message:**\r\n```\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"test3.py\", line 92, in <module>\r\n    b_st.indices, b_st.values, b_st.dense_shape)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/ops/linalg/sparse/gen_sparse_csr_matrix_ops.py\", line 1332, in sparse_tensor_to_csr_sparse_matrix\r\n    dense_shape=dense_shape, name=name)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 522, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1525, in convert_to_tensor\r\n    raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n\r\n```", "comments": ["The problem exists because the `b_st` is the value from the calling the function of `dense_tensor_to_sparse_tensor`. From the perspective of  a user, it should not be the problem, which means whether `b_st` is directly the assigned sparse tensor or the return value from the function should not be different.", "I've made some arrangement of the code, resulting in the same problem.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n# import tensorflow.compat.v1 as tf\r\n# tf.disable_v2_behavior()\r\n\r\n\r\nfrom tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops\r\n\r\na_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])\r\na_values = np.array([1.0, 5.0, -1.0, -2.0], np.float32)\r\na_dense_shape = [4, 5]\r\n\r\na_st = tf.sparse.SparseTensor(a_indices, a_values, a_dense_shape)\r\n\r\nB = tf.constant([2.0, 7.0, 8.0,9.0,10.0])\r\n\r\ndef sparse_matrix_dense_tensor1d_matmul(sparse_matrix, tensor1d):\r\n    # Convert SparseTensors to CSR SparseMatrix\r\n    a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\r\n        sparse_matrix.indices, sparse_matrix.values, sparse_matrix.dense_shape)\r\n\r\n    col = tensor1d.shape.as_list()[0]\r\n\r\n    x = np.arange(col)\r\n    y = np.zeros(1)\r\n    xi, yi = np.meshgrid(x, y)\r\n    b_st = tf.sparse.SparseTensor(indices=np.array(np.vstack([xi.ravel(), yi.ravel()]).T),\r\n                                  values=tensor1d.numpy(),\r\n                                  dense_shape=[col, 1])\r\n\r\n    b_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(\r\n        b_st.indices, b_st.values, b_st.dense_shape)\r\n\r\n    # Compute the CSR SparseMatrix matrix multiplication\r\n    c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(\r\n        a=a_sm, b=b_sm, type=tf.float32)\r\n\r\n    # Convert the CSR SparseMatrix product to a dense Tensor\r\n    c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(\r\n        c_sm, tf.float32)\r\n\r\n    return c_sm_dense\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    sparse_matrix_dense_tensor1d_matmul(a_st, B)\r\n```\r\n\r\n### **Error**\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test3.py\", line 114, in <module>\r\n    sparse_matrix_dense_tensor1d_matmul(a_st, B)\r\n  File \"test3.py\", line 85, in sparse_matrix_dense_tensor1d_matmul\r\n    sparse_matrix.indices, sparse_matrix.values, sparse_matrix.dense_shape)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/ops/linalg/sparse/gen_sparse_csr_matrix_ops.py\", line 1332, in sparse_tensor_to_csr_sparse_matrix\r\n    dense_shape=dense_shape, name=name)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 522, in _apply_op_helper\r\n    preferred_dtype=default_dtype)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py\", line 163, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1525, in convert_to_tensor\r\n    raise RuntimeError(\"Attempting to capture an EagerTensor without \"\r\nRuntimeError: Attempting to capture an EagerTensor without building a function.\r\n\r\n```", "@sjtusmartboy \r\nInstead of defining your own function to convert dense tensor to sparse tensor you can use \r\n`tf.sparse.from_dense`. So you wont need eager tensors at all. Also you are using tensorflow1 which uses Graph execution but your function returns an eager tensor which is not allowed, and thus the error. ", "@old-school-kid Thank you, I've solved the problem", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50978\">No</a>\n", "@old-school-kid If `a` is a sparse column vector`(n*1)`, then how to get the result of `a matmul a.T`, which is also the sparse matrix with dense_shape `(n*n)` ? Any available solution in tensorflow? By the way, `a matmul a.T` is too large to be stored in the memory, haha.", " Hi @sjtusmartboy I believe this is what you are looking for\r\n`tf.linalg.matmul(tf.sparse.to_dense(a),\r\n              tf.sparse.to_dense( tf.sparse.transpose(a)),\r\n              a_is_sparse=True, b_is_sparse=True)`\r\n\r\nIf the product is too large you can store the values and index only and reconstruct the tensor when needed. Details can be found in the [doc](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)", "@old-school-kid It seems `tf.linalg.matmul` returns the dense matrix of `a matmul a.T`, which is too large to be stored in the memory. ", "I have same issue in code below:\r\n```\r\ndef simcse_loss(y_true, y_pred):\r\n      batch = tf.shape(y_pred)[0]\r\n      idx = tf.range(0, batch, dtype=tf.int32)\r\n      idx_1 = idx[None, :]\r\n      idx_2 = (idx + 1 - idx % 2 * 2)[:, None]\r\n      y_true = tf.equal(idx_1, idx_2)\r\n      y_true = tf.cast(y_true, tf.float32)\r\n      y_pred = tf.math.l2_normalize(y_pred, axis=1)\r\n      similarities = tf.matmul(y_pred, tf.transpose(y_pred))\r\n      similarities = similarities - tf.eye(tf.shape(y_pred)[0]) * 1e12\r\n      similarities = similarities * 20\r\n      loss = tf.keras.losses.categorical_crossentropy(y_true, similarities, from_logits=True)\r\n      return tf.reduce_mean(loss)\r\n```\r\n`tf.eye` trigger the error \"RuntimeError: Attempting to capture an EagerTensor without building a function.\"\r\nNotably, in tf2.5 it works fine, but throw error in tf2.3", "> I have same issue in code below:\r\n> \r\n> ```\r\n> def simcse_loss(y_true, y_pred):\r\n>       batch = tf.shape(y_pred)[0]\r\n>       idx = tf.range(0, batch, dtype=tf.int32)\r\n>       idx_1 = idx[None, :]\r\n>       idx_2 = (idx + 1 - idx % 2 * 2)[:, None]\r\n>       y_true = tf.equal(idx_1, idx_2)\r\n>       y_true = tf.cast(y_true, tf.float32)\r\n>       y_pred = tf.math.l2_normalize(y_pred, axis=1)\r\n>       similarities = tf.matmul(y_pred, tf.transpose(y_pred))\r\n>       similarities = similarities - tf.eye(tf.shape(y_pred)[0]) * 1e12\r\n>       similarities = similarities * 20\r\n>       loss = tf.keras.losses.categorical_crossentropy(y_true, similarities, from_logits=True)\r\n>       return tf.reduce_mean(loss)\r\n> ```\r\n> \r\n> `tf.eye` trigger the error \"RuntimeError: Attempting to capture an EagerTensor without building a function.\"\r\n> Notably, in tf2.5 it works fine, but throw error in tf2.3\r\n\r\nI guess that tensorflow doesn't allow user to create new tensor in some context. But I can't find any documents for this", "@rxy1212 I agree. tf is so difficult to use. It cannot let us focus on the model optimization, but wasting lots of time on these meaningless bugs.", "Hi @rxy1212 \r\nTensorflow 2 by default executes in Eager mode and thus `tf.shape(y_pred)[0]` will throw an error. Had it been in graph mode which can be turned on by disabling the tensorflow2 behavior there would have been no such error. There was a major update in Tensorflow2.5 which made it more friendly to use and in the background tf.autograph is executed for you. Thus you will face the error in Tf2.3 and not in Tf2.5.\r\n\r\nAlso @sjtusmartboy \r\nA framework is merely a tool and the most important part of DL is the [maths](https://www.tensorflow.org/guide/intro_to_graphs) underneath. ", "@old-school-kid I agree with, the most import part is the math, but I find it not easy to make iteration of algorithms in a short time", "You should avoid using the Session API in TF2. I'm not 100% sure, but it likely is the cause for your error. See [this guide](https://www.tensorflow.org/guide/migrate#1_replace_v1sessionrun_calls).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50978\">No</a>\n"]}, {"number": 50977, "title": "Issue with saving trained model of module TensorFlow Recommenders (tfrs)", "body": "Hi,\r\n\r\nI'm currently working on a model of module TensorFlow Recommenders (tfrs):\r\n\r\nhttps://www.tensorflow.org/recommenders/examples/quickstart?hl=en\r\n\r\nI want to save the model after builting, compling and fitting it, and load it for prediction, but it doesn't work with functions .save() from tf.saved_model and .save_model() from tf.keras.models. I'll appriciate it if someone share any expiriences about it.  Thanks :)\r\n\r\n", "comments": ["@tahaabdolkarimi \r\n\r\nIt looks like the Issue relates to the Keras component. Please submit it to the [github.com/keras-team/keras](https://github.com/keras-team/keras/issues) repository instead. As previously [announced](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999) all future development of Keras is expected to happen in the keras-team/keras repository. If your issue lies with the TF-Core area please comment back with your explanation and we can look into it further. Thanks!\r\n\r\n", "@UsharaniPagadala \r\n\r\nAs far as I got it, the issue relates to the TF, because the TensorFlow Recommenders (tfrs) is based on Tensorflow and the Issue Tracking on the official Website reffer to this Github page. You can also take a look to the link that I mentioned at perious comment. Thanks in advance.", "@tahaabdolkarimi \r\n\r\nFill the issue [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md)\r\n\r\nCould you please provide the colab gist with all the dependencies to analyse the issue better.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50977\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50977\">No</a>\n", "I'm also facing the same issue, I agree with @tahaabdolkarimi  , the issue is not Keras-related yet ; I tried to save the trained model from quick start tutorial as `tf.saved_model.save(model, ...)` and loaded similarly , I haven't seen any issues all while but when a loaded model is passed a dictionary of inputs I see the following error message:\r\n\r\n`ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\r\n  Positional arguments (2 total):\r\n    * {'user_id': <tf.Tensor 'features_1:0' shape=(1, 1682) dtype=int64>, 'movie_title': <tf.Tensor 'features:0' shape=(1, 1682) dtype=int64>}\r\n    * Inference\r\n  Keyword arguments: {}\r\n\r\n Expected these arguments to match one of the following 1 option(s):\r\n\r\nOption 1:\r\n  Positional arguments (2 total):\r\n    * {'movie_title': RaggedTensorSpec(TensorShape([None, None]), tf.string, 1, tf.int64), 'user_id': RaggedTensorSpec(TensorShape([None, None]), tf.string, 1, tf.int64)}\r\n    * Train\r\n  Keyword arguments: {}`\r\n  \r\n  \r\n  Somewhat surprisingly , when I pass my dictionary of inputs to the model instance which was trained and in-memory ( not the saved one ) , everything runs smoothly and I get scores. Would really appreciate if someone could point to a detailed documentation on how to save TFR models, I checked TFR -> Keras -> Saved_Model format , but couldn't understand examples provided there."]}, {"number": 50976, "title": "Add missing closing backtick in parse_sequence_example docstring", "body": "", "comments": []}, {"number": 50975, "title": "failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal", "body": "Hello,\r\n\r\nI am trying to train a neural network with multiple GPUs on multiple nodes, I have a .pbs script which starts, for each node/worker, the following program:\r\n\r\n```\r\nimport os\r\nimport sys\r\nimport json\r\nimport time\r\nimport tensorflow as tf\r\n\r\n\r\nper_worker_batch_size = 256\r\nhosts = sys.argv[1].split(',')\r\ntask_id = int(sys.argv[2])\r\nnum_workers = len(hosts)\r\ntab = []\r\nfor host in hosts:\r\n    tab.append(str(host)+\":2222\")\r\ncluster = tf.train.ClusterSpec({'worker': tab})\r\ncluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(cluster,\r\n                                         task_type='worker', task_id=task_id,\r\n                                         num_accelerators={'GPU': 0})\r\ndata_options = tf.data.Options()\r\ndata_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\noptions = tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(cluster_resolver=cluster_resolver, communication_options=options)       \r\nglobal_batch_size = per_worker_batch_size * num_workers\r\n\r\nfrom Controller import *\r\nfrom Utils import *\r\nimport numpy as np\r\n\r\ndef main():\r\n\r\n    with strategy.scope():\r\n        model = get_compiled_cnn_model()\r\n\r\n\r\n    X = np.random.rand(10000,8,900)\r\n    y = np.random.choice(2, 10000)\r\n    y = keras.utils.to_categorical(y, num_classes=2)\r\n    \r\n    \r\n    epochs_=5\r\n    history = train_child(X, y, model, global_batch_size, epochs_, data_options, callbacks=[])\r\n\r\n\r\n\r\n    \r\ndef get_compiled_cnn_model():\r\n\r\n    outputs = []\r\n    input_shape = (8, 900, 1)\r\n    inputs = keras.Input(shape=(8, 900))\r\n    x = layers.Reshape((8,900,1))(inputs)\r\n    x = layers.BatchNormalization()(x)\r\n    x = layers.Conv2D(10, (2, 10), padding=\"same\", activation='relu')(x)\r\n    outputs.append(x)\r\n\r\n    x = layers.Flatten()(x)\r\n    x = layers.BatchNormalization()(x)\r\n    x = layers.Dropout(0.25)(x)\r\n    x = layers.Dense(10)(x)\r\n    x = layers.Dropout(.25)(x)\r\n\r\n    outputss = layers.Dense(2, activation='softmax')(x)\r\n    model = keras.Model(inputs=inputs, outputs=outputss)\r\n    loss = keras.losses.CategoricalCrossentropy()\r\n    model.compile(loss=loss, optimizer=keras.optimizers.Adam(1e-3), metrics=[\"accuracy\"])\r\n\r\n    return model\r\n\r\n\r\n\r\ndef train_child(X, y, model, batch_size, epochs_child, options, callbacks):\r\n\r\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=True)\r\n    train_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\r\n    val_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\r\n\r\n    train_data = train_data.batch(batch_size)\r\n    val_data = val_data.batch(batch_size)\r\n    train_data = train_data.with_options(options)\r\n    val_data = val_data.with_options(options)\r\n\r\n    call = None\r\n\r\n    if(len(callbacks)>0):\r\n        call = callbacks\r\n    history = model.fit(\r\n            train_data,\r\n            validation_data=val_data,\r\n            epochs=epochs_child,\r\n            batch_size=batch_size,\r\n            callbacks=call,\r\n            verbose=1,\r\n        )\r\n    return history     \r\n\r\n    \r\nif __name__ == \"__main__\":\r\n\r\n    main()\r\n```\r\n\r\nThe following error appear when running the script:\r\n\r\n> F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_INVALID_DEVICE: invalid device ordinal\r\n\r\n\r\nThis code is working on 1 node with mutiple GPUs but it is not working on multiple nodes.\r\n\r\n\r\nAlso here is the content of the yml file from the conda env I am using (called TestPythonGPU):\r\n```\r\n\r\nname: TestPythonGPU\r\nchannels:\r\n  - anaconda\r\n  - conda-forge\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=4.5=1_gnu\r\n  - _tflow_select=2.1.0=gpu\r\n  - absl-py=0.13.0=py38h06a4308_0\r\n  - aiohttp=3.7.4=py38h27cfd23_1\r\n  - astor=0.8.1=py38h06a4308_0\r\n  - astunparse=1.6.3=py_0\r\n  - async-timeout=3.0.1=py38h06a4308_0\r\n  - attrs=21.2.0=pyhd3eb1b0_0\r\n  - blas=1.0=openblas\r\n  - blinker=1.4=py38h06a4308_0\r\n  - brotlipy=0.7.0=py38h27cfd23_1003\r\n  - c-ares=1.17.1=h27cfd23_0\r\n  - ca-certificates=2021.7.5=h06a4308_1\r\n  - cachetools=4.2.2=pyhd3eb1b0_0\r\n  - cairo=1.14.12=h8948797_3\r\n  - certifi=2021.5.30=py38h06a4308_0\r\n  - cffi=1.14.5=py38h261ae71_0\r\n  - chardet=3.0.4=py38h06a4308_1003\r\n  - click=8.0.1=pyhd3eb1b0_0\r\n  - colorama=0.4.4=pyhd3eb1b0_0\r\n  - coverage=5.5=py38h27cfd23_2\r\n  - cryptography=3.4.7=py38hd23ed53_0\r\n  - cudatoolkit=10.1.243=h6bb024c_0\r\n  - cudnn=7.6.5=cuda10.1_0\r\n  - cupti=10.1.168=0\r\n  - cycler=0.10.0=py38_0\r\n  - cython=0.29.23=py38h2531618_0\r\n  - dbus=1.13.18=hb2f20db_0\r\n  - expat=2.4.1=h2531618_2\r\n  - fontconfig=2.13.1=h6c09931_0\r\n  - freetype=2.10.4=h5ab3b9f_0\r\n  - fribidi=1.0.10=h7b6447c_0\r\n  - future=0.18.2=py38_1\r\n  - gast=0.4.0=py_0\r\n  - glib=2.56.2=hd408876_0\r\n  - google-auth=1.32.0=pyhd3eb1b0_0\r\n  - google-auth-oauthlib=0.4.4=pyhd3eb1b0_0\r\n  - google-pasta=0.2.0=py_0\r\n  - graphite2=1.3.14=h23475e2_0\r\n  - graphviz=2.40.1=h21bd128_2\r\n  - grpcio=1.38.1=py38hdd6454d_0\r\n  - gst-plugins-base=1.14.0=hbbd80ab_1\r\n  - gstreamer=1.14.0=hb31296c_0\r\n  - h5py=2.10.0=nompi_py38h9915d05_106\r\n  - harfbuzz=1.8.8=hffaf4a1_0\r\n  - hdf5=1.10.6=hb1b8bf9_0\r\n  - icu=58.2=he6710b0_3\r\n  - idna=2.10=pyhd3eb1b0_0\r\n  - importlib-metadata=3.10.0=py38h06a4308_0\r\n  - intel-openmp=2021.2.0=h06a4308_610\r\n  - joblib=1.0.1=pyhd3eb1b0_0\r\n  - jpeg=9b=h024ee3a_2\r\n  - keras=2.4.3=0\r\n  - keras-base=2.4.3=py_0\r\n  - keras-preprocessing=1.1.2=pyhd3eb1b0_0\r\n  - keras-tuner=1.0.1=py_0\r\n  - kiwisolver=1.3.1=py38h2531618_0\r\n  - lcms2=2.12=h3be6417_0\r\n  - ld_impl_linux-64=2.35.1=h7274673_9\r\n  - libffi=3.3=he6710b0_2\r\n  - libgcc-ng=9.3.0=h5101ec6_17\r\n  - libgfortran-ng=7.5.0=ha8ba4b0_17\r\n  - libgfortran4=7.5.0=ha8ba4b0_17\r\n  - libgomp=9.3.0=h5101ec6_17\r\n  - libopenblas=0.3.13=h4367d64_0\r\n  - libpng=1.6.37=hbc83047_0\r\n  - libprotobuf=3.14.0=h8c45485_0\r\n  - libstdcxx-ng=9.3.0=hd4cf53a_17\r\n  - libtiff=4.2.0=h85742a9_0\r\n  - libuuid=1.0.3=h1bed415_2\r\n  - libwebp-base=1.2.0=h27cfd23_0\r\n  - libxcb=1.14=h7b6447c_0\r\n  - libxml2=2.9.12=h03d6c58_0\r\n  - lz4-c=1.9.3=h2531618_0\r\n  - markdown=3.3.4=py38h06a4308_0\r\n  - matplotlib=3.3.4=py38h578d9bd_0\r\n  - matplotlib-base=3.3.4=py38h62a2d02_0\r\n  - mkl=2021.2.0=h06a4308_296\r\n  - mkl-service=2.3.0=py38h27cfd23_1\r\n  - multidict=5.1.0=py38h27cfd23_2\r\n  - ncurses=6.2=he6710b0_1\r\n  - numpy=1.19.1=py38h30dfecb_0\r\n  - numpy-base=1.19.1=py38h75fe3a5_0\r\n  - oauthlib=3.1.0=py_0\r\n  - olefile=0.46=py_0\r\n  - openblas=0.3.4=h9ac9557_1000\r\n  - openssl=1.1.1k=h27cfd23_0\r\n  - opt_einsum=3.3.0=pyhd3eb1b0_1\r\n  - pandas=1.1.3=py38he6710b0_0\r\n  - pango=1.42.4=h049681c_0\r\n  - pcre=8.45=h295c915_0\r\n  - pillow=8.2.0=py38he98fc37_0\r\n  - pip=21.1.3=py38h06a4308_0\r\n  - pixman=0.40.0=h7b6447c_0\r\n  - protobuf=3.14.0=py38h2531618_1\r\n  - pyasn1=0.4.8=py_0\r\n  - pyasn1-modules=0.2.8=py_0\r\n  - pycparser=2.20=py_2\r\n  - pydot=1.4.1=py38_0\r\n  - pyjwt=1.7.1=py38_0\r\n  - pyopenssl=20.0.1=pyhd3eb1b0_1\r\n  - pyparsing=2.4.7=pyhd3eb1b0_0\r\n  - pyqt=5.9.2=py38h05f1152_4\r\n  - pysocks=1.7.1=py38h06a4308_0\r\n  - python=3.8.10=h12debd9_8\r\n  - python-dateutil=2.8.1=pyhd3eb1b0_0\r\n  - python-flatbuffers=2.0=pyhd8ed1ab_0\r\n  - python_abi=3.8=1_cp38\r\n  - pytz=2021.1=pyhd3eb1b0_0\r\n  - pyyaml=5.4.1=py38h27cfd23_1\r\n  - qt=5.9.7=h5867ecd_1\r\n  - readline=8.1=h27cfd23_0\r\n  - requests=2.25.1=pyhd3eb1b0_0\r\n  - requests-oauthlib=1.3.0=py_0\r\n  - rsa=4.7.2=pyhd3eb1b0_1\r\n  - scikit-learn=0.24.1=py38ha9443f7_0\r\n  - scipy=1.6.2=py38hf56f3a7_1\r\n  - setuptools=52.0.0=py38h06a4308_0\r\n  - sip=4.19.13=py38he6710b0_0\r\n  - six=1.16.0=pyh6c4a22f_0\r\n  - sqlite=3.36.0=hc218d9a_0\r\n  - tabulate=0.8.9=py38h06a4308_0\r\n  - tensorboard=2.4.0=pyhc547734_0\r\n  - tensorboard-plugin-wit=1.8.0=pyh44b312d_0\r\n  - tensorflow=2.4.1=gpu_py38h8a7d6ce_0\r\n  - tensorflow-base=2.4.1=gpu_py38h29c2da4_0\r\n  - tensorflow-estimator=2.5.0=pyh81a9013_1\r\n  - tensorflow-gpu=2.4.1=h30adc30_0\r\n  - termcolor=1.1.0=py38h06a4308_1\r\n  - terminaltables=3.1.0=py_0\r\n  - threadpoolctl=2.1.0=pyh5ca1d4c_0\r\n  - tk=8.6.10=hbc83047_0\r\n  - tornado=6.1=py38h27cfd23_0\r\n  - tqdm=4.61.2=pyhd3eb1b0_1\r\n  - typing-extensions=3.10.0.0=hd8ed1ab_0\r\n  - typing_extensions=3.10.0.0=pyha770c72_0\r\n  - urllib3=1.26.6=pyhd3eb1b0_1\r\n  - werkzeug=1.0.1=pyhd3eb1b0_0\r\n  - wheel=0.36.2=pyhd3eb1b0_0\r\n  - wrapt=1.12.1=py38h7b6447c_1\r\n  - xz=5.2.5=h7b6447c_0\r\n  - yaml=0.2.5=h7b6447c_0\r\n  - yarl=1.6.3=py38h27cfd23_0\r\n  - zipp=3.5.0=pyhd3eb1b0_0\r\n  - zlib=1.2.11=h7b6447c_3\r\n  - zstd=1.4.9=haebb681_0\r\nprefix: /data/leuven/339/vsc33965/miniconda3/envs/TestPythonGPU\r\n\r\n```\r\n\r\n\r\n\r\nThank you for your support\r\n\r\nAymeric\r\n", "comments": ["@aymeric75 ,\r\n\r\nCan you please try installing TensorFlow v2.4 with CUDA 11.0 and cuDNN 8 and check if you are facing the same error. For more information please take a look at the tested build [configurations](https://www.tensorflow.org/install/source#gpu).Thanks!\r\n", "Hello, thank you for your support . You can see on the list of my conda env that the version of tensorflow is 2.4 as well as for tensorflow-gpu.\r\n\r\nThe PBS file I use also load CUDA 11.2.2 and cuDNN 8.1.1, here it is:\r\n```\r\n\r\n\r\n#!/usr/bin/env bash\r\n#PBS -N tensorflowtest\r\n#PBS -l nodes=1:ppn=9:gpus=1:skylake+1:ppn=9:gpus=1:skylake\r\n#PBS -l partition=gpu\r\n#PBS -l walltime=00:05:00\r\n#PBS -W x=nmatchpolicy:exactnode\r\n#PBS -A my_account\r\n\r\ncd $PBS_O_WORKDIR\r\n\r\nmodule purge\r\nmodule use /2019b/modules/all\r\nmodule load cuDNN/8.1.1.33-CUDA-11.2.2\r\nsource $VSZ_DATA/miniconda3/etc/profile.d/conda.sh\r\nsource activate TestPythonGPU\r\n\r\n\r\nexport HOSTS=`sort -u $PBS_NODEFILE | paste -s -d,`\r\n\r\nindex=0\r\nfor host in `sort -u $PBS_NODEFILE`; do\r\n  outfile=$host\".out\"\r\n  errfile=$host\".err\"\r\n  echo $host;\r\n  if [ $HOSTNAME == $host ]; then\r\n     python test.py $HOSTS $index 1>$outfile 2>$errfile &\r\n  else\r\n     ssh -f $host \"bash --login -c 'cd $PBS_O_WORKDIR; module purge; module use /2019b/modules/all;module load cuDNN/8.1.1.33-CUDA-11.2.2;source $VSZ_DATA/miniconda3/etc/profile.d/conda.sh;source activate TestPythonGPU; nohup python test.py $HOSTS $index 1>$outfile 2>$errfile &'\"\r\n  fi\r\n  index=$((index+1))\r\ndone\r\n\r\nwait\r\n\r\n\r\n```\r\n\r\nRegards\r\n\r\nAymeric", "@aymeric75 ,\r\n\r\nAs mentioned in tested build configurations tensorflow v2.4 is compatible with CUDA 11.0 and cuDNN 8.Can you please try to [install](https://www.tensorflow.org/install) with the compatible configurations and let us know if you are facing same issue.Also Installation issues within the Anaconda environment are tracked in the Anaconda repo.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50975\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50975\">No</a>\n"]}, {"number": 50974, "title": "None of the Demo Edge TPU Models Work - \u201cModule Provided has model identifier 'l><b', should be 'TFL3'\u201d", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 10\r\n\r\n- TensorFlow installed from (source or binary):\r\nTensorflow 2.5\r\n\r\n- Python version:\r\nPython 3.8\r\n- Installed using virtualenv? pip? conda?:\r\npip\r\n\r\n\r\n****Describe the problem****\r\n\r\nUsing a Coral Devboard, and trying to run the `detect.py` file from https://github.com/google-coral/pycoral/blob/master/examples/detect_image.py with the models on the https://coral.ai/models/object-detection/ page. Tried `ssd mobilenetv2` and `EfficientDet Lite`, both get the same error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"detect.py\", line 108, in <module>\r\n    main()\r\n  File \"detect.py\", line 73, in main\r\n    interpreter = make_interpreter(args.model)\r\n  File \"/usr/lib/python3/dist-packages/pycoral/utils/edgetpu.py\", line 93, in make_interpreter\r\n    model_path=model_path_or_content, experimental_delegates=delegates)\r\n  File \"/usr/lib/python3/dist-packages/tflite_runtime/interpreter.py\", line 351, in __init__\r\n    experimental_preserve_all_tensors))\r\nValueError: Model provided has model identifier 'l><b', should be 'TFL3'\r\n```\r\n\r\nNot sure what the problem is, since both get the exact same error. Help is appreciated!\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI ran the command `python3 detect.py -m efficientdet_lite3_512_ptq_edgetpu.tflite -i parking.jpg -l coco_labels.txt -o result.jpg`\r\n", "comments": ["Could you file this bug report at the google coral project instead?", "Yes sure! Thank you", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50974\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50974\">No</a>\n"]}, {"number": 50973, "title": "Documentation: old installation requirements", "body": "Hi,\r\n\r\non the [page with installation instructions](https://www.tensorflow.org/install?hl=en), the listed requirements have not been updated: it lists Python 3.6\u20133.8 explicitly.\r\n\r\nAccording to the [pip installation guide](https://www.tensorflow.org/install/pip), Python 3.9 is a valid requirement.\r\n\r\nI suggest updating the page.\r\n\r\nBest regards\r\nThomas", "comments": ["@ternite I fixed it.\r\nhttps://www.tensorflow.org/install?hl=en\r\n![Screenshot from 2021-07-28 13-21-04](https://user-images.githubusercontent.com/33194443/127263222-0abca4f2-1263-4291-b753-8a871e821646.png)\r\n", "Moving this to closed status as issue is resolved."]}, {"number": 50972, "title": "[PJRT] add platfrom_version for rocm in pjrt/gpu_device", "body": "This PR fixes the returned value from platform_version() for ROCm.\r\n@hawkinsp @deven-amd ", "comments": ["@hawkinsp, could you please post the \"feedback/copybara\" failure log here.", "This is the error \r\n\r\n` error loading package 'third_party/tensorflow/compiler/xla/pjrt': Encountered error while reading extension file 'rocm/build_defs.bzl': no such package '@local_config_rocm//rocm': no such package '@local_config_rocm//': Accessing external repositories is not supported`\r\n\r\n"]}, {"number": 50971, "title": "Keras.io XRay TPU example fails on 2.6.0-rc1", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: none\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0-rc1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: Current Colab (July 27, 2021)\r\n- GPU model and memory: TPU, 8 core, Colab standard\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nExample fails in model.fit()\r\n\r\n**Describe the expected behavior**\r\nComplete notebook finishes in Colab with TPU, standard memory size\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nOpen this example in Colab. Set runtime to TPU with standard size memory. \r\n\r\nhttps://keras.io/examples/vision/xray_classification_with_tpus/\r\n\r\nInstall Tensorflow 2.6.0-rc1 at beginning:\r\n!pip uninstall -y tensorflow\r\n!pip install -q tensorflow-2.6.0-rc1\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nRun entire notebook via F9. You will eventually receive this stack trace in model.fit(), before finishing any epochs:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nNotFoundError                             Traceback (most recent call last)\r\n<ipython-input-38-2dcbec06520a> in <module>()\r\n      1 with strategy.scope():\r\n----> 2     model = build_model()\r\n      3 \r\n      4     METRICS = [\r\n      5         tf.keras.metrics.BinaryAccuracy(),\r\n\r\n26 frames\r\n<ipython-input-34-b10aa2c50d05> in build_model()\r\n      3     inputs = keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\r\n      4     x = preprocessing.Rescaling(1.0 / 255)(inputs)\r\n----> 5     x = layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(x)\r\n      6     x = layers.Conv2D(16, 3, activation=\"relu\", padding=\"same\")(x)\r\n      7     x = layers.MaxPool2D()(x)\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    975     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n    976       return self._functional_construction_call(inputs, args, kwargs,\r\n--> 977                                                 input_list)\r\n    978 \r\n    979     # Maintains info about the `Layer.call` stack.\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1113       # Check input assumptions set after layer building, e.g. input shape.\r\n   1114       outputs = self._keras_tensor_symbolic_call(\r\n-> 1115           inputs, input_masks, args, kwargs)\r\n   1116 \r\n   1117       if outputs is None:\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)\r\n    846       return tf.nest.map_structure(keras_tensor.KerasTensor, output_signature)\r\n    847     else:\r\n--> 848       return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n    849 \r\n    850   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)\r\n    884           # overridden).\r\n    885           # TODO(kaftan): do we maybe_build here, or have we already done it?\r\n--> 886           self._maybe_build(inputs)\r\n    887           inputs = self._maybe_cast_inputs(inputs)\r\n    888           outputs = call_fn(inputs, *args, **kwargs)\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py in _maybe_build(self, inputs)\r\n   2657         # operations.\r\n   2658         with tf_utils.maybe_init_scope(self):\r\n-> 2659           self.build(input_shapes)  # pylint:disable=not-callable\r\n   2660       # We must set also ensure that the layer is marked as built, and the build\r\n   2661       # shape is stored since user defined build functions may not be calling\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/layers/convolutional.py in build(self, input_shape)\r\n    202         constraint=self.kernel_constraint,\r\n    203         trainable=True,\r\n--> 204         dtype=self.dtype)\r\n    205     if self.use_bias:\r\n    206       self.bias = self.add_weight(\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\r\n    661         synchronization=synchronization,\r\n    662         aggregation=aggregation,\r\n--> 663         caching_device=caching_device)\r\n    664     if regularizer is not None:\r\n    665       # TODO(fchollet): in the future, this should be handled at the\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\r\n    816         dtype=dtype,\r\n    817         initializer=initializer,\r\n--> 818         **kwargs_for_getter)\r\n    819 \r\n    820     # If we set an initializer and the variable processed it, tracking will not\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\r\n    127       synchronization=synchronization,\r\n    128       aggregation=aggregation,\r\n--> 129       shape=variable_shape if variable_shape else None)\r\n    130 \r\n    131 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)\r\n    264   def __call__(cls, *args, **kwargs):\r\n    265     if cls is VariableV1:\r\n--> 266       return cls._variable_v1_call(*args, **kwargs)\r\n    267     elif cls is Variable:\r\n    268       return cls._variable_v2_call(*args, **kwargs)\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\r\n    225         synchronization=synchronization,\r\n    226         aggregation=aggregation,\r\n--> 227         shape=shape)\r\n    228 \r\n    229   def _variable_v2_call(cls,\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py in getter(**kwargs)\r\n     65 \r\n     66   def getter(**kwargs):\r\n---> 67     return captured_getter(captured_previous, **kwargs)\r\n     68 \r\n     69   return getter\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py in creator_with_resource_vars(next_creator, **kwargs)\r\n   2125         checkpoint_restore_uid = None\r\n   2126 \r\n-> 2127       created = self._create_variable(next_creator, **kwargs)\r\n   2128 \r\n   2129       if checkpoint_restore_uid is not None:\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py in _create_variable(self, next_creator, **kwargs)\r\n   1167         self._container_strategy(), _real_mirrored_creator,\r\n   1168         distribute_utils.TPU_VARIABLE_CLASS_MAPPING,\r\n-> 1169         distribute_utils.TPU_VARIABLE_POLICY_MAPPING, **kwargs)\r\n   1170 \r\n   1171   def _gather_to_implementation(self, value, destinations, axis, options):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_utils.py in create_mirrored_variable(strategy, real_mirrored_creator, class_mapping, policy_mapping, **kwargs)\r\n    306   # here.\r\n    307   with tape.stop_recording():\r\n--> 308     value_list = real_mirrored_creator(**kwargs)\r\n    309     # MirroredVariable is recreated during saved_model loading, and its\r\n    310     # component variables (value_list) will have None initializer. We\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py in _real_mirrored_creator(**kwargs)\r\n   1146             with maybe_init_scope():\r\n   1147               initial_value = initial_value() if callable(\r\n-> 1148                   initial_value) else initial_value\r\n   1149 \r\n   1150           if i > 0:\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/initializers/initializers_v2.py in __call__(self, shape, dtype, **kwargs)\r\n    515     else:\r\n    516       limit = math.sqrt(3.0 * scale)\r\n--> 517       return self._random_generator.random_uniform(shape, -limit, limit, dtype)\r\n    518 \r\n    519   def get_config(self):\r\n\r\n/usr/local/lib/python3.7/dist-packages/keras/initializers/initializers_v2.py in random_uniform(self, shape, minval, maxval, dtype)\r\n    971       op = tf.random.uniform\r\n    972     return op(\r\n--> 973         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\r\n    974 \r\n    975   def truncated_normal(self, shape, mean, stddev, dtype):\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    204     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    205     try:\r\n--> 206       return target(*args, **kwargs)\r\n    207     except (TypeError, ValueError):\r\n    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/random_ops.py in random_uniform(shape, minval, maxval, dtype, seed, name)\r\n    313           result = math_ops.multiply(result, maxval)\r\n    314       else:\r\n--> 315         result = math_ops.add(result * (maxval - minval), minval, name=name)\r\n    316     # TODO(b/132092188): C++ shape inference inside functional ops does not\r\n    317     # cross FuncGraph boundaries since that information is only available in\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n   1365         #   r_binary_op_wrapper use different force_same_dtype values.\r\n   1366         x, y = maybe_promote_tensors(x, y, force_same_dtype=False)\r\n-> 1367         return func(x, y, name=name)\r\n   1368       except (TypeError, ValueError) as e:\r\n   1369         # Even if dispatching the op failed, the RHS may be a tensor aware\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    204     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    205     try:\r\n--> 206       return target(*args, **kwargs)\r\n    207     except (TypeError, ValueError):\r\n    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in subtract(x, y, name)\r\n    546 @dispatch.add_dispatch_support\r\n    547 def subtract(x, y, name=None):\r\n--> 548   return gen_math_ops.sub(x, y, name)\r\n    549 \r\n    550 \r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in sub(x, y, name)\r\n  10642       return _result\r\n  10643     except _core._NotOkStatusException as e:\r\n> 10644       _ops.raise_from_not_ok_status(e, name)\r\n  10645     except _core._FallbackException:\r\n  10646       pass\r\n\r\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6939   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6940   # pylint: disable=protected-access\r\n-> 6941   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6942   # pylint: enable=protected-access\r\n   6943 \r\n\r\n/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nNotFoundError: '_EagerConst' is neither a type of a primitive operation nor a name of a function registered in binary running on n-d02110c1-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:Sub]\r\n```", "comments": ["@Saduf2019 ,\r\nI was able to reproduce the issue in tf [v2.6](https://colab.research.google.com/gist/tilakrayal/1e490fbef49f3e0b81229fd97b538588/2-6xray_classification_with_tpus.ipynb#scrollTo=l73bljfopCML) and in [v2.4](https://colab.research.google.com/gist/tilakrayal/4402177ef22d4c65a0f1f42bce61b3f1/2-4xray_classification_with_tpus.ipynb) and [v2.5](https://colab.research.google.com/gist/tilakrayal/f7ac7341195a1b10048fea3db698badf/2-5xray_classification_with_tpus.ipynb) code was running without any error.Please find the gist here.", "@LanceNorskog \r\nPlease post this issue on keras-team/keras repo.\r\nTo know more refer to:\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50971\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50971\">No</a>\n", "I have posted this on the keras issues page. I have some trouble believing that it is a Keras issue, given that all platforms make the same TF calls."]}, {"number": 50970, "title": "Node number 106 (FlexTensorScatterUpdate) failed to prepare.", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- TensorFlow installed from (source or binary):source  ,pip install tensorflow-gpu==2.5.0\r\n- TensorFlow version (or github SHA if from source):2.5.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\r\nWARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\r\nModel: \"model_1\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_images (InputLayer)    [(1, 128, 128, 3)]        0         \r\n_________________________________________________________________\r\nface_detetor_1 (FaceDetetor) (None, 1, 17)             101390    \r\n=================================================================\r\nTotal params: 101,390\r\nTrainable params: 101,390\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n[colab](https://colab.research.google.com/drive/1T9kF-dn4WeH1ndN19ao5Hj0SV9Kuutrr?usp=sharing) \r\n[blazeface_tf.h5](https://drive.google.com/file/d/1yKLW5F7-6x9ylH65MgVxH5N5oQJgK8wq/view?usp=sharing)\r\n\r\n**Any other info / logs**\r\nI use this [codes](https://colab.research.google.com/drive/1T9kF-dn4WeH1ndN19ao5Hj0SV9Kuutrr?usp=sharing)  to  add some postprocess layer since I need to use NMS op after the origin model.After add postprocess layer and convert keras model to savedmodel, I convert the savedmodel to tf-lite model and there is a correct result. Then I need to depoly the tf-lite model in Android, I got an error when loading the tf-lite model:\r\n```\r\ntry {\r\n      detector =\r\n          TFLiteObjectDetectionAPIModel.create(\r\n              getAssets(),\r\n              TF_OD_API_MODEL_FILE,\r\n              TF_OD_API_LABELS_FILE,\r\n              TF_OD_API_INPUT_SIZE,\r\n              TF_OD_API_IS_QUANTIZED);\r\n      cropSize = TF_OD_API_INPUT_SIZE;\r\n    } catch (final IOException e) {\r\n      e.printStackTrace();\r\n      LOGGER.e(e, \"Exception initializing classifier!\");\r\n      Toast toast =\r\n          Toast.makeText(\r\n              getApplicationContext(), \"Classifier could not be initialized\", Toast.LENGTH_SHORT);\r\n      toast.show();\r\n      finish();\r\n    }\r\n```\r\nlog\r\n```\r\nI/tflite: Initialized TensorFlow Lite runtime.\r\nE/tensorflow: CameraActivity: Exception!\r\n    java.lang.RuntimeException: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 106 (FlexTensorScatterUpdate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:129)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:122)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1264)\r\n        at android.os.Handler.dispatchMessage(Handler.java:110)\r\n        at android.os.Looper.loop(Looper.java:219)\r\n        at android.app.ActivityThread.main(ActivityThread.java:8393)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:513)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1055)\r\n     Caused by: java.lang.IllegalStateException: Internal error: Unexpected failure when preparing tensor allocations: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.\r\n    Node number 106 (FlexTensorScatterUpdate) failed to prepare.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.allocateTensors(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:87)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:266)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:251)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:127)\r\n        \t... 9 more\r\n```\r\n \r\nThank you for your reply!\r\n\r\n", "comments": ["The converted model has the Select TF op(s). Consider adding the dependency of the Select TF op library. ", "https://www.tensorflow.org/lite/guide/ops_select#android_aar", "@whalefa1I, Did you tried as suggested [here](https://github.com/tensorflow/tensorflow/issues/50970#issuecomment-887351872)?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50970\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50970\">No</a>\n"]}, {"number": 50969, "title": "add platfrom_version for rocm in pjrt/gpu_device", "body": "This PR fixes the returned value from platform_version() for ROCm.\r\n", "comments": []}, {"number": 50968, "title": "At Chrome run Tensorflow.js.", "body": "Use the square layer tf.layers.conv2d, This file prompts a run error.\r\n\r\nwebgl_util.js\r\n\r\nError: Failed to compile fragment shader.", "comments": ["@blcyzycc \r\n\r\nThe issue posted  looks like related to [Tensorflow/tfjs](https://github.com/tensorflow/tfjs/issues) repo. Could you please post this issue in that repo. Hence there is a big community to support for tfjs issues. Thanks", "This error does not always appear, it can run normally on Edge, and this error will not appear.", "@blcyzycc \r\nPlease check these issues:[ link](https://stackoverflow.com/questions/51296173/failed-to-compile-fragment-shader-when-using-tensorflow-on-angular),[link2](https://github.com/tensorflow/tfjs/issues/1644) in case the issue still exist please create an issue @ [this repo](https://github.com/tensorflow/tfjs/issues/) and move this to closed status.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50968\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50968\">No</a>\n"]}, {"number": 50967, "title": "protobuf error while saving the model ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Ubuntu 16.04.7 LTS\r\n- TensorFlow installed from: conda\r\n-  TensorFlow version: 1.15.0\r\n- Python version: 3.6.13 from Anaconda\r\n- GPU: Quadro P6000 (Memory 24.5GB)\r\n\r\nI am trying to adopt the pixelcnn++ code available at https://github.com/openai/pixel-cnn. Initiallly with tensorflow version 1.10.0 it was working fine. Later I updated the code with an addition of incorporating ` tf.compat.v1.extract_image_patches` and run it using tensorflow version 1.15.0 (as ` tf.compat.v1.extract_image_patches` is nor compatible with tensorflow 1.10.0). \r\n\r\nWhile saving the model after every 1000 iterations with: \r\n`saver.save(sess, args.save_dir + '/best_params_' + args.data_set + '.ckpt') `      \r\nerror occurs:\r\n```\r\n[libprotobuf ERROR google/protobuf/wire_format_lite.cc:577] String field 'tensorflow.TensorShapeProto.Dim.name' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes. \r\nTraceback (most recent call last):\r\n  File \"train_pxpp_softmax_random_patch_extraction.py\", line 325, in <module>\r\n    saver.save(sess, args.save_dir + '/best_params_' + args.data_set + '.ckpt')\r\n  File \"/home/nisar/anaconda3/envs/pixelcnn/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 1203, in save\r\n    save_debug_info=save_debug_info)\r\n  File \"/home/nisar/anaconda3/envs/pixelcnn/lib/python3.6/site-packages/tensorflow_core/python/training/saver.py\", line 1246, in export_meta_graph\r\n    graph_def=ops.get_default_graph().as_graph_def(add_shapes=True),\r\n  File \"/home/nisar/anaconda3/envs/pixelcnn/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3238, in as_graph_def\r\n    result, _ = self._as_graph_def(from_version, add_shapes)\r\n  File \"/home/nisar/anaconda3/envs/pixelcnn/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\", line 3166, in _as_graph_def\r\n    graph.ParseFromString(compat.as_bytes(data))\r\ngoogle.protobuf.message.DecodeError: Error parsing message\r\n```\r\nThough it saves the model succesffuly at iteration 0. I also tried to save the model after every 500 iterations and it works fine for 0 and 500 but same error occurs at 1000 iteration. Following is the complete updated pixelcnn++ code with additions:\r\n", "comments": ["@zeeshannisar ,\r\n\r\nWe see that you are using tf version 1.15, 1.x is not actively supported, please update to 2.x and let us know if you are using same issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50967\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50967\">No</a>\n"]}, {"number": 50966, "title": "The max version of bazel supported by tensorflow should be increased to 4.1.0 or higher", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10 21H1\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: 2.4\r\n\r\n**Describe the problem**\r\nI'm a member of Microsoft VCPKG team, we recently upgraded the version of bazel used in vcpkg to 4.1.0, then tensorflow build failed with following error:\r\n```\r\nYou have bazel 4.1.0 installed.\r\nPlease downgrade your bazel installation to version 3.99.0 or lower to build TensorFlow! To downgrade: download the installer for the old version (from https://github.com/bazelbuild/bazel/releases) then run the installer.\r\n```\r\nThis issue is due to the fact that tensorflow has set the maximum supported version of bazel to 3.99.0 on line 53 of the https://github.com/tensorflow/tensorflow/blob/master/configure.py file.\r\n\r\nFor fixing this issue, the max version of bazel supported by tensorflow should be increased to 4.1.0 or higher.\r\n\r\n**Any other info / logs**\r\nI have tested that tensorflow could installed successfully when using bazel 4.1.0.\r\n", "comments": ["@Cheney-W \r\nA [pr](https://github.com/tensorflow/tensorflow/pull/50882) has been created to reflect the changes once its merged this issue will move to closed status.", "@Saduf2019 \r\nThe related PR has been merged, but the max version of bazel is still 3.99.0. \r\nI saw Mihai Maruseac changed the value to 4.1.0 but then changed it back.\r\nFor the moment, this problem still exists.", "Commented on #51045: we have had some compile fail when we last tried to update. Afaik, it's related to the protobuf dep being too tied in in both TF and Bazel", "Thanks for your help! "]}, {"number": 50964, "title": "SparseCategoricalCrossentropy and Mixed Precision Training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):  Binary\r\n- TensorFlow version (use command below): v2.5.0-0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2\r\n- GPU model and memory: A6000\r\n\r\n**Describe the current behavior**\r\n\r\n`sparse_categorical_crossentropy` in `losses.py` performs an unnecessary [cast](https://github.com/tensorflow/tensorflow/blob/8c541bab667488249a34404170985dff8a9bab56/tensorflow/python/keras/losses.py#L1751) of `y_true` to `y_pred.dtype` since it's then [cast](https://github.com/keras-team/keras/blob/d3688b72924a4235598f0f80038de8c897f44799/keras/backend.py#L4952) to `int64` in `sparse_categorical_crossentropy` in `keras.backend.py`.\r\n\r\nThis seems to be the same code as in `categorical_crossentropy`, but causes issues with sparse, especially with mixed precision training and float16 as the loss in precision causes incorrect encodings or labels outside the domain resulting in incorrect or `nan` loss. With float16, issues start with a couple thousand labels and a couple hundred labels with bfloat16.\r\n\r\n**Describe the expected behavior**\r\n\r\nCast to `y_pred.dtype` should be skipped.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): \r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nhttps://colab.research.google.com/drive/1oRbNOnCo1i2HcXD2V4_-D1Bz2EVxiT65\r\n\r\n", "comments": ["@zuyezheng ,\r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "Created https://github.com/keras-team/keras/issues/15012, thanks.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50964\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50964\">No</a>\n"]}, {"number": 50963, "title": "bug", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50963\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50963\">No</a>\n"]}, {"number": 50962, "title": "Crash fix cudaMallocAsync usage of TF_CUDA_MALLOC_ASYNC_SUPPORTED_PREALLOC.", "body": "Some of the refactoring caused crash when using TF_CUDA_MALLOC_ASYNC_SUPPORTED_PREALLOC.\r\nThis PR fix them and add a test to be sure it continue to work.\r\n\r\nThis PR is on top of https://github.com/tensorflow/tensorflow/pull/50961 as otherwise there would be code conflict.\r\n\r\nNow, GpuCudaMallocAsyncAllocator doesn't create its own compute stream. But it use one stream set after construction when GpuCudaMallocAsyncAllocator::SetStream is called. So it is now impossible to preallocate all memory during the object construction.\r\nSo this PR postpone the memory preallocation to when the stream is available, in SetStream.\r\n\r\nThis PR also check that the stream isn't set when SetStream is called.\r\n\r\n@sanjoy ", "comments": ["Please make the PR description a bit more descriptive (what was the bug?).\r\n\r\nCC @hanbinyoon ", "I updated the description.", "@nouiz Can you please check @sanjoy's comments and keep us posted ? Thanks!", "I rebased this PR since the dependent PR is merged.\r\nI also did the review comment. So it should be ready to merge now.", "Any update?", "The CI seems to have failed:\r\n```\r\nfeedback/copybara \u2014 Google internal checks FAILED for runs with create time 2021-09-09T07:29:11.802148007Z.\r\n```\r\n\r\nIs this related to this PR?"]}, {"number": 50961, "title": "Add back \"PR #49173: [Crash fix] Fix cudaMallocAsync crashes.\"", "body": "This reverts commit a4553f8ad74f5ad486057da400fb8e101f12e09f that was reverting #49173.\r\n\r\nWhen I run it now, //tensorflow/core/common_runtime/gpu:gpu_device_test in asan give me the same output as before. It was already crashing before this PR with my command line:\r\n\r\n```bazel test --distinct_host_configuration=false --javabase=@bazel_tools//tools/jdk:remote_jdk11 --config=asan -c opt --config=cuda //tensorflow/core/common_runtime/gpu:gpu_device_test```\r\n\r\n@penpornk \r\n\r\nfixes #50669 ", "comments": ["I just pushed a fix that fix a test error in NDEBUG mode.", "@nouiz Thank you so much for taking care of this. And sorry again about the revert. :(\r\nThe PR looks good to me. I applied your additional changes and it seems the `cuda_asan` test is still failing internally and might need further fixing, so I'm reassigning this PR to @cheshire. (I don't usually work on GPU PRs unless it's related to sparse kernels.)", "Any update on this? It is 3 mount since I started to get this fix in the first time.", "@cheshire Note, the problem is the internal `cuda_asan` test fail. I do not have any information about it. So I can't fix it. I'm also not able to reproduce it.", "> @cheshire Note, the problem is the internal `cuda_asan` test fail. I do not have any information about it. So I can't fix it. I'm also not able to reproduce it.\r\n\r\nThe error is actually another one:\r\nF0817 14:52:42.569874    7691 gpu_cudamallocasync_allocator.cc:133] Failed to get device attribute: CUDA error: invalid argument (CUDA_ERROR_INVALID_VALUE)\r\n\r\nThis happens when running the gpu_device_test without cuda_asan. I guess that in the meantime, the surrounding code has changed, and you will have to adapt your change to that.\r\nI couldn't reproduce the cuda_asan error, so I guess that is fixed now.", "I tried the commit in this PR and the gpu_device_test passed. I rebased it and pushed the rebased version and it still pass. The Github Linux CI passed too. So I'm not able to reproduce any error here.\r\n\r\nWhat idea what would be different in your system that would make it fail?", "> I tried the commit in this PR and the gpu_device_test passed. I rebased it and pushed the rebased version and it still pass. The Github Linux CI passed too. So I'm not able to reproduce any error here.\r\n> \r\n> What idea what would be different in your system that would make it fail?\r\n\r\nInternally we are running with cuda 11.3, in open source it is cuda 11.2 (as far as I can see). Otherwise I think there is no difference. The line I quoted as failing is triggered in the test that you are adding in this PR. It did not fail at the time when your PR was first merged, but I think at that time we were not using cuda 11.3 yet. So this could indeed be the reason.", "I tried with our CUDA 11.2 and 11.3 container and the test pass.\r\nHopefully it was an not-related error that won't happen with the new approval.", "> I tried with our CUDA 11.2 and 11.3 container and the test pass.\r\n> Hopefully it was an not-related error that won't happen with the new approval.\r\n\r\nIt did happen with the new approval, same error, and only when executing your newly added test. Unfortunately it looks definitely related. I believe this error would have been triggered before in the first merge as well had we been running with a cuda version >= 11.2, but first time essentially your new code was disabled and therefore couldn't trigger this error. I don't know of any other difference (except the newer cuda version). Can you make sense of the cuda error that is returned? This is line 133 of gpu_cudamallocasync_allocator.cc. In which cases would such an error be returned? Maybe we can narrow it down through that.\r\n\r\n@cheshire any idea what else could be different why it fails internally and not externally?", "@akuegel Can you check in the log if there is any mention that the GPU is enabled and working in that job?\r\n\r\nI updated this PR with extra information in that error message. Mostly, the error is that some argument to the function aren't valid. So I print them. The only argument that I think could fail is the device id. Can you run the test with that extra error message?", "> @akuegel Can you check in the log if there is any mention that the GPU is enabled and working in that job?\r\n> \r\n> I updated this PR with extra information in that error message. Mostly, the error is that some argument to the function aren't valid. So I print them. The only argument that I think could fail is the device id. Can you run the test with that extra error message?\r\n\r\nWith this additional debug statement, I can see: \"On device: 0\". Failed to get device attribute : CUDA error: invalid argument (CUDA_ERROR_INVALID_VALUE)\r\nIs it possible that a certain minimum driver version is needed to run this test successfully?\r\n\r\n@Artem-B Do you know which driver version are we currently running internally?", "> @Artem-B Do you know which driver version are we currently running internally?\r\n\r\nWe're using [cuda_compat](https://docs.nvidia.com/deploy/cuda-compatibility/) 470.57.02 on top of the drivers 450.80.02.\r\n\r\n", "cudaMallocAsync only care about the user driver. So 470.* should be good.\r\n\r\nBut I added explicit test that the driver is recent enough. So if there is a strange problem and that this test run with an older user driver, the error will be explicit.\r\nCan you trigger the test again?", "@akuegel Did the internal test fail? If so, what is the new error message?", "> @akuegel Did the internal test fail? If so, what is the new error message?\r\n\r\nDuring my work hours yesterday, the change hadn't been imported yet, I have read there were issues with the tool which does the import/export, possibly caused by Github problems. Today I see that the change was imported, and the test failed again, this time with an insightful error message:\r\n\r\ngpu_cudamallocasync_allocator.cc:136] Disable cuda_malloc_async or update your CUDA driver to a version compitible with CUDA 11.2 or higher. We detected a version compatible with: 11000\r\n\r\nSo it is indeed the problem that we don't have a recent enough driver. So either the cuda_compat package isn't being used, or doesn't help here.\r\n@Artem-B is there a way to check whether we indeed are running our tests with cuda_compat?", "> @Artem-B is there a way to check whether we indeed are running our tests with cuda_compat?\r\n\r\nMy answer above applies only to the build/test of the internal version of TF, not to the OSS one.\r\nI'm a bit out of date on the state of OSS test infrastructure and don't know what we use there ATM.\r\n\r\nIn general, OSS builds would need to have cuda_compat installed within the container they are running. My guess is that it's not. I can check tomorrow.", "> > @Artem-B is there a way to check whether we indeed are running our tests with cuda_compat?\r\n> \r\n> My answer above applies only to the build/test of the internal version of TF, not to the OSS one.\r\n> I'm a bit out of date on the state of OSS test infrastructure and don't know what we use there ATM.\r\n> \r\n> In general, OSS builds would need to have cuda_compat installed within the container they are running. My guess is that it's not. I can check tomorrow.\r\n\r\nSorry, I should have clarified, this test is failing internally with the output I posted above (seemingly indicating that the driver is not new enough). In OSS, the test runs successfully.", "@akuegel I updated the test to silently pass on driver or toolkit is under 11.2.\r\nThis is an experimental features and so we shouldn't bump the min driver/toolkit for this test.", "So @Artem-B has figured out why we had the older driver version when running the test. A tensorrt target was loading libcuda.so before we could load the cuda compat package. This is fixed now, so the test should actually also be running fine internally :)", "Great. I still see one of the Github Check to be:\r\n`@copybara-service\r\nfeedback/copybara \u2014 Google internal checks FAILED for runs with create time 2021-08-26T16:28:03.4`\r\n\r\nShould this be approved again to trigger a new run with that fix?", "(Checking back on this PR out of curiosity. Congrats on resolving the driver version issue!)\r\n\r\n> Great. I still see one of the Github Check to be:\r\n> `@copybara-service feedback/copybara \u2014 Google internal checks FAILED for runs with create time 2021-08-26T16:28:03.4`\r\n> \r\n> Should this be approved again to trigger a new run with that fix?\r\n\r\nThis is because the ROCm build failed. Maybe add an `#ifdef` guard?\r\n\r\n```\r\ntensorflow/core/common_runtime/gpu/gpu_device_test.cc: In member function 'virtual void tensorflow::GPUDeviceTest_CudaMallocAsync_Test::TestBody()':\r\ntensorflow/core/common_runtime/gpu/gpu_device_test.cc:120:64: error: 'CUDA_VERSION' was not declared in this scope\r\n   LOG(INFO) << \"CUDA toolkit too old, skipping this test: \" << CUDA_VERSION;\r\n                                                                ^~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device_test.cc:120:64: note: suggested alternative: '_SC_VERSION'\r\n   LOG(INFO) << \"CUDA toolkit too old, skipping this test: \" << CUDA_VERSION;\r\n                                                                ^~~~~~~~~~~~\r\n                                                                _SC_VERSION\r\ntensorflow/core/common_runtime/gpu/gpu_device_test.cc:124:3: error: 'cuDriverGetVersion' was not declared in this scope\r\n   cuDriverGetVersion(&driverVersion);\r\n   ^~~~~~~~~~~~~~~~~~\r\ntensorflow/core/common_runtime/gpu/gpu_device_test.cc:124:3: note: suggested alternative: 'driverVersion'\r\n   cuDriverGetVersion(&driverVersion);\r\n   ^~~~~~~~~~~~~~~~~~\r\n   driverVersion\r\n```\r\n\r\n", "I updated the test again."]}, {"number": 50960, "title": "\"ValueError: Cannot take the length of Shape with unknown rank\" or \"InvalidArgumentError: Length for attr 'output_shapes' of 0 must be at least minimum 1\" using tf.data.Dataset tensors", "body": "Hello I'm new to machine learning and for my project I need to load training set and validation set from disk into a tf.data.Dataset since they are too big to fit in memory.\r\n\r\nrunning the code in google colab using gpu or in my local machine with tf 2.5.0 i get the following problems:\r\n\r\nfirst I just load the datasets reading the tf record\r\n\r\n```\r\n#reading training and validation dataset\r\ndef read_tfrecord(example):\r\n  tfrecord_format = (\r\n      {\r\n          \"x\": tf.io.FixedLenFeature([], tf.string),\r\n          \"y\": tf.io.FixedLenFeature([], tf.string),\r\n      }\r\n  )\r\n  example = tf.io.parse_single_example(example, tfrecord_format)\r\n  x = tf.io.parse_tensor(example['x'], out_type=tf.float32)\r\n  y = tf.io.parse_tensor(example['y'], out_type=tf.double)\r\n    \r\n\r\n  return x,y\r\n\r\nbatch_size = 32\r\n\r\nfilename = \"training.tfrecord\"\r\ntraining_dataset = tf.data.TFRecordDataset(filename).map(read_tfrecord)\r\ntraining_dataset = training_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\r\n\r\nfilename = \"validation.tfrecord\"\r\nvalidation_dataset = tf.data.TFRecordDataset(filename).map(read_tfrecord)\r\nvalidation_dataset = validation_dataset.batch(batch_size)\r\n\r\n\r\nmodel.fit(training_dataset,\r\n          validation_data=validation_dataset,\r\n          epochs=35,\r\n          verbose=1)\r\n```\r\n\r\nand then I get the error ` ValueError: Cannot take the length of Shape with unknown rank\r\n`\r\nSo as suggested on this thread https://github.com/tensorflow/tensorflow/issues/24520 I set set_shape on read_tfrecord method \r\n\r\n```\r\ndef read_tfrecord(example):\r\n  tfrecord_format = (\r\n      {\r\n          \"x\": tf.io.FixedLenFeature([], tf.string),\r\n          \"y\": tf.io.FixedLenFeature([], tf.string),\r\n      }\r\n  )\r\n  example = tf.io.parse_single_example(example, tfrecord_format)\r\n  x = tf.io.parse_tensor(example['x'], out_type=tf.float32)\r\n  y = tf.io.parse_tensor(example['y'], out_type=tf.double)\r\n    \r\n  x = x.set_shape((None, None, None))\r\n  y = y.set_shape((None, None)\r\n\r\n  return x,y\r\n```\r\n\r\nSo I get the error\r\n`InvalidArgumentError: Length for attr 'output_shapes' of 0 must be at least minimum 1`\r\n\r\nAny suggestion?", "comments": ["@james901 \r\n\r\nCould you please refer similar issues [link](https://stackoverflow.com/questions/66624228/length-for-attr-output-shapes-of-0-must-be-at-least-minimum-1) and [#24520](https://github.com/tensorflow/tensorflow/issues/24520#issuecomment-493787420),let us know if it helps.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50959, "title": "Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0\r\n- Python version: 3.6.13\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2 8.1\r\n- GPU model and memory: RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nTypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"Placeholder_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"Placeholder:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"PlaceholderWithDefault:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.\r\n\r\n**Describe the expected behavior**\r\nThe same results as dense tensor.\r\n\r\nWe encounter this issue when we try to use sparse tensors in keras. We checked ReLU and BatchNormalization layer and we met this error. From the document \"Working with sparse tensors\", \"The tf.keras API natively supports sparse tensors without any expensive casting or conversion ops\". But we found a similar issue 25980 which meets a similar error but it's about dropout layer. I checked dropout layer and confirmed it is fixed in tf2.5, but other layers may still have bugs.\r\n\r\nSo my question is, is it safe to assume all tf.keras apis work with sparse tensors?\r\n\r\n**Standalone code to reproduce the issue**\r\nimport tensorflow as tf\r\ninput = tf.keras.layers.Input(shape=(4), sparse=True)\r\noutput = tf.keras.layers.ReLU()(input)\r\n\\# output = tf.keras.layers.BatchNormalization()(input)\r\nmodel = tf.keras.Model(inputs=[input], outputs=[output])\r\nprint(model(np.array([[1, 0, 1, 0]])))\r\n", "comments": ["@jiannanWang ,\r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "Thank you for your reply! I have created a new issue in keras-team/keras: https://github.com/keras-team/keras/issues/15009. Closing this issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50959\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50959\">No</a>\n"]}, {"number": 50958, "title": "Code not visible in dark theme in the Tensorflow documentation", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (WSL2)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached. <br>\r\nThis is the currently output shown in tensorflow documentation :  https://www.tensorflow.org/api_docs/python/tf/keras/Model <br>\r\n![image](https://user-images.githubusercontent.com/47894634/127043452-73d5c066-007e-4833-8cfe-7abdf6fee682.png)\r\n", "comments": ["@Mohitkumar6122 \r\nThis has been fixed, please verify and move this to closed status as this is resolved now.", "> @Mohitkumar6122\r\n> This has been fixed, please verify and move this to closed status as this is resolved now.\r\n\r\n@Saduf2019, It's fixed now!\r\nThanks for the quick response."]}, {"number": 50956, "title": "Reading TFRecords from S3 with a window shift leads to a large performance degradation", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): tried both types\r\n- TensorFlow version (use command below): 2.4.2\r\n- Python version: 3.8.0\r\n- Bazel version (if compiling from source): 4.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n\r\n**Describe the current behavior**\r\nI have a list of filenames of TFRecords that are stored on S3. Each TFRecord contains a list of frames stored in a type: `tf.io.FixedLenFeature((), tf.string)`. If I want to select every 100th frame from a TFRecord, TFRecordDataset downloads the whole file and then select required frames. \r\n\r\nI'm running p3dn.24xlarge that have 100Gb network bandwidth (EC2 and S3 on the same region + VPC).\r\n\r\n`iftop` shows average bandwidth around 20Gb and 25 threads (probably, because of this hard limitation: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc#L69 )\r\n\r\nIncreasing `num_parallel_calls` don't have any affect.\r\n\r\n```python\r\ndl = tf.data.TFRecordDataset.from_tensor_slices(filenames)\r\ndl = dl.interleave(\r\n    lambda y: tf.data.TFRecordDataset(y).window(size=1, shift=100, stride=5, drop_remainder=True).flat_map(lambda x: x.batch(1)), \r\nnum_parallel_calls=10)\r\n\r\n```\r\n**Describe the expected behavior**\r\nI would've expect any of the two following cases:\r\n* TFRecordDataset downloads only selected frames\r\n* TFRecordDataset scales up to 100Gb of network bandwidth\r\n", "comments": ["@BogdanRuzh \r\nCode provided is not sufficient to replicate the issue faced, Can you replicate the issue in colab gist and share for us to analyse.\r\n\r\nPlease refer to [this link](https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file) and let us know.", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50955, "title": "Could not find the DLL(s) 'msvcp140.dll or msvcp140_1.dll'", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.0\r\n- Python version: 3.8.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Intel(R) Iris(R) Plus Graphics\r\n\r\n\r\n**Describe the problem**\r\n\r\n`import tensorflow `\r\nyields\r\n\r\n`ImportError: Could not find the DLL(s) 'msvcp140.dll or msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading \"Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019\" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads`\r\n\r\neven when I have both `msvcp140.dll` and `msvcp140_1.dll` installed and they are both placed in the directory specified by %PATH% environment variable\r\n\r\n```\r\nC:\\Users\\Rostyslav>where msvcp140.dll\r\nC:\\Program Files\\Java\\jdk-16\\bin\\msvcp140.dll\r\nC:\\Windows\\System32\\msvcp140.dll\r\nC:\\Users\\Rostyslav\\AppData\\Local\\Programs\\Python\\Python39\\msvcp140.dll <-- HERE\r\nC:\\Users\\Rostyslav\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\msvcp140.dll\r\n```\r\n\r\nPath environment variable looks like this:\r\n\r\n![image](https://user-images.githubusercontent.com/49762976/127005254-b1c797c5-ec19-43c5-bb81-564ae13230ab.png)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nin cmd prompt I run `python run_model.py --train --save`, which is a script from github repository that can be found \r\n[here](https://machine-reasoning-ufrgs.github.io/GNN-GCP/)\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nI also tried to get more information about the problem, so I modified tensorflow's `self_check.py`. In `preload_check` in the `except OSError:` handler I added a line `import traceback; traceback.print_exc()`\r\n\r\nThe whole traceback is\r\n```(base) PS C:\\Users\\Rostyslav\\git\\gnn-gcp> python run_model.py --train --save\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 50, in preload_check\r\n    ctypes.WinDLL(dll_name)\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\ctypes\\__init__.py\", line 381, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 87] The parameter is incorrect\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 50, in preload_check\r\n    ctypes.WinDLL(dll_name)\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\ctypes\\__init__.py\", line 381, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 87] The parameter is incorrect\r\nTraceback (most recent call last):\r\n  File \"run_model.py\", line 6, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    self_check.preload_check()\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\lib\\site-packages\\tensorflow\\python\\platform\\self_check.py\", line 55, in preload_check\r\n    raise ImportError(\r\nImportError: Could not find the DLL(s) 'msvcp140.dll or msvcp140_1.dll'. TensorFlow requires that these DLLs be installed in a directory that is named in your %PATH% environment variable. You may install these DLLs by downloading \"Microsoft C++ Redistributable for Visual Studio 2015, 2017 and 2019\" for your platform from this URL: https://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads```\r\n", "comments": ["@RostyslavUA \r\nPlease refer to [this link](https://stackoverflow.com/questions/60157335/cant-pip-install-tensorflow-msvcp140-1-dll-missing) and let us know. [[link1](https://stackoverflow.com/questions/50777013/tensorflow-importerror-could-not-find-msvcp140-dll/50777261),[link2](https://www.programmersought.com/article/89164220428/)]\r\n\r\nYou have to installing the DLL as in the error message for which verify:\r\n\r\nCheck the path variable it must be inside local/prpgrams/python39 if u have previously installed 32 bit python the path must be inside roaming\r\nSo make sure you hyave 64 bit python installed\r\nC++ redistributable may vary for your system so download it according to your configuration", "I have already VC_redist.x64.exe installed as error-message suggests. \r\nThe list of my installed programs are as follows:\r\n```\r\nC:\\Users\\Rostyslav>wmic\r\nwmic:root\\cli>product get name\r\nName\r\nPython 3.9.1 Core Interpreter (64-bit)\r\nPython 3.9.1 Tcl/Tk Support (64-bit)\r\nPython 3.9.1 Standard Library (64-bit)\r\nPython 3.9.1 pip Bootstrap (64-bit)\r\nPython 3.9.1 Utility Scripts (64-bit)\r\nPython 3.9.1 Test Suite (64-bit)\r\nPython 3.9.1 Executables (64-bit)\r\nPython 3.9.1 Development Libraries (64-bit)\r\nPython 3.9.1 Documentation (64-bit)\r\nOffice 16 Click-to-Run Extensibility Component\r\nOffice 16 Click-to-Run Localization Component\r\nOffice 16 Click-to-Run Localization Component\r\nOffice 16 Click-to-Run Licensing Component\r\nTexmaker 5.0.4 (64-bit)\r\nMicrosoft Visual C++ 2008 Redistributable - x64 9.0.30729.4148\r\nMicrosoft Visual C++ 2008 Redistributable - x64 9.0.21022\r\nMicrosoft Visual C++ 2019 X64 Additional Runtime - 14.29.30040\r\nMicrosoft Visual C++ 2013 x86 Minimum Runtime - 12.0.21005\r\nMicrosoft Visual C++ 2013 x86 Additional Runtime - 12.0.21005\r\nMicrosoft Visual C++ 2019 X64 Minimum Runtime - 14.29.30040\r\nJava 8 Update 271 (64-bit)\r\nMicrosoft Update Health Tools\r\nESET Security\r\nMicrosoft Visual C++ 2008 Redistributable - x86 9.0.30729.6161\r\nVerification / 1.90\r\nPython Launcher\r\nPDF-XChange Editor\r\nWAGO BACnetObjectMerger\r\n\r\nMicrosoft Visual C++ 2012 x86 Additional Runtime - 11.0.61030\r\nHP PC Hardware Diagnostics UEFI\r\nMicrosoft Visual C++ 2010  x86 Redistributable - 10.0.30319\r\nMicrosoft Visual C++ 2012 x86 Minimum Runtime - 11.0.61030\r\nJava(TM) SE Development Kit 16 (64-bit)\r\nJava Auto Updater\r\n```\r\n\r\nI also have 64 bit architecture system:\r\n\r\n```\r\nC:\\Users\\Rostyslav>set processor\r\nPROCESSOR_ARCHITECTURE=AMD64\r\nPROCESSOR_IDENTIFIER=Intel64 Family 6 Model 126 Stepping 5, GenuineIntel\r\nPROCESSOR_LEVEL=6\r\nPROCESSOR_REVISION=7e05\r\n```\r\nAs in the accepted answer at the [link](https://stackoverflow.com/a/60336262/15452880) you suggested, I can find `msvcp140.dll` and `msvcp140_1.dll` in `C:\\Windows\\System32` directory.\r\n\r\nBut none of this unfortunately solves the problem. What else could I try?\r\nThank you!", "@RostyslavUA \r\nCan you verify the version of the DLL, should be Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019 only.", "After many days of search and struggle, I ended up with the conclusion that I have conflicting packages with tensorflow. I got the information from [link1](https://github.com/tensorflow/tensorflow/issues/22512), [link2](https://github.com/protocolbuffers/protobuf/issues/5046#issuecomment-413726611), [link3](https://answers.microsoft.com/en-us/windows/forum/all/msvcp1401dll-missing-even-after-installing/9ffe1025-000c-4dda-b686-08b3ae00d074), [link4](https://stackoverflow.com/a/59957040/15452880). This is because I get similar error and/or similar traceback.\r\nI have downgraded my tensorflow to `tensorflow 2.0`, because it often solved the same problem of peoples, but it did not work in my case. Now I get this error\r\n\r\n```\r\n(hiwi-kt) PS C:\\Users\\Rostyslav\\git\\gnn-gcp> python run_model.py --train --save\r\nTraceback (most recent call last):\r\n  File \"run_model.py\", line 6, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 947, in _find_and_load_unlocked\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 26, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 26, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\python\\keras\\activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\__init__.py\", line 38, in <module>\r\n    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\multi_gpu_utils.py\", line 22, in <module>\r\n    from tensorflow.python.keras.engine.training import Model\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 47, in <module>\r\n    from tensorflow.python.keras.engine import training_arrays\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 41, in <module>\r\n    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\scipy\\__init__.py\", line 106, in <module>\r\n    from . import _distributor_init\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\site-packages\\scipy\\_distributor_init.py\", line 26, in <module>\r\n    WinDLL(os.path.abspath(filename))\r\n  File \"C:\\Users\\Rostyslav\\anaconda3\\envs\\hiwi-kt\\lib\\ctypes\\__init__.py\", line 348, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n```\r\n\r\nI have tried to follow [link4](https://stackoverflow.com/a/59957040/15452880) to obtain the same package versions to resolve the issue, and now my current list looks like this:\r\n\r\n```\r\n# Name                    Version                   Build  Channel\r\nabsl-py                   0.13.0                   pypi_0    pypi\r\nargon2-cffi               20.1.0                   pypi_0    pypi\r\nastor                     0.8.1                    pypi_0    pypi\r\nastunparse                1.6.3                    pypi_0    pypi\r\nasync-generator           1.10                     pypi_0    pypi\r\nattrs                     21.2.0                   pypi_0    pypi\r\nbackcall                  0.2.0                    pypi_0    pypi\r\nbleach                    3.3.1                    pypi_0    pypi\r\nca-certificates           2021.7.5             haa95532_1\r\ncachetools                4.2.2                    pypi_0    pypi\r\ncertifi                   2021.5.30        py36haa95532_0\r\ncffi                      1.14.6                   pypi_0    pypi\r\ncharset-normalizer        2.0.3                    pypi_0    pypi\r\ncolorama                  0.4.4                    pypi_0    pypi\r\ncpuonly                   1.0                           0    pytorch\r\ndataclasses               0.8                      pypi_0    pypi\r\ndebugpy                   1.4.1                    pypi_0    pypi\r\ndecorator                 5.0.9                    pypi_0    pypi\r\ndefusedxml                0.7.1                    pypi_0    pypi\r\nentrypoints               0.3                      pypi_0    pypi\r\nflatbuffers               1.12                     pypi_0    pypi\r\ngast                      0.2.2                    pypi_0    pypi\r\ngoogle-auth               1.33.1                   pypi_0    pypi\r\ngoogle-auth-oauthlib      0.4.4                    pypi_0    pypi\r\ngoogle-pasta              0.2.0                    pypi_0    pypi\r\ngrpcio                    1.34.1                   pypi_0    pypi\r\nh5py                      2.10.0                   pypi_0    pypi\r\nidna                      3.2                      pypi_0    pypi\r\nimportlib-metadata        3.7.3                    pypi_0    pypi\r\nipykernel                 6.0.3                    pypi_0    pypi\r\nipython                   7.25.0                   pypi_0    pypi\r\nipython-genutils          0.2.0                    pypi_0    pypi\r\nipywidgets                7.6.3                    pypi_0    pypi\r\njedi                      0.18.0                   pypi_0    pypi\r\njinja2                    3.0.1                    pypi_0    pypi\r\njsonschema                3.2.0                    pypi_0    pypi\r\njupyter                   1.0.0                    pypi_0    pypi\r\njupyter-client            6.1.12                   pypi_0    pypi\r\njupyter-console           6.4.0                    pypi_0    pypi\r\njupyter-core              4.7.1                    pypi_0    pypi\r\njupyterlab-pygments       0.1.2                    pypi_0    pypi\r\njupyterlab-widgets        1.0.0                    pypi_0    pypi\r\nkeras-applications        1.0.8                    pypi_0    pypi\r\nkeras-nightly             2.5.0.dev2021032900          pypi_0    pypi\r\nkeras-preprocessing       1.1.2                    pypi_0    pypi\r\nmarkdown                  3.3.4                    pypi_0    pypi\r\nmarkupsafe                2.0.1                    pypi_0    pypi\r\nmatplotlib-inline         0.1.2                    pypi_0    pypi\r\nmistune                   0.8.4                    pypi_0    pypi\r\nnbclient                  0.5.3                    pypi_0    pypi\r\nnbconvert                 6.1.0                    pypi_0    pypi\r\nnbformat                  5.1.3                    pypi_0    pypi\r\nnest-asyncio              1.5.1                    pypi_0    pypi\r\nnotebook                  6.4.0                    pypi_0    pypi\r\nnumpy                     1.19.5                   pypi_0    pypi\r\noauthlib                  3.1.1                    pypi_0    pypi\r\nopenssl                   1.1.1k               h2bbff1b_0\r\nopt-einsum                3.3.0                    pypi_0    pypi\r\npackaging                 21.0                     pypi_0    pypi\r\npandocfilters             1.4.3                    pypi_0    pypi\r\nparso                     0.8.2                    pypi_0    pypi\r\npickleshare               0.7.5                    pypi_0    pypi\r\npillow                    8.3.1                    pypi_0    pypi\r\npip                       20.0.2                   py36_3\r\nprometheus-client         0.11.0                   pypi_0    pypi\r\nprompt-toolkit            3.0.19                   pypi_0    pypi\r\nprotobuf                  3.8.0                    pypi_0    pypi\r\npyasn1                    0.4.8                    pypi_0    pypi\r\npyasn1-modules            0.2.8                    pypi_0    pypi\r\npycparser                 2.20                     pypi_0    pypi\r\npygments                  2.9.0                    pypi_0    pypi\r\npyparsing                 2.4.7                    pypi_0    pypi\r\npyrsistent                0.18.0                   pypi_0    pypi\r\npython                    3.6.8                h9f7ef89_7\r\npython-dateutil           2.8.2                    pypi_0    pypi\r\npywin32                   301                      pypi_0    pypi\r\npywinpty                  1.1.3                    pypi_0    pypi\r\npyzmq                     22.1.0                   pypi_0    pypi\r\nqtconsole                 5.1.1                    pypi_0    pypi\r\nqtpy                      1.9.0                    pypi_0    pypi\r\nrequests                  2.26.0                   pypi_0    pypi\r\nrequests-oauthlib         1.3.0                    pypi_0    pypi\r\nrsa                       4.7.2                    pypi_0    pypi\r\nscipy                     1.4.1                    pypi_0    pypi\r\nsend2trash                1.7.1                    pypi_0    pypi\r\nsetuptools                52.0.0           py36haa95532_0\r\nsix                       1.16.0                   pypi_0    pypi\r\nsqlite                    3.36.0               h2bbff1b_0\r\ntensorboard               2.0.2                    pypi_0    pypi\r\ntensorboard-data-server   0.6.1                    pypi_0    pypi\r\ntensorboard-plugin-wit    1.8.0                    pypi_0    pypi\r\ntensorflow                2.0.0                    pypi_0    pypi\r\ntensorflow-estimator      2.0.0                    pypi_0    pypi\r\ntermcolor                 1.1.0                    pypi_0    pypi\r\nterminado                 0.10.1                   pypi_0    pypi\r\ntestpath                  0.5.0                    pypi_0    pypi\r\ntorch                     1.9.0                    pypi_0    pypi\r\ntorchaudio                0.9.0                    pypi_0    pypi\r\ntorchvision               0.10.0                   pypi_0    pypi\r\ntornado                   6.1                      pypi_0    pypi\r\ntraitlets                 5.0.5                    pypi_0    pypi\r\ntyping-extensions         3.7.4.3                  pypi_0    pypi\r\nurllib3                   1.26.6                   pypi_0    pypi\r\nvc                        14.2                 h21ff451_1\r\nvs2015_runtime            14.27.29016          h5e58377_2\r\nwcwidth                   0.2.5                    pypi_0    pypi\r\nwebencodings              0.5.1                    pypi_0    pypi\r\nwerkzeug                  2.0.1                    pypi_0    pypi\r\nwheel                     0.36.2             pyhd3eb1b0_0\r\nwidgetsnbextension        3.5.1                    pypi_0    pypi\r\nwincertstore              0.2              py36h7fe50ca_0\r\nwrapt                     1.12.1                   pypi_0    pypi\r\nzipp                      3.5.0                    pypi_0    pypi\r\n```\r\n\r\nThere is probably still some conflict between the packages, but I am not sure where.\r\n\r\nAlso note that I have erased and installed `anaconda3` from scratch. \r\n\r\nRegarding your question about the version of DLL - I am not sure how to verify it. But note that I have also tried to install the newest DLLs suitable for my architecture from [here](https://www.dll-files.com/msvcp140.dll.html). However, this does not resolve the issue.\r\n\r\nSorry if I confuse you introducing different types of errors. Thus feel free to point me to any of those or to ask for any missing information.\r\n\r\nThank you!", "@RostyslavUA \r\n\r\nThis issue is more suitable on Continuum Anaconda repo since its related to TF installation with Anaconda.\r\nPlease post it on [Continuum Anaconda](https://github.com/ContinuumIO/anaconda-issues/issues).Thanks!\r\n\r\nYou may also refer to this [issue once](https://github.com/tensorflow/tensorflow/issues/41831#issuecomment-665388718)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50955\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50955\">No</a>\n", "I had the same problem, this guide helped me:\r\nhttps://winbindex.m417z.com/?file=msvcp140_1.dll"]}, {"number": 50954, "title": "[MLIR][DISC] add ral logger and registry", "body": "add ral logger and registry.", "comments": ["Sorry I've been a bit stuck on the use of `gtest.h`: it is the first one in mlir-hlo and I don't know how to handle it yet (We need to have it work in https://github.com/tensorflow/mlir-hlo )\r\n\r\nAlso I'm traveling in Europe for 2 more weeks, so handling this only on my own time when I can!", "Sorry for the delay :)"]}, {"number": 50950, "title": "cl.exe failed during building tensorflow from source", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master branch\r\n- Python version: 3.8,6\r\n- Installed using virtualenv? pip? conda?: bazel\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): VS2019\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the problem**\r\nThe build works for most of the time but at the end it crashes into a c++ compilation error (cl.exe) and gives unknown option error for optimization flags\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\npython ./configure.py\r\nbazel --output_user_root=C:\\tfb build --config=opt --jobs=3 //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nI didnt checkout to any branch as i had problems with r2.5 and r2.4 and had to limit jobs to 3 due to ram constraint.\r\nI put y for this question `` Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: `` and gave `` --copt=-march=native `` as the optimization flag.\r\n\r\n**Any other info / logs**\r\n```\r\nERROR: C:/tensorflow/tensorflow/python/util/BUILD:632:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/tfb/xv6zejqw/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\SUNDAR~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\SUNDAR~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.29.30037/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/pybind11 /Ibazel-out/x64_windows-opt/bin/external/pybind11 /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/pybind11/_virtual_includes/pybind11 /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazel-out/x64_windows-opt/bin/external/pybind11/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions --copt=-march=native /std:c++14 -fno-strict-aliasing -fexceptions /Fobazel-out/x64_windows-opt/bin/tensorflow/python/util/_objs/fast_module_type.so/fast_module_type.obj /c tensorflow/python/util/fast_module_type.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\ncl : Command line warning D9002 : ignoring unknown option '--copt=-march=native'\r\ncl : Command line warning D9002 : ignoring unknown option '-fno-strict-aliasing'\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include\\complex(675): error C2039: 'copysign': is not a member of '`global namespace''\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include\\complex(675): error C3861: 'copysign': identifier not found\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 5030.345s, Critical Path: 387.99s\r\nINFO: 8960 processes: 2679 internal, 6281 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nI have tried the same build with the following optimization flags \r\n\r\n- ``-march=native``\r\n- ``-mavx -mavx2 -msse4.2 -mfma`` (suggested on stackoverflow)\r\n- ``--copt=-mavx --copt=-mavx2 --copt=-msse4.2 --copt=-mfma``\r\n\r\nHowever I get the same error (`` cl : Command line warning D9002 : ignoring unknown option `` for each flag)\r\nHow do i compile tensorflow with avx2 and sse4.2 without the unknown option error?", "comments": ["@akkshay0107 \r\n\r\nCould you please refer similar issues [#36576](https://github.com/tensorflow/tensorflow/issues/36576) and [#23795](https://github.com/tensorflow/tensorflow/issues/23795) and let us know if it helps.Thanks", "i have tried doing the build with and without passing optimization flags\r\n\r\nWithout optimization flags, the error remains the same \r\n```\r\nC:\\tensorflow>bazel --output_user_root=C:\\tfb build --jobs=3 --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/lib/site-packages --python_path=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file c:\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file c:\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: C:/tfb/xv6zejqw/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (431 packages loaded, 29062 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/tensorflow/tensorflow/python/util/BUILD:610:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/tfb/xv6zejqw/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\SUNDAR~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\SUNDAR~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.29.30037/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/pybind11 /Ibazel-out/x64_windows-opt/bin/external/pybind11 /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/pybind11/_virtual_includes/pybind11 /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazel-out/x64_windows-opt/bin/external/pybind11/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 -fno-strict-aliasing -fexceptions /Fobazel-out/x64_windows-opt/bin/tensorflow/python/util/_objs/fast_module_type.so/fast_module_type.obj /c tensorflow/python/util/fast_module_type.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\ncl : Command line warning D9002 : ignoring unknown option '-fno-strict-aliasing'\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include\\complex(675): error C2039: 'copysign': is not a member of '`global namespace''\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include\\complex(675): error C3861: 'copysign': identifier not found\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 4343.727s, Critical Path: 600.97s\r\nINFO: 7872 processes: 2300 internal, 5572 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nwith optimization flags (/arch:AVX2 and /arch:AVX512) passed to configure.py i get a protocompile error\r\n```\r\nC:\\tensorflow>bazel --output_user_root=C:\\tfb build --jobs=3 --config=opt //tensorflow/tools/pip_package:build_pip_package\r\nExtracting Bazel installation...\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=120\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/lib/site-packages --python_path=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Reading rc options for 'build' from c:\\tensorflow\\.bazelrc:\r\n  'build' options: --verbose_failures\r\nINFO: Found applicable config definition build:short_logs in file c:\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file c:\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX2 --host_copt=/arch:AVX2 --copt=/arch:AVX512 --host_copt=/arch:AVX512\r\nINFO: Found applicable config definition build:windows in file c:\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false --config=no_tfrt\r\nINFO: Found applicable config definition build:monolithic in file c:\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:no_tfrt in file c:\\tensorflow\\.bazelrc: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/toolchains/archive/d781e89e2ee797ea7afd0c8391e761616fc5d50d.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/f938dc0ed756045d9a7db171145c628c4aa00c90.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/de7a4e53a22b27d47503b9fc513898251319d5c7.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/files.pythonhosted.org/packages/12/59/eaa15ab9710a20e22225efd042cd2d6a0b559a0656d5baba9641a2a4a921/gast-0.4.0.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  C:/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  C:/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  C:/tfb/xv6zejqw/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  C:/tfb/xv6zejqw/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/28407b24f04694a1353ffca91248e89bc250eb5b.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (434 packages loaded, 30786 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/tensorflow/tensorflow/compiler/mlir/lite/quantization/BUILD:58:17: ProtoCompile tensorflow/compiler/mlir/lite/quantization/quantization_info.pb.h failed (Exit -1073741795): protoc.exe failed: error executing command\r\n  cd C:/tfb/xv6zejqw/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;c:\\Program Files (x86)\\Intel\\Intel(R) Management Engine Components\\DAL;c:\\Program Files\\Intel\\Intel(R) Management Engine Components\\DAL;C:\\Program Files\\Git\\cmd;C:\\Program Files\\nodejs\\;C:\\Users\\Sundara Rajan\\AppData\\Local\\Programs\\Python\\Python38\\Scripts\\;C:\\Users\\Sundara Rajan\\AppData\\Local\\Programs\\Python\\Python38\\;C:\\Users\\Sundara Rajan\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Sundara Rajan\\Desktop\\Microsoft VS Code\\bin;C:\\Users\\Sundara Rajan\\AppData\\Local\\Polycom\\RealPresence Desktop\\;C:\\Users\\Sundara Rajan\\AppData\\Roaming\\npm;C:\\Users\\Sundara Rajan\\AppData\\Local\\Programs\\Bazel;C:\\msys64\\usr\\bin;c:\\protoc\\bin;\r\n    SET PYTHON_BIN_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n  bazel-out/x64_windows-opt/bin/external/com_google_protobuf/protoc.exe --cpp_out=bazel-out/x64_windows-opt/bin -I. -Iexternal/com_google_protobuf/src -Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src tensorflow/compiler/mlir/lite/quantization/quantization_info.proto\r\nExecution platform: @local_execution_config_platform//:platform\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 2232.149s, Critical Path: 452.03s\r\nINFO: 6646 processes: 2274 internal, 4372 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@akkshay0107 \r\nCould you please try this in 2.5 and let us know, as per [this comment](https://github.com/tensorflow/tensorflow/issues/45823#issuecomment-854892564) it should not occur. [as master branch is subjected to changes and is work in progress please verify on a stable version]", "@Saduf2019 \r\ni have tried it with r2.5 and r2.4 but i get a proto compile error , the only way i could fix it was moving to r2.6 ", "@akkshay0107 \r\nThis has been fixed in 2.6, can you use 2.6, we will look into other versions meanwhile.", "@Saduf2019 sure i will try with r2.6 and get back", "Error with r2.6 : \r\n```\r\nERROR: C:/tensorflow/tensorflow/python/util/BUILD:610:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/tfb/xv6zejqw/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\SUNDAR~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\SUNDAR~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.29.30037/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/pybind11 /Ibazel-out/x64_windows-opt/bin/external/pybind11 /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/pybind11/_virtual_includes/pybind11 /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazel-out/x64_windows-opt/bin/external/pybind11/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 -fno-strict-aliasing -fexceptions /Fobazel-out/x64_windows-opt/bin/tensorflow/python/util/_objs/fast_module_type.so/fast_module_type.obj /c tensorflow/python/util/fast_module_type.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\ncl : Command line warning D9002 : ignoring unknown option '-fno-strict-aliasing'\r\ncl : Command line warning D9002 : ignoring unknown option '-fexceptions'\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include\\complex(675): error C2039: 'copysign': is not a member of '`global namespace''\r\nC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include\\complex(675): error C3861: 'copysign': identifier not found\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```\r\nError with r2.5 : \r\n```\r\nERROR: C:/tensorflow/tensorflow/core/protobuf/BUILD:65:17: ProtoCompile tensorflow/core/protobuf/conv_autotuning.pb.h failed (Exit -1073741515): protoc.exe failed: error executing command\r\n  cd C:/bazel_build/xv6zejqw/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PYTHON_BIN_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n  bazel-out/x64_windows-opt/bin/external/com_google_protobuf/protoc.exe --cpp_out=bazel-out/x64_windows-opt/bin -I. -Iexternal/com_google_protobuf/src -Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src tensorflow/core/protobuf/conv_autotuning.proto\r\nExecution platform: @local_execution_config_platform//:platform\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\n```", "@akkshay0107 \r\nCan you please refer to [this issue](https://github.com/tensorflow/tensorflow/issues/48557#issuecomment-825459308), remove all the versions you have and try with one.", "@Saduf2019 i have switched to python 3.9.2 and it seems like the old error is gone \r\n```\r\nERROR: C:/tensorflow/tensorflow/core/kernels/BUILD:3401:18: C++ compilation of rule '//tensorflow/core/kernels:cwise_op' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/tfb/xv6zejqw/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\include;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30037\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\MSBuild\\Current\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\devinit;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\\\MSBuild\\Current\\Bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\Tools\\;;C:\\WINDOWS\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python39/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/Sundara Rajan/AppData/Local/Programs/Python/Python39/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=C:\\Users\\SUNDAR~1\\AppData\\Local\\Temp\r\n    SET TF2_BEHAVIOR=1\r\n    SET TMP=C:\\Users\\SUNDAR~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio/2019/BuildTools/VC/Tools/MSVC/14.29.30037/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/curl/include /Ibazel-out/x64_windows-opt/bin/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/bin/external/jsoncpp_git/include /DCURL_STATICLIB /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 -DMLIR_GENERATED_CPU_KERNELS_ENABLED -DMLIR_GENERATED_GPU_KERNELS_ENABLED -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL -DTENSORFLOW_MONOLITHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADPOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COMPILE_LIBRARY /DEIGEN_STRONG_INLINE=inline -DTENSORFLOW_USE_XLA=1 -DINTEL_MKL=1 /Fobazel-out/x64_windows-opt/bin/tensorflow/core/kernels/_objs/cwise_op/cwise_op_not_equal_to_1.obj /c tensorflow/core/kernels/cwise_op_not_equal_to_1.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\ncl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release\r\ncl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'\r\nC:\\tfb\\xv6zejqw\\execroot\\org_tensorflow\\external\\eigen_archive\\unsupported\\Eigen\\CXX11\\src/Tensor/TensorBlock.h(1144): fatal error C1060: compiler is out of heap space\r\n```\r\ni now get a fatal error of not having enough heap space , is there a way to solve this ?", "@akkshay0107 \r\nPlease refer to [this issue for the new error](https://github.com/tensorflow/tensorflow/issues/10799#issuecomment-309235859), in case the issue persist please create a new issue as the root issue is resolved we will have to create new issue for new error.", "@Saduf2019 thanks a lot for helping ill close this issue", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50950\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50950\">No</a>\n"]}]