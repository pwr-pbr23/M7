[{"number": 28720, "title": "Clean up TFOptimizer.get_updates()", "body": "", "comments": ["Can one of the admins verify this patch?", "This is not waiting for review, right?", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28719, "title": "[XLA] rename xla compile mark from _XlaCompile to XlaCompile", "body": "code in mark_for_compilation_pass.cc shows that we can force an op to be compiled by xla by adding a mark \"kXlaCompileAttr\" to the op in its op.cc. However,  tf framework does not support a node attr name with the beginning of \"_\" such as \"_XlaCompile\", which makes the design in mark_for_compilation.cc  not work. \r\ni have tested this solution in nccl_ops.cc and it worked. ", "comments": ["i'm trying to add a nccl_ops(NcclAllReduce, NcclBroadCast ... ) support for xla. One method is taking advantage of the code below (mark_for_compilation_pass.cc +1115):\r\n`    if (GetNodeAttr(n->attrs(), kXlaCompileAttr, &compile_attr).ok()) {\r\n      marked_for_compilation = compile_attr;\r\n    } else if (options.flib_def->GetAttr(*n, kXlaCompileAttr, &compile_attr)\r\n                   .ok()) {\r\n      marked_for_compilation = compile_attr;\r\n    }`\r\n\r\nit seems that an op can be added to xla engine by adding an attribute named kXlaCompileAttr. but when i tried to do this , this problem occurred and then i fixed according to my understanding.   ", "To add support for `NcclAllReduce`, `NcclBroadCast` etc. you need to add `XlaOpKernel`s for these in `tf2xla/kernels`.  Once you you have registered kernels for these ops, mark for compilation should pick these up automatically.", "but actually, MarkForCompilePass does not add a registered xla kernel unconditionally. there are several code blocks deciding  whether to pick up a specific registered xla kernel and put it into the final graph. in my case, MarkForCompilationPass  can find  the new added nccl xla kernel, but will not mark this op as this op does not satisfy the following condition:\r\n```\r\n    if (effective_cluster_sizes[cluster] >= min_cluster_size ||\r\n        (effective_cluster_sizes[cluster] > 0 && marked_for_compilation)) {\r\n      string& name = cluster_names[cluster];\r\n```\r\nso i want to make the variable \"marked_for_compilation\" be true to satisfy this condition.  one way do this is add a kXlaCompleAttr in this op:\r\n```\r\n    if (GetNodeAttr(n->attrs(), kXlaCompileAttr, &compile_attr).ok()) {\r\n      marked_for_compilation = compile_attr;\r\n    } else if (options.flib_def->GetAttr(*n, kXlaCompileAttr, &compile_attr)\r\n                   .ok()) {\r\n      marked_for_compilation = compile_attr;\r\n    }\r\n```\r\nbut when i tried to add this attr,  i found it impossible to add it manually without changed the existed xla framework. so i push this PR. \r\n\r\nif we cannot add a  kXlaCompileAttr like this,  what is the correct way to satisfy this condition ?  Is there a previous optimizer adding this attr automatically?  \r\n ", "This logic:\r\n\r\n```\r\n    if (effective_cluster_sizes[cluster] >= min_cluster_size ||\r\n        (effective_cluster_sizes[cluster] > 0 && marked_for_compilation)) {\r\n      string& name = cluster_names[cluster];\r\n```\r\n\r\nis a profitability heuristic.  There are two ways to override this for testing purposes:\r\n\r\n - Preferred:  use `xla.compile` to force the cluster you're trying to compile to get compiled.\r\n - Less preferred: assign the nodes you want to be compiled to `XLA_GPU` or `XLA_CPU` devices, which will force them to get compiled.\r\n\r\nIf you're in a situation where you think the heuristic is wrong for profitability reasons (e.g. you have a case where a cluster containing `NcclAllReduce` should be compiled even if `effective_cluster_sizes[cluster] < min_cluster_size` since that results in better performance) then we should adjust the heuristic directly instead of hacking around it via the `_XlaCompile` attribute."]}, {"number": 28718, "title": "Transfer Learning Using Pretrained ConvNets tutorial should use sigmoid activation", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/alpha/tutorials/images/transfer_learning\r\n\r\n## Description of issue (what needs changing):\r\nclassification head should use sigmoid activation.\r\n### Clear description\r\nThe tutorial has this paragraph:\r\n\r\nYou don't need an activation function here because this prediction will be treated as a logit, or a raw prediciton value. Positive numbers predict class 1, negative numbers predict class 0.\r\n\r\nI think we need sigmoid activation. \r\nLater we use loss=\"binary_crossentropy\" in model.compile. binary_crossentropy by default has from_logits=False and expect a probability as is documented [here ](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses/BinaryCrossentropy) and [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy)\r\n", "comments": ["An activation adds uncertainty, causes increased model size and latency by a tiny bit -- so it's best if we avoid it. There is a workaround to set `from_logits=True` in the loss function as follows:\r\n\r\n**Option 1 ([Source](https://www.tensorflow.org/tutorials/text/text_generation#attach_an_optimizer_and_a_loss_function))**\r\n```\r\ndef loss(labels, logits):\r\n  return tf.keras.losses.binary_crossentropy(labels, logits, from_logits=True)\r\n\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n              loss=loss,\r\n              metrics=['accuracy'])\r\n\r\n```\r\n**Option 2 (Optimized version of Option 1 using Lambda functions)**\r\n```\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n              loss=lambda labels, logits: tf.keras.losses.binary_crossentropy(labels, logits, from_logits=True),\r\n              metrics=['accuracy'])\r\n```\r\n\r\n**Option 3: (Adheres to the [Keras documentation](https://keras.io/losses/#binary_crossentropy))**\r\n```\r\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\r\n              loss=lambda y_true, y_pred: tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=True),\r\n              metrics=['accuracy'])\r\n```", "**The incoherence is still present in the documentation**: still mentioning `activation=None` (logit) and `loss=\"binary_crossentropy\"`.\r\n\r\nThis personally made me lose a couple of days of work, as one assumes TF's documentation is correct.\r\n\r\nHappy with all the solutions suggested above. Another idea could be to use `binary_cross_entropy_with_logits` perhaps?\r\n\r\nIf somebody thinks the current tutorial is correct, it'd be great to have a thorough explanation. Otherwise, **can we have a hot fix?** ", "A fix is inflight,  Sorry for the confusion. \r\n\r\nThe old version does work as intended, because setting loss to a string value triggers some keras magic that does the right thing behind the scenes.\r\n\r\nBut it's best to avoid magic.", "Closing this issue since the associated PR has been merged which fixes the issue. Thanks!"]}, {"number": 28717, "title": "Upgrade Docker images to Ubuntu 18.04 (Py 3.6)", "body": "This upgrades all TF official docker images aside from custom-op\r\n(which does not use this partial-related system) to use Ubuntu 18.04,\r\nup from 16.04.\r\n\r\nResolves #28495, Resolves #22292.", "comments": []}, {"number": 28716, "title": "Merge pull request #1 from tensorflow/master", "body": "Update repo from source", "comments": ["I'm going to go ahead and close this PR, because  there are no changes to review . If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28715, "title": "Merge pull request #1 from tensorflow/master", "body": "Update repo from source", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F28715) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 28714, "title": "tensorflow gradient tape training speed performance issue (much slower than keras .fit training)", "body": "**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution: Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary pip3\r\n- TensorFlow version (use command below): 2.0.0-dev20190510\r\n- Python version: 3.5.2\r\n- CUDA/cuDNN version: 10.0/7\r\n- GPU model and memory: Titan V 12GB\r\n\r\n**Describe the current behavior**\r\ntensorflow.keras .fit training time is way faster than tensorflow gradient tape\r\n\r\n**Describe the expected behavior**\r\nkeras .fit is just a wrapper why would it be way faster??? They should be the same on the same dataset with the same loss func and optimizer.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\nfrom time import perf_counter\r\n\r\ndef get_uncompiled_model():\r\n  inputs = keras.Input(shape=(784,), name='digits')\r\n  x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\r\n  x = layers.Dense(64, activation='relu', name='dense_2')(x)\r\n  outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\r\n  model = keras.Model(inputs=inputs, outputs=outputs)\r\n  return model\r\n\r\ndef get_compiled_model():\r\n  model = get_uncompiled_model()\r\n  model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['sparse_categorical_accuracy'])\r\n  return model\r\n\r\n# Load a toy dataset for the sake of this example\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\n# Preprocess the data (these are Numpy arrays)\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\nx_test = x_test.reshape(10000, 784).astype('float32') / 255\r\n\r\n# Reserve 10,000 samples for validation\r\nx_val = x_train[-10000:]\r\ny_val = y_train[-10000:]\r\nx_train = x_train[:-10000]\r\ny_train = y_train[:-10000]\r\n\r\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\r\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\r\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\r\ntest_dataset = test_dataset.batch(64)\r\n\r\n# model = get_compiled_model()\r\n\r\n# # keras train\r\n# model.fit(train_dataset, epochs=3)\r\n\r\n# manual train\r\nmodel = get_uncompiled_model()\r\noptimizer = keras.optimizers.RMSprop(learning_rate=1e-3)\r\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\r\n\r\n@tf.function\r\ndef train():\r\n    # Iterate over epochs.\r\n    for epoch in range(3):\r\n      print('Start of epoch %d' % (epoch))\r\n\r\n      for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\r\n        with tf.GradientTape() as tape:\r\n          logits = model(x_batch_train)  # Logits for this minibatch\r\n          loss_value = loss_fn(y_batch_train, logits)\r\n        grads = tape.gradient(loss_value, model.trainable_variables)\r\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\ns = perf_counter()\r\ntrain()\r\nprint(perf_counter() - s)\r\n\r\nmodel = get_compiled_model()\r\ns = perf_counter()\r\nmodel.fit(train_dataset, epochs=3)\r\nprint(perf_counter() - s)\r\n```\r\nThe snippet is taken from here: https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#low-level_handling_of_metrics\r\n\r\n**Other info**\r\nGPU utilization of the gradient tape training is almost zero - with and without tf.function. Using tf.debugging.set_log_device_placement(True), I can see that the model is allocated on GPU.", "comments": ["I tried it with tf.function, and found the problem is actually in enumerate() term. It'd be great if you can look into that.", "How about that moving iteration logic to the outside of tf.function? like this:\r\n```\r\n@tf.function\r\ndef train_batch(x_batch_train, y_batch_train):\r\n    with tf.GradientTape() as tape:\r\n        logits = model(x_batch_train)  # Logits for this minibatch\r\n        loss_value = loss_fn(y_batch_train, logits)\r\n    grads = tape.gradient(loss_value, model.trainable_variables)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n\r\n# Iterate over epochs.\r\nfor epoch in range(3):\r\n    print('Start of epoch %d' % (epoch))\r\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\r\n        train_batch(x_batch_train, y_batch_train)\r\n        \r\ns = perf_counter()\r\n```", "> How about that moving iteration logic to the outside of tf.function? like this:\r\n> \r\n> ```\r\n> @tf.function\r\n> def train_batch(x_batch_train, y_batch_train):\r\n>     with tf.GradientTape() as tape:\r\n>         logits = model(x_batch_train)  # Logits for this minibatch\r\n>         loss_value = loss_fn(y_batch_train, logits)\r\n>     grads = tape.gradient(loss_value, model.trainable_variables)\r\n>     optimizer.apply_gradients(zip(grads, model.trainable_variables))\r\n> \r\n> # Iterate over epochs.\r\n> for epoch in range(3):\r\n>     print('Start of epoch %d' % (epoch))\r\n>     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\r\n>         train_batch(x_batch_train, y_batch_train)\r\n>         \r\n> s = perf_counter()\r\n> ```\r\n\r\nThis works. However, if you compare the performance with and without the @tf.function, I actually see some differences. This is small in mnist (about 40 ms/epoch), but it will make a difference in larger dataset. ", "Was not able to reproduce the issue.\r\nThis question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 28713, "title": "ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.", "body": "**System information**\r\nWindows 10 Pro\r\nTensorflow 1.13.1 (didn't work with 2.0.0a0 or 1.10.0)\r\npython 3.6.8\r\ninstalled using pip\r\n\r\nI installed using pip following the instructions on the website. The only solutions I could find told me that i need cuDNN, but this is CPU only, or they told me that I need an AVX-enabled processor, which I have. It gave me the following error when I tried importing it in the command prompt:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "comments": ["I tried to recreate this issue in anaconda: \r\nOS: Windows 10 home\r\nTensorflow 1.13.1\r\npython 3.6.8\r\nI used 'pip install tensorflow' in anaconda terminal to install tensorflow and was able to run \r\n`from tensorflow.python import pywrap_tensorflow`\r\n`from tensorflow.python.pywrap_tensorflow_internal import *`", "it doesn't require cuDNN, I removed cuDNN and checked.", "I have the same problem as you.\r\nWindows 8.1\r\npython3.7.3\r\ntensorflow1.13.1\r\nthen I knew that my cpu didn't support  AVX. \r\nBecause \"Starting from 1.6 release, our prebuilt binaries will use AVX instructions.\r\nThis may break TF on older CPUs.\"  noted in [https://github.com/tensorflow/tensorflow/releases/tag/v1.5.0](url).\r\n\r\nMy solution:\r\n1. Install the python3.6.8\r\n2. virtualenv -p c:\\python36\\python.exe envname  #(if you have different python version and want to choose a python version what you want in a new virtualenv, just do it)\r\n3. activate your new virtualenv\r\n4. pip install tensorflow==1.5\r\n\r\nHope it can help you.\r\n\r\nps: for other one who is using python3.7 with older cpu.  Tensorflow1.5 doesn't support python3.7, you will encounter a problem with \"pip install tensorflow==1.5\"  if you still use python3.7.\r\n\r\n", "Just to verify, did you try the steps mentioned in [TensorFlow website](https://www.tensorflow.org/install/source_windows). Also can you try with lower tensorflow version as suggested by @disposa1 and let us know if that does not help. Thanks!", "Same problem here!\r\n\r\nWindows 10\r\nPython 3.6.0\r\nCUDA 9.0\r\ncudnn 7.5.1 for CUDA 9.0\r\ntensorflow 1.13.1\r\ntensorflow-gpu 1.13.1\r\nGTX 1070\r\nIntel i5-6600\r\n\r\n\r\nCUDA is working fine since i build the device check from the samples and it worked fine. According to process monitor the last dll python loads is System32\\kernell.appcore.dll so probably it is this one or the one after that fails to load. Someone has an idea what dll is causing the problem or probably missing?", "I encountered the same problem!\r\n\r\nWindows 10\r\nPython: 3.7.3\r\nTensorflow: 1.31.1\r\n\r\nMy Solution: Downgrade Python to version 3.6.8", "There seemes to be an issue with python and protobuf. Using Python 3.6.1 fixed the issue with protobuf. I also hat to install CUDA 10.0 and the according cudnn library. On the tensorflow site it says tensorflow 1.13.1 is using CUDA 9 but 10 seems to be obligatory at least with the tensorflow i got from pip.", "@lmiller210 : Please have a look on #26899 and let us know if you have followed the same steps. Thanks!", "You need to Download and install the Microsoft Visual C++ 2015 Redistributable Update 3.\r\nhttps://www.tensorflow.org/install/pip (select windows)", "@achandraa @ymodak I followed all of those steps and its still giving me what appears to be the same error. I'll post it here just in case it is different:\r\n\r\nC:\\Users\\Lucas>python\r\nPython 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Lucas\\AppData\\Local\\Programs\\Python\\Python36\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.", "Apologies for the delay in response. This issue slipped.\r\n**TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.**\r\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\r\n\r\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\r\n* Try Google Colab to use TensorFlow.\r\n    * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install``` to install any other preferred TF version.\r\n    * It has an added advantage since you can you easily switch to different hardware accelerators     \r\n      (cpu, gpu, tpu) as per the task. \r\n    * All you need is a good internet connection and you are all set.\r\n* Try to build TF from sources by changing CPU optimization flags.\r\n\r\nPlease let us know if this helps.", "Python 3.6.8 + tensorflow1.5+ keras2.1.5 works fine on win10.\r\nThanks disposa1!"]}, {"number": 28712, "title": "tensorflowjs_converter install fails due to tf-nightly-2.0-preview missing from Python 3.7", "body": "**Describe the problem**\r\n\r\nCommand used:\r\ntensorflowjs_converter --input_format=tf_frozen_model --output_node_names=final_result  output/saved_model.pb web_model\r\n\r\nGetting below error:\r\ntensorflowjs_converter: command not found\r\n", "comments": ["You need to pip install tensorflowjs before running the converter.\r\n```pip install tensorflowjs```", "Hi @ymodak ,\r\n\r\ntensorflowjs is already installed on my system. Still, I am getting above error.", "Please provide following information; Thanks!\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "could you please check below explanation?\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.3\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not on mobile device\r\nTensorFlow installed from (source or binary): installed pip from terminal using command \"pip install tensorflow\"\r\nTensorFlow version (use command below):1.13.1\r\nPython version:2.7.10\r\nBazel version (if compiling from source):No\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version: could you please tell how the check this?\r\nGPU model and memory:are you talking about tensorflow GPU or system GPU here?\r\n\r\nI was training AI model and I got stuck at below command:\r\ntensorflowjs_converter --input_format=tf_frozen_model --output_node_names=final_result  output/saved_model.pb web_model\r\nPlease check the above command on below page:\r\nhttps://medium.com/testdotai/training-data-for-app-classifier-f217dc005523\r\n\r\nThanks!!\r\n\r\n\r\n\r\n\r\n", "@MonarkA this looks like a path issue where your system can't locate the `tensorflowjs_converter` script. Did you get any errors when doing `pip install tensorflowjs`? (note its a separate pip package from `tensorflow`). Could you try re-installing it?\r\n\r\ncc @pyu10055 @caisq ", "Same here. Use\r\n```bash\r\nsudo pip3 install tensorflowjs\r\n```\r\ngot\r\n\r\n**ERROR: Could not find a version that satisfies the requirement tf-nightly-2.0-preview>=2.0.0.dev20190502 (from tensorflowjs) (from versions: none)\r\nERROR: No matching distribution found for tf-nightly-2.0-preview>=2.0.0.dev20190502 (from tensorflowjs)**\r\n\r\noperating system: OSX 10.14.4\r\nmy python version: 3.7.3\r\npip3 --version\r\npip 19.1.1 from /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pip (python 3.7)\r\n\r\nAnybody can help me out?\r\n", "@cfkxzsat It is weird that your pip can't find a suitable version of tensorflow.\r\nCan you try the standalone install command `pip install tf-nightly-2.0-preview` and see what version you get?", "Same error,bro @caisq\r\n```bash\r\npip3 install tf-nightly-2.0-preview\r\nCollecting tf-nightly-2.0-preview\r\n  ERROR: Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: none)\r\nERROR: No matching distribution found for tf-nightly-2.0-preview\r\n```", "@cfkxzsat Is it possible that you are configured to use a non-public (custom) PyPI? \r\n\r\ntf-nightly-2.0-preview is clearly available: https://pypi.org/project/tf-nightly-2.0-preview/", "@tafsiri ,\r\n\r\nI tried reinstalling tensorflow but I am getting same issue.", "For anyone still running into this, you need to be using python 3.6(and it's associated pip version).  I was still running into issues after `conda install python=3.6`.  It turns out that I had an alias in my bashrc between pip and pip3.  If things still aren't working after changing python versions, try `unalias pip`.\r\n", "@csaroff Interesting. So this is because tf-nightly-2.0-preview is unavailable for Python 3.7. Changed the title of the issue to reflect that.\r\n\r\ncc @gunan @yifeif @av8ramit ", "This is working with latest version , closing this issue. Thank you "]}, {"number": 28711, "title": "Tensorflow lite demo app gives wrong result in GPU delegate (Honor Play Android 9.0 GPU Turbo)", "body": "\r\n**System information**\r\n\r\n- OS Platform and Distribution: Android 9.0 (API 28)\r\n- Mobile device : Huawei Honor Play 9.0 ,GPU: Mali G72 MP12 (GPU Turbo)\r\n- TensorFlow installed from: tensorflow-lite:0.0.0-gpu-experimental\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe tensorflow lite gpu delegate demo application gives incorrect results for image classification , when it is run on GPU; whereas the same float model gives correct results when it is run on CPU.Even the quantized model in the demo application gave correct inference results in this application.We even tried the official deeplab model for semantic segmentation with the same phone; but even in this scenario it gave wrong results (square/stripes), instead of correct masks.\r\nThe same model is running in other phones with Adreno GPU and also in some phones with Mali GPU (eg: Samsung A8+ Android 9.0).\r\n\r\n**Describe the expected behavior**\r\nThe tensorflow lite gpu inference should give same results in cpu and gpu in all the android phones.\r\n\r\n**Other info / logs**\r\nIt looks like the  phone uses a new feature called GPU Turbo .Initially the model\r\nwas working correctly with android 8.1 (stock os).But after the 9.0 upgrade the tflite models are giving wrong results.\r\n", "comments": ["It is working fine in tensorflow-lite:0.0.1-gpu-experimental and latest nightly version ...", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28711\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28711\">No</a>\n"]}, {"number": 28710, "title": "steps_per_epoch parameter in fit not working with tf.keras.utils.Sequence", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Ubuntu 14.04`\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): `binary (conda)`\r\n- TensorFlow version (use command below): `1.13.1`\r\n- Python version: `3.6`\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nsetting the `steps_per_epoch` parameter in `model.fit` (`tf.keras`) to for example 10 should only perform 10 updates per epoch. When passing numpy arrays as inputs to `model.fit` this works as expected, but when using a custom `tf.keras.utils.Sequence`  instance `steps_per_epoch` does not have any effect.\r\n\r\n**Describe the expected behavior**\r\nWhen passing `steps_per_epoch` to `model.fit` as a parameter when training on data fed via a `tf.keras.utils.Sequence` instance the model should be updated exactly `steps_per_epoch` times in every epoch.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nimport math\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass SomeFeeder(tf.keras.utils.Sequence):\r\n    \"\"\"A dummy Sequence.\r\n\r\n    Slightly modified tf.keras.utils.Sequence example.\r\n    \"\"\"\r\n\r\n    def __init__(self, x_set, y_set, batch_size):\r\n            self.x, self.y = x_set, y_set\r\n            self.batch_size = batch_size\r\n\r\n    def __len__(self):\r\n        return math.ceil(len(self.x) / self.batch_size)\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\r\n        return batch_x, batch_y\r\n\r\n\r\n# Simple dummy model.\r\ninput_x = tf.keras.Input((4,))\r\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(input_x)\r\nmodel = tf.keras.Model(inputs=[input_x], outputs=[output])\r\nmodel.compile(\r\n    optimizer=tf.keras.optimizers.SGD(lr=0.1), loss='binary_crossentropy')\r\n\r\n# Dummy data.\r\nx = np.random.rand(100, 4).astype(np.float32)\r\ny = np.random.choice([0, 1], size=(100,)).astype(np.float32)\r\nx = SomeFeeder(x, y, batch_size=4)\r\n\r\n# Train the model with `steps_per_epoch=5`.\r\nmodel.fit(x=x, epochs=10, steps_per_epoch=5)\r\n```\r\n\r\n**Other info / logs**\r\nProduces:\r\n```\r\nEpoch 1/10\r\n25/25 [==============================] - 0s 13ms/step - loss: 0.7041\r\nEpoch 2/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.7007\r\nEpoch 3/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.6993\r\nEpoch 4/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.6981\r\nEpoch 5/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.6939\r\nEpoch 6/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.6954\r\nEpoch 7/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.6936\r\nEpoch 8/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.6914\r\nEpoch 9/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.6928\r\nEpoch 10/10\r\n25/25 [==============================] - 0s 5ms/step - loss: 0.6916\r\n```\r\nI would have expected `5/5` instead of `25/25`\r\n", "comments": ["Can reproduce this. Looks a bug to me.", "I was able to reproduce the mentioned output with TensorFlow version 1.13.1.", "I can't reproduce this issue in tf.nightly, which indicate that this issue has been recently fixed, but hasn't reach release yet.", "TF version = 1.14.1-dev20190523\r\n\r\nEpoch 1/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.6458\r\n5/5 [==============================] - 0s 39ms/step - loss: 0.7210\r\nEpoch 2/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.9641\r\n5/5 [==============================] - 0s 1ms/step - loss: 0.7426\r\nEpoch 3/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.9364\r\n5/5 [==============================] - 0s 1ms/step - loss: 0.7591\r\nEpoch 4/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.6825\r\n5/5 [==============================] - 0s 1ms/step - loss: 0.6975\r\nEpoch 5/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.6900\r\n5/5 [==============================] - 0s 1ms/step - loss: 0.6927\r\nEpoch 6/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.6488\r\n5/5 [==============================] - 0s 20ms/step - loss: 0.6489\r\nEpoch 7/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.7134\r\n5/5 [==============================] - 0s 1ms/step - loss: 0.7097\r\nEpoch 8/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.7574\r\n5/5 [==============================] - 0s 1ms/step - loss: 0.7197\r\nEpoch 9/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.6375\r\n5/5 [==============================] - 0s 1ms/step - loss: 0.6965\r\nEpoch 10/10\r\n1/5 [=====>........................] - ETA: 0s - loss: 0.6357\r\n5/5 [==============================] - 0s 1ms/step - loss: 0.6624\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28710\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28710\">No</a>\n", "I am using TensorFlow version 1.13.1. Is there any work around for this?", "Can confirm, ran a training run in 1.13.1 and had this issue, upgraded to 1.14.0 without doing anything else and now works correctly.", "This bug is still present in 1.13.2."]}, {"number": 28709, "title": "defun + random + addition = explode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: Colab or WSL Ubuntu 18.04\r\n- Mobile device if the issue happens on mobile device:\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: 1.13.1 or 1.14.1-dev20190514\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nUsing `defun` in eager mode, this very simple function seems to cause havoc. In particular, the first run takes a *long* time and a *lot* of memory, both scaling seemingly exponentially with the `n_iter` parameter below.\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\ntf.enable_v2_behavior()\r\n\r\ndef f(D, num_iter):\r\n    x = tf.random_uniform((D,))\r\n    for i in range(num_iter):\r\n        x = x + x\r\n    return x\r\n\r\n# DANGER: Increasing this much beyond 20 may use up all your memory...\r\nn_iter = 22\r\n\r\nt0 = time.time()\r\nf(10, n_iter)\r\nprint(time.time() - t0)  # order ms\r\n\r\nf_g = tf.contrib.eager.defun(f)\r\nfor i in range(3):\r\n    t0 = time.time()\r\n    f_g(10, n_iter)\r\n    print(time.time() - t0)  # around 30s first run for n_iter=22! After that it's fast.\r\n```\r\nReplacing `x = tf.random_uniform((D,))` with `x = tf.ones((D,))` eliminates the problem, as does replacing `x = x + x` with `x = 2*x`.\r\nIn graph mode the problem does not occur:\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\ndef graph_fun(node):\r\n    with tf.Session() as sesh:\r\n        for i in range(3):\r\n            t0 = time.time()\r\n            sesh.run(node)\r\n            print(time.time() - t0)\r\n\r\ndef f(D, num_iter):\r\n    x = tf.random_uniform((D,))\r\n    for i in range(num_iter):\r\n        x = x + x\r\n    return x\r\n\r\nn_iter = 100\r\n\r\ntf.reset_default_graph()\r\nt0 = time.time()\r\nfres = f(10, n_iter)\r\nprint(time.time() - t0)  # around 0.4s\r\n\r\ngraph_fun(fres)  # around 0.6 s first run, then essentially zero.\r\n```\r\n\r\n**Describe the expected behavior**\r\nThis very simple defun'd function should execute quickly, including the graph-building and graph optimization steps.\r\n\r\n", "comments": ["@amilsted Able to reproduce the issue in eager mode as below.\r\n0.0017082691192626953\r\n28.53006386756897\r\n0.0010156631469726562\r\n0.0005068778991699219.\r\n\r\nAfter Replacing x = tf.random_uniform((D,)) with x = tf.ones((D,)) eliminates the problem, as does replacing x = x + x with x = 2*x.\r\n0.0022535324096679688\r\n0.0447695255279541\r\n0.0005967617034912109\r\n0.0005047321319580078", "This is a known issue because n_iter is a python scalar in your example, so this just generates a very large graph. \r\n\r\nThe small operations which improve things should just make graph building quicker (afaict).", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28709\">No</a>\n", "> \r\n> \r\n> This is a known issue because n_iter is a python scalar in your example, so this just generates a very large graph.\r\n> \r\n> The small operations which improve things should just make graph building quicker (afaict).\r\n\r\n`n_iter` is `22` in the example. This is not a \"large graph\". Also, what should be the same graph is fast in graph mode, but (apparently) *exponentially* slower (space and time) in the `n_iter` variable when using `defun`. If this is expected behavior I am somewhat flabbergasted!", "Hi There,\n\n We are checking to see if you still need help on this issue, as you are using an older version of tensorflow(1.x) which is officially considered as end of life. We recommend that you upgrade to 2.4 or later version and let us know if the issue still persists in newer versions. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, Please open a new issue for any help you need against 2.x, and we will get you the right help. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28709\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28709\">No</a>\n"]}, {"number": 28708, "title": "gcloud ai-platform fails with TFv2 Saved Model", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below):  v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.6.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI am following this tutorial: https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models and using a model I created in TFv2. I am able to create the Saved_Model in my Colab using the following code:\r\n```\r\nimport time\r\nsaved_model_path = \"/content/gdrive/My Drive/Colab Notebooks/{}\".format(int(time.time()))\r\ntf.keras.experimental.export_saved_model(restored_model, saved_model_path)\r\n```\r\nThis created a folder with the PB file and assets and variables folder. I then upload that to my bucket in the Google Cloud. I then ran the command to predict:\r\n`gcloud ai-platform local predict --model-dir=$MODEL_DIR --text-instances ci.txt --framework TENSORFLOW`\r\nThe ci.txt is a comma separated list of my input numbers.\r\nI get the following error that shows my values:\r\n\r\n> Traceback (most recent call last):\r\n>   File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 184, in <module>\r\n>     main()\r\n>   File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 179, in main\r\n>     signature_name=args.signature_name)\r\n>   File \"/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_lib.py\", line 102, in local_predict\r\n>     predictions = model.predict(instances, signature_name=signature_name)\r\n>   File \"/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_utils.py\", line 268, in predict\r\n>     preprocessed, stats=stats, **kwargs)\r\n>   File \"/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py\", line 363, in predict\r\n>     \"Exception during running the graph: \" + str(e))\r\n> cloud.ml.prediction.prediction_utils.PredictionError: Failed to run the provided model: Exception during running the graph: invalid literal for float(): 81,71,80,76,1,3 (Error code: 2)\r\n\r\n\r\n**Describe the expected behavior**\r\nI expect the code to return a single number.\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\nHere is the saved_model: \r\n[ver1.zip](https://github.com/tensorflow/tensorflow/files/3178529/ver1.zip)\r\n\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["**Full Error**  \r\nERROR: (gcloud.ai-platform.local.predict) 2019-05-14 10:19:23.674836: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2019-05-14 10:19:23.687172: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\r\n2019-05-14 10:19:23.687418: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556c01ae34a0 executing computations on platform Host. Devices:\r\n2019-05-14 10:19:23.687444: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nWARNING:tensorflow:From /google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:210: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nWARNING:tensorflow:From /google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:210: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\nWARNING:root:Error updating signature __saved_model_init_op: The name 'init_1' refers to an Operation, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".\r\nERROR:root:Exception during running the graph: invalid literal for float(): 81,71,80,76,1,3\r\nTraceback (most recent call last):\r\n  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 184, in <module>\r\n    main()\r\n  File \"lib/googlecloudsdk/command_lib/ml_engine/local_predict.py\", line 179, in main\r\n    signature_name=args.signature_name)\r\n  File \"/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_lib.py\", line 102, in local_predict\r\n    predictions = model.predict(instances, signature_name=signature_name)\r\n  File \"/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/prediction_utils.py\", line 268, in predict\r\n    preprocessed, stats=stats, **kwargs)\r\n  File \"/google/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py\", line 363, in predict\r\n    \"Exception during running the graph: \" + str(e))\r\ncloud.ml.prediction.prediction_utils.PredictionError: Failed to run the provided model: Exception during running the graph: invalid literal for float(): 81,71,80,76,1,3 (Error code: 2)", "@ehennis Can you recheck and confirm that your cpu supports AVX2? ", "I built the model in Google Colab and exported there as well. Does my computer supporting that matter?\r\n\r\nIf it does, I am running Windows 10 and couldn't find an easy way to determine if my cpu supports that. I did see a place to run a check in C++ code.", "@ehennis This issue is more suitable on [Google Cloud AI Platform](https://github.com/GoogleCloudPlatform/cloudml-samples/issues). Please post it on [Google Cloud AI Platform](https://github.com/GoogleCloudPlatform/cloudml-samples/issues). Thanks!", "Created ticket 420\r\n\r\nhttps://github.com/GoogleCloudPlatform/cloudml-samples/issues/420"]}, {"number": 28707, "title": "How to apply gradient clipping in TensorFlow 2.0?  ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): Tensorflow 2.0 Alpha\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n\r\nI want to apply gradient clipping in TF 2.0, the best solution is to decorator optimizer with `tf.contrib.estimator.clip_gradients_by_norm` in TF 1.x.\r\n\r\nHowever, I can't find this function in TF2.0 after trying many methods. As I know, the tf.contrib has been clean up in TF 2.0\r\n", "comments": ["@tanzhenyu Is this already supported in keras optimizers v2? ", "A simple method to apply gradient clipping in TensorFlow 2.0:\r\n```\r\nfrom tensorflow.keras import optimizers\r\nsgd = optimizers.SGD(lr=0.01, clipvalue=0.5)\r\n```", "Hi,\r\n\r\nYou can clip the gradients as we used to do in tfx1.0\r\nTry this code\r\n\r\n```\r\nwith tf.GradientTape() as tape:\r\n            predictions= model(inputs, training=True)\r\n            loss = get_loss(targets, predictions)\r\n\r\ngradients = tape.gradient(loss, model.trainable_variables)\r\ngradients = [(tf.clip_by_value(grad, -1.0, 1.0))\r\n                                  for grad in gradients]\r\noptimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n```", "There are essentially 2 ways to do this, as mentioned above, so I will just summarize here:\r\n1. pass clipvalue or clipnorm to optimizer constructor, this will clip all gradients\r\n2. customized clip using gradients=tape.gradient, gradients=[tf.process_gradient_???(grad) for grad in gradients]\r\n\r\nThey are both correct, it's just the 2nd option gives you more flexibility.", "As it is mentioned [here](https://github.com/tensorflow/tensorflow/issues/33929), there are cases where optimizer clipvalue & clipnorm are totally ignored in tf 2.0 and 2.1.\r\nHowever, it is fixed in tf 2.2 (see [here](https://github.com/tensorflow/tensorflow/commit/69da929ad4d5ba605507efa1f52b382a55b6a969))", "> There are essentially 2 ways to do this, as mentioned above, so I will just summarize here:\r\n> \r\n> 1. pass clipvalue or clipnorm to optimizer constructor, this will clip all gradients\r\n> 2. customized clip using gradients=tape.gradient, gradients=[tf.process_gradient_???(grad) for grad in gradients]\r\n> \r\n> They are both correct, it's just the 2nd option gives you more flexibility.\r\n\r\nIf we use `clipnorm=1` in the constructor of `keras.optimizers.Optimizer`, the optimizer clip gradients using `clipnorm` for each Variable, not the global norm for all Variable. An example is as followings.\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow import keras \r\n \r\nx = tf.Variable([3.0, 4.0]) \r\ny = tf.Variable([1.0, 1.0, 1.0, 1.0]) \r\nz = tf.reduce_sum(x ** 2) + tf.reduce_sum(y) \r\nadam = keras.optimizers.Adam(0.01, clipnorm=1.0) \r\ngrads = adam.get_gradients(z, [x, y]) \r\nsess = tf.Session() \r\nsess.run(tf.global_variables_initializer()) \r\nprint(sess.run(grads))  \r\n# outputs: [0.6, 0.8], [0.5, 0.5, 0.5, 0.5]\r\n# that means the optimizer clip gradients using the clipnorm parameter as a local norm for each Variable\r\n```", "> > There are essentially 2 ways to do this, as mentioned above, so I will just summarize here:\r\n> > \r\n> > 1. pass clipvalue or clipnorm to optimizer constructor, this will clip all gradients\r\n> > 2. customized clip using gradients=tape.gradient, gradients=[tf.process_gradient_???(grad) for grad in gradients]\r\n> > \r\n> > They are both correct, it's just the 2nd option gives you more flexibility.\r\n> \r\n> If we use `clipnorm=1` in the constructor of `keras.optimizers.Optimizer`, the optimizer clip gradients using `clipnorm` for each Variable, not the global norm for all Variable. An example is as followings.\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> from tensorflow import keras \r\n>  \r\n> x = tf.Variable([3.0, 4.0]) \r\n> y = tf.Variable([1.0, 1.0, 1.0, 1.0]) \r\n> z = tf.reduce_sum(x ** 2) + tf.reduce_sum(y) \r\n> adam = keras.optimizers.Adam(0.01, clipnorm=1.0) \r\n> grads = adam.get_gradients(z, [x, y]) \r\n> sess = tf.Session() \r\n> sess.run(tf.global_variables_initializer()) \r\n> print(sess.run(grads))  \r\n> # outputs: [0.6, 0.8], [0.5, 0.5, 0.5, 0.5]\r\n> # that means the optimizer clip gradients using the clipnorm parameter as a local norm for each Variable\r\n> ```\r\n\r\n`global_clipnorm` is your solution. Though that's a 2.4 feature or tf-nightly given we recently pushed it", "> > > There are essentially 2 ways to do this, as mentioned above, so I will just summarize here:\r\n> > > \r\n> > > 1. pass clipvalue or clipnorm to optimizer constructor, this will clip all gradients\r\n> > > 2. customized clip using gradients=tape.gradient, gradients=[tf.process_gradient_???(grad) for grad in gradients]\r\n> > > \r\n> > > They are both correct, it's just the 2nd option gives you more flexibility.\r\n> > \r\n> > \r\n> > If we use `clipnorm=1` in the constructor of `keras.optimizers.Optimizer`, the optimizer clip gradients using `clipnorm` for each Variable, not the global norm for all Variable. An example is as followings.\r\n> > ```python\r\n> > import tensorflow as tf\r\n> > from tensorflow import keras \r\n> >  \r\n> > x = tf.Variable([3.0, 4.0]) \r\n> > y = tf.Variable([1.0, 1.0, 1.0, 1.0]) \r\n> > z = tf.reduce_sum(x ** 2) + tf.reduce_sum(y) \r\n> > adam = keras.optimizers.Adam(0.01, clipnorm=1.0) \r\n> > grads = adam.get_gradients(z, [x, y]) \r\n> > sess = tf.Session() \r\n> > sess.run(tf.global_variables_initializer()) \r\n> > print(sess.run(grads))  \r\n> > # outputs: [0.6, 0.8], [0.5, 0.5, 0.5, 0.5]\r\n> > # that means the optimizer clip gradients using the clipnorm parameter as a local norm for each Variable\r\n> > ```\r\n> \r\n> `global_clipnorm` is your solution. Though that's a 2.4 feature or tf-nightly given we recently pushed it\r\nThanks! How to perform global gradient clip at tf 1.x?\r\n", "> > > > There are essentially 2 ways to do this, as mentioned above, so I will just summarize here:\r\n> > > > \r\n> > > > 1. pass clipvalue or clipnorm to optimizer constructor, this will clip all gradients\r\n> > > > 2. customized clip using gradients=tape.gradient, gradients=[tf.process_gradient_???(grad) for grad in gradients]\r\n> > > > \r\n> > > > They are both correct, it's just the 2nd option gives you more flexibility.\r\n> > > \r\n> > > \r\n> > > If we use `clipnorm=1` in the constructor of `keras.optimizers.Optimizer`, the optimizer clip gradients using `clipnorm` for each Variable, not the global norm for all Variable. An example is as followings.\r\n> > > ```python\r\n> > > import tensorflow as tf\r\n> > > from tensorflow import keras \r\n> > >  \r\n> > > x = tf.Variable([3.0, 4.0]) \r\n> > > y = tf.Variable([1.0, 1.0, 1.0, 1.0]) \r\n> > > z = tf.reduce_sum(x ** 2) + tf.reduce_sum(y) \r\n> > > adam = keras.optimizers.Adam(0.01, clipnorm=1.0) \r\n> > > grads = adam.get_gradients(z, [x, y]) \r\n> > > sess = tf.Session() \r\n> > > sess.run(tf.global_variables_initializer()) \r\n> > > print(sess.run(grads))  \r\n> > > # outputs: [0.6, 0.8], [0.5, 0.5, 0.5, 0.5]\r\n> > > # that means the optimizer clip gradients using the clipnorm parameter as a local norm for each Variable\r\n> > > ```\r\n> > \r\n> > \r\n> > `global_clipnorm` is your solution. Though that's a 2.4 feature or tf-nightly given we recently pushed it\r\n> > Thanks! How to perform global gradient clip at tf 1.x?\r\n\r\nIf you're using tf-nightly, then tf.compat.v1 everything", "> As it is mentioned [here](https://github.com/tensorflow/tensorflow/issues/33929), there are cases where optimizer clipvalue & clipnorm are totally ignored in tf 2.0 and 2.1.\r\n> However, it is fixed in tf 2.2 (see [here](https://github.com/tensorflow/tensorflow/commit/69da929ad4d5ba605507efa1f52b382a55b6a969))\r\n\r\nThe fix doesn't seem to work in tf 2,2, and is also not working for tf 2.3"]}, {"number": 28706, "title": "Keep version fixed when searching API docs", "body": "When using the search bar in the [API docs](https://www.tensorflow.org/versions/r1.13/api_docs/python/tf) with a version in the URL, any search will include pages from different API versions, which means you often end up in the wrong module and have to click on the correct version _again_.\r\n\r\n## How to reproduce\r\n\r\n- Go to: https://www.tensorflow.org/versions/r1.13/api_docs/python/tf\r\n- Type in 'unsorted_segment_' and wait for the results to appear (see image)\r\n- Choose the top 'Pages' result\r\n- You're now in the 1.9 API rather than 1.13. \r\n\r\n<img width=\"394\" alt=\"Screenshot 2019-05-14 at 16 13 38\" src=\"https://user-images.githubusercontent.com/49023008/57705141-8d8a4600-7663-11e9-8640-5a3fdde51ae8.png\">", "comments": ["@ianhellstrom I see 1.13.1 on top of the results. Could you check again or let me know if I am making any mistake. Here is the screenshot. Thanks!\r\n\r\nunsorted_segment_\r\n<img width=\"914\" alt=\"Screen Shot 2019-05-22 at 8 14 34 PM\" src=\"https://user-images.githubusercontent.com/46058173/58223447-8819a500-7cce-11e9-827a-d1ce53a8111a.png\">\r\n", "@ianhellstrom Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Hi @ianhellstrom \r\n\r\nWe've hidden all the old versions of the docs from search. We've also moved most of the old versions off the site, Into [branches on the tensorflow_docs repository](https://github.com/tensorflow/docs/tree/r1.9/site/en/api_docs).\r\n\r\nSo I think this is fixed.\r\n"]}, {"number": 28705, "title": "How to build Tensorflow C++ API with Visual Studio 2017", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution : Windows 10 Home \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: N/A\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): Visual C++\r\n- CUDA/cuDNN version: 7.4\r\n- GPU model and memory: RTX 2080 ti\r\n\r\n\r\n\r\n**Description of the problem**\r\nI have created a Mask-RCNN model and need to deploy it inside a C++ application. I could not find any guide to do it for TF 2.0 so after looking around I found that we can save the model as a Tensorflow SavedModel and then load the model to predict (forward pass) using Tensorflow C++ API. \r\nBut the problem is I cannot find a way to get Tensorflow C++ API working with Visual Studio 2017. \r\n\r\nI tried the steps outlined by https://joe-antognini.github.io/machine-learning/build-windows-tf\r\nBut it uses Cmake which is not supported by the Tesnorflow C++ API.\r\n\r\nCould anyone please point me to a guide/resource that can help me solve this problem.\r\n\r\nOr could you suggest any alternative where I could call my python model (forward pass only) from the C++ application. \r\n\r\n\r\n**Using CMake**\r\n\r\nThis is what I got when I used the guide above ->\r\n\r\n[Executed command] ->  cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=\"C:\\Users\\Soumya Mohanty\\swig\\swigwin-4.0.0\\swig.exe\" -DPYTHON_EXECUTABLE=\"C:\\Anaconda3\\python.exe\" -DPYTHON_LIBRARIES=\"C:\\Anaconda3\\libs\\python3.lib\" -DPYTHON_INCLUDE_DIR=\"C:\\Anaconda3\\include\" -DNUMPY_INCLUDE_DIR=\"C:\\Anaconda3\\Lib\\site-packages\\numpy\"\r\n\r\nError Message:\r\n\r\n-- Building for: Visual Studio 15 2017\r\nCMake Warning at CMakeLists.txt:9 (message):\r\nYour current cmake generator is set to use 32 bit toolset architecture.\r\nThis may cause \"compiler out of heap space\" errors when building. Consider\r\nusing the flag -Thost=x64 when running cmake. Ignore this if you are on\r\nCMake GUI.\r\n-- Selecting Windows SDK version 10.0.17134.0 to target Windows 10.0.14393.\r\n-- The C compiler identification is MSVC 19.22.27706.96\r\n-- The CXX compiler identification is MSVC 19.22.27706.96\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe\r\n-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe -- works\r\n-- Detecting C compiler ABI info\r\n-- Detecting C compiler ABI info - done\r\n-- Detecting C compile features\r\n-- Detecting C compile features - done\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe\r\n-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2017/Enterprise/VC/Tools/MSVC/14.14.26428/bin/Hostx64/x64/cl.exe -- works\r\n-- Detecting CXX compiler ABI info\r\n-- Detecting CXX compiler ABI info - done\r\n-- Detecting CXX compile features\r\n-- Detecting CXX compile features - done\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED\r\n-- Performing Test COMPILER_OPT_ARCH_NATIVE_SUPPORTED - Failed\r\n-- Performing Test MSVC_OPENMP_SUPPORT\r\n-- Performing Test MSVC_OPENMP_SUPPORT - Success\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED\r\n-- Performing Test COMPILER_OPT_WIN_CPU_SIMD_SUPPORTED - Success\r\n-- D:/Tensorflow/build_x64/abseil_cpp/src/abseil_cpp_build\r\n-- Found PythonInterp: C:/ProgramData/Anaconda3/python.exe (found version \"3.6.5\")\r\n-- Found PythonLibs: C:/ProgramData/Anaconda3/libs/python36.lib (found version \"3.6.5\")\r\nCMake Error at tf_python.cmake:217 (message):\r\nPython module not found: tensorflow/contrib/tpu/ops\r\nCall Stack (most recent call first):\r\nCMakeLists.txt:612 (include)\r\n\r\nThis fails because the new Tensorflow repo doesn't have an \"ops\" file in the  tpu folder. \r\n\r\n**Using Bazel**\r\nI have been trying to build tensorflow.dll using Bazel as explained in #24885 but that too is not working. I am running command :  bazel build --config=opt //tensorflow:tensorflow.dll\r\n\r\nError message:\r\nC:\\tensorflow> bazel build --config=opt //tensorflow:tensorflow.dll\r\nINFO: Build options --copt and --define have changed, discarding analysis cache.\r\nINFO: Analysed target //tensorflow:tensorflow.dll (1 packages loaded, 8366 targets configured).\r\nINFO: Found 1 target...\r\nERROR: C:/users/soumya mohanty/_bazel_soumya mohanty/xv6zejqw/external/gif_archive/BUILD.bazel:45:1: Executing genrule @gif_archive//:windows_unistd_h failed (Exit -1). Note: Remote connection/protocol failed with: execution failed: bin failed: error executing command\r\n  cd C:/users/soumya mohanty/_bazel_soumya mohanty/xv6zejqw/execroot/org_tensorflow\r\n  SET PATH=C:\\msys64\\usr;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\bin;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0\\libnvvp;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NGX;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\NVIDIA Corporation\\NVIDIA NvDLISR;C:\\Program Files\\Git\\cmd;C:\\cmake\\cmake-3.14.3-win64-x64\\bin;C:\\Anaconda3\\envs\\Deep_Learning;C:\\Anaconda3\\envs\\Deep_Learning\\Scripts;C:\\toolkit\\Bazel-0.24.0;C:\\msys64;C:\\msys64\\usr\\bin;C:\\Anaconda3\\envs\\Deep_Learning\\Scripts;C:\\Users\\Soumya Mohanty\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\Soumya Mohanty\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\r\n    SET PYTHON_BIN_PATH=C:/Anaconda3/envs/Deep_Learning/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Anaconda3/envs/Deep_Learning/lib/site-packages\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n  C:/msys64/usr/bin -c source external/bazel_tools/tools/genrule/genrule-setup.sh; touch bazel-out/x64_windows-opt/genfiles/external/gif_archive/windows/unistd.h\r\nExecution platform: @bazel_tools//platforms:host_platform\r\nAction failed to execute: java.io.IOException: ERROR: src/main/native/windows/processes-jni.cc(454): CreateProcessW(\"C:\\msys64\\usr\\bin\" -c \"source external/bazel_tools/tools/genrule/genrule-setup.sh; touch bazel-out/x64_windows-opt/genfiles/external/gif_archive/windows/unistd.h\"): Access is denied.\r\n\r\nTarget //tensorflow:tensorflow.dll failed to build\r\nINFO: Elapsed time: 1.492s, Critical Path: 0.02s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\nThere is a space in the user name and that is causing the cd C:/users/soumya mohanty/_bazel_soumya mohanty/xv6zejqw/execroot/org_tensorflow command to fail.\r\nBut I am running as Administrator so I dont really know how this user name is coming into play.\r\n\r\n\r\n**Using Bazel with another user(name does not have spaces)**\r\nI tried using another user which does not have a space in the user name but then I get a different error:\r\n\r\nERROR: C:/users/tensorflowuser/_bazel_tensorflowuser/xv6zejqw/external/nsync/BUILD:463:1: C++ compilation of rule '@nsync//:nsync_cpp' failed (Exit 2): cl.exe failed: error executing command\r\n  cd C:/users/tensorflowuser/_bazel_tensorflowuser/xv6zejqw/execroot/org_tensorflow\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\INCLUDE;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\BIN\\amd64;C:\\Windows\\Microsoft.NET\\Framework64\\v4.0.30319;C:\\Windows\\Microsoft.NET\\Framework64\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\x86;;C:\\Windows\\system32\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Anaconda3/envs/Deep_Learning/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Anaconda3/envs/Deep_Learning/lib/site-packages\r\n    SET TEMP=C:\\Users\\TENSOR~1\\AppData\\Local\\Temp\r\n    SET TF_DOWNLOAD_CLANG=0\r\n    SET TF_NEED_CUDA=0\r\n    SET TF_NEED_OPENCL_SYCL=0\r\n    SET TF_NEED_ROCM=0\r\n    SET TMP=C:\\Users\\TENSOR~1\\AppData\\Local\\Temp\r\n  C:/Program Files (x86)/Microsoft Visual Studio 14.0/VC/bin/amd64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=\"redacted\" -D__TIMESTAMP__=\"redacted\" -D__TIME__=\"redacted\" /Gy /Gw -w -DWIN32_LEAN_AND_MEAN y /TP -DNSYNC_ATOMIC_CPP11 -DNSYNC_USE_CPP11_TIMEPOINT -I./external/nsync//platform/c++11 -I./external/nsync//platform/win32 -I./external/nsync//platform/msvc -I./external/nsync//platform/x86_64 -I./external/nsync//public -I./external/nsync//internal -I./external/nsync//platform/posix /Fobazel-out/x64_windows-opt/bin/external/nsync/_objs/nsync_cpp/nsync_semaphore_mutex.obj /c external/nsync/platform/c++11/src/nsync_semaphore_mutex.cc\r\nExecution platform: @bazel_tools//platforms:host_platform\r\ny\r\nc1xx: fatal error C1083: Cannot open source file: 'y': No such file or directory\r\nGenerating Code...\r\nTarget //tensorflow:tensorflow.dll failed to build\r\nINFO: Elapsed time: 90.347s, Critical Path: 5.10s\r\nINFO: 120 processes: 120 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n**Update**\r\nbazel build //tensorflow:libtensorflow_cc.so works and builds successfully. \r\n\r\nAnd after running the above command the build for bazel build //tensorflow:tensorflow.dll also works successfully\r\n\r\nBut I have no idea where to find it and how to link it to Visual Studio  2017, so I could write some Tensorflow code in C++ and load my SavedModel \r\n \r\nThank you", "comments": ["Does this issue help?\r\nhttps://github.com/tensorflow/tensorflow/issues/24885", "I have mentioned this issue in my post as well. It did help but does not completely solve my problem. \r\nI was able to build it successfully and have included some header file paths. But I am still unable to get all the required symbols. \r\nI renamed tensorflow.dll.if.lib to tensorflow.lib and added it to the linker in visual studio. This solves some missing symbols but many are still missing. \r\n\r\nCould you please tell me any way to add the missing symbols?\r\n\r\nAs of now I have the following error :\r\n\r\nerror LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::LoadSavedModel(struct tensorflow::SessionOptions const &,class tensorflow::RunOptions const &,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const &,class std::unordered_set<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::hash<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > const &,struct tensorflow::SavedModelBundle * const)\" (?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@Z) ", "@rounakskm Try adding required .ddl files to the folder where the .exe is present.", "try this but only for vs2015 https://github.com/tensorflow/tensorflow/tree/r2.0/tensorflow/contrib/cmake", "Hi @jume2003 \r\nI tried to download r2.0 tensorflow code for building the dll for c++. Initially, I found CMake issues while generating tensorflow.sln visual studio solution. After correcting the tf_core_ops.cmake and tf_core_kernerls.cmake files, tensorflow sln has been generated. But while building x64 Release version, I saw almost 397 compilation errors including sqlite and dnn.pb.h. Have you seen these types of issues while building the solution? Have you successfully built the VisualStudio 2017 solution? If yes, please let me know what the changes are required to tensorfow r2.0 code\r\nThanks\r\nMadan", "You are right in saying that compiling TF through CMake is not supported. It won't work. The only solution is to rely on bazel.", "> I have mentioned this issue in my post as well. It did help but does not completely solve my problem.\r\n> I was able to build it successfully and have included some header file paths. But I am still unable to get all the required symbols.\r\n> I renamed tensorflow.dll.if.lib to tensorflow.lib and added it to the linker in visual studio. This solves some missing symbols but many are still missing.\r\n> \r\n> Could you please tell me any way to add the missing symbols?\r\n> \r\n> As of now I have the following error :\r\n> \r\n> error LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::LoadSavedModel(struct tensorflow::SessionOptions const &,class tensorflow::RunOptions const &,class std::basic_string<char,struct std::char_traits,class std::allocator > const &,class std::unordered_set<class std::basic_string<char,struct std::char_traits,class std::allocator >,struct std::hash<class std::basic_string<char,struct std::char_traits,class std::allocator > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits,class std::allocator > >,class std::allocator<class std::basic_string<char,struct std::char_traits,class std::allocator > > > const &,struct tensorflow::SavedModelBundle * const)\" (?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@z)\r\n\r\nI get the same issue...", "@richaolas, this may help.\r\nhttps://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows", "@ktsumura Thanks lot. I think it will be helpful.", "> > I have mentioned this issue in my post as well. It did help but does not completely solve my problem.\r\n> > I was able to build it successfully and have included some header file paths. But I am still unable to get all the required symbols.\r\n> > I renamed tensorflow.dll.if.lib to tensorflow.lib and added it to the linker in visual studio. This solves some missing symbols but many are still missing.\r\n> > Could you please tell me any way to add the missing symbols?\r\n> > As of now I have the following error :\r\n> > error LNK2001: unresolved external symbol \"class tensorflow::Status __cdecl tensorflow::LoadSavedModel(struct tensorflow::SessionOptions const &,class tensorflow::RunOptions const &,class std::basic_string<char,struct std::char_traits,class std::allocator > const &,class std::unordered_set<class std::basic_string<char,struct std::char_traits,class std::allocator >,struct std::hash<class std::basic_string<char,struct std::char_traits,class std::allocator > >,struct std::equal_to<class std::basic_string<char,struct std::char_traits,class std::allocator > >,class std::allocator<class std::basic_string<char,struct std::char_traits,class std::allocator > > > const &,struct tensorflow::SavedModelBundle * const)\" (?LoadSavedModel@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@AEBVRunOptions@1@AEBV?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@AEBV?$unordered_set@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@U?$hash@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@2@U?$equal_to@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@2@V?$allocator@V?$basic_string@DU?$char_traits@D@std@@v?$allocator@D@2@@std@@@2@@6@QEAUSavedModelBundle@1@@z)\r\n> \r\n> I get the same issue...\r\n\r\nI get the same issue... does anyone has any update i have been stuck on this for the last two weeks", "@rounakskm We see that you are using old version of tensorflow, We recommend that you upgrade to 2.6.0 and let us know if the issue persists ? Please have a look at the [link ](https://www.tensorflow.org/install/source_windows) and similar [issue](https://stackoverflow.com/questions/41070330/is-it-possible-to-use-tensorflow-c-api-on-windows) for more reference.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28705\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/28705\">No</a>\n"]}, {"number": 28704, "title": "tfexample_decoder_test.py fails on s390x for dtype float32 and image_format 'raw'", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):Source\r\n- TensorFlow version (use command below):1.12.0\r\n- Python version:2.7.12\r\n- Bazel version (if compiling from source):0.15.0\r\n- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609\r\n- CUDA/cuDNN version:NA\r\n- GPU model and memory:NA\r\n\r\n**Describe the current behavior**\r\npython ./tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py TFExampleDecoderTest.testDecodeExampleWithRawEncodingFloatDtype fails\r\n\r\n**Describe the expected behavior**\r\nIt should pass on s390x like on Intel.\r\n\r\n**Code to reproduce the issue**\r\npython ./tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py TFExampleDecoderTest.testDecodeExampleWithRawEncodingFloatDtype \r\n\r\n**Other info / logs**\r\npython ./tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py TFExampleDecoderTest.testDecodeExampleWithRawEncodingFloatDtype\r\n\r\n======================================================================\r\nFAIL: testDecodeExampleWithRawEncodingFloatDtype (__main__.TFExampleDecoderTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"./tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py\", line 242, in testDecodeExampleWithRawEncodingFloatDtype\r\n    image, serialized_example = self.GenerateImage(\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 1591, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 1561, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 1496, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 1395, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py\", line 778, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=0\r\nMismatched value: a is different from b.\r\nnot close where = (array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]), array([0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2]), array([1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]))\r\nnot close lhs = [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.\r\n  16.  17.]\r\nnot close rhs = [  4.60060299e-41   8.96831017e-44   2.30485571e-41   4.60074312e-41\r\n   5.74868682e-41   6.89663052e-41   8.04457422e-41   9.10844002e-44\r\n   5.83080291e-42   1.15705214e-41   1.73102399e-41   2.30499584e-41\r\n   2.87896769e-41   3.45293955e-41   4.02691140e-41   4.60088325e-41\r\n   4.88786917e-41]\r\nnot close dif = [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.\r\n  16.  17.]\r\nnot close tol = [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\ndtype = float32, shape = (2, 3, 3)\r\n(mismatch 94.4444444444%)\r\n x: array([[[  0.,   1.,   2.],\r\n        [  3.,   4.,   5.],\r\n        [  6.,   7.,   8.]],...\r\n y: array([[[  0.000000e+00,   4.600603e-41,   8.968310e-44],\r\n        [  2.304856e-41,   4.600743e-41,   5.748687e-41],\r\n        [  6.896631e-41,   8.044574e-41,   9.108440e-44]],...\r\n\r\n----------------------------------------------------------------------\r\nRan 1 test in 2.034s\r\n\r\nFAILED (failures=1)\r\n\r\n", "comments": ["In this test case https://github.com/tensorflow/tensorflow/blob/bcee53ab6f4131b6d5a4061c0ad2f2a48e286b90/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py#L233 , image and decoded_image do not match on BE, however on LE, it matches. While debugging more observed that, `image.dtype.str` is `>f4` on s390x however on Intel, its `<f4`. This shows byte ordering of NumPy array is interpreted according to machine endianness. If byte ordering of `image` array is modified in GenerateImage function as below:\r\n\r\n```diff\r\n+import sys\r\n@@ -93,7 +93,14 @@ class TFExampleDecoderTest(test.TestCase):\r\n     num_pixels = image_shape[0] * image_shape[1] * image_shape[2]\r\n     image = np.linspace(\r\n         0, num_pixels - 1,\r\n        num=num_pixels).reshape(image_shape).astype(image_dtype)\r\n\r\n+  # If machine is big endian, change the byte ordering \r\n+  # so that it should be interpreted correctly.\r\n+    if image_dtype == np.float32 and sys.byteorder == \"big\":\r\n+      image = image.astype('<f4')\r\n     tf_encoded = self._Encoder(image, image_format)\r\n```\r\n\r\nWith this change, dtype and data values remain same for image array. image and decoded_image arrays match. @martinwicke @mrry @gunan Please let me know your inputs in this and if I can do PR for this. \r\n\r\nAlso I am curious to know what is the real time use case where this functionality can be encountered?\r\n\r\n\r\n", "Hi @ymodak,\r\nCould you please put your inputs on this?", "Hi @ymodak @martinwicke @mrry Could you please share your thoughts on the proposed change?", "Is changing the test the right way to do this or should this be fixed inside _Encoder?\r\n\r\nOr should it be fixed in `constant`?", "Hi @martinwicke,\r\nThe fix can be kept in _Encoder function but its defined in same test [here](https://github.com/tensorflow/tensorflow/blob/6db4b700fe1a286cc4dda030c1044dcc6c9919b2/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py#L66). We can not fix it inside `constant` because dtype conversion has to be done before calling tostring() on `image` https://github.com/tensorflow/tensorflow/blob/6db4b700fe1a286cc4dda030c1044dcc6c9919b2/tensorflow/contrib/slim/python/slim/data/tfexample_decoder_test.py#L75\r\n\r\nThe diff looks like below in case of _Encoder:\r\n\r\n```diff\r\n+import sys\r\n from tensorflow.contrib.slim.python.slim.data import tfexample_decoder\r\n from tensorflow.core.example import example_pb2\r\n from tensorflow.core.example import feature_pb2\r\n@@ -72,6 +72,9 @@ class TFExampleDecoderTest(test.TestCase):\r\n       tf_image = constant_op.constant(image, dtype=dtypes.uint8)\r\n       return image_ops.encode_png(tf_image)\r\n     if image_format in ['raw', 'RAW']:\r\n+      #If machine is big endian, change the byte ordering\r\n+      if image.dtype == np.float32 and sys.byteorder == \"big\":\r\n+        image = image.astype('<f4')\r\n       return constant_op.constant(image.tostring(), dtype=dtypes.string)\r\n\r\n   def GenerateImage(self, image_format, image_shape, image_dtype=np.uint8):\r\n```\r\n\r\nPlease let us know your inputs. ", "I'm fine with either fix, if you want to make a PR, we'll merge it. \r\n\r\nI'm only concerned that this is fixing the test and not the underlying issue, but I don't have a great solution for that.", "Thanks @martinwicke We have raised a PR for the above changes. Please find it here https://github.com/tensorflow/tensorflow/pull/29527."]}, {"number": 28703, "title": "AttributeError: 'Sequential' object has no attribute 'prediction'", "body": "from flask import Flask\r\nfrom flask_restful import Api, Resource, reqparse\r\n\r\napp = Flask(__name__)\r\napi = Api(app)\r\n\r\nimport os\r\nimport json\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\nimport logging\r\n\r\nfrom collections import Counter\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nfrom keras.models import load_model\r\nfrom keras import backend as K\r\n\r\nfrom keras.preprocessing.image import array_to_img, img_to_array, load_img\r\nfrom scipy.misc import imresize\r\n\r\nfrom PIL import Image\r\n\r\n\r\nusers = [\r\n    {\r\n        \"name\": \"Nicholas\",\r\n        \"age\": 42,\r\n        \"occupation\": \"Network Engineer\",\r\n       \r\n    },\r\n    {\r\n        \"name\": \"Elvin\",\r\n        \"age\": 32,\r\n        \"occupation\": \"Doctor\"\r\n       \r\n    },\r\n    {\r\n        \"name\": \"Jass\",\r\n        \"age\": 22,\r\n        \"occupation\": \"Web Developer\"\r\n      \r\n    }\r\n]\r\n\r\nK.clear_session()\r\n\r\nclass User(Resource):\r\n    def get(self, name):\r\n        for user in users:\r\n            if(name == user[\"name\"]):\r\n                return user, 200\r\n        return \"User not found\", 404\r\n\r\n    def post(self, name):\r\n        parser = reqparse.RequestParser()\r\n        parser.add_argument(\"image\")\r\n        #get the image\r\n        model = load_model('C:/Users/ASUS/Downloads/mse-04-0.0877.h5')\r\n        img_height, img_width, channels = 350, 350, 3\r\n\r\n        img = load_img('C:/xampp/htdocs/faceDetection/image/logo.png')\r\n\r\n        img = imresize(img, size=(img_height, img_width))\t  \r\n        test_x = img_to_array(img).reshape(img_height, img_width, channels) \r\n        test_x = test_x / 255.\r\n        test_x = test_x.reshape((1,) + test_x.shape)\r\n        predicted = model.prediction(test_x)\r\n        self.graph = tf.get_default_graph()\r\n\r\n        return{\r\n        \"name\": \"Nicholas\",\r\n        \"age\": 42,\r\n        \"occupation\": \"Network Engineer\",\r\n        \"prediction\": str(predicted[0][0])\r\n\r\n    }, 201\r\n\r\n\r\n    def put(self, name):\r\n        parser = reqparse.RequestParser()\r\n        parser.add_argument(\"age\")\r\n        parser.add_argument(\"occupation\")\r\n        args = parser.parse_args()\r\n\r\n        for user in users:\r\n            if(name == user[\"name\"]):\r\n                user[\"age\"] = args[\"age\"]\r\n                user[\"occupation\"] = args[\"occupation\"]\r\n                return user, 200\r\n        \r\n        user = {\r\n            \"name\": name,\r\n            \"age\": args[\"age\"],\r\n            \"occupation\": args[\"occupation\"]\r\n        \r\n        }\r\n        users.append(user)\r\n        return user, 201\r\n\r\n    def delete(self, name):\r\n        global users\r\n        users = [user for user in users if user[\"name\"] != name]\r\n        return \"{} is deleted.\".format(name), 200\r\n      \r\napi.add_resource(User, \"/user/<string:name>\")\r\n\r\napp.run(debug=True)\r\n\r\n\r\n\r\n\r\n### Here is the error...\r\nUsing TensorFlow backend.\r\n * Debugger is active!\r\n * Debugger PIN: 301-406-345\r\n * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-05-14 21:20:36.954666: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\nWARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.cast instead.\r\napp.py:66: DeprecationWarning: `imresize` is deprecated!\r\n`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.\r\nUse Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.\r\n  img = imresize(img, size=(img_height, img_width))\r\n127.0.0.1 - - [14/May/2019 21:21:13] \"\u001b[1m\u001b[35mPOST /user/Nicholas HTTP/1.1\u001b[0m\" 500 -\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2309, in __call__\r\n    return self.wsgi_app(environ, start_response)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2295, in wsgi_app\r\n    response = self.handle_exception(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 269, in error_router\r\n    return original_handler(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1741, in handle_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 34, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 2292, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1815, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 269, in error_router\r\n    return original_handler(e)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1718, in handle_user_exception\r\n    reraise(exc_type, exc_value, tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 34, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1813, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1799, in dispatch_request\r\n    return self.view_functions[rule.endpoint](**req.view_args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 458, in wrapper\r\n    resp = resource(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask\\views.py\", line 88, in view\r\n    return self.dispatch_request(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\flask_restful\\__init__.py\", line 573, in dispatch_request\r\n    resp = meth(*args, **kwargs)\r\n  File \"C:\\xampp\\htdocs\\faceDetection\\app.py\", line 70, in post\r\n    predicted = model.prediction(test_x)\r\nAttributeError: 'Sequential' object has no attribute 'prediction'\r\n", "comments": ["@yensan0512  It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from [here](https://github.com/tensorflow/tensorflow/issues/new/choose). We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.\r\n", "@yensan0512 Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28702, "title": "gives an error when import tensorflow", "body": ">>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<pyshell#1>\", line 1, in <module>\r\n    import tensorflow\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed with error code -1073741795\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n>>> \r\n\r\nC:\\Users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439>pip freeze\r\nabsl-py==0.7.1\r\nastor==0.7.1\r\net-xmlfile==1.0.1\r\ngast==0.2.2\r\ngrpcio==1.20.1\r\nh5py==2.9.0\r\njdcal==1.4.1\r\nKeras==2.2.4\r\nKeras-Applications==1.0.7\r\nKeras-Preprocessing==1.0.9\r\nMarkdown==3.1\r\nmock==3.0.5\r\nnumpy==1.16.3\r\nopenpyxl==2.6.2\r\nPillow==6.0.0\r\nprotobuf==3.7.1\r\nPyYAML==5.1\r\nscipy==1.2.1\r\nsix==1.12.0\r\ntensorboard==1.13.1\r\ntensorflow==1.13.1\r\ntensorflow-estimator==1.13.0\r\ntermcolor==1.1.0\r\nWerkzeug==0.15.2\r\n\r\npython 3.7.0 \r\n\r\n\r\nWhat could be the problem?", "comments": ["I don't think tensorflow supports python 3.7.0 right now.", "@igorgertman   Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary? Thanks!\r\n", "Windows 7 64 bit operating system. AMD phenom ii n930 processor.  on the K10 architecture, tensorflow version 1.13.1. Installed through PIP install tensorflow.", "pip 19.1.1 from c:\\users\\\u0434\u043e\u043c\u0430\u0448\u043d\u0438\u0439\\appdata\\local\\programs\\python\\python37\\lib\\sit\r\ne-packages\\pip (python 3.7)\r\nPython 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32", "@igorgertman Please check your processor supports AVX instructions? ", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 28701, "title": "Possible wrong comments in minimax_discriminator_loss", "body": "## URL(s) with the issue:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py#L468\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/gan/python/losses/python/losses_impl.py#L477\r\n\r\n## Description of issue (what needs changing):\r\n\r\nOne line 468:     `# -log((1 - label_smoothing) - sigmoid(D(x)))`\r\n\r\nShouldn't it be `-(1-label_smoothing) * log(sigmoid(D(x))` ?\r\nI'm uncertain about the label_smoothing part, but I think the argument of the `log` is wrong.\r\n\r\nOn line 477: `# -log(- sigmoid(D(G(x))))`\r\nShouldn't it be `-log(1-sigmoid(D(G(x))`?", "comments": ["@brianwa84 Can you PTAL at this issue? Thanks!\r\n", "@joelshor would probably be better", "@joel-shor, that is", "Yes I think you're right. The comment is inconsistent with the function-level description, which is correct. Do you want to change it? Please not that `tf.contrib` will be going away in future version of tensorflow, so it probably makes more sense to change it in `tensorflow_gan` here and here. Do you want to send me a CL to fix this?\r\n\r\nhttps://github.com/tensorflow/gan/blob/master/tensorflow_gan/python/losses/losses_impl.py#L549\r\nhttps://github.com/tensorflow/gan/blob/master/tensorflow_gan/python/losses/losses_impl.py#L558\r\n\r\nFeel free to ignore the label smoothing in the description; it would make more sense to just match the terms in the function-level description.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28701\">No</a>\n"]}, {"number": 28700, "title": " Shown unsupported ops after compilling  tf.lite.TFLiteConverter.from_frozen_graph on SSD_Mobilenet_v1_0.75_depth_coco model", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (or github SHA if from source):1.13.1\r\n\r\n\r\nI tried running TensorflowLite converter as follow:\r\n\r\nimport tensorflow as tf\r\n\r\ngraph_def_file=\"./frozen_inference_graph.pb\"\r\ninput_arrays=[\"image_tensor\"]\r\noutput_arrays=[\"detection_boxes\",\"detection_scores\",\"num_detections\",\"detection_classes\"]\r\ninput_tensor={\"image_tensor\":[1,300,300,3]}\r\n\r\nconverter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays,input_tensor)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\ntflite_model = converter.convert()\r\nopen(\"detect.tflite\", \"wb\").write(tflite_model)\r\n\r\nOnce it is complete, I get the following output of unsupported ops:\r\n\r\nNonMaxSuppressionV2,TensorArrayGatherV3,TensorArrayReadV3,Size,Where,Enter,TensorArraySizeV3,LoopCond,Exit,TensorArrayV3.\r\n\r\nThis is a ssd_mobilenet_v1_0.75_depth_coco model trained through tensorflow object detection API . It is performing well on tensorflow, but it contains ops not supported by TensorflowLite. I have tried several other models from the tensorflow model zoo, and they all have similar unsupported ops.\r\n\r\nDownloading link for the .pb file are :\r\n[http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03.tar.gz](url)", "comments": ["@harsh020goyal Request you to please refer the [link1](https://github.com/tensorflow/tensorflow/issues/25331), [link2](https://github.com/tensorflow/tensorflow/issues/27832). Please try if that helps and let us know how it progresses.", "@harsh020goyal Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 28699, "title": "[LITE] Reduce all op support", "body": "", "comments": ["@talumbau Could you please have a look at this PR. TIA", "@talumbau Can you please review this PR? Thanks!", "@siju-samuel Can you please resolve conflicts? Thanks!", "@siju-samuel gentle ping to resolve conflicts? Thanks!", "@talumbau I have updated the testcase based on your review comment. please help to review again.", "@talumbau Thanks a lot for review and pointing out the bug. I have updated the code. ", "@gbaned Could u please help to merge this PR before new conflicts comes. TIA", "Hi @siju-samuel, can you link to the models which require this operator? Thanks. The bar for adding new builtin ops has been raised, as we want to make the builtin op set a clear reference for the most important operators required for mobile execution.", "@siju-samuel Did you get a chance to look on jdduke's  comments? Please let us know on the update. Thanks!", "For inference purpose, currently I'm not using any model which have reduce-all op.", "@siju-samuel Could you please resolve the conflicts? Thanks!", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 28698, "title": "ConvLSTM2D - Include feedback loop from c_tm1 (carry state)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.13\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\nCurrently, the ConvLSTM2D layer does not include any feedback from C_tm1 (the carry state from the previous timestamp), see https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/keras/layers/convolutional_recurrent.py#L717\r\n\r\nThis stands in contrast to the description of the original paper which is cited as a reference: https://arxiv.org/abs/1506.04214v1 .\r\n\r\nThe missing feedback loop is already documented \"The current implementation does not include the feedback loop on the cells output\". However, is there a reason that this feedback was omitted? If not, I don't see why it should be different from the paper....\r\n\r\n**Will this change the current api? How?** \r\nWe might add a boolean flag to either incorporate such feedback or not.\r\n\r\n**Who will benefit with this feature?**\r\nAny researcher looking for an implementation exactly like the original paper.\r\n\r\n**Any Other info.**\r\n", "comments": ["P.s.: If I am not mistaken, it might not be straightforward, because we would need to store three different kernels:\r\n\r\nW_x = [W_xi, W_xf, W_xo, W_xc] = [filter_height, filter_width, num_batch, num_filter x 4]\r\nW_h = [W_hi, W_hf, W_ho, W_hc] = [filter_height, filter_width, num_filter, num_filter x 4]\r\nW_c = [W_ci, W_cf, W_co] = [num_batch, input_width, input_height, 3] - Must be same width and height as X_T and C_t due to Hadamard product\r\n\r\nHowever, there is only self.kernel and self.recurrent_kernel for storage available. Can I simply add a self.recurrent_kernel_2 or is there a better option? I could also concatenate W_x and W_h, but that seems like a hacky version...", "I think the code was originally from keras implementation in https://github.com/keras-team/keras/pull/3906. The context of not having feedback loop from previous state is kind of lost. ", "I took a look of the paper, it seems that the original paper was not using the vanilla version of LSTM, but a fully connected version. With that version, there is an extra weight for carry state of T - 1. I think we should have a new weight field for it, which only fuses 3 gates (input, forget, output, no carry gate). Probably we can call it recurrent_carry_kernel.", "Sorry for the very late reply. This feature is quite stale and we actually don't have much bandwidth to address it. I am closing this for now, and feel free to reopen it if this is feature request is critical."]}, {"number": 28697, "title": "tf.lite.TFLiteConverter error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow version (use command below): tf-nightly 1.14.1.dev20190513\r\n- Python version: 3.5.2\r\n\r\n**Describe the current behavior**\r\nI create a model from tensorflow, and it's a simple model.\r\n\r\nThe graph is as following:\r\n1. Placeholder: type=tf.float32, shape=[None, 640, 480, 1]  (format: NHWC)\r\n2. Conv2D: kernel_size = 5, filter_num = 32, stride=1, padding='same' ( tf.layer.con2d)\r\n3. relu (tf.nn.relu)\r\n\r\nI save this graph to test.pb. \r\n\r\nI want to convert this model to tflite model with int8, so i use tf.lite.converter to convert my test.pb to convert_test.tflite. \r\n\r\nBut something strange...\r\n\r\nWhen i use Netron to parse test.pb, the Conv2D layer is still Conv2D. But in my convert_test.tflite, the Conv2D layer is been changed to \"DepthwiseConv2D\".\r\n\r\nWhat happen...?\r\nAnd, what can i do?", "comments": ["@tsa10262 : That's expected behavior, based on your model ops, however it does not affect your output.\r\nPlease check for desired output, if it solves your purpose, then well and good. If my post helped resolve your issue, please close the thread, Thanks!", "@ANSHUMAN87: Thanks for answering. It's not affect the output. \r\nI'll close the thread.\r\n\r\nThanks a lot. ", "@ANSHUMAN87 I have a follow-up question about this issue. I had a same issue. What should I do to get \"Conv2D\" layer instead of \"DepthwiseConv2D\"? \r\n\r\nFirst Conv2D layer is converted to \"DepthwiseConv2D\", but second Conv2D layer is converted to \"Conv2D\". I should have get every conv2d layers converted to conv2d due to post-processing steps.\r\n\r\nmodel = keras.models.Sequential([\r\n    keras.layers.Conv2D(filters=6, kernel_size=3, activation='relu', input_shape=(28, 28, 1), name=\"input_node\"),\r\n    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\r\n    keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu'),\r\n    keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2),\r\n    keras.layers.Flatten(),\r\n    keras.layers.Dense(120, activation='relu'),\r\n    keras.layers.Dense(84, activation='relu'),\r\n    keras.layers.Dense(10, activation='softmax', name='output_node')\r\n])"]}, {"number": 28696, "title": "TensorFlow Install failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Na\r\n- TensorFlow installed from (source or binary): PyCharm automatic installation via PiP. \r\n- TensorFlow version: 2.0.0a0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: Pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: 8 GB Ram; INtel i7; NVIDIA GeForce 1060 6GB \r\n\r\n\r\n\r\n**Describe the problem**\r\nAfter Installation via PiP the Tensorflow library is not available. \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nUsed the following script: \r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nmnist = tf.keras.datasets.mnist\r\n\r\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train, epochs=5)\r\nmodel.evaluate(x_test, y_test)\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nC:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\Scripts\\python.exe C:/Users/bunga/PycharmProjects/bahndaten/bahndatenleser.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\bunga\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\bunga\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/bunga/PycharmProjects/bahndaten/bahndatenleser.py\", line 2, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\_api\\v2\\audio\\__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\bunga\\PycharmProjects\\bahndaten\\venv\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\bunga\\Anaconda3\\lib\\imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\bunga\\Anaconda3\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: Eine DLL-Initialisierungsroutine ist fehlgeschlagen.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n\r\n", "comments": ["@exactlysameguy Did you download and install CUDA and cuDNN? Can you please confirm? Thanks!", "I had downloaded and installed CUDA and cuDNN. I was able to solve the problem in the meantime. I changed the environment that pycharm uses to the environment that my console uses. \r\n\r\nThanks for your time !", "Closing since its resolved. Thanks! \r\n"]}, {"number": 28695, "title": "Always same random values in eager mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS X 10.14\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.1-dev20190513\r\n- Python version: 3.6.5\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): no\r\n- CUDA/cuDNN version: no\r\n- GPU model and memory: no\r\n\r\n**Describe the current behavior**\r\nRandom output should be different on each call.\r\nThat is true for graph mode, but not for eager one.\r\n\r\n**Describe the expected behavior**\r\nRandom output should be different on each call in both graph and eager modes.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\n# Graph mode\r\nimport tensorflow as tf\r\n\r\nx = tf.random.normal([5], seed=1)\r\n\r\nwith tf.Session() as sess:\r\n    for i in range(3):\r\n        print(sess.run(x))\r\n\r\n# Output\r\n# [-0.8113182   1.4845988   0.06532937 -2.4427042   0.0992484 ]\r\n# [-0.36332107 -0.07205155 -0.5527937   0.10289733 -0.39558855]\r\n# [-0.666205   -0.416783    1.8211031   0.680353   -0.26143482]\r\n```\r\n\r\n```python\r\n# Eager mode\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\nx = tf.random.normal([5], seed=1)\r\n\r\nfor i in range(3):\r\n    print(x.numpy())\r\n\r\n# Output\r\n# [-0.8113182   1.4845988   0.06532937 -2.4427042   0.0992484 ]\r\n# [-0.8113182   1.4845988   0.06532937 -2.4427042   0.0992484 ]\r\n# [-0.8113182   1.4845988   0.06532937 -2.4427042   0.0992484 ]\r\n```", "comments": ["When you use the seed parameter, It's the correct behaviour. Please remove the 'seed' argument to get different result each time.\r\nPlease close this thread.", "Same behaviour without seed.", "x.numpy() doesn't re-run the operation, it just copies the existing result to a numpy array. Try moving the normal() call into the loop.", "Ok, let me explain the source of my report.\r\nI've got a custom TF-based code with randomness inside. For simplicity in example below i replaced that code with  `tf.random.normal`. \r\nAnd i've got a unit test. It looks near like this:\r\n```Python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import test_util\r\n\r\n@test_util.run_all_in_graph_and_eager_modes\r\nclass DownSampleTest(tf.test.TestCase):\r\n    def testRand(self):\r\n        x = tf.random.normal([5], seed=1)\r\n        y1 = self.evaluate(x)\r\n        y2 = self.evaluate(x)\r\n        # ... check that y1 not equals y2\r\n```\r\nIn test i want to check that x changes on each call.\r\nThat is true for graph-mode and not true for eager mode.\r\n\r\nAs result, i can't test my code working as expected in both modes.", "I sometimes use functools.partial here. So x_func = functools.partial(tf.random.normal, args...), then run self.evaluate on x_func() a few times.", "Please have a look on @allenlavoie's suggestion and let us know if you are stuck. Thanks!", "It works as workaround, but makes testing for both graph and eager modes messy.", "Good to know the issue is resolved. Will close this for now. If you are stuck anywhere, feel free to open separate ticket. We will be happy to help. Thanks!"]}, {"number": 28694, "title": "tf.data.Dataset.flat_map shape assertion bug on Dataset with single tensor ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes which I have added as an example\r\n- Windows 7 Enterprise\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.0-9492-g2c319fb415 2.0.0-alpha0\r\n- Python version: 3.5\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory:  GeForce GTX 1050, 4GB\r\n\r\n\r\n**Describe the current behavior**\r\nApplying tf.data.Dataset.flat_map() to a dataset that just contains one tensor (in contrast to a tupled pair of tensors) leads to an incorrect error (from the example below):\r\nValueError: Tensor's shape (2,) is not compatible with supplied shape (None, 2)\r\nThe supplied shape seems to be incorrectly derived. This works without problem when the dataset contains two tensors (see example.)\r\n\r\n**Describe the expected behavior**\r\nThe tensor with Tensor's shape (2,) is the correct output when apllying flat_map to a dataset containing x sample, each consting of two values.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nmode = 1  # 1 for single tensor example, 0 for two tensor example\r\n\r\n\r\ndef get_sample(n_element):\r\n    if mode == 1:\r\n        samples = np.random.sample((n_element.numpy(), 2))\r\n    else:\r\n        samples = (np.random.sample((n_element.numpy(), 2)), np.random.sample((n_element.numpy(), 2)))\r\n\r\n    tensors = []\r\n    for t in samples:\r\n        tensors.append(tf.convert_to_tensor(t, dtype=tf.float64))\r\n    return tensors\r\n\r\n\r\ndef read_wrapper(number_ds):\r\n    if mode == 1:\r\n        dtypes = [tf.float64]\r\n    else:\r\n        dtypes = [tf.float64, tf.float64]\r\n\r\n    ex = tf.py_function(get_sample, [number_ds], dtypes)\r\n\r\n    tensors = []\r\n    for t in ex:\r\n        t.set_shape([None, 2])\r\n        tensors.append(t)\r\n\r\n    return tensors\r\n\r\n\r\ndef get_dataset(n_element_list):\r\n    print('Initial List: ', n_element_list)\r\n    ds = tf.data.Dataset.from_tensor_slices(n_element_list)\r\n    print('After Slicing: ', ds)\r\n    ds = ds.map(map_func=read_wrapper)\r\n    print('After Mapping: ', ds)\r\n    if mode == 1:\r\n        ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x))\r\n    else:\r\n        ds = ds.flat_map(lambda e, l: tf.data.Dataset.zip((\r\n            tf.data.Dataset.from_tensor_slices(e),\r\n            tf.data.Dataset.from_tensor_slices(l))))\r\n\r\n    print('After Flattening: ', ds)\r\n    return ds\r\n\r\n\r\nif __name__ == '__main__':\r\n    np.random.seed(42)\r\n    n_e_list = np.random.randint(2, 4, size=3)\r\n    n_e_list = tf.squeeze(tf.convert_to_tensor(n_e_list))\r\n    data_set = get_dataset(n_e_list)\r\n    data_set = data_set.prefetch(1)\r\n\r\n    if mode == 1:\r\n        for x_train in data_set:\r\n            print('Finished Batch: ', x_train)\r\n    else:\r\n        for x_train, y_train in data_set:\r\n            print('Finished Batch: ', x_train, y_train)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nwith mode= 1  The erros occures. Output:\r\n\r\n2019-05-14 10:37:23.354903: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-05-14 10:37:23.354903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-05-14 10:37:23.554905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.43\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.86GiB\r\n2019-05-14 10:37:23.554905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-05-14 10:37:24.124911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-14 10:37:24.124911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 \r\n2019-05-14 10:37:24.124911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N \r\n2019-05-14 10:37:24.124911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3581 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nInitial List:  tf.Tensor([2 3 2], shape=(3,), dtype=int32)\r\nAfter Slicing:  <TensorSliceDataset shapes: (), types: tf.int32>\r\nAfter Mapping:  <MapDataset shapes: ((None, 2),), types: (tf.float64,)>\r\nTraceback (most recent call last):\r\n  File \"DirectoryName/Documents/Repositories/LiverSegmentation/scripts/test_dataset.py\", line 60, in <module>\r\n    for x_train in data_set:\r\n  File \"DirectoryName\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 557, in __next__\r\n    return self.next()\r\n  File \"DirectoryName\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 586, in next\r\nAfter Flattening:  <MapDataset shapes: ((None, 2),), types: (tf.float64,)>\r\n[TensorShape([None, 2])]\r\n    return self._next_internal()\r\n  File \"DirectoryName\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 580, in _next_internal\r\n    return self._structure._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n  File \"DirectoryName\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\", line 393, in _from_compatible_tensor_list\r\n    flat_ret.append(structure._from_compatible_tensor_list(sub_value))\r\n  File \"DirectoryName\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py\", line 473, in _from_compatible_tensor_list\r\n    flat_value[0].set_shape(self._shape)\r\n  File \"DirectoryName\\AppData\\Local\\Programs\\Python\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 928, in set_shape\r\n    (self.shape, shape))\r\nValueError: Tensor's shape (2,) is not compatible with supplied shape (None, 2)\r\n\r\nProcess finished with exit code 1\r\n\r\nwith mode =0 everything runs correctly:\r\n\r\n2019-05-14 10:36:57.849895: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\r\n2019-05-14 10:36:57.859895: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll\r\n2019-05-14 10:36:58.041899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1467] Found device 0 with properties: \r\nname: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.43\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.86GiB\r\n2019-05-14 10:36:58.041899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1546] Adding visible gpu devices: 0\r\n2019-05-14 10:36:58.621912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1015] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-14 10:36:58.621912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1021]      0 \r\n2019-05-14 10:36:58.621912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1034] 0:   N \r\n2019-05-14 10:36:58.621912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1149] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3581 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\nInitial List:  tf.Tensor([2 3 2], shape=(3,), dtype=int32)\r\nAfter Slicing:  <TensorSliceDataset shapes: (), types: tf.int32>\r\nAfter Mapping:  <MapDataset shapes: ((None, 2), (None, 2)), types: (tf.float64, tf.float64)>\r\nAfter Flattening:  <FlatMapDataset shapes: ((2,), (2,)), types: (tf.float64, tf.float64)>\r\n[TensorShape([2]), TensorShape([2])]\r\nFinished Batch:  tf.Tensor([0.18343479 0.779691  ], shape=(2,), dtype=float64) tf.Tensor([0.09997492 0.45924889], shape=(2,), dtype=float64)\r\nFinished Batch:  tf.Tensor([0.59685016 0.44583275], shape=(2,), dtype=float64) tf.Tensor([0.33370861 0.14286682], shape=(2,), dtype=float64)\r\nFinished Batch:  tf.Tensor([0.65088847 0.05641158], shape=(2,), dtype=float64) tf.Tensor([0.61748151 0.61165316], shape=(2,), dtype=float64)\r\nFinished Batch:  tf.Tensor([0.72199877 0.93855271], shape=(2,), dtype=float64) tf.Tensor([0.00706631 0.02306243], shape=(2,), dtype=float64)\r\nFinished Batch:  tf.Tensor([7.78765841e-04 9.92211559e-01], shape=(2,), dtype=float64) tf.Tensor([0.52477466 0.39986097], shape=(2,), dtype=float64)\r\nFinished Batch:  tf.Tensor([0.04666566 0.97375552], shape=(2,), dtype=float64) tf.Tensor([0.61838601 0.38246199], shape=(2,), dtype=float64)\r\nFinished Batch:  tf.Tensor([0.23277134 0.09060643], shape=(2,), dtype=float64) tf.Tensor([0.98323089 0.46676289], shape=(2,), dtype=float64)\r\n\r\nProcess finished with exit code 0", "comments": ["@alenaschnurr Able to reproduce the issue with provided code.\r\n\r\nValueError: Tensor's shape (2,) is not compatible with supplied shape (None, 2)", "@alenaschnurr I changed shape in one function as follows. I don't see any error for both mode=1 or 2. Please let me know what you think? Thanks!\r\n\r\n```\r\ndef read_wrapper(number_ds):\r\n    if mode == 1:\r\n        dtypes = [tf.float64]\r\n    else:\r\n        dtypes = [tf.float64, tf.float64]\r\n\r\n    ex = tf.py_function(get_sample, [number_ds], dtypes)\r\n\r\n    tensors = []\r\n    for t in ex:\r\n      if mode == 1:\r\n        t.set_shape([2])\r\n      else:\r\n        t.set_shape([None, 2])\r\n      tensors.append(t)\r\n\r\n    return tensors\r\n```", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28694\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28694\">No</a>\n"]}, {"number": 28693, "title": "[Train on official Docker image & custom TF build (no AVX)] failed to query event: CUDA_ERROR_LAUNCH_FAILED", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): used https://github.com/minimaxir/gpt-2-simple for finetuning\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 & Docker\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): custom build TF (without AVX) from https://github.com/yaroslavvb/tensorflow-community-wheels/issues/109\r\n- TensorFlow version (use command below): b'v1.13.1-0-g6612da8' 1.13.1\r\n- Python version: Python 3.5.2\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): Same as docker `tensorflow/tensorflow:latest-gpu-py3` & `tensorflow/tensorflow:devel-gpu-py3` (error reproduce in both)\r\n- CUDA/cuDNN version: 10.0 / 7.4.1.5-1 ( https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile )\r\n- GPU model and memory: GeForce GTX 1080 Ti (11 Gb)\r\n\r\n**Describe the current behavior**\r\n\r\n> root@63f592f02a0e:/gpt2# python imdb_reviews.py\r\n> ...\r\n> 2019-05-14 07:05:22.669722: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n> 2019-05-14 07:05:22.669812: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n> Aborted (core dumped)\r\n\r\n\r\n**Describe the expected behavior**\r\nTrain process (tested in google colab)...\r\n\r\n\r\n**Code to reproduce the issue**\r\n```bash\r\n# tested on devel-gpu-py3 (where tf wheel builded) and latest-gpu-py3\r\ndocker pull tensorflow/tensorflow:latest-gpu-py3\r\n\r\n# deep into docker image\r\ndocker run --runtime=nvidia -it  -v ~/projects/gpt2-simple:/gpt2 tensorflow/tensorflow:latest-gpu-py3 bash\r\n```\r\n\r\nIn Docker:\r\n```bash\r\npip uninstall tensorflow-gpu\r\n\r\ncd /gpt2\r\n# from https://github.com/yaroslavvb/tensorflow-community-wheels/issues/109\r\npip install tensorflow-1.13.1-cp35-cp35m-linux_x86_64.whl\r\n\r\n# install gpt2-simple\r\npip install gpt_2_simple\r\n\r\n\r\n# from https://gist.github.com/saippuakauppias/4f41ce1072a04588a2bab7dae00f9bb7\r\npython imdb_reviews.py\r\n```\r\n\r\n**Other info / logs**\r\n1. [tf_env.txt](https://github.com/tensorflow/tensorflow/files/3176534/tf_env.txt) attached.\r\n2. Its error looks like https://github.com/tensorflow/tensorflow/issues/22477 but I dont understand where solution in that issue.\r\n3. docker run without `-u $(id -u):$(id -g)` because with this I cant uninstall tensorflow:\r\n\r\n<details>\r\n  <summary>Example</summary>\r\n\r\n```\r\ndocker run --runtime=nvidia -u $(id -u):$(id -g) -v ~/projects/gpt2-simple:/gpt2 -it tensorflow/tensorflow:latest-gpu-py3 bash\r\n\r\nYou are running this container as user with ID 1000 and group 1000,\r\nwhich should map to the ID and group for your user on the Docker host. Great!\r\n\r\ntf-docker / > pip uninstall tensorflow-gpu\r\nWARNING: The directory '/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\r\nUninstalling tensorflow-gpu-1.13.1:\r\n  Would remove:\r\n    /usr/local/bin/freeze_graph\r\n    /usr/local/bin/saved_model_cli\r\n    /usr/local/bin/tensorboard\r\n    /usr/local/bin/tf_upgrade_v2\r\n    /usr/local/bin/tflite_convert\r\n    /usr/local/bin/toco\r\n    /usr/local/bin/toco_from_protos\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow/*\r\n    /usr/local/lib/python3.5/dist-packages/tensorflow_gpu-1.13.1.dist-info/*\r\nProceed (y/n)? y\r\nERROR: Exception:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/shutil.py\", line 538, in move\r\n    os.rename(src, real_dst)\r\nPermissionError: [Errno 13] Permission denied: '/usr/local/bin/freeze_graph' -> '/tmp/pip-uninstall-n9t_qerr/freeze_graph'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/pip/_internal/cli/base_command.py\", line 178, in main\r\n    status = self.run(options, args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/pip/_internal/commands/uninstall.py\", line 75, in run\r\n    auto_confirm=options.yes, verbose=self.verbosity > 0,\r\n  File \"/usr/local/lib/python3.5/dist-packages/pip/_internal/req/req_install.py\", line 825, in uninstall\r\n    uninstalled_pathset.remove(auto_confirm, verbose)\r\n  File \"/usr/local/lib/python3.5/dist-packages/pip/_internal/req/req_uninstall.py\", line 388, in remove\r\n    moved.stash(path)\r\n  File \"/usr/local/lib/python3.5/dist-packages/pip/_internal/req/req_uninstall.py\", line 277, in stash\r\n    renames(path, new_path)\r\n  File \"/usr/local/lib/python3.5/dist-packages/pip/_internal/utils/misc.py\", line 305, in renames\r\n    shutil.move(old, new)\r\n  File \"/usr/lib/python3.5/shutil.py\", line 553, in move\r\n    os.unlink(src)\r\nPermissionError: [Errno 13] Permission denied: '/usr/local/bin/freeze_graph'\r\n```\r\n</details>\r\n\r\n4. Tensorflow debugger with mnist works fine and use GPU (I see it in nvidia-smi).\r\n<details>\r\n  <summary>Log</summary>\r\n\r\n```\r\nroot@4697d32838f4:/gpt2# python -m tensorflow.python.debug.examples.debug_mnist\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/debug/examples/debug_mnist.py:46: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/mnist_data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting /tmp/mnist_data/train-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.one_hot on tensors.\r\nExtracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n2019-05-14 17:07:28.376774: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800000000 Hz\r\n2019-05-14 17:07:28.378926: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x6115c40 executing computations on platform Host. Devices:\r\n2019-05-14 17:07:28.378987: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-05-14 17:07:28.598760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-05-14 17:07:28.601785: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x61c1dd0 executing computations on platform CUDA. Devices:\r\n2019-05-14 17:07:28.601829: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-05-14 17:07:28.602453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\r\npciBusID: 0000:0b:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.76GiB\r\n2019-05-14 17:07:28.602502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\r\n2019-05-14 17:07:28.605463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-05-14 17:07:28.605502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \r\n2019-05-14 17:07:28.605525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \r\n2019-05-14 17:07:28.605978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0b:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\n2019-05-14 17:07:29.756412: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\nAccuracy at step 0: 0.1094\r\nAccuracy at step 1: 0.098\r\nAccuracy at step 2: 0.098\r\nAccuracy at step 3: 0.098\r\nAccuracy at step 4: 0.098\r\nAccuracy at step 5: 0.098\r\nAccuracy at step 6: 0.098\r\nAccuracy at step 7: 0.098\r\nAccuracy at step 8: 0.098\r\nAccuracy at step 9: 0.098\r\n```\r\n</details>", "comments": ["I also builded tensorflow v1.12.2 and v.1.11.0 (CUDA 9 / cuDNN 7 / Bazel 0.15.0) from old official dockerfile + some things from dockerfile v1.13.1 (https://gist.github.com/saippuakauppias/eac64a0e09ac9e0e45d7a590ed76c87e ) and I get same results:\r\n```\r\nroot@fe4fdf4cda67:/my-devel/gpt2-simple# python imdb_reviews.py\r\nLoading checkpoint models/345M/model.ckpt\r\n2019-05-15 07:00:34.445685: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-05-15 07:00:34.445688: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:706] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2019-05-15 07:00:34.445785: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n2019-05-15 07:00:34.445793: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:706] failed to record completion event; therefore, failed to create inter-stream dependency\r\nAborted (core dumped)\r\n```\r\n\r\nMaybe problem in my Ubuntu? How to find the problem?", "@saippuakauppias Make sure that LD_LIBRARY_PATH is set up correctly. Thanks!", "@gadagashwini, what is the correct path to be?\r\n\r\nI check `LD_LIBRARY_PATH` in docker image `tensorflow/tensorflow:1.13.1-gpu-py3`:\r\n```\r\nroot@c119f9a4779c:/gpt2# echo $LD_LIBRARY_PATH \r\n/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n```\r\n\r\nAnd with this env I get same error.\r\n\r\nI changed `LD_LIBRARY_PATH` to:\r\n```\r\n/usr/local/cuda-10.0/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\n```\r\n\r\nAnd again get error in start post...\r\n\r\nI checked again `tf_env_collect.sh` and get same results (as in first post in attachment).", "@ymodak , @gadagashwini any suggestions for me?\r\nPlease...", "@saippuakauppias Please follow these steps to install Tensorflow-gpu using docker\r\n$sudo docker pull tensorflow/tensorflow:latest-gpu\r\n$sudo docker run -it tensorflow/tensorflow:latest-gpu\r\n$#python\r\n>>import tensorflow  \r\n\r\nPlease let us know how it progresses. Thanks! \r\n\r\n\r\n", "``` bash\r\n$ sudo docker run -it tensorflow/tensorflow:latest-gpu\r\n$ # python\r\n```\r\n\r\n```\r\nPython 2.7.12 (default, Nov 12 2018, 14:36:49) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\nImportError: libcuda.so.1: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```\r\n\r\nand:\r\n``` bash\r\n$ sudo docker run --runtime=nvidia -it tensorflow/tensorflow:latest-gpu\r\n$ # python\r\n```\r\n\r\n```\r\nPython 2.7.12 (default, Nov 12 2018, 14:36:49) \r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\n2019-05-31 18:24:40.299660: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.\r\nAborted (core dumped)\r\n\r\n```", "Did you [install nvidia-docker](https://www.tensorflow.org/install/docker#tensorflow_docker_requirements) for gpu support?", "Yes, of course.", "@saippuakauppias, I'm sorry, but we don't provide support for community-built packages.\r\n\r\nIIRC, all of our official packages (and therefore the packages provided in our Docker images) depend on a CPU with AVX instructions to load, which is why your later tests have failed.", "Can anyone look at my tf_env.txt and say if everything is normal there? I looked at similar problems with tf_env.txt file and there were radically different files.\r\n\r\nI suspect that I have a problem with Cuda, and not with custom build tensorflow.", "@angersson @ymodak @gadagashwini can you check my tf_env.txt from start post? Please", "Same problem on v1.14.0 :(\r\n```\r\nroot@f58a4b0f357f:/gpt2# python imdb_reviews.py\r\nFetching checkpoint: 1.05Mit [00:00, 574Mit/s]                                                      \r\nFetching encoder.json: 1.05Mit [00:00, 4.81Mit/s]                                                   \r\nFetching hparams.json: 1.05Mit [00:00, 777Mit/s]                                                    \r\nFetching model.ckpt.data-00000-of-00001:  46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f         | 228M/498M [00:20<00:26, 10.1Mit/s]\r\nFetching model.ckpt.data-00000-of-00001: 498Mit [00:44, 11.1Mit/s]                                  \r\nFetching model.ckpt.index: 1.05Mit [00:00, 524Mit/s]                                                \r\nFetching model.ckpt.meta: 1.05Mit [00:00, 8.16Mit/s]                                                \r\nFetching vocab.bpe: 1.05Mit [00:00, 6.46Mit/s]                                                      \r\n2019-08-07 20:01:28.881234: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800000000 Hz\r\n2019-08-07 20:01:28.883414: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x508f1f0 executing computations on platform Host. Devices:\r\n2019-08-07 20:01:28.883468: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\r\n2019-08-07 20:01:28.887221: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\r\n2019-08-07 20:01:29.334170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-07 20:01:29.336751: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x536d0e0 executing computations on platform CUDA. Devices:\r\n2019-08-07 20:01:29.336814: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1\r\n2019-08-07 20:01:29.337238: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-07 20:01:29.339789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6575\r\npciBusID: 0000:0b:00.0\r\n2019-08-07 20:01:29.340142: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-07 20:01:29.342040: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n2019-08-07 20:01:29.343430: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\r\n2019-08-07 20:01:29.343813: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\r\n2019-08-07 20:01:29.345995: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\r\n2019-08-07 20:01:29.347726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\r\n2019-08-07 20:01:29.352638: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\r\n2019-08-07 20:01:29.352833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-07 20:01:29.355504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-07 20:01:29.358011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\r\n2019-08-07 20:01:29.358084: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\r\n2019-08-07 20:01:29.361205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2019-08-07 20:01:29.361231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \r\n2019-08-07 20:01:29.361254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \r\n2019-08-07 20:01:29.361753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-07 20:01:29.364404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2019-08-07 20:01:29.366944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10470 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0b:00.0, compute capability: 6.1)\r\n\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0807 20:01:36.900164 139762445911872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse tf.where in 2.0, which has the same broadcast rule as np.where\r\n\r\nLoading checkpoint models/117M/model.ckpt\r\nW0807 20:01:56.259600 139762445911872 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse standard file APIs to check for files with this prefix.\r\n2019-08-07 20:01:57.971873: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure\r\n2019-08-07 20:01:57.971963: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1\r\nAborted (core dumped)\r\n\r\n```", "I am closing this issue, since we don't support non-official packages.", "The problem was in the video card. There were memory errors (can be verified through https://github.com/ForkLab/cuda_memtest or OCCT in Windows).\r\nSolution: replace the video card.", "I am facing the same issue (Windows, tensorflow 2.1):\r\nRandom `Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure` errors when training LSTM networks, causing the kernel to fail.\r\n\r\nWhen replacing the LSTM layers with GRU layers, everything works like a charm.\r\nOCCT did not reveal any issues. Other stress tests worked perfectly.\r\n\r\nAny hints/suggestions?\r\n"]}, {"number": 28692, "title": "empty array: [] on prediction (CMLE)", "body": "I have implemented a custom tfx example after working trough the tfx taxi example.\r\n\r\nThe model that is trained in the tfx pipeline is a canned estimator estimator: DNNRegressor.\r\n\r\ninside model_fn of custom estimator\r\n```python\r\ntags = tf.feature_column.numeric_column(\r\n        key=_transformed_name(_FEATURE_KEY), shape=[500], dtype=tf.int64)\r\n\r\n    return tf.estimator.DNNRegressor(\r\n        config=config,\r\n        feature_columns=[tags],\r\n        hidden_units=[25],\r\n        activation_fn=tf.nn.relu\r\n    )\r\n```\r\n\r\ndefinition of serving receiver\r\n\r\n```python\r\ndef _example_serving_receiver_fn(transform_output, schema):\r\n    \"\"\"Build the serving in inputs.\r\n    Args:\r\n        transform_output: directory in which the tf-transform model was written\r\n        during the preprocessing step.\r\n        schema: the schema of the input data.\r\n    Returns:\r\n        Tensorflow graph which parses examples, applying tf-transform to them.\r\n    \"\"\"\r\n\r\n    raw_feature_spec = _get_raw_feature_spec(schema)\r\n    raw_feature_spec.pop(_LABEL_KEY)\r\n\r\n  # if only this function wasn't 100% black-box\r\n    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\r\n        raw_feature_spec, default_batch_size=None)\r\n\r\n    serving_input_receiver = raw_input_fn()\r\n\r\n    _, transformed_features = (\r\n        saved_transform_io.partially_apply_saved_transform(\r\n            os.path.join(transform_output, transform_fn_io.TRANSFORM_FN_DIR),\r\n            serving_input_receiver.features))\r\n\r\n    return tf.estimator.export.ServingInputReceiver(\r\n        transformed_features, serving_input_receiver.receiver_tensors)\r\n```\r\n\r\nactually calling serving receiver\r\n\r\n```python\r\n    def serving_receiver_fn(): return _example_serving_receiver_fn(  # pylint: disable=g-long-lambda\r\n            hparams.transform_output, schema)\r\n\r\n    exporter = tf.estimator.FinalExporter(\r\n        'stackoverflow-tfx', serving_receiver_fn)\r\n    eval_spec = tf.estimator.EvalSpec(\r\n        eval_input_fn,\r\n        steps=hparams.eval_steps,\r\n        exporters=[exporter],\r\n        name='stackoverflow-tfx-eval')\r\n```\r\n\r\nThe custom implementation of the tfx pipeline runs without problems all the way.\r\nThe ml engine model & version exist, the saved model.pb exists.\r\n\r\nBut when I try to make a prediction to the ml engine model it returns an empty array.\r\nlike: {'predictions': []}\r\n\r\nI used to make predictions via code but found that the UI also works the same\r\n\r\n![image](https://user-images.githubusercontent.com/26407191/57837718-a14cbe00-77c3-11e9-9b7f-3e0540348ac4.png)\r\n\r\n\r\nWhere do I start debugging or does anyone know what could be going on?\r\n\r\nTraining on ml engine with python version 2.7\r\nml engine runtimeVersion 1.12 for both training and serving (assuming that means tensrflow 1.12)", "comments": ["Looks like it is related to TFX. We can open an issue on this [repository](https://github.com/tensorflow/tfx/issues) for better help on the same. Thanks!", "@achandraa someone in TFX asked me to post the issue here.\r\n[Original issue](https://github.com/tensorflow/tfx/issues/114#issue-443364979)", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. If possible can you provide the code snippet so that we can replicate the issue. You can also refer the [template](https://github.com/tensorflow/tensorflow/issues/new/choose). Thanks!\r\n", "Okay, so I turns out that even with a canned DNNRegressor estimator the exact same issue happens.\r\n\r\nThis is where I expect things are going wrong: \r\n\r\n```python\r\ndef _example_serving_receiver_fn(transform_output, schema):\r\n    \"\"\"Build the serving in inputs.\r\n    Args:\r\n        transform_output: directory in which the tf-transform model was written\r\n        during the preprocessing step.\r\n        schema: the schema of the input data.\r\n    Returns:\r\n        Tensorflow graph which parses examples, applying tf-transform to them.\r\n    \"\"\"\r\n\r\n    raw_feature_spec = _get_raw_feature_spec(schema)\r\n    raw_feature_spec.pop(_LABEL_KEY)\r\n\r\n    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\r\n        raw_feature_spec, default_batch_size=None)\r\n\r\n    serving_input_receiver = raw_input_fn()\r\n\r\n    _, transformed_features = (\r\n        saved_transform_io.partially_apply_saved_transform(\r\n            os.path.join(transform_output, transform_fn_io.TRANSFORM_FN_DIR),\r\n            serving_input_receiver.features))\r\n\r\n    return tf.estimator.export.ServingInputReceiver(\r\n        transformed_features, serving_input_receiver.receiver_tensors)\r\n```\r\n\r\nThis code is supposed to build a serving input receiver that takes in raw data (before tf transform is applied), transforms that data accordingly and finally passes that data to the estimator\r\n\r\nMy tf transform uses compute_and_apply_vocabulary so I am stuck using this implementation instead of one with tf.placeholders.\r\n\r\nestimator looks like this\r\n```python\r\ntags = tf.feature_column.numeric_column(\r\n        key=_transformed_name(_FEATURE_KEY), shape=[500], dtype=tf.int64)\r\n\r\n    return tf.estimator.DNNRegressor(\r\n        config=config,\r\n        feature_columns=[tags],\r\n        hidden_units=[25],\r\n        activation_fn=tf.nn.relu\r\n    )\r\n```\r\ninput to the model is a multi one hot encoded array of size 500 each representing one categorical feature that has 500 options, output is a single number\r\n\r\nWhen I open the ml engine logs where the serving graph is exported it logs this:\r\n```\r\nSignatures INCLUDED in export for Regress: ['serving_default', 'regression']\r\n```\r\n'regression' is probably created by the estimator but why are there 2 signatureDefs there?\r\ndoes that mean there are 2 ways of predicting against the model and i'm using the wrong one?\r\n\r\nAlso I am continuously trying to fix this bug so the error's and logs may not be from the exact same trials. Stopping and waiting 24h is not an option.", "![image](https://user-images.githubusercontent.com/26407191/57836727-a14bbe80-77c1-11e9-9e3b-e6a6d13cd490.png)\r\n\r\nI enabled logging for both the example model and my model and both log status 200 on prediction. Which is technically true, since it does return something.\r\n\r\nI expect that the estimator is not correctly receiving the inputs from the prediction request.\r\nWhat is weird is that it returns like {'predictions': []} instead of {'predictions': {'some_key': []}}\r\n\r\nIs there a way to log values in the serving_receiver_fn()?\r\n\r\nIf you need any additional logs please ask", "I'm sorry for spreading all this information in many comments but this issue involves way too much code that could be incorrect.\r\n\r\nHere is my preprocessing function used by tf transform (supposed to be applied during training AND inference)\r\n\r\n```python\r\ndef preprocessing_fn(inputs):\r\n    \"\"\"tf.transform's callback function for preprocessing inputs.\r\n    Args:\r\n        inputs: map from feature keys to raw not-yet-transformed features.\r\n    Returns:\r\n        Map from string feature key to transformed feature operations.\r\n    \"\"\"\r\n    outputs = {}\r\n\r\n    dense_features = tf.reshape(\r\n        tf.sparse_tensor_to_dense(\r\n            inputs[_FEATURE_KEY],\r\n            default_value=''\r\n        ),\r\n        [-1]\r\n    )\r\n\r\n    dense_labels = tf.reshape(\r\n        tf.sparse_tensor_to_dense(\r\n            inputs[_LABEL_KEY],\r\n            default_value=-1\r\n        ),\r\n        [-1]\r\n    )\r\n\r\n    sparse_split = tf.string_split(dense_features, delimiter='|')\r\n\r\n    sparse_indexes = tft.compute_and_apply_vocabulary(\r\n        sparse_split,\r\n        default_value=-1,  # -1 will cause the one_hot_encoding to return all 0 array\r\n        top_k=500\r\n    )\r\n\r\n    dense_indexes = tf.sparse_tensor_to_dense(\r\n        sparse_indexes,\r\n        default_value=-1\r\n    )\r\n\r\n    single_max_index = tf.reduce_max(\r\n        dense_indexes\r\n    )\r\n\r\n    one_hot = tf.one_hot(\r\n        indices=dense_indexes,\r\n        depth=500,\r\n        on_value=1,\r\n        off_value=0)\r\n\r\n    multi_one_hot = tf.reduce_sum(\r\n        one_hot,\r\n        1\r\n    )\r\n\r\n    int64_multi_one_hot = tf.cast(\r\n        multi_one_hot,\r\n        dtype=tf.int64\r\n    )\r\n\r\n    outputs[_transformed_name(_FEATURE_KEY)] = int64_multi_one_hot\r\n    outputs[_transformed_name(_LABEL_KEY)] = dense_labels\r\n\r\n    return outputs\r\n```", "The problem could also be that I am calling the prediction incorrectly.\r\n\r\nIn the taxi example they use a function called _fill_in_missing to replace all empty tensors during serving with zeros or empty strings. What is the correct json format to call my model?\r\n\r\n```python\r\ndef _fill_in_missing(x):\r\n  \"\"\"Replace missing values in a SparseTensor.\r\n  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\r\n  Args:\r\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\r\n      in the second dimension.\r\n  Returns:\r\n    A rank 1 tensor where missing values of `x` have been filled in.\r\n  \"\"\"\r\n  default_value = '' if x.dtype == tf.string else 0\r\n  return tf.squeeze(\r\n      tf.sparse.to_dense(\r\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\r\n          default_value),\r\n      axis=1)\r\n```\r\n\r\n```python\r\ndef preprocessing_fn(inputs):\r\n  \"\"\"tf.transform's callback function for preprocessing inputs.\r\n  Args:\r\n    inputs: map from feature keys to raw not-yet-transformed features.\r\n  Returns:\r\n    Map from string feature key to transformed feature operations.\r\n  \"\"\"\r\n  outputs = {}\r\n  for key in _DENSE_FLOAT_FEATURE_KEYS:\r\n    # Preserve this feature as a dense float, setting nan's to the mean.\r\n    outputs[_transformed_name(key)] = tft.scale_to_z_score(\r\n        _fill_in_missing(inputs[key]))\r\n\r\n  for key in _VOCAB_FEATURE_KEYS:\r\n    # Build a vocabulary for this feature.\r\n    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\r\n        _fill_in_missing(inputs[key]),\r\n        top_k=_VOCAB_SIZE,\r\n        num_oov_buckets=_OOV_SIZE)\r\n\r\n  for key in _BUCKET_FEATURE_KEYS:\r\n    outputs[_transformed_name(key)] = tft.bucketize(\r\n        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\r\n\r\n  for key in _CATEGORICAL_FEATURE_KEYS:\r\n    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])\r\n\r\n  # Was this passenger a big tipper?\r\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\r\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\r\n  outputs[_transformed_name(_LABEL_KEY)] = tf.where(\r\n      tf.is_nan(taxi_fare),\r\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\r\n      # Test if the tip was > 20% of the fare.\r\n      tf.cast(\r\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\r\n\r\n  return outputs\r\n\r\n```", "I found the critical code that was causing the empty return. It was the serving receiver function that is responsible for the signatureDef in the exported model.\r\n```python\r\ndef _example_serving_receiver_fn(transform_output, schema):\r\n    \"\"\"Build the serving in inputs.\r\n    Args:\r\n        transform_output: directory in which the tf-transform model was written\r\n        during the preprocessing step.\r\n        schema: the schema of the input data.\r\n    Returns:\r\n        Tensorflow graph which parses examples, applying tf-transform to them.\r\n    \"\"\"\r\n\r\n    raw_feature_spec = _get_raw_feature_spec(schema)\r\n    raw_feature_spec.pop(_LABEL_KEY)\r\n\r\n    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\r\n        raw_feature_spec, default_batch_size=None)\r\n\r\n    serving_input_receiver = raw_input_fn()\r\n\r\n    _, transformed_features = (\r\n        saved_transform_io.partially_apply_saved_transform(\r\n            os.path.join(transform_output, transform_fn_io.TRANSFORM_FN_DIR),\r\n            serving_input_receiver.features))\r\n\r\n    return tf.estimator.export.ServingInputReceiver(\r\n        transformed_features, serving_input_receiver.receiver_tensors)\r\n```\r\n\r\nTo get around this I implemented a serving receiver the regular way, without using tf transform.\r\nI still don't know which one of the functions in this snippet is the cause.\r\n\r\nI replaced it with:\r\n```python\r\ndef _example_serving_receiver_fn():\r\n    feature_placeholders = {_FEATURE_KEY:\r\n                            tf.placeholder(tf.int64, shape=(None, 5))}\r\n\r\n    one_hot = tf.one_hot(\r\n        indices=feature_placeholders[_FEATURE_KEY],\r\n        depth=500,\r\n        on_value=1,\r\n        off_value=0)\r\n\r\n    multi_one_hot = tf.reduce_sum(\r\n        one_hot,\r\n        1\r\n    )\r\n\r\n    int64_multi_one_hot = tf.cast(\r\n        multi_one_hot,\r\n        dtype=tf.int64\r\n    )\r\n\r\n    reshaped_multi_hot = tf.reshape(\r\n        int64_multi_one_hot,\r\n        [-1, 500]\r\n    )\r\n\r\n    features = {\r\n        _transformed_name(_FEATURE_KEY): reshaped_multi_hot\r\n    }\r\n\r\n    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\r\n```\r\n\r\nIn theory the first snippet is supposed to result in the same as the second by reusing the saved transform function. My main problem was that every function in the first snippet was a black-box and I have no idea whats happening or where its going wrong.", "I am having same issue what was the solution to above problem?"]}, {"number": 28691, "title": "Error installing tf-nightly-2.0-preview with pip", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: latest tf-nightly-2.0-preview\r\n- Python version: Python 3.6.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nOfficial colab link for tensorboard 2.0 demo is able to reproduce the bug:\r\n\r\nhttps://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/r2/get_started.ipynb\r\n\r\n```\r\n$ pip install -q tf-nightly-2.0-preview\r\n\r\nERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\r\n```", "comments": ["@wxdao When I ran the command pip install -q tf-nightly-2.0-preview , I get the error. And if I remove -q then the error is not shown.", "It does work with the method you mentioned, i.e without the -q parameter. Thank you.\r\n\r\nBut why? It's so frustrating.", "@wxdao Where did you see that `-q` command? In [TF website](https://www.tensorflow.org/install/pip), it was not mentioned with `-q`. Thanks!", "> @wxdao Where did you see that `-q` command? In [TF website](https://www.tensorflow.org/install/pip), it was not mentioned with `-q`. Thanks!\r\n\r\nIt's in every official tutorial colab notebook, like [this one](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/r2/get_started.ipynb), so I thought I was supposed to add -q too.", "`-q` is a `pip` flag:\r\n\r\n```\r\n  -q, --quiet                 Give less output. Option is additive, and can be\r\n                              used up to 3 times (corresponding to WARNING,\r\n                              ERROR, and CRITICAL logging levels).\r\n```\r\n\r\nThe issue here is not caused by TensorFlow. In the python environment you're using (virtualenv? global install?) you already have an incompatible version of `thinc`. You can try uninstalling and reinstalling that with the required version number", "Closing this out since I understand it to be resolved, but please let me know if I'm mistaken. Thanks!"]}]