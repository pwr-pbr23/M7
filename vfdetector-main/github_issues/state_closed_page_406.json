[{"number": 41748, "title": "Tensorflow build from source doesn\u2019t see GPUs gets \u201c failed call to cuInit: UNKNOWN ERROR (-1)\u201d", "body": "System information:\r\nTensorflow 2.4.0 (built from tensorflow/tensorflow:develgpu and then followed Build from source on 24Jul2020)\r\nNvidia-cuda 11.0\r\nNvidia-driver-450\r\n\r\nThis from nvidia-smi \r\n\r\nnvidia-smi\r\nSun Jul 26 20:50:35 2020       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 450.57       Driver Version: 450.57       CUDA Version: 11.0     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1070    On   | 00000000:01:00.0 Off |                  N/A |\r\n|  0%   28C    P8     4W / 166W |    191MiB /  8116MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce GTX 1070    On   | 00000000:05:00.0 Off |                  N/A |\r\n|  0%   29C    P8     5W / 166W |    150MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce GTX 107...  On   | 00000000:06:00.0 Off |                  N/A |\r\n|  0%   34C    P8     5W / 180W |    168MiB /  8119MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                 36MiB |\r\n|    0   N/A  N/A      1708      G   /usr/bin/gnome-shell                9MiB |\r\n|    0   N/A  N/A      4316      C   nvidia-cuda-mps-server            141MiB |\r\n|    1   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    1   N/A  N/A      4316      C   nvidia-cuda-mps-server            141MiB |\r\n|    2   N/A  N/A      1010      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    2   N/A  N/A      4316      C   nvidia-cuda-mps-server            159MiB |\r\n\r\nI got beyond the error above when I changed the LD_LIBRARY_PATH using:\r\n\r\nexport LD_LIBRARY_PATH=\u201c/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\u201d \r\n\r\nThis deleted the stubs library. \r\n\r\nAnd then I started to get CUDA_ERROR_MAP_FAILED\r\nAnd in /var/log/nvidia-mps/control.log I see: \r\n\r\nUser did not send valid credentials\r\n\r\nlooking up this error. Someone suggested adding \u2014ipc=\u201chost\u201d to the docker run command and that fixed the problem for me. \r\n\r\nSo what your team should do is document that when build from source and trying to execute under that container the \u201cstubs\u201d library needs to be deleted from the LD_LIBRARY_PATH \r\nAND\r\nthe container needs to be run with \u2014ipc=\u201chost\u201d on the docker run command.\r\n\r\n\r\n\r\n<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@RayLucchesi,\r\nIn order to expedite the trouble-shooting process, please provide the 'System information' details in the template.\r\n\r\nAlso as per the [tested build configurations](https://www.tensorflow.org/install/source#gpu), please try building TensorFlow with CUDA 10.1 and cuDNN 7.6 and check if it works. Thanks!", "@amahendrakar i've now build from source 2 in the last week or so. Each time took on the order of 11hrs of dedicated server time. And ~20GB of disk space on my 128GB SSD to commit the container and save the image file. In your tested configuration table you don't seem to have Tf 2.4 or even 2.3 so it was unclear what I was supposed to use for CUDA and cuDNN so I just took the latest stable levels of all that. \r\n\r\nI ended up building twice because I couldn't understand why cuinit.cc was throwing an unknown error (-1), so thought I had done the build wrong. \r\n\r\nSorry I'm not going to retry with these the CUDA and cuDNN levels you suggest are valid for TF2.4. I would need to down level my now working system and spend another 11 hrs compiling something that I'm going to throw out anyway. \r\n\r\nYou can go ahead and close this issue if you wish. Or you could re-attempt the build yourself. I had to build from source because of my Skylake CPU was not supported by the tf/tf: containers that were already built. Or you could examine the tf/tf:devel-gpu container and see that the LD_LIBRARY_PATH indeed includes the STUBs library (which won't work after you build from source AND check to see if when using NVIDIA-MPS that the --ipc=\"host\" parameter needs to be specified of if it could be allowed to be defaulted. \r\n\r\nBY THE WAY, It was not clear whether I should be running NVIDIA-MPS or not but as it's running in a docker container and there's the outside possibility that I might be running something else on the GPUs it seemed to me (and others) to be the right thing to use.\r\n\r\nThanks,", "@RayLucchesi,\r\nI understand your concern, we'll help you through the process and fix the issue. \r\n\r\nCould you please provide the platform details of the machine you're trying to build TensorFlow on. Also, list out the exact sequence of commands / steps that you executed before running into the problem. Thanks!", "I Build from source using the information provided in https://www.tensorflow.org/install/source.\r\nCommitted the container into an image file (and saved it).\r\n\r\nand then issued the following commands at a terminal:\r\n\r\nsudo nvidia-smi -pm 1\r\ndocker run --gpus 1 --ipc=\"host\" -it -w /tensorflow -v $PWD:/mnt -p 8888:8888 -e HOST_PERMS=\"$(id -u):$(id -g)\" tensorflow/tensorflow:from-src2 bash \r\nexport LD_LIBRARY_PATH=\u201c/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/include/x64_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\u201d\r\npip install jupyter\r\npip install jupyter_http_over_ws\r\njupyter serverextension enable --py jupyter_http_over_ws\r\njupyter notebook --no-browser --notebook-dir=/mnt/notebooks --ip=0.0.0.0 --MappingKernelManager.cull_interval=1200 --debug --NotebookApp.allow_origin='https://www.example.com' --NotebookApp.allow_remote_access=True --allow-root\r\n\r\nAt this point if I run your text-classification notebook it bombs with the error.  \r\n\r\nI'll send sys info in next comment.\r\n", "lspci command results. \r\n\r\n00:00.0 Host bridge: Intel Corporation Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor Host Bridge/DRAM Registers (rev 07)\r\n00:01.0 PCI bridge: Intel Corporation Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor PCIe Controller (x16) (rev 07)\r\n00:08.0 System peripheral: Intel Corporation Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th/8th Gen Core Processor Gaussian Mixture Model\r\n00:14.0 USB controller: Intel Corporation 200 Series/Z370 Chipset Family USB 3.0 xHCI Controller\r\n00:14.2 Signal processing controller: Intel Corporation 200 Series PCH Thermal Subsystem\r\n00:16.0 Communication controller: Intel Corporation 200 Series PCH CSME HECI #1\r\n00:17.0 SATA controller: Intel Corporation 200 Series PCH SATA controller [AHCI mode]\r\n00:1b.0 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #17 (rev f0)\r\n00:1b.2 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #19 (rev f0)\r\n00:1c.0 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #1 (rev f0)\r\n00:1c.5 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #6 (rev f0)\r\n00:1c.6 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #7 (rev f0)\r\n00:1c.7 PCI bridge: Intel Corporation 200 Series PCH PCI Express Root Port #8 (rev f0)\r\n00:1f.0 ISA bridge: Intel Corporation 200 Series PCH LPC Controller (Z270)\r\n00:1f.2 Memory controller: Intel Corporation 200 Series/Z370 Chipset Family Power Management Controller\r\n00:1f.3 Audio device: Intel Corporation 200 Series PCH HD Audio\r\n00:1f.4 SMBus: Intel Corporation 200 Series/Z370 Chipset Family SMBus Controller\r\n01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070 Ti] (rev a1)\r\n01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n03:00.0 USB controller: ASMedia Technology Inc. ASM2142 USB 3.1 Host Controller\r\n05:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n05:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n06:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070] (rev a1)\r\n06:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)\r\n07:00.0 Ethernet controller: Qualcomm Atheros Killer E2500 Gigabit Ethernet Controller (rev 10)", "here is output from lshw command:\r\n\r\nubuntu-ai                   \r\n    description: Desktop Computer\r\n    product: MS-7A62 (Default string)\r\n    vendor: MSI\r\n    version: 1.0\r\n    serial: Default string\r\n    width: 64 bits\r\n    capabilities: smbios-3.0.0 dmi-3.0.0 smp vsyscall32\r\n    configuration: boot=normal chassis=desktop family=Default string sku=Default string uuid=00000000-0000-0000-0000-4CCC6AF7447F\r\n  *-core\r\n       description: Motherboard\r\n       product: Z270 GAMING M3 (MS-7A62)\r\n       vendor: MSI\r\n       physical id: 0\r\n       version: 1.0\r\n       serial: H316061861\r\n       slot: Default string\r\n     *-firmware\r\n          description: BIOS\r\n          vendor: American Megatrends Inc.\r\n          physical id: 0\r\n          version: 1.10\r\n          date: 02/07/2017\r\n          size: 64KiB\r\n          capacity: 16MiB\r\n          capabilities: pci upgrade shadowing cdboot bootselect socketedrom edd int13floppy1200 int13floppy720 int13floppy2880 int5printscreen int9keyboard int14serial int17printer acpi usb biosbootspecification uefi\r\n     *-memory\r\n          description: System Memory\r\n          physical id: 3c\r\n          slot: System board or motherboard\r\n          size: 8GiB\r\n        *-bank:0\r\n             description: [empty]\r\n             physical id: 0\r\n             slot: ChannelA-DIMM0\r\n        *-bank:1\r\n             description: DIMM DDR4 Synchronous 2400 MHz (0.4 ns)\r\n             product: BLS4G4D240FSB.8FBD2\r\n             vendor: 859B\r\n             physical id: 1\r\n             serial: A624BDFD\r\n             slot: ChannelA-DIMM1\r\n             size: 4GiB\r\n             width: 64 bits\r\n             clock: 2400MHz (0.4ns)\r\n        *-bank:2\r\n             description: [empty]\r\n             physical id: 2\r\n             slot: ChannelB-DIMM0\r\n        *-bank:3\r\n             description: DIMM DDR4 Synchronous 2400 MHz (0.4 ns)\r\n             product: BLS4G4D240FSB.8FBD2\r\n             vendor: 859B\r\n             physical id: 3\r\n             serial: A624BDFC\r\n             slot: ChannelB-DIMM1\r\n             size: 4GiB\r\n             width: 64 bits\r\n             clock: 2400MHz (0.4ns)\r\n     *-cache:0\r\n          description: L1 cache\r\n          physical id: 42\r\n          slot: L1 Cache\r\n          size: 128KiB\r\n          capacity: 128KiB\r\n          capabilities: synchronous internal write-back unified\r\n          configuration: level=1\r\n     *-cache:1\r\n          description: L2 cache\r\n          physical id: 43\r\n          slot: L2 Cache\r\n          size: 512KiB\r\n          capacity: 512KiB\r\n          capabilities: synchronous internal write-back unified\r\n          configuration: level=2\r\n     *-cache:2\r\n          description: L3 cache\r\n          physical id: 44\r\n          slot: L3 Cache\r\n          size: 3MiB\r\n          capacity: 3MiB\r\n          capabilities: synchronous internal write-back unified\r\n          configuration: level=3\r\n     *-cpu\r\n          description: CPU\r\n          product: Intel(R) Pentium(R) CPU G4400 @ 3.30GHz\r\n          vendor: Intel Corp.\r\n          physical id: 45\r\n          bus info: cpu@0\r\n          version: Intel(R) Pentium(R) CPU G4400 @ 3.30GHz\r\n          serial: To Be Filled By O.E.M.\r\n          slot: U3E1\r\n          size: 3206MHz\r\n          capacity: 4005MHz\r\n          width: 64 bits\r\n          clock: 100MHz\r\n          capabilities: lm fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp x86-64 constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave rdrand lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust erms invpcid rdseed smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d cpufreq\r\n          configuration: cores=2 enabledcores=2 threads=2\r\n     *-pci\r\n          description: Host bridge\r\n          product: Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor Host Bridge/DRAM Registers\r\n          vendor: Intel Corporation\r\n          physical id: 100\r\n          bus info: pci@0000:00:00.0\r\n          version: 07\r\n          width: 32 bits\r\n          clock: 33MHz\r\n          configuration: driver=skl_uncore\r\n          resources: irq:0\r\n        *-pci:0\r\n             description: PCI bridge\r\n             product: Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor PCIe Controller (x16)\r\n             vendor: Intel Corporation\r\n             physical id: 1\r\n             bus info: pci@0000:00:01.0\r\n             version: 07\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pm msi pciexpress normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:120 ioport:e000(size=4096) memory:de000000-df0fffff ioport:c0000000(size=301989888)\r\n           *-display\r\n                description: VGA compatible controller\r\n                product: GP104 [GeForce GTX 1070 Ti]\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0\r\n                bus info: pci@0000:01:00.0\r\n                version: a1\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress vga_controller bus_master cap_list rom\r\n                configuration: driver=nvidia latency=0\r\n                resources: irq:137 memory:de000000-deffffff memory:c0000000-cfffffff memory:d0000000-d1ffffff ioport:e000(size=128) memory:c0000-dffff\r\n           *-multimedia\r\n                description: Audio device\r\n                product: GP104 High Definition Audio Controller\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0.1\r\n                bus info: pci@0000:01:00.1\r\n                version: a1\r\n                width: 32 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress bus_master cap_list\r\n                configuration: driver=snd_hda_intel latency=0\r\n                resources: irq:17 memory:df080000-df083fff\r\n        *-generic:0 UNCLAIMED\r\n             description: System peripheral\r\n             product: Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th/8th Gen Core Processor Gaussian Mixture Model\r\n             vendor: Intel Corporation\r\n             physical id: 8\r\n             bus info: pci@0000:00:08.0\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: msi pm cap_list\r\n             configuration: latency=0\r\n             resources: memory:dd12f000-dd12ffff\r\n        *-usb\r\n             description: USB controller\r\n             product: 200 Series/Z370 Chipset Family USB 3.0 xHCI Controller\r\n             vendor: Intel Corporation\r\n             physical id: 14\r\n             bus info: pci@0000:00:14.0\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: pm msi xhci bus_master cap_list\r\n             configuration: driver=xhci_hcd latency=0\r\n             resources: irq:127 memory:dd110000-dd11ffff\r\n           *-usbhost:0\r\n                product: xHCI Host Controller\r\n                vendor: Linux 5.4.0-42-generic xhci-hcd\r\n                physical id: 0\r\n                bus info: usb@1\r\n                logical name: usb1\r\n                version: 5.04\r\n                capabilities: usb-2.00\r\n                configuration: driver=hub slots=16 speed=480Mbit/s\r\n              *-usb:0\r\n                   description: Mouse\r\n                   product: USB Optical Mouse\r\n                   vendor: Logitech\r\n                   physical id: 7\r\n                   bus info: usb@1:7\r\n                   version: 72.00\r\n                   capabilities: usb-2.00\r\n                   configuration: driver=usbhid maxpower=100mA speed=2Mbit/s\r\n              *-usb:1\r\n                   description: Keyboard\r\n                   product: USB Keyboard\r\n                   vendor: Logitech\r\n                   physical id: 8\r\n                   bus info: usb@1:8\r\n                   version: 64.00\r\n                   capabilities: usb-1.10\r\n                   configuration: driver=usbhid maxpower=90mA speed=2Mbit/s\r\n           *-usbhost:1\r\n                product: xHCI Host Controller\r\n                vendor: Linux 5.4.0-42-generic xhci-hcd\r\n                physical id: 1\r\n                bus info: usb@2\r\n                logical name: usb2\r\n                version: 5.04\r\n                capabilities: usb-3.00\r\n                configuration: driver=hub slots=10 speed=5000Mbit/s\r\n        *-generic:1 UNCLAIMED\r\n             description: Signal processing controller\r\n             product: 200 Series PCH Thermal Subsystem\r\n             vendor: Intel Corporation\r\n             physical id: 14.2\r\n             bus info: pci@0000:00:14.2\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: pm msi cap_list\r\n             configuration: latency=0\r\n             resources: memory:dd12e000-dd12efff\r\n        *-communication\r\n             description: Communication controller\r\n             product: 200 Series PCH CSME HECI #1\r\n             vendor: Intel Corporation\r\n             physical id: 16\r\n             bus info: pci@0000:00:16.0\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: pm msi bus_master cap_list\r\n             configuration: driver=mei_me latency=0\r\n             resources: irq:132 memory:dd12d000-dd12dfff\r\n        *-sata\r\n             description: SATA controller\r\n             product: 200 Series PCH SATA controller [AHCI mode]\r\n             vendor: Intel Corporation\r\n             physical id: 17\r\n             bus info: pci@0000:00:17.0\r\n             logical name: scsi0\r\n             version: 00\r\n             width: 32 bits\r\n             clock: 66MHz\r\n             capabilities: sata msi pm ahci_1.0 bus_master cap_list emulated\r\n             configuration: driver=ahci latency=0\r\n             resources: irq:131 memory:dd128000-dd129fff memory:dd12c000-dd12c0ff ioport:f050(size=8) ioport:f040(size=4) ioport:f020(size=32) memory:dd12b000-dd12b7ff\r\n           *-disk\r\n                description: ATA Disk\r\n                product: SanDisk SDSSDA12\r\n                physical id: 0.0.0\r\n                bus info: scsi@0:0.0.0\r\n                logical name: /dev/sda\r\n                version: 80RL\r\n                serial: 171833459715\r\n                size: 111GiB (120GB)\r\n                capabilities: gpt-1.00 partitioned partitioned:gpt\r\n                configuration: ansiversion=5 guid=725cc7ba-faa9-4fe5-9e62-6b7b8a0b7aa7 logicalsectorsize=512 sectorsize=512\r\n              *-volume:0\r\n                   description: Windows FAT volume\r\n                   vendor: mkfs.fat\r\n                   physical id: 1\r\n                   bus info: scsi@0:0.0.0,1\r\n                   logical name: /dev/sda1\r\n                   logical name: /boot/efi\r\n                   version: FAT32\r\n                   serial: 1773-3e94\r\n                   size: 510MiB\r\n                   capacity: 511MiB\r\n                   capabilities: boot fat initialized\r\n                   configuration: FATs=2 filesystem=fat mount.fstype=vfat mount.options=rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro state=mounted\r\n              *-volume:1\r\n                   description: EXT4 volume\r\n                   vendor: Linux\r\n                   physical id: 2\r\n                   bus info: scsi@0:0.0.0,2\r\n                   logical name: /dev/sda2\r\n                   logical name: /boot\r\n                   version: 1.0\r\n                   serial: af28e231-0e6d-41f6-a632-b96cc017ffba\r\n                   size: 1GiB\r\n                   capabilities: journaled extended_attributes large_files huge_files dir_nlink recover 64bit extents ext4 ext2 initialized\r\n                   configuration: created=2020-07-22 19:01:51 filesystem=ext4 lastmountpoint=/boot modified=2020-08-03 17:23:18 mount.fstype=ext4 mount.options=rw,relatime mounted=2020-08-03 17:23:18 state=mounted\r\n              *-volume:2\r\n                   description: EFI partition\r\n                   physical id: 3\r\n                   bus info: scsi@0:0.0.0,3\r\n                   logical name: /dev/sda3\r\n                   serial: PYSAzy-ypr8-pyaN-oJcD-y4NN-YdMJ-hPwFpv\r\n                   size: 110GiB\r\n                   capabilities: lvm2\r\n        *-pci:1\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #17\r\n             vendor: Intel Corporation\r\n             physical id: 1b\r\n             bus info: pci@0000:00:1b.0\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:121\r\n        *-pci:2\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #19\r\n             vendor: Intel Corporation\r\n             physical id: 1b.2\r\n             bus info: pci@0000:00:1b.2\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:122 memory:df300000-df3fffff\r\n           *-usb\r\n                description: USB controller\r\n                product: ASM2142 USB 3.1 Host Controller\r\n                vendor: ASMedia Technology Inc.\r\n                physical id: 0\r\n                bus info: pci@0000:03:00.0\r\n                version: 00\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: msi msix pm pciexpress xhci bus_master cap_list\r\n                configuration: driver=xhci_hcd latency=0\r\n                resources: irq:18 memory:df300000-df307fff\r\n              *-usbhost:0\r\n                   product: xHCI Host Controller\r\n                   vendor: Linux 5.4.0-42-generic xhci-hcd\r\n                   physical id: 0\r\n                   bus info: usb@3\r\n                   logical name: usb3\r\n                   version: 5.04\r\n                   capabilities: usb-2.00\r\n                   configuration: driver=hub slots=2 speed=480Mbit/s\r\n              *-usbhost:1\r\n                   product: xHCI Host Controller\r\n                   vendor: Linux 5.4.0-42-generic xhci-hcd\r\n                   physical id: 1\r\n                   bus info: usb@4\r\n                   logical name: usb4\r\n                   version: 5.04\r\n                   capabilities: usb-3.10\r\n                   configuration: driver=hub slots=2 speed=10000Mbit/s\r\n        *-pci:3\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #1\r\n             vendor: Intel Corporation\r\n             physical id: 1c\r\n             bus info: pci@0000:00:1c.0\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:123\r\n        *-pci:4\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #6\r\n             vendor: Intel Corporation\r\n             physical id: 1c.5\r\n             bus info: pci@0000:00:1c.5\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:124 ioport:d000(size=4096) memory:dc000000-dd0fffff ioport:a0000000(size=301989888)\r\n           *-display\r\n                description: VGA compatible controller\r\n                product: GP104 [GeForce GTX 1070]\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0\r\n                bus info: pci@0000:05:00.0\r\n                version: a1\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress vga_controller bus_master cap_list rom\r\n                configuration: driver=nvidia latency=0\r\n                resources: irq:138 memory:dc000000-dcffffff memory:a0000000-afffffff memory:b0000000-b1ffffff ioport:d000(size=128) memory:dd000000-dd07ffff\r\n           *-multimedia\r\n                description: Audio device\r\n                product: GP104 High Definition Audio Controller\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0.1\r\n                bus info: pci@0000:05:00.1\r\n                version: a1\r\n                width: 32 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress bus_master cap_list\r\n                configuration: driver=snd_hda_intel latency=0\r\n                resources: irq:18 memory:dd080000-dd083fff\r\n        *-pci:5\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #7\r\n             vendor: Intel Corporation\r\n             physical id: 1c.6\r\n             bus info: pci@0000:00:1c.6\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:125 ioport:c000(size=4096) memory:da000000-db0fffff ioport:80000000(size=301989888)\r\n           *-display\r\n                description: VGA compatible controller\r\n                product: GP104 [GeForce GTX 1070]\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0\r\n                bus info: pci@0000:06:00.0\r\n                version: a1\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress vga_controller bus_master cap_list rom\r\n                configuration: driver=nvidia latency=0\r\n                resources: irq:139 memory:da000000-daffffff memory:80000000-8fffffff memory:90000000-91ffffff ioport:c000(size=128) memory:db000000-db07ffff\r\n           *-multimedia\r\n                description: Audio device\r\n                product: GP104 High Definition Audio Controller\r\n                vendor: NVIDIA Corporation\r\n                physical id: 0.1\r\n                bus info: pci@0000:06:00.1\r\n                version: a1\r\n                width: 32 bits\r\n                clock: 33MHz\r\n                capabilities: pm msi pciexpress bus_master cap_list\r\n                configuration: driver=snd_hda_intel latency=0\r\n                resources: irq:19 memory:db080000-db083fff\r\n        *-pci:6\r\n             description: PCI bridge\r\n             product: 200 Series PCH PCI Express Root Port #8\r\n             vendor: Intel Corporation\r\n             physical id: 1c.7\r\n             bus info: pci@0000:00:1c.7\r\n             version: f0\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: pci pciexpress msi pm normal_decode bus_master cap_list\r\n             configuration: driver=pcieport\r\n             resources: irq:126 ioport:b000(size=4096) memory:df200000-df2fffff\r\n           *-network\r\n                description: Ethernet interface\r\n                product: Killer E2500 Gigabit Ethernet Controller\r\n                vendor: Qualcomm Atheros\r\n                physical id: 0\r\n                bus info: pci@0000:07:00.0\r\n                logical name: enp7s0\r\n                version: 10\r\n                serial: 4c:cc:6a:f7:44:7f\r\n                size: 1Gbit/s\r\n                capacity: 1Gbit/s\r\n                width: 64 bits\r\n                clock: 33MHz\r\n                capabilities: pm pciexpress msi msix bus_master cap_list ethernet physical tp 10bt 10bt-fd 100bt 100bt-fd 1000bt-fd autonegotiation\r\n                configuration: autonegotiation=on broadcast=yes driver=alx duplex=full ip=10.0.0.184 latency=0 link=yes multicast=yes port=twisted pair speed=1Gbit/s\r\n                resources: irq:19 memory:df200000-df23ffff ioport:b000(size=128)\r\n        *-isa\r\n             description: ISA bridge\r\n             product: 200 Series PCH LPC Controller (Z270)\r\n             vendor: Intel Corporation\r\n             physical id: 1f\r\n             bus info: pci@0000:00:1f.0\r\n             version: 00\r\n             width: 32 bits\r\n             clock: 33MHz\r\n             capabilities: isa bus_master\r\n             configuration: latency=0\r\n        *-memory UNCLAIMED\r\n             description: Memory controller\r\n             product: 200 Series/Z370 Chipset Family Power Management Controller\r\n             vendor: Intel Corporation\r\n             physical id: 1f.2\r\n             bus info: pci@0000:00:1f.2\r\n             version: 00\r\n             width: 32 bits\r\n             clock: 33MHz (30.3ns)\r\n             configuration: latency=0\r\n             resources: memory:dd124000-dd127fff\r\n        *-multimedia\r\n             description: Audio device\r\n             product: 200 Series PCH HD Audio\r\n             vendor: Intel Corporation\r\n             physical id: 1f.3\r\n             bus info: pci@0000:00:1f.3\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             capabilities: pm msi bus_master cap_list\r\n             configuration: driver=snd_hda_intel latency=32\r\n             resources: irq:133 memory:dd120000-dd123fff memory:dd100000-dd10ffff\r\n        *-serial\r\n             description: SMBus\r\n             product: 200 Series/Z370 Chipset Family SMBus Controller\r\n             vendor: Intel Corporation\r\n             physical id: 1f.4\r\n             bus info: pci@0000:00:1f.4\r\n             version: 00\r\n             width: 64 bits\r\n             clock: 33MHz\r\n             configuration: driver=i801_smbus latency=0\r\n             resources: irq:16 memory:dd12a000-dd12a0ff ioport:f000(size=32)\r\n     *-pnp00:00\r\n          product: PnP device PNP0c02\r\n          physical id: 1\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:01\r\n          product: PnP device PNP0303\r\n          physical id: 2\r\n          capabilities: pnp\r\n          configuration: driver=i8042 kbd\r\n     *-pnp00:02\r\n          product: PnP device PNP0f03\r\n          physical id: 3\r\n          capabilities: pnp\r\n          configuration: driver=i8042 aux\r\n     *-pnp00:03\r\n          product: PnP device PNP0501\r\n          physical id: 4\r\n          capabilities: pnp\r\n          configuration: driver=serial\r\n     *-pnp00:04\r\n          product: PnP device PNP0c02\r\n          physical id: 5\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:05\r\n          product: PnP device PNP0c02\r\n          physical id: 6\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:06\r\n          product: PnP device PNP0b00\r\n          physical id: 7\r\n          capabilities: pnp\r\n          configuration: driver=rtc_cmos\r\n     *-pnp00:07\r\n          product: PnP device INT3f0d\r\n          physical id: 8\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:08\r\n          product: PnP device PNP0c02\r\n          physical id: 9\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:09\r\n          product: PnP device PNP0c02\r\n          physical id: a\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:0a\r\n          product: PnP device PNP0c02\r\n          physical id: b\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n     *-pnp00:0b\r\n          product: PnP device PNP0c02\r\n          physical id: c\r\n          capabilities: pnp\r\n          configuration: driver=system\r\n  *-power UNCLAIMED\r\n       description: To Be Filled By O.E.M.\r\n       product: To Be Filled By O.E.M.\r\n       vendor: To Be Filled By O.E.M.\r\n       physical id: 1\r\n       version: To Be Filled By O.E.M.\r\n       serial: To Be Filled By O.E.M.\r\n       capacity: 32768mWh\r\n  *-network:0\r\n       description: Ethernet interface\r\n       physical id: 2\r\n       logical name: vetha11e727\r\n       serial: 7e:40:7f:0a:04:8c\r\n       size: 10Gbit/s\r\n       capabilities: ethernet physical\r\n       configuration: autonegotiation=off broadcast=yes driver=veth driverversion=1.0 duplex=full link=yes multicast=yes port=twisted pair speed=10Gbit/s\r\n  *-network:1\r\n       description: Ethernet interface\r\n       physical id: 3\r\n       logical name: docker0\r\n       serial: 02:42:69:a5:23:2a\r\n       capabilities: ethernet physical\r\n       configuration: broadcast=yes driver=bridge driverversion=2.3 firmware=N/A ip=172.17.0.1 link=yes multicast=yes\r\n", "Let me know if there is any other system info you need. ", "Provided the requested info in the issue \nRay\n\n> On Aug 4, 2020, at 9:56 AM, Abhilash Mahendrakar <notifications@github.com> wrote:\n> \n> \n> @RayLucchesi <https://github.com/RayLucchesi>,\n> I understand your concern, we'll help you through the process and fix the issue.\n> \n> Could you please provide the platform details of the machine you're trying to build TensorFlow on. Also, list out the exact sequence of commands / steps that you executed before running into the problem. Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/41748#issuecomment-668680152>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ABS3S75VV6TTNRMIGFZEDGLR7AVTZANCNFSM4PIE77LA>.\n> \n\n", "it seems this is due to having STUBS in the LD_LIBRARY_PATH when I remove that it works.", "@RayLucchesi Glad that it works. Can we close this issue as it has been resolved. Thanks!", "Yes\nRay\n\n> On Sep 22, 2020, at 1:18 PM, gowthamkpr <notifications@github.com> wrote:\n> \n> \n> @RayLucchesi <https://github.com/RayLucchesi> Glad that it works. Can we close this issue as it has been resolved. Thanks!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub <https://github.com/tensorflow/tensorflow/issues/41748#issuecomment-696927349>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/ABS3S76IKBHM4ZA5GD6RRTTSHD2BNANCNFSM4PIE77LA>.\n> \n\n", "Closing this issue as it has been resolved. Please add additional comments for us to open this issue again. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41748\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41748\">No</a>\n"]}, {"number": 41747, "title": "Replaced the batch sizes with 'None' instead '?'", "body": "I have replaced the default text to be shown if batch sizes are not present as 'None' instead of '?' because it is more readable and understandable during the plotting of the model. Furthermore, it avoids the confusion of any error in the model and signifies that the fact that the programmer voluntarily haven't given any batch sizes", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41747) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41747) for more info**.\n\n<!-- ok -->", "Thanks for the PR.\r\n\r\nI think the \"None\" is a more standard way in python to express unassigned/unknown value. Do u have any specific example that \"None\" is causing any confusion?", "> Thanks for the PR.\r\n> \r\n> I think the \"None\" is a more standard way in python to express unassigned/unknown value. Do u have any specific example that \"None\" is causing any confusion?\r\n\r\n\r\nThere might be some confusion my pull request actually **replaces the \"?\" question mark with \"None\" because in the vis_utils.py original file the default is \"?\" question mark** which somehows signifies error rather than a missing value.\r\n\r\n\"None\" specifies that voluntarily I have not specified values. But the default parameter in the keras vis_utils file is \"?\" question mark. Therefore I made a pull request where I have replaced the \"?\" question mark with \"None\". As you stated \"None\" is a more standard way in python to express unassigned/unknown value rather than a question mark.\r\n \r\n![Original tensorflow code](https://user-images.githubusercontent.com/50369708/89232207-9c606e00-d604-11ea-8709-a1e842489866.PNG)\r\n\r\n\r\nRight now the default is \"?\" question mark if value is not specified. I changed it to \"None\" therefore I made a pull request. It is much easier to understand. \r\n\r\nFor instance look at the figures below :\r\n![model with ?](https://user-images.githubusercontent.com/50369708/89231593-450dce00-d603-11ea-8ca4-dd51b9241342.png)\r\n\r\nTo change the above image with \"None\"  I have to locally go to the tensorflow library installed in my computer and then change the vis_utils.py line from \"?\" to \"None\" to get the following result.\r\n\r\n![model with \"None\"](https://user-images.githubusercontent.com/50369708/89231727-87370f80-d603-11ea-9336-c0da16a17baf.png)\r\n\r\nSo I think it would be easy if the default parameter for non-specified values is \"None\" instead of a \"?\" question mark that's why in this pull request **I have replaced the \"?\" question mark with \"None\" in the vis_utils.py file.**\r\n"]}, {"number": 41745, "title": "Fix the usage of uninitialized variable in adaptive_shared_batch_scheduler", "body": "The variable best_score is uninitialized.", "comments": []}, {"number": 41744, "title": "Question mark (?) in the model image using plot_model instead of None", "body": "I am using the tf.keras.utils.plot_model function to plot the model. The code is given below. Now instead of getting None for the batch sizes I am getting question mark (?). So can you please address why this is happening and how I can replace None inplace of ? question mark.\r\n\r\ntf.keras.utils.plot_model(\r\n    model,\r\n    to_file='model2.png',\r\n    show_shapes=True,\r\n    show_layer_names=True\r\n)\r\n\r\n![model image](https://user-images.githubusercontent.com/50369708/88482269-d9ca5900-cf7d-11ea-8a0b-7150d4b056e1.png)\r\n", "comments": ["What you see there is caused by this line in keras utils:\r\nhttps://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/utils/vis_utils.py#L183\r\n\r\nIt's actually the desired behavior. \r\nMy understanding is that the None sized dimensions are effectively variable sized, or unknown '?' when the graph is compiled. \r\nTherefore tensor with other dimensions matching can fit. Notice how output shape of a layer and the input shape of the following layer always matches. \r\n\r\nNow you could replace the '?' string with 'None' string, but what would it be good for? It wouldn't make it any more readable. ", "> What you see there is caused by this line in keras utils:\r\n> https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/utils/vis_utils.py#L183\r\n> \r\n> It's actually the desired behavior.\r\n> My understanding is that the None sized dimensions are effectively variable sized, or unknown '?' when the graph is compiled.\r\n> Therefore tensor with other dimensions matching can fit. Notice how output shape of a layer and the input shape of the following layer always matches.\r\n> \r\n> Now you could replace the '?' string with 'None' string, but what would it be good for? It wouldn't make it any more readable.\r\n\r\nActually I am working on a research paper in which I need the image of the model architecture although it wouldn't make it any more readable from a programmer's point of view but from a layman's understanding it would atleast come out to be a tad bit readable as the '?' mark signifies as if there's an error or something is missing where as None kind of explains that I voluntary didn't add the batch sizes. Thanks for the answer though!", "Worst case scenario you can add an explanatory footnote. ", "@V2dha \r\n\r\nIt is expected behavior.Please, see this [tutorial](https://www.tensorflow.org/tutorials/images/segmentation), [link](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model) for your reference.Thanks!", "Okay thanks!"]}, {"number": 41743, "title": "Building tensorflow 2.2 fails", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.2\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: GeForce GTX 1080 8111MiB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nEvery time I use the ```bazel``` command to build tensorflow, this error appears:\r\n\r\n```\r\nERROR: /home/amir/Documents/tensorflow/tensorflow/tensorflow/core/kernels/BUILD:2734:1: output 'tensorflow/core/kernels/_objs/eye_functor_gpu/eye_functor_gpu.cu.pic.o' was not created\r\n```\r\n\r\nor something like above that includes ```_gpu.cu.pic.o```.\r\nI have built previous versions of tensorflow many times including tensorflow 1.9-1.13 with previous versions of cuda without problem\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build --config=opt --config=mkl //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n\r\n**Any other info / logs**\r\n\r\nHere is the .tf_configure_bazelrc configurations:\r\n\r\n```\r\nbuild --action_env PYTHON_BIN_PATH=\"/home/amir/Documents/tensorflow/py-env/bin/python\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/home/amir/Documents/tensorflow/py-env/lib/python3.8/site-packages\"\r\nbuild --python_path=\"/home/amir/Documents/tensorflow/py-env/bin/python\"\r\nbuild --config=xla\r\nbuild --action_env TF_CUDA_VERSION=\"10.1\"\r\nbuild --action_env TF_CUDNN_VERSION=\"7.6\"\r\nbuild --action_env TF_NCCL_VERSION=\"2.7\"\r\nbuild --action_env TF_CUDA_PATHS=\"/usr/local/cuda/cuda-10.1/extras/CUPTI,/usr/local/cuda/cuda-10.1/nccl/nccl-2.7,/usr/local/cuda/cuda-10.1/cudnn/cudnn-7.6,/usr/local/cuda/cuda-10.1\"\r\nbuild --action_env CUDA_TOOLKIT_PATH=\"/usr/local/cuda/cuda-10.1\"\r\nbuild --action_env TF_CUDA_COMPUTE_CAPABILITIES=\"6.1\"\r\nbuild --action_env LD_LIBRARY_PATH=\"/usr/local/cuda/cuda-10.1/extras/CUPTI/lib64:/usr/local/cuda/cuda-10.1/nccl/nccl-2.7/lib:/usr/local/cuda/cuda-10.1/cudnn/cudnn-7.6/lib64:/usr/local/cuda/cuda-10.1/lib64\"\r\nbuild --action_env GCC_HOST_COMPILER_PATH=\"/usr/bin/x86_64-linux-gnu-gcc-7\"\r\nbuild --config=cuda\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial\r\ntest:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu\r\ntest:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only\r\ntest:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n", "comments": ["@amirekhlasi,\r\nLooking at similar issues [#18980](https://github.com/tensorflow/tensorflow/issues/18980) and [#18522](https://github.com/tensorflow/tensorflow/issues/18522), the GCC version installed seems to be the issue. \r\n\r\nAs per the [cuda installation guide](https://docs.nvidia.com/cuda/archive/10.1/cuda-installation-guide-linux/index.html), could you please try installing GCC v8.2.0 and check if you are able to build TensorFlow. Thanks!", "That didn't solve the problem. I use gcc-8.2 and gcc-8.4 and the problem didn't solve. But that error didn't occur using cuda version 10.0. However in the final stages of compilation the following error occured:\r\n```\r\nERROR: /home/amir/program-tools/tensorflow/tensorflow/tensorflow/python/BUILD:2308:1: Executing genrule //tensorflow/python:test_ops_pygenrule failed (Exit 127)\r\nbazel-out/host/bin/tensorflow/python/gen_test_ops_py_wrappers_cc: error while loading shared libraries: libiomp5.so: cannot open shared object file: No such file or directory\r\n```\r\nbut the above library exist in bazel cache:\r\n\r\n```\r\n./home/amir/.cache/bazel/_bazel_amir/4630ddb84c6dfcc49e38b17c2862818c/external/mkl_linux/lib/libiomp5.so\r\n```", "The latter can be solved by copying ```*/mkl_linux``` directory to an arbitrary location and adding ```*/mkl_linux/lib``` to ```LD_LIBRARY_PATH```.  \r\nHowever the CUDA 10.1 problem is a serious bug and I was able to build Tensorflow 2.2 using both CUDA 10.0 and 10.2 but not 10.1. It seems CUDA 10.1 has serious problems, specially TensorRT 7 supports CUDA 9.x, 10.0, 10.2 but not 10.1!", "@amirekhlasi,\r\nThank you for the update. \r\n\r\nIs this still an issue? Please feel free to close the issue if resolved.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41743\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41743\">No</a>\n"]}, {"number": 41742, "title": "Run entire epoch as compiled tf.function in model.fit()", "body": "As mentioned in #40881, this PR PR changes Keras `model.fit()` and `model.evaluate()` to automatically compile the iteration over one epoch into a `tf.function` to improve performance. This new fastpath will be triggered starting from the second epoch in the following cases:\r\n- `run_eagerly` is `False`\r\n- No batch level callback hooks need to be called (usually true when `verbose != 1`)\r\n- `steps_per_execution == 1`\r\n- `data_handler.inferred_steps` are defined\r\n\r\nThese changes should prevent users needing to learn about `experimental_steps_per_execution` and has the potential to greatly improve performance on GPUs or TPUs.\r\n\r\nI briefly benchmarked these changes on a CPU only machine to make sure that this doesn't introduce any obvious regressions. @omalleyt12 I think you are in a much better place to judge this, so it would be great if you could run this against your internal benchmarks using TPUs or GPUs as well.\r\n\r\nI think these changes should be covered by the current test cases, but please let me know if there is something I am missing here.", "comments": ["@omalleyt12 Do you have time to take a look?", "@omalleyt12 I rebase this PR onto master since AutoGraph now supports creating new symbols in a loop, the code could be simplified and support added for `model.evaluate()`.\r\n\r\nCould you please take a look at this PR as it has to potential to greatly speedup training time in some cases without the users needing to worry about setting `steps_per_execution`.", "@gbaned @omalleyt12 @fchollet Any news on this PR, it has been open for a long time and still awaits a PR review", "@lgeiger Thanks for the PR! Sorry for the delay in review!\r\n\r\nDiscussed with @fchollet , we think the best thing to do would be to \"auto-tune\" the `steps_per_execution` such that the timing of one execution takes > ~200ms-1s. This would achieve most of the benefits of running an entire epoch in one execution while still allowing the progress bar to update quickly in a Colab/Jupyter notebook and also ensuring that any error that occurs doesn't invalidate a whole epoch of training.\r\n\r\nWe should only do this in the case where (mostly as you said):\r\n\r\n1. run_eagerly is False\r\n2. No batch level callback hooks need to be called\r\n3. User passes `steps_per_execution=None` (we should make sure a user can hard-code `steps_per_execution=1`, bc the way we do multiple steps still requires extra graph tracing which may cause OOM or performance issues)\r\n4. `data_handler.inferred_steps` are defined\r\n\r\nHowever, under this approach, it's ok to do for `verbose=1`, and we can exempt the `ProgressBarLogger` callback from criteria 2.\r\n\r\nYou've already done most of the leg-work for an approach like this in this PR, would you be interested in taking this on?\r\n\r\nThe tricky parts off the top of my head:\r\n\r\n1. Going from `steps_per_execution == 1` to `steps_per_execution > 1` will require clearing the `tf.function` training step and recalling `Model.make_train_function` (All other changes to `steps_per_execution` don't need a retracing)\r\n2. Ensuring the Callbacks and DataHandler are both keeping track of the correct step number\r\n\r\n", "@lgeiger Can you please check @omalleyt12's comments and keep us posted ? Thanks!", "Hi @omalleyt12 thanks for the response, and sorry for my late reply. Do I understand it correctly that the main reason to auto-tune `steps_per_execution` is to have better support for the progressbar?\r\n\r\nTo me this seems to require quite a bit of code to properly handle all edge cases.  Since `steps_per_execution` gets passed in `compile()` instead of `fit()` autotuning it would effectively make `Model.train_function` not deterministic (if autotuned based on the time per step) which might be problematic if users rely on this to function to run for a fixed number of steps?. I don't think I understand enough of the reasoning behind the design of `steps_per_execution` to properly implement auto-tuning in a short amount of time (e.g. for me intuitively `steps_per_execution` would be an implementation detail of the training process so I'm a bit confused why it is already set in `compile()` and not in fit `fit()`, although that fact probably has a very good reason too :)).\r\n\r\nUnfortunately I don't have the time to take another look to properly implement autotuning in the near future so I'd prefer to close this PR if the current solution isn't an option for you.\r\n\r\nPersonally, I've never really used a batch level progress bar for any performance critical long running trainings, as many remote execution services (including AI Platform last time I checked) don't have great support for handling ANSI carriage returns anyway and when using a Jupyter notebook I usually only train for a very short time during debugging where performance is not really critical (or the model even executes eagerly). So I am not sure yet if I agree with the motivation of autotuning here (although I don't have the full picture over all Keras users, so feel free to ignore this comment).", "@lgeiger Can you please resolve conflicts? Thanks!", "@lgeiger  Any update on this PR? Please. Thanks!", "@gbaned I haven't heard back from @omalleyt12 about https://github.com/tensorflow/tensorflow/pull/41742#issuecomment-705246013 so rebasing doesn't make sense before the general question has been resolved.", "@omalleyt12  Can you please take a look on the above comment from @lgeiger. Thanks!", "Thanks for the PR, and sorry the change didn't work out. Feel free to reopen a new PR if you would still like to see this change (you can also contact me at fchollet@google.com to discuss API & design specifics).", "> you can also contact me at fchollet@google.com to discuss API & design specifics).\r\n\r\nThanks for the offer! Unfortunately, I can't dedicate the time needed to really have a think about possible improvements to the API in the near future. But I will definitely reach to you if I get a chance to look into this again or will provide feedback if there is a public RFC to comment on."]}, {"number": 41741, "title": "No object detection for panoramic photos Tensorflow", "body": "Good day. I ask for your help.\r\n\r\nI tried almost all the models, but I didn't get the desired result.\r\n\r\nTF finds objects but only at close proximity, if the photo is panoramic, then the detection does not work or works extremely poorly \r\n![\u0410\u043d\u043d\u043e\u0442\u0430\u0446\u0438\u044f 2020-07-26 090233](https://user-images.githubusercontent.com/32834586/88474754-59323b00-cf32-11ea-87bc-007034ddb758.png)\r\n![\u0410\u043d\u043d\u043e\u0442\u0430\u0446\u0438\u044f 2020-07-26 090334](https://user-images.githubusercontent.com/32834586/88474756-5afbfe80-cf32-11ea-905c-89420a937b48.png)\r\nThe marked images contain both panoramic and close-up photographs. About the same amount\r\n\r\n```model {\r\n  faster_rcnn {\r\n    num_classes: 12\r\n    image_resizer {\r\n      keep_aspect_ratio_resizer {\r\n        min_dimension: 600\r\n        max_dimension: 1024\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"faster_rcnn_resnet50\"\r\n      first_stage_features_stride: 16\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        height_stride: 16\r\n        width_stride: 16\r\n        scales: 0.25\r\n        scales: 0.5\r\n        scales: 1.0\r\n        scales: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.00999999977648\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.699999988079\r\n    first_stage_max_proposals: 50\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 14\r\n    maxpool_kernel_size: 2\r\n    maxpool_stride: 2\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.300000011921\r\n        iou_threshold: 0.600000023842\r\n        max_detections_per_class: 20\r\n        max_total_detections: 20\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n    second_stage_batch_size: 49\r\n  }\r\n}\r\ntrain_config {\r\n  batch_size: 1\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  optimizer {\r\n    momentum_optimizer {\r\n      learning_rate {\r\n        manual_step_learning_rate {\r\n          initial_learning_rate: 0.000300000014249\r\n          \r\n          schedule {\r\n            step: 900000\r\n            learning_rate: 2.99999992421e-05\r\n          }\r\n          schedule {\r\n            step: 1200000\r\n            learning_rate: 3.00000010611e-06\r\n          }\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.899999976158\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint: \"C:/tensorflow1/models/research/object_detection/faster_rcnn_resnet50_lowproposals_coco_2018_01_28/model.ckpt\"\r\n  from_detection_checkpoint: true\r\n  num_steps: 200000\r\n}\r\ntrain_input_reader {\r\n  label_map_path: \"C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"C:/tensorflow1/models/research/object_detection/train.record\"\r\n  }\r\n}\r\neval_config {\r\n  num_examples: 316\r\n  max_evals: 10\r\n  use_moving_averages: false\r\n}\r\neval_input_reader {\r\n  label_map_path: \"C:/tensorflow1/models/research/object_detection/training/labelmap.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n  tf_record_input_reader {\r\n    input_path: \"C:/tensorflow1/models/research/object_detection/test.record\"\r\n  }\r\n}\r\n```\r\n\r\nObject_detection_image.py\r\n```\r\n######## Image Object Detection Using Tensorflow-trained Classifier #########\r\n# Import packages\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\n\r\n# This is needed since the notebook is stored in the object_detection folder.\r\nsys.path.append(\"..\")\r\n\r\n# Import utilites\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\n\r\n# Name of the directory containing the object detection module we're using\r\nMODEL_NAME = 'inference_graph'\r\nIMAGE_NAME = 'test1.jpeg'\r\n\r\n# Grab path to current working directory\r\nCWD_PATH = os.getcwd()\r\n\r\n# Path to frozen detection graph .pb file, which contains the model that is used\r\n# for object detection.\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\r\n\r\n# Path to label map file\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')\r\n\r\n# Path to image\r\nPATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\r\n\r\n# Number of classes the object detector can identify\r\nNUM_CLASSES = 12\r\n\r\n# Load the label map.\r\n# Label maps map indices to category names, so that when our convolution\r\n# network predicts `5`, we know that this corresponds to `king`.\r\n# Here we use internal utility functions, but anything that returns a\r\n# dictionary mapping integers to appropriate string labels would be fine\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n# Load the Tensorflow model into memory.\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\n    sess = tf.Session(graph=detection_graph)\r\n\r\n# Define input and output tensors (i.e. data) for the object detection classifier\r\n\r\n# Input tensor is the image\r\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n\r\n# Output tensors are the detection boxes, scores, and classes\r\n# Each box represents a part of the image where a particular object was detected\r\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n\r\n# Each score represents level of confidence for each of the objects.\r\n# The score is shown on the result image, together with the class label.\r\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n\r\n# Number of objects detected\r\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\n# Load image using OpenCV and\r\n# expand image dimensions to have shape: [1, None, None, 3]\r\n# i.e. a single-column array, where each item in the column has the pixel RGB value\r\nimage = cv2.imread(PATH_TO_IMAGE)\r\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\nimage_expanded = np.expand_dims(image_rgb, axis=0)\r\n\r\n# Perform the actual detection by running the model with the image as input\r\n(boxes, scores, classes, num) = sess.run(\r\n    [detection_boxes, detection_scores, detection_classes, num_detections],\r\n    feed_dict={image_tensor: image_expanded})\r\n\r\n# Draw the results of the detection (aka 'visulaize the results')\r\n\r\nvis_util.visualize_boxes_and_labels_on_image_array(\r\n    image,\r\n    np.squeeze(boxes),\r\n    np.squeeze(classes).astype(np.int32),\r\n    np.squeeze(scores),\r\n    category_index,\r\n    use_normalized_coordinates=True,\r\n    line_thickness=2,\r\n    min_score_thresh=0.30)\r\n\r\n# All the results have been drawn on image. Now display the image.\r\ncv2.imshow('Object detector', image)\r\n\r\n# Press any key to close the image\r\ncv2.waitKey(0)\r\n\r\n# Clean up\r\ncv2.destroyAllWindows()\r\n```\r\n![tvorog_dop_pon (80)](https://user-images.githubusercontent.com/32834586/88474783-95659b80-cf32-11ea-8bb2-e3ea0047fa95.jpeg)\r\n![tvorog_dop_pon (82)](https://user-images.githubusercontent.com/32834586/88474786-9991b900-cf32-11ea-846b-534eef0262f0.jpeg)\r\n![tvorog_dop_sr (3)](https://user-images.githubusercontent.com/32834586/88474797-add5b600-cf32-11ea-8671-3b31e656e2d3.jpeg)\r\n\r\nI have now copied the panoramic photo to the training folder in the folder for object_detection. No matches found that is, a completely identical image from the training set does not find a match", "comments": ["There are two simple possibilities: \r\n\r\n1. Preprocessing. How does the panoramic image actually 'look' when passed to mode? It might be distorted. \r\n2. It is possible that the model was trained only on small images. In that case it might not recognize objects on larger images, because it contains different features. Or rather does not contain expected features. ", "> There are two simple possibilities:\r\n> \r\n> 1. Preprocessing. How does the panoramic image actually 'look' when passed to mode? It might be distorted.\r\n> 2. It is possible that the model was trained only on small images. In that case it might not recognize objects on larger images, because it contains different features. Or rather does not contain expected features.\r\n\r\nGood day. I did not understand about point 1.\r\nPoint 2: No, the training kit also includes panoramic and close-up photos. That is why I have attached sample photos", "About point 1: \r\nWhen you pass an input to the model it can be transformed in various ways, depending on how you set up your pipeline. \r\nResolution, colors, cropping etc. it is possible that your evaluation data undergoes different transformation than your training data, if that's the case your model will have trouble recognizing the features it learned on training data. \r\n\r\nAnyway, I do recommend that you look at the raw output of your model: `boxes, scores, classes, num`. It will tell you more than the resulting bounding boxes.", "> About point 1:\r\n> When you pass an input to the model it can be transformed in various ways, depending on how you set up your pipeline.\r\n> Resolution, colors, cropping etc. it is possible that your evaluation data undergoes different transformation than your training data, if that's the case your model will have trouble recognizing the features it learned on training data.\r\n> \r\n> Anyway, I do recommend that you look at the raw output of your model: `boxes, scores, classes, num`. It will tell you more than the resulting bounding boxes.\r\n\r\nThanks for your reply. Yes, I understand what you mean.\r\nHowever, I am using the same .config to train and find objects. I have attached the code for this config", "My object_detection.py \r\n```\r\n######## Image Object Detection Using Tensorflow-trained Classifier #########\r\n#\r\n# Author: Evan Juras\r\n# Date: 1/15/18\r\n# Description: \r\n# This program uses a TensorFlow-trained neural network to perform object detection.\r\n# It loads the classifier and uses it to perform object detection on an image.\r\n# It draws boxes, scores, and labels around the objects of interest in the image.\r\n\r\n## Some of the code is copied from Google's example at\r\n## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\r\n\r\n## and some is copied from Dat Tran's example at\r\n## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py\r\n\r\n## but I changed it to make it more understandable to me.\r\n\r\n# Import packages\r\nimport os\r\nimport cv2\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport sys\r\n\r\n# This is needed since the notebook is stored in the object_detection folder.\r\nsys.path.append(\"..\")\r\n\r\n# Import utilites\r\nfrom utils import label_map_util\r\nfrom utils import visualization_utils as vis_util\r\n\r\n# Name of the directory containing the object detection module we're using\r\nMODEL_NAME = 'inference_graph'\r\nIMAGE_NAME = 'test1.jpeg'\r\n\r\n# Grab path to current working directory\r\nCWD_PATH = os.getcwd()\r\n\r\n# Path to frozen detection graph .pb file, which contains the model that is used\r\n# for object detection.\r\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\r\n\r\n# Path to label map file\r\nPATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')\r\n\r\n# Path to image\r\nPATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\r\n\r\n# Number of classes the object detector can identify\r\nNUM_CLASSES = 12\r\n\r\n# Load the label map.\r\n# Label maps map indices to category names, so that when our convolution\r\n# network predicts `5`, we know that this corresponds to `king`.\r\n# Here we use internal utility functions, but anything that returns a\r\n# dictionary mapping integers to appropriate string labels would be fine\r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n# Load the Tensorflow model into memory.\r\ndetection_graph = tf.Graph()\r\nwith detection_graph.as_default():\r\n    od_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\r\n        serialized_graph = fid.read()\r\n        od_graph_def.ParseFromString(serialized_graph)\r\n        tf.import_graph_def(od_graph_def, name='')\r\n\r\n    sess = tf.Session(graph=detection_graph)\r\n\r\n# Define input and output tensors (i.e. data) for the object detection classifier\r\n\r\n# Input tensor is the image\r\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\r\n\r\n# Output tensors are the detection boxes, scores, and classes\r\n# Each box represents a part of the image where a particular object was detected\r\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\r\n\r\n# Each score represents level of confidence for each of the objects.\r\n# The score is shown on the result image, together with the class label.\r\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\r\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\r\n\r\n# Number of objects detected\r\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\r\n\r\n# Load image using OpenCV and\r\n# expand image dimensions to have shape: [1, None, None, 3]\r\n# i.e. a single-column array, where each item in the column has the pixel RGB value\r\nimage = cv2.imread(PATH_TO_IMAGE)\r\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\nimage_expanded = np.expand_dims(image_rgb, axis=0)\r\n\r\n# Perform the actual detection by running the model with the image as input\r\n(boxes, scores, classes, num) = sess.run(\r\n    [detection_boxes, detection_scores, detection_classes, num_detections],\r\n    feed_dict={image_tensor: image_expanded})\r\n\r\n# Draw the results of the detection (aka 'visulaize the results')\r\n\r\nvis_util.visualize_boxes_and_labels_on_image_array(\r\n    image,\r\n    np.squeeze(boxes),\r\n    np.squeeze(classes).astype(np.int32),\r\n    np.squeeze(scores),\r\n    category_index,\r\n    use_normalized_coordinates=True,\r\n    line_thickness=2,\r\n    min_score_thresh=0.30)\r\n\r\n# All the results have been drawn on image. Now display the image.\r\ncv2.imshow('Object detector', image)\r\n\r\n# Press any key to close the image\r\ncv2.waitKey(0)\r\n\r\n# Clean up\r\ncv2.destroyAllWindows()\r\n```", "Try Feature Pyramid Network on top of Faster R-CNN. They are good at recognizing objects at vastly different scales. ", "> Try Feature Pyramid Network on top of Faster R-CNN. They are good at recognizing objects at vastly different scales.\r\n\r\nThanks for your reply. Could you give a link where I can study about this? Desirable with a real example?\r\n\r\nI understand correctly that you think that the problem is with the model, not with something else", "> > Try Feature Pyramid Network on top of Faster R-CNN. They are good at recognizing objects at vastly different scales.\r\n> \r\n> Thanks for your reply. Could you give a link where I can study about this? Desirable with a real example?\r\n> \r\n> I understand correctly that you think that the problem is with the model, not with something else\r\n\r\nhttps://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\r\n\r\nhttps://github.com/DetectionTeamUCAS/FPN_Tensorflow", "> > > Try Feature Pyramid Network on top of Faster R-CNN. They are good at recognizing objects at vastly different scales.\r\n> > \r\n> > \r\n> > Thanks for your reply. Could you give a link where I can study about this? Desirable with a real example?\r\n> > I understand correctly that you think that the problem is with the model, not with something else\r\n> \r\n> https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c\r\n> \r\n> https://github.com/DetectionTeamUCAS/FPN_Tensorflow\r\n\r\nThank you so much. I started training, I'm testing.\r\n\r\nQuestion: I have a photo where I mark one item, but there is another item on it that I have not marked. Could this affect the bottom line?\r\n\r\nIn other words, I can have 2 identical photos where the first one is marked with PRODUCT1, and the second with PRODUCT2.\r\n\r\nOr if I add a photo to the tutorial, do I need to mark all the goods on it?\r\nExample: There are two products in this ![photo](https://user-images.githubusercontent.com/32834586/88616650-5c960580-d09d-11ea-8794-d636c9e1f930.png), but I only have one tagged. And on another photo, the item in a red frame will be marked, and the item above is not marked.\r\nThis does not mean that all my photos are the same. There are just coincidences and I do not mark all the goods on them.", "You should mark all the product you are detecting because if your network finds some similar product in the picture which are not marked but should be then the weights will adjust accordingly and accuracy may decrease.", "@hashtriton As this issue has been resolved, can we close this issue?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41740, "title": "tf.estimator. adjust learning rate by loss on validation set", "body": "dear all, \r\n\r\n- by using tf.estimator, how to adjust learning rate by loss on validation set ?\r\n\r\n> for example, if the loss in not decreasing, i want to half learning rate\r\n\r\nthanks", "comments": ["@psy2013GitHub \r\n\r\nPlease, go through [link1](https://stackoverflow.com/questions/45844320/how-to-use-a-decaying-learning-rate-with-an-estimator-in-tensorflow),[link2](https://stackoverflow.com/questions/61994141/loss-and-learning-rate-scaling-strategies-for-tensorflow-distributed-training-wh),[link3](https://github.com/ibab/tensorflow-wavenet/issues/267) and see if it helps you.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "> @psy2013GitHub\r\n> \r\n> Please, go through [link1](https://stackoverflow.com/questions/45844320/how-to-use-a-decaying-learning-rate-with-an-estimator-in-tensorflow),[link2](https://stackoverflow.com/questions/61994141/loss-and-learning-rate-scaling-strategies-for-tensorflow-distributed-training-wh),[link3](https://github.com/ibab/tensorflow-wavenet/issues/267) and see if it helps you.\r\n> \r\n> This question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!\r\n\r\nThanks a lot", "@psy2013GitHub \r\n\r\nPlease feel free to close the issue if resolved.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41739, "title": "Confusion regarding TensorFlow licensing", "body": "## URL(s) with the issue: \r\nhttps://www.tensorflow.org/install/lang_c\r\nand \r\nhttps://www.tensorflow.org\r\n\r\n## Description of issue (what needs changing):\r\n- There is no text in the documentation regarding the licensing of the C API.\r\n- There is no text in the documentation regarding the licensing of TensorFlow as a whole (see below).\r\n\r\n### Clear description\r\nWhen downloading the tar.gz file of the C API, or when building that C API, it comes with two license files: LICENSE and THIRD_PARTY_TF_C_LICENSES. The second file contains the GPL license for everything under ./bench/btl.\r\n\r\nYet, when looking at the package, there is no such path. Actually, this path is not available also in the source code of TensorFlow. This is confusing and may lead to the conclusion that this license is not applicable for the C API code or to TensorFlow as a whole and is provided by mistake.  \r\n\r\nHowever, when compiling the TensorFlow code, such a path is created. Yet, still not in the package of the C API.\r\n\r\nCould this be clarified: what is the license of the C API libraries? Is it allowed to link these libraries to commercial code without being exposed to GPL? \r\n\r\nAnd what is the license of TensorFlow code as a whole? Is it allowed to, for example, use TensorFlow code as part of a commercial Python application without being exposed to GPL?\r\n\r\nI think that this information should come in the above mentioned URLs - the page about C API and the main page of TensorFlow.", "comments": ["All files start with a license header https://github.com/tensorflow/tensorflow/blob/76319741cd303273a542eae0cdf78df61e2c4e83/tensorflow/c/c_api.h#L1-L14\r\n\r\nAdding a few more people to answer the questions", "@lamberta for potential API doc consequence.\r\n\r\nThere is no strict need for the documentation to specify the license as it is mentioned in the source.\r\nHowever, for ease of users recognizing this it's worth considering making this information obvious in documentation.\r\n", "All files indeed start with a license header. This is correct. But if we look at the example given - the C API library - one will have to go through many many files to check which licenses are used. Moreover, it seems that with respect to the specific issue mentioned above, the Eigen library can be compiled using the EIGEN_MPL2_ONLY flag. And this flag is used in the TensorFlow code, but I am not sure how exactly and if it is used everywhere. The bench/btl that is mentioned above seem to be related to comparing performance and running benchmarks, so probably it does not have to be included when building the C API library or the rest of TensorFlow. But how can a user that download these libraries, when they are already built, know if it is included or not?  \r\n\r\n", "For docs and code samples. this is the text at the footer of each developer page (such as https://www.tensorflow.org/install/lang_c):\r\n\r\n> Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\r\n", "None of the explanations above address the most important issue here: when downloading one of the *compiled* packages from https://www.tensorflow.org/install/lang_c, it comes with a THIRD_PARTY_TF_C_LICENSES file that contains, among other things, the following:\r\n> Following applies to:\r\n>   everything under ./bench/btl\r\n> \r\n>                     GNU GENERAL PUBLIC LICENSE\r\n>                        Version 3, 29 June 2007\r\n> ...\r\n\r\nNowhere it is mentioned how the package itself was compiled. This leaves the user with doubts with respect to the licensing of that package. As licensing is a delicate matter, I think that this should be clarified both in the packages themselves and in the page that provides the link to download them.\r\n", "That is the Eigen license https://github.com/tensorflow/tensorflow/blob/57395f70dbc24d3ac5f97626520194f6773303e8/third_party/eigen3/LICENSE#L1013-L1017\r\n\r\nHowever, we don't import those benchmark files. https://github.com/tensorflow/tensorflow/tree/57395f70dbc24d3ac5f97626520194f6773303e8/third_party/eigen3", "Thank mihaimaruseac - you write that the Eigen benchmark files are not imported. Does this mean that the downloads in https://www.tensorflow.org/install/lang_c are built without them? But then, why do the .zip files contain that GPL license in the THIRD_PARTY_TF_C_LICENSES ? \r\n\r\nIf the DLL is built without the Eigen files, then the GPL license is not applicable for this download, and should be removed from the THIRD_PARTY_TF_C_LICENSES file. \r\n\r\nSo the question here is simple but crucial for companies that need to work with the TensorFlow C API library - what licenses are applicable to the DLL that is downloaded?\r\n\r\n", "When looking further into this, I find that in https://github.com/tensorflow/tensorflow/blob/master/third_party/eigen.BUILD it is mentioned that \r\n\r\n> \"Eigen is an MPL2 library that includes GPL v3 and LGPL v2.1+ code. We've taken special care to not reference any restricted code.\"\r\n\r\n> \"Guarantees that any non-MPL2 file added to the list above will fail to compile\", \r\n\r\nand\r\n\r\n> \"This define (mostly) guarantees we don't link any problematic code. We use it, but we do not rely on it, as evidenced above\". \r\n\r\nIt is therefore looks like it is simply a mistake to include the GPL in the THIRD_PARTY_TF_C_LICENSES file. \r\n\r\nCould you please remove the GPL mention from that file, or explain why it is included there?\r\n", "Could you please react to my last comment above? This issue is a blocking issue for me. \r\n\r\nI have just checked the and the file libtensorflow-cpu-windows-x86_64-2.4.0.zip (of the new TensorFlow 2.4.0 release) still contain the mention of GPL with respect to bench/btl.\r\n\r\n(please note also that the page https://www.tensorflow.org/api_docs/python/tf is not updated with the correct links to the latest builds).  ", "@theadactyl can you take a look?", "Hi @rani-pinchuk thanks for explaining this issue to us. We will take a look at this in the next couple of days as the team returns from holiday. ", "Thanks, all.\r\nhttps://github.com/tensorflow/tensorflow/commit/8ab9ab6217aba0b771f05b3bd883ee33ae860931 clarifies that TensorFlow has taken special care to not reference any restricted code and removes the GPL from that license section."]}, {"number": 41738, "title": "TensorFlow Lite currently doesn't support control flow ops", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): 1.15\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n\r\n```\r\nTensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RESHAPE, REVERSE_V2, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, LoopCond, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArrayV3, TensorArrayWriteV3.\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\toco_from_protos-script.py\", line 10, in <module>\r\n    sys.exit(main())\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 93, in main\r\n    app.run(main=execute, argv=[sys.argv[0]] + unparsed)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\absl\\app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\absl\\app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\lite\\toco\\python\\toco_from_protos.py\", line 56, in execute\r\n    enable_mlir_converter)\r\nException: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md\r\n and pasting the following:\r\n\r\nTensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, FULLY_CONNECTED, LESS, LOGICAL_AND, LOGISTIC, MAX_POOL_2D, MUL, RESHAPE, REVERSE_V2, SPLIT, TANH, TRANSPOSE. Here is a list of operators for which you will need custom implementations: CTC_BEAM_SEARCH_DECODER, LoopCond, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArrayV3, TensorArrayWriteV3.\r\n```\r\n\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n", "comments": ["TFLite supports v2 control flow ops only. Could you try the conversion after enabling https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_control_flow_v2 while creating a TF graph?", "@sushantag9 \r\nPlease update as per above comment.", "I'll try the suggested changes and update the issue.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41736, "title": "In Ubuntu 20.04 not able to register GT 730 GPU - Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file", "body": "**System information**\r\n- OS Platform and Distribution  Linux Ubuntu 20.04\r\n\r\n- TensorFlow installed with following command :\r\n\r\n`pip3 install --upgrade tensorflow`\r\n\r\n- TensorFlow version:\r\n\r\n`2.2.0`\r\n\r\n- Python version: 3.8.2\r\n\r\n- Installed using virtualenv? pip? conda?:\r\npip3\r\n\r\n- GCC/Compiler version (if compiling from source):\r\n\r\n- CUDA/cuDNN version:\r\n\r\n`nvcc --version` gives me below \r\n\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Wed_Oct_23_19:24:38_PDT_2019\r\nCuda compilation tools, release 10.2, V10.2.89\r\n\r\n```\r\n\r\nAnd \r\n\r\n\r\n```\r\n$ cat /usr/local/cuda/version.txt\r\nCUDA Version 11.0.207\r\n```\r\n\r\nAnd also \r\n\r\nCheck for installed CUDA toolkit package:\r\n\r\n```\r\n$ dpkg -l | grep cuda-toolkit\r\nii  cuda-toolkit-11-0                             11.0.2-1                                  amd64        CUDA Toolkit 11.0 meta-package\r\nii  nvidia-cuda-toolkit                           10.1.243-3                                amd64        NVIDIA CUDA development toolkit\r\n\r\n```\r\n\r\nAnd \r\n\r\n```\r\ngcc --version\r\ngcc (Ubuntu 9.3.0-10ubuntu2) 9.3.0\r\nCopyright (C) 2019 Free Software Foundation, Inc.\r\n```\r\n\r\n\r\n- GPU model and memory:\r\n\r\n GeForce GT 730 computeCapability: 3.5\r\n\r\n**Describe the problem**\r\n\r\nFor installing the cuDnn, I strictly followed [this SO ans](https://askubuntu.com/a/1251052/308359) by creating a developer account - \r\n\r\n## MAIN ISSUE IS - After running Tensorflow code getting\r\n\r\n### Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\r\n\r\nAnd below is the full log of the error in the terminal after a running a single .py file with Tensorflow\r\n\r\n```\r\n020-07-26 05:36:40.742760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-26 05:36:40.774143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-26 05:36:40.774446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GT 730 computeCapability: 3.5\r\ncoreClock: 0.9015GHz coreCount: 2 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 11.92GiB/s\r\n2020-07-26 05:36:40.774705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-26 05:36:40.776037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-26 05:36:40.776832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-26 05:36:40.777094: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-26 05:36:40.779055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-26 05:36:40.779835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-26 05:36:40.779996: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\r\n2020-07-26 05:36:40.780007: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n2020-07-26 05:36:40.780309: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-07-26 05:36:40.785339: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3392485000 Hz\r\n2020-07-26 05:36:40.785677: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f71a4000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-26 05:36:40.785698: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-26 05:36:40.787154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-26 05:36:40.787171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]    \r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["It seems that I met the similar problem, too.\r\nAll problems are related to cudnn, which I couldn't find a usable ```.deb``` version for Ubuntu 20.04 LTS on NVIDIA.\r\nSo I follow the tutorials on the Internet and operate directly on the cudnn library.\r\nBut something wrong happens like @rohan-paul mentioned above.\r\nThis is what I got when I run the ```mnist_cnn.py```\r\n```\r\nInvalid MIT-MAGIC-COOKIE-1 keyx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\n2020-07-26 14:12:51.958717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-07-26 14:12:52.005335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-26 14:12:52.006557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce GTX 1650 SUPER computeCapability: 7.5\r\ncoreClock: 1.725GHz coreCount: 20 deviceMemorySize: 3.82GiB deviceMemoryBandwidth: 178.84GiB/s\r\n2020-07-26 14:12:52.009482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-26 14:12:52.058634: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-07-26 14:12:52.074376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-07-26 14:12:52.080761: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-07-26 14:12:52.106325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-26 14:12:52.115398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-07-26 14:12:52.115994: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\r\n2020-07-26 14:12:52.116040: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n```", "@rohan-paul,\r\nPlease take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu) and try installing TensorFlow with CUDA 10.1 and cuDNN 7.6.\r\n\r\nAlso, please check [this similar](https://github.com/tensorflow/tensorflow/issues/20271) issue and let us know if it helps. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41736\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41736\">No</a>\n"]}, {"number": 41735, "title": "[MLIR:LITE] Verify unpack op", "body": "Please inform me if any check is missing :-)\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/unpack.cc#L34-L83", "comments": []}, {"number": 41734, "title": "[Wsign-compare] warning resolutions, by directory, 1", "body": "Resolution affecting directories: `\r\n`tensorflow/compiler/jit`", "comments": ["@mihaimaruseac ", "updates applied, 2. "]}, {"number": 41733, "title": "Implemented TFLogSinks", "body": "- added a `no_default_logger` config setting\r\n- changed `TFLogEntry::log_severity()` to `TFLogEntry::Severity()`\r\n- changed `TFLogEntry::ToString()` to `TFLogEntry::Message()`\r\n- added `TFLogEntry::FName()` and `TFLogEntry::Line()`\r\n- added `TFDefaultLogSink`\r\n- `TFAddLogSink` and `TFRemoveLogSink` now work\r\n- Issue #37390", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41733) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F41733) for more info**.\n\n<!-- ok -->", "Rohan, adding you because it looks like you expressed interest in reviewing in the attached issue. But please comment if that's changed.\r\n\r\nRohan is currently out, so I wouldn't expect an immediate response.", "@rohan100jain  Any update on this PR? Please. Thanks!", "@rohan100jain  Any update on this PR? Please. Thanks!", "@rohan100jain Any update on this PR? Please. Thanks!", "Come on y'all. This has been pending since July. Any updates?\r\n", "Look, I know this is not the most sophisticated logging scheme: this will work just the same for what it already did and allow users to actually redirect TF logging. It is not an asynchronous logging system, so adding multiple sinks will degrade performance. However, this implements the interface that existed in TF for years and lied about it being implemented the whole time. \r\n\r\nIf you have any questions about whether the code is valid, I'll certainly answer them, but I don't understand why this is taking so long to approve and why so many people want to look at it before merging. Sure, it adds critical functionality to TF, but this is textbook stuff, not exactly rocket science. At the end of the day, it doesn't change the existing behavior one bit, and it's a starting point for someone to make this a better logging system in the future if you ever decide to dedicate resources to this. This is one of many patches I already have to apply to our TensorFlow build, please make this one less.\r\n\r\nP.S. We actually back this by a proper, high performance logger, as I think most users would. A logging hook like this is absolutely critical for any big library.", "I too am interested in seeing this make it into tensorflow. This has been needed for a while.", "Alright, I see the CI now! \r\n\r\nSorry about the broken build: I was merging this from a patch to 1.15.4, must have made a mistake with that. Will fix today.", "OK, fixed. Everything should build cleanly now.", "@kkimdev, I addressed all of your PR comments. The ones I fixed, I marked with :+1: , the ones that I don't think should be changed, I marked with :-1: , and there's one on which I need more feedback after you read my reply, it's marked with :confused:\r\n\r\nThe ones marked with :-1: don't mean \"I refuse\" to change them. I just felt pretty sure that you would agree that they were fine as is once I provided more information.", "I'm not clear on why all the tests failed in that build. I'm not familiar with looking through Bazel build logs: the only thing I saw was a timeout, I didn't seen any build failures. I don't know if these verification builds isolate one PR or multiple ones, I also don't know whether test build failures just show up as test failures. Please advise.", "@kkimdev, ~looks like your change to `assert` and print `?` instead of `I` is not the expected behavior. I'm going to revert that bit and see if that fixes things.~ That's incorrect, it's missing the `info` clause, since it was previously the same as `default`. Adding that in...\r\n", "@kkimdev, would you kick the builds again? It should be good to go now I think. Sorry, I simply don't have the setup to test everything locally.", "@kkimdev, I fixed everything I saw in the test builds so far. I've never actually built the Android target, so hopefully it's all there now. The only resource I have to get feedback on those changes are your CI builds. I fixed everything I saw in the CI, but I'm working blindly there. Linux builds should hopefully be fine now after I corrected for that Buildifier check.", "@kkimdev, I don't know if it's expensive to run those builds, but would you kick them again if possible? I think they should all pass or be very close with the latest changes...", "I'm seeing build failures on MacOS and Windows, but they don't look like they have anything to do with this PR. Should I just ignore these? Should I merge `master` again?", "> I'm seeing build failures on MacOS and Windows, but they don't look like they have anything to do with this PR. Should I just ignore these? Should I merge `master` again?\r\n\r\nYeah please ignore those failures now.  I'm quite busy today but will try to take a look and comment later today.  Thanks for bearing with us!", "> > I'm seeing build failures on MacOS and Windows, but they don't look like they have anything to do with this PR. Should I just ignore these? Should I merge `master` again?\r\n> \r\n> Yeah please ignore those failures now. I'm quite busy today but will try to take a look and comment later today. Thanks for bearing with us!\r\n\r\nNo worries. I think we're about done. Once you look at my comments and make your decisions I think we can wrap it up pretty soon.", "Please leave a comment when it's ready to be reviewed.  Thanks!", "I think I've addressed everything now. Ready to be reviewed", "@kkimdev \r\nOK, this is hopefully my last checkin for this branch. I've merged `master` and cleaned up the test code a bit. Everything should be there now (unless I missed something).\r\n", "One question, I'll be backporting this into the 1.15 branch, and probably the 2.3 branch. Do you want PRs on those (after this one's wrapped up)?", "@kkimdev, sorry, I didn't notice your other comment, so I made two checkins instead of one. I think you'll need to re-kick the CI.", "@kkimdev , all your comments have been addressed. It should all be there now...\r\n", "@gbaned I don't see the corresponding CL internally, is this because it didn't pass all the Kokoro tests?", "Update: getting internal reviews now, we should be able to land after getting one more lgtm internally! :)", "@avitebskiy I had to update quite a bit to be consistent with internal logger behavior and just removed linux tests that didn't work internally for some reason, but the functionality should be merged and available now.  In case if you have a follow-up changes, please keep it coming.  Thanks!"]}, {"number": 41731, "title": "ValueError: The first argument to `Layer.call` must always be passed.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["@ChrisChaw \r\n\r\nRequest you to fill [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nPlease, share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "I am also facing a similar issue. \r\n\r\nWhenever I try to pass a neural network model into cross_val_score, I see \"ValueError: The first argument to `Layer.call` must always be passed.\"\r\n\r\nWould appreciate if someone can address this please, thank you!"]}, {"number": 41730, "title": "Prefer TensorShape(...) over as_shape(...) when passing lists", "body": "Directly calling `tensor_shape.TensorShape(...)` is equivalent to `tensor_shape.as_shape(...)` when passing tuples or lists. This PR changes the usage to prefer the former.", "comments": []}, {"number": 41729, "title": "I'm getting this error. Can you help me out?", "body": "tensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol Zeros is already exposed as ().", "comments": ["Please fully fill in the issue template. Please provide a minimal reproducing code.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41728, "title": "run tht tfmot.quantization.keras.quantize_model(model) and raise the error \"`model` must be a built model. been built yet\" while model.built is True", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- TensorFlow version (use command below):2.1.0\r\n- Python version:3.7.7\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI write my own `GRUCell` and create a model by `keras.RNN()`.  Here's my model\r\n```python\r\nclass GRUCell(keras.layers.Layer):\r\n    def __init__(self, units, **kwargs):\r\n        self.units = units\r\n        self.state_size = tf.TensorShape([units])\r\n        self.output_size = tf.TensorShape([units])\r\n        super(GRUCell, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.dim = input_shape[-1]\r\n\r\n        self.w_r = self.add_weight(shape=[self.dim+self.units, self.units], initializer='uniform', name='reset_gate', trainable=True)\r\n        self.b_r = self.add_weight(shape=[self.units], initializer='zeros', name='reset gate bias', trainable=True)\r\n\r\n        self.w_z = self.add_weight(shape=[self.dim+self.units, self.units], initializer='uniform', name='update_gate', trainable=True)\r\n        self.b_z = self.add_weight(shape=[self.units], initializer='zeros', name='update gate bias', trainable=True) \r\n\r\n        self.w_n = self.add_weight(shape=[self.dim+self.units, self.units], initializer='uniform', name='intetim', trainable=True)\r\n        self.b_n = self.add_weight(shape=[self.units], initializer='zeros', name='interim gate bias', trainable=True)\r\n\r\n        self.build = True\r\n\r\n\r\n    def call(self, inputs, states):\r\n        \r\n        states, = states\r\n        \r\n        r = tf.nn.sigmoid(inputs @ self.w_r[:self.dim] + states @ self.w_r[self.dim:] + self.b_r)\r\n        z = tf.nn.sigmoid(inputs @ self.w_z[:self.dim] + states @ self.w_z[self.dim:] + self.b_z)\r\n        n = tf.nn.tanh(inputs @ self.w_n[:self.dim] + (states * r) @ self.w_n[self.dim:] + self.b_n)\r\n        \r\n        output = (1 - z) * states + z * n\r\n\r\n        return output, output\r\n\r\n    def get_config(self):\r\n        return {\"units\": self.units}\r\n\r\ndef create_model(units):\r\n    model = keras.Sequential([\r\n            keras.layers.RNN(GRUCell(units)),\r\n            keras.layers.Dense(1)\r\n            ])\r\n    model.compile(optimizer=keras.optimizers.RMSprop(),\r\n                loss='mae', metrics=['mse'])\r\n\r\n    return model\r\n```\r\nThis model works well.\r\n![](https://raw.githubusercontent.com/ChenHaoHere/PicGo/master/20200725222620.png)\r\nBut when I'd like to use the snippet from (https://www.tensorflow.org/model_optimization/guide/quantization/training_example), the error **ValueError: `model` must be a built model. been built yet. Please call `model.build(input_shape)` before quantizing your model.** was raised. But `model.built` is True.\r\n\r\n![](https://raw.githubusercontent.com/ChenHaoHere/PicGo/master/20200725223938.png)\r\n\r\n\r\n", "comments": ["@ChenHaoHere \r\nI ran the code shared but face a different issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/8ef662ce3db69aecb3a4c61bba6ccf3e/untitled288.ipynb). Can you share complete stand alone code such that we can replicate error faced or if possible share a colab gist with the error.\r\n\r\nWith respect to error shared, please share error log in text format, and refer to similar issue [link](https://stackoverflow.com/questions/55908188/this-model-has-not-yet-been-built-error-on-model-summary).", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41728\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41728\">No</a>\n"]}, {"number": 41727, "title": "Tensorflow using Intel GPU instead of NVIDIA GPU", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home 64-bit\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 10.1/7.6\r\n- GPU model and memory: NVIDIA GTX 1050 (4 GB) \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTensorFlow, by default is using GPU 0, which on my PC is my built-in Intel GPU while I have a dedicated NVIDIA GPU and have CUDA installed yet TF is using Intel.\r\n\r\n**Describe the expected behavior**\r\nTensorFlow should display current device to be NVIDIA GPU\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n![Annotation 2020-07-25 190353](https://user-images.githubusercontent.com/35456031/88458751-b8983880-cea9-11ea-8237-44bcc66eef19.png)\r\n![Annotation 2020-07-25 190430](https://user-images.githubusercontent.com/35456031/88458754-bafa9280-cea9-11ea-9375-de1ba6feec2b.png)\r\n\r\n", "comments": ["@tamimmirza \r\n\r\nCan you list all gpus by following the below link and then manually place the gpu you need.\r\nhttps://www.tensorflow.org/guide/gpu#setup\r\n\r\nThanks!\r\n", "I did that before and it shows me that it has detected one GPU despite me having two as you can see in the task manager windows above:\r\n\r\n![Annotation 2020-07-27 164537](https://user-images.githubusercontent.com/35456031/88538614-18691d80-d029-11ea-8672-5543a5ae77ee.png)\r\n\r\nWhich is weird considering I have installed up-to-date drivers and CUDA Toolkit 10.1 Update 2 (along with cuDNN 7.6)\r\n\r\n![Annotation 2020-07-27 164719](https://user-images.githubusercontent.com/35456031/88538664-2e76de00-d029-11ea-9df4-672b20dd33d9.png)\r\n\r\nHere is the verification that CUDA is installed via Visual Studio 2019 IDE\r\n\r\n![Annotation 2020-07-27 165055](https://user-images.githubusercontent.com/35456031/88538806-6bdb6b80-d029-11ea-89e8-8fca793b850a.png)\r\n", "@tamimmirza \r\n\r\nPlease try install TensorFlow gpu using the below command \r\n`pip install tensorflow-gpu==2.2.0` and check if you are still facing the same issue. Thanks! ", "I did that prior to opening the issue\r\nI uninstalled `tensorflow` and installed `tensorflow-gpu` and I still got the same result\r\n\r\nI also did a fresh install of Miniconda\r\n\r\nUpdate: With the release of TF 2.3.0 I am opting to fresh install Miniconda and create a new environment with TF 2.3.0 and test it further", "I installed TensorFlow 2.3.0 along with a fresh install of Miniconda3 and it solved my problem and correctly identifies the proper GPU\r\n\r\n![Annotation 2020-07-28 173250](https://user-images.githubusercontent.com/35456031/88665704-7add1f00-d0f8-11ea-8e5e-d722d6110e9d.png)"]}, {"number": 41726, "title": "Directly call sample_distorted_bounding_box_v2 to prevent deprecation warnings", "body": "`sample_distorted_bounding_box` is deprected and the warning recommends using `sample_distorted_bounding_box_v2` instead:\r\nhttps://github.com/tensorflow/tensorflow/blob/055c5e107db5a3b7b0899d8974e14ab6ce772fc0/tensorflow/python/ops/image_ops_impl.py#L2942-L2946\r\n\r\nHowever, `sample_distorted_bounding_box_v2` internally calls `sample_distorted_bounding_box` so users will see deprecation warnings in both cases.\r\n\r\nThis PR changes `sample_distorted_bounding_box_v2` to directly call the underlying C++ op to prevent deprecation warnings from being raised.", "comments": []}, {"number": 41725, "title": "Tensorflow serving docker example", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.4\r\n- GPU model and memory: GEForce GT 730, 2GB\r\n\r\n\r\n**Describe the current behavior**\r\nI am trying out the example code of docker based tensorflow serving basic example as provided  in\r\nhttps://www.tensorflow.org/tfx/serving/docker\r\nCurrent behaviour is that the docker throws an error. The error is mentioned at the end of this report.\r\nI suspect that this is docker problem, but i did not find any minimum docker version mentioned in document. However I am trying to upgrade docker on my computer. Current  version is \"Docker version 1.13.1, build 64e9980/1.13.1\"\r\n\r\n**Describe the expected behavior**\r\nThe expected behaviour as per the example is to get an output as described in the tutorial.\r\n# Returns => { \"predictions\": [2.5, 3.0, 4.5] }\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n# Download the TensorFlow Serving Docker image and repo\r\ndocker pull tensorflow/serving\r\n\r\ngit clone https://github.com/tensorflow/serving\r\n# Location of demo models\r\nTESTDATA=\"$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata\"\r\n\r\n# Start TensorFlow Serving container and open the REST API port\r\ndocker run -t --rm -p 8501:8501 \\\r\n    -v \"$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two\" \\\r\n    -e MODEL_NAME=half_plus_two \\\r\n    tensorflow/serving &\r\n\r\n# Query the model using the predict API\r\ncurl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\r\n    -X POST http://localhost:8501/v1/models/half_plus_two:predict\r\n\r\n# Returns => { \"predictions\": [2.5, 3.0, 4.5] }\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nThe error message i get is\r\n\r\n2020-07-25 13:10:40.668221: E tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc:362] FileSystemStoragePathSource encountered a filesystem access error: /models/half_plus_two; Permission denied\r\n\r\nI have checked all paths. permission for the serving directory is\r\ndrwxr-xr-x. 7 srikrishnan docker       249 Jul 18 22:40 serving", "comments": ["I upgraded docker. I can verify it is working. Current docker version \r\nDocker version 19.03.12, build 48a66213fe\r\nWould be helpful if a minimum docker version is mentioned explicitly, or maybe i missed it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41725\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41725\">No</a>\n"]}, {"number": 41724, "title": "Tensorflow stuck in training with more than two GPUs", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nNo, I am using the example presented in https://www.tensorflow.org/guide/gpu#using_multiple_gpus\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 18.04.3 LTS\r\n\r\n- TensorFlow installed from (source or binary):\r\nI am using the TensorFlow Docker image tensorflow/tensorflow:latest-gpu-jupyter\r\n\r\n- TensorFlow version (use command below):\r\n2.2.0\r\n\r\n- Python version:\r\n3.6.9\r\n\r\n- CUDA/cuDNN version:\r\nCUDA version: 10.1\r\n- GPU model and memory:\r\nNvidia Tesla P100-SXM2\r\n\r\n\r\n**Describe the current behavior**\r\nWhen I try to run the code below in a jupyter notebook with more than two GPUs it get stuck in the training step. It works fine with two GPUs.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nstrategy = tf.distribute.MirroredStrategy()\r\ninput_values = tf.random.uniform((256,), minval=0, maxval=1, dtype=tf.dtypes.float32)\r\nwith strategy.scope():\r\n    inputs = tf.keras.layers.Input(shape=(1,))\r\n    predictions = tf.keras.layers.Dense(1)(inputs)\r\n    model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\r\n    model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))\r\n    \r\n    model.fit(input_values, input_values, batch_size=32)\r\n```\r\n", "comments": ["Hi @g-soto, I ran this code with 4 GPUs and training completed successfully.\r\nSince I am unable to reproduce, I'm wondering if maybe there's an issue with one of your GPUs. Can you first run a test to make sure each GPU is functioning by running training with a single GPU at a time. If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default, so you'll need to specify which GPU you want utilized. You can do that with tf.config.set_visible_devices to specify which devices are visible to the runtime.", "Hi @nikitamaia, I ran the training on each GPU individually without a problem. ", "Can you try the following:\r\n`strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())`\r\n this is to check whether NCCL is causing problems.", "With the `cross_device_ops=tf.distribute.ReductionToOneDevice()` argument it works perfectly in eight GPUs. Thanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41724\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41724\">No</a>\n"]}, {"number": 41723, "title": "Want to see the CNN Filter (image format)", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n2.2.0\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThere is no such feature for what filter did the model came up while training is completed. Please make a function for it. \r\n\r\n**Will this change the current api? How?**\r\nThe API will become more Awosome than before it was. Present every one is getting accuracy on just predefined test and train cases , but after this feature people will know what their model is predecting in the given image\r\n\r\n**Who will benefit with this feature?**\r\nEspecailly people who are learning like me rather than the people who are just run behind accuracy\r\n**Any Other info.**\r\nPlease If u want any help , do tell me , Big fan of Google's Code Works . And also make a function for it like model.show_filter_con2d() ; ", "comments": ["@UdayKiranPadhy \r\n\r\nDo you have any use case that requires the feature you are interested in? Please feel free to submit a PR if you have use cases that supports that feature.Thanks!", "Do you have any use case that requires the feature you are interested in?\r\nFor example, When i am training a model on a dataset of cats and dogs . I want to know what my model learn from those images. Did it successfully recognize its eyes and ears or not ? Yeah it may be tested using test data set but i want to know on bases on what features the model is predicting the image is cat or a dog\r\n\r\nPlease feel free to submit a PR if you have use cases that supports that feature.Thanks!\r\nWhat is PR and how to submit it . Can u Please provide me a link for it .\r\n\r\nThank You , Have a nice day\r\n", "@UdayKiranPadhy \r\n\r\nCan you please refer the [link](https://www.tensorflow.org/community/contribute/code) to submit PR.Thanks!", "Hey i dont know how to code , i only suggested a feature for tensorflow library , if i knew to code i would implement my self why will i tell to tenserflow , The link which u send me is telling me to contribute via code .", "@UdayKiranPadhy There are many tutorials on how to plot weights. For example, Can you please follow this [resource](https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/). \r\n\r\nPlease feel free to close the issue if this resolved the issue for you. Thanks!", "@UdayKiranPadhy,\r\nIn addition to the resource above, which shows how to visualize Feature Maps and Filters,  Please find [this article](https://www.pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/) which demonstrates how to Visualize Class Activation Maps i.e., what features of an Image contributes to the prediction corresponding to its class. Thanks!"]}, {"number": 41722, "title": "How to convet \"tf.nn.ctc_greedy_decoder\" output into readable format?", "body": "I am using `tf.nn.ctc_greedy_decoder` to decode the text but I am not able to read `decoder[0].indices`. how to read this decoder data (or how to convert it to NumPy)?\r\n\r\ncode:\r\n`\r\ndecoder, _ = tf.compat.v1.nn.ctc_greedy_decoder(inputs=inputs, \r\n\t\t\t\t\t\t\t\t\t\t\t\t\tsequence_length=seqLen, \r\n\t\t\t\t\t\t\t\t\t\t\t\t\tmerge_repeated=True)\r\nprint(decoder[0].indices)\r\n`\r\n\r\n\r\n", "comments": ["@vishalthengane,\r\nIn order to expedite the trouble-shooting process, could you please provide the TensorFlow version, complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 41721, "title": "Added a \"note\" in tf.where documentation suggesting a workaround for issue #38349", "body": "Added a note in nt.where documentation...", "comments": ["can anyone please explain this to me", "> can anyone please explain this to me\r\n\r\nHow can I help you?", "> > can anyone please explain this to me\r\n> \r\n> How can I help you?\r\n\r\nActually I am new to this issue and request stuff, where can i find proper documentation on dealing with this.", "This does not look like a valid PR. Closing.\r\n\r\nFor contributing, see https://www.tensorflow.org/community/contribute", "> Nothing to review\r\n\r\nDear @mihaimaruseac \r\nI'm a regular contributor... and I tried to follow contribution guidelines...\r\nAccording to @mdanatg suggestion, (please see issue #38349 ) this contribution is just a workaround note meanwhile me and any other is interested  are working on a complete solution (not so easy to find)...\r\nThis is the reason why there is no code to review.\r\nWhere am I wrong?\r\nThank you.", "> > > can anyone please explain this to me\r\n> > \r\n> > \r\n> > How can I help you?\r\n> \r\n> Actually I am new to this issue and request stuff, where can i find proper documentation on dealing with this.\r\n\r\nI'm not sure to get a correct understanding of your question...\r\n\r\nAll information were into the issue #38349 description.\r\nI just followed Dan Moldovan's suggestion to split the work in two: (a) to patch the documentation and (2) to address the issue (because it looks like very hard...)\r\nFirs part was trivial and I submitted a Pull Request wit a few lines...\r\nIf you wish, we can work together on the second part. I'll be proud of it.\r\n\r\nBy the way, I don't understand why mr. Mirai Marusac closed the PR and I asked him a clarification...\r\n", "@codeadmin-peritiae The PR appears to be completely empty - it should at least contain the documentation updates.", "Apologies @codeadmin-peritiae. The PR looked empty, no code changes were present", "> Apologies @codeadmin-peritiae. The PR looked empty, no code changes were present\r\n\r\nIt's me I have to apologise for...\r\nMy Sourcetree GitHub client had went in trouble with a SSH certificate...\r\nI re-submitted my contribution as PR #41775 \r\nThanks a lot for your support.\r\nG. ", "> @codeadmin-peritiae The PR appears to be completely empty - it should at least contain the documentation updates.\r\n\r\nI'm sorry, please see my comment above. Thanks. G."]}, {"number": 41720, "title": "golang: array bound is too large at raspberry pi", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry PI 3B+ / Buster\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): Self build\r\n- TensorFlow version (use command below): 2.3.0-rc2\r\n- Python version:\r\n- Bazel version (if compiling from source): default from cross compile container\r\n- GCC/Compiler version (if compiling from source): default from cross compile container\r\n- CUDA/cuDNN version: No CUDA\r\n- GPU model and memory: No GPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\n$ go test github.com/tensorflow/tensorflow/tensorflow/go\r\n# github.com/tensorflow/tensorflow/tensorflow/go [github.com/tensorflow/tensorflow/tensorflow/go.test]\r\ntensorflow/go/graph.go:123:13: array bound is too large\r\ntensorflow/go/graph.go:123:20: constant 1125899906842623 overflows int\r\ntensorflow/go/tensor.go:349:13: array bound is too large\r\ntensorflow/go/tensor.go:349:20: constant 1125899906842623 overflows int\r\nFAIL    github.com/tensorflow/tensorflow/tensorflow/go [build failed]\r\n```\r\n**Describe the expected behavior**\r\ngo test should pass\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["TensorFlow Go is currently supported on Linux/macOS versions only.\r\nSee https://www.tensorflow.org/install/lang_go#supported_platforms", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41720\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41720\">No</a>\n"]}, {"number": 41719, "title": "[MLIR:LITE] Fix PackOp verify", "body": "Axis can be in [-rank - 1, rank + 1), but the original codes only verify [-rank, rank + 1).\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/pack.cc#L41-L47\r\n\r\nThe test case seems to be wrong. For rank 2 tensors, axis=-3 is equal to axis=0, axis=-2 is equal to axis=1.\r\n\r\nhttps://colab.research.google.com/drive/1WRPjkdv70njl1uHlGAh_J7xp0v1QH11j?usp=sharing", "comments": []}, {"number": 41717, "title": "Remove deprecated calls to iterator_ops.get_next_as_optional", "body": "This PR replaces a few deprecated calls to `iterator_ops.get_next_as_optional(...)` with `iterator_ops.OwnedIterator.get_next_as_optional()`. There are still some deprecated calls left in the source code, but there it looks like a simple replacement isn't possible yet.", "comments": []}, {"number": 41716, "title": "Add gpu benchmark examples for other models.", "body": "", "comments": []}, {"number": 41715, "title": "Keras mixed precision API 50x slower than mixed precision graph rewrite", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.2.0, 2.3.0rc2, 2.4.0.dev2020072401\r\n- Python version: 3.7.8\r\n- CUDA/cuDNN version: 10.1 / 7.6.5.32\r\n- GPU model and memory: NVIDIA V100\r\n\r\nI currently use the [mixed precision graph rewrite](https://www.tensorflow.org/api_docs/python/tf/train/experimental/enable_mixed_precision_graph_rewrite) which will apply mixed precision optimizations to the entire TF graph. Since 2.1 there is a [new Keras API](https://www.tensorflow.org/guide/keras/mixed_precision) which replaces the graph rewrite and provides layerwise customization of the dtype policy. I tried switching to the new API and ran into the following issue.\r\n\r\n**Describe the current behavior**\r\n\r\nThe Keras mixed precision API is 50 times slower than the legacy graph rewrite and the float32 baseline. When running the code example below on a NVIDIA V100 I get the following runtimes:\r\n\r\n|  | float32 | Keras AMP | Keras AMP (float32 depthwise) | AMP graph rewrite\r\n--- | --- | --- | --- | ---\r\ntime / step | 120ms  | **2000ms** | 38ms | 37ms\r\ntime / epoch | 6s | **90s** | 2s | 2s\r\n\r\n**Describe the expected behavior**\r\n\r\nPerformance of the Keras mixed precision API should be on par with the graph rewrite and should outperform normal float32 training.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\npolicy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\r\ntf.keras.mixed_precision.experimental.set_policy(policy)\r\n\r\n\r\ndef preprocessing(data):\r\n    return tf.cast(data[\"image\"], tf.float32) / 255.0, data[\"label\"]\r\n\r\n\r\ndataset = (\r\n    tfds.load(\"cifar10\", split=\"train\")\r\n    .map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n    .cache()\r\n    .batch(1024)\r\n    .prefetch(1)\r\n)\r\n\r\nmodel = tf.keras.models.Sequential(\r\n    [\r\n        tf.keras.layers.Conv2D(8, 3, padding=\"same\", activation=\"relu\", input_shape=(32, 32, 3)),\r\n        tf.keras.layers.DepthwiseConv2D(3, depth_multiplier=8, padding=\"same\", activation=\"relu\"),\r\n        tf.keras.layers.GlobalAveragePooling2D(),\r\n        tf.keras.layers.Dense(10),\r\n        tf.keras.layers.Activation(\"softmax\", dtype=\"float32\"),\r\n    ]\r\n)\r\n\r\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\r\n\r\nmodel.fit(dataset, epochs=3, callbacks=[tf.keras.callbacks.TensorBoard(\"logs\")])\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis big slowdown is cause by the depthwise convolution in the model which takes up the bulk of the computation in the toy model mentioned above. When looking at the generated profile the issue it looks like Keras AMP runs the very slow\r\n```cpp\r\nDepthwiseConv2dBackpropFilterGPUKernelNHWC(DepthwiseArgs, Eigen::half const*, Eigen::half const*, Eigen::half*, int)\r\n```\r\nkernel whereas `float32` runs\r\n```cpp\r\nDepthwiseConv2dBackpropFilterGPUKernelNCHW(DepthwiseArgs, float const*, float const*, float*, int)\r\n```\r\nand the AMP graph rewrite executes\r\n```cpp\r\nDepthwiseConv2dBackpropFilterGPUKernelNHWC(DepthwiseArgs, float const*, float const*, float*, int)\r\n```\r\nIt looks like the `float32` versions of the kernels are a lot faster, although I am not sure why the baseline uses `NCHW` instead of `NHWC`.\r\n\r\nThe underlying issue is definitely the unusable `float16` `DepthwiseConv2dBackpropFilter` kernel (see also #27780), although I think it would be good to automatically workaround this in Keras AMP or grappler as well since this makes Keras AMP not usable for networks with depthwise convolutions.\r\n\r\n/cc @reedwm \r\n", "comments": ["For anyone stumbling across this issue as well, one can workaround the problem by explicitely setting `dtype=\"float32\"` for `tf.keras.layers.DepthwiseConv2D`.", "@lgeiger I also got this problem on both DepthWiseConv and GroupConv, seem mixed-precision not yet optimize for those layers. Also, I see in cuDNN 8.0 releases notes states that mixed-precision now support for GroupConv, I don't know if it also fixes DepthWiseConv. (https://docs.nvidia.com/deeplearning/sdk/cudnn-release-notes/rel_8.html). BTW, there are so many ways to do mixed in TF2 now, I always use:\r\n\r\n```\r\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\r\n```\r\n\r\nthen wrap optimizer by:\r\n```\r\ntf.keras.mixed_precision.experimental.LossScaleOptimizer\r\n```", "@lgeiger,\r\nIs this still an issue? Could you please let us know if @dathudeptrai's workaround has fixed the issue. Thanks!", "@amahendrakar yes, this is still an issue. As mentioned in the issue description switching to an entirely different API is a valid workaround, but it doesn't fix the underlying issue with the Keras mixed precision API. It is the API that is recommended in most guides on the website so many people will likely run into this issue when trying to follow the guides.", "It looks like the backward filter op is slow with fp16, but not the forward or backward input op. \r\n\r\nI think a grappler pass is changing the data_format, as NHWC is typically faster for fp16 and NCHW is typically faster for fp32. Both data formats are extremely slow in fp16 and fast in fp32, so the data_format issue is a bit of a red herring.\r\n\r\nThe function which is slow in fp16 and fast in fp32 is [DepthwiseConv2dBackpropFilterGPUKernelNHWC](https://github.com/tensorflow/tensorflow/blob/1da0ebb453374dd3b1b30272cda11c27dd9dc276/tensorflow/core/kernels/depthwise_conv_op_gpu.h#L1030). I'm not sure why it's so slow in fp16 though. Maybe the atomics are slower in fp16? CC @chsigg can you fix this?\r\n\r\nAlso the comment at the top of the function makes me worried about the numeric stability:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/1da0ebb453374dd3b1b30272cda11c27dd9dc276/tensorflow/core/kernels/depthwise_conv_op_gpu.h#L1024-L1033\r\n\r\n@nluehr doing accumulation in fp16 is dangerous, right? I think I'll have the Keras API force `DepthwiseConv2D` in float32 when cudnn is not used, both for performance and numeric stability\r\n\r\n\r\n\r\nHere is a benchmark to verify the slowness:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\ndef bench(dtype, data_format, func):\r\n  if data_format == 'NHWC':\r\n    x = tf.random.normal((32, 32, 32, 8), dtype=dtype)\r\n    out_grad = tf.random.normal((32, 32, 32, 64), dtype=dtype)\r\n  else:\r\n    x = tf.random.normal((32, 8, 32, 32), dtype=dtype)\r\n    out_grad = tf.random.normal((32, 64, 32, 32), dtype=dtype)\r\n  f = tf.random.normal((3, 3, 8, 8), dtype=dtype)\r\n\r\n  p = tf.constant(0.)\r\n\r\n  def run():\r\n    if func == 'forward':\r\n      tf.nn.depthwise_conv2d(x, f, [1, 1, 1, 1], 'SAME',\r\n                             data_format=data_format)\r\n    elif func == 'backprop_filter':\r\n      tf.nn.depthwise_conv2d_backprop_filter(\r\n          x, f.shape, out_grad, [1, 1, 1, 1], 'SAME', data_format=data_format)\r\n    else:\r\n      assert func == 'backprop_input'\r\n      tf.nn.depthwise_conv2d_backprop_input(\r\n          x.shape, f, out_grad, [1, 1, 1, 1], 'SAME', data_format=data_format)\r\n\r\n  # Warmup\r\n  run()\r\n\r\n  start = time.time()\r\n  for _ in range(10):\r\n    run()\r\n  # Synchronize GPU by sending result of computation to CPU\r\n  p = p + 1.\r\n  p.numpy()\r\n\r\n  end = time.time()\r\n  print('time for %s %s %s: %s' % (dtype, data_format, func, end - start))\r\n\r\nbench('float32', 'NHWC', 'forward')\r\nbench('float32', 'NCHW', 'forward')\r\nbench('float16', 'NHWC', 'forward')\r\nbench('float16', 'NCHW', 'forward')\r\nbench('float32', 'NHWC', 'backprop_input')\r\nbench('float32', 'NCHW', 'backprop_input')\r\nbench('float16', 'NHWC', 'backprop_input')\r\nbench('float16', 'NCHW', 'backprop_input')\r\nbench('float32', 'NHWC', 'backprop_filter')\r\nbench('float32', 'NCHW', 'backprop_filter')\r\nbench('float16', 'NHWC', 'backprop_filter')\r\nbench('float16', 'NCHW', 'backprop_filter')\r\n```\r\n\r\nThe output is\r\n\r\n```\r\ntime for float32 NHWC forward: 0.002748727798461914\r\ntime for float32 NCHW forward: 0.0024683475494384766\r\ntime for float16 NHWC forward: 0.0024971961975097656\r\ntime for float16 NCHW forward: 0.0023937225341796875\r\ntime for float32 NHWC backprop_input: 0.0015697479248046875\r\ntime for float32 NCHW backprop_input: 0.0016202926635742188\r\ntime for float16 NHWC backprop_input: 0.0015521049499511719\r\ntime for float16 NCHW backprop_input: 0.0016052722930908203\r\ntime for float32 NHWC backprop_filter: 0.010046243667602539\r\ntime for float32 NCHW backprop_filter: 0.04735541343688965\r\ntime for float16 NHWC backprop_filter: 7.651637315750122\r\ntime for float16 NCHW backprop_filter: 4.234391450881958\r\n```", "@reedwm Thanks for investigating!\r\n\r\nLooks like there is not much performance difference between fp16 and fp32 apart from the backprop filter kernel which is almost unusable in fp16, so I guess it won't hurt to disable fp16 for keras depthwise layers by default.", "Thanks for the fix @reedwm, looks like this also closes #27780. ", "Good catch, I should have gotten to that bug earlier. Thanks for the short, self-contained example to reproduce the issue!"]}]