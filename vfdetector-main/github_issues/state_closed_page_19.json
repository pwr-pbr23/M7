[{"number": 54896, "title": "Internal document update", "body": "Internal document update\n\nOn the TFLite full int quantization operator coverage.\n", "comments": []}, {"number": 54895, "title": "[tf.data] The main purpose of this CL is to allow `Prefetch` legacy autotuner to check for memory consumption before increasing the `Prefetch` buffer. In order to do this, it saves the `cpu_budget` and `ram_budget` in `Model`. It also creates a `maximum_buffered_bytes_` that is updated after every iteration of Autotune optimization, which is done to prevent having to create a snapshot of the `Model` and compute the maximum buffered bytes in the `Prefetch` legacy autotuner.", "body": "[tf.data] The main purpose of this CL is to allow `Prefetch` legacy autotuner to check for memory consumption before increasing the `Prefetch` buffer. In order to do this, it saves the `cpu_budget` and `ram_budget` in `Model`. It also creates a `maximum_buffered_bytes_` that is updated after every iteration of Autotune optimization, which is done to prevent having to create a snapshot of the `Model` and compute the maximum buffered bytes in the `Prefetch` legacy autotuner.\n", "comments": []}, {"number": 54894, "title": "Introduce experimental_placeholder_value to enable supertype tracing", "body": "Introduce experimental_placeholder_value to enable supertype tracing\n", "comments": []}, {"number": 54893, "title": "Internal experimental code change.", "body": "Internal experimental code change.\n", "comments": []}, {"number": 54892, "title": "\"Expose compilability_check_util to friend packages of tensorflow/compiler/jit.\"", "body": "\"Expose compilability_check_util to friend packages of tensorflow/compiler/jit.\"\n\nFUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/54504 from ROCmSoftwarePlatform:deven/upstream_rocm_config_changes 74e9961ac19004143f23ff7fa5a0cb0d447066de\n", "comments": []}, {"number": 54891, "title": "Fix the race condition issue in asynchronous checkpoint.", "body": "Fix the race condition issue in asynchronous checkpoint.\n", "comments": []}, {"number": 54890, "title": "[XNNPACK] Support CONCATENATION operator with 2 inputs", "body": "[XNNPACK] Support CONCATENATION operator with 2 inputs\n", "comments": []}, {"number": 54889, "title": "RFFT is the mode that expands dims, not IRFFT", "body": "RFFT is the mode that expands dims, not IRFFT\n", "comments": []}, {"number": 54888, "title": "Use InferTensorType instead", "body": "Use InferTensorType instead\n", "comments": []}, {"number": 54887, "title": "Increase compression_utils_test memory limit to 24g and re-enable msan.", "body": "Increase compression_utils_test memory limit to 24g and re-enable msan.\n", "comments": []}, {"number": 54886, "title": "[mhlo] Verifier for mhlo.fft", "body": "[mhlo] Verifier for mhlo.fft\n", "comments": []}, {"number": 54885, "title": "Create workflow to automate RBE image updates", "body": "This creates a new workflow that runs once per week or can be triggered manually to create a PR that automatically updates the list of sha256 hashes for the RBE docker containers to the latest versions available (for the SIG Build containers.)\n", "comments": ["Here is an example PR (note that I fixed the broken link after this): https://github.com/angerson/tensorflow/pull/9\r\n\r\nI'm expecting that we'll have some problem with the CLA approval bot on this, but we'll probably be able to find a workaround."]}, {"number": 54883, "title": "Dynamic bounds storage.", "body": "Dynamic bounds storage.\n\nThis adds bounds.{cc,h}, which allow to store dynamic bound information\nin the \"encoding\" attribute of RankedTensorType, via either function call\nor by attaching an (external) attribute interface to IntegerSetAttr.\n", "comments": []}, {"number": 54882, "title": "Fix dependencies for random_index_shuffle.", "body": "Fix dependencies for random_index_shuffle.\n", "comments": []}, {"number": 54881, "title": "Reimplement tf2xla ResizeBilinearOp using gather.", "body": "Reimplement tf2xla ResizeBilinearOp using gather.\n\nPrior to this change, the `ResizeBilinearOp` was marked slow and skipped during XLA autoclustering. This change unmarks it so it can be clustered.\n", "comments": []}, {"number": 54880, "title": "Remove 'reachability_' as a member in InstructionFusion", "body": "Remove 'reachability_' as a member in InstructionFusion\n\nIt was only being used as a shortcut for accessing a loop variable, instead of just directly passing it as a parameter to where it's used. This makes it difficult to see where it is and is not used, mutated, etc.\n", "comments": []}, {"number": 54879, "title": "Add a unit test for single core TPU jit compile with outside compilation.", "body": "Add a unit test for single core TPU jit compile with outside compilation.\n\nThis test currently fails but should work in the future once the TF2XLA rewrite passes are enabled for this flow.\n", "comments": []}, {"number": 54878, "title": "Export Tracing Protocol", "body": "Export Tracing Protocol\n", "comments": []}, {"number": 54877, "title": "Support 2-output split nodes using copy operators", "body": "Support 2-output split nodes using copy operators\n\nA 2-output split is implemented as 2 strided copy operators.\n", "comments": []}, {"number": 54876, "title": "Remove the unused 'module_' member from InstructionFusion", "body": "Remove the unused 'module_' member from InstructionFusion\n", "comments": []}, {"number": 54875, "title": "Support Keras saving/loading for ShardedVariables with arbitrary partitions.", "body": "Support Keras saving/loading for ShardedVariables with arbitrary partitions.\n", "comments": []}, {"number": 54874, "title": "Change VLOG(0) -> VLOG(1) in nvptx_compiler.cc to reduce log spam at the default vlog level.", "body": "Change VLOG(0) -> VLOG(1) in nvptx_compiler.cc to reduce log spam at the default vlog level.\n", "comments": []}, {"number": 54873, "title": "[XLA] Add a utility function to hoist entry program parameters.", "body": "[XLA] Add a utility function to hoist entry program parameters.\n\nHoisting these to the beginning of the computation increase opportunities to\nprefetch them. This does not increase the amount of memory needed since these\nbuffers are live throughout the program.\n", "comments": []}, {"number": 54872, "title": "Removing unused `ctx` argument.", "body": "Removing unused `ctx` argument.\n", "comments": []}, {"number": 54871, "title": "[tf.data] The main purpose of this CL is to allow `Prefetch` legacy autotuner to check for memory consumption before increasing the `Prefetch` buffer. In order to do this, it saves the `cpu_budget` and `ram_budget` in `Model`. It also creates a `cached_maximum_buffered_bytes_` that is updated after every iteration of Autotune optimization, which is done to prevent having to create a snapshot of the `Model` and compute the maximum buffered bytes in the `Prefetch` legacy autotuner.", "body": "[tf.data] The main purpose of this CL is to allow `Prefetch` legacy autotuner to check for memory consumption before increasing the `Prefetch` buffer. In order to do this, it saves the `cpu_budget` and `ram_budget` in `Model`. It also creates a `cached_maximum_buffered_bytes_` that is updated after every iteration of Autotune optimization, which is done to prevent having to create a snapshot of the `Model` and compute the maximum buffered bytes in the `Prefetch` legacy autotuner.\n", "comments": []}, {"number": 54870, "title": "Fix default_types most_specific_common_supertype", "body": "Fix default_types most_specific_common_supertype\n", "comments": []}, {"number": 54869, "title": "Disable tensorflow.msan for compression_utils_test.", "body": "Disable tensorflow.msan for compression_utils_test.\n", "comments": []}, {"number": 54868, "title": "How to suppress outputs stemming from running Tensorflow when not using eager execution (UNSOLVED)", "body": "This is not an error but an inconvenience. When I do not use eager execution, whenever I call the model, the model fills up the terminal with graph information. How do I suppress such outputs? This is an issue because it can fill up the terminal with garbage and crash the system.\r\n\r\nExample:\r\n\r\n\r\n```\r\nfast_rnn_cell_keras/W/Initializer/random_normal/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/W/Initializer/random_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/W/Initializer/random_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/W/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/W/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/W/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/U/Initializer/random_normal/RandomStandardNormal: (RandomStandardNormal): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/U/Initializer/random_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/U/Initializer/random_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/U/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/U/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/U/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/alpha/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/alpha/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/alpha/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/beta/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/beta/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/beta/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/B_h/IsInitialized/VarIsInitializedOp: (VarIsInitializedOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/B_h/Assign: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nfast_rnn_cell_keras/B_h/Read/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/FastRNNCellKerasZeroState/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/FastRNNCellKerasZeroState/concat: (ConcatV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/FastRNNCellKerasZeroState/zeros: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/FastRNNCellKerasZeroState/ExpandDims_1: (ExpandDims): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/transpose: (Transpose): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/Shape_1: (Shape): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/strided_slice_1: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/TensorArrayV2: (TensorListReserve): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/TensorArrayUnstack/TensorListFromTensor: (TensorListFromTensor): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/strided_slice_2: (StridedSlice): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/MatMul/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/MatMul_1/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/add: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/add_1/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/add_1: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/Tanh: (Tanh): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/Sigmoid/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/Sigmoid: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/Sigmoid_1/ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/Sigmoid_1: (Sigmoid): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/mul_1: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/fast_rnn_cell_keras_2/fast_rnn_cell_keras/add_2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/TensorArrayV2_1: (TensorListReserve): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/while/EmptyTensorList: (EmptyTensorList): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/while/EmptyTensorList_1: (EmptyTensorList): /job:localhost/replica:0/task:0/device:GPU:0\r\nrnn/while/EmptyTensorList_2: (EmptyTensorList): /job:localhost/replica:0/task:0/device:GPU:0\r\n```", "comments": ["@swapnilsayansaha, \r\nYou can suppress all debugging logs using below code snippet. Try setting log level before importing tf  \r\n\r\n```\r\nimport os\r\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\r\nimport tensorflow as tf\r\n```", "No that doesn't work. I actually have it set to level 3. It does not suppress non-eager execution information.", "@swapnilsayansaha, \r\nTry this \r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n```", "I tried this also before posting the problem here. This also doesn't work. \r\n\r\nWhen you are not using eager execution (tf.disable_eager_execution()), whenever you compile (using model.compile()) or call a model (e.g. using model.predict(data) or model(data)), the logging information I showed above appears. ", "Here's link to the code: https://drive.google.com/drive/folders/1UZaFmmG8WtULs_c4yzL9W8xCs2zVjE2-?usp=sharing \r\n\r\nThe main python file is fastgrnn.py. The code imports some data, but that's not important and that part can be omitted (Xtest is a nX550X6 matrix, which can be generated randomly using numpy).", "@swapnilsayansaha,\r\n```\r\nimport logging\r\nlogging.getLogger('tensorflow').disabled = True\r\n```\r\nTake a look at comment on similar issue [#8340](https://github.com/tensorflow/tensorflow/issues/8340#issuecomment-294618310) (`sys.flags.interactive = False`)", "I added the lines. It still does not work.", "@swapnilsayansaha,\r\n\r\n`silence-tensorflow` , Simple python package to shut up Tensorflow warnings and logs.\r\n\r\n`pip install silence_tensorflow`\r\n\r\n```\r\nimport silence_tensorflow.auto\r\nimport tensorflow as tf\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "No it still does not work. I am surprised so many tools exist to disable logging, yet TF cannot interface with these tools properly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54868\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/54868\">No</a>\n"]}, {"number": 54867, "title": "Disable tensorflow.msan for `tensor_util_test`.", "body": "Disable tensorflow.msan for `tensor_util_test`.\n", "comments": []}, {"number": 54866, "title": "Add FusedSparseConvGpuV2 to AllowList ops in FP16", "body": "Add FusedSparseConvGpuV2 to AllowList ops in FP16\n", "comments": []}]