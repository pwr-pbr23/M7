[{"number": 5303, "title": "Error compiling on ARM with -mfpu=neon: can\u2019t find a register in class \u2018LO_REGS\u2019 while reloading \u2018asm\u2019", "body": "Hi,\r\n\r\nI am trying to compile TF for an ARM device and to exploit NEON-related optimizations.\r\nBasically, I follow these instructions for PI (my device is a Sabre Board, not PI): https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile\r\n\r\nbut with a bit different flags, namely:\r\n`make -f tensorflow/contrib/makefile/Makefile OPTFLAGS=\"-Ofast -mfpu=neon -funsafe-math-optimizations -ftree-vectorize\" HOST_OS=LINUX`\r\n\r\nbecause my CPU does not support vfpv4:\r\n\r\n```\r\ncat /proc/cpuinfo\r\nprocessor\t: 0\r\nmodel name\t: ARMv7 Processor rev 10 (v7l)\r\nBogoMIPS\t: 7.54\r\nFeatures\t: half thumb fastmult vfp edsp neon vfpv3 tls vfpd32\r\nCPU implementer\t: 0x41\r\nCPU architecture: 7\r\nCPU variant\t: 0x2\r\nCPU part\t: 0xc09\r\nCPU revision\t: 10\r\n```\r\n\r\nThe error I got:\r\n```\r\n/root/tensorflow_exp/tensorflow/contrib/makefile/downloads/gemmlowp/meta/streams_arm_32.h: In static member function \u2018static void gemmlowp::meta::GemmExecutorPackLHS::ExecuteDispatch3D(const P&) [with P = gemmlowp::meta::GemmParams<unsigned char, int, gemmlowp::meta::ColumnMajorWithSum, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::QuantizedStaticPreprocessedAsInt32, gemmlowp::meta::RowMajor>; int m = 1; int n = 8; int k = 8; int m_leftovers = 0; int n_leftovers = 7; int k_leftovers = 4]\u2019:\r\n/root/tensorflow_exp/tensorflow/contrib/makefile/downloads/gemmlowp/meta/streams_arm_32.h:4211:59: error: can\u2019t find a register in class \u2018LO_REGS\u2019 while reloading \u2018asm\u2019\r\n         \"d25\", \"d26\", \"d27\", \"d28\", \"d29\", \"cc\", \"memory\");\r\n                                                           ^\r\n/root/tensorflow_exp/tensorflow/contrib/makefile/downloads/gemmlowp/meta/streams_arm_32.h:4211:59: error: \u2018asm\u2019 operand has impossible constraints\r\nmake: *** [/root/tensorflow_exp/tensorflow/contrib/makefile/gen/obj/tensorflow/core/kernels/meta_support.o] Error 1\r\n```\r\n\r\nAs far as I understand, it complains about too little registers, which wonders me, since ARM has 16 registers. If I remove ` -mfpu=neon` flag, everything works like a charm. \r\n\r\nI would greatly appreciated for any suggestions.\r\n\r\n### Environment info\r\nUbuntu 14.04.4 LTS\r\n`Linux sabresd 4.1.15-1.0.0+g3924425 #1 SMP PREEMPT Sun Mar 13 14:09:51 CST 2016 armv7l armv7l armv7l GNU/Linux`\r\n\r\nInstalled version of CUDA and cuDNN: \r\nNo CUDA installed\r\nTF commit hash: `1fcd6d1294564066c6f92b121a3aaf4ed186dc1a`\r\n", "comments": ["This looks like a problem compiling gemmlowp. So you should perhaps file a bug on http://github.com/google/gemmlowp/issues ... On their github README, they say that this is sometimes caused by having an insufficient compiler.\n", "@maciekcc, could you please comment on this?\n", "I'm on it. What compiler are you using? Some quick things you might check: try adding -fomit-frame-pointer if it's not enabled (it saves one extra register), or remove '-mthumb' (-mno-thumb) if you are using that.\n", "@aselle : Yes, it's gemmlowp code. \n@maciekcc: \n\n```\n# gcc --version\ngcc (Ubuntu/Linaro 4.8.5-2ubuntu1~14.04.1) 4.8.5\n```\n\nDid you mean -mno-thumb-interwork? Thanks for the hint, but I tried this:\n\n`make -f tensorflow/contrib/makefile/Makefile OPTFLAGS=\"-Ofast -mfpu=neon -funsafe-math-optimizations -ftree-vectorize -mno-thumb-interwork -fomit-frame-pointer\" HOST_OS=LINUX`\n\nwith the same result...\n", "I tried to compile TF on RPi 3 model B:\n\n```\nmake -f tensorflow/contrib/makefile/Makefile HOST_OS=PI TARGET=PI OPTFLAGS=\"-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize\" CXX=g++-4.8\n```\n\nIt worked fine, except some minor issues with the Makefile.\n", "Currently updating my [TensorFlow on Raspberry Pi guide](https://github.com/samjabrahams/tensorflow-on-raspberry-pi) and compiling version 0.12 (and hopefully 1.0.0-alpha afterward); just wanted to note that adding  `--copt=\"-fomit-frame-pointer\"` as a flag to `bazel build` let me get past the gemmlowp compilation steps.\r\n\r\nHere's hoping everything else is peachy!", "I managed to build TF on RPi as well, I can't do that on Sabre Board though. ", "I should have been more specific- my last comment was for compiling TensorFlow through Bazel instead of using the Makefile.", "Same problem on Orange Pi+ 2E. Did you find the solution?", "@samjabrahams \r\nHi. I compile tensorflow on cortex-a17 encounterd the same question, but I have used  \"-fomit-frame-pointer\" option. Would you help me. Thank you.\r\n\r\nIn file included from external/gemmlowp/meta/streams.h:293:0,\r\n                 from external/gemmlowp/meta/quantized_mul_kernels.h:22,\r\n                 from ./tensorflow/core/kernels/meta_support.h:21,\r\n                 from tensorflow/core/kernels/meta_support.cc:18:\r\nexternal/gemmlowp/meta/streams_arm_32.h: In static member function 'static void gemmlowp::meta::GemmExecutorPackLHS::ExecuteDispatch3D(const P&) [with P = gemmlowp::meta::GemmParams<unsigned char, int, gemmlowp::meta::ColumnMajorWithSum, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::QuantizedStaticPreprocessedAsInt32, gemmlowp::meta::RowMajor>; int m = 1; int n = 8; int k = 8; int m_leftovers = 0; int n_leftovers = 7; int k_leftovers = 4]':\r\nexternal/gemmlowp/meta/streams_arm_32.h:4211:59: error: can't find a register in class 'LO_REGS' while reloading 'asm'\r\n         \"d25\", \"d26\", \"d27\", \"d28\", \"d29\", \"cc\", \"memory\");\r\n                                                           ^\r\nexternal/gemmlowp/meta/streams_arm_32.h:4211:59: error: 'asm' operand has impossible constraints\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 183.585s, Critical Path: 179.60s\r\n\r\nbazel build -c opt --copt=\"-mfpu=neon-vfpv4\" --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package\r\n\r\nprocessor       : 3\r\nmodel name      : ARMv7 Processor rev 1 (v7l)\r\nBogoMIPS        : 48.00\r\nFeatures        : swp half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt vfpd32 evtstrm \r\nCPU implementer : 0x41\r\nCPU architecture: 7\r\nCPU variant     : 0x0\r\nCPU part        : 0xc0d\r\nCPU revision    : 1", "Does it compile without: --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" ?", "I used this compile option:\r\nbazel build -c opt --copt=\"-mfpu=neon-vfpv4\" --copt=\"-std=c++11\"  --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --local_resources 1024,1.0,1.0 --verbose_failures tensorflow/tools/pip_package:build_pip_package\r\n", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you.", "I'm still having this issue compiling with gcc-4.8.4. It works if I compile without the neon optimization, however, I believe there is significant performance to be gained using the neon optimization with eigen", "Any updates on this? I'm still having the issue with TF 1.3.0 rc1:\r\n```\r\n/tensorflow/core/kernels/_objs/quantized_ops/tensorflow/core/kernels/meta_support.pic.o): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nIn file included from external/gemmlowp/meta/streams.h:293:0,                                                                            from external/gemmlowp/meta/quantized_mul_kernels.h:22,\r\n                 from ./tensorflow/core/kernels/meta_support.h:21,                                                                       from tensorflow/core/kernels/meta_support.cc:18:                                                       external/gemmlowp/meta/streams_arm_32.h: In static member function 'static void gemmlowp::meta::GemmExecutorPackLHS::ExecuteDispatch3D(const P&) [with P = gemmlowp::meta::GemmParams<unsigned char, int, gemmlowp::meta::ColumnMajorWithSum, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::QuantizedStaticPreprocessedAsInt32, gemmlowp::meta::RowMajor>; int m = 1; int n = 8; int k = 8; int m_leftovers = 0; int n_leftovers = 7; int k_leftovers = 4]':\r\nexternal/gemmlowp/meta/streams_arm_32.h:4211:59: error: can't find a register in class 'LO_REGS' while reloading 'asm'\r\n         \"d25\", \"d26\", \"d27\", \"d28\", \"d29\", \"cc\", \"memory\");\r\n                                                           ^\r\nexternal/gemmlowp/meta/streams_arm_32.h:4211:59: error: 'asm' operand has impossible constraints\r\nTarget //tensorflow:libtensorflow.so failed to build\r\nINFO: Elapsed time: 17200.207s, Critical Path: 601.22s\r\n```\r\n\r\nwith:\r\n```\r\n$ gcc --version\r\ngcc (Ubuntu/Linaro 4.8.5-4ubuntu2) 4.8.5\r\n```\r\nand:\r\n```\r\n$cat /proc/cpuinfo\r\nprocessor       : 0\r\nmodel name      : ARMv7 Processor rev 4 (v7l)\r\nBogoMIPS        : 76.80\r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt vfpd32 lpae evtstrm crc32\r\nCPU implementer : 0x41\r\nCPU architecture: 7\r\nCPU variant     : 0x0\r\nCPU part        : 0xd03\r\nCPU revision    : 4\r\n\r\nprocessor       : 1\r\nmodel name      : ARMv7 Processor rev 4 (v7l)\r\nBogoMIPS        : 76.80\r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt vfpd32 lpae evtstrm crc32\r\nCPU implementer : 0x41\r\nCPU architecture: 7\r\nCPU variant     : 0x0\r\nCPU part        : 0xd03\r\nCPU revision    : 4\r\n\r\nprocessor       : 2\r\nmodel name      : ARMv7 Processor rev 4 (v7l)\r\nBogoMIPS        : 76.80\r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt vfpd32 lpae evtstrm crc32\r\nCPU implementer : 0x41\r\nCPU architecture: 7\r\nCPU variant     : 0x0\r\nCPU part        : 0xd03\r\nCPU revision    : 4\r\n\r\nprocessor       : 3\r\nmodel name      : ARMv7 Processor rev 4 (v7l)\r\nBogoMIPS        : 76.80\r\nFeatures        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt vfpd32 lpae evtstrm crc32\r\nCPU implementer : 0x41\r\nCPU architecture: 7\r\nCPU variant     : 0x0\r\nCPU part        : 0xd03\r\nCPU revision    : 4\r\n\r\nHardware        : BCM2709\r\nRevision        : a02082\r\nSerial          : 0000000081b86d8e\r\n```\r\nwhile compiling with Bazel with:\r\n```\r\nbazel build -c opt --copt=\"-mfpu=neon-vfpv4\" --copt=\"-funsafe-math-optimizations\" --copt=\"-ftree-vectorize\" --copt=\"-fomit-frame-pointer\" --local_resources 1024,1.0,1.0 --verbose_failures //tensorflow:libtensorflow.so\r\n```", "The fix is already in gemmlowp (since May actually) : https://github.com/google/gemmlowp/commit/9941cad948313bffd44b8cafe759aa6d8d6a9d75\r\n\r\nI will check if it ended up in tensorflow.", "@maciekcc Doesn't seems so. The most recent commit from the [bazel workspace](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl#L229) is [this one](https://github.com/google/gemmlowp/commit/a6f29d8ac48d63293f845f2253eccbf86bc28321).", "+1\r\n\r\n```\r\nmake -f tensorflow/contrib/makefile/Makefile TARGET=PI OPTFLAGS=\"-O3 -mcpu=cortex-a15 -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -Llibs\" -j100 CXX=/opt/toolchains/gcc-linaro-arm-linux-gnueabihf-4.8-2013.09_linux/bin/arm-linux-gnueabihf-g++\r\n```\r\n\r\n\r\n```\r\n/home/zxi/projects/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp/meta/streams_arm_32.h: In static member function \u2018static void gemmlowp::meta::GemmExecutorPackLHS::ExecuteDispatch3D(const P&) [with P = gemmlowp::meta::GemmParams<unsigned char, int, gemmlowp::meta::ColumnMajorWithSum, gemmlowp::meta::RowMajorWithSum, gemmlowp::meta::QuantizedStaticPreprocessedAsInt32, gemmlowp::meta::RowMajor>; int m = 1; int n = 8; int k = 8; int m_leftovers = 0; int n_leftovers = 7; int k_leftovers = 4]\u2019:\r\n/home/zxi/projects/tensorflow/tensorflow/contrib/makefile/downloads/gemmlowp/meta/streams_arm_32.h:4211:59: error: can\u2019t find a register in class \u2018LO_REGS\u2019 while reloading \u2018asm\u2019\r\n         \"d25\", \"d26\", \"d27\", \"d28\", \"d29\", \"cc\", \"memory\");\r\n```"]}, {"number": 5302, "title": "C++ memory leak after `Session::Close()` /  C++ equivalent to Python `reset_default_graph()` ?", "body": "Using C++ API, memory (RAM) does not appear to be fully released after `Session::Close()` is called. \r\n\r\nSee code snippet below. Running it within a loop eventually eats up all RAM and gets killed by system.\r\n\r\nFrom reading around, I understand that the closing of the session does not release the underlying graph. In Python it seems that `reset_default_graph()` releases the graph resources.\r\n\r\nIs there any equivalent to `reset_default_graph()` in C++ ?\r\n\r\nI initially reported this question on the mailing list, https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/NG7n6emda78 with no echo, so posted as an issue here. My apologies if this isn't the right procedure.\r\n\r\nNote:\r\nGPU memory appears to not be released when looking it up with `nvidia-smi` but I believe it is an `nvidia-smi` problem as in practice the GPU memory appears to be reusable, so no problem on this side.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttp://stackoverflow.com/questions/35695183/tensorflow-memory-leak-even-while-closing-session\r\nhttps://github.com/tensorflow/tensorflow/issues/1578\r\nhttps://github.com/fchollet/keras/issues/2102\r\nhttps://github.com/tensorflow/tensorflow/issues/700\r\nhttps://github.com/tensorflow/tensorflow/issues/3106\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nUbuntu 16.04 LTS\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\n```\r\n-rw-r--r-- 1 root root   560184 Sep  7 18:22 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Sep  7 18:22 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Sep  7 18:22 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n-rwxr-xr-x 1 root root   394472 Sep  7 18:22 /usr/local/cuda/lib64/libcudart.so.8.0.27\r\n-rw-r--r-- 1 root root   737516 Sep  7 18:22 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Sep 11 08:45 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Sep 11 08:45 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Sep 11 08:45 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Sep 11 08:45 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\n\r\n### From source installation\r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n\r\n```\r\n5c1ca717e8ddd16b0be8410a798dc174380a600d\r\n```\r\n\r\n2. The output of `bazel version`\r\n\r\n```\r\nBuild label: 0.3.2\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\r\nBuild timestamp: 1475861110\r\nBuild timestamp as int: 1475861110\r\n```\r\n### Short code snippet\r\n\r\nPut the code below into a loop, and RAM keeps on growing, replace `_inputLayer` and `_outputLayer` with proper network layer names, and `vtfinputs` is a `std::vector<tensorflow::Tensor>`.\r\n\r\n```C++\r\ntensorflow::GraphDef graph_def;\r\ntensorflow::Status graphLoadedStatus = ReadBinaryProto(tensorflow::Env::Default(),graphFile,&graph_def);\r\ntensorflow::SessionOptions options;\r\ntensorflow::ConfigProto &config = options.config;\r\nconfig.mutable_gpu_options()->set_allow_growth(true);\r\nstd::unique_ptr<tensorflow::Session> session = std::unique_ptr<tensorflow::Session>(tensorflow::NewSession(options));\r\ntensorflow::Status session_create_status = session->Create(graph_def);                                                        \r\n...                                                                               \r\ntensorflow::Status run_status  = session->Run({{_inputLayer,*(vtfinputs.begin())}},{_outputLayer},{},&finalOutput)  /* runs file, results are what they should be, and are acquired via finalOutput. */\r\n...\r\nsession->Close();                                                                                                          \r\nsession.reset();\r\n\r\n/* RAM keeps building up if code above put into a loop */\r\n```\r\n### What other attempted solutions have you tried?\r\n\r\nusing ```session->Reset(options,containers)``` does not appear to compile (Reset does not exist for Session). Looking at ```session.h``` and ```direct_session.h``` I don't yet understand why this is the case. \r\n\r\n\r\n", "comments": ["Are you using tcmalloc? If not you should try that first. tcmalloc is a much more efficient malloc than default which prevents memory fragmentation which can easily look like a memory leak.\n", "@aselle stunning, memory now perfectly handled! Many thanks for the quick answer.\n\nI guess TF is developed with and thus for `tcmalloc` as default. Using various `resnet` architectures, the C++ code above was in practice not usable with standard `malloc`.\n\nSlightly more info for the users possibly finding this thread in the future:\n- TF + tcmalloc appears to be the way to go: https://github.com/tensorflow/tensorflow/issues/2384#issuecomment-228577365 and https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-235993119\n- some mitigated results, depending on number of threads, \n  https://github.com/tensorflow/tensorflow/issues/3009#issuecomment-255633245\n", "@beniz I love you", "@lordadamson I love you back :)", "Hi, I have the same problem (It looks like GPU doesn't release its memory after session.Close() is called, but apparently there are memory available as I can deploy a saved model in my app many times) and tcmalloc doesn't seem to solve the problem. I use Windows and I add tcmalloc library to my visual studio project using the following instruction. \r\nhttps://github.com/gperftools/gperftools/blob/master/README_windows.txt\r\n\r\nIs there anything I can try?\r\nThank you,", "@ktsumura I just got tcmalloc working under Windows, the difference is spectacular to say the least, memory is given back and shared between sessions, I love both @beniz  for pointing out what was becoming a show stopper for me, that I couldn't rely on GPU memory being given back in a tidy way.", "@samhodge, could you post some C++ sample code or tips for setting up for the Windows case using tcmalloc?  I am also seeing steady memory creep after cycles of NewSession() ... Session.Close() in a heavy load context.  In my case, it is a CPU-only inference context (almost ready) for production use and this is a clear show stopper. Thanks", "There isn\u2019t much to tell\r\n\r\nSee this\r\n\r\n\r\nhttps://github.com/gperftools/gperftools/tree/master/vsprojects/libtcmalloc_minimal\r\n\r\nCompile it as a shared library and link against it\r\n\r\nThere were a few modifications to make it a shared library after that close and reset actually worked.", "This should have everything you need \r\n\r\nhttps://github.com/gperftools/gperftools/blob/master/README_windows.txt", "Oh acttI did the whole archive trick to force all the symbols in there rather than the __tcmalloc thing the guide suggests", "@samhodge Many thanks.\r\n", "@parwarrick\r\n\r\nGlad to help, please respond if it actually works out for you", "@samhodge Finally got around to trying this.  It was fairly straightforward. I needed to use the VC2015 property to \"force\" the __tcmalloc symbol to be included in the link.  The main benefit that I can observe is finer grained allocations."]}, {"number": 5301, "title": "Feature request: Automated builds for AVX2 ", "body": "Would it be possible to add also AVX2 optimized builds to the automated PIP builds provided by Google? ", "comments": ["@gunan, what do you think? \n@hholst80 , can you provide a little more detailed info on your request. For example, what operating system and python version are you interested in. Also, are you looking for CPU-only pip builds or GPU-capable ones?\n", "Unfortunately, we do not have machines that have avx2 support for all configurations we have.\nI can see ourselves upgrading some of the machines soon, however as Apple did not update mac pro's this year, I see no way for us to release avx2 images for mac.\nFor linux, this might be possible in some near future, but not immediately.\n", "For us Linux amd_64 and Python 3.5. We have no need for MacOSX or Python 2.7 support at the moment.\n\nDocker images would be a bonus, but we roll our own Docker images from the PIP sources right now.\n", "I just checked, and no machines test and release infra currently has avx2 extensions.\nWe are all blocked on services providing these machines to us, therefore for the time being, we can build, but we cannot test it for avx2.\n\nGiven the fragility of the avx build, we have to test it and fix all the issues with avx2.\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu-mavx/\n\nAll things considered, official support for avx2 is infeasible right now. We will reevaluate this in a quarter. I am closing this issue for now.\n"]}, {"number": 5300, "title": "Add atrous_conv2d_transpose python function", "body": "Issue description : https://github.com/tensorflow/tensorflow/issues/4668\r\n\r\nThis is the first commit of the issue.\r\nI will commit the unit test later.\r\nI will also add the comment for the method.\r\n\r\n**Thank you for any advice.**\r\n\r\nNow I test it with:\r\n\r\n```\r\n\r\n# Input, output: [batch, height, width, depth]\r\nx_image = tf.placeholder(tf.float32,shape=[1])\r\nx = tf.reshape(x_image,[1,1,1,1])\r\n\r\n#Filter: W [kernel_height, kernel_width, output_depth, input_depth]\r\nW_cpu = np.array([[1,-1,1],[1,1,1],[-1,1,-1]],dtype=np.float32)\r\nW = tf.Variable(W_cpu)\r\nW = tf.reshape(W, [3,3,1,1])\r\n\r\nstrides=[1, 1, 1, 1]\r\npadding='VALID'\r\n\r\ny = tf.nn.atrous_conv2d_transpose(x, W, [1,5,5,1], 2, strides, padding)\r\n\r\nx_data = np.array([1],dtype=np.float32)\r\nwith tf.Session() as sess:\r\n    init = tf.initialize_all_variables()\r\n    sess.run(init)\r\n\r\n    x = (sess.run(x, feed_dict={x_image: x_data}))\r\n    W = (sess.run(W, feed_dict={x_image: x_data}))\r\n    y = (sess.run(y, feed_dict={x_image: x_data}))\r\n\r\n    print \"The shape of x:\\t\", x.shape, \",\\t and the x.reshape(1) is :\"\r\n    print x.reshape(1)\r\n    print \"\"\r\n\r\n    print \"The shape of x:\\t\", W.shape, \",\\t and the W.reshape(3,3) is :\"\r\n    print W.reshape(3,3)\r\n    print \"\"\r\n\r\n    print \"The shape of y:\\t\", y.shape, \",\\t and the y.reshape(5,5) is :\"\r\n    print y.reshape(5,5)\r\n    print \"\"\r\n```\r\n\r\nand \r\n\r\n```\r\n  def testAtrousConv2DTransposeSingleStride(self):\r\n    with self.test_session():\r\n      strides = [1, 1, 1, 1]\r\n\r\n      # Input, output: [batch, height, width, depth]\r\n      x_shape = [2, 6, 4, 3]\r\n      y_shape = [2, 6, 4, 2]\r\n\r\n      # Filter: [kernel_height, kernel_width, output_depth, input_depth]\r\n      f_shape = [3, 3, 2, 3]\r\n\r\n      x = tf.constant(1.0, shape=x_shape, name=\"x\", dtype=tf.float32)\r\n      f = tf.constant(1.0, shape=f_shape, name=\"filter\", dtype=tf.float32)\r\n\r\n      output = tf.nn.atrous_conv2d_transpose(x, f, y_shape, 2, strides=strides,\r\n                                      padding=\"SAME\")\r\n\r\n      value = output.eval()\r\n      print(value)\r\n```", "comments": ["Can one of the admins verify this patch?\n", "@guotong1988, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @yuefengz to be potential reviewers.\n", "Looks nice at a high level!  Let's add the tests in the same PR.\n", "I'm working.\n", "What's the current status here @guotong1988 ?\r\nNeed some help? What need's to be done to get this merged?", "I got a little busy to change my job in the past three weeks. And I just finished it.\r\nI am working on it now.", "Nice! Ping me if you need help.", " @sbrodehl In fact I'm not sure that my only test case can cover that much . Please provide more test case, if you have. Thank you .", "I write another example ,  and I think my commit may be right.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Input, output: [batch, height, width, depth]\r\nx_image = tf.placeholder(tf.float32,shape=[2,2])\r\nx = tf.reshape(x_image,[1,2,2,1])\r\n\r\n#Filter: W [kernel_height, kernel_width, output_depth, input_depth]\r\nW_cpu = np.array([[1,-1,1],[1,1,1],[-1,1,-1]],dtype=np.float32)\r\nW = tf.Variable(W_cpu)\r\nW = tf.reshape(W, [3,3,1,1])\r\n\r\nstrides=[1, 1, 1, 1]\r\npadding='VALID'\r\n\r\ny = tf.nn.atrous_conv2d_transpose(x, W, [1,5,5,1], 2, strides, padding)\r\n\r\nx_data = np.array([[1,1],[1,1]],dtype=np.float32)\r\nwith tf.Session() as sess:\r\n    init = tf.initialize_all_variables()\r\n    sess.run(init)\r\n\r\n    x = (sess.run(x, feed_dict={x_image: x_data}))\r\n    W = (sess.run(W, feed_dict={x_image: x_data}))\r\n    y = (sess.run(y, feed_dict={x_image: x_data}))\r\n\r\n    print \"The shape of x:\\t\", x.shape, \",\\t and the x.reshape(2,2) is :\"\r\n    print x.reshape([2,2])\r\n    print \"\"\r\n\r\n    print \"The shape of x:\\t\", W.shape, \",\\t and the W.reshape(3,3) is :\"\r\n    print W.reshape(3,3)\r\n    print \"\"\r\n\r\n    print \"The shape of y:\\t\", y.shape, \",\\t and the y.reshape(6,6) is :\"\r\n    print y.reshape(6,6)\r\n    print \"\"\r\n```\r\n", "Now that I'm sure that my commit is right , because I write this example which is similar to the above one.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Input, output: [batch, height, width, depth]\r\nx_image = tf.placeholder(tf.float32,shape=[2,3])\r\nx = tf.reshape(x_image,[1,2,3,1])\r\n\r\n#Filter: W [kernel_height, kernel_width, output_depth, input_depth]\r\nW_cpu = np.array([[1,-1,1],[1,1,1],[-1,1,-1]],dtype=np.float32)\r\nW = tf.Variable(W_cpu)\r\nW = tf.reshape(W, [3,3,1,1])\r\n\r\nstrides=[1, 1, 1, 1]\r\npadding='VALID'\r\n\r\ny = tf.nn.atrous_conv2d_transpose(x, W, [1,5,5,1], 2, strides, padding)\r\n\r\nx_data = np.array([[1,1,1],[1,1,1]],dtype=np.float32)\r\nwith tf.Session() as sess:\r\n    init = tf.initialize_all_variables()\r\n    sess.run(init)\r\n\r\n    x = (sess.run(x, feed_dict={x_image: x_data}))\r\n    W = (sess.run(W, feed_dict={x_image: x_data}))\r\n    y = (sess.run(y, feed_dict={x_image: x_data}))\r\n\r\n    print \"The shape of x:\\t\", x.shape, \",\\t and the x.reshape(2,2) is :\"\r\n    print x.reshape([2,3])\r\n    print \"\"\r\n\r\n    print \"The shape of x:\\t\", W.shape, \",\\t and the W.reshape(3,3) is :\"\r\n    print W.reshape(3,3)\r\n    print \"\"\r\n\r\n    print \"The shape of y:\\t\", y.shape, \",\\t and the y.reshape(6,7) is :\"\r\n    print y.reshape(6,7)\r\n    print \"\"\r\n\r\n```", "Then I think I could start to finish the unit test.", "I tried running your code on MNIST. The output shape looks wrong to me:\r\n\r\n```\r\n        print(inputs.get_shape(), W.get_shape())\r\n        outputs = atrous_conv2d_transpose(inputs, W, [100, 28, 28, 1],\r\n                                          2, [1,1,1,1], padding='SAME')\r\n        print(outputs.get_shape())\r\n```\r\ngives:\r\n```\r\n(100, 28, 28, 8) (5, 5, 1, 8)\r\n(100, 36, 36, 1)\r\n```\r\nI was expecting the output shape, i.e. the last line above, to read `(100, 28, 28, 1)`.", "@Fenugreek Yes , I fixed it.", "@gpapan just sent me a full PR for implementing the transpose, btw.", "@vrv I will finish the unit test.", "@gpapan, since you wrote the internal one, I'll let you decide which one we should accept :)\r\n\r\n(based on correctness / merits, of course.)", "I nearly finish. But the code need to review and edit, of course.", "Thank you for your code. I learn a lot. Close this PR please.", "Thanks @guotong1988 <https://github.com/guotong1988>!\n@vrv: Can you please close #4668 and #5300?\n\nOn Mon, Dec 5, 2016 at 12:40 AM, \u90ed\u540cjet \u00b7 \u8010\u5fc3 <notifications@github.com>\nwrote:\n\n> Thank you for your code. I learn a lot. Close this PR please.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/5300#issuecomment-264795777>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AF8Y_V16SQBnwOD5zAEWa-thMmHqWVh4ks5rE836gaJpZM4Kk04b>\n> .\n>\n\n\n\n-- \nBest,\nGeorge\n"]}, {"number": 5299, "title": "How can I make distributed tensorFlow support failover?", "body": "I create a 4 nodes tensorflow cluster, 2 worker, 2 ps. When the worker or ps fails, I relaunch it with the same configuration on the machine. However, it cannot continue to work from the checkpoint.\r\nDoes distributed tensorflow still not support failover?\r\n\r\nThis file \"model.ckpt-24\" is only on the machine( task 0 of worker), the trace is as follows.\r\n\r\nTraceback (most recent call last):\r\n  File \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 107, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 87, in main\r\n    with sv.managed_session(server.target) as sess:\r\n  File \"/usr/lib64/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 969, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 797, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 958, in managed_session\r\n    start_standard_services=start_standard_services)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 715, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 227, in prepare_session\r\n    config=config)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 173, in _restore_checkpoint\r\n    saver.restore(sess, ckpt.model_checkpoint_path)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1345, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 915, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 985, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors.InvalidArgumentError: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/model.ckpt-24: Not found: /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003\r\n\t [[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S3, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]\r\n\r\nCaused by op u'save/restore_slice_1', defined at:\r\n  File \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 107, in <module>\r\n    tf.app.run()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"/dump/9/nm-local-dir/usercache/danrtsey.wy/appcache/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/app/install/trainer.py\", line 71, in main\r\n    saver = tf.train.Saver()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 986, in __init__\r\n    self.build()\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1015, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 620, in build\r\n    restore_sequentially, reshape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 357, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 270, in restore_op\r\n    preferred_shard=preferred_shard))\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 204, in _restore_slice\r\n    preferred_shard, name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 359, in _restore_slice\r\n    preferred_shard=preferred_shard, name=name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 749, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2380, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1298, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003/model.ckpt-24: Not found: /dump/6/nm-logs/application_1477899492621_0004/container_e07_1477899492621_0004_01_000003\r\n\t [[Node: save/restore_slice_1 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:ps/replica:0/task:1/cpu:0\"](_recv_save/Const_0_S3, save/restore_slice_1/tensor_name, save/restore_slice_1/shape_and_slice)]]", "comments": ["This is a question better suited for StackOverflow, which we also monitor. Please ask it there and tag it with the `tensorflow` tag. @mrry may have additional comments, however.\n", "OK, thanks\n"]}, {"number": 5298, "title": "[example] fix typo", "body": "May be --> maybe", "comments": ["Can one of the admins verify this patch?\n", "@Haroenv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @caisq, @tensorflower-gardener and @danmane to be potential reviewers.\n", "@tensorflow-jenkins test this please\n", "PR merged. Thanks, @Haroenv !\n", "You're welcome, small things count too \ud83d\ude04 \n"]}, {"number": 5297, "title": "Issue on Compile protobuf.sh?", "body": "Hi,\r\n\r\nI am trying to setup latest version from tensorflow for iOS, in the process first step to  run **download_dependencies.sh** works fine, and then in the next step to run the **compile_ios_protobuf.sh**  is resulting in following error:\r\n\r\n    .....\r\n    glibtoolize: copying file 'm4/ltsugar.m4'\r\n    glibtoolize: copying file 'm4/ltversion.m4'\r\n    glibtoolize: copying file 'm4/lt~obsolete.m4'\r\n    configure.ac:61: installing './compile'\r\n    configure.ac:48: installing './missing'\r\n    benchmarks/Makefile.am: installing './depcomp'\r\n    + rm -rf autom4te.cache config.h.in~\r\n    + exit 0\r\n    + '[' 0 -ne 0 ']'\r\n    + make distclean\r\n    make: *** No rule to make target `distclean'.  Stop.\r\n\r\nLet me know If I am missing anything, Please guide.\r\n\r\nThanks ", "comments": ["What version of MacOS? Did you do\n`brew install automake` yet?\n", "I am using macOS Seirra version 10.12. On running brew automake i get the following \n\n```\nWarning: automake-1.15 already installed\nWarning: You are using OS X 10.12.\n```\n\njfyi I am having Xcode 7.3 and Xcode 8.0 on the same system, hope that should not be a problem.\n\nThanks\n", "I cannot replicate the problem (I am on Sierra)\n\nTry doing the following from the tensorflow root directory (make sure everything else was compiled :\n\n``` bash\nbrew install automake autoconf libtool\ntensorflow/contrib/makefile/build_all_ios.sh\n```\n\nIf that works, the documentation has to updated, if it doesn't, what is the error that fails? Note that the process might take some time to finish.\n", "Also, can you copy-paste the following in the tensorflow root directory:\n\n``` bash\nTF=`pwd`\n./configure \\\n  --build=x86_64-apple-darwin14.0.0 \\\n  --host=x86_64-apple-darwin14.0.0 \\\n  --disable-shared \\\n  --enable-cross-compile \\\n  --with-protoc=${TF}/tensorflow/contrib/makefile/gen/protobuf-host/bin/protoc \\\n  --prefix=${TF}/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_x86_64 \\\n  --exec-prefix=${TF}/tensorflow/contrib/makefile/gen/protobuf_ios/lib/iossim_x86_64 \\\n  'CFLAGS=-DNDEBUG -Os -pipe -fPIC -fno-exceptions -mios-simulator-version-min=8.2 -arch x86_64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.1.sdk' \\\n  CXX= 'CXXFLAGS=-DNDEBUG -Os -pipe -fPIC -fno-exceptions -std=c++11 -stdlib=libc++ -mios-simulator-version-min=8.2 -arch x86_64 -isysroot /Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.1.sdk' \\\n  'LDFLAGS=-arch x86_64 -mios-simulator-version-min=8.2 -stdlib=libc++ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.1.sdk/usr/lib/ -L/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator10.1.sdk/usr/lib/system' \\\n  'LIBS=-lc++ -lc++abi'\n```\n\nJust to explain what it does, so you wouldn't think it's malicious:\n- `--build` and `--host` are architecture specs, where `x86_64-apple-darwin14.x` is iOS. I am assuming you will be running and debugging it on iOS, so both build and host are the same\n- `--disabled-shared` we don't need shared libraries, only static ones!\n- `--enable-cross-compile` exactly what it says\n- `--with-protoc` this comes from [here](https://github.com/google/protobuf/blob/master/src/README.md)\n- `--prefix` this is where everything is gonna be installed\n- `--exec-prefix` same as the previous\n- `CXX, LDFLAGS, LIBS` these are the compilation settings for compiler, library flags, and libraries to include\n\nIf it takes too long, add this flag: `--expunge_async`\n\nI cannot really replicate your problem, but yet again, I have a lot of stuff installed, and not sure which one is the fix (I don't have a virtual machine). \n\nI think the problem is with the protobuffers, so maybe if you try installing the `protobuf` separately, it might fix it: [https://github.com/google/protobuf/releases](https://github.com/google/protobuf/releases),or using brew: `brew install protobuf`\n", "Let me try the above steps and get back. Thanks\n", "Try this (see #5184)\n\n```\nbrew uninstall libtool && brew install libtool\n```\n", "I have tried all the above steps but still stuck with the same issue. I also tried to run build_all_ios\nby running tensorflow/contrib/makefile/build_all_ios.sh, getting the below issue\n\n> checking whether to enable maintainer-specific portions of Makefiles... yes\n> checking build system type... x86_64-apple-darwin14.0.0\n> checking host system type... x86_64-apple-darwin14.0.0\n> checking target system type... x86_64-apple-darwin14.0.0\n> checking for a BSD-compatible install... /opt/local/bin/ginstall -c\n> checking whether build environment is sane... yes\n> checking for a thread-safe mkdir -p... /opt/local/bin/gmkdir -p\n> checking for gawk... no\n> checking for mawk... no\n> checking for nawk... no\n> checking for awk... awk\n> checking whether make sets $(MAKE)... yes\n> checking whether make supports nested variables... yes\n> checking whether UID '501' is supported by ustar format... yes\n> checking whether GID '20' is supported by ustar format... yes\n> checking how to create a ustar tar archive... gnutar\n> checking for x86_64-apple-darwin14.0.0-gcc... no\n> checking for gcc... gcc\n> checking whether the C compiler works... yes\n> checking for C compiler default output file name... a.out\n> checking for suffix of executables... \n> checking whether we are cross compiling... configure: error: in `/Users/jsahil/projects/tensorflow/latest_november/tensorflow/tensorflow/contrib/makefile/downloads/protobuf':\n> configure: error: cannot run C compiled programs.\n> If you meant to cross compile, use`--host'.\n> See `config.log' for more details\n", "@petewarden, do you have any suggestions here. I don't yet have access to a Sierra dev machine, so I can't really try this on my end.\n", "I tried to setup a new mac which had MacOS Seirra tried to follow the same steps and again stuck with the same issue. \n\nWhen I run compile_ios_protobuf.sh I am getting the following error:\n\n> .....\n> glibtoolize: copying file 'm4/ltsugar.m4'\n> glibtoolize: copying file 'm4/ltversion.m4'\n> glibtoolize: copying file 'm4/lt~obsolete.m4'\n> configure.ac:61: installing './compile'\n> configure.ac:48: installing './missing'\n> benchmarks/Makefile.am: installing './depcomp'\n> - rm -rf autom4te.cache config.h.in~\n> - exit 0\n> - '[' 0 -ne 0 ']'\n> - make distclean\n>   make: **\\* No rule to make target `distclean'.  Stop.\n\nand On running the build_all_ios.sh getting the following error \n\n> checking for gcc... gcc\n> checking whether the C compiler works... yes\n> checking for C compiler default output file name... a.out\n> checking for suffix of executables... \n> checking whether we are cross compiling... configure: error: in /Users/jsahil/projects/tensorflow/latest_november/tensorflow/tensorflow/contrib/makefile/downloads/protobuf': configure: error: cannot run C compiled programs. If you meant to cross compile, use--host'.\n> See `config.log' for more details\n", "This is apparently the same error as #5333.\n", "I suggest find the requirements / dependencies -\nThen create a Gemfile\n\n```\nsource 'https://rubygems.org'\nruby \"2.2.3\"\n```\n\nthen you can use RVM + bundler to lock into specific version requirements.\n\n```\ncurl -sSL https://get.rvm.io | bash    \nrvm install ruby-2.2.3   \ngem install rvm  (restart terminal)      \ngem install bundler  \nbundle install  \nbundle exec make\n```\n", "trying to dig more, came across #4640 which is exactly the same issue as I am facing. I just followed the fixes pointed out removing  all occurence of **X86_64** bits from the build scripts compile_ios_protobuf.sh & compile_ios_tensorflow.sh and then running build_all_ios.sh\nworks great on device.\n", "I had the same distclean error (using El Capitan) and the workaround from #4640 didn't help for the compile_ios_protobuf.sh file. Just saw you wrote you should run build_all_ios.sh, didn't do that...\n\nFor me the solution was to do a git pull (don't know if that helped), turned sleep mode to never under energy saver (search for it) and run build_all_ios.sh again. Then executing compile_ios_protobuf.sh worked.\n\nMy pc went to sleep several times during first execution of build_all_ios.sh. Either that or the git pull helped with this error.\n", "This error is specific to MacOS Seirra and building tensorflow for simulator, so making corresponding changes in the scripts and running them works for me. May be the latest git pull that you did already have the changes that I did manually in the scripts. \n\nIt has nothing to do with energy save mode, since I have it switched off and still didn't work. \n", "Chiming in from https://github.com/tensorflow/tensorflow/issues/4640 , I'm pretty sure this has something to do with a clang change in Sierra (downgrading Xcode does not fix).  I believe the make distclean issue is a related issue of the shell scripts getting confused on failure.\n\nI'm not really sure how to get help diagnosing a compiler toolchain issue like this.  If we can't figure it out, perhaps would it be prudent to detect Sierra in the script and conditional out the simulator build with some messaging?  MacOS people are hitting this as they upgrade and there are now three github issues with the same problem.\n\nThe work-around I posted is good enough for people to do on-device work & to see the demo, but it is still impossible to do any simulator testing in Sierra.\n", "I have the same issue on Mac OSX El-Capitan when building `tensorflow/contrib/makefile/build_all_android.sh`\r\n\r\nAlso tried the steps above.. Any update on this? Thanks! \r\n"]}, {"number": 5296, "title": "R0.11", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@wasimshigri, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @yifeif to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n"]}, {"number": 5295, "title": "Python import error, undefined symbol: _Z14tf_git_versionv", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\n\r\n### Environment info\r\nOperating System: Ubuntu16.04\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and 5\r\n\r\nPython 2.7.12 (default, Jul  1 2016, 15:12:24)\r\n[GCC 5.4.0 20160609] on linux2\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\", line 23, in <module>\r\n    from tensorflow.python import *\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\nImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: _Z14tf_git_versionv\r\n\r\nI'm getting this error above with r0.10, can you please give me some insights or solutions to this? \r\n\r\nThanks", "comments": ["Can you attach the copy of:\n\n```\nnm /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n```\n\nDid you build this yourself or did you download a binary distribution?\n", "I am suspecting that this is an older version of the so file, and since we introduced the `tf.GIT_VERSION` later, you have the new python files loading the old shared object.\n", "I built this myself with this commit: \ngit clone https://github.com/tensorflow/tensorflow\ngit checkout a5f8f42cb29d890650c4ca5fac6654884856a8e1\n\nYes, TF version is r0.10. \n\nThis is part of the logs with nm /usr/.../_pywrap_tensorflow.so\n\n0000000006e4b078 b g_initializations\n0000000006e4d950 b g_initialized\n00000000000000b8 b g_initialized_sigmask\n0000000006e4b880 b g_init_mu\n0000000006e4bac8 b g_ipv6_loopback_available\n0000000000b85d70 t __git_version___swigconstant\n00000000065dfb58 a _GLOBAL_OFFSET_TABLE_\n0000000000b0b928 t _GLOBAL__sub_I_adjust_contrast_op.cc\n0000000000b157e8 t _GLOBAL__sub_I_aggregate_ops.cc\n0000000000b7cfd0 t _GLOBAL__sub_I_algorithm.cc\n0000000000b7f9a8 t _GLOBAL__sub_I_allocation_description.pb.cc\n0000000000b7eb48 t _GLOBAL__sub_I_allocator.cc \n...\n0000000000b891b8 t _Z11IsDirectoryRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEP9TF_Status\n0000000000b89b08 T _Z12AppendToFileRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPN10tensorflow12WritableFileEP9TF_Status\n                 U _Z14tf_git_versionv\n0000000000b8a4b0 t _Z16GetMatchingFilesRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEP9TF_Status\n0000000000b88078 t _Z16ReadFileToStringRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEP9TF_Status\n", "Can you check the dates on the so file and the python file? Are you using virtualenv?\n", "No I'm not using virtualenv. \n\n$ ls -alh /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n-rwxr-xr-x 1 root staff 129M Oct 31 04:05 /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow.so\n\n$ ls -al /usr/bin/python\nlrwxrwxrwx 1 root root 9 Dec 10  2015 /usr/bin/python -> python2.7\n\nls -al /usr/bin/python2.7\n-rwxr-xr-x 1 root root 3111304 Jul  2 20:31 /usr/bin/python2.7\n\nActually, I just reproduced build procedures that I've already been through before. \n", "OK, it looks recent enough. `nm` output, please.\n", "The nm output is too big (24 mb txt file) Is there any specific lines you want to check? \nI can search a few lines with \"_Z14tf_git_versionv\" \n0000000000b891b8 t _Z11IsDirectoryRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEP9TF_Status\n0000000000b89b08 T _Z12AppendToFileRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPN10tensorflow12WritableFileEP9TF_Status\nU _Z14tf_git_versionv\n0000000000b8a4b0 t _Z16GetMatchingFilesRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEP9TF_Status\n0000000000b88078 t _Z16ReadFileToStringRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEP9TF_Status\n", "Is this related to gcc version ? or c++ mingling? \n", "Yes, it shows as U, which means undefined. I'd need to check later, but I'm guessing that the script failed to define the symbol. Could you bazel clean, to be sure? See what happens to the `tf_version_info_generate`. It's supposed to put something in `bazel-genfiles/tensorflow/util/version_info.cc`. That should have the git version in there. If it's there, check the lib file and so on.\n", "Ok, Thanks, I'm rebuilding after bazel clean now. Let me check the path and the file then will let you know. \n", "I can't find version_info.cc  file, neither util dir. :( \n$ ls ./bazel-genfiles/tensorflow/\ncontrib  core  models  python\n", "Do you have this?\n\n```\nbazel-bin/tensorflow/core/_objs/version_lib/tensorflow/core/util/version_info.o\n```\n\nWhat does \n\n```\nnm bazel-bin/tensorflow/libtensorflow.so|grep _git_version\n```\n\nsay?\n", "I'd also try to upgrade to 0.11 -- I believe there might have been some git_version issues in 0.10 that have been long since fixed.\n", "I built tensorflow r0.10 on aarch64 with my own patch. In the patch,\nI had to uncomment \":version_lib\" in tensorflow/core/BUILD due to \"duplicate version\" error.\nWith the same version and commit, 2 - 3 weeks later, I tried building again, but I had the issue mentioned above. I don't still know why, but anyway,  it turns out \":version_lib\" is needed at this moment, again.\n\nThank you for your help!\n", "@jart FYI\n"]}, {"number": 5294, "title": "Update os_setup.md", "body": "Changed all Cuda 7.5 to Cuda 8.0 as it is now the supported version for the pip files.\n", "comments": ["@wagonhelm, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ahundt, @martinwicke and @keveman to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your PR @wagonhelm! We have updated the instruction in our release branch via #5097, as well as on our[ website](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#download-and-setup), but looks like it did not get merge back to master properly. \nWe plan to do another merge soon. Closing this for now so we don't get conflicts during the merge. Sorry for the inconvenience and thanks again for your contribution!\n"]}, {"number": 5293, "title": "TFRecord file support with Hadoop Mapreduce/Spark", "body": "MR/Spark are commonly used for ETL and feature generation, it's better to support close integration with such systems. More specifically, supporting the following:\n1. TFRecord file Mapreduce InputFormat/OutputFormat\n2. Integrating Feature/Example proto classes\n", "comments": ["@jhseu for comments\n\nThe native format for Spark is parquet, for which you can now get a C++ reader/writer from impala.\n\nThe ORC format does have a C++ reader, but it lacks bloom filters and a variety of compression algorithms. It has been extracted out of the hadoop code as a standalone recently.\n\nThe legacy MR RCFile format is very, very Java dependent and I wouldn't want to support that.\n", "@drpngx I mean writing TFRecord file with MR and Spark directly (e.g. to HDFS/GCS or customized file system which can be accessed by tensorflow), avoiding unnecessary and slow data conversion in the python code. We have some code to make this possible and would like to contribute if applicable.\n", "That sounds great!\n", "@llhe Yeah, that sounds useful. If you have it working, please send a pull request to http://github.com/tensorflow/ecosystem instead of the core TensorFlow repository.\n", "@jhseu Yeah, I noticed that repo, looks like it's mainly for deployment. Perhaps I can also add this application level stuff. Thanks.\n", "@jhseu @drpngx I just created a pull request for ecosystem: https://github.com/tensorflow/ecosystem/pull/18\n", "Nice!\n"]}, {"number": 5292, "title": "reduce_logsumexp fix for reduction_indices.  Fixes issue #5291", "body": "Fix #5291\n", "comments": ["Can one of the admins verify this patch?\n", "@hoangmit, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @tensorflower-gardener and @mrry to be potential reviewers.\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please\n\nNm, I did it myself.\n", "Well that's unfortunate, it looks like the argument to squeeze has to be an array, not a Tensor, which means we can't use _ReductionDims().  \n", "Use earlier change instead\n", "@tensorflow-jenkins test this please\n"]}, {"number": 5291, "title": "[Bug] reduce_logsumexp examples do not work in 0.11.0rc1", "body": "Version: 0.11.0rc1\n\nThese examples do not work:\n\n```\n# 'x' is [[0, 0, 0]]\n#         [0, 0, 0]]\ntf.reduce_logsumexp(x, 0) ==> [log(2), log(2), log(2)]\ntf.reduce_logsumexp(x, 1) ==> [log(3), log(3)]\n```\n\nBecause:\n`tf.squeeze(t, squeeze_dims)` does not take integer for `squeeze_dims`\n", "comments": ["Nice find! So you mean we should use `0.0` etc? Feel free to send a PR for the docs.\n", "No. I mean this is a bug. Currently, `tf.reduce_logsumexp(x, 0)` crashes. It should return the result in the doc.\n", "Does it crash or return an error?\n", "It returns an error: \n\n```\n...lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in reduce_logsumexp(input_tensor, reduction_indices, keep_dims, name)\n   1293         keep_dims=True)) + my_max\n   1294     if not keep_dims:\n-> 1295       result = array_ops.squeeze(result, reduction_indices)\n   1296     return result\n   1297 \n\n...lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc in squeeze(input, squeeze_dims, name)\n   2644   \"\"\"\n   2645   result = _op_def_lib.apply_op(\"Squeeze\", input=input,\n-> 2646                                 squeeze_dims=squeeze_dims, name=name)\n   2647   return result\n   2648 \n\n...lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in apply_op(self, op_type_name, name, **keywords)\n    636         if attr_def.type.startswith(\"list(\"):\n    637           if not _IsListValue(value):\n--> 638             raise TypeError(\"Expected list for attr \" + key)\n    639           if attr_def.has_minimum:\n    640             if len(value) < attr_def.minimum:\n\nTypeError: Expected list for attr squeeze_dims\n```\n", "Oh, you can't send a list directly, you have to construct a tensor, for instance, with `zero_initializer`.\n", "It is not the problem with the input. (e.g.# 'x' is [[0, 0, 0], [0, 0, 0]])\nI did construct a tensor and pass it in. \n\nThe problem is at line 1295 in file \"tensorflow/python/ops/math_ops.py\". `array_ops.squeeze` does not take integer as the 2nd parameter.\n", "Ah, ok, thanks! Feel free to submit a PR with doc changes.\n"]}, {"number": 5290, "title": "enable sparse_matmul and immutable_constant kernels for windows", "body": "", "comments": ["Can one of the admins verify this patch?\n", "@guschmue, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @keveman to be potential reviewers.\n", "originally this was part of https://github.com/tensorflow/tensorflow/pull/5279\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 5289, "title": "TF freezes and gets killed while training /saving a network", "body": "I am trying to train a deep network from scratch (a 4 layer [CIFAR](https://www.tensorflow.org/versions/r0.11/tutorials/deep_cnn/index.html#cifar-10-model) network) on an image collection of 100K images. The TF instance hangs (while training or while saving using tf.Saver) and then gets killed without any error message.\n\nI've tried the following things without any use:\n\na. Reduced the batch size from 32 to 8.\n\nb. Set config's allow GPU growth option to True\n\nBut the problem still persists. \n\nHas anybody else faced this issue? Is this because of insufficient memory? Is there a way to train a model under constrained memory conditions (although, 12 GB isn't bad)? \n Any tips to avoid this would be very helpful. \n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI've looked at other similar issues posted but haven't found any useful solution.\nhttps://github.com/tensorflow/tensorflow/issues/2121\nhttp://stackoverflow.com/questions/38958737/tensorflow-training-got-stuck-after-some-steps-how-to-investigate\nhttps://github.com/tensorflow/tensorflow/issues/1962\n### Environment info\n\nGPU details: I am running this model on a  Tesla K40c (12GB memory).\nOperating System:  4.7.0-1-amd64 #1 SMP Debian 4.7.6-1 (2016-10-07) x86_64 GNU/Linux\n\nInstalled version of CUDA and cuDNN: \n/opt/cuda-8.0/lib64/libcudnn.so.5\n/opt/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\n\n(Cuda version: 8.0 and cuDNN version 5)\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n   0.11.0rc1\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n   ec7f37e40fedb23435bfb7e28668e5fa63ff52f3\n2. The output of `bazel version`\n\nBuild label: 0.3.2\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Oct 7 17:25:10 2016 (1475861110)\nBuild timestamp: 1475861110\nBuild timestamp as int: 1475861110\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nThis issue is happening when I am trying to train/ save a model\n", "comments": ["There should always be a log somewhere when you get killed, unless you actually run out of memory.\n\nTry setting the env var `GLOG_logtostderr=1`, and also look at `dmesg` to see if you have been sacrificed.\n", "Does GLOG_logtostderr do anything? I thought that was for gflags only which\nisn't in TF. BTW, you can get hangs when the system isn't allowing me to write the file (ie, if you write to NFS and network is down)\n\nOn Sun, Oct 30, 2016 at 2:24 PM, drpngx notifications@github.com wrote:\n\n> There should always be a log somewhere when you get killed, unless you\n> actually run out of memory.\n> \n> Try setting the env var GLOG_logtostderr=1, and also look at dmesg to see\n> if you have been sacrificed.\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5289#issuecomment-257180381,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHIAvqUfR_EbS-amExjPhdcN1i5aiks5q5QsOgaJpZM4KkfA-\n> .\n", "Oh, we don't use glog, right.\n", "I am seeing what I think is a similar issue, but am only training on CPU; when I sample the process in a hung state, I see get the following info. In my case, I just see an indefinite hang. Seems to happen randomly, but consistently if I run the program for a few hours, unfortunately. I am also on version, 0.11.0rc0, if that matters. \n\n[tflow_hang.txt](https://github.com/tensorflow/tensorflow/files/565125/tflow_hang.txt)\n", "@nroth1 -- a common way for \"random hangs\" is by causing a deadlock with\nqueues, partial solution  is to set operation timeout as [here](https://github.com/tensorflow/tensorflow/issues/2130#issuecomment-215180165)\nand retry on error. More fundamentally is to analyze your queues and\nprevent deadlock from occuring, as [here](https://github.com/tensorflow/tensorflow/issues/4917)\n\nOn Tue, Nov 1, 2016 at 2:45 PM, nroth1 notifications@github.com wrote:\n\n> I am seeing what I think is a similar issue, but am only training on CPU;\n> when I sample the process in a hung state, I see get the following info. In\n> my case, I just see an indefinite hang. Seems to happen randomly, but\n> consistently if I run the program for a few hours, unfortunately. I am also\n> on version, 0.11.0rc0, if that matters.\n> \n> tflow_hang.txt\n> https://github.com/tensorflow/tensorflow/files/565125/tflow_hang.txt\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5289#issuecomment-257708344,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHBm5M5lHsKvIUr-8rrAXgy2CARPhks5q57LjgaJpZM4KkfA-\n> .\n", "Hi, thanks for writing back!\n\nI agree that the partial solution would maybe be a work around, but I was hoping to understand what was happening and maybe get a cleaner fix. I am actually producing my issue without queues, in the strictest sense; I just have some python threads running in parallel (I am aware that I won't get true multithreading in python). In pseudo code this just boils down to: \n\n`thread1:\n       my_session.run(some_op)\n  thread2:\n       saver.save(my_session)\n`\n\nIn this case, there is no dequeue op or queue set of threads that I explicitly call, so it seems like I ought not get the deadlocking issue as described in #4917? But maybe under the covers tensorflow parallelizes access to this shared session object in a way which still leaves this problem open even if you aren't using tensorflow threads/queues to make your calls? \n\nThanks a bunch for your help!!\n", "If you don't have queues, you shouldn't get deadlocks like this. The only thing I can think of is that your write to disk is hanging on the OS level, so the saving op never terminates\n", "hmm, ok, thanks a bunch! I will poke around and update if I find any more information. \n", "So, I have been able to isolate the issue to roughly when I call save on a session at the same time that I call session.run(my_op). Interestingly, it is the session.run call that hangs, not the saver. When I do so, I get the following backtrace: \n\n\"\n2592 _wrap_TF_Run(_object_, _object_)  (in _pywrap_tensorflow.so) + 1301  [0x106599365]\n\n2592 tensorflow::TF_Run_wrapper(TF_Session_, TF_Buffer const_, _object_, tensorflow::gtl::InlinedVector<char const_, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status_, tensorflow::gtl::InlinedVector<_object_, 8>_, TF_Buffer_)  (in _pywrap_tensorflow.so) + 55  [0x1065b25e7]\n\n2592 tensorflow::TF_Run_wrapper_helper(TF_Session_, char const_, TF_Buffer const_, _object_, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status_, tensorflow::gtl::InlinedVector<_object_, 8>_, TF_Buffer_)  (in _pywrap_tensorflow.so) + 4930  [0x1065b1492]\n\n2592 PyEval_RestoreThread  (in Python) + 62  [0x10577defa]\n                                                                                                                                                                                                             2592 PyThread_acquire_lock  (in Python) + 101  [0x1057ad790]\n                                                                                                                                                                                                               2592 _pthread_cond_wait  (in libsystem_pthread.dylib) + 712  [0x7fffaed7c97a]\n\n\"\n\nI also saw a system log stating: \n\n'pthread_cond_wait: Resource busy'\n\nIt looks like the session run call is trying to acquire a lock? Is that expected? \n\nIs it not safe to a save a session at the same time it is evaluating some other op? I can't find this discussed explicitly in the docs. Thanks again so much for the help!\n", "\"PyThread_acquire_lock\" is how Python GIL is acquired. If GIL is the issue,\nthere must be another Python thread holding the lock. However, session.run\nis supposed to be releasing the lock because of line below in tf_session.i,\nso running saver ops shouldn't block evaluation.\n\n// Release the Python GIL for the duration of most methods.\n%exception {\n  Py_BEGIN_ALLOW_THREADS;\n  $action\n  Py_END_ALLOW_THREADS;\n}\n\nhttps://github.com/tensorflow/tensorflow/blob/47dd089db3cd16d76595791b2e8483e2fd0b0a25/tensorflow/python/client/tf_session.i#L45\n\nYou could look at existing Python threads to see which other thread is\nhanging onto the GIL\n\n def stacktraces(self):\n        code = []\n        for threadId, stack in sys._current_frames().items():\n            code.append(\"\\n thread: %s\" % threadId)\n            for filename, lineno, name, line in\ntraceback.extract_stack(stack):\n                code.append('\\n**\\*  File: \"%s\", line %d, in %s' % (filename,\n                                                                lineno,\nname))\n                if line:\n                    code.append(\"  %s\" % (line.strip()))\n\n```\n    for line in code:\n        sys.stderr.write(line)\n    sys.stderr.write(\"\\n\")\n```\n\nOn Wed, Nov 2, 2016 at 6:40 PM, nroth1 notifications@github.com wrote:\n\n> So, I have been able to isolate the issue to roughly when I call save on a\n> session at the same time that I evaluate an op on the session. When I do\n> so, I get the following backtrace:\n> \n> 2592 _wrap_TF_Run(_object_, _object_) (in _pywrap_tensorflow.so) + 1301\n> [0x106599365]\n> 2592 tensorflow::TF_Run_wrapper(TF_Session_, TF_Buffer const_, _object_,\n> tensorflow::gtl::InlinedVector<char const_, 8> const&, tensorflow::gtl::InlinedVector<char\n> const*, 8> const&, TF_Status_, tensorflow::gtl::InlinedVector<_object_,\n> 8>_, TF_Buffer_) (in _pywrap_tensorflow.so) + 55 [0x1065b25e7]\n> 2592 tensorflow::TF_Run_wrapper_helper(TF_Session_, char const_,\n> TF_Buffer const_, _object_, tensorflow::gtl::InlinedVector<char const*,\n> 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&,\n> TF_Status_, tensorflow::gtl::InlinedVector<_object_, 8>_, TF_Buffer_) (in\n> _pywrap_tensorflow.so) + 4930 [0x1065b1492]\n> 2592 PyEval_RestoreThread (in Python) + 62 [0x10577defa]\n> 2592 PyThread_acquire_lock (in Python) + 101 [0x1057ad790]\n> 2592 _pthread_cond_wait (in libsystem_pthread.dylib) + 712 [0x7fffaed7c97a]\n> 2592 __psynch_cvwait (in libsystem_kernel.dylib) + 10 [0x7fffaec93c8a]\n> \n> It looks like the session run call is trying to acquire a lock? Is that\n> expected?\n> \n> Is it not safe to a save a session at the same time it is being evaluated?\n> I can't find this discussed explicitly in the docs. Thanks again so much\n> for the help!\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5289#issuecomment-258048119,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AABaHFYzkqwrsl4YRsFOKP16sAAIUWIcks5q6TuEgaJpZM4KkfA-\n> .\n", "So I am still able to occasionally reproduce the freezing. This only occurs when a save operation and a session.run operation occur at roughly the same time. I see the  following backtrace (abbreviated to just show the tensorflow call where the deadlock happens), which seems to indicate that the hang is happening when tflow tries to acquire a lock in ops.py:\n\n`\n**\\*  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 717, in run  run_metadata_ptr)\n\n**\\*  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 869, in _run  allow_operation=False)\n\n**\\*  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2457, in as_graph_element  with self._lock:`\n\nAny idea on why we would see deadlock there? Or what else could be holding that graph lock. As before, the save op seems to successfully terminate. \n\nFor what it is worth, I had a very similar version of this code running before I updated to the latest tensorflow (was all the way back on .8) , and did not observe this behavior. \n\nThanks a bunch for the time and help!\n", "@nroth1 could this be a freeze connected to you running out of memory? If you switch from `Supervisor` to the new MonitoredTrainingSession framework, saving happens in the same Python thread as everything else, so you don't have unlucky parallel `.run` exploding the memory occasionally", "Hi, sorry for the delayed response. I don't believe the freeze correlated to high memory usage. I have recently upgraded to tensorflow 1.0, so will write back with more details if the problem reproduces itself. ", "I am able to reproduce the issue on version 1.0, and will follow up with more details. Thanks again for all your time!", "I'm able to reproduce the hangs on a smaller example, and have logs attached. I'm not sure if this is necessarily the same hang as I was seeing above, as the freeze seems to happen in a numpy call, not at the tensorflow level. The example is pretty silly, but I basically just make and evaluate two graphs, while saving them in the background on separate threads. Eventually, it hangs at:\r\n\r\n***  File: \"crash_eg.py\", line 93, in <module>  main_thread2()\r\n***  File: \"crash_eg.py\", line 79, in main_thread2  session2.run(outputs2,feed_dict = fd)\r\n***  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run  run_metadata_ptr)\r\n***  File: \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 938, in _run  np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\r\n***  File: \"/usr/local/lib/python2.7/site-packages/numpy/core/numeric.py\", line 531, in asarray  return array(a, dtype, copy=False, order=order)\r\n\r\nI have attached the full logs below. Need to investigate more about whether this is the same issue as reported above, but thought I would leave the info just in case. I should mention I am on a mac, and have read that there can be some issues with numpy and threading on Mac OS, so maybe that is involved. I am not sure if anyone else has ever seen a similar issue? Thanks again for all your time!\r\n\r\n[hang_logs.txt](https://github.com/tensorflow/tensorflow/files/786206/hang_logs.txt)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport threading\r\nimport numpy as np\r\nimport time\r\nimport sys\r\nimport traceback\r\n\r\ng1 = tf.Graph()\r\ng2 = tf.Graph()\r\n\r\nembedding_size = 150\r\npad_len = 100\r\n\r\nwith g1.as_default():\r\n    batch_sentence = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]\r\n    cell = tf.contrib.rnn.GRUCell(embedding_size) \r\n    output1, states1 = tf.contrib.rnn.static_rnn(cell, batch_sentence, dtype = tf.float32)\r\n    loss = tf.reduce_sum(output1[0] - output1[99])\r\n    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)\r\n    optimizer.minimize(loss)\r\n    session1 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))\r\n    session1.run(tf.initialize_all_variables())\r\n    saver1 = tf.train.Saver()\r\n\r\nwith g2.as_default():\r\n    batch_sentence2 = [tf.placeholder(tf.float32, shape = [None,embedding_size],name= 'tokens') for _ in range(pad_len)]\r\n    cell = tf.contrib.rnn.GRUCell(embedding_size) \r\n    outputs2, states2 = tf.contrib.rnn.static_rnn(cell, batch_sentence2, dtype = tf.float32)\r\n    loss = tf.reduce_sum(outputs2[0] - outputs2[99])\r\n    optimizer = tf.train.AdamOptimizer(1, epsilon=1e-5)\r\n    optimizer.minimize(loss)\r\n    session2 = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1))\r\n    session2.run(tf.initialize_all_variables())\r\n    saver2 = tf.train.Saver()\r\n\r\nlock_map = [threading.Lock(),threading.Lock()]\r\n\r\n\r\ndef report_thread():\r\n    while(True):\r\n        def stacktraces():\r\n            code = []\r\n            for threadId, stack in sys._current_frames().items():\r\n                code.append(\"\\n thread: %s\" % threadId)\r\n                for filename, lineno, name, line in traceback.extract_stack(stack):\r\n                    code.append('\\n***  File: \"%s\", line %d, in %s' % (filename, lineno, name))\r\n                    if line:\r\n                        code.append(\"  %s\" % (line.strip()))\r\n            for line in code:\r\n                sys.stderr.write(line)\r\n            sys.stderr.write(\"\\n\")\r\n        stacktraces()\r\n        time.sleep(300)\r\n\r\n\r\ndef save_wrapper():\r\n    print time.time()\r\n    curr_lock = lock_map[0]\r\n    with curr_lock:\r\n        saver1.save(session1,'out1')\r\n    print time.time()\r\n\r\ndef save_wrapper2():\r\n    print time.time()\r\n    curr_lock = lock_map[1]\r\n    with curr_lock:\r\n        saver2.save(session2,'out2')\r\n    print time.time()\r\n\r\ndef main_thread2():\r\n    X = np.random.random((5,150))\r\n    while(True):\r\n        print '------'\r\n        for _ in range(60):\r\n            fd = {}\r\n\r\n            for i in range(pad_len):\r\n                fd[batch_sentence2[i]] = X\r\n            session2.run(outputs2,feed_dict = fd)\r\n        save_thread = threading.Thread(target = save_wrapper2)\r\n        save_thread.start()\r\n\r\n        for _ in range(60):\r\n            fd = {}\r\n            for i in range(pad_len):\r\n                fd[batch_sentence[i]] = X\r\n            session1.run(output1,feed_dict = fd)\r\n        save_thread = threading.Thread(target = save_wrapper)\r\n        save_thread.start()\r\n        \r\nt1 = threading.Thread(target = report_thread)\r\nt1.start()\r\nmain_thread2()\r\n\r\n\r\n```\r\n\r\n\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5288, "title": "fixes a memory corruption on windows/gpu", "body": "this fixes 2 crashes on windows. Originally part of  https://github.com/tensorflow/tensorflow/pull/5279\n", "comments": ["Can one of the admins verify this patch?\n", "@guschmue, thanks for your PR! By analyzing the history of the files in this pull request, we identified @keveman, @vrv and @davidzchen to be potential reviewers.\n", "@tensorflow-jenkins test this please.\n", "@tensorflow-jenkins test this please.\n"]}, {"number": 5287, "title": "[tf.learn] MetricSpec doesn't work with streaming_mean", "body": "MetricSpecs always pass both values and labels to metrics, meaning metrics which don't use labels like streaming_mean are broken.\n", "comments": ["I wouldn't say that streaming_mean is broken. Rather, it just doesn't fit the mold of the other metrics. We should consider moving it and other similar \"metrics\" like streaming_concat into their own library of primitives for streaming computation.\n", "Maybe.. To me both the Metric and Monitor patterns feel too restrictive to me, for what they are (basically run this thing in the training loop somewhere). They feel similar but different enough to be frustrating. I don't have any concrete suggestions yet but I'm thinking about it (along with a lot of other tf.learn thoughts). \n\nEDIT: less abstract: I guess the docstring is fairly clear. The problem here is \"metric\" being a dramatically overloaded term in TF. Feel free to open a new bug to track this if you want. \n"]}, {"number": 5286, "title": "gcc compile failure on Cray XC30", "body": "Operating System:Cray XC30 (Linux)\n\n$ git rev-parse HEAD\n6bf2cc7e618ef16ab36b903752d51316ce99ad99\n$ bazel version\nBuild label: unknown-2016-10-30 (@5abb90d)\nBuild target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Sun Oct 30 14:59:29 2016 (1477839569)\nBuild timestamp: 1477839569\nBuild timestamp as int: 1477839569\n\n$ bazel build -c opt //tensorflow/tools/pip_package:build_pip_package\n\n$ gcc -v\nUsing built-in specs.\nCOLLECT_GCC=/opt/gcc/4.8.1/bin/../snos/bin/gcc\nCOLLECT_LTO_WRAPPER=/opt/gcc/4.8.1/snos/libexec/gcc/x86_64-suse-linux/4.8.1/lto-wrapper\nTarget: x86_64-suse-linux\nConfigured with: ../cray-gcc-4.8.1/configure --prefix=/opt/gcc/4.8.1/snos --disable-nls --libdir=/opt/gcc/4.8.1/snos/lib --enable-languages=c,c++,fortran --with-gxx-include-dir=/opt/gcc/4.8.1/snos/include/g++ --with-slibdir=/opt/gcc/4.8.1/snos/lib --with-system-zlib --enable-shared --enable-__cxa_atexit --build=x86_64-suse-linux --with-mpc=/opt/gcc/mpc/0.8.1 --with-mpfr=/opt/gcc/mpfr/2.4.2 --with-gmp=/opt/gcc/gmp/4.3.2\nThread model: posix\ngcc version 4.8.1 20130531 (Cray Inc.) (GCC) \n\n$ uname -a\nLinux - 3.0.101-0.47.86.1.11753.0.PTF-default #1 SMP Wed Oct 19 14:11:00 UTC 2016 (56c73f1) x86_64 x86_64 x86_64 GNU/Linux\n\n$ python\nPython 2.7.6 (default, Mar 10 2014, 14:13:45) \n[GCC 4.8.1 20130531 (Cray Inc.)] on linux2\n\nLog attached.\n[log.txt](https://github.com/tensorflow/tensorflow/files/560597/log.txt)\n", "comments": ["Sorry, that arch is not supported, that said, look at #5279 for a replacement with std::bind (although that fix breaks on linux)\n", "@drpngx It is Linux, the CPUs are x86_64 Intel Xeon Skylakes. Wouldn't that be supported and wouldn't the linked fix not work as it's Linux? Thanks.\n", "Oh, what distro is this? We only support Ubuntu and CentOS.\n\nIn any case, it's a bug in the compiler AFAICT. All you need is to make that closure work. You need to capture `sparse_slice`, `slice`, and `slice_num*j`. Alternatively you can change the declaration of `sparse_slice` to `SparseSlice<TL>*`.\n", "Closing due to lack of activity.\n"]}, {"number": 5285, "title": "(0.11) parse_single_example doesn't play well with read_file", "body": "minimal reproduction:\n\n```\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\nindex = ['a', 'b', 'c']\n\nwith tf.python_io.TFRecordWriter('test.pb2') as writer:\n    writer.write(tf.train.Example(features=tf.train.Features(\n        feature={'index': tf.train.Feature(bytes_list=tf.train.BytesList(\n            value=index))})).SerializeToString())\n\nindex_example = tf.read_file('test.pb2')\nindex = tf.parse_single_example(\n    index_example,\n    {'index': tf.FixedLenFeature([3], tf.string)}\n)['index']\n\nwith tf.Session() as sess:\n    tf.logging.debug(index_example.eval())\n    tf.logging.debug(index.eval())\n```\n\nYou'll notice that read_file works just fine, and outputs bytes as expected, but then the process will completely lock up, and does not respond to SIGINT, responding only to SIGKILL or SIGTERM. Not sure what's going on here, although I suspect that read_file is reading some leading/trailing bytes that TFRecordReader strips off. \n\nStill it seems like TFRecords should be a valid I/O format without requiring a filename queue, (e.g. for reading in initial vocabulary files), and I thought that `read_file` + `parse_single_example` would be the expected way to do this. Instead I guess I need to use `make_tensor_proto` or something? Not sure why this is a separate serialization strategy... and completely missing from the docs.\n\nAt the very least, `parse_single_example` should fail and not livelock when it gets these strings. \n\nEDIT: For people wondering what the correct pattern is it appears to be something like:\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib.util import make_tensor_proto\n\ntf.logging.set_verbosity(tf.logging.DEBUG)\nindex = ['a', 'b', 'c']\n\nwith open('test.pb2', 'wb') as writer:\n    writer.write(make_tensor_proto(index, dtype=tf.string, shape=[3]).SerializeToString())\n\nindex_example = tf.read_file('test.pb2')\nindex = tf.parse_tensor(index_example, tf.string)\n\nwith tf.Session() as sess:\n    tf.logging.debug(index_example.eval())\n    tf.logging.debug(index.eval())\n```\n", "comments": ["Let me repro this, it looks like the example doesn't work.\n\nWhich versions did you try?\n", "0.11.0rc0 Also. I think this may be related to system encodings for python, since when running it on Cloud ML I get a utf8 encoding error... sys.getdefaultencoding is ascii (the default). I'm on Goobuntu 14.04.\n", "Yeah these parse errors have been appearing, some of them are correct, some aren't, I haven't traced this down yet.\n", "@drpngx What's the status of this bug?", "It has been 14 days with no activity and the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Nagging Awaiting TensorFlower: It has been 14 days with no activityand the `awaiting tensorflower` label was assigned. Please update the label and/or status accordingly.", "Is that still an issue with the new dataset api?", "It doesn't appear to be a problem anymore, I just reran the snippet code above and it now fails fast and with an `InvalidArgumentError` which is somewhat useful.\r\n\r\nSide Note: `tf.parse_single_example` still exists, so it doesn't really matter whether or not this is the preferable way of doing things, it shouldn't fail this catastrophically. "]}, {"number": 5284, "title": "tf.contrib.learn output shapes : shapes (?, 1) and (?,) are incompatible", "body": "I tried to train a binary DNNClassifier  similar as the example on https://www.tensorflow.org/versions/r0.11/tutorials/tflearn/index.html#tf-contrib-learn-quickstart.\n\nI got the following traceback\n[traceback.txt](https://github.com/tensorflow/tensorflow/files/560406/traceback.txt)\n\nI explored the tensorflow source code and found that it may relate to _get_in_out_shape function in tensorflow/tensorflow/contrib/learn/python/learn/learn_io/data_feeder.py\n\n![image](https://cloud.githubusercontent.com/assets/11754923/19836102/024ac634-9ed2-11e6-89cc-e9dc2623424b.png)\n\nWhen doing a binary classification,y_shape is something like[batch_size,1].Line 52 changes it into [1].Line 54,55 change it into [].And finally the output_shape is [batch_size,].However the correct ouput shape should be [batch_size,1].\n\nTo sum up,we do not need to skip 1st dimension if it is 1 and len(y_shape)=1.\n", "comments": ["That looks like a bug in our doc. Could you send a PR?\n", "I got same issue..\n\n2016-10-31 0:13 GMT+07:00 drpngx notifications@github.com:\n\n> That looks like a bug in our doc. Could you send a PR?\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5284#issuecomment-257164061,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ARx8HlEH0wpjr5alidLF9scwxr5lH-Y3ks5q5NA7gaJpZM4KkWSW\n> .\n\n## \n\nbest regards,\n\nDini Rahmawati Akmalia\ndr.akmalia@gmail.com\n+62896-5488-4113\nMathematics Faculty of Mathematics and Natural Science University Indonesia\n2010\n_Outstanding Life_\n", "Has this been fixed? I tried to replicate and was not able to. \n\nSpecifically, I created a script with the following code, and it ran without any errors.\n\n``` python\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\n\n# Data sets\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TEST = \"iris_test.csv\"\n\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TRAINING,\n                                                       target_dtype=np.int, features_dtype=np.float)\ntest_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TEST,\n                                                   target_dtype=np.int, features_dtype=np.float)\n\n# Specify that all features have real-value data\nfeature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nclassifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n                                            hidden_units=[10, 20, 10],\n                                            n_classes=3,\n                                            model_dir=\"/tmp/iris_model\")\n\n# Fit model.\nclassifier.fit(x=training_set.data,\n               y=training_set.target,\n               steps=2000)\n\n# Evaluate accuracy.\naccuracy_score = classifier.evaluate(x=test_set.data,\n                                     y=test_set.target)[\"accuracy\"]\nprint('Accuracy: {0:f}'.format(accuracy_score))\n\n# Classify two new flower samples.\nnew_samples = np.array(\n    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\ny = classifier.predict(new_samples)\nprint('Predictions: {}'.format(str(y)))\n```\n", "I don't know yet, when I used another data with more rows and columns, I\nstill got this issue..\n\n  File\n\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\",\nline 435, in fit\n    max_steps=max_steps)\n\n  File\n\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\",\nline 333, in fit\n    max_steps=max_steps)\n\n  File\n\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\",\nline 662, in _train_model\n    train_op, loss_op = self._get_train_ops(features, targets)\n\n  File\n\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\",\nline 963, in _get_train_ops\n    _, loss, train_op = self._call_model_fn(features, targets,\nModeKeys.TRAIN)\n\n  File\n\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\",\nline 944, in _call_model_fn\n    return self._model_fn(features, targets, mode=mode, params=self.params)\n\n  File\n\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py\",\nline 258, in _dnn_classifier_model_fn\n    weight=_get_weight_tensor(features, weight_column_name))\n\n  File\n\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py\",\nline 329, in sigmoid_cross_entropy\n\nlogits.get_shape().assert_is_compatible_with(multi_class_labels.get_shape())\n\n  File\n\"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\",\nline 750, in assert_is_compatible_with\n    raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\nValueError: Shapes (?, 1) and (?,) are incompatible\n\n2016-11-09 6:48 GMT+07:00 Finbarr Timbers notifications@github.com:\n\n> Has this been fixed? I tried to replicate and was not able to.\n> \n> Specifically, I created a script with the following code, and it ran\n> without any errors.\n> \n> from **future** import absolute_importfrom **future** import divisionfrom **future** import print_function\n> import tensorflow as tfimport numpy as np\n> \n> # Data setsIRIS_TRAINING = \"iris_training.csv\"IRIS_TEST = \"iris_test.csv\"\n> \n> # Load datasets.\n> \n> training_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TRAINING,\n>                                                        target_dtype=np.int, features_dtype=np.float)\n> test_set = tf.contrib.learn.datasets.base.load_csv_with_header(filename=IRIS_TEST,\n>                                                    target_dtype=np.int, features_dtype=np.float)\n> \n> # Specify that all features have real-value data\n> \n> feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n> \n> # Build 3 layer DNN with 10, 20, 10 units respectively.\n> \n> classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n>                                             hidden_units=[10, 20, 10],\n>                                             n_classes=3,\n>                                             model_dir=\"/tmp/iris_model\")\n> \n> # Fit model.\n> \n> classifier.fit(x=training_set.data,\n>                y=training_set.target,\n>                steps=2000)\n> \n> # Evaluate accuracy.\n> \n> accuracy_score = classifier.evaluate(x=test_set.data,\n>                                      y=test_set.target)[\"accuracy\"]print('Accuracy: {0:f}'.format(accuracy_score))\n> \n> # Classify two new flower samples.\n> \n> new_samples = np.array(\n>     [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\n> y = classifier.predict(new_samples)print('Predictions: {}'.format(str(y)))\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/5284#issuecomment-259295822,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ARx8HpaIqAMTsisnKou1-JI8AORVKVisks5q8QpmgaJpZM4KkWSW\n> .\n\n## \n\nbest regards,\n\nDini Rahmawati Akmalia\ndr.akmalia@gmail.com\n+62896-5488-4113\nMathematics Faculty of Mathematics and Natural Science University Indonesia\n2010\n_Outstanding Life_\n", "Could you provide a working example? i.e. can you show me the code that you've written that causes this error?\n", "This is the same issue as #4715 \nIt is already fixed in master, but not in v0.11.0\n", "Got it. Do you know which commit fixed it by any chance? It might be worth for us to cherry-pick into 0.11.\n", "@martinwicke\nMight knows about it.\n", "@gunan @yifeif do we still cherry-pick 0.11 or have we moved to 0.12?\n", "We already released 0.11.\n", "I tried to use the compiled version  tensorflow  and it seems that this has been fixed.\n", "I am having the same problem, i installed tensorflow using the anaconda instructions, how can i upgrade my installation in order to fix the problem?", "You can run pip install command with the --upgrade command to install the\nnew version while you have the old version.\n\nOn Nov 26, 2016 12:03 PM, \"cjcarvajal\" <notifications@github.com> wrote:\n\n> I am having the same problem, i install tensorflow using the anaconda\n> instructions, how can i upgrade my installation in order to fix the problem?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/5284#issuecomment-263082743>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AHlCOX7vic0_YtNwL5cllFHuUJNDxEpLks5rCJCDgaJpZM4KkWSW>\n> .\n>\n"]}, {"number": 5283, "title": "Is there a way of 'cutting' a network?", "body": "I wonder if there is a way of 'cutting' the network. What I want to achieve is fine-tuning of the whole inception v3 network and then implement transfer learning by replacing the last 2 inception modules with new layers. I did the whole fine-tuning using this code https://github.com/tensorflow/models/tree/master/inception on my own dataset taking the last checkpoint that I need but now I don't know how I can implement the transfer learning by taking the output of 8th inception module and train the new layers. Is there a standard tensorflow-way of doing this operation? \n", "comments": ["You can keep the layers in the graph. Just add a new connection and `sess.run(x)`.\n", "If I add a new connection, I will have new variables and I think I will have problem with the checkpoint restoration.\n", "I have worked at this code https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py\ntoo and the transfer learning here is easy to understand, but if I decide to implement it in this way I need to create a protocol buffer file for my 'fine-tuning' graph. But I hope there is an easier way.\n", "Unless there is a bug you suspect, please re-ask this issue on StackOverflow. That forum is a better place to ask \"how do I do something.\" Thanks!\n"]}, {"number": 5282, "title": " Couldn't open CUDA library libcupti.so.7.5", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System: Linux Ubuntu 14.04 LTS (64bit)\n\nInstalled version of CUDA and cuDNN:  CUDA7.5, cuDNN4\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`): \n\n-rw-r--r-- 1 root root 189170 Apr 23  2016 libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Apr 23  2016 libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Apr 23  2016 libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Apr 23  2016 libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Apr 23  2016 libcudart_static.a\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`) c82f4962b9b8ba871880958a459d6d4f3587111b\n2. The output of `bazel version` 0.3.2\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\ncodes are from: https://github.com/tensorflow/tensorflow/raw/r0.11/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n\ngoes wrong here:\n\n```\nif i % 100 == 99:  # Record execution stats\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        run_metadata = tf.RunMetadata()\n        summary, _ = sess.run([merged, train_step],\n                              feed_dict=feed_dict(True),\n                              options=run_options,\n                              run_metadata=run_metadata)\n        train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n        train_writer.add_summary(summary, i)\n        print('Adding run metadata for', i)\n```\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n\n```\nAccuracy at step 0: 0.1045\nAccuracy at step 10: 0.6843\nAccuracy at step 20: 0.8074\nAccuracy at step 30: 0.8511\nAccuracy at step 40: 0.878\nAccuracy at step 50: 0.8869\nAccuracy at step 60: 0.89\nAccuracy at step 70: 0.8928\nAccuracy at step 80: 0.893\nAccuracy at step 90: 0.8945\nI tensorflow/stream_executor/dso_loader.cc:119] Couldn't open CUDA library libcupti.so.7.5. LD_LIBRARY_PATH: /home/wk15/intel/mkl/lib/intel64:/usr/local/cuda/lib64:/home/wk15/intel/mkl/lib/intel64::/home/wk15/intel/mkl/lib/:/home/wk15/lib:/home/wk15/local/lib:/home/wk15/intel/mkl/lib/:/home/wk15/lib:/home/wk15/local/lib:/home/wk15/boost-1-61-binary/lib\nF tensorflow/core/platform/default/gpu/cupti_wrapper.cc:59] Check failed: ::tensorflow::Status::OK() == (::tensorflow::Env::Default()->GetSymbolFromLibrary( GetDsoHandle(), kName, &f)) (OK vs. Not found: /home/wk15/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so: undefined symbol: cuptiActivityRegisterCallbacks)could not find cuptiActivityRegisterCallbacksin libcupti DSO\n```\n", "comments": ["It looks like you need to install the package\n\n```\nsudo apt-get install libcupti-dev\n```\n\n@gunan this looks like this is a new requirement. `libcupti` appears to be a dev profiling library. Not sure when we introduced this requirement.\n", "@drpngx it works for me. thanks!\n", "Nice!\n", "@aselle @zheng-xq @martinwicke \nI think this is a clue right here:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu#L90\n\nAny idea when/how we started depending on libcupti?\n", "@yifeif \n", "I just ran into this issue with a fresh install of r0.12 with pip. Running `sudo apt-get install libcupti-dev` installed the missing dependency, however, it appears that this is may still be a problem.\r\n", "It is now clear that we are depending on libcupti.\r\nWe will add it to the list of our dependencies.", "Great to hear, thanks for the quick response.", "This seems to be an issue with the current Docker-gpu image as well. That is, `libcupti-dev` is missing. ", "The libcupti.dylib file is in the CUDA extras folder (e.g., /Developer/NVIDIA/CUDA-8.0/extras/CUPTI/lib/libcupti.8.0.dylib).  \r\n\r\nI created a symbolic link from /usr/local/cuda/lib with \r\n\r\n    sudo ln -s /Developer/NVIDIA/CUDA-8.0/extras/CUPTI/lib/libcupti.8.0.dylib libcupti.dylib\r\n\r\n"]}, {"number": 5281, "title": "feature: permutation", "body": "This is a feature request on a Tensor creation function based on problems met when using tf.transpose.\nSuppose I want to transpose a Tensor `a` (not variable) into a new Tensor, exchanging its two dimensions. Indices of the two dimensions are represented by two Tensors `b` and `c`.\nFrom what I understand, there are only one complicated way of generating the argument `perm` in tf.transpose to achieve this in Tensorflow. That's by using tf.select twice.\n\n``` python\ndims = tf.range(5)\nb = tf.constant(1)\nc = tf.constant(3)\nb_mask = tf.cast(tf.one_hot(b, 5), tf.bool)\nc_mask = tf.cast(tf.one_hot(c, 5), tf.bool)\nbs = tf.ones(5, tf.int32) * b\ncs = tf.ones(5, tf.int32) * c\nperm_1 = tf.select(b_mask, cs, dims)\nperm_2 = tf.select(c_mask, bs, perm_1)\n# ==> perm2: [0, 3, 2, 1, 4]\n```\n\nI personally feel this is too redundant for achieving such a simple operation. Could it be possible to add some op that generate permutations when it is not possible to be achieved by tf.gather (target dims are dynamic)?\n", "comments": ["Sorry, what is the problem with `tf.transpose`? Maybe in a larger example?\n", "@drpngx Sorry for not being clear. I mean generating `perm` for tf.transpose() is a little too complex in this simple condition. Could it be possible to add an op that help generate Tensors like `perm`?\n", "@rmlarsen for advice\n", "I suppose something like tf.swap(a,i,j), where a, i, j are tensors, that swap a[i[:], ...] and a[j[:], ...] would be a reasonable thing to add (sort of a companion to gather).\n", "@mrry Maybe Derek has a clever idea how to do this?\n", "Nothing springs to mind alas. I did a little search for NumPy implementations of the same operation, and everything I found used in-place updates, which won't really work for the non-Variable-tensor case. I did notice that NumPy has [a `swapaxes()` function](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.swapaxes.html), which looks like it would solve the original problem, and if we added one it would keep our API in sync with NumPy.\n", "Thanks @rmlarsen @mrry . `tf.swapaxes` sounds good.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 5280, "title": "Ubuntu 16.04 - GPU-enabled binary for Python3.5 in Anaconda - NVidia Compute Capability 2.1 GPU not detected", "body": "### Issue\n\nI have an old GPU (GTX 560 Ti) with a power compute of 2.1, according to the Nvidia website. \n\nI am using Anaconda3 on Ubuntu, and I used the following binary to install Tensorflow:\n\n```\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc1-cp35-cp35m-linux_x86_64.whl\n```\n\nTensorflow is running, but trying the [Tensorflow.org test about using GPUs](https://www.tensorflow.org/versions/r0.11/how_tos/using_gpu/index.html), I get this output:\n\n```\nDevice mapping: no known devices.\nI tensorflow/core/common_runtime/direct_session.cc:252] Device mapping:\n\nMatMul_5: /job:localhost/replica:0/task:0/cpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:819] MatMul_5: /job:localhost/replica:0/task:0/cpu:0\n```\n\nTrying a manual device placement using `with tf.device('/gpu:0'):`, I get \n\n```\nInvalidArgumentError: Cannot assign a device to node 'MatMul_4': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0\n     [[Node: MatMul_4 = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/device:GPU:0\"](a_4, b_4)]]\n```\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI found this [https://github.com/tensorflow/tensorflow/issues/227](old issue): Compute capability < 3.5. The OP had the same GPU, but it was for a build from source.\n### Environment info\n\nOperating System: **Ubuntu 16.04**\n\nInstalled version of CUDA and cuDNN: **8.0.27 + 5.1.5**\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root   560184 Okt 29 20:06 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Okt 29 20:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Okt 29 20:06 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 Okt 29 20:05 /usr/local/cuda/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 Okt 29 20:06 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 79337624 Okt 29 20:11 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 79337624 Okt 29 20:11 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 79337624 Okt 29 20:11 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Okt 29 20:11 /usr/local/cuda/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed:\n\n```\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc1-cp35-cp35m-linux_x86_64.whl\n```\n1. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n0.11.0rc1\n```\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\nI tried the example in the test from the link above:\n\n```\n# Creates a graph.\nwith tf.device('/gpu:0'):\n  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n  c = tf.matmul(a, b)\n# Creates a session with log_device_placement set to True.\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n# Runs the op.\nprint sess.run(c)\n```\n", "comments": ["Sorry, I don't think TF works with nvidia compute 2.1 -- our install page says that we require 3.0 and above. :(\n"]}, {"number": 5279, "title": "- fix for memory corruption: on windows use _aligned_free() for memor\u2026", "body": "\u2026y allocated with aligned_alloc()\n- enabled sparse_matmul and immutable_constant_op\n- don't print LD_LIBRARY_PATH on error for windows (its null and crashes)\n  with this python kernel tests pass (well, some error left related to stl float Inf)\n", "comments": ["Can one of the admins verify this patch?\n", "@guschmue, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @tensorflower-gardener and @keveman to be potential reviewers.\n", "Jenkins, test this please.\n", "Woops, it doesn't like `std::bind` and other things. Please take a look at the errors:\n\nhttps://ci.tensorflow.org/job/tensorflow-pull-requests-multijob/2416/\n\nAlso, if you're comfortable with git, you could split the commits.\n", "ah, should have tried on linux first. Working on it.\n", "looking over this, you are right: let me split this into a couple of commits. That also helps to get this aligned_free fix in.\n", "closing this one and file some more targeted pr's instead. Going to reference this pr so we don't lose the review feedback.\n", "Please merge with #5207, which will probably create some conflicts on the `meta_support.*`\n", "I closed this pr and split it - too many things in it. I think I'll split it in 4 prs:\n1. the memory corruption (addressed your review comments and send a new pr  #5288)\n2. enable sparse mat ops (nearly done)\n3. enable immutable ops\n4. enable debug_op \nI'll make sure sure they are on top of the meta_support.\\* changes \nHope that works.\n", "nice, thanks!\n", "BTW, for the `std::bind` thing, we're seeing an error in #5286, possibly the easiest fix would be to use `SparseSlice<TL>*` instead of `auto*`.\n", "Oh, BTW, flipping through some code today, it occurred to me that we should probably `#define WIN32_LEAN_AND_MEAN 1` before including windows.h headers. I usually don't like to have them in header files, only in cpp files, because there a ton of symbols in there.\n", "Ah, lean and mean we have: gets added in CMakeLists.txt to the compiler defines for windows.\nGoing to try SparseSlice<TL>\\* shortly. Would love to get rid of the std::bind() without a ifdef.\n", "turns out I did not need the std::bind(). We had made this change before nvidia support vs2015.update3 (we used vs2015.update1 until the official cuda 8.0 came out). Now that the std:bind() made trouble for gcc I started over on sparse_maxmul_op.cc and its fine without std:bind(): builds on linux and windows.\n", "Ah, OK, great to hear! Thanks for making these changes!\n"]}, {"number": 5278, "title": "add type info described in issue #5236", "body": "A pr add the type info mentioned in [https://github.com/tensorflow/tensorflow/issues/5236](https://github.com/tensorflow/tensorflow/issues/5236)\n", "comments": ["@burness, thanks for your PR! By analyzing the history of the files in this pull request, we identified @vrv, @keveman and @asimshankar to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it!\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "Can one of the admins verify this patch?\n", "@vrv  Hi,  I have add this type info to the LOG message. Please review\n", "Nevermind, I did it for you (apparently GitHub allows that).\n\n@tensorflow-jenkins test this please\n", "This can't break cmake, so assuming infra failure.\n"]}, {"number": 5277, "title": "Eigen implemented CPU op is 10 times slower than OpenMP", "body": "I implemented a phase CPU operator consisting of four loop levels. I couldn't find any Eigen tensor docs at that stage so I use OpenMP to trivially parallelise the outer loops. I recently found the Eigen tensor documentation so I thought I'd take advantage of it and get all the multithreading/AVX/SSE goodies for free!\n\nUnfortunately the Eigen version is about 10 times slower!\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n### Environment info\n\nOperating System: Ubuntu 16.04\n\nInstalled version of CUDA and cuDNN: CUDA 8.0 and cuDNN 5.1\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n$ ls -l /usr/local/cuda-8.0/lib64/libcud*\n-rw-r--r-- 1 root root 558720 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root     19 Sep 15 01:05 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rw-r--r-- 1 root root 415432 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root 775162 Sep 15 01:02 /usr/local/cuda-8.0/lib64/libcudart_static.a\n```\n\n```\n$ ls -l /usr/local/cudnn-5.1-cuda-8.0/lib64/lib*\nlrwxrwxrwx 1 root root       13 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5\nlrwxrwxrwx 1 root root       17 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5\n-rwxr-xr-x 1 root root 79337624 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Oct 28 10:07 /usr/local/cudnn-5.1-cuda-8.0/lib64/libcudnn_static.a\n```\n\nIf installed from binary pip package, provide:\n1. A link to the pip package you installed: python 2.7 linux GPU nightly\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\n$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so locally\n0.11.0rc1\n```\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`)\n2. The output of `bazel version`\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n- [Source code](https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/phase_op_cpu.h).\n- [Makefile](https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/Makefile). In terms of optimisations I'm using **-O2** and **-fopenmp**\n- [Test Case](https://github.com/ska-sa/montblanc/blob/07fa1dd8860de8f3e95a1ceada7524bdaa5c87a7/montblanc/impl/rime/tensorflow/rime_ops/test_phase.py) and timing code.\n#### OpenMP\n- Timings\n\n```\nTensorflow custom GPU time 0.342187\nTensorflow expression GPU time 0.270267\nTensorflow CPU time 0.417076\nNumpy CPU time 2.542890\n```\n- Code\n\n``` cpp\n// Compute the complex phase\n#pragma omp parallel for\nfor(int src=0; src<nsrc; ++src)\n{\n    FT l = lm(src,0);\n    FT m = lm(src,1);\n    FT n = std::sqrt(1.0 - l*l - m*m) - 1.0;\n\n    for(int time=0; time<ntime; ++time)\n    {\n        for(int antenna=0; antenna<na; ++antenna)\n        {\n            FT u = uvw(time,antenna,0);\n            FT v = uvw(time,antenna,1);\n            FT w = uvw(time,antenna,2);\n\n            FT real_phase_base = minus_two_pi_over_c*(l*u + m*v + n*w);\n\n            for(int chan=0; chan<nchan; ++chan)\n            {\n                // Our real phase input to the exponential function is purely imaginary so we can\n                // can elide a call to std::exp<complex<FT>> and just compute the cos and sin\n                FT real_phase = real_phase_base*frequency(chan);\n                complex_phase(src,time,antenna,chan) = { std::cos(real_phase), std::sin(real_phase) };\n            }\n        }\n    }\n}\n```\n#### Eigen\n- Timings\n\n```\nTensorflow custom GPU time 0.344653\nTensorflow expression GPU time 0.275525\nTensorflow CPU time 9.616667\nNumpy CPU time 2.505482\n```\n- Code\n\n``` cpp\n// Doing it this way might give us SIMD's and threading automatically...\nconst CPUDevice & device = context->eigen_device<CPUDevice>();\n\n// Shapes for reshaping and broadcasting\nEigen::DSizes<int, 4>   lm_shape(nsrc, 1,     1,  1    );\nEigen::DSizes<int, 4>  uvw_shape(1,    ntime, na, 1    );\nEigen::DSizes<int, 4> freq_shape(1,    1,     1,  nchan);\n\nauto l = lm.slice(\n        Eigen::DSizes<int, 2>(0,    0),\n        Eigen::DSizes<int, 2>(nsrc, 1))\n    .reshape(lm_shape);\nauto m = lm.slice(\n        Eigen::DSizes<int, 2>(0,    1),\n        Eigen::DSizes<int, 2>(nsrc, 1))\n    .reshape(lm_shape);\n\nauto u = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  0),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\nauto v = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  1),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\nauto w = uvw.slice(\n        Eigen::DSizes<int, 3>(0,     0,  2),\n        Eigen::DSizes<int, 3>(ntime, na, 1))\n    .reshape(uvw_shape);\n\n// Compute n\nauto n = (l.constant(1.0) - l*l - m*m).sqrt() - l.constant(1.0);\n\n// Compute the real phase\nauto real_phase = (\n    l.broadcast(uvw_shape)*u.broadcast(lm_shape) +\n    m.broadcast(uvw_shape)*v.broadcast(lm_shape) +\n    n.broadcast(uvw_shape)*w.broadcast(lm_shape))\n        .broadcast(freq_shape);\n\n// Reshape and broadcast frequency to match real_phase\nauto f = frequency.reshape(freq_shape).broadcast(\n    Eigen::DSizes<int, 4>(nsrc, ntime, na, 1));\n\n// Calculate the phase\nauto phase = real_phase*f*real_phase.constant(minus_two_pi_over_c);\nauto sinp = phase.unaryExpr(Eigen::internal::scalar_sin_op<FT>());\nauto cosp = phase.unaryExpr(Eigen::internal::scalar_cos_op<FT>());\n\n// Now evaluate the complex phase on the device\n// by combining the cosine and sine of the phase\n// to form a complex number\ncomplex_phase.device(device) = cosp.binaryExpr(\n    sinp, make_complex_functor<FT>());\n```\n### What other attempted solutions have you tried?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n", "comments": ["Thanks! That's pretty interesting. That's an Eigen issue technically, but it might prompt us to have both ops available, at least for comparison's sake.\n\n@annarev @benoitsteiner \n", "It's likely that part of this problem is that sinp and cosp depends on a common sub expression, phase. Eigen is probably evaluating it, and most of the computation,  twice, _if_ it doesn't reuse phase. Even so, there's about an order of magnitude difference in the implementations.\n\nI'll try force early evaluation of phase some time to confirm.\n", "Early evaluation of the common sub expression does reduce the overall Eigen compute time by a factor of 2 to 4.71 seconds.\n", "@sjperkins Can you try to see how much speedup you get from IndexList ? For example, you can replace Eigen::DSizes<int, 3>(0,     0,  1) with Eigen::IndexListEigen::type2index<0, Eigen::type2index<0>, Eigen::type2index<1>>.\n\nYou can also mix in runtime values. For example, you could replace Eigen::DSizes<int, 4>   lm_shape(nsrc, 1,     1,  1    ); with Eigen::IndexList<int, Eigen::type2index<1>, Eigen::type2index<1>, Eigen::type2index<1>> list; list.set(0, nsrc);\n", "@benoitsteiner OK, with `IndexList` and early evaluation of the `phase` sub-expressions Eigen now takes `2.9` seconds vs OpenMP's `0.41` seconds. Did this in https://github.com/ska-sa/montblanc/commit/7b7a43aff8f1ad537d23e44051ecb51d507d8ccf.\n\nRelevant code is here:\n\n``` cpp\n// Doing it this way might give us SIMD's and threading automatically...\nconst CPUDevice & device = context->eigen_device<CPUDevice>();\n\nusing idx0 = Eigen::type2index<0>;\nusing idx1 = Eigen::type2index<1>;\nusing idx2 = Eigen::type2index<2>;\n\n// Shapes for reshaping and broadcasting\nEigen::IndexList<int, idx1, idx1, idx1> lm_shape;\nlm_shape.set(0, nsrc);\n\nEigen::IndexList<idx1, int, int, idx1> uvw_shape;\nuvw_shape.set(1, ntime);\nuvw_shape.set(2, na);\n\nEigen::IndexList<idx1, idx1, idx1, int> freq_shape;\nfreq_shape.set(3, nchan);\n\nEigen::IndexList<idx0, idx0> l_slice_offset;\nEigen::IndexList<idx0, idx1> m_slice_offset;\n\nEigen::IndexList<int, idx1> lm_slice_size;\nlm_slice_size.set(0, nsrc);\n\n// Slice lm to get l and m arrays\nauto l = lm.slice(l_slice_offset, lm_slice_size)\n    .reshape(lm_shape);\nauto m = lm.slice(m_slice_offset, lm_slice_size)\n    .reshape(lm_shape);\n\nEigen::IndexList<idx0, idx0, idx0> u_slice_offset;\nEigen::IndexList<idx0, idx0, idx1> v_slice_offset;\nEigen::IndexList<idx0, idx0, idx2> w_slice_offset;\nEigen::IndexList<int, int, idx1> uvw_slice_size;\nuvw_slice_size.set(0, ntime);\nuvw_slice_size.set(1,  na);\n\n// Slice uvw to get u, v and w arrays\nauto u = uvw.slice(u_slice_offset, uvw_slice_size)\n    .reshape(uvw_shape);\n\nauto v = uvw.slice(v_slice_offset, uvw_slice_size)\n    .reshape(uvw_shape);\n\nauto w = uvw.slice(w_slice_offset, uvw_slice_size)\n    .reshape(uvw_shape);\n\n// Compute n\nauto n = (l.constant(1.0) - l*l - m*m).sqrt() - l.constant(1.0);\n\n// Compute the real phase\nauto real_phase = (\n    l.broadcast(uvw_shape)*u.broadcast(lm_shape) +\n    m.broadcast(uvw_shape)*v.broadcast(lm_shape) +\n    n.broadcast(uvw_shape)*w.broadcast(lm_shape))\n        .broadcast(freq_shape);\n\nEigen::IndexList<int, int, int, idx1> freq_broad;\nfreq_broad.set(0, nsrc);\nfreq_broad.set(1, ntime);\nfreq_broad.set(2, na);\n\n// Reshape and broadcast frequency to match real_phase\nauto f = frequency.reshape(freq_shape).broadcast(freq_broad);\n\n// Evaluate common sub-expression early so that its\n// not recomputed twice for sin and cosine.\nEigen::Tensor<FT, 4, Eigen::RowMajor> phase(nsrc, ntime, na, nchan);\nphase.device(device) = real_phase*f*real_phase.constant(minus_two_pi_over_c);\n// Calculate the phase\n//auto phase = real_phase*f*real_phase.constant(minus_two_pi_over_c);\nauto sinp = phase.unaryExpr(Eigen::internal::scalar_sin_op<FT>());\nauto cosp = phase.unaryExpr(Eigen::internal::scalar_cos_op<FT>());\n\n// Now compute the complex phase by combining the cosine\n// and sine of the phase to from a complex number\ncomplex_phase.device(device) = cosp.binaryExpr(\n    sinp, make_complex_functor<FT>());\n\n```\n", "There are 2 simple things you can do to speed things up:\n   l_l -> l.square()\n   m_m -> m.square()\n\nAlso, you're using l and m both to compute real_phase and n: you could force their evaluation (like you've done for phase) to avoid their recomputation.\n\nLast but not least, try to add an eval() before the broadcasts. For example, u.broadcast(lm_shape) will probably evaluate faster if you write it as u.eval().broadcast(lm_shape)\n", "Those changes take the total evaluation time to between `2.87` and `2.89` seconds.\n\nImplemented in https://github.com/ska-sa/montblanc/commit/2400e551861e633c3bc27bbf0ee1f36ef7ab03fd.\n", "@tfboyd If we decide to worry more about CPU performance, this information may come in handy. Not sure whether this holds up considering AVX or MKL.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "@tatianashp FYI for the ongoing OpenMP discussions.", "It's a custom op with two implementations - OpenMP and Eigen. OpenMP is default.   The code is at https://github.com/ska-sa/montblanc/blob/master/montblanc/impl/rime/tensorflow/rime_ops/phase_op_cpu.h.  Performance gain from using OpenMP is impressive. Thanks for the pointer.\r\n\r\n"]}, {"number": 5276, "title": "TensorBoard histograms NaN value handling", "body": "Recently I had to fight a lot with exploding gradients and thereby weights turning NaN after only several iterations event hough I was using _tf.contrib.layers.variance_scaling_initializer_ correctly. To monitor the problem I started saving weight histograms which seemed totally messed up. I thought this was due to me using the initializer wrongly and only after some debugging I realised my initialization was totally fine. The NaN values appearing in iteration _n_ seem to mess with the visualization of values in previous iterations which seems like a very misleading behaviour to me. Histograms and distributions should be visualized correctly until this is not really possible because of ill values.\n\nI can easily provide tensorflow summary data to reproduce this behaviour.\n", "comments": ["@danmane not sure what the proper thing to do with NaN is, maybe report them in a separate counter or ignore?\n", "Yes, @timmeinhardt please attach summary data in case we want to repro.\n", "Yes, of course. Here is some example summary data. All weights were initialised with the default parameters for _tf.contrib.layers.variance_scaling_initializer_.\n\n[events.out.tfevents.1477816723.Tims-MacBook-Pro.local.zip](https://github.com/tensorflow/tensorflow/files/560359/events.out.tfevents.1477816723.Tims-MacBook-Pro.local.zip)\n", "I migrated this issue to tensorflow/tensorboard#87 because TensorBoard has moved into its own, separate repo. Lets continue discussion there.", "The weights being written to the histogram needs to be clipped. \r\nAdding this line in `def _log_weights(self, epoch)` should help:\r\n`weight = tf.clip_by_value(weight, -1e12, 1e12)`"]}, {"number": 5275, "title": "[Windows] Cannot import _pywrap_tensorflow when zlib.dll is not in PATH", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\n[#1013](https://github.com/tensorflow/tensorflow/issues/1013)\netc...\n### Environment info\n\nOperating System:\n\nWindows 10\nInstalled version of CUDA and cuDNN: \nNo CUDA (\"CPU support only\")\n\nPip wheel was self-build. Here's the output of `python -c \"import tensorflow; print(tensorflow.__version__)\"`: [in attatchment](https://github.com/tensorflow/tensorflow/files/559847/output.txt)\n\nIf installed from source, provide \ngit-rev output - 3737ac321e67410bf061257d5f644eae8abbf79b\nNo bazel (is it required?)\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n`import tensorflow`\n### What other attempted solutions have you tried?\n\nInstall with `python setup.py install` and generally avoid any tedious building/making but with no success.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment or provide link).\n[output.txt](https://github.com/tensorflow/tensorflow/files/559849/output.txt)\n", "comments": ["We now have windows GPU support with cmake. It should work with cuda 6.5.\n", "That's great that you do!\nBut i don't have Nvidia, sorry :P\nOh, and I focused so much on that form that I forgot to write what's wrong xD\nBut that seems evident from the logs. It installs correctly (no cmake errors) but builds with warnings. I have the wheel and folder with setup.py. Both give the same error on import.\n", "Oh, I see. Well, you have to debug why it's not loading. There are usually two reasons for that, and python would typically give the same error message:\n- The _pywrap_tensorflow.dll is not found in the path. You have to fiddle with the system paths. There are lots of hits on the web to figure that out\n- the dll itself does not resolve some transitive dependencies (e.g. it cannot find libcudart). You can download the dependency walker for that\n", "I copied the dll and libs and other stuff to Python folders already, with no success. But I tried the dependency walker, and here's what I found:\nAll API-MS-_.dll are missing\nAll EXT-MS-_.dll are missing\nalso DEVICECLOCKHELPERS.DLL, EMCLIENT.DLL, ZLIB.DLL are missing\n", "try getting zlib1 or zlib in place. The dirtiest, but easiest, is probably to put in in `C:\\windows\\system`. If you poke around the installation directory these DLLs should be there.\n", "I copied this zlib.dll to some places, but nothing changed. And I couldn't find those previous two (or any of those prefixed with *-MS-[...])\n", "Can you give the full names?\n", "Here you go: [dll_names.txt](https://github.com/tensorflow/tensorflow/files/561571/dll_names.txt)\n", "@mrry, do you have any further insight?. I have no way to test this. You might try adding the DLL directories to PYTHONPATH and your windows PATH. \n", "I looked at one of them and it might be part of the msvc crt. Can you run again with a shell that has the MSVC path etc enabled? \nThere should be a batch file somewhere in program files x86 microsoft visual studio\\* binn or something like that.\n\nAlternatively, try installing https://www.microsoft.com/en-us/download/details.aspx?id=48145 on your system.\n", "Only suggestion I can think of is to try again with Anaconda 3.5, which is the only Python distribution that I've successfully used to build on Windows. (I'm assuming from the paths in your log that you're not using Anaconda.) In particular, it includes NumPy and zlib libraries which might be required by various parts of the build process.\n", "I'll check with that msvc crt later today.\nI understand that there are out-of-the-box implementations of python for data science, but unfortunately I cannot use it in the project.\nThe truth is that I switched to sklearn since time is of the essence here, but if I'm able to solve that in my free time I might devise some sort of tutorial on how to solve problems that I've encountered.\n\n@drpngx  Oh and about that script, you told me to run something with a shell again, but do you mean simply to run python and try to import again, or try to build/install again?\n", "I ran into this problem too after `pip install`ing the cmake-built `.whl` (this is on vanilla Python 3.5 64-bit and built with VS2015). Although dependency walker suggested a bunch of missing dlls, the only one it actually needed was `zlib.dll`; after copying that from the `build` directory into the package installation directory under `site-packages`, the import worked just fine. I realize this is something that @KamodaP already tried unsuccessfully, but I thought it'd be worth chiming in anyway.\n\nTo that effect, should `zlib.dll` be vendored inside the wheel by cmake, or can it be listed as a dependency?\n", "I haven't done one thing yet that I think I might try. Because I've copied the dll to DLLs in Python (`C:\\[...]\\Python35\\DLLs`) directory, perhaps that's not the right thing to do and I should put it in `C:\\Windows` directory?\n", "@reiv Same story here, same mistake and copying `zlib.dll` helped, thanks for the suggestion. I too think the wheel could address this issue better.\n", "@zandaqo did you copy it to windows directory?\n", "@KamodaP No, just a random directory that was in my `PATH`. I'm not yet sure where to give a permanent place, so I checked by adding custom directory with `zlib.dll` to my `PATH` and it solved the importing issue.\n", "Hi folks, thanks for digging into this! It looks like we end up depending on both the static and dynamic versions of `zlib` on Windows, for some reason... I'm guessing it's one of the external project on which we depend, maybe libpng?\n\nI'm going to rename the bug and I'll look into removing this dependency on `zlib.dll`.\n", "So it does work after putting zlib.dll into a directory that is referenced in PATH environment variable, but it DOES NOT work when put in directory that is in PYTHONPATH (or something like that) so including DLLs directory.\n", "In win10 x64 cmake error \n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - found\n-- Found Threads: TRUE\nthis is the only error appeared during cmake ,is it very important for the msbuild?\nor i can just ignore it.\nWith ignore it ,i msbuild the project ,but stuck during clone protobuf and jsoncpp ,is it related the missing of pthreads.lib?\ni have successfully install the pthread protobuf and jsoncpp by testing in vs2015, so i don' t think the issue comes from installing .\nIn vs2015 i use prgma linking to the pthreads.lib where can i add prgma in cmake files ,i haven;t find the target file, so i add prgma to every (5) file including pthread.h ,how can i fix this problem .\n"]}, {"number": 5274, "title": "Update classifier.fit()", "body": "Needed to change input parameters as followed by tutorial too.\n", "comments": ["@tegg89, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @jart and @MrQianJinSi to be potential reviewers.\n", "Can one of the admins verify this patch?\n", "Hi @tegg89,\n\nThanks for submitting this PR. As you note, this tutorial needs to be updated for TF version 0.11. We're planning to release a new version in the next few weeks.\n\nIn the meantime, I'm not sure there's much value in removing the monitors argument from `fit()`, as the purpose of the example is to train using monitors. I'm going to close this PR, as we'll address the issues with the example code in a more comprehensive tutorial update.\n\nThanks again for your submission flagging this issue,\nSanders\n"]}]