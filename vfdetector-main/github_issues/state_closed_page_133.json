[{"number": 50949, "title": "Update losses.py", "body": "Changed the Code Example of Binary Cross Entropy to Fix https://github.com/keras-team/keras/issues/14955", "comments": ["It looks like your PR relates to the Keras component. Please submit it to the [github.com/keras-team/keras](github.com/keras-team/keras)  repository instead. Thankyou.\r\n@fchollet, @qlzh727"]}, {"number": 50948, "title": "import tensorflow get a bug", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):pip install tensorflow-gpu 2.5.0\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:cuda 11.4 \r\n- cudnn :libcudnn8_8.2.1.32-1+cuda11.3_amd64.deb\r\nlibcudnn8-dev_8.2.1.32-1+cuda11.3_amd64.deb\r\nlibcudnn8-samples_8.2.1.32-1+cuda11.3_amd64.deb\r\n\r\n- GPU model and memory:nvidia 2080s 8g \r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I upgraded tensorflow in the virtual environment anaconda, there was a bug that couldn't run. I don't know what the problem is. I have reinstalled the driver and CUDA cudnn, but it still doesn't work. There should be no mistake in installing the three files of cudnn. It seems that it reads tensorflow directly from the virtual environment rather than the absolute path. I don't know what the problem is. It should be a tensorflow bug. In addition, my soft link settings are as follows:\r\n\r\n\r\nexport PATH=$PATH:/usr/local/cuda-11.4/bin\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.4/lib64\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.4/extras/CUPTI/lib64\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n2021-07-26 19:11:29.807559: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\nTraceback (most recent call last):\r\n  File \"/home/zy/Project_save/stock_hht_pred/code/online_prediction_market.py\", line 8, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/zy/.local/lib/python3.6/site-packages/tensorflow/__init__.py\", line 444, in <module>\r\n    _ll.load_library(_main_dir)\r\n  File \"/home/zy/.local/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py\", line 154, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /home/zy/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@anavanab99 ,\r\n\r\nEvery TensorFlow release is compatible with a certain version, for more information please take a look at the tested build [configurations](https://www.tensorflow.org/install/source#gpu).In this case, can you please try installing TensorFlow v2.5 with CUDA 11.2 and cuDNN 8.1 and check if you are facing the same error. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50948\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50948\">No</a>\n"]}, {"number": 50947, "title": "TF-TRT Apply batch size check for FC conversion path of MatMul", "body": "The TF-TRT conversion of the MatMul op has two possible paths: one using TRT IMatrixMultiplyLayer and one using IFullyConnectedLayer (FC). This PR fixes a bug in the FC path: having batch dims (here batch dims mean leading dims of the matrices, eg B.shape[0:-2]) larger than one are not compatible with TensorRT, therefore we cannot use the FC conversion path. In such case we have to use the IMatrixMultiplyLayer.\r\n\r\nTagging @bixia1 for review and @DEKHTIARJonathan for visibility.", "comments": ["@bixia1 I have resolved the problem with test data initialization."]}, {"number": 50946, "title": "Build TensorFlow Lite for Android and reduce binary size to use with C API ", "body": "Hello, \r\n\r\nI would like to build TensorFlow Lite for Android reducing the binary size. I want to optimize its size based on the operations used by my model. Then, I want to use the generated `.so` for `arm64-v8a` and `armeabi-v7a` in JNI with C API.\r\n\r\nI am able to build TensorFlow Lite for Android as described in [Build Android](https://www.tensorflow.org/lite/guide/build_android#build_and_install) and use it in my Android Studio project. However, the binary files are too big for my application. In order to reduce the binary size, I followed the instructions described in [Reduce Binary Size](https://www.tensorflow.org/lite/guide/reduce_binary_size), but as stated in [Known Limitations](https://www.tensorflow.org/lite/guide/reduce_binary_size#known_issueslimitations) it does not support C API. The generated `.aar` does not contain the C headers and when I try to use the `.so` files in my Android Studio project, none of the C functions are available.\r\n\r\nIs there any other way to reduce TensorFlow Lite binary size to be used in JNI with C API?\r\n\r\nThanks.", "comments": ["Bazel can do the selective build mechanism for C API.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/c/BUILD#L115", "The tflite_custom_c_library build rule can generate the selectively built TensorFlow Lite C library.", "Cool, I replaced `tflite_custom_c_library` build rule with\r\n\r\n```\r\ntflite_custom_c_library(\r\n    name = \"selectively_built_c_api_test_lib\",\r\n    testonly = 1,\r\n    models = [\r\n        \"//tensorflow/lite:testdata/mymodel1.bin\",\r\n        \"//tensorflow/lite:testdata/mymodel2.bin\",\r\n    ],\r\n    visibility = [\"//visibility:private\"],\r\n)\r\n```\r\n\r\nThen, built `armeabi-v7a` with\r\n\r\n```\r\nbazel build -c opt --config=android_arm //tensorflow/lite/c:selectively_built_c_api_test_lib\r\n```\r\n\r\nand `arm64-v8a` with\r\n\r\n```\r\nbazel build -c opt --config=android_arm64 //tensorflow/lite/c:selectively_built_c_api_test_lib\r\n```\r\n\r\nTested the generated `.so` files with my app in Android Studio and it built successfully. \r\n\r\nBinaries reduction:\r\n\r\n- `arm64-v8a`: 2.5 MB to 1.5 MB\r\n- `armeabi-v7a`: 1.9 MB to 1.7 MB\r\n\r\nIs there any reason why binary for `armeabi-v7a` reduces less than `arm64-v8a`?\r\n\r\nThanks for pointing me to the right direction @abattery.", "I haven't looked at the details. Looks like the pruned op kernels consume more spaces at `arm64-8va`.", "Hey, I still couldn't make it work. \r\n\r\nWhen I build TensorFlow Lite using selective build to generate the library and use the library with JNI, my app in Android Studio builds. However, when I flash it to the mobile it cannot load the library. For selective build I use:\r\n\r\n```\r\nbazel build -c opt --config=android_arm64 //tensorflow/lite/c:selectively_built_c_api_test_lib\r\n```\r\n\r\nIn order to verify whether the issue was related to the selective build or not, I also tried building without selective build. Again, I could build my app in Android Studio using the generated library, however it did not work when I flashed it to the mobile. For default build, I use:\r\n\r\n```\r\nbazel build -c opt --config=android_arm64 //tensorflow/lite/c:tensorflowlite_c\r\n```\r\n\r\nBefore, I used to build the library using `tensorflow/lite/tools/build_aar.sh` and I could use the generated library in my app successfully. Successful build:\r\n\r\n```\r\n./tensorflow/lite/tools/build_aar.sh --target_archs=arm64-v8a\r\n```\r\n\r\nThe issue is that, as stated in [Reduce Binary Size](https://www.tensorflow.org/lite/guide/reduce_binary_size#known_issueslimitations), selective build is not supported for C API and I need to reduce my binary size.\r\n\r\nI have the following questions: what is the difference between using `bazel build -c opt --config=android_arm64 //tensorflow/lite/c:tensorflowlite_c` and `./tensorflow/lite/tools/build_aar.sh --target_archs=arm64-v8a`? Am I missing anything in `tensorflowlite_c` build rule in order to make it behave like `./tensorflow/lite/tools/build_aar.sh`?\r\n\r\nThanks!\r\n\r\n", "Could you share how the compiled shared object is not working on the mobile app? If possible, could you share the error message you've got?", "Hey @abattery, thanks for your reply.\r\n\r\n> Could you share how the compiled shared object is not working on the mobile app? If possible, could you share the error message you've got?\r\n\r\nUnfortunately, I don't have access to the java source code of my mobile app. I am just replacing the compiled shared object of the app and the error it gives me is not very descriptive. It is something like `WARNING: Could not load libMyLib.so`.\r\n\r\nOn the other hand, I think I figured out what was my issue. As I said, I don't have access to the java source code, so I need to use a specific name for the compiled shared object which was already being used by the java part. My issue was that I was compiling the shared object with `tensorflowlite_c` build rule and then renaming the generated shared object. My app couldn't use the compiled shared object because of the renaming, so I replaced `tensorflowlite_c` build rule name by `my_lib_specific_name` and it generated the shared object with the proper name. This time, it worked with my mobile app.\r\n\r\nThen, I tried to do the same for the selective build in order to reduce the library size. Initially, it did not work because I was trying to use `selectively_built_c_api_test_lib` build rule by its own. However, as pointed [here](https://github.com/tensorflow/tensorflow/issues/48068#issuecomment-813926471) the correct way of doing it (I think) is using `tensorflowlite_c` build rule replacing \r\n\r\n```\r\n\":c_api\",\r\n\":c_api_experimental\",\r\n```\r\n\r\nwith\r\n\r\n```\r\n\"selectively_built_c_api_test_lib\",\r\n```\r\n\r\nI did it and it worked. The shared object reduced from 2.5MB to 1.1MB. I used it with my mobile app and it worked!\r\n\r\nHere is the diff with the necessary changes:\r\n![image](https://user-images.githubusercontent.com/23619546/127526444-39b596af-0a9f-4ead-bb70-894039f81f7e.png)\r\n\r\nAgain, thanks for your help @abattery.\r\n", "Great, thanks for sharing your success!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50946\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50946\">No</a>\n"]}, {"number": 50945, "title": "Can't save the LSTM model. (AttributeError: 'CuDNNLSTM' object has no attribute 'unroll')", "body": "Hey,\r\n\r\nI'm trying to build a `LSTM` model with CuDNN and this is the model I'm using.\r\n\r\n```\r\nmodel = Sequential()\r\nmodel.add(CuDNNLSTM(256, input_shape=(train_x.shape[1:]), return_sequences=True))\r\nmodel.add(Dropout(0.2))\r\n\r\nmodel.add(CuDNNLSTM(256, return_sequences=False))\r\nmodel.add(Dropout(0.2))\r\nmodel.add(Dense(1))\r\nmodel.add(Activation(\"tanh\"))\r\n\r\n# Compile the model.\r\nmodel.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\r\nprint(\"Compiled!\")\r\n\r\n# Train the model.\r\nmodel.fit(train_x, train_y, epochs=epoch, batch_size=512, validation_split=0.05)\r\n```\r\n\r\nBut when I try to save the model with  `model.save(...)` I get this error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\save.py\", line 150, in save_model\r\n    saved_model_save.save(model, filepath, overwrite, include_optimizer,\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\save.py\", line 90, in save\r\n    saved_nodes, node_paths = save_lib.save_and_return_nodes(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1103, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def,\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1290, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def,\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1207, in _build_meta_graph_impl\r\n    signatures = signature_serialization.find_function_to_export(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_serialization.py\", line 99, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 154, in list_functions\r\n    obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py\", line 2688, in _list_functions_for_serialization\r\n    functions = super(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 2992, in _list_functions_for_serialization\r\n    return (self._trackable_saved_model_saver\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\base_serialization.py\", line 93, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 73, in functions_to_serialize\r\n    return (self._get_serialized_attributes(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 89, in _get_serialized_attributes\r\n    object_dict, function_dict = self._get_serialized_attributes_internal(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\model_serialization.py\", line 53, in _get_serialized_attributes_internal\r\n    super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 99, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 149, in wrap_layer_functions\r\n    original_fns = _replace_child_layer_functions(layer, serialization_cache)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 279, in _replace_child_layer_functions\r\n    child_layer._trackable_saved_model_saver._get_serialized_attributes(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 89, in _get_serialized_attributes\r\n    object_dict, function_dict = self._get_serialized_attributes_internal(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 151, in _get_serialized_attributes_internal\r\n    super(RNNSavedModelSaver, self)._get_serialized_attributes_internal(\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 99, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 156, in wrap_layer_functions\r\n    call_collection = LayerCallCollection(layer)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 406, in __init__\r\n    self._input_signature = self._generate_input_signature(layer)\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 431, in _generate_input_signature\r\n    layer._use_input_spec_as_call_signature):  # pylint: disable=protected-access\r\n  File \"C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\layers\\recurrent.py\", line 437, in _use_input_spec_as_call_signature\r\n    if self.unroll:\r\nAttributeError: 'CuDNNLSTM' object has no attribute 'unroll'\r\npython-BaseException\r\n```\r\n", "comments": ["@tarik0\r\n\r\nCould you please refer similar issues [comment](https://stackoverflow.com/questions/61659414/attributeerror-module-tensorflow-has-no-attribute-cudnnlstm) and also refer this [documentation](https://www.tensorflow.org/guide/keras/rnn#performance_optimization_and_cudnn_kernels) for more information,let us know if it helps.Thanks\r\n", "@UsharaniPagadala \r\nNo they are not similar. The problem is I can successfully save the normal LSTM model but I can't save the CuDNNLSTM.", "@tarik0 \r\nPlease update the [template](https://github.com/tensorflow/tensorflow/issues/new?assignees=&labels=type%3Abug&template=00-bug-issue.md)\r\nCould you please provide the colab gist with all the dependencies to analyse the issue better.Thanks", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50945\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50945\">No</a>\n"]}, {"number": 50944, "title": "Problems on TensorFlow-ROCm installation", "body": "Hi,\r\n\r\nMy PC has AMD CPU(Ryzen 5900X) and an AMD graphic card(RX6800XT). The operating system is openSUSE Tumbleweed(20210726). I want to use my GPU to do some scientific computation by tensorflow-rocm. \r\n\r\nI add the repository resource via: `sudo zypper addrepo --no-gpgcheck http://repo.radeon.com/rocm/zyp/zypper/ rocm`\r\nBut I don't know which are the necessary packages for the ROCm.\r\nAnd I find that:\r\n\r\nSome packages can be installed directly.\r\n`rock-dkms`\r\n`rocm-gdb`: requires me to install python36 (My OS has python38 installed)\r\n`rocminfo`\r\n`rocm-opencl`\r\n`rocm-opencl-devel`\r\n`rocm-device-libs`\r\n`hsakmt-roct`\r\n`hipify-clang`\r\n...\r\n\r\nSome packages cannot be installed with dependencies problems.\r\n`rocm-dkms` \r\n`rocm-dev`\r\n`rccl`\r\n`rocm-libs`\r\n`hip-samples`\r\n...\r\n\r\nI'm new to computational science, could you tell me the minimal installation of ROCm environment(Could let me use tensorflow-rocm) requires me to install which packages from the [official repo](http://repo.radeon.com/rocm/zyp/zypper/)?\r\nI just want to use [tensorflow-rocm](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream).\r\n\r\nI would appreciate it if you can respond to me.\r\n \r\n\r\n", "comments": ["@Photonico \r\nPlease refer to [this link ](https://medium.com/analytics-vidhya/install-tensorflow-2-for-amd-gpus-87e8d7aeb812)and let us know.", "> @Photonico\r\n> Please refer to [this link ](https://medium.com/analytics-vidhya/install-tensorflow-2-for-amd-gpus-87e8d7aeb812)and let us know.\r\n\r\nThanks a lot for your reply. I have installed ROCm.\r\nHowever, ROCm current version 4.2 does not support my graphic card: RX6800XT(gfx 1030). \r\nI need to wait for the ROCm 4.3 released.\r\n ", "@Photonico \r\nCan we please move this to closed status and open in case you face any issues after release.", "> Can we please move this to closed status and open in case you face any issues after release.\r\n\r\nof course", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50944\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50944\">No</a>\n"]}, {"number": 50943, "title": "Error build: target 'cuda/bin/nvcc' not declared in package", "body": "Above error when building from source with GPU on Ubuntu 18.04.\r\n\r\nTo reproduce:\r\n\r\n```\r\n\u00a0git clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n./configure # all defaults except for CUDA: y\r\nbazel build --copt=-march=native --config=nonccl --config=cuda //tensorflow/tools/pip_package:build_pip_package\r\n```\r\nOS: Ubuntu 18.04\r\nPython 3.9.6: Anaconda managed environment\r\nGPU: NVIDIA 1080 Ti\r\nCUDA: V11.2.152\r\nbazel 3.7.2\r\n\r\nFull stacktrace:\r\n```\r\nbazel build --copt=-march=native --config=nonccl --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures --explain=file.txt --verbose_failures 2>&1 | tee bazel.log\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/sergey/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/sergey/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/sergey/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/sergey/anaconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/sergey/anaconda3/lib/python3.9/site-packages --python_path=/home/sergey/anaconda3/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=/home/sergey/hadoop/lib/native/:/usr/lib/x86_64-linux-gnu/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda\r\nINFO: Found applicable config definition build:short_logs in file /home/sergey/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/sergey/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /home/sergey/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:nonccl in file /home/sergey/tensorflow/.bazelrc: --define=no_nccl_support=true\r\nINFO: Found applicable config definition build:cuda in file /home/sergey/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:linux in file /home/sergey/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/sergey/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /home/sergey/.cache/bazel/_bazel_sergey/3b767bf3a32053879a4e137b3de41895/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nDEBUG: /home/sergey/.cache/bazel/_bazel_sergey/3b767bf3a32053879a4e137b3de41895/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:118:10:\r\nAuto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\\Windows\\Temp' as default\r\nLoading:  (1 packages loaded)\r\nLoading: 1 packages loaded\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (2 packages loaded, 0 targets configured)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (57 packages loaded, 17 targets configured)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (227 packages loaded, 3960 targets configured)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (246 packages loaded, 3960 targets configured)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (248 packages loaded, 3960 targets configured)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (249 packages loaded, 3960 targets configured)\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /home/sergey/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /home/sergey/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /home/sergey/.cache/bazel/_bazel_sergey/3b767bf3a32053879a4e137b3de41895/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/sergey/.cache/bazel/_bazel_sergey/3b767bf3a32053879a4e137b3de41895/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (332 packages loaded, 5329 targets configured)\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (427 packages loaded, 13775 targets configured)\r\nERROR: /home/sergey/.cache/bazel/_bazel_sergey/3b767bf3a32053879a4e137b3de41895/external/rules_cuda/cuda/BUILD:128:20: every rule of type cuda_toolchain_info implicitly depends upon the target '@local_cuda//:cuda/bin/nvcc', but this target could not be found because of: no such target '@local_cuda//:cuda/bin/nvcc': target 'cuda/bin/nvcc' not declared in package '' (did you mean 'cuda/bin/h5cc'?) defined by /home/sergey/.cache/bazel/_bazel_sergey/3b767bf3a32053879a4e137b3de41895/external/local_cuda/BUILD\r\nINFO: Repository eigen_archive instantiated at:\r\n  /home/sergey/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:1091:28: in workspace\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:61:11: in _initialize_third_party\r\n  /home/sergey/tensorflow/third_party/eigen3/workspace.bzl:12:20: in repo\r\n  /home/sergey/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /home/sergey/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nINFO: Repository org_sqlite instantiated at:\r\n  /home/sergey/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:1098:21: in workspace\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:347:20: in _tf_repositories\r\n  /home/sergey/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /home/sergey/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nINFO: Repository llvm-project instantiated at:\r\n  /home/sergey/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:1098:21: in workspace\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:657:9: in _tf_repositories\r\n  /home/sergey/tensorflow/third_party/llvm/workspace.bzl:10:20: in repo\r\n  /home/sergey/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /home/sergey/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nINFO: Repository cudnn_frontend_archive instantiated at:\r\n  /home/sergey/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:1098:21: in workspace\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:158:20: in _tf_repositories\r\n  /home/sergey/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /home/sergey/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nAnalyzing: target //tensorflow/tools/pip_package:build_pip_package (427 packages loaded, 13776 targets configured)\r\nINFO: Repository mkl_dnn_v1 instantiated at:\r\n  /home/sergey/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:1098:21: in workspace\r\n  /home/sergey/tensorflow/tensorflow/workspace2.bzl:181:20: in _tf_repositories\r\n  /home/sergey/tensorflow/third_party/repo.bzl:113:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /home/sergey/tensorflow/third_party/repo.bzl:66:35: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 13.023s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (427 packages loaded, 13776 targets configured)\r\nFAILED: Build did NOT complete successfully (427 packages loaded, 13776 targets configured)\r\n```\r\nTensorflow 2.5.0 builds though on the same machine.", "comments": ["@sbushmanov \r\nCould you please try and let us know:\r\n``` $ bazel clean```\r\n```$ ./configure # make sure to choose gpu support```\r\n```$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer```\r\n\r\nSimilar issues you may refer to:[link1](https://github.com/tensorflow/tensorflow/issues/4841), [link](https://forums.developer.nvidia.com/t/cuda-8-0-toolkit-install-nvcc-not-found-ubuntu-16-04/47997)\r\n", "@Saduf2019\r\n\r\n```\r\n$ bazel clean\r\n\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=123\r\nINFO: Reading rc options for 'clean' from /home/sergey/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'clean' from /home/sergey/tensorflow/.bazelrc:\r\n  Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'clean' from /home/sergey/tensorflow/.tf_configure.bazelrc:\r\n  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/sergey/anaconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/sergey/anaconda3/lib/python3.9/site-packages --python_path=/home/sergey/anaconda3/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=/home/sergey/hadoop/lib/native/:/usr/lib/x86_64-linux-gnu/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda\r\nINFO: Found applicable config definition build:short_logs in file /home/sergey/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/sergey/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /home/sergey/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:linux in file /home/sergey/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/sergey/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: Blaze clean does not support starlark options. Ignoring options: [--@local_config_cuda//:enable_cuda]\r\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\r\n```\r\n\r\n```\r\n$ ./configure\r\n\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /home/sergey/anaconda3/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /home/sergey/anaconda3/lib/python3.9/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/sergey/anaconda3/lib/python3.9/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]:\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.2 in:\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/lib/x86_64-linux-gnu\r\n    /usr/include\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]:\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]:\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]:\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]:\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v1          \t# Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n```\r\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\r\n\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=123\r\nINFO: Reading rc options for 'build' from /home/sergey/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/sergey/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from /home/sergey/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/sergey/anaconda3/bin/python3 --action_env PYTHON_LIB_PATH=/home/sergey/anaconda3/lib/python3.9/site-packages --python_path=/home/sergey/anaconda3/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=/home/sergey/hadoop/lib/native/:/usr/lib/x86_64-linux-gnu/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 --config=cuda\r\nINFO: Found applicable config definition build:short_logs in file /home/sergey/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/sergey/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file /home/sergey/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda in file /home/sergey/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:linux in file /home/sergey/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/sergey/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: /home/sergey/.cache/bazel/_bazel_sergey/3b767bf3a32053879a4e137b3de41895/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nERROR: Skipping '//tensorflow/cc:tutorials_example_trainer': no such target '//tensorflow/cc:tutorials_example_trainer': target 'tutorials_example_trainer' not declared in package 'tensorflow/cc' defined by /home/sergey/tensorflow/tensorflow/cc/BUILD\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such target '//tensorflow/cc:tutorials_example_trainer': target 'tutorials_example_trainer' not declared in package 'tensorflow/cc' defined by /home/sergey/tensorflow/tensorflow/cc/BUILD\r\nINFO: Elapsed time: 3.347s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded)\r\n```\r\n\r\n", "Anything from @Saduf2019", "Anything from @ymodak ?", "Anything from @mihaimaruseac ?", "Anything from @sanjoy ?", "@sbushmanov I suggest using the `tensorflow/tensorflow:devel-gpu` docker image (details [here](https://www.tensorflow.org/install/docker#download_a_tensorflow_docker_image)) which should have the correct deps installed.", "@sanjoy \r\n\r\nThere are many ways to circumvent this problem including pip or conda install tensorflow-gpu.\r\nAny thoughts how to resolve the error while building from source?", "> Any thoughts how to resolve the error while building from source?\r\n\r\nI was suggesting that you build TensorFlow in the `tensorflow/tensorflow:devel-gpu` docker image.", "Were you able to fix it @sbushmanov ? I am also running into the same issue. I am using RTX 3080 with Cuda version 11.4 and cudnn 8.2.4", "@sbushmanov, Did you tried as suggested [in this comment](https://github.com/tensorflow/tensorflow/issues/50943#issuecomment-939570221)?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Were you able to fix it @sbushmanov ? I am also running into the same issue. I am using RTX 3080 with Cuda version 11.4 and cudnn 8.2.4\r\n\r\n@thesigmaguy \r\nYes, I did. The issue was due to Anaconda interfering with native cudatoolkit. After disabling Anaconda (and switching it back after successful build) the build proceeded successfully.\r\nAs another solution, you may try their docker, try to reproduce your environment within docker, and see where it fails.", "@sbushmanov, Please feel free to close the issue if it is resolved? Thanks! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50943\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50943\">No</a>\n"]}, {"number": 50942, "title": "TFLite Conversion Error: Element Shape Required to be 1D", "body": "### 1. System information\r\n\r\n- Windows 10 PC, TensorFlow version 2.5.0\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport model as modellib\r\nimport coco\r\nimport os \r\nimport sys\r\n\r\n# Enable eager execution\r\ntf.compat.v1.enable_eager_execution()\r\n\r\nclass InferenceConfig(coco.CocoConfig):\r\n    GPU_COUNT = 1\r\n    IMAGES_PER_GPU = 1\r\nconfig = InferenceConfig()\r\nmodel = modellib.MaskRCNN(mode=\"inference\", model_dir='logs', config=config)\r\nmodel.load_weights('mask_rcnn_coco.h5', by_name=True)\r\nmodel = model.keras_model\r\n\r\ntf.saved_model.save(model, \"tflite\")\r\n\r\n# Preparing before conversion - making the representative dataset\r\nROOT_DIR = os.path.abspath(\"../\")\r\nCARS = os.path.join(ROOT_DIR, 'Mask_RCNN\\\\mrcnn\\\\smallCar')\r\n\r\nIMAGE_SIZE = 224\r\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\r\n\r\ndef representative_data_gen():\r\n    dataset_list = tf.data.Dataset.list_files(CARS)\r\n    for i in range(100):\r\n        image = next(iter(dataset_list))\r\n        image = tf.io.read_file(image)\r\n        image = tf.io.decode_jpeg(image, channels=3)\r\n        image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\r\n        image = tf.cast(image / 255., tf.float32)\r\n        image = tf.expand_dims(image, 0)\r\n        yield [image]\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n# This enables quantization\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n# This sets the representative dataset for quantization\r\nconverter.representative_dataset = representative_data_gen\r\n# This ensures that if any ops can't be quantized, the converter throws an error\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\r\nconverter.target_spec.supported_types = [tf.int8]\r\n# These set the input and output tensors to uint8 (added in r2.3)\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\n\r\nwith open('modelQuantized.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\n### 3. Failure after conversion\r\nI get the following error for the `tflite_model = converter.convert()` line:\r\n\r\n`error: 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass`,\r\n\r\nFULL:\r\n\r\n```\r\nloc(callsite(\"mask_rcnn/mrcnn_detection/map/TensorArrayV2_1\"(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\deprecation.py\":535:0) at callsite(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\deprecation.py\":602:0 at callsite(\"d:\\\\Mask_RCNN\\\\mrcnn\\\\model.py\":774:0 at callsite(\"d:\\\\Mask_RCNN\\\\mrcnn\\\\model.py\":839:0 at callsite(\"d:\\\\Mask_RCNN\\\\mrcnn\\\\utils.py\":820:0 at callsite(\"d:\\\\Mask_RCNN\\\\mrcnn\\\\model.py\":837:0 at callsite(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\autograph\\\\impl\\\\api.py\":645:0 at callsite(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\keras\\\\engine\\\\base_layer.py\":1030:0 at callsite(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\keras\\\\engine\\\\functional.py\":556:0 at \"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\keras\\\\engine\\\\functional.py\":420:0)))))))))): error: 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass\r\nloc(callsite(\"mask_rcnn/mrcnn_detection/map/TensorArrayV2_1\"(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\deprecation.py\":535:0) at callsite(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\deprecation.py\":602:0 at callsite(\"d:\\\\Mask_RCNN\\\\mrcnn\\\\model.py\":774:0 at callsite(\"d:\\\\Mask_RCNN\\\\mrcnn\\\\model.py\":839:0 at callsite(\"d:\\\\Mask_RCNN\\\\mrcnn\\\\utils.py\":820:0 at callsite(\"d:\\\\Mask_RCNN\\\\mrcnn\\\\model.py\":837:0 at callsite(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\autograph\\\\impl\\\\api.py\":645:0 at callsite(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\keras\\\\engine\\\\base_layer.py\":1030:0 at callsite(\"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\keras\\\\engine\\\\functional.py\":556:0 at \"C:\\\\Python39\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\keras\\\\engine\\\\functional.py\":420:0)))))))))): error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\nTraceback (most recent call last):\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 291, in toco_convert_protos\r\n    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\wrap_toco.py\", line 32, in wrapped_toco_convert\r\n    return _pywrap_toco_api.TocoConvert(\r\nException: C:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:535:0: error: 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:602:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:774:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:839:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\utils.py:820:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:837:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:645:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:535:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:602:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:774:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:839:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\utils.py:820:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:837:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:645:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420:0: note: called from\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\Mask_RCNN\\mrcnn\\convertQuantize.py\", line 53, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1057, in convert\r\n    result = super(TFLiteKerasModelConverterV2,\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 782, in convert\r\n    result = _toco_convert_impl(\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 698, in toco_convert_impl\r\n    data = toco_convert_protos(\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\", line 297, in toco_convert_protos\r\n    raise ConverterError(str(e))\r\ntensorflow.lite.python.convert.ConverterError: C:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:535:0: error: 'tf.TensorListReserve' op requires element_shape to be 1D tensor during TF Lite transformation pass\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:602:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:774:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:839:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\utils.py:820:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:837:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:645:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:535:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:602:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:774:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:839:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\utils.py:820:0: note: called from\r\nd:\\Mask_RCNN\\mrcnn\\model.py:837:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:645:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556:0: note: called from\r\nC:\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420:0: note: called from\r\n```\r\n\r\nNot sure what the error is, or how to fix it. Any help is appreciated!\r\n", "comments": ["converter.target_spec.supported_ops = [\r\n          tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n      ]\r\nconverter._experimental_lower_tensor_list_ops = True", "Please use the Select TF ops and disable tensor list op lowering.", "Hey @abattery \r\n\r\n> \r\n> \r\n> Please use the Select TF ops and disable tensor list op lowering.\r\n\r\nThank you for the response!\r\nI tried the above code you attached, and got this error instead:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 69, in __init__\r\n    _calibration_wrapper.CalibrationWrapper(\r\nTypeError: pybind11::init(): factory function returned nullptr\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\Mask_RCNN\\mrcnn\\convertQuantize.py\", line 58, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1057, in convert\r\n    result = super(TFLiteKerasModelConverterV2,\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 795, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 517, in _calibrate_quantize_model\r\n    calibrate_quantize = _calibrator.Calibrator(result,\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 73, in __init__\r\n    raise ValueError(\"Failed to parse the model: %s.\" % e)\r\nValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.\r\n```\r\nNot sure what this means :(\r\n\r\nThe code added is as follows:\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n# This enables quantization\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n# This sets the representative dataset for quantization\r\nconverter.representative_dataset = representative_data_gen\r\n# This ensures that if any ops can't be quantized, the converter throws an error\r\n\r\nconverter.target_spec.supported_ops = [\r\ntf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nconverter._experimental_lower_tensor_list_ops = False\r\n\r\n# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\r\nconverter.target_spec.supported_types = [tf.int8]\r\n# These set the input and output tensors to uint8 (added in r2.3)\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\ntflite_model = converter.convert()\r\n\r\nwith open('modelQuantized.tflite', 'wb') as f:\r\n  f.write(tflite_model)\r\n```\r\n\r\nI got the same error using both `True` and `False` for `converter._experimental_lower_tensor_list_ops`, since I wasn't sure which you meant! :)", "Please try out tf-nightly version.", "Hey @abattery \r\n\r\nSorry I'm not sure I follow, this is my first model-project!! Can you elaborate? Do I just uninstall tensorflow and install tf-nightly instead? What do I do?\r\n\r\nThank you", "Yes, there is a chance to fix your conversion problem with the very recent version.", "Trying it now, will tell you how it goes! :)", "Welp, I got a new issue this time :/\r\n\r\n```\r\n2021-07-26 01:34:28.913146: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-26 01:34:29.401304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1504] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5555 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\r\nWARNING:tensorflow:From C:\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse fn_output_signature instead\r\n2021-07-26 01:34:55.793182: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n2021-07-26 01:38:48.185690: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:351] Ignored output_format.\r\n2021-07-26 01:38:48.189239: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:354] Ignored drop_control_dependency.\r\n2021-07-26 01:38:48.193918: I tensorflow/cc/saved_model/reader.cc:98] Reading SavedModel from: C:\\Users\\HANAPC~1\\AppData\\Local\\Temp\\tmpvn6f94hw\r\n2021-07-26 01:38:48.305417: I tensorflow/cc/saved_model/reader.cc:98] Reading SavedModel from: C:\\Users\\HANAPC~1\\AppData\\Local\\Temp\\tmpvn6f94hw\r\n2021-07-26 01:38:48.413851: I tensorflow/cc/saved_model/reader.cc:69] Reading meta graph with tags { serve }\r\n2021-07-26 01:38:48.418463: I tensorflow/cc/saved_model/reader.cc:131] Reading SavedModel debug info (if present) from: C:\\Users\\HANAPC~1\\AppData\\Local\\Temp\\tmpvn6f94hw\r\n2021-07-26 01:38:48.976753: I tensorflow/cc/saved_model/loader.cc:212] Restoring SavedModel bundle.\r\n2021-07-26 01:38:50.403641: I tensorflow/cc/saved_model/loader.cc:196] Running initialization op on SavedModel bundle at path: C:\\Users\\HANAPC~1\\AppData\\Local\\Temp\\tmpvn6f94hw\r\n2021-07-26 01:38:50.741048: I tensorflow/cc/saved_model/loader.cc:289] SavedModel load for tags { serve }; Status: success: OK. Took 2435630 microseconds.\r\n2021-07-26 01:38:52.040270: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2021-07-26 01:38:55.378590: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1861] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following flex op(s):\r\nFlex ops: FlexCropAndResize, FlexDenseToDenseSetOperation, FlexTensorListFromTensor, FlexTensorListGetItem, FlexTensorListReserve, FlexTensorListSetItem, FlexTensorListStack\r\nDetails:\r\n        tf.CropAndResize(tensor<?x?x?x256xf32>, tensor<?x4xf32>, tensor<?xi32>, tensor<2xi32>) -> (tensor<?x14x14x256xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}\r\n        tf.CropAndResize(tensor<?x?x?x256xf32>, tensor<?x?xf32>, tensor<?xi32>, tensor<2xi32>) -> (tensor<?x7x7x256xf32>) : {T = f32, device = \"\", extrapolation_value = 0.000000e+00 : f32, method = \"bilinear\"}  \r\n        tf.DenseToDenseSetOperation(tensor<1x?xi64>, tensor<1x?xi64>) -> (tensor<?x2xi64>, tensor<?xi64>, tensor<2xi64>) : {T = i64, device = \"\", set_operation = \"intersection\", validate_indices = true}\r\n        tf.TensorListFromTensor(tensor<?xi32>, tensor<0xi32>) -> (tensor<!tf.variant<tensor<i32>>>) : {device = \"\"}\r\n        tf.TensorListGetItem(tensor<!tf.variant<tensor<i32>>>, tensor<i32>, tensor<0xi32>) -> (tensor<i32>) : {device = \"\"}\r\n        tf.TensorListReserve(tensor<i32>, tensor<i32>) -> (tensor<!tf.variant<tensor<*xi64>>>) : {device = \"\"}\r\n        tf.TensorListSetItem(tensor<!tf.variant<tensor<*xi64>>>, tensor<i32>, tensor<?xi64>) -> (tensor<!tf.variant<tensor<*xi64>>>) : {device = \"\"}\r\n        tf.TensorListStack(tensor<!tf.variant<tensor<*xi64>>>, tensor<1xi32>) -> (tensor<?x100xi64>) : {device = \"\", num_elements = -1 : i64}\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\n2021-07-26 01:38:56.607760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1504] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5555 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\r\nINFO: TfLiteFlexDelegate delegate: 13 nodes delegated out of 555 nodes with 6 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 18 nodes with 2 partitions.\r\n\r\nTraceback (most recent call last):\r\n  File \"d:\\Mask_RCNN\\mrcnn\\convertQuantize.py\", line 58, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 731, in wrapper\r\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 717, in _convert_and_export_metrics\r\n    result = convert_func(self, *args, **kwargs)\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1124, in convert\r\n    saved_model_convert_result = self._convert_as_saved_model()\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 1106, in _convert_as_saved_model\r\n    return super(TFLiteKerasModelConverterV2,\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 906, in convert\r\n    return self._optimize_tflite_model(\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py\", line 222, in wrapper\r\n    raise error from None  # Re-throws the exception.\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py\", line 212, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 691, in _optimize_tflite_model\r\n    model = self._quantize(\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\", line 513, in _quantize\r\n    calibrated = calibrate_quantize.calibrate(\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py\", line 222, in wrapper\r\n    raise error from None  # Re-throws the exception.\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py\", line 212, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Python39\\lib\\site-packages\\tensorflow\\lite\\python\\optimize\\calibrator.py\", line 184, in calibrate\r\n    self._calibrator.Prepare([list(s.shape) for s in sample])\r\nValueError: Invalid input shapes: expected 3 items got 1 items.\r\n```\r\n\r\nI really appreciate the help. Thank you for taking the time, sorry I'm so new haha!", "According to the message, the calibration function should return three input values but it provided only one input value. Please provide the input values based on the TensorFlow graph input/output speicifications.", "Yes, but I\u2019m not sure how to fix that. Is there something wrong with the representative data gen or?", "Yes, the representative data gen is related.", "Ah, do you know how I would need to change the function? :)", "It is related with the model structure you are using. Please check out that model input/output speicifications and make sure the calibration generator follows that.", "Hey Hana, \r\n\r\nIn your representative_dataset function, you can add more data fields for model input as below\r\nyield [image, data_field1, data_field2] \r\n\r\nI guess for mask_rcnn model, you might need to provide input shape and anchor box data. ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50941, "title": "Where is convert_all_kernels_in_model in tensorflow 2.5.0?", "body": "I can't import `convert_all_kernels_in_model` from `tensorflow.keras.utils` in `tf 2.5.0`\r\n\r\n```python\r\nfrom tensorflow.keras.utils import convert_all_kernels_in_model\r\n```\r\n\r\nI get this error:\r\n\r\n```bash\r\nImportError: cannot import name 'convert_all_kernels_in_model' from 'tensorflow.keras.utils' ...\r\n```\r\n\r\nI have an old code written in `tf 2.3.0` and I need to port it to `tf 2.5.0`.  \r\n\r\nWhere did that function go? ", "comments": ["@chudur-budur ,\r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50941\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50941\">No</a>\n"]}, {"number": 50940, "title": "Build TensorFlowLite C API with GPU delegate with CMake", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: nightly\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: No\r\n- Bazel version (if compiling from source): N/A, using CMake 3.20\r\n- GCC/Compiler version (if compiling from source): GCC 8.2\r\n- CUDA/cuDNN version: CUDA 11.1, cuDNN 8\r\n- GPU model and memory: GTX 1650\r\n\r\n\r\n**Describe the problem**\r\nI am trying to build the C API for Tensorflow Lite, with GPU delegate enabled, and using CMake. The resulting `libtensorflowlite_c.so` does not have GPU delegate symbols.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI basically followed https://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library . With the only difference that I added `-DTFLITE_ENABLE_GPU=ON` in the CMake config step. From what I gathered from `tensorflow/lite/c/CMakeLists.txt`, we build `libtensorflow-lite.a` (C++ API, static) first, and bake this into `libtensorflowlite_c.so`. The GPU delegate symbols can be found inside `libtensorflow-lite.a`, but are stripped away from `libtensorflowlite_c.so`. I found a workaround which consists of changing:\r\n```\r\ntarget_link_libraries(tensorflowlite_c\r\n  tensorflow-lite\r\n)\r\n```\r\nto\r\n```\r\ntarget_link_libraries(tensorflowlite_c\r\n  -Wl,--whole-archive\r\n  tensorflow-lite\r\n  -Wl,--no-whole-archive\r\n)\r\n```\r\ninside `tensorflow/lite/c/CMakeLists.txt` and removing `${TFLITE_C_SRCS}` from the list of source files associated to the `tensorflow-lite` target inside `tensorflow/lite/CMakeLists.txt`. This latter step is necessary because both `tensorflowlite_c` and `tensorflow-lite` targets include this same set of source files and will lead to a linker multiple-definition error.\r\n\r\nIs there better way to achieve this without having to patch the CMake files on my end? Can we add better support for delegates for the CMake workflow?\r\n\r\n\r\n", "comments": ["That change makes sense to me. Could you provide a PR?\r\n\r\nBTW, did you verify the GPU delegate works well with C API?", "Hi @terryheo , sorry for taking so long. I got sidetracked onto another project and was only able to return to this recently. So basically I can confirm that with GPU delegate does work with C API on NVidia GPUs if I build using CMake with the aforementioned changes. Also, I was able to build for Android after making one additional modification to `tensorflow/lite/CMakeLists.txt` in order to exclude `minimal_logging_default.cc` from the list of source files to build. Otherwise, on Android, it will attempt to compile both `minimal_logging_default.cc` and `minimal_logging_android.cc` and link them to produce a multiple definition error.\r\n\r\nBefore I make a PR for these fixes, I noticed a crash on exit bug in the latest master and nightly branches:\r\nhttps://github.com/tensorflow/tensorflow/issues/51751\r\n\r\nI was only able to get everything successfully building and running on the v2.6.0 tag.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50940\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50940\">No</a>\n", "[This problem](https://github.com/tensorflow/tensorflow/issues/50940#issuecomment-908498923) is still unfixed:\r\n\r\n> Also, I was able to build for Android after making one additional modification to tensorflow/lite/CMakeLists.txt in order to exclude minimal_logging_default.cc from the list of source files to build. Otherwise, on Android, it will attempt to compile both minimal_logging_default.cc and minimal_logging_android.cc and link them to produce a multiple definition error.\r\n>\r\n> Before I make a PR for these fixes, I noticed a crash on exit bug in the latest master and nightly branches:\r\nhttps://github.com/tensorflow/tensorflow/issues/51751"]}, {"number": 50938, "title": "Performance issue on Macbook Pro M1", "body": "**System information**\r\n- Script can be found below\r\n- MacBook Pro M1 (Mac OS Big Sir (11.5))\r\n- TensorFlow installed from (source)\r\n- TensorFlow version (2.5 version) with Metal Support\r\n- Python version: 3.9\r\n- GPU model and memory: MacBook Pro M1 and 16 GB\r\n\r\nSteps needed for installing Tensorflow with metal support.\r\nhttps://developer.apple.com/metal/tensorflow-plugin/\r\n\r\nI am trying to train a model on Macbook Pro M1, but the performance is so bad and the train doesn't work properly. It takes a ridiculously long time just for a single epoch. \r\n\r\nCode needed for reproducing this behavior.\r\n`\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM\r\nfrom tensorflow.keras.losses import BinaryCrossentropy\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\n\r\n# Model configuration\r\nadditional_metrics = ['accuracy']\r\nbatch_size = 128\r\nembedding_output_dims = 15\r\nloss_function = BinaryCrossentropy()\r\nmax_sequence_length = 300\r\nnum_distinct_words = 5000\r\nnumber_of_epochs = 5\r\noptimizer = Adam()\r\nvalidation_split = 0.20\r\nverbosity_mode = 1\r\n\r\n# Disable eager execution\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# Load dataset\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_distinct_words)\r\nprint(x_train.shape)\r\nprint(x_test.shape)\r\n\r\n# Pad all sequences\r\npadded_inputs = pad_sequences(x_train, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\r\npadded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length, value = 0.0) # 0.0 because it corresponds with <PAD>\r\n\r\n# Define the Keras model\r\nmodel = Sequential()\r\nmodel.add(Embedding(num_distinct_words, embedding_output_dims, input_length=max_sequence_length))\r\nmodel.add(LSTM(10))\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\n# Compile the model\r\nmodel.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\r\n\r\n# Give a summary\r\nmodel.summary()\r\n\r\n# Train the model\r\nhistory = model.fit(padded_inputs, y_train, batch_size=batch_size, epochs=number_of_epochs, verbose=verbosity_mode, validation_split=validation_split)\r\n\r\n# Test the model after training\r\ntest_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\r\nprint(f'Test results - Loss: {test_results[0]} - Accuracy: {100*test_results[1]}%')\r\n\r\n`\r\n\r\n", "comments": ["I have noticed this same problem with LSTM layers", "@OriAlpha ,\r\n\r\nPlease post this issue on [keras-team/keras](https://github.com/keras-team/keras/issues) repo.\r\nTo know more refer to:\r\n[https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999](https://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999)\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 50937, "title": "validation_split for tensorflow.dataset", "body": "ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>]\r\n\r\nI believe there is explicitly no support to divide a tensorflow dataset  into train and validation on the go while calling model.fit. Would be really helpful if this feature can be added.\r\n**System information**\r\n- TensorFlow version (you are using): 2.5\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n", "comments": ["@old-school-kid ,\r\n\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "@tilakrayal \r\nThe error I am speaking about can be found in this [notebook](https://colab.research.google.com/drive/1BySKPLDhoBGN5Sby-_klwuESr61Qu4dF?usp=sharing)\r\n\r\nSo what I wanted was to split the dataset on the go while training.", "@old-school-kid,\r\nValidation Split can be obtained using the code shown below:\r\n\r\n**`test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])`**\r\n\r\nPlease refer in [this comment](https://github.com/tensorflow/datasets/issues/291#issuecomment-474668134) and the [respective Tensorflow Documentation](https://www.tensorflow.org/datasets/splits) for more information.", "@rmothukuru \r\nI believe you meant `ds1, ds2 = tfds.load(..., split=[ 'train[:50%]', 'train[50%:]'])` as mentioned [in this issue](https://github.com/tensorflow/datasets/issues/1998#issuecomment-625293235). \r\nAlso I guess this support is only present for datasets present in tensorflow_datsets. If there is a way around for custom dataset please let me know how to split since the arguments in both the Slicing APIs [as mentioned in the document](https://www.tensorflow.org/datasets/splits) require the dataset to be present in tensorflow_dataset.", "@rmothukuru `mode.fit(..., validation_split=...)` is a Keras API, so this would be better routed to the Keras team.", "@aaudiber @rmothukuru \r\nSo do I need to close this issue here and head over to the keras repository. But also isn't the issue with tensorflow.data.dataset?\r\n", "@old-school-kid \r\nIt makes sense for this issue to stay here -- `tf.keras` is a part of this repo: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/keras\r\n\r\n`tf.data.Dataset`s aren't randomly accessible, so there isn't an efficient way to split a dataset into train/eval datasets. Instead, users need to decide what data should be part of the train dataset and which should be part of the eval dataset.", "This Issue seems interesting to me. We cannot randomly access the dataset. However, we can randomly choose to not train a particular record and use that as validation. Would that be beneficial? Storing those Indicies throughout epochs makes sure that training set and validation set remain saperated.", "@old-school-kid The [TF doc](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#fit) clearly mentions that it is not supported when x is a dataset\r\n\r\n> This argument is not supported when x is a dataset, generator or keras.utils.Sequence instance. validation_split is not yet supported with tf.distribute.experimental.ParameterServerStrategy.\r\n\r\nIf you still need this feature, open the issue as a feature in keras-team/keras repo as Keras development moved there to focus entirely on Keras.\r\n\r\nKeras development moved to another repository to focus on only keras. Could you please repost this issue on [keras-team/keras repo.](https://github.com/keras-team/keras/issues)", "@jvishnuvardhan I suppose for the time being I have to go with @aaudiber and decide train and test split beforehand. So I guess I will close the issue.\r\nThank you."]}, {"number": 50936, "title": "add colon", "body": "add colon", "comments": []}, {"number": 50933, "title": "'tf.MLCConv2D' op is neither a custom op nor a flex op", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Mac OS 11.4 (Hardware: Mac Mini M1)\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.0rc0\r\n- Python version: 3.8.10\r\n\r\n### 2. Description of problem\r\n\r\nI am using TFLite with the code from this repo (https://github.com/hunglc007/tensorflow-yolov4-tflite) to save on inference time. When I try to quantize weights into float 16, this is the error that appears:\r\n\r\n`\r\n 'tf.MLCConv2D' op is neither a custom op nor a flex op\r\n`\r\n\r\n### 3. Code\r\n\r\nThis error was surfaced through the following actions:\r\nDownload this repo: https://github.com/hunglc007/tensorflow-yolov4-tflite\r\n\r\nRun the following commands inside that repo:\r\n```\r\npython save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4 \r\npython save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4 --framework tflite\r\npython convert_tflite.py --weights ./checkpoints/yolov4-416 --output ./checkpoints/yolov4-416.tflite\r\n```\r\nOn the third command, the error will show up. \r\n\r\n### 5. (optional) Any other info / logs\r\nHere is the info that was emitted along with the error:\r\n```\r\n@Amols-Mac-mini-2 tensorflow-yolov4-tflite-master % python convert_tflite.py --weights ./checkpoints/yolov4-416 --output ./checkpoints/yolov4-416.tflite\r\n2021-06-30 06:06:32.420247: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\r\n2021-06-30 06:06:32.420283: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\r\n2021-06-30 06:06:32.420288: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:325] Ignored change_concat_input_ranges.\r\n2021-06-30 06:06:32.420843: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: ./checkpoints/yolov4-416\r\n2021-06-30 06:06:32.469275: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\r\n2021-06-30 06:06:32.469304: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: ./checkpoints/yolov4-416\r\n2021-06-30 06:06:32.614124: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\r\n2021-06-30 06:06:32.661105: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\r\n2021-06-30 06:06:32.694735: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\r\n2021-06-30 06:06:33.529179: I tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: ./checkpoints/yolov4-416\r\n2021-06-30 06:06:33.717920: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 1297076 microseconds.\r\n2021-06-30 06:06:34.386654: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:194] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nloc(callsite(callsite(\"model/conv2d/Conv2D@__inference__wrapped_model_11175\" at \"StatefulPartitionedCall@__inference_signature_wrapper_47226\") at \"StatefulPartitionedCall\")): error: 'tf.MLCConv2D' op is neither a custom op nor a flex op\r\n[~1000 similar lines to above omitted for brevity]\r\n<unknown>:0: error: loc(callsite(callsite(\"model/conv2d_109/Conv2D@__inference__wrapped_model_11175\" at \"StatefulPartitionedCall@__inference_signature_wrapper_47226\") at \"StatefulPartitionedCall\")): 'tf.MLCConv2D' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: loc(callsite(callsite(\"model/conv2d_109/Conv2D@__inference__wrapped_model_11175\" at \"StatefulPartitionedCall@__inference_signature_wrapper_47226\") at \"StatefulPartitionedCall\")): 'tf.MLCConv2D' op is neither a custom op nor a flex op\r\n<unknown>:0: note: loc(\"StatefulPartitionedCall\"): called from\r\n<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n        tf.MLCConv2D {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], num_args = 0 : i64, padding = \"SAME\", strides = [1, 1, 1, 1], transpose = false, use_cudnn_on_gpu = true}\r\n        tf.MLCConv2D {T = f32, data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], num_args = 0 : i64, padding = \"VALID\", strides = [1, 2, 2, 1], transpose = false, use_cudnn_on_gpu = true}\r\n```", "comments": ["MLC conv2d op is not available on mobile platforms. Please consider using general conv2d ops in the original TF graph.", "Is Mac M1 considered a mobile device?", "https://github.com/apple/tensorflow_macos/issues/133#issuecomment-774826217\r\n\r\nPlease check out the above comment.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50933\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50933\">No</a>\n", "We would recommend using the Google version of TensorFlow for converting your model to the corresponding TFLite models. Those MLC ops are not official supported ops by the TensorFlow Core project.", "@abattery What repo should I look at to get information / file an issue with MLC ops?"]}, {"number": 50932, "title": "[oneDNN] Improving Graph Rewrite Performance", "body": "This PR improves the latency of graph rewrite by avoiding doing kernel registry lookup more than once per kernel. Similar fix was to done before to eager mode, so the refactoring is done to manage and maintain that kernel registry info in one place (mkl_graph_util.h)", "comments": ["@penpornk  Can you please review this PR ? Thanks!\r\n", "Thank you for reviewing the PR. I have addressed your comments."]}, {"number": 50931, "title": "[INTEL MKL] Update mkl auto_mixed_precision_lists", "body": "Updating the MKL auto_mixed_precision_lists to allow more ops in bfloat16. This significantly reduces the number of Cast ops in models running bf16 inference with auto_mixed_precision and improves broad model performance.", "comments": []}, {"number": 50930, "title": "Fix numpy 1.20 deprecation warnings (round two)", "body": "* Make tf.float64 point to np.float64 (rather than np.double, which\r\n  is almost always np.float64, but not always).\r\n* Replace np.object with np.object_.\r\n* Replace np.string_ with np.bytes_ (which are identical).\r\n* Replace np.str and np.unicode_ with np.str_ (which are identical).", "comments": ["Sorry for the second pull request.  I actually did searches twice during the time the last pull request was waiting for review.  I guess a few problems snuck in at the very end."]}, {"number": 50929, "title": "use `std::make_tuple` instead of non-empty braced-init-lists", "body": "Non-empty braced-init-lists cause some errors when I build from source. I don't know the exact reason. It may be related to my build tools, but the safest way is not to use non-empty braced-init-lists.\r\nThe error is:\r\n```\r\nexternal/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc: In function \u2018void stream_executor::LogPtxasTooOld(const string&, int, int)\u2019:\r\nexternal/org_tensorflow/tensorflow/stream_executor/gpu/asm_compiler.cc:190:62: error: converting to \u2018absl::lts_2020_02_25::container_internal::raw_hash_set<absl::lts_2020_02_25::container_internal::FlatHashSetPolicy<std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, int> >, absl::lts_2020_02_25::hash_internal::Hash<std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, int> >, std::equal_to<std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, int> >, std::allocator<std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, int> > >::init_type {aka std::tuple<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, int>}\u2019 from initializer list would use explicit constructor \u2018constexpr std::tuple< <template-parameter-1-1> >::tuple(_UElements&& ...) [with _UElements = {const std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&, int&, int&}; <template-parameter-2-2> = void; _Elements = {std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, int, int}]\u2019\r\n   if (already_logged->insert({ptxas_path, cc_major, cc_minor}).second) {\r\n```\r\nhttps://github.com/google/jax/issues/6046#issuecomment-809613061 reported a same error.\r\nUsing `std::make_tuple` instead makes the error disappear.", "comments": ["Out of curiosity, what was the error for cupti_tracer.cc?\r\n\r\nUsing `std::make_tuple` makes sense. FWIW I think another pair of curly braces will fix the compiler's confusion:\r\n```diff\r\n-  if (already_logged->insert({ptxas_path, cc_major, cc_minor}).second) {\r\n+  if (already_logged->insert({{ptxas_path, cc_major, cc_minor}}).second) {\r\n```", "It's a similar error in `cupti_tracer.cc` (sorry I didn't record the log). These lines work fine in v2.4.1 using `std::make_tuple`:\r\nhttps://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/core/profiler/internal/gpu/cupti_tracer.cc#L156-L232\r\n\r\nAnd 60b9283c9e5308bae6fe19d2db531a6c230baffd breaks it. So I just reverted them.", "I think the changes from asm_compiler.cc (curly braces confuses a flat_hash_map of tuple) makes sense so +1 to the changes there. I think that is the same error as the JAX issue over at  https://github.com/google/jax/issues/6046#issuecomment-800439215.\r\n\r\nHowever, regarding cupti_tracer.cc (curly braces for initializing tuple directly, no hash maps), it is not clear to me whether the change fixes anything. Could we revert cupti_tracer.cc here and isolate it into a separate PR? This way the asm_compiler.cc fix can go in first.", "By the way, thanks for the investigation + fix, @njzjz!", "Ok, I've reverted changes to `cupti_tracer.cc`. You are welcome!"]}, {"number": 50928, "title": "ValueError: Could not find matching function to call loaded from the SavedModel", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[https://github.com/webster1309/colab/blob/main/tlex.ipynb]\r\n", "comments": ["@webster1309 ,\r\n\r\nPlease take a look at this [comment](https://github.com/tensorflow/tensorflow/issues/37973#issuecomment-605448707) and [link](https://stackoverflow.com/questions/58575586/could-not-find-matching-function-to-call-loaded-from-the-savedmodel) for similar error log.It helps.Thanks!", "@tilakrayal,\r\n\r\nthx, this helped", "@webster1309 ,\r\nGlad the suggestion worked for you,please move this to closed status.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50928\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50928\">No</a>\n"]}, {"number": 50926, "title": "Segmentation fault when using NCCL in MultiWorkerMirroredStrategy", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.2.0\r\n- CUDA/cuDNN version: 11.1.1\r\n- GPU model and memory: A100\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running the distributed training with MultiWorkerMirroredStrategy and NCCL the application will crash during a NCCL reduction operation due to a bug in TF.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSimilar to #50853 run the following on at least 2 nodes with at least 2 GPUs: \r\n\r\n```\r\nimport tensorflow as tf\r\nfrom mpi_cluster_resolver import MPIClusterResolver\r\n\r\nresolver = MPIClusterResolver()\r\ncommunication = tf.distribute.experimental.CollectiveCommunication.NCCL\r\noptions = tf.distribute.experimental.CommunicationOptions(implementation=communication)\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=options, cluster_resolver=resolver)\r\n\r\nwith strategy.scope():\r\n    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\r\n    x_train = x_train / 255.0\r\n    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(128)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n        tf.keras.layers.Dense(128, activation='relu'),\r\n        tf.keras.layers.Dense(10),\r\n    ])\r\n\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.SGD(),\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n        metrics=['accuracy'])\r\n\r\nis_master = resolver.task_id == 0\r\nverbose = 2 if is_master else 0\r\nmodel.fit(train_data.repeat(), epochs=1, steps_per_epoch=10, verbose=verbose)\r\n```\r\n\r\nWith \r\n[mpi_cluster_resolver.py.txt](https://github.com/tensorflow/tensorflow/files/6823734/mpi_cluster_resolver.py.txt)\r\n\r\n**Other info / logs**\r\n\r\nI was able to collect the following backtrace:\r\n```\r\n   #0  tensorflow::NcclCommunicator::Enqueue(std::shared_ptr<tensorflow::CollectiveContext>, std::function<void (tensorflow::Status const&)>) (this=0x1ebf9260, \r\nstd::shared_ptr<tensorflow::CollectiveContext> (use count 5, weak count 0) = {...}, done=...) at tensorflow/core/nccl/collective_communicator.cc:78\r\n#1  0x00002aaacc95de30 in tensorflow::NcclReducer::Run(std::function<void (tensorflow::Status const&)>) ()\r\n   from <prefix>/tensorflow/python/_pywrap_tensorflow_internal.so\r\n#2  0x00002aaaeb699dc0 in std::_Function_handler<void (), tensorflow::BaseCollectiveExecutor::ExecuteAsync(tensorflow::OpKernelContext*, tensorflow::CollectiveParams const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::function<void (tensorflow::Status const&)>)::{lambda()#3}>::_M_invoke(std::_Any_data const&) ()\r\n   from <prefix>/tensorflow/python/../libtensorflow_framework.so.2\r\n#3  0x00002aaaeb2cdbf6 in std::function<void ()>::operator()() const ()\r\n   from <prefix>/tensorflow/python/../libtensorflow_framework.so.2\r\n#4  0x00002aaaeb9350a9 in tensorflow::UnboundedWorkQueue::PooledThreadFunc() ()\r\n   from <prefix>/tensorflow/python/../libtensorflow_framework.so.2\r\n#5  0x00002aaaeb935164 in std::_Function_handler<void (), tensorflow::UnboundedWorkQueue::Schedule(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\r\n   from <prefix>/tensorflow/python/../libtensorflow_framework.so.2\r\n#6  0x00002aaaeb2cdbf6 in std::function<void ()>::operator()() const ()\r\n   from <prefix>/tensorflow/python/../libtensorflow_framework.so.2\r\n#7  0x00002aaaeb93c799 in tensorflow::(anonymous namespace)::PThread::ThreadFn(void*) ()\r\n   from <prefix>/tensorflow/python/../libtensorflow_framework.so.2\r\n#8  0x00002aaaab7deea5 in start_thread () from /lib64/libpthread.so.0\r\n#9  0x00002aaaabcf496d in clone () from /lib64/libc.so.6\r\n```\r\n\r\nFurther debugging led me to https://github.com/tensorflow/tensorflow/blob/98765aab1af708f8663fb983aca75115d70da8df/tensorflow/core/nccl/collective_communicator.cc#L89 which calls effectively `col_ctx->op_ctx->device()->tensorflow_gpu_device_info()` which returns `nullptr` which is then returned an dereferenced in `col_ctx->op_ctx->op_device_context()->stream()` which obviously segfaults.\r\nSome further debugging examining `col_ctx->op_ctx->device()->name()` shows that in the case of the crash `/job:worker/replica:0/task:1/device:CPU:0` or `/job:worker/replica:0/task:0/device:CPU:0` is used (both of the 2 nodes crash), so a CPU device ends up there which of course does not have a GPU info set.", "comments": ["The VERBOSE 2 log shows:\r\n```\r\n2021-07-23 17:56:53.724673: I tensorflow/core/common_runtime/collective_param_resolver_local.cc:688] CompleteInstanceLocal /job:worker/replica:0/task:1/device:CPU:0 instance_key: 101 gr 0x2b35e80016f0\r\n2021-07-23 17:56:53.724702: I tensorflow/core/common_runtime/collective_param_resolver_local.cc:681] AssignCollectiveType NcclReduce\r\n2021-07-23 17:56:53.724717: I tensorflow/core/kernels/collective_ops.cc:579] CollectiveReduceV2 ExecuteAsync start for collective CollectiveReduceV2: ReduceV2(Add,Id) device /job:worker/replica:0/task:1/device:CPU:0 group 2 instance 101\r\n2021-07-23 17:56:53.724732: I tensorflow/core/common_runtime/base_collective_executor.cc:395] CreateCollective type int64 name NcclReduce\r\n```", "I have the same issue using SlurmClusterResolver , with tensorflow-2.5.0 and 2.6.0. \r\nI have a very similar environment : compiled from source, RHEL 8, bazel 3.7.2, gcc 8.3.1, CUDA 11.2, NCCL 2.8.3, V100\r\n\r\nAny news?", "We investigated a bit and compared the tensorflow logs for tensorflow-2.4.1 (no segfault) and tensorflow-2.5.0 (segfault). We use 1 node with 4 GPUs, 2 workers, and each worker managing 2 GPUs. We also use Keras (`model.fit()`) and the MultiWorkerMirroredStrategy. \r\n\r\nUsing tensorflow-2.4.1: \r\n```\r\nEpoch 1/10\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 4, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 4, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 4, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 4, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 1, group_size = 2, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 6 all-reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 4, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 4, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 4, implementation = AUTO, num_packs = 1\r\nINFO:tensorflow:Collective batch_all_reduce: 1 all-reduces, num_devices = 2, group_size = 4, implementation = AUTO, num_packs = 1\r\n```\r\n\r\nUsing tensorflow-2.5.0:\r\n```\r\nEpoch 1/10\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 6 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 1, group_size = 2, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 6 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\nINFO:tensorflow:Collective all_reduce tensors: 1 all_reduces, num_devices = 2, group_size = 4, implementation = NCCL, num_packs = 1\r\n```\r\nWe see that the communication implementation does not switch to AUTO with tensorflow-2.5.0 when `all_reduces = 1`.\r\n\r\nHere is the code line where the switch is supposed to take place:\r\nhttps://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/distribute/cross_device_ops.py#L1124\r\n\r\nThe value of `self._limited_nccl` is always False with tensorflow-2.5.0.\r\n\r\nJust to see what happens, we set `self._limited_nccl = True` and the training ends properly.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50926\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50926\">No</a>\n"]}, {"number": 50925, "title": "Error in saving model: TypeError: No conversion path for dtype: dtype('<U30')", "body": "<em>I am building a new model based an existed model. However, I found that I could not save the new model</em>\r\n\r\n**Important Notice**\r\n\r\nI have an existed model which was built based on VGG16 (namely, 'base_network' in the following picture). Now, I am building a new model based on the existed model (namely, 'model' in the following picture). However, the existed model could be saved using the keras.models.save, but the new model could not.\r\n\r\nFor simplicity, the code is as follows,\r\n\r\n<img width=\"414\" alt=\"\u5fae\u4fe1\u622a\u56fe_20210723233351\" src=\"https://user-images.githubusercontent.com/24711658/126806731-1f1828be-fcf2-4d0d-bc18-1ec57be44189.png\">\r\n\r\nAs you can see, the first base_network.save('out.h5') works. the base_network can be saved. However, when it comes to model.save('out2.h5'), problems occur.\r\n\r\nI do not know why. the base_network and model looks the same in these codes. Could anyone help me? thanks.\r\n\r\n<img width=\"688\" alt=\"20210723234456\" src=\"https://user-images.githubusercontent.com/24711658/126807518-c2160c9e-47f7-42a0-976c-afad5ac294ff.png\">\r\n\r\nHere, I also uploaded the summary information.\r\n<img width=\"618\" alt=\"20210723234641\" src=\"https://user-images.githubusercontent.com/24711658/126807803-5b111478-2d82-4349-8fe4-e8bf3a2a11a3.png\">\r\n<img width=\"588\" alt=\"20210723234659\" src=\"https://user-images.githubusercontent.com/24711658/126807810-74a744bc-e363-4c64-939b-4ca167ff4852.png\">\r\n\r\n\r\n\r\n\r\n", "comments": ["problems solved when I built a new environment. Looks like it is caused by the environment."]}, {"number": 50924, "title": "Building TF.Lite benchmark tool with TF.text ops support", "body": "Does anyone have an example of re-building a TF.Lite binary with support for tensorflow-text operations?\r\n\r\nI have a TF.Lite model which includes [USE-multilingual](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3), which uses several Sentencepiece ops from tensorflow-text. After reading https://github.com/tensorflow/hub/issues/463 and a few others, I was able to successfully covert the model, and it returns results during inference as expected when using the TF.Lite Interpreter.\r\n\r\nI'd like to run the TF.Lite native benchmark tool against this model as well, but doing so requires building the tool from source with tensorflow-text ops. Thus far, I haven't been able to build a working version of the tool for my model. A few things I've tried are listed below, any help would be much appreciated. Thanks!\r\n\r\n**Attempt 1: Include tensorflow-text as an external repository and target tensorflow_text:ops_lib**\r\n\r\nMy first attempt was to build `tensorflow_text:ops_lib` directly into the tool from source, which seems to be the more Bazel-appropriate way to tackle the problem. The diff below shows my most recent attempt to build the benchmark tool in this manner.\r\n\r\n<details><summary>Diff</summary><p>\r\n\r\n```diff\r\ndiff --git a/tensorflow/BUILD b/tensorflow/BUILD\r\nindex 3ef74d742ef..c653a384c69 100644\r\n--- a/tensorflow/BUILD\r\n+++ b/tensorflow/BUILD\r\n@@ -38,7 +38,7 @@ load(\r\n load(\"@bazel_skylib//:bzl_library.bzl\", \"bzl_library\")\r\n \r\n package(\r\n-    default_visibility = [\":internal\"],\r\n+    default_visibility = [\"//visibility:public\"],\r\n     licenses = [\"notice\"],  # Apache 2.0\r\n )\r\n \r\ndiff --git a/tensorflow/lite/c/BUILD b/tensorflow/lite/c/BUILD\r\nindex 3119cf30557..3b7ff060518 100644\r\n--- a/tensorflow/lite/c/BUILD\r\n+++ b/tensorflow/lite/c/BUILD\r\n@@ -38,6 +38,8 @@ tflite_cc_shared_object(\r\n         \":c_api_experimental\",\r\n         \":exported_symbols.lds\",\r\n         \":version_script.lds\",\r\n+        \"//tensorflow/lite/delegates/flex:delegate\",\r\n+        \"@org_tensorflow_text//tensorflow_text:ops_lib\",\r\n     ],\r\n )\r\n \r\ndiff --git a/tensorflow/lite/tools/benchmark/BUILD b/tensorflow/lite/tools/benchmark/BUILD\r\nindex 815efb776e9..0793db4a417 100644\r\n--- a/tensorflow/lite/tools/benchmark/BUILD\r\n+++ b/tensorflow/lite/tools/benchmark/BUILD\r\n@@ -24,9 +24,12 @@ cc_library(\r\n     deps = [\r\n         \":benchmark_tflite_model_lib\",\r\n         \"//tensorflow/lite/tools:logging\",\r\n+        \"//tensorflow/lite/delegates/flex:delegate\",\r\n+        \"@org_tensorflow_text//tensorflow_text:ops_lib\",\r\n     ],\r\n )\r\n \r\n+\r\n cc_binary(\r\n     name = \"benchmark_model\",\r\n     copts = common_copts,\r\ndiff --git a/tensorflow/workspace0.bzl b/tensorflow/workspace0.bzl\r\nindex 22374bc1297..f8483aa2f53 100644\r\n--- a/tensorflow/workspace0.bzl\r\n+++ b/tensorflow/workspace0.bzl\r\n@@ -102,6 +102,38 @@ def workspace():\r\n         ],\r\n     )\r\n \r\n+    # TF.text repo\r\n+    http_archive(\r\n+        name = \"org_tensorflow_text\",\r\n+        sha256 = \"d82856dc04c04bbce347f72d0ad7df59bb6b26b7030f96f25f036cfe8a138312\",\r\n+        strip_prefix = \"text-2.5.0\",\r\n+        urls = [\r\n+            \"https://github.com/tensorflow/text/archive/v2.5.0.zip\",\r\n+        ],\r\n+        patches = [\"@//third_party/tf_text:tftext10.patch\"],\r\n+        patch_args = [\"-p1\"],\r\n+    )\r\n+\r\n+    # TF.text dependencies\r\n+    http_archive(\r\n+        name = \"com_google_sentencepiece\",\r\n+        strip_prefix = \"sentencepiece-1.0.0\",\r\n+        sha256 = \"c05901f30a1d0ed64cbcf40eba08e48894e1b0e985777217b7c9036cac631346\",\r\n+        urls = [\r\n+            \"https://github.com/google/sentencepiece/archive/1.0.0.zip\",\r\n+        ],\r\n+    )\r\n+\r\n+    http_archive(\r\n+        name = \"com_google_glog\",\r\n+        sha256 = \"1ee310e5d0a19b9d584a855000434bb724aa744745d5b8ab1855c85bff8a8e21\",\r\n+        strip_prefix = \"glog-028d37889a1e80e8a07da1b8945ac706259e5fd8\",\r\n+        urls = [\r\n+            \"https://mirror.bazel.build/github.com/google/glog/archive/028d37889a1e80e8a07da1b8945ac706259e5fd8.tar.gz\",\r\n+            \"https://github.com/google/glog/archive/028d37889a1e80e8a07da1b8945ac706259e5fd8.tar.gz\",\r\n+        ],\r\n+    )\r\n+\r\n     bazel_toolchains_repositories()\r\n \r\n     # Use `swift_rules_dependencies` to fetch the toolchains. With the\r\n@@ -117,6 +149,7 @@ def workspace():\r\n     grpc_extra_deps()\r\n     config_googleapis()\r\n \r\n+\r\n # Alias so it can be loaded without assigning to a different symbol to prevent\r\n # shadowing previous loads and trigger a buildifier warning.\r\n tf_workspace0 = workspace\r\ndiff --git a/third_party/eigen3/BUILD b/third_party/eigen3/BUILD\r\nindex e2253e18f8a..c0802240b4b 100644\r\n--- a/third_party/eigen3/BUILD\r\n+++ b/third_party/eigen3/BUILD\r\n@@ -47,7 +47,7 @@ filegroup(\r\n         [\"**/*\"],\r\n         exclude = [\"**/OWNERS\"],\r\n     ),\r\n-    visibility = [\"//tensorflow:__subpackages__\"],\r\n+    visibility = [\"//visibility:public\"],\r\n )\r\n \r\n filegroup(\r\n@@ -74,4 +74,5 @@ genrule(\r\n     done\r\n     \"\"\",\r\n     tags = [\"manual\"],\r\n+    visibility = [\"//visibility:public\"],\r\n )\r\ndiff --git a/third_party/icu/BUILD.bazel b/third_party/icu/BUILD.bazel\r\nindex 14cadffc841..691585ce5ba 100644\r\n--- a/third_party/icu/BUILD.bazel\r\n+++ b/third_party/icu/BUILD.bazel\r\n@@ -19,6 +19,16 @@ cc_library(\r\n     ],\r\n )\r\n \r\n+alias(\r\n+    name = \"nfkc\",\r\n+    actual = \":common\",\r\n+)\r\n+\r\n+alias(\r\n+    name = \"nfkc_cf\",\r\n+    actual = \":common\",\r\n+)\r\n+\r\n cc_library(\r\n     name = \"common\",\r\n     hdrs = glob([\"icu4c/source/common/unicode/*.h\"]),\r\ndiff --git a/third_party/tf_text/tftext10.patch b/third_party/tf_text/tftext10.patch\r\nnew file mode 100644\r\nindex 00000000000..cd313cd3ad3\r\n--- /dev/null\r\n+++ b/third_party/tf_text/tftext10.patch\r\n@@ -0,0 +1,29 @@\r\n+diff --git a/tensorflow_text/tftext.bzl b/tensorflow_text/tftext.bzl\r\n+index 7f703b3..b840c0f 100644\r\n+--- a/tensorflow_text/tftext.bzl\r\n++++ b/tensorflow_text/tftext.bzl\r\n+@@ -93,8 +93,8 @@ def tf_deps(deps = []):\r\n+             \"@org_tensorflow//tensorflow/core:portable_tensorflow_lib_lite\",\r\n+         ],\r\n+         \"//conditions:default\": [\r\n+-            \"@local_config_tf//:libtensorflow_framework\",\r\n+-            \"@local_config_tf//:tf_header_lib\",\r\n++            \"@org_tensorflow//tensorflow:libtensorflow_framework_import_lib\",\r\n++            #\"@org_tensorflow//tensorflow/c:headers\",\r\n+         ] + deps + oss_deps,\r\n+     })\r\n+ \r\n+diff --git a/third_party/sentencepiece/processor.patch b/third_party/sentencepiece/processor.patch\r\n+index 5fa1b84..c5a7358 100644\r\n+--- a/third_party/sentencepiece/processor.patch\r\n++++ b/third_party/sentencepiece/processor.patch\r\n+@@ -22,8 +22,7 @@ index b4298d2..7ce779f 100644\r\n+ +              \"@org_tensorflow//tensorflow/core:tflite_portable_logging\",\r\n+ +            ],\r\n+ +            \"//conditions:default\": [\r\n+-+              \"@local_config_tf//:libtensorflow_framework\",\r\n+-+              \"@local_config_tf//:tf_header_lib\",\r\n+++              \"@org_tensorflow//tensorflow:libtensorflow_framework_import_lib\",\r\n+ +              \"@com_google_absl//absl/functional:function_ref\",\r\n+ +              \"@com_google_absl//absl/strings:cord\",\r\n+ +            ],\r\n```\r\n\r\n</p></details>\r\n\r\nThis build fails with a number of different dependency path/access errors, depending on Bazel build order. Typically, it either can't find an `Eigen` header or a `Protobuf` header when starting from tensorflow-text source (this is what led me to attempt changing some of the rule visibilities, but with no luck). I've included a couple of these error messages for reference.\r\n\r\n<details><summary>Example 1</summary><p>\r\n\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/b813f517b143a3c1665dd902035fe00f/external/org_tensorflow_text/tensorflow_text/core/kernels/BUILD:355:23: C++ compilation of rule '@org_tensorflow_text//tensorflow_text/core/kernels:sentencepiece_kernels' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/b813f517b143a3c1665dd902035fe00f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib: \\\r\n    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/cuda:/home/ubuntu/src/tensorflow/.venv/bin:/opt/amazon/openmpi/bin/:/opt/amazon/efa/bin/:/home/ubuntu/anaconda3/condabin:/home/ubuntu/.dl_binaries/bin:/usr/local/cuda/bin:/opt/aws/neuron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/ubuntu/src/tensorflow/.venv/bin/python3 \\\r\n    PYTHON_LIB_PATH=/home/ubuntu/src/tensorflow/.venv/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/core/kernels/_objs/sentencepiece_kernels/sentencepiece_kernels.d '-frandom-seed=bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/core/kernels/_objs/sentencepiece_kernels/sentencepiece_kernels.o' -iquote external/org_tensorflow_text -iquote bazel-out/k8-opt/bin/external/org_tensorflow_text -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/com_google_sentencepiece -iquote bazel-out/k8-opt/bin/external/com_google_sentencepiece -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/com_github_gflags_gflags -iquote bazel-out/k8-opt/bin/external/com_github_gflags_gflags -iquote external/com_google_glog -iquote bazel-out/k8-opt/bin/external/com_google_glog -iquote external/com_google_googletest -iquote bazel-out/k8-opt/bin/external/com_google_googletest -iquote . -iquote bazel-out/k8-opt/bin -Ibazel-out/k8-opt/bin/external/com_google_glog/_virtual_includes/glog -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/com_github_gflags_gflags/include -isystem bazel-out/k8-opt/bin/external/com_github_gflags_gflags/include -isystem external/com_google_googletest/googlemock -isystem bazel-out/k8-opt/bin/external/com_google_googletest/googlemock -isystem external/com_google_googletest/googlemock/include -isystem bazel-out/k8-opt/bin/external/com_google_googletest/googlemock/include -isystem external/com_google_googletest/googletest -isystem bazel-out/k8-opt/bin/external/com_google_googletest/googletest -isystem external/com_google_googletest/googletest/include -isystem bazel-out/k8-opt/bin/external/com_google_googletest/googletest/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/org_tensorflow_text/tensorflow_text/core/kernels/sentencepiece_kernels.cc -o bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/core/kernels/_objs/sentencepiece_kernels/sentencepiece_kernels.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from ./tensorflow/core/framework/bounds_check.h:21,\r\n                 from external/org_tensorflow_text/tensorflow_text/core/kernels/sentencepiece_kernels.cc:25:\r\n./third_party/eigen3/Eigen/Core:1:10: fatal error: Eigen/Core: No such file or directory\r\n #include \"Eigen/Core\"\r\n          ^~~~~~~~~~~~\r\ncompilation terminated. \r\n\r\n</p></details>\r\n\r\n<details><summary>Example 2</summary><p>\r\n\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/b813f517b143a3c1665dd902035fe00f/external/org_tensorflow_text/tensorflow_text/BUILD:1162:19: C++ compilation of rule '@org_tensorflow_text//tensorflow_text:wordpiece_tokenizer_cc' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/b813f517b143a3c1665dd902035fe00f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib: \\\r\n    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/home/ubuntu/src/tensorflow/.venv/bin:/opt/amazon/openmpi/bin/:/opt/amazon/efa/bin/:/home/ubuntu/anaconda3/condabin:/home/ubuntu/.dl_binaries/bin:/usr/local/cuda/bin:/opt/aws/neuron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/ubuntu/src/tensorflow/.venv/bin/python \\\r\n    PYTHON_LIB_PATH=/home/ubuntu/src/tensorflow/.venv/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/_objs/wordpiece_tokenizer_cc/wordpiece_op.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/_objs/wordpiece_tokenizer_cc/wordpiece_op.pic.o' -fPIC -iquote external/org_tensorflow_text -iquote bazel-out/k8-opt/bin/external/org_tensorflow_text -iquote external/icu -iquote bazel-out/k8-opt/bin/external/icu -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -isystem external/icu/icu4c/source/common -isystem bazel-out/k8-opt/bin/external/icu/icu4c/source/common -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/org_tensorflow_text/tensorflow_text/core/ops/wordpiece_op.cc -o bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/_objs/wordpiece_tokenizer_cc/wordpiece_op.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from ./tensorflow/core/framework/op_def_builder.h:24,\r\n                 from ./tensorflow/core/framework/op.h:23,\r\n                 from external/org_tensorflow_text/tensorflow_text/core/ops/wordpiece_op.cc:15:\r\nbazel-out/k8-opt/bin/tensorflow/core/framework/op_def.pb.h:10:10: fatal error: google/protobuf/port_def.inc: No such file or directory\r\n #include <google/protobuf/port_def.inc>\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n\r\n</p></details>\r\n\r\n**Attempt 2: Compile tensorflow-text ops separately and link library**\r\n\r\nI was able to compile tensorflow-text with `linkstatic=1` and `linkshared=True`, hoping this would give me the best chance at a usable library (based on the `linkshared` description in the [Bazel `cc_binary` docs](https://docs.bazel.build/versions/main/be/c-cpp.html#cc_binary)). However, when I try to link this in `tensorflow/lite/tools/benchmark:benchmark_model`, the build completes, but gives the following runtime error:\r\n```\r\nubuntu@ip-10-22-3-51:~/src/tensorflow$ bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --help\r\nbazel-bin/tensorflow/lite/tools/benchmark/benchmark_model: error while loading shared libraries: libtensorflow_framework.so.2: cannot open shared object file: No such file or directory\r\n```\r\n\r\nWhen I try to link that library as well (which doesn't make much sense to include in a separate TF build, admittedly), I get the following runtime crash:\r\n```\r\nubuntu@ip-10-22-3-51:~/src/tensorflow$ bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --help\r\nterminate called after throwing an instance of 'std::bad_alloc'\r\n  what():  std::bad_alloc\r\nAborted (core dumped)\r\n```\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: checked out from v2.5.0\r\n- Python version: 3.6.3\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 3.7.2 (via Bazelisk)\r\n- GCC/Compiler version (if compiling from source): gcc 8.4.0\r\n- CUDA/cuDNN version: CUDA Version 10.0.130/cuDNN 7.5\r\n- GPU model and memory: NVIDIA GV100GL Tesla V100 SXM2 (16GB)\r\n", "comments": ["Hi Tian / Thai,\r\n\r\nWondering if you have an example of using building TFLite with tf.text flex ops?\r\n\r\nThanks,\r\nTiezhen", "For the first attempt, you should modify `benchmark_model_plus_flex` and use that tool instead of `benchmark_model`\r\n\r\nIf you want a smaller binary size, You might try to define a new custom library as: https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/custom_ops/BUILD#L10 then use it to replace the `//tensorflow/lite/delegates/flex:delegate` in `benchmark_model_plus_flex`", "Hey @thaink, @wangtz, thanks for the quick response!\r\n\r\nI hadn't seen the flex benchmark tool, was just adding the flex delegate to the base `benchmark_model`. Will use `benchmark_model_plus_flex` going forward.\r\n\r\nI've tried building that tool this morning, but I'm seeing an `Eigen` dependency error on the build similar to what I was seeing previously. Details below:\r\n\r\n<details><summary>Additional change for `benchmark_model_plus_flex`</summary><p>\r\n\r\n```diff\r\ndiff --git a/tensorflow/lite/tools/benchmark/BUILD b/tensorflow/lite/tools/benchmark/BUILD\r\nindex 0793db4a417..91d7b4dfa15 100644\r\n--- a/tensorflow/lite/tools/benchmark/BUILD\r\n+++ b/tensorflow/lite/tools/benchmark/BUILD\r\n@@ -91,6 +91,7 @@ tf_cc_binary(\r\n         \"//tensorflow/lite/delegates/flex:delegate\",\r\n         \"//tensorflow/lite/testing:init_tensorflow\",\r\n         \"//tensorflow/lite/tools:logging\",\r\n+        \"@org_tensorflow_text//tensorflow_text:ops_lib\",\r\n     ],\r\n )\r\n```\r\n\r\n</p></details>\r\n\r\n<details><summary>Build error</summary><p>\r\n\r\nbazel build --config=monolithic  -c opt tensorflow/lite/tools/benchmark:benchmark_model_plus_flex --verbose_failures\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=202\r\nINFO: Reading rc options for 'build' from /home/ubuntu/src/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/ubuntu/src/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/ubuntu/src/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/ubuntu/src/tensorflow/.venv/bin/python3 --action_env PYTHON_LIB_PATH=/home/ubuntu/src/tensorflow/.venv/lib/python3.6/site-packages --python_path=/home/ubuntu/src/tensorflow/.venv/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0\r\nINFO: Found applicable config definition build:short_logs in file /home/ubuntu/src/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/ubuntu/src/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:monolithic in file /home/ubuntu/src/tensorflow/.bazelrc: --define framework_shared_object=false\r\nINFO: Found applicable config definition build:linux in file /home/ubuntu/src/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/ubuntu/src/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target //tensorflow/lite/tools/benchmark:benchmark_model_plus_flex (259 packages loaded, 21443 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/ubuntu/.cache/bazel/_bazel_ubuntu/b813f517b143a3c1665dd902035fe00f/external/org_tensorflow_text/tensorflow_text/BUILD:1010:19: C++ compilation of rule '@org_tensorflow_text//tensorflow_text:unicode_script_tokenizer_cc' failed (Exit 1): gcc failed: error executing command \r\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/b813f517b143a3c1665dd902035fe00f/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \\\r\n    LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib: \\\r\n    PATH=/home/ubuntu/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/cuda:/opt/amazon/openmpi/bin/:/opt/amazon/efa/bin/:/home/ubuntu/anaconda3/condabin:/home/ubuntu/.dl_binaries/bin:/usr/local/cuda/bin:/opt/aws/neuron/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/ubuntu/src/tensorflow/.venv/bin/python3 \\\r\n    PYTHON_LIB_PATH=/home/ubuntu/src/tensorflow/.venv/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/_objs/unicode_script_tokenizer_cc/unicode_script_tokenize_op.d '-frandom-seed=bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/_objs/unicode_script_tokenizer_cc/unicode_script_tokenize_op.o' -iquote external/org_tensorflow_text -iquote bazel-out/k8-opt/bin/external/org_tensorflow_text -iquote external/icu -iquote bazel-out/k8-opt/bin/external/icu -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -isystem external/icu/icu4c/source/common -isystem bazel-out/k8-opt/bin/external/icu/icu4c/source/common -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external/org_tensorflow_text/tensorflow_text/core/ops/unicode_script_tokenize_op.cc -o bazel-out/k8-opt/bin/external/org_tensorflow_text/tensorflow_text/_objs/unicode_script_tokenizer_cc/unicode_script_tokenize_op.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nIn file included from ./tensorflow/core/framework/tensor_shape.h:21,\r\n                 from ./tensorflow/core/framework/partial_tensor_shape.h:20,\r\n                 from ./tensorflow/core/framework/attr_value_util.h:23,\r\n                 from ./tensorflow/core/framework/node_def_util.h:23,\r\n                 from ./tensorflow/core/framework/shape_inference.h:21,\r\n                 from ./tensorflow/core/framework/common_shape_fns.h:20,\r\n                 from external/org_tensorflow_text/tensorflow_text/core/ops/unicode_script_tokenize_op.cc:18:\r\n./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:10: fatal error: unsupported/Eigen/CXX11/Tensor: No such file or directory\r\n #include \"unsupported/Eigen/CXX11/Tensor\"\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncompilation terminated.\r\n\r\n</p></details>\r\n\r\nAny other ideas on what may be wrong here? I can build a working benchmark tool, even with flex ops, so I think the external dependencies are available for use. It's just that they aren't able to be found when the target file is in the `@org_tensorflow_text` external repo.", "@broken Is possible to depend on `@org_tensorflow_text//tensorflow_text:ops_lib` outside of the text repo?", "Yes; this is what we do with model server.\r\n\r\nI have to admit that I'm inexperienced building tf text as flex ops, but the [tflite_cc_shared_object](https://github.com/tensorflow/text/blob/master/tensorflow_text/BUILD#L46) in our build does and is working.\r\n\r\nFrom the error message, it looks like the build is using the normal TF deps rather than the mobile target & portable tflite deps. Would adding --config=android fix this?\r\n\r\n---\r\n\r\nFYI; the [tflite-support library](https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/custom_ops) has a few TF Lite version of TF Text ops which includes a mobile version of Sentencepiece. This will be more performant. We're currently working on getting these in the main tf text repo and expanding the offering of native tf lite/text ops.\r\n", "@broken @thaink Thanks for the quick feedback!! I've been continuing to work on this based on your suggestions. (I wasn't able to get it working yet with that tflite-support library, but it looks really cool -- will try again to use it in the future.) In case others are interested, I was able to finally get builds working by applying the diff I've included here. \r\n\r\n<details><summary>Diff</summary><p>\r\n\r\n```diff\r\ndiff --git a/tensorflow/lite/c/BUILD b/tensorflow/lite/c/BUILD\r\nindex 3119cf30557..3b7ff060518 100644\r\n--- a/tensorflow/lite/c/BUILD\r\n+++ b/tensorflow/lite/c/BUILD\r\n@@ -38,6 +38,8 @@ tflite_cc_shared_object(\r\n         \":c_api_experimental\",\r\n         \":exported_symbols.lds\",\r\n         \":version_script.lds\",\r\n+        \"//tensorflow/lite/delegates/flex:delegate\",\r\n+        \"@org_tensorflow_text//tensorflow_text:ops_lib\",\r\n     ],\r\n )\r\n \r\ndiff --git a/tensorflow/lite/tools/benchmark/BUILD b/tensorflow/lite/tools/benchmark/BUILD\r\nindex 815efb776e9..c100bad4223 100644\r\n--- a/tensorflow/lite/tools/benchmark/BUILD\r\n+++ b/tensorflow/lite/tools/benchmark/BUILD\r\n@@ -88,6 +88,7 @@ tf_cc_binary(\r\n         \"//tensorflow/lite/delegates/flex:delegate\",\r\n         \"//tensorflow/lite/testing:init_tensorflow\",\r\n         \"//tensorflow/lite/tools:logging\",\r\n+        \"@org_tensorflow_text//tensorflow_text:ops_lib\"\r\n     ],\r\n )\r\n \r\ndiff --git a/tensorflow/workspace0.bzl b/tensorflow/workspace0.bzl\r\nindex 22374bc1297..abf1eb96677 100644\r\n--- a/tensorflow/workspace0.bzl\r\n+++ b/tensorflow/workspace0.bzl\r\n@@ -102,6 +102,38 @@ def workspace():\r\n         ],\r\n     )\r\n \r\n+    # TF.text repo\r\n+    http_archive(\r\n+        name = \"org_tensorflow_text\",\r\n+        sha256 = \"d82856dc04c04bbce347f72d0ad7df59bb6b26b7030f96f25f036cfe8a138312\",\r\n+        strip_prefix = \"text-2.5.0\",\r\n+        urls = [\r\n+            \"https://github.com/tensorflow/text/archive/v2.5.0.zip\",\r\n+        ],\r\n+        patches = [\"@//third_party/tf_text:tf_text_delegate.patch\"],\r\n+        patch_args = [\"-p1\"],\r\n+    )\r\n+\r\n+    # TF.text dependencies\r\n+    http_archive(\r\n+        name = \"com_google_sentencepiece\",\r\n+        strip_prefix = \"sentencepiece-1.0.0\",\r\n+        sha256 = \"c05901f30a1d0ed64cbcf40eba08e48894e1b0e985777217b7c9036cac631346\",\r\n+        urls = [\r\n+            \"https://github.com/google/sentencepiece/archive/1.0.0.zip\",\r\n+        ],\r\n+    )\r\n+\r\n+    http_archive(\r\n+        name = \"com_google_glog\",\r\n+        sha256 = \"1ee310e5d0a19b9d584a855000434bb724aa744745d5b8ab1855c85bff8a8e21\",\r\n+        strip_prefix = \"glog-028d37889a1e80e8a07da1b8945ac706259e5fd8\",\r\n+        urls = [\r\n+            \"https://mirror.bazel.build/github.com/google/glog/archive/028d37889a1e80e8a07da1b8945ac706259e5fd8.tar.gz\",\r\n+            \"https://github.com/google/glog/archive/028d37889a1e80e8a07da1b8945ac706259e5fd8.tar.gz\",\r\n+        ],\r\n+    )\r\n+\r\n     bazel_toolchains_repositories()\r\n \r\n     # Use `swift_rules_dependencies` to fetch the toolchains. With the\r\ndiff --git a/third_party/icu/BUILD.bazel b/third_party/icu/BUILD.bazel\r\nindex 14cadffc841..691585ce5ba 100644\r\n--- a/third_party/icu/BUILD.bazel\r\n+++ b/third_party/icu/BUILD.bazel\r\n@@ -19,6 +19,16 @@ cc_library(\r\n     ],\r\n )\r\n \r\n+alias(\r\n+    name = \"nfkc\",\r\n+    actual = \":common\",\r\n+)\r\n+\r\n+alias(\r\n+    name = \"nfkc_cf\",\r\n+    actual = \":common\",\r\n+)\r\n+\r\n cc_library(\r\n     name = \"common\",\r\n     hdrs = glob([\"icu4c/source/common/unicode/*.h\"]),\r\ndiff --git a/third_party/tf_text/BUILD b/third_party/tf_text/BUILD\r\nnew file mode 100644\r\nindex 00000000000..e69de29bb2d\r\ndiff --git a/third_party/tf_text/tf_text_delegate.patch b/third_party/tf_text/tf_text_delegate.patch\r\nnew file mode 100644\r\nindex 00000000000..b0cba5225d5\r\n--- /dev/null\r\n+++ b/third_party/tf_text/tf_text_delegate.patch\r\n@@ -0,0 +1,30 @@\r\n+diff --git a/tensorflow_text/tftext.bzl b/tensorflow_text/tftext.bzl\r\n+index 7f703b3..888bea4 100644\r\n+--- a/tensorflow_text/tftext.bzl\r\n++++ b/tensorflow_text/tftext.bzl\r\n+@@ -93,8 +93,8 @@ def tf_deps(deps = []):\r\n+             \"@org_tensorflow//tensorflow/core:portable_tensorflow_lib_lite\",\r\n+         ],\r\n+         \"//conditions:default\": [\r\n+-            \"@local_config_tf//:libtensorflow_framework\",\r\n+-            \"@local_config_tf//:tf_header_lib\",\r\n++            \"@org_tensorflow//tensorflow/core:framework\",\r\n++            \"@org_tensorflow//tensorflow/core:lib\",\r\n+         ] + deps + oss_deps,\r\n+     })\r\n+ \r\n+diff --git a/third_party/sentencepiece/processor.patch b/third_party/sentencepiece/processor.patch\r\n+index 5fa1b84..427c104 100644\r\n+--- a/third_party/sentencepiece/processor.patch\r\n++++ b/third_party/sentencepiece/processor.patch\r\n+@@ -22,8 +22,8 @@ index b4298d2..7ce779f 100644\r\n+ +              \"@org_tensorflow//tensorflow/core:tflite_portable_logging\",\r\n+ +            ],\r\n+ +            \"//conditions:default\": [\r\n+-+              \"@local_config_tf//:libtensorflow_framework\",\r\n+-+              \"@local_config_tf//:tf_header_lib\",\r\n+++              \"@org_tensorflow//tensorflow/core:framework\",\r\n+++              \"@org_tensorflow//tensorflow/core:lib\",\r\n+ +              \"@com_google_absl//absl/functional:function_ref\",\r\n+ +              \"@com_google_absl//absl/strings:cord\",\r\n+ +            ],\r\n```\r\n</p></details>\r\n\r\nHere are also a few other error messages that I saw along the way but didn't post above, in case it helps someone else arrive here:\r\n\r\n<details><summary>Error message</summary><p>\r\n\r\nERROR: Op type not registered 'SentencepieceOp' in binary running on ip-10-22-3-51. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) tf.contrib.resampler should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 11 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\n\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 17 nodes with 0 partitions.\r\n\r\nERROR: Delegate kernel was not initialized\r\nERROR: Node number 304 (TfLiteFlexDelegate) failed to prepare.\r\n\r\n</p></details>\r\n\r\n<details><summary>Error message</summary><p>\r\n\r\n/home/ubuntu/.cache/bazel/_bazel_ubuntu/..../external/org_tensorflow_text/tensorflow_text/BUILD:1162:19: no such package '@local_config_tf//': The repository '@local_config_tf' could not be resolved and referenced by '@org_tensorflow_text//tensorflow_text:wordpiece_tokenizer_cc'\r\n\r\n</p></details>\r\n\r\n<details><summary>Error message</summary><p>\r\n\r\nERROR: /home/ubuntu/src/tensorflow/tensorflow/lite/delegates/flex/BUILD:73:23: error loading package '@org_tensorflow_text//tensorflow_text': Every .bzl file must have a corresponding package, but '@org_tensorflow_text//tensorflow/lite:build_def.bzl' does not have one. Please create a BUILD file in the same or any parent directory. Note that this BUILD file does not need to do anything except exist. and referenced by '//tensorflow/lite/delegates/flex:delegate'\r\n\r\n</p></details>\r\n\r\n<details><summary>Error message</summary><p>\r\n\r\nERROR: Analysis of target '//tensorflow_text:tensorflowlite_c' failed; build aborted: no such package 'tensorflow': BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package.\r\n\r\n</p></details>", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50924\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50924\">No</a>\n", "I'm glad you got it working, and thanks for such a detailed closing comment for the people of the future."]}, {"number": 50923, "title": "XNNPACK build error fixes #50920", "body": "Fixes an error in the XNNPACK enabled build of TensorFlow Lite by Bazel. https://github.com/tensorflow/tensorflow/issues/50920\r\nI have confirmed that the build finishes successfully on armhf and aarch64, x86_64.", "comments": ["This should be in master first.", "It has already been committed to master.", "Oh, in that case, please make it as a cherrypick of the commits, if possible.", "Understood. I have issued the pull request again. cherrypick: https://github.com/tensorflow/tensorflow/pull/51069"]}, {"number": 50922, "title": "SECURITY.md", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\n- Are you willing to contribute it (Yes/No):\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n[SECURITY.md](https://github.com/tensorflow/tensorflow/files/6868801/SECURITY.md)\r\n\r\n**Any Other info.**\r\n", "comments": ["@Apidwalin ,\r\n\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "This is spam"]}, {"number": 50921, "title": "AttributeError: 'NoneType' object has no attribute 'PluginAssets'", "body": "tensorflow 2.5\r\n\r\n**This problem occurs when I'm using the tensorboard to do the profiling.**\r\n\r\nThe code is as follows.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport datetime\r\n\r\n# The function to be traced.\r\n@tf.function\r\ndef my_func(x, y):\r\n  # A simple hand-rolled layer.\r\n  return tf.nn.relu(tf.matmul(x, y))\r\n\r\n# Set up logging.\r\nstamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\nlogdir = 'logs/func/%s' % stamp\r\nwriter = tf.summary.create_file_writer(logdir)\r\n\r\n# Sample data for your function.\r\nx = tf.random.uniform((3, 3))\r\ny = tf.random.uniform((3, 3))\r\n\r\n# Bracket the function call with\r\n# tf.summary.trace_on() and tf.summary.trace_export().\r\ntf.summary.trace_on(graph=True, profiler=True)\r\n# Call only one tf.function when tracing.\r\nz = my_func(x, y)\r\nwith writer.as_default():\r\n  tf.summary.trace_export(\r\n      name=\"my_func_trace\",\r\n      step=0,\r\n      profiler_outdir=logdir)\r\n```\r\n\r\n**It produces no error when running this code, seeing from the output.**\r\n\r\nsjtusmartboy@sjtusmartboy-ThinkPad-X230:~/opt/Projects/RandLA-Net$ python test3.py \r\n```\r\n2021-07-23 19:41:34.589229: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-23 19:41:36.138644: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-07-23 19:41:36.170275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.171081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.835GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-07-23 19:41:36.171145: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-23 19:41:36.176195: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-07-23 19:41:36.176262: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\n2021-07-23 19:41:36.178526: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-07-23 19:41:36.178833: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-07-23 19:41:36.179436: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\r\n2021-07-23 19:41:36.180522: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\r\n2021-07-23 19:41:36.180748: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-07-23 19:41:36.180909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.181553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.182112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-07-23 19:41:36.183934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.184486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \r\npciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 computeCapability: 6.1\r\ncoreClock: 1.835GHz coreCount: 20 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 298.32GiB/s\r\n2021-07-23 19:41:36.184588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.185197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.185693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-07-23 19:41:36.185742: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n2021-07-23 19:41:36.773690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-07-23 19:41:36.773748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-07-23 19:41:36.773763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n2021-07-23 19:41:36.773975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.774569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.775196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-07-23 19:41:36.775721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6857 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\r\nWARNING:tensorflow:From /home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1263: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\r\nInstructions for updating:\r\nuse `tf.profiler.experimental.start` instead.\r\n2021-07-23 19:41:36.884168: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\r\n2021-07-23 19:41:36.884221: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\r\n2021-07-23 19:41:36.884273: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 1 GPUs\r\n2021-07-23 19:41:36.885256: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcupti.so.11.2\r\n2021-07-23 19:41:37.125954: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\n2021-07-23 19:41:37.126748: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2793545000 Hz\r\n2021-07-23 19:41:37.141797: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\r\n2021-07-23 19:41:37.573205: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\r\nWARNING:tensorflow:From /home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1319: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\r\nInstructions for updating:\r\nuse `tf.profiler.experimental.stop` instead.\r\n2021-07-23 19:41:37.576615: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\r\n2021-07-23 19:41:37.576966: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed\r\n2021-07-23 19:41:37.607794: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 11 callback api events and 145 activity events. \r\n2021-07-23 19:41:37.608909: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\r\nWARNING:tensorflow:From /home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1319: save (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\r\nInstructions for updating:\r\n`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\r\nWARNING:tensorflow:From /home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorflow/python/eager/profiler.py:151: maybe_create_event_file (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\r\nInstructions for updating:\r\n`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\r\n```\r\n\r\n**However, when I type the tensorboard command, `tensorboard --logdir logs/func`, and open the tensorboard webpage, it gives out the following error.**\r\n\r\n```\r\n2021-07-23 19:42:10.506328: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\r\n\r\nNOTE: Using experimental fast data loading logic. To disable, pass\r\n    \"--load_fast=false\" and report issues on GitHub. More details:\r\n    https://github.com/tensorflow/tensorboard/issues/4784\r\n\r\nServing TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\r\nTensorBoard 2.5.0 at http://localhost:6006/ (Press CTRL+C to quit)\r\nException in thread DynamicProfilePluginIsActiveThread:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorboard_plugin_profile/profile_plugin.py\", line 311, in compute_is_active\r\n    self._is_active = any(self.generate_run_to_tools())\r\n  File \"/home/sjtusmartboy/.local/lib/python3.6/site-packages/tensorboard_plugin_profile/profile_plugin.py\", line 693, in generate_run_to_tools\r\n    plugin_assets = self.multiplexer.PluginAssets(PLUGIN_NAME)\r\nAttributeError: 'NoneType' object has no attribute 'PluginAssets'\r\n\r\n\r\n```\r\n\r\n", "comments": ["Please tell me what's wrong and what I can do to manage this problem so that I can do the profiling work with tensorflow 2.5. Thanks a million.", "I am having a similar problem. A keras model is trained with a callback\r\n`tensorboard_callback = TensorBoard(log_dir='logs/{}'.format(log_dir_name))`.\r\nWhen i run tensorboard and activate the \"Profile\" menu item, i am getting the error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/FP/interq/ITP/.itp/lib/python3.6/site-packages/tensorboard_plugin_profile/profile_plugin.py\", line 311, in compute_is_active\r\n    self._is_active = any(self.generate_run_to_tools())\r\n  File \"/home/FP/interq/ITP/.itp/lib/python3.6/site-packages/tensorboard_plugin_profile/profile_plugin.py\", line 693, in generate_run_to_tools\r\n    plugin_assets = self.multiplexer.PluginAssets(PLUGIN_NAME)\r\nAttributeError: 'NoneType' object has no attribute 'PluginAssets'\r\n```\r\n\r\n\r\ntensorboard==2.5.0\r\ntensorboard-data-server==0.6.1\r\ntensorboard-plugin-profile==2.4.0\r\ntensorboard-plugin-wit==1.8.0\r\ntensorflow==2.5.0\r\ntensorflow-estimator==2.5.0\r\n\r\n", "@rmothukuru Any progress yet?", "@japie1235813,\r\nThis issue has been assigned to the respective Engineer. We will revert soon. Thanks! ", "@sjtusmartboy, thanks for the report. @one-two-one-two, thanks for the helpful additional details.\r\n\r\nIn my case I have no problems with tensorflow==2.5.0, tensorboard==2.5.0 and tensorboard-plugin-profile==2.5.0.\r\n\r\nHowever, if I downgrade to tensorboard-plugin-profile==2.4.0, then I can reproduce the error you are seeing.\r\n\r\nIt does not surprise me that tensorboard-plugin-profile==2.4.0 is incompatible with newer versions of tensorboard since we have made some changes to plugin management this year.\r\n\r\nCould you try upgrading to tensorboard-plugin-profile==2.5.0 and let me know if the problem is resolved?", "Yes, problem solved.\r\nthanks a lot", "Yes, problem solved.\r\nthanks a lot", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50921\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50921\">No</a>\n"]}, {"number": 50920, "title": "XNNPACK enable cross-compilation failure using Bazel on r2.6 armhf or v2.6.0-rc1 armhf TensorFlow Lite pip", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 (HostPC), Ubuntu 16.04 (Docker)\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.6, v2.6.0-rc1\r\n- Python version: 3.7 (Docker)\r\n- Bazel version (if compiling from source): 3.7.2 (Docker)\r\n\r\n**Describe the problem**\r\nWhen I apply the changes [#50893](https://github.com/tensorflow/tensorflow/pull/50893) (Related issues #50826), the build with XNNPACK enabled on **`aarch64`** succeeds, but when I enable XNNPACK on **`armhf`**, I get XNNPACK build errors. This error is a result of a problem with the source of XNNPACK that was discovered as a result of #50826 being resolved by commit #50893.\r\n```bash\r\nSUBCOMMAND: # @XNNPACK//:neondot_ukernels [action 'Compiling XNNPACK/src/qs8-gemm/gen/4x8c4-minmax-fp32-neondot.c', configuration: 2e7b4c6d74b28315d87b125d5d5ce89c3c9e63c1078a7d749a0d0caf18244a66, execution platform: @local_execution_config_platform//:platform]\r\n(cd /home/xxxxx/work/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python3.7 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /home/xxxxx/work/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neondot_ukernels/0/4x8c4-minmax-fp32-neondot.pic.d '-frandom-seed=bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neondot_ukernels/0/4x8c4-minmax-fp32-neondot.pic.o' -fPIC -DPTHREADPOOL_NO_DEPRECATED_API -iquote external/XNNPACK -iquote bazel-out/armhf-opt/bin/external/XNNPACK -iquote external/FP16 -iquote bazel-out/armhf-opt/bin/external/FP16 -iquote external/pthreadpool -iquote bazel-out/armhf-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/armhf-opt/bin/external/FXdiv -iquote external/cpuinfo -iquote bazel-out/armhf-opt/bin/external/cpuinfo -iquote external/clog -iquote bazel-out/armhf-opt/bin/external/clog -Ibazel-out/armhf-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/armhf-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/armhf-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/armhf-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/armhf-opt/bin/external/clog/_virtual_includes/clog -isystem external/XNNPACK/include -isystem bazel-out/armhf-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/armhf-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/armhf-opt/bin/external/FP16/include -isystem external/pthreadpool/include -isystem bazel-out/armhf-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/armhf-opt/bin/external/FXdiv/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=armv7-a' '-mfpu=neon-vfpv4' -O3 -fno-tree-pre -fpermissive -Iinclude -Isrc -marm '-march=armv8.2-a+dotprod' '-mfpu=neon-fp-armv8' '-std=c99' -O2 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/XNNPACK/src/qs8-gemm/gen/4x8c4-minmax-fp32-neondot.c -o bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neondot_ukernels/0/4x8c4-minmax-fp32-neondot.pic.o)\r\nERROR: /home/xxxxx/work/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/XNNPACK/BUILD.bazel:4507:19: C++ compilation of rule '@XNNPACK//:neondot_ukernels' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command \r\n  (cd /home/xxxxx/work/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    LD_LIBRARY_PATH='' \\\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/local/bin/python3.7 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /home/xxxxx/work/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/armhf_linux_toolchain/bin/arm-linux-gnueabihf-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neondot_ukernels/0/1x8c4-minmax-fp32-neondot.pic.d '-frandom-seed=bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neondot_ukernels/0/1x8c4-minmax-fp32-neondot.pic.o' -fPIC -DPTHREADPOOL_NO_DEPRECATED_API -iquote external/XNNPACK -iquote bazel-out/armhf-opt/bin/external/XNNPACK -iquote external/FP16 -iquote bazel-out/armhf-opt/bin/external/FP16 -iquote external/pthreadpool -iquote bazel-out/armhf-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/armhf-opt/bin/external/FXdiv -iquote external/cpuinfo -iquote bazel-out/armhf-opt/bin/external/cpuinfo -iquote external/clog -iquote bazel-out/armhf-opt/bin/external/clog -Ibazel-out/armhf-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/armhf-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/armhf-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/armhf-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/armhf-opt/bin/external/clog/_virtual_includes/clog -isystem external/XNNPACK/include -isystem bazel-out/armhf-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/armhf-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/armhf-opt/bin/external/FP16/include -isystem external/pthreadpool/include -isystem bazel-out/armhf-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/armhf-opt/bin/external/FXdiv/include -w -DAUTOLOAD_DYNAMIC_KERNELS '-march=armv7-a' '-mfpu=neon-vfpv4' -O3 -fno-tree-pre -fpermissive -Iinclude -Isrc -marm '-march=armv8.2-a+dotprod' '-mfpu=neon-fp-armv8' '-std=c99' -O2 -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -no-canonical-prefixes -fno-canonical-system-headers -c external/XNNPACK/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c -o bazel-out/armhf-opt/bin/external/XNNPACK/_objs/neondot_ukernels/0/1x8c4-minmax-fp32-neondot.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\nexternal/XNNPACK/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c: In function 'xnn_qs8_gemm_minmax_fp32_ukernel_1x8c4__neondot':\r\nexternal/XNNPACK/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c:93:16: error: incompatible types when assigning to type 'int32x4_t' from type 'int'\r\n     vacc0x0123 = vcvtnq_s32_f32(vproduct0x0123);\r\n                ^\r\nexternal/XNNPACK/src/qs8-gemm/gen/1x8c4-minmax-fp32-neondot.c:94:16: error: incompatible types when assigning to type 'int32x4_t' from type 'int'\r\n     vacc0x4567 = vcvtnq_s32_f32(vproduct0x4567);\r\n                ^\r\nTarget //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build\r\nINFO: Elapsed time: 1268.607s, Critical Path: 183.02s\r\nINFO: 9070 processes: 1642 internal, 7428 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```bash\r\n$ git clone -b r2.6 https://github.com/tensorflow/tensorflow.git\r\n$ cd tensorflow\r\n```\r\nEdit `tensorflow/tensorflow/.bazelrc`\r\n```\r\n# TFLite build configs for generic embedded Linux\r\nbuild:elinux --crosstool_top=@local_config_embedded_arm//:toolchain\r\nbuild:elinux --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\r\nbuild:elinux_aarch64 --config=elinux\r\nbuild:elinux_aarch64 --cpu=aarch64\r\nbuild:elinux_aarch64 --distinct_host_configuration=true\r\nbuild:elinux_armhf --config=elinux\r\nbuild:elinux_armhf --cpu=armhf\r\nbuild:elinux_armhf --distinct_host_configuration=true\r\n```\r\nAdded FlexDelegate and XNNPACK as build options.\r\nEdit `tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh`\r\n```sh\r\n# Build python interpreter_wrapper.\r\ncd \"${BUILD_DIR}\"\r\ncase \"${TENSORFLOW_TARGET}\" in\r\n  armhf)\r\n    BAZEL_FLAGS=\"--config=elinux_armhf\r\n      --copt=-march=armv7-a --copt=-mfpu=neon-vfpv4\r\n      --copt=-O3 --copt=-fno-tree-pre --copt=-fpermissive\r\n      --define tensorflow_mkldnn_contraction_kernel=0\r\n      --define=raspberry_pi_with_neon=true\r\n      --define=tflite_pip_with_flex=true\r\n      --define=tflite_with_xnnpack=true\"\r\n    ;;\r\n  aarch64)\r\n    BAZEL_FLAGS=\"--config=elinux_aarch64\r\n      --define tensorflow_mkldnn_contraction_kernel=0\r\n      --define=tflite_pip_with_flex=true\r\n      --define=tflite_with_xnnpack=true\r\n      --copt=-O3\"\r\n    ;;\r\n  native)\r\n    BAZEL_FLAGS=\"--copt=-O3 --copt=-march=native\r\n      --define=tflite_pip_with_flex=true\r\n      --define=tflite_with_xnnpack=true\"\r\n    ;;\r\n  *)\r\n    BAZEL_FLAGS=\"--copt=-O3\r\n      --define=tflite_pip_with_flex=true\r\n      --define=tflite_with_xnnpack=true\"\r\n    ;;\r\nesac\r\n```\r\nBuilding **`armhf`** with cross-compilation.\r\n```bash\r\nsudo CI_DOCKER_EXTRA_PARAMS=\"-e CI_BUILD_PYTHON=python3.7 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.7\" \\\r\n  tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \\\r\n  tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh armhf\r\n```\r\n\r\n**Any other info / logs**\r\nI found that if I change the commit hash of XNNPACK as follows, the build finishes successfully on both armhf and aarch64. I don't know if this will help, but I have issued a pull request. https://github.com/tensorflow/tensorflow/pull/50923\r\n\r\nXNNPACK https://github.com/google/XNNPACK/commit/476eb84d6a8e6f8249d5584d30759c6fbdbf791d\r\n\r\nEdit `tensorflow/workspace2.bzl`\r\n\r\n**From:**\r\n```bzl\r\n    tf_http_archive(\r\n        name = \"XNNPACK\",\r\n        sha256 = \"7320355409ae5dd2c8600cafbd07b56c379cd13666a7c971ffd3a01025c0f63e\",\r\n        strip_prefix = \"XNNPACK-56b78a03e359ac04a3ba758596cd28b198a8000f\",\r\n        urls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/56b78a03e359ac04a3ba758596cd28b198a8000f.zip\",\r\n            \"https://github.com/google/XNNPACK/archive/56b78a03e359ac04a3ba758596cd28b198a8000f.zip\",\r\n        ],\r\n    )\r\n```\r\n\r\n**To:**\r\n```bzl\r\n    tf_http_archive(\r\n        name = \"XNNPACK\",\r\n        sha256 = \"e1fee5a16e4a06d3bd77ab33cf87b1c6d826715906248a308ab790486198d3c9\",\r\n        strip_prefix = \"XNNPACK-476eb84d6a8e6f8249d5584d30759c6fbdbf791d\",\r\n        urls = [\r\n            \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/476eb84d6a8e6f8249d5584d30759c6fbdbf791d.zip\",\r\n            \"https://github.com/google/XNNPACK/archive/476eb84d6a8e6f8249d5584d30759c6fbdbf791d.zip\",\r\n        ],\r\n    )\r\n```", "comments": ["Hi Terry / Chao,\r\n\r\nCould you have a look at this build issue related to XNNPACK ? \r\n\r\nThanks,\r\nTiezhen", "The pull request has been rejected and will be closed.\r\nhttps://github.com/tensorflow/tensorflow/pull/50923", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50920\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50920\">No</a>\n"]}, {"number": 50919, "title": "Are these two lines of code okay? It may cause compilation errors", "body": "https://github.com/tensorflow/tensorflow/blob/b61c987109eaad755c2b36ba51a0b233d54d4e57/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/fusion_utils.cc#L203\r\nhttps://github.com/tensorflow/tensorflow/blob/b61c987109eaad755c2b36ba51a0b233d54d4e57/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/fusion_utils.cc#L204\r\n@wyzero \r\n\r\ntensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/fusion_utils.cc:203:27: error: 'class mlir::Operation' has no member named 'region'\r\n   for (Operation& op : op.region().getBlocks().front()) {\r\n", "comments": ["@jasonjx-w ,\r\n\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled and also the tensorflow version you are using, could you please do so as it helps us analyse the issue.Thanks!", "\r\n![image](https://user-images.githubusercontent.com/27774893/154613430-d9c61388-5554-4b33-8c38-d007957b9044.png)\r\n\r\nI have the same problem when i use GCC 5.4.0. Maybe use GCC 7.3.1 can solve this problem.", "@crischeng thx\uff01"]}, {"number": 50918, "title": "TensorFlow Import Error", "body": "### System information\r\nIntel 8th gen i7 with integrated graphics\r\n\r\nusing stock script, import tensorflow as tf caused failure\r\n\r\nWIndows 10\r\nTensorflow installed from pip, version 2.0.0 (also tried 2.5.0, same error)\r\nPython Version 3.7.9\r\nNo CUDA and GPU\r\n\r\nExact command to reproduce: import tensorflow as tf\r\n\r\n\r\n### Describe the problem\r\nThere is a problem to import tensorflow 2.0.0 using Python 3.7. Installation appears to have worked on pip with all the requirements satisfied, but the import command results in an OSError: [WinError 126]. **Any help is greatly appreciated, thank you so much in advance**\r\n\r\n### Source code / logs\r\nSource Code: ``` import tensorflow as tf```\r\nError: \r\n```\r\nC:\\Users\\steven.f>python3\r\nPython 3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 16:30:00) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\nimport tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\__init__.py\", line 98, in <module>\r\n    from tensorflow_core import *\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 959, in _find_and_load_unlocked\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\r\n    module = self._load()\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\r\n    module = _importlib.import_module(self.__name__)\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2544.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\__init__.py\", line 83, in <module>\r\n    from tensorflow.python import keras\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 26, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\keras\\__init__.py\", line 26, in <module>\r\n    from tensorflow.python.keras import activations\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\keras\\activations.py\", line 23, in <module>\r\n    from tensorflow.python.keras.utils.generic_utils import deserialize_keras_object\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\keras\\utils\\__init__.py\", line 38, in <module>\r\n    from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\keras\\utils\\multi_gpu_utils.py\", line 22, in <module>\r\n    from tensorflow.python.keras.engine.training import Model\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 47, in <module>\r\n    from tensorflow.python.keras.engine import training_arrays\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\", line 41, in <module>\r\n    from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\scipy\\__init__.py\", line 136, in <module>\r\n    from . import _distributor_init\r\n  File \"C:\\Users\\steven.f\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\scipy\\_distributor_init.py\", line 61, in <module>\r\n    WinDLL(os.path.abspath(filename))\r\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.7_3.7.2544.0_x64__qbz5n2kfra8p0\\lib\\ctypes\\__init__.py\", line 364, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 126] The specified module could not be found\r\n```\r\n", "comments": ["@stevenf7 Regarding the `OSError`, please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have all the compatible software installed. Also, make sure that you have installed Python from [python.org](https://www.python.org/downloads/windows/) and not from the Microsoft Store. \r\n\r\nAlso, try to uninstall and then re-nstall the  tensorflow package  that may fix the issue. Thanks!", "Thank you so for your response! I uninstalled that version of python from the Microsoft store and reinstalled python 3.9 from python.org and that worked!\r\n\r\nThanks again!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50918\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/50918\">No</a>\n"]}, {"number": 50917, "title": "Fix unchecked int32 indexing in some GPU kernels", "body": "- Many op kernels use the `To32Bit()` function, which does not do any bounds checking and can cause silent corruption due to int32 overflow.\r\n  E.g., `tf.zeros/ones/fill` silently give incorrect results when the number of elements is >= 2**31.\r\n- This commit adds a new helper function `MaybeWith32BitIndexing()` that checks the indexing bounds and converts tensors to 32-bit only if it is safe to do so (and only for GPU devices). This fixes the cases that were previously broken, and simplifies the ones that already had explicit bounds checks.\r\n\r\ncc @nluehr @sanjoy ", "comments": ["> We have an ongoing engagement with @kushanam and @sherhut to do this without bloating the binary size by JIT compiling the rarely used codepaths. So I would rather invest into that effort unless we've already judged the JIT-based approach to be infeasible.\r\n\r\nOK I was wondering about that. If the timeline is not too long then that would be the way to go.", "My take on this is that we can close it if the new JIT kernels work will be enabled before the release of 2.7.\r\nIf that won't make 2.7, then we should either merge this PR or modify it to simply throw a fatal error when 32-bit indexing is insufficient.\r\n\r\nFor reference, I checked the effect of this PR on binary size (_pywrap_tensorflow_internal.so) in my build:\r\nBefore: 547MB\r\nAfter:  566MB (+3.5%)", "We should land this change independently of the kernel generator work, as it also covers other operations (like reductions) that are not currently covered by kernel generator.\r\n\r\nI suspect that the (relatively) small change in size is due to us not generating all the cwise operations via the eigen code path anymore, so the generation of two kernels due to `cwise_cpu_common` does not hit us."]}, {"number": 50916, "title": "[PluggableDevice] fix While segment fault on PluggableDevice", "body": "While op will  cause a segment fault when executing on PluggableDevice because of invalid memory copy operation.", "comments": ["Can you write a test for the issue?", "@allenlavoie \r\n> Can you write a test for the issue?\r\n\r\n1. I didn't see any tests for PluggableDevice, because we may need a seprated dynamic library to simulate a registeration of PluggableDevice. Do you have any suggestion about this path?\r\n2. I'm trying to register another `DeviceFactory` which uses `GPUDeviceFactory` with `is_pluggable_device` flag enabled. Then I will use benchmarks for gpu kernels to test it. Do you have any suggestions? And benchmarks seems not work even for other existing tests. Is there any problems about my usage? Thanks!\r\n\r\nregister `PLUG` device\r\n``` c++\r\nstatic int PluggableDeviceFactoryForTest = [](){ \r\n       DeviceFactory::Register(\"PLUG\", new GPUDeviceFactory(), 50, /*is_pluggable_device*/ true);\r\n       return 0;\r\n}();\r\n``` \r\nbenckmark test command\r\n``` shell\r\nbazel run --config=opt --config=cuda  --verbose_failures --javabase=@bazel_tools//tools/jdk:remote_jdk11 --cache_test_results=no //tensorflow/core/kernels:functional_ops_test_gpu  -- --benchmarks=..\r\n```", "@allenlavoie @penpornk Fixed and add a new UT. Please review."]}]