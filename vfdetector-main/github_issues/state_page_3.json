[{"number": 55575, "title": "Unwanted log messages and crash/force termination", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: Yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 8.5\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**: No\r\n-   **TensorFlow installed from (source or binary)**: Binary\r\n-   **TensorFlow version (use command below)**: 2.5.3\r\n-   **Python version**: 3.8.8\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**: CUDA 11.2.2\r\n-   **GPU model and memory**:  P100\r\n\r\n### Describe the problem\r\nWhile running a model training using `.fit` function using a tf.data.Dataset we observe a lot of prints of the statement `Cleanup called...`. Example:\r\n```\r\nCleanup called...\r\nCleanup called...\r\nCleanup called...\r\nCleanup called...\r\nCleanup called...\r\nCleanup called...\r\nCleanup called...\r\n```\r\nFollowed by Python code termination with an error code:\r\n`Aborted (core dumped) python run.py`\r\n\r\nThis problem occurs with TF 2.5.3. The same code runs perfectly with TF 2.5.2.\r\nEven though the Python interpreter errors out with the core dumped message, actual model training works fine and the model also gets saved.\r\n\r\nWe use the function `tf.io.decode_image` in the tf.data pipeline which seems to be causing these prints.", "comments": ["@prabalakanti ,\r\nIn order to expedite the trouble-shooting process, I request you, could you please provide a complete code you are using.\r\n\r\nAlso there is a high possibility that this was fixed with stable TF versions. Perhaps you can try to test your code in latest stable tf version 2.8 for your case. Thanks!\r\n\r\n", "@tilakrayal Unfortunately it is not possible for me to share the complete code. Based on our analysis the log message is coming from: https://github.com/tensorflow/tensorflow/blob/959e9b2a0c06df945f9fb66bd367af8832ca0d28/tensorflow/core/kernels/image/decode_image_op.cc#L337\r\n\r\nThis seems to have been removed in the latest TF version 2.8: https://github.com/tensorflow/tensorflow/commit/09dab4f0e0f3785c3b764b64b476831e529bb139\r\n\r\nAn upgrade to TF 2.8 is not immediately feasible for us as we have to update the code for TF2.5 to TF2.8 upgrade.\r\n\r\nIs there any alternative way to fix this in TF2.5.3 by some flag/param to skip these log messages?", "Code snippets from the tf.data pipeline:\r\n\r\n```python\r\ndef get_images_dataset(data_dir, num_labels, prefix=\"train\"):\r\n    tfrecord_files = tf.data.Dataset.list_files(\r\n        os.path.join(data_dir, f\"{prefix}-*.tfrecord\"))\r\n\r\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_files)\r\n\r\n    features = {\r\n        \"image/height\": tf.io.FixedLenFeature([], tf.int64),\r\n        \"image/width\": tf.io.FixedLenFeature([], tf.int64),\r\n        \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\r\n        \"image/labels\": tf.io.FixedLenFeature([num_labels], tf.int64)\r\n    }\r\n\r\n    def _parse_example(example):\r\n        parsed_tensors = tf.io.parse_single_example(example, features)\r\n        image = tf.io.decode_image(\r\n            parsed_tensors[\"image/encoded\"], channels=3, expand_animations=False)\r\n        image.set_shape([None, None, 3])\r\n        label_vector = parsed_tensors[\"image/labels\"]\r\n        return image, label_vector\r\n\r\n    dataset = raw_dataset.map(\r\n        _parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n\r\n    return dataset\r\n```\r\n\r\nAdditionally we run these functions on top of the created dataset:\r\n```\r\ndataset = dataset.cache()\r\ndataset = dataset.shuffle(buffer_size=num_images)\r\ndataset = dataset.repeat()\r\ndataset = dataset.batch(batch_size)\r\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\r\n```\r\n\r\nTraining is run using the `model.fit` function of tf.keras:\r\n```\r\nmodel.fit(\r\n            train_ds,\r\n            steps_per_epoch=steps_per_epoch,\r\n            epochs=epochs,\r\n            validation_data=valid_ds,\r\n            validation_steps=validation_steps,\r\n            callbacks=cbks,\r\n            verbose=1,\r\n        )\r\n```\r\n\r\n", "@prabalakanti ,\r\nOn running the given code snippet, I am facing an error stating `NameError: name 'dataset' is not defined`'. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/71a906fec4b5da800f380b34a07be7bb/untitled305.ipynb) and provide the complete code to reproduce the issue.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55574, "title": "Difference in Inference result in JAVA and Swift using same TFLite Model", "body": "I have trained a Mobile Bert Custom TFLite model for Text Classification and the model works as expected in Android. The output values match the expected values from running the TFLite model in Python. However, while running the same model in iOS with same input, I am getting different results. \r\n\r\nI am new to iOS, this is the first time I'm working on iOS so my gut feeling is that I'm doing something wrong. The only clue I currently have is the slight difference in my Java Code and Swift Code to extract the result from Model Output.\r\n\r\nJava\r\n\r\n`float[][][] bert_output = new float[1][MAX_SEQ_LEN][BERT_HIDDEN_LAYERS];` \r\n`bert_model.run(inputIds, bert_output); `\r\ninputIds here is an Int Array\r\nIn Java I directly call **run** in the interpreter and pass the Input in inputIds as well as the output array object and at the end of it I get a populated Output array in bert_output.\r\n\r\nSwift\r\n\r\nIn Swift the inference seems to work a little different. You have to convert Your input(Int Array) to Tensor format and also fetch the output in Tensor format and convert it back to Float array. I feel this is the part where things are going wrong but no way to confirm.\r\n\r\nConvert Input into Tensor\r\n`let input_tensor = inputIds.withUnsafeBufferPointer(Data.init)`\r\nCopy Input to Model Input Layer 0\r\n`try interpreter_bert?.copy(input, toInputAt: 0)`\r\nInference\r\n`try interpreter_bert?.invoke()`\r\nCopy Output Tensor from Output Layer 0\r\n`let temp_res = try interpreter_bert?.output(at: 0)`\r\nConvert Output Tensor Data to Float Array\r\n`let mResult = BertHelper.TensorToArray<Float32>(tensor: temp_res!)`\r\nYou can find the Conversion code [here](https://pastebin.com/nThQw730)\r\n\r\nAfter the Conversion, the Result Shape matches the Java result and Expected result but the probability values don't match up. \r\n\r\nIt would be a big help if someone here could help me. Tensorflow documentation and examples where Model Interpreter is directly called, only showcase Models which work on images and the NLP examples use TensorflowTaskLibrary, hence putting my issue up here.", "comments": []}, {"number": 55572, "title": "Tensorflow Lite build for Android results in issue - class file not found", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: master repo / nightly repo\r\n- Python version: 3.7.3\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 5.1.1\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n- CUDA/cuDNN version: not using cuda\r\n- GPU model and memory: not using GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI am trying to use custom-built tensorflow lite AAR on model personalization example of official tensorflow repo from the [link](https://github.com/tensorflow/examples/tree/master/lite/examples/model_personalization)\r\n\r\nBefore I change anything from the master repository, I followed the steps in this [link](https://www.tensorflow.org/lite/guide/build_android) to build Tensorflow Lite AAR with bazel.\r\n\r\nActually the bazel build did not flawlessly work at once; I had to change the visibility option of 'op_resolver_internal' to public in tensorflow/lite/core/api/BUILD to successfully build the AAR.\r\n\r\nAnyways, when I substituted the tensorflow-lite api with my built AARs, I get the following error:\r\n\r\nError 1:\r\nandroid\\transfer_api\\src\\main\\java\\org\\tensorflow\\lite\\examples\\transfer\\api\\LiteMultipleSignatureModel.java:57: error: cannot access InterpreterApi\r\n    this.interpreter.runSignature(inputs, outputs, \"load\");\r\n                    ^\r\n  class file for org.tensorflow.lite.InterpreterApi not found\r\n\r\nError 2:\r\nandroid\\transfer_api\\src\\main\\java\\org\\tensorflow\\lite\\examples\\transfer\\api\\LiteMultipleSignatureModel.java:105: error: cannot access Tensor\r\n    return this.interpreter.getInputTensorFromSignature(\"bottleneck\", \"train\").shape()[1];\r\n                                                       ^\r\n  class file for org.tensorflow.lite.Tensor not found\r\n\r\nWhy does this happen?? Is there anything wrong with the current bazel build?\r\n\r\nI also built the tensorflow-lite-select-tf-ops.aar and included that in build.gradle.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nDescribed above\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Hi @jaemin-shin ! Could you please share your command on bazel build  method too?", "**Hello! Here is the bazel command that I used:**\r\n\r\nbazel build -c opt --fat_apk_cpu=arm64-v8a  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tensorflow/lite/java:tensorflow-lite\r\n\r\n**For ./configure in the tensorflow root directory, I configured as follows:**\r\n\r\n(base) jmshin@ubuntu:~/tensorflow > ./configure\r\nYou have bazel 5.1.1 installed.\r\nPlease specify the location of python. [Default is /home/jmshin/miniconda3/bin/python3]:\r\n\r\n\r\nFound possible Python library paths:\r\n  /home/jmshin/miniconda3/lib/python3.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/jmshin/miniconda3/lib/python3.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: -O3\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y\r\nSearching for NDK and SDK installations.\r\n\r\nPlease specify the home path of the Android NDK to use. [Default is /home/jmshin/Android/Sdk/ndk-bundle]: /home/jmshin/Android/Sdk/ndk/19.2.5345600\r\n\r\n\r\nPlease specify the (min) Android NDK API level to use. [Available levels: ['16', '17', '18', '19', '21', '22', '23', '24', '26', '27', '28']] [Default is 21]:\r\n\r\n\r\nPlease specify the home path of the Android SDK to use. [Default is /home/jmshin/Android/Sdk]:\r\n\r\n\r\nPlease specify the Android SDK API level to use. [Available levels: ['32']] [Default is 32]:\r\n\r\n\r\nPlease specify an Android build tools version to use. [Available versions: ['30.0.3', '32.0.0', '32.1.0-rc1']] [Default is 32.1.0-rc1]: 30.0.3\r\n\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished", "Ok @jaemin-shin ! I think it is [missing a dependency](https://stackoverflow.com/questions/66880515/cannot-access-tensorflow-lite-support-tensorbuffer)  of lite support in the build.gradle file.  \r\n\r\nDid you try after adding aar files [to the project ](https://www.tensorflow.org/lite/guide/build_android#add_aar_directly_to_project)and [local maven repository](https://www.tensorflow.org/lite/guide/build_android#install_aar_to_local_maven_repository)?\r\n\r\n If that does not work , please try again after adding more architecture in bazel config and let us know the results.\r\n \r\n` bazel build -c opt --fat_apk_cpu=arm64-v8a armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tensorflow/lite/java:tensorflow-lite`\r\n\r\nThanks!\r\n", "Dear @mohantym,\r\n\r\nThank you for the suggestion! Unfortunately, it all did not work; I tried adding them by aar file, or adding them through local maven repository, and repeated all process with adding more architecture in bazel config. All was out of luck.\r\n\r\nI think the aar is well recognized by the Android Studio, since it does not show on error on\r\n`import org.tensorflow.lite.Interpreter;`\r\n\r\nbut shows error on followings:\r\n\r\nError 1:\r\nandroid\\transfer_api\\src\\main\\java\\org\\tensorflow\\lite\\examples\\transfer\\api\\LiteMultipleSignatureModel.java:57: error: cannot access InterpreterApi\r\nthis.interpreter.runSignature(inputs, outputs, \"load\");\r\n^\r\nclass file for org.tensorflow.lite.InterpreterApi not found\r\n\r\nError 2:\r\nandroid\\transfer_api\\src\\main\\java\\org\\tensorflow\\lite\\examples\\transfer\\api\\LiteMultipleSignatureModel.java:105: error: cannot access Tensor\r\nreturn this.interpreter.getInputTensorFromSignature(\"bottleneck\", \"train\").shape()[1];\r\n^\r\nclass file for org.tensorflow.lite.Tensor not found\r\n\r\n\r\nI also tried bazel build on release branch r2.8 as they should be stable,\r\nbut the same error occurred...", "Hi @chunduriv ! Could you please look at this issue?", "Hi @chunduriv! May I ask if there's any update on this?! Have a nice day :) "]}, {"number": 55569, "title": "Illegal Instruction", "body": "I literally just imported the library........\r\nthat's it, I import it and it errors. A brutal re-welcoming and a goodbye to this library.\r\n\r\nUbuntu\r\nI was using Python.\r\nI'm on an Intel laptop. ", "comments": ["Please fill in issue template and give details about your scenario. Minimal reproducer, error you saw, what you installed, operating system.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55566, "title": "Suppress the warning in rootless mode containers", "body": "This is going to change the confusing feedback we give to users when they are working with our images in rootless mode.", "comments": ["What do you need here?", "@angerson Can you please review this PR ? Thank you!", "Can you explain this more in the file, please? The comment should explain what rootless mode is and why this check works.", "> Can you explain this more in the file, please? The comment should explain what rootless mode is and why this check works.\r\n\r\nLet me know exactly what string you want and I will put it there.\r\n\r\nRootless is: Run the Docker daemon as a non-root user as in the official Docker terminology https://docs.docker.com/engine/security/rootless/. \r\n\r\nIf you are running Docker in rootless mode I suppose what it means.\r\n\r\nThe check it is explained as on the host (initial user namespace)  in `/proc/self/uid_map` we have `4294967295`. This is the same approach as referenced and used in `Kubernetes/kind`\r\n\r\n"]}, {"number": 55564, "title": "Tensorflow ERROR - Testing a linear regression model using TensorFlow", "body": "I'm trying to resolve an error in tensoflow but I can't understand what the error is. I can't understand what the type [('resource', 'u1')] is and what that means.\r\nI am studying the book Mastering OpenCV 4 with Python by the author Alberto Fernandez Villan, but the code is outdated, I have already updated everything according to version 2.8 of tensorflow, but now this error appears. can you help me? Here is the complete code and the error.\r\n\r\nnumpy version: 1.22.3\r\ntensorflow version: 2.8.0\r\n\r\n-------------------------------------------------------------------------------------\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n# Number of points:\r\nN = 50\r\n\r\n# Make random numbers predictable:\r\nnp.random.seed(101)\r\ntf.compat.v1.set_random_seed(101)\r\n\r\n# Generate random data composed by 50 (N = 50) points:\r\nx = np.linspace(0, N, N) #float64\r\ny = 3 * np.linspace(0, N, N) + np.random.uniform(-10, 10, N) #float64\r\n\r\n# Number of points to predict:\r\nM = 3\r\n\r\n# Define 'M' more points to get the predictions using the trained model:\r\nnew_x = np.linspace(N + 1, N + 10, M) #float64\r\n\r\n\r\n# Restore the model.\r\n# First step when loading a model is to load the graph from '.meta':\r\ntf.compat.v1.reset_default_graph()\r\nimported_meta = tf.compat.v1.train.import_meta_graph(\"linear_regression.meta\")\r\n\r\n# The second step when loading a model is to load the values of the variables:\r\n# Note that values only exist within a session\r\nwith tf.compat.v1.Session() as sess:\r\n    imported_meta.restore(sess, './linear_regression')\r\n    # Run the model to get the values of the variables W, b and new prediction values:\r\n    W_estimated = sess.run('W:0')\r\n    b_estimated = sess.run('b:0')\r\n    new_predictions = sess.run(['y_model:0'], {'X:0': new_x}) #[array([153.04472, 166.54755, 180.05037], dtype=float32)]\r\n    print(W_estimated.dtype)\r\n    print(b_estimated.dtype)\r\n\r\n# Reshape for proper visualization:\r\nnew_predictions = np.reshape(new_predictions, (M, -1)) #float32\r\n\r\n# Calculate the predictions:\r\npredictions = W_estimated * x + b_estimated\r\n\r\n# Create the dimensions of the figure and set title:\r\nfig = plt.figure(figsize=(12, 5))\r\nplt.suptitle(\"Linear regression using TensorFlow\", fontsize=14, fontweight='bold')\r\nfig.patch.set_facecolor('silver')\r\n\r\n# Plot training data:\r\nplt.subplot(1, 3, 1)\r\nplt.plot(x, y, 'ro', label='Original data')\r\nplt.xlabel('x')\r\nplt.ylabel('y')\r\nplt.title(\"Training Data\")\r\nplt.legend()\r\n\r\n# Plot results:\r\nplt.subplot(1, 3, 2)\r\nplt.plot(x, y, 'ro', label='Original data')\r\nplt.plot(x, predictions, label='Fitted line')\r\nplt.xlabel('x')\r\nplt.ylabel('y')\r\nplt.title('Linear Regression Result')\r\nplt.legend()\r\n\r\n# Plot new predicted data:\r\nplt.subplot(1, 3, 3)\r\nplt.plot(x, y, 'ro', label='Original data')\r\nplt.plot(x, predictions, label='Fitted line')\r\nplt.plot(new_x, new_predictions, 'bo', label='New predicted data')\r\nplt.xlabel('x')\r\nplt.ylabel('y')\r\nplt.title('Predicting new points')\r\nplt.legend()\r\n\r\n# Show the Figure:\r\nplt.show()\r\n\r\n\r\n---------------------------------------------------------------------------------\r\nERROR: line 51, in <module>predictions = W_estimated * x + b_estimated\r\nnumpy.core._exceptions.UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype([('resource', 'u1')]), dtype('float64')) -> None", "comments": ["Updated training code that works.\r\nRequired for the test code that shows an error to work.\r\n----------------------------------------------------------------------------------\r\n# Training a linear regression model using TensorFlow\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\ntf.compat.v1.disable_eager_execution() # sugest\u00e3o encontrada na internet para evitar erro\r\n\r\n# Path to the folder that we want to save the logs for Tensorboard:\r\nlogs_path = \"./logs\"\r\n# Number of points:\r\nN = 50\r\n\r\n# Make random numbers predictable:\r\nnp.random.seed(101)\r\ntf.compat.v1.set_random_seed(101)\r\n\r\n# Generate random data composed by 50 (N = 50) points:\r\nx = np.linspace(0, N, N)\r\ny = 3 * np.linspace(0, N, N) + np.random.uniform(-10, 10, N)\r\n\r\n# You can check the shape of the created training data:\r\nprint(x.shape)\r\nprint(y.shape)\r\n\r\n# Create the placeholders in order to feed our training examples into the optimizer while training:\r\nX = tf.compat.v1.placeholder(\"float\", name='X')\r\nY = tf.compat.v1.placeholder(\"float\", name='Y')\r\n\r\n# Declare two trainable TensorFlow Variables for the Weights and Bias\r\n# We are going to initialize them randomly. Another way can be to set '0.0':\r\nW = tf.Variable(np.random.randn(), name=\"W\")\r\nb = tf.Variable(np.random.randn(), name=\"b\")\r\n\r\n# Define the hyperparameters of the model:\r\nlearning_rate = 0.01\r\ntraining_epochs = 1000\r\n\r\n# This will be used to show results after every 25 epochs:\r\ndisp_step = 100\r\n\r\n# Construct a linear model:\r\ny_model = tf.add(tf.multiply(X, W), b, name=\"y_model\")\r\n\r\n# Define cost function, in this case, the Mean squared error\r\n# (Note that other cost functions can be defined)\r\ncost = tf.reduce_sum(tf.pow(y_model - Y, 2)) / (2 * N)\r\n\r\n# Create the gradient descent optimizer that is going to minimize the cost function modifying the\r\n# values of the variables W and b:\r\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(cost)\r\n\r\n# Initialize all variables:\r\ninit = tf.compat.v1.global_variables_initializer()\r\n\r\n# Create a Saver object:\r\nsaver = tf.compat.v1.train.Saver()\r\n\r\n# Start the training procedure inside a TensorFlow Session:\r\nwith tf.compat.v1.Session() as sess:\r\n    # Run the initializer:\r\n    sess.run(init)\r\n\r\n    # Uncomment if you want to see the created graph\r\n    # summary_writer = tf.summary.FileWriter(logs_path, sess.graph)\r\n\r\n    # Iterate over all defined epochs:\r\n    for epoch in range(training_epochs):\r\n\r\n        # Feed each training data point into the optimizer:\r\n        for (_x, _y) in zip(x, y):\r\n            sess.run(optimizer, feed_dict={X: _x, Y: _y})\r\n\r\n        # Display the results every 'display_step' epochs:\r\n        if (epoch + 1) % disp_step == 0:\r\n            # Calculate the actual cost, W and b:\r\n            c = sess.run(cost, feed_dict={X: x, Y: y})\r\n            w_est = sess.run(W)\r\n            b_est = sess.run(b)\r\n            print(\"epoch {}: cost = {} W = {}  b = {}\".format(epoch + 1, c, w_est, b_est))\r\n\r\n    # Save the final model\r\n    saver.save(sess, './linear_regression')\r\n\r\n    # Storing necessary values to be used outside the session\r\n    training_cost = sess.run(cost, feed_dict={X: x, Y: y})\r\n    weight = sess.run(W)\r\n    bias = sess.run(b)\r\n\r\nprint(\"Training finished!\")\r\n\r\n# Calculate the predictions:\r\npredictions = weight * x + bias\r\n\r\n# Create the dimensions of the figure and set title:\r\nfig = plt.figure(figsize=(8, 5))\r\nplt.suptitle(\"Linear regression using TensorFlow\", fontsize=14, fontweight='bold')\r\nfig.patch.set_facecolor('silver')\r\n\r\n# Plot training data:\r\nplt.subplot(1, 2, 1)\r\nplt.plot(x, y, 'ro', label='Original data')\r\nplt.xlabel('x')\r\nplt.ylabel('y')\r\nplt.title(\"Training Data\")\r\n\r\n# Plot results:\r\nplt.subplot(1, 2, 2)\r\nplt.plot(x, y, 'ro', label='Original data')\r\nplt.plot(x, predictions, label='Fitted line')\r\nplt.xlabel('x')\r\nplt.ylabel('y')\r\nplt.title('Linear Regression Result')\r\nplt.legend()\r\n\r\n# Show the Figure:\r\nplt.show()\r\n", "Hello @NathFarinha237 ,\r\nOn running the given code snippet, I am facing an error stating `OSError: File does not exist. Received: {filename}`. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/fb61ab0f893d14efb33aa26335ed11ea/untitled297.ipynb).\r\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55561, "title": "Fix some typos.", "body": null, "comments": ["Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nFor more information, open the [CLA check for this pull request](https://github.com/tensorflow/tensorflow/pull/55561/checks?check_run_id=5954961333).", "> \r\n\r\nThanks for pointing out the error, it has now been fixed."]}, {"number": 55560, "title": "Question: how to implement tf.gather with some other tf ops ?", "body": "I am trying to implement tf.gather with some simple and common tf ops, because tf.gather is not supported in the ncnn deployment framework currently, for now I can only think of implementing it with numpy:\r\n```\r\nimport tensorflow as tf \r\n\r\nindices = [3, 5, 2]\r\n\r\nparams = tf.constant([[[11, 12, 13], \r\n                    [14, 15, 16], \r\n                    [17, 18, 19],\r\n                    [21, 22, 23],\r\n                    [24, 25, 26], \r\n                    [27, 28, 29]],\r\n                    [[31, 32, 33],\r\n                    [34, 35, 36],\r\n                    [37, 38, 39],\r\n                    [41, 42, 43],\r\n                    [44, 45, 46],\r\n                    [47, 48, 49]]])\r\nprint(\"==>> params.shape: \", params.shape)\r\nres = tf.gather(indices = indices, params = params, axis=1)\r\nprint(\"==>> res: \", res)\r\nprint(\"==>> res.shape: \", res.shape)\r\n\r\ndef gather(params, indices):\r\n    \"\"\" equivalent to tf.gather(axis=1)\"\"\"\r\n    output_shape = [params.shape[0], len(indices), params.shape[2]]\r\n    params = params.numpy()\r\n    output_tensor = params[:,indices,:]\r\n    return tf.constant(output_tensor,dtype=tf.float32)\r\n\r\nres2 = gather(params,indices)\r\nprint(\"==>> res2: \", res2)\r\nprint(\"==>> res2.shape: \", res2.shape)\r\n``` \r\nIf I don't convert the type of tensor to numpy by `params = params.numpy()`, it will reports error:\r\n```\r\nTypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got [3, 5, 2]\r\n```\r\nSo how to implement tf.gather with some other tf ops and  without numpy, thanks in advance !", "comments": ["or please tell me where is the minimal c++ implementation code of tf.gather, thanks !", "@wwdok Could you please have a look at this[ link](https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/gather) to get more details on Gather and let us know if it helps?Thanks!", "@sushreebarsa I'd rather know the code-level implementation than the superficial documentation. I notice the [gather_op.cc](https://github.com/tensorflow/tensorflow/blob/0b6b491d21d6a4eb5fbab1cca565bc1e94ca9543/tensorflow/core/kernels/gather_op.cc) and [gather_functor.cc](https://github.com/tensorflow/tensorflow/blob/0b6b491d21d6a4eb5fbab1cca565bc1e94ca9543/tensorflow/core/kernels/gather_functor.cc), are they the low-level implementation of tf.gather ?", "@gadagashwini I was able to reproduce this issue on colab using TF v2.7.0 , 2.8.0 and tf-nightly ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/428c2abe68f22930951688e3af851e39/55560.ipynb).Thanks!", "@wwdok,\r\nHi, Are you looking for https://github.com/tensorflow/tensorflow/blob/aa64f94acc83889d5e165d14f2e5c1dbe71bef3b/tensorflow/python/ops/array_ops.py#L5066"]}, {"number": 55558, "title": "Add DEVICE_DEFAULT registration for DataFormatDimMap and DataFormatVecPermute", "body": null, "comments": ["@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 55557, "title": "[PluggableDevice] Add TF_KernelBuilder_Label", "body": "Setting a label on a kernel allows us to choose which version to choose at runtime and avoid GPU<->CPU copies, e.g. with `DataFormatPermute` or `DataFormatDimMap`.", "comments": ["@rohan100jain Can you please review this PR ? Thank you!"]}, {"number": 55556, "title": "Added support for big tensor pooling", "body": "Cudnn legacy API only supports int32 indexing and can handle a maximum of 2^31-1 elements. This PR added support of pooling operations over big tensor. In particular, we split the big tensor along the batch axis when necessary and call cudnn API sequentially.\r\n\r\ncc. @nluehr ", "comments": []}, {"number": 55554, "title": "Fix crash in GPU SparseToDense when validate_indices is false.", "body": "This essentially extends the fix from commit 23c6926c4e4 to the\r\nvalidate_indices=false code path.\r\n\r\nCC: @reedwm ", "comments": ["I don't think the fix is necessary on the validate_indices=false path. The reason it was necessary on the validate_indices=true path is because we passed a lambda to `event_mgr->ThenExecute` and had to ensure the tensor was still in scope when the lambda was eventually used. But the validate_indices=false path currently does not use a lambda, and does not need to use the tensor after the function returns, so I don't think the issue can occur on that path.", "Prior to this fix we observed intermittent CUDA segfaults during training when validate_indices was false. (this was in a TF1 model with identical SparseToDense GPU implementation)", "Calling `event_mgr->ThenExecute` effectively causes a GPU synchronization to occur before the op after SparseToDense can execute, which perhaps is causing some race condition to not occur, fixng the segfault. I don't think SparseToDense is the cause though.\r\n\r\nIf you give us an example to reproduce, we might be able to find where the segfault is coming from. But if the example is too large, there's a good chance we won't have time to debug it, unfortunately.", "Hi @nluehr Can you please check @reedwm's comments and keep us posted ? Thank you!"]}, {"number": 55551, "title": "Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (1280 != 0)", "body": "### 1. System information\r\n\r\nOS: Windows 10:\r\nTensorFlow: 2.7.1:\r\n\r\n### 2. Code\r\n\r\npython version : 3.7.13  / 64 bit\r\n\r\n#### Code used to create and convert transfer learning tflite model \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\nIMG_SIZE = 32\r\nNUM_CLASSES = 10\r\nNUM_FEATURES = 1 * 1 * 1280\r\nBATCH_SIZE = 32\r\n\r\n\r\nclass TransferLearningModel(tf.Module):\r\n\r\n    def __init__(self, learning_rate=0.01):\r\n        \"\"\"\r\n        Initializes a transfer learning model instance.\r\n\r\n        Parameters:\r\n            learning_rate (float) : A learning rate for the optimzer.\r\n        \"\"\"\r\n\r\n        # - head model\r\n        # ? DEBATABLE IF THE INPUT SHAPE SHOULD BE DECLARED ON THE FIRST LAYER\r\n        self.head_model = tf.keras.Sequential([\r\n            tf.keras.layers.Dense(128, activation='relu', name='dense_1', input_shape=([NUM_FEATURES])),\r\n            tf.keras.layers.Dense(NUM_CLASSES, name='dense_2')])\r\n\r\n        # - base model\r\n        self.base_model = tf.keras.applications.MobileNetV2(\r\n            input_shape=(IMG_SIZE, IMG_SIZE, 3),\r\n            alpha=1.0,\r\n            include_top=False,\r\n            weights='imagenet')\r\n\r\n        # ? from_logits = True or False\r\n        # - loss function\r\n        self.loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\r\n\r\n        # - optimizer\r\n        self.optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\r\n\r\n        self.head_model.compile(optimizer=self.optimizer,\r\n                                loss=self.loss_fn)\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE, 3], tf.float32), ])\r\n    # * TESTED\r\n    def load(self, feature):\r\n        \"\"\"\r\n        Generates and loads bottleneck features from the given image batch.\r\n\r\n        Parameters:\r\n            feature: A tensor of image feature batch to generate the bottleneck from.\r\n        Returns:\r\n            Map of the bottleneck.\r\n        \"\"\"\r\n\r\n        # - Preprocesses a tensor or Numpy array encoding a batch of images.\r\n        x = tf.keras.applications.mobilenet_v2.preprocess_input(\r\n            tf.multiply(feature, 255))\r\n\r\n        # - reshapes the base_model output to 1,1*1*1280(1 is image size downsampled five times\r\n        # - and 1280 is the number of features extracted)\r\n        base_model_output = self.base_model(x, training=False)\r\n        bottleneck = tf.reshape(\r\n            base_model_output, (-1, NUM_FEATURES))\r\n\r\n        return {'bottleneck': bottleneck}\r\n\r\n    # - passes the bottleneck features trought the head model\r\n    # * TESTED\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec([None, NUM_FEATURES], tf.float32),\r\n        tf.TensorSpec([None, NUM_CLASSES], tf.float32), ])\r\n    def train(self, bottleneck, label):\r\n        \"\"\"\r\n        Runs one training step with the given bottleneck features and labels.\r\n\r\n        Parameters:\r\n            bottleneck: A tensor of bottleneck features generated from the base model.\r\n            label: A tensor of class labels for the given batch.\r\n        Returns:\r\n            Map of the training loss.\r\n        \"\"\"\r\n\r\n        with tf.GradientTape() as tape:\r\n            logits = self.head_model(bottleneck)\r\n            prediction = tf.nn.softmax(logits)\r\n\r\n            loss = self.head_model.loss(prediction, label)\r\n            # ? loss=self.loss_fn(prediction,label)\r\n\r\n        gradients = tape.gradient(loss, self.head_model.trainable_variables)\r\n\r\n        self.head_model.optimizer.apply_gradients(\r\n            zip(gradients, self.head_model.trainable_variables))\r\n        # ? self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\r\n\r\n        result = {\"loss\": loss}\r\n        for grad in gradients:\r\n            result[grad.name] = grad\r\n        return result\r\n\r\n    # * TESTED\r\n    @tf.function(input_signature=[tf.TensorSpec([None, IMG_SIZE, IMG_SIZE, 3], tf.float32)])\r\n    def infer(self, image):\r\n        \"\"\"\r\n        Invokes an inference on the given image.\r\n\r\n        Parameters:\r\n                feature: A tensor of image feature batch to invoke an inference on.\r\n        Returns:\r\n                Map of the softmax output.\r\n        \"\"\"\r\n        x = tf.keras.applications.mobilenet_v2.preprocess_input(\r\n            tf.multiply(image, 255))\r\n        bottleneck = tf.reshape(\r\n            self.base_model(x, training=False), (-1, NUM_FEATURES))\r\n        logits = self.head_model(bottleneck)\r\n        return {'output': tf.nn.softmax(logits)}\r\n\r\n    # * TESTED\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n    def save(self, checkpoint_path: str):\r\n        \"\"\"\r\n        Saves the trainable weights to the given checkpoint file.\r\n\r\n        Parameters:\r\n                checkpoint_path (String) : A file path to save the model.\r\n        Returns:\r\n                Map of the checkpoint file path.\r\n        \"\"\"\r\n\r\n        tensor_names = [weight.name for weight in self.head_model.weights]\r\n        tensors_to_save = [weight.read_value() for weight in self.head_model.weights]\r\n        tf.raw_ops.Save(\r\n            filename=checkpoint_path,\r\n            tensor_names=tensor_names,\r\n            data=tensors_to_save,\r\n            name='save')\r\n\r\n        return {'checkpoint_path': checkpoint_path}\r\n\r\n    # * TESTED\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\r\n    def restore(self, checkpoint_path):\r\n        \"\"\"\r\n        Restores the serialized trainable weights from the given checkpoint file.\r\n\r\n        Paramaters:\r\n            checkpoint_path (String) : A path to a saved checkpoint file.\r\n        Returns:\r\n            Map of restored weights and biases.\r\n        \"\"\"\r\n        restored_tensors = {}\r\n        for tensor in self.head_model.weights:\r\n            restored = tf.raw_ops.Restore(file_pattern=checkpoint_path,\r\n                                          tensor_name=tensor.name,\r\n                                          dt=tensor.dtype,\r\n                                          name='restore')\r\n            tensor.assign(restored)\r\n            restored_tensors[tensor.name] = restored\r\n\r\n        return restored_tensors\r\n\r\n    # * TESTED\r\n    @tf.function\r\n    def extract_weights(self):\r\n        \"\"\"\r\n        Extracts the traininable weights of the head model as a list of numpy arrays.\r\n\r\n        Paramaters:\r\n\r\n        Returns:\r\n            Map of extracted weights and biases.\r\n        \"\"\"\r\n        tmp_dict = {}\r\n        tensor_names = [weight.name for weight in self.head_model.weights]\r\n        tensors_to_save = [weight.read_value() for weight in self.head_model.weights]\r\n        for index, layer in enumerate(tensors_to_save):\r\n            tmp_dict[tensor_names[index]] = layer\r\n\r\n        return tmp_dict\r\n\r\n\r\ndef convert_and_save(saved_model_dir='saved_model_new'):\r\n    \"\"\"\r\n    Converts and saves the TFLite Transfer Learning model.\r\n\r\n    Parameters:\r\n        saved_model_dir: A directory path to save a converted model.\r\n    Returns:\r\n        NONE\r\n    \"\"\"\r\n    transfer_learning_model = TransferLearningModel()\r\n\r\n    tf.saved_model.save(\r\n        transfer_learning_model,\r\n        saved_model_dir,\r\n        signatures={\r\n            'load': transfer_learning_model.load.get_concrete_function(),\r\n            'train': transfer_learning_model.train.get_concrete_function(),\r\n            'infer': transfer_learning_model.infer.get_concrete_function(),\r\n            'save': transfer_learning_model.save.get_concrete_function(),\r\n            'restore': transfer_learning_model.restore.get_concrete_function(),\r\n            'extract': transfer_learning_model.extract_weights.get_concrete_function()\r\n        })\r\n\r\n    # Convert the model\r\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n        tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n    ]\r\n\r\n    converter.experimental_enable_resource_variables = True\r\n    tflite_model = converter.convert()\r\n\r\n    model_file_path = os.path.join('model.tflite')\r\n    with open(model_file_path, 'wb') as model_file:\r\n        model_file.write(tflite_model)\r\n\r\nif __name__ == '__main__':\r\n    model = TransferLearningModel()\r\n    convert_and_save()\r\n```\r\nAndroid Dependency:\r\n\r\nimplementation 'org.tensorflow:tensorflow-lite:2.7.0'\r\n\r\nI have to specify that the inputs map will load an object of size [1][32][32][3]\r\nand outputs of [1][1280] as i've seen while debugging.\r\n\r\nAndroid method called:\r\n```\r\n\r\n  float[] loadBottleneck(float[][][] image) {\r\n\r\n    Map<String, Object> inputs = new HashMap<>();\r\n    inputs.put(\"feature\", new float[][][][]{image});\r\n    Map<String, Object> outputs = new HashMap<>();\r\n    float[][] bottleneck = new float[1][BOTTLENECK_SIZE];\r\n    outputs.put(\"bottleneck\", bottleneck);\r\n    this.interpreter.runSignature(inputs, outputs, \"load\");\r\n    return bottleneck[0];\r\n  }\r\n```\r\n\r\nError got :\r\n\r\n Internal error: Unexpected failure when preparing tensor allocations: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (1280 != 0)\r\n    Node number 70 (RESHAPE) failed to prepare.\r\n\r\n\r\nNode 70 (the reshape one) from netron visualization:\r\n![image](https://user-images.githubusercontent.com/62502494/162421884-07b510eb-fdd5-4c24-98de-ff2b43578c1f.png)\r\n![image](https://user-images.githubusercontent.com/62502494/162422010-7d168c59-ee6e-427d-8afc-2fb210bebf2c.png)\r\n\r\n\r\n", "comments": ["Hi @Rares926 ! Sorry for the late response. I was getting **shape errors while saving the model** and getting an **empty array** while executing  **from the get_input_tensor details command**. @chunduriv ! Could you please look at this issue?  Attaching gist in [2.6.3 ](https://colab.sandbox.google.com/gist/mohantym/6bf3b76a0d1d9a0c70d2767140d4a7d7/git_55551_2-7.ipynb#scrollTo=p39AiL7gy1i_), [2.7.1](https://colab.sandbox.google.com/gist/mohantym/c25de5077779fc37e13fd297d1a3601c/git_55551_2-7.ipynb#scrollTo=cgV-trpvzQv8) , [2.8](https://colab.sandbox.google.com/gist/mohantym/ab026d1e258749efe1b41b9f243ff5a4/git_55551_2-7.ipynb#scrollTo=Ho7H4UVv1Dbn) and [nightly ](https://colab.sandbox.google.com/gist/mohantym/b4524cfe39523746245c38ef8a299959/git_55551_2-7.ipynb#scrollTo=pK5nCYpSipwH) for reference . Thanks!", "@Rares926 is it possible to also add code that shows TFLite inference using the `tf.lite.Interpreter`? Refer to [this code](https://www.tensorflow.org/lite/guide/inference). If you are using samples images, please upload them as well.", "Do you mean this one ? \r\n\r\n```\r\n  float[] runInference(float[][][] testImage) {\r\n    // Run the inference.\r\n    Map<String, Object> inputs = new HashMap<>();\r\n    inputs.put(\"image\", new float[][][][] {testImage});\r\n\r\n    Map<String, Object> outputs = new HashMap<>();\r\n    float[][] output = new float[1][numClasses];\r\n    outputs.put(\"output\", output);\r\n    this.interpreter.runSignature(inputs, outputs, \"infer\");\r\n    \r\n    return output[0];\r\n  }\r\n```", "https://www.dropbox.com/s/coeixr4kh8ljw6o/cifar10.zip?dl=1     --> the dataset i use can be downloaded from here ", "Could you provide a [colab](https://colab.sandbox.google.com/) where you convert the model and run inference on it using the Python interpreter? ", "You can find the colab here [colab](https://colab.research.google.com/drive/1GXp65tf9-kwvif2_BfHDKShaqKkJuNTT?usp=sharing) . \r\nMy questions are the following .\r\n -->  Firstly why is the Interpreter crashing when i try to add the extract function?\r\n -->  Secondly why is the load output on the test_image an array of zeros ?  Is it possible for the feature extractor to extract nothing from an image??"]}, {"number": 55550, "title": "tf.io.decode_image(img, channels=1) has different results for a RGB image and a RGBA image where A is set to 255. ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Build 19044\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  /\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.8.8\r\n- Bazel version (if compiling from source): /\r\n- GCC/Compiler version (if compiling from source): /\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\n**Describe the current behavior**\r\n`tf.io.decode_image(img, channels=1)` should convert the input image to grayscale ([see link](https://www.tensorflow.org/api_docs/python/tf/io/decode_png#accepted_values_are)), the results for an RGB image and a RGBA image where the A dimensions is just 255 should be the same but they are not.\r\n\r\n`tf.image.rgb_to_grayscale(img)` also does not yield the same result as the `decode_image` version. \r\n\r\n**Describe the expected behavior**\r\nIt seems to me that both the RGB and the RGBA image (with A=255) should yield the same result when using `decode_image `with channels=1?  The output from `rgb_to_grayscale`  is also different which is odd, shouldn't the implementation be very similar?\r\n\r\n**Standalone code to reproduce the issue**\r\nExample image and notebook in attachment:\r\n[decode_image_bug.zip](https://github.com/tensorflow/tensorflow/files/8449964/decode_image_bug.zip)\r\n\r\n**Other info / logs** \r\n/\r\n\r\n\r\n", "comments": ["@StefRommes I tried to replicate this issue on colab using TF v2.8.0 ,could you please find the [gist](https://colab.research.google.com/gist/sushreebarsa/855da7e39c859795072a2226cb47d900/gist55550.ipynb) and confirm the same?\r\nThanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55549, "title": "GPU has no significant performance benefit over CPU.", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11 [21H2 - 22000.556]\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.8.0-rc1-32-g3f878cff5b6 2.8.0\r\n- Python version: Python 3.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2022 NVIDIA Corporation\r\nBuilt on Tue_Mar__8_18:36:24_Pacific_Standard_Time_2022\r\nCuda compilation tools, release 11.6, V11.6.124\r\nBuild cuda_11.6.r11.6/compiler.31057947_0\r\n```\r\n- GPU model and memory:\r\n```\r\nname: NVIDIA GeForce RTX 3080 Laptop GPU\r\nmemory: 13626 MB\r\ncompute capability: 8.6\r\n```\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nTraining a 3-layer neural network on CPU on the MNIST fashion dataset:\r\nEpoch 1:\r\n```\r\nCPU: 72s 1ms/step\r\nGPU: 60s 977us/step\r\n```\r\nEpoch 2:\r\n```\r\nCPU: 73s 1ms/step\r\nGPU: 58s 974us/step\r\n```\r\nEpoch 3:\r\n```\r\nCPU: 73s 1ms/step\r\nGPU: 59s 976us/step\r\n```\r\n\r\nFor reference, my CPU and System Memory are: \r\n```\r\n11th Gen Intel(R) Core(TM) i9-11900H @ 2.50GHz (Max Turbo: 4.90 GHz)\r\nL1 cache: 640KB\r\nL2 cache: 10.0MB\r\nL3 cache: 24.0MB\r\n\r\n32GB DDR4 (Hardware Reserved: 323 MB)\r\n```\r\n\r\n**Describe the expected behavior**\r\nWhile I'm fairly new to working with Tensorflow in the grand scheme of things, I've worked with Tensorflows using GPUs in the past. \r\n\r\nAs one can note from the current behavior, the performance benefits for training over a GPU vs a CPU are minimal at best (max 15s time reduction per epoch). I may be incorrect, but shouldn't the performance benefits be far more substantial? Considering GPUs can parallelize a large chunk of this. \r\n\r\n**Standalone code to reproduce the issue**\r\n```py\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport matplotlib.pyplot as plt\r\n\r\ntf.get_logger().setLevel(\"INFO\")\r\n# tf.debugging.set_log_device_placement(True)\r\ntf.config.experimental.set_memory_growth(\r\n    device=tf.config.list_physical_devices(\"GPU\")[0],\r\n    enable=True\r\n)\r\n\r\ndef test_gpu():\r\n    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\n\r\n\r\n# Image Classes (Input Image)\r\n# Input Image = 28x28\r\n# Input Layer: (28 * 28) = 784 Neurons\r\n# Hidden Layer [1]\r\n# Output Classes: 5\r\n# Output Layer: 5 Neurons (Val: [0, 1]), simga(output_layer) = 1\r\n# Activation FX: f(sigma(layer) + b)\r\n# Cost/Loss FX: distance(expected, current) [MSE, MAE, HL]\r\n# Gradient Descent: Function to tell us how to make cost/loss\r\n# as minimum as possible. (Optimizing cost/loss fx)\r\n# Optimizers just implement the above functions in different ways.\r\ndef nn_task():\r\n    mnist = keras.datasets.fashion_mnist\r\n    # (60K, 28, 28), [0, 0, 0] = 0  ---- 0: Black, 255: White\r\n    (train_features, train_labels), (test_features, test_labels) = mnist.load_data()\r\n    print(train_features.shape)\r\n    label_names = [\r\n        \"Top\", \"Bottom\", \"Pullover\",\r\n        \"Dress\", \"Coat\", \"Sandal\",\r\n        \"Dress Shirt\", \"Sneaker\",\r\n        \"Bag\", \"Ankle Boot\"\r\n    ]\r\n\r\n    # plt.figure()\r\n    # plt.imshow(train_features[0])\r\n    # plt.colorbar()\r\n    # plt.gray()\r\n    # plt.plot()\r\n    # plt.pause(5)\r\n\r\n    # Normalize to scale of [0, 1]\r\n    # Ensures that values only range from 0 to 1.\r\n    train_features = train_features / 255\r\n    test_features = test_features / 255\r\n\r\n    model = keras.Sequential([\r\n        # Flattens the input to 784 neurons.\r\n        keras.layers.Flatten(input_shape=(28, 28), name=\"Input\"),\r\n\r\n        # Dense just means all neurons from previous\r\n        # layer are connected to this layer too.\r\n        # The number 128 is arbitrary - usually, you'd\r\n        # want this number to be less than input neurons\r\n        # but more than output neurons.\r\n        # ReLU: x < 0 => y = 0 || x >= 0 => y = x\r\n        keras.layers.Dense(128, activation=\"relu\", name=\"DataParser\"),\r\n\r\n        # Dense just means all neurons from previous\r\n        # layer are connected to this layer too.\r\n        # Layer size is based on how many classes we have.\r\n        # Softmax: sigma(layer) = 1\r\n        keras.layers.Dense(len(label_names), activation=\"softmax\", name=\"Output\")\r\n    ], \"Fashion-Cloth-Classifier\")\r\n\r\n    model.compile(\r\n        optimizer=\"adam\", \r\n        loss=\"sparse_categorical_crossentropy\",  # We use SCCE for loss.\r\n        metrics=[\"accuracy\"]  # Check only the accuracy metric.\r\n    )\r\n    model.summary()\r\n\r\n    with tf.device(\"/GPU:0\"):  # Change this line to train on CPU or GPU\r\n        model.fit(train_features, train_labels, epochs=10, batch_size=1)\r\n\r\n    t_loss, t_acc = model.evaluate(test_features, test_labels, verbose=1)\r\n\r\n    print(\"Test Loss: \", t_loss)\r\n    print(\"Test Accuracy: \", t_acc)\r\n\r\n\r\ndef work():\r\n    nn_task()\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_gpu()\r\n    work()\r\n\r\n```\r\n", "comments": ["@sushreebarsa ~~I'm going to try and use the Tensorboard profiler for Tensorflow in order to find the root cause.~~ Seems like Tensorflow won't generate profiler data.", "Did you get the chance to look into this? @sushreebarsa \r\n\r\nI wasn't able to get any profiler data like I mentioned above.", "<img width=\"296\" alt=\"image\" src=\"https://user-images.githubusercontent.com/44123859/163693552-e0b8b90f-8bf9-45aa-bfbf-c9c597947dd3.png\">\r\n\r\n@gadagashwini I was able to use the Profiler and found out that no operations are running on GPU. Which was something I thought might be happening. No clue why...", "@TheBlackPlague,\r\nThere are several factors that can contribute to low GPU utilization. Below are some scenarios commonly observed when looking at the [trace viewer](https://www.tensorflow.org/guide/profiler#trace_viewer) and potential solutions.\r\nTake a look at [Tensorflow doc](https://www.tensorflow.org/guide/gpu_performance_analysis#2_debug_the_performance_of_one_gpu). Thanks!", "The issue is, it's not just low GPU usage, it's literally none. ", "@TheBlackPlague,\r\nTested it on my local machine, looks like its using GPU for training\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM\r\n\r\ntf.get_logger().setLevel(\"INFO\")\r\n# tf.debugging.set_log_device_placement(True)\r\ntf.config.experimental.set_memory_growth(\r\n    device=tf.config.list_physical_devices(\"GPU\")[0],\r\n    enable=True\r\n)\r\n\r\nmnist = keras.datasets.fashion_mnist\r\n    # (60K, 28, 28), [0, 0, 0] = 0  ---- 0: Black, 255: White\r\n(train_features, train_labels), (test_features, test_labels) = mnist.load_data()\r\nprint(train_features.shape)\r\n label_names = [\r\n        \"Top\", \"Bottom\", \"Pullover\",\r\n        \"Dress\", \"Coat\", \"Sandal\",\r\n        \"Dress Shirt\", \"Sneaker\",\r\n        \"Bag\", \"Ankle Boot\"]\r\n\r\n    # plt.figure()\r\n    # plt.imshow(train_features[0])\r\n    # plt.colorbar()\r\n    # plt.gray()\r\n    # plt.plot()\r\n    # plt.pause(5)\r\n\r\n    # Normalize to scale of [0, 1]\r\n    # Ensures that values only range from 0 to 1.\r\ntrain_features = train_features / 255\r\ntest_features = test_features / 255\r\n\r\nmodel = keras.Sequential([\r\n     # Flattens the input to 784 neurons.\r\nkeras.layers.Flatten(input_shape=(28, 28), name=\"Input\"),\r\n\r\n     # Dense just means all neurons from previous\r\n        # layer are connected to this layer too.\r\n        # The number 128 is arbitrary - usually, you'd\r\n        # want this number to be less than input neurons\r\n        # but more than output neurons.\r\n        # ReLU: x < 0 => y = 0 || x >= 0 => y = x\r\nkeras.layers.Dense(128, activation=\"relu\", name=\"DataParser\"),\r\n\r\n        # Dense just means all neurons from previous\r\n        # layer are connected to this layer too.\r\n        # Layer size is based on how many classes we have.\r\n        # Softmax: sigma(layer) = 1\r\nkeras.layers.Dense(len(label_names), activation=\"softmax\", name=\"Output\")\r\n    ], \"Fashion-Cloth-Classifier\")\r\nmodel.compile(\r\n        optimizer=\"adam\", \r\n        loss=\"sparse_categorical_crossentropy\",  # We use SCCE for loss.\r\n        metrics=[\"accuracy\"]  # Check only the accuracy metric.\r\n    )\r\n    model.summary()\r\n\r\n# Create a TensorBoard callback\r\nlogs = \"logs/\" #+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\ntboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\r\n                                                 histogram_freq = 1,\r\n                                                 profile_batch = '500,520')\r\nwith tf.device(\"/GPU:0\"):  # Change this line to train on CPU or GPU\r\n        model.fit(train_features, train_labels, epochs=10, batch_size=1, callbacks = [tboard_callback])\r\n \r\n```\r\n\r\n<img width=\"1546\" alt=\"Screen Shot 2022-04-20 at 8 36 00 AM\" src=\"https://user-images.githubusercontent.com/99852755/164142257-1c135a44-2bda-490f-872e-a48bb481f8e5.png\">\r\n", "Hello. Are you able to get any GPU usage in the profile?"]}, {"number": 55548, "title": "Converter for LogicalOr and LogicalAnd operations.", "body": "Implementation of the converters for LogicalOr and LogicalAnd operations.", "comments": ["Replacement for [PR#55275](https://github.com/tensorflow/tensorflow/pull/55275)"]}, {"number": 55546, "title": "TensorFlow Dataset has not correct shape", "body": "Hi,\r\n   I think the issue here is from the dataset, which doesn't populate the shape info for the sequence dim.\r\n\r\n  It is becouse print(dataset) gives me:\r\n`<ZipDataset element_spec=(TensorSpec(shape=(None, None, 25, 81), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 25, 1), dtype=tf.float64, name=None))> `.\r\n\r\nThe model knows from the dataset object only the last two axis, but I need to it knows the last 3 axis. \r\nThe shape must be defined for example: `shape=(None, 7, 25, 81)` or `shape=(None, 7, 25, 1)`.\r\n\r\nThanks. \r\n\r\n## System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 12.3 / Debian 5.10.70-1 (2021-09-30)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0 / 2.9.0-dev20220329\r\n- Python version: Python 3.9.7 / Python 3.9.2\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): Clang 11.1.0 / GCC 10.2.1\r\n- CUDA/cuDNN version: No\r\n- GPU model and memory: Apple M1, 16GB / No\r\n\r\n## Code\r\n```python\r\nfrom tensorflow.keras.layers import Dense, Dropout, Layer, Input\r\nfrom tensorflow.keras.initializers import TruncatedNormal\r\nfrom tensorflow.keras.models import Model\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass PositionalEmbedding(Layer):\r\n    def __init__(self, units, dropout_rate, **kwargs):\r\n        super(PositionalEmbedding, self).__init__(**kwargs)\r\n\r\n        self.units = units\r\n\r\n        self.projection = Dense(units, kernel_initializer=TruncatedNormal(stddev=0.02))\r\n        self.dropout = Dropout(rate=dropout_rate)\r\n\r\n    def build(self, input_shape):\r\n        super(PositionalEmbedding, self).build(input_shape)\r\n\r\n        print(input_shape, '\\n')\r\n        self.position = self.add_weight(\r\n            name=\"position\",\r\n            shape=(1, input_shape[1], input_shape[2], self.units),\r\n            initializer=TruncatedNormal(stddev=0.02),\r\n            trainable=True,\r\n        )\r\n\r\n    def call(self, inputs, training):\r\n        x = self.projection(inputs)\r\n        x += self.position\r\n\r\n        return self.dropout(x, training=training)\r\n\r\nclass Transformer(Model):\r\n    def __init__(\r\n        self,\r\n        embed_dim,\r\n        dropout_rate,\r\n        **kwargs\r\n    ):\r\n        super(Transformer, self).__init__(**kwargs)\r\n\r\n        # Input\r\n        self.pos_embs = PositionalEmbedding(embed_dim, dropout_rate)\r\n\r\n    def compile(self, optimizer, loss):\r\n        super(Transformer, self).compile()\r\n        self.optimizer = optimizer\r\n        self.loss = loss\r\n\r\n    def call(self, inputs, training):\r\n        inputs, targets = inputs\r\n        \r\n        return self.pos_embs(inputs, training=training)\r\n\r\n    def train_step(self, inputs):\r\n        inputs, targets = inputs\r\n\r\n        print(inputs.shape)\r\n        print(targets.shape, '\\n')\r\n\r\n        targets_inputs = targets[:, :-1]\r\n        targets_real = targets[:, 1:]\r\n\r\n        with tf.GradientTape() as tape:\r\n            y_pred = self([inputs, targets_inputs], training=True)\r\n            loss = self.loss(targets_real, y_pred)\r\n\r\n        trainable_vars = self.trainable_variables\r\n        gradients = tape.gradient(loss, trainable_vars)\r\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n\r\n        return {\r\n            \"loss\": loss,\r\n        }\r\n\r\ndef load_dataset(batch_size, window_size):\r\n  x_all = np.ones((1000, 25, 81)) #np.load('./dataset/X_all.npy')\r\n  y_all = np.ones((1000, 25, 1)) #np.load('./dataset/y_all.npy')\r\n\r\n  inputs_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n      x_all, None, sequence_length=window_size, sequence_stride=(window_size // 2), batch_size=batch_size)\r\n  targets_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n      y_all, None, sequence_length=window_size, sequence_stride=(window_size // 2), batch_size=batch_size)\r\n\r\n  return inputs_dataset, targets_dataset\r\n\r\n\r\n\r\n# load dataset\r\ninputs_dataset, targets_dataset = load_dataset(\r\n      batch_size=64,\r\n      window_size=7,\r\n)\r\ndataset = tf.data.Dataset.zip((inputs_dataset, targets_dataset))\r\n\r\nfor batch in dataset:\r\n    inputs, targets = batch\r\n    print(inputs.shape)\r\n    print(targets.shape, '\\n')\r\n    break\r\n\r\nsample_transformer = Transformer(\r\n    embed_dim=256, dropout_rate=0.1,\r\n)\r\n\r\nsample_transformer.compile(\r\n      loss=tf.keras.losses.mean_squared_error,\r\n      optimizer=tf.keras.optimizers.Adam(),\r\n)\r\n\r\n# Train model\r\nsample_transformer.fit(\r\n      dataset,\r\n      epochs=10\r\n)\r\n```", "comments": ["Hello @markub3327 ,\r\nI tried to execute the given code snippet, I am facing different error stating. Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/773ef1a7f6105767925cd885691e4c21/untitled296.ipynb).\r\n\r\n\r\n", "@tilakrayal \r\n\r\nIt's the same error as I saw. \r\n```shell\r\nValueError: in user code:\r\n\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\r\n        return step_function(self, iterator)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\r\n        outputs = model.train_step(data)\r\n    File \"<ipython-input-1-75c76593a629>\", line 66, in train_step\r\n        y_pred = self([inputs, targets_inputs], training=True)\r\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\r\n        raise e.with_traceback(filtered_tb) from None\r\n\r\n    ValueError: Exception encountered when calling layer \"transformer\" (type Transformer).\r\n    \r\n    in user code:\r\n    \r\n        File \"<ipython-input-1-75c76593a629>\", line 54, in call  *\r\n            return self.pos_embs(inputs, training=training)\r\n        File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\r\n            raise e.with_traceback(filtered_tb) from None\r\n        File \"<ipython-input-1-75c76593a629>\", line 25, in build\r\n            trainable=True,\r\n    \r\n        ValueError: Can't convert Python sequence with mixed types to Tensor.\r\n    \r\n    \r\n    Call arguments received:\r\n      \u2022 inputs=['tf.Tensor(shape=(None, None, 25, 81), dtype=float32)', 'tf.Tensor(shape=(None, None, 25, 1), dtype=float32)']\r\n      \u2022 training=True\r\n```\r\n\r\nThe problem is with `print(dataset)` that gives me:\r\n  ` <ZipDataset element_spec=(TensorSpec(shape=(None, None, 25, 81), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 25, 1), dtype=tf.float64, name=None))>`\r\n\r\nAnd I need to propagate the last 3 dims from Dataset to model during build process.\r\nMore info about this you found here: https://github.com/keras-team/keras/issues/16315", "@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.7, v2.8 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/b448144440cb53b755f6c58d7ddc6e1c/16315.ipynb).", "It looks like `tf.keras.preprocessing.timeseries_dataset_from_array` builds the sequences in such a way that their length can't be inferred automatically. If the second dimension is guaranteed to be equal to the sequence length, you can use [ensure_shape](https://www.tensorflow.org/api_docs/python/tf/ensure_shape) to communicate the knowledge to the shape inference system:\r\n\r\n```python\r\ntargets_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n      y_all, None, sequence_length=window_size, sequence_stride=(window_size // 2), batch_size=batch_size)\r\n# Set the shape for the elements of the dataset.\r\ntargets_dataset = targets_dataset.map(lambda x: tf.ensure_shape(x, <expected shape>)\r\n```\r\n\r\n@fchollet would it make sense for `timeseries_dataset_from_array` to call `ensure_shape` for the elements produced by `timeseries_dataset_from_array`?"]}, {"number": 55545, "title": "tf.keras.layers.BatchNormalization computes moving variances inconsistently when `fused=True` vs. when `fused=False` ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.8.0 (gpu)\r\n- Python version: 3.10.2\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: 11.5/8.3.1\r\n- GPU model and memory: 1080 Ti 12GB\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.keras.layers.BatchNormalization` computes moving variances inconsistently when `fused=True` vs when `fused=False`. When `fused=False` it computes it with \r\n\r\n             next_variance=old_variance*momentum+batch_variance*(1-momentum)\r\n\r\nwhereas when `fused=True` it computes it with: \r\n\r\n             next_variance=old_variance*momentum+batch_variance*(1-momentum)*bessel_coefficient_correction\r\n\r\n**This is a very minor bug when n is large** as bessel_coefficient_correction tends to 1. But when number of elements per channel is small it becomes noticeable. Below I gave an example code for when elements per channel is 2.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect `fused=True` and `fused=False` to be consistent.\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Solution**\r\n\r\nI think the issue lies [here](https://github.com/tensorflow/tensorflow/blob/b89a2f2a1b761ac68aa808f2bcd847314b15c6c1/tensorflow/core/kernels/fused_batch_norm_op.cc#L213). In which you can see the bessel_coefficient_correction as `rest_size_adjust`. Either that should be removed in fused batch norm or it should be added for non-fused batch norm.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom math import sqrt\r\n\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n# with tf.device(\"/gpu:0\"):\r\n    var = 5.0\r\n    std = sqrt(var)\r\n    n=2\r\n    inp = tf.random.normal((1,n,1,1))\r\n    true_mean = tf.reduce_mean(inp, axis=(0, 1, 2)).numpy()\r\n    print(\"True mean: \" + str(true_mean))\r\n    true_var = tf.math.reduce_variance(inp, axis=(0, 1, 2)).numpy()\r\n    print(\"True variance: \" + str(true_var))\r\n\r\n    for momentum in [0.0, 0.25, 0.75, 0.9]:\r\n        for fused in [True, False]:\r\n            layer = tf.keras.layers.BatchNormalization(\r\n                scale=True, center=True, trainable=True, momentum=momentum, fused=fused\r\n            )\r\n            initial_mean = 0.0\r\n            initial_var = 1.0\r\n            out = layer(inp, training=True)\r\n            print(\"\\n===========================\")\r\n            print(\"Momentum: \" + str(momentum) + \" Fused: \" + str(fused))\r\n            print(\"Moving mean: \" + str(layer.moving_mean.numpy()))\r\n            print(\r\n                \"Moving mean always correctly computed: \"\r\n                + str(initial_mean * momentum + true_mean * (1 - momentum))\r\n            )\r\n            print(f\"Moving var computed with fused={fused}: \" + str(layer.moving_variance.numpy()))\r\n            print(\r\n                \"Moving var with Bessel correction: \"\r\n                + str(initial_var * momentum + n/(n-1) * true_var * (1 - momentum))\r\n            )\r\n            print(\r\n                \"Moving var without Bessel correction: \"\r\n                + str(initial_var * momentum + true_var * (1 - momentum))\r\n            )\r\n            print(\"===========================\\n\")\r\n\r\n```\r\n\r\nSee this gist: https://gist.github.com/canbakiskan/9a1fe01a218277539f9602081719a1c3", "comments": ["@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/185c8c0b3a40c26d8e36542210fc1be4/55545.ipynb).", "You can find some details when fused is set to True. Let us know if this helps you.\r\nhttps://github.com/keras-team/keras/blob/d8fcb9d4d4dad45080ecfdd575483653028f8eda/keras/layers/normalization/batch_normalization.py#L197-L205", "Ok I found the culprit. It's not that the batch's variance is multiplied by 2 but rather by Bessel's coefficient correction which is n/(n-1) where n is the number of elements in a channel of the tensor. See [this line](https://github.com/tensorflow/tensorflow/blob/b89a2f2a1b761ac68aa808f2bcd847314b15c6c1/tensorflow/core/kernels/fused_batch_norm_op.cc#L213).\r\n\r\nIt's still inconsistent behavior with respect to `fused=False` case though. I'm changing the title and first post accordingly. So this gist is no longer up to date: \r\n> @sachinprasadhs , I was able to reproduce the issue in tf v2.8, v2.7 and nightly.Please find the gist [here](https://colab.research.google.com/gist/tilakrayal/185c8c0b3a40c26d8e36542210fc1be4/55545.ipynb).\r\n\r\n", "Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55544, "title": "[PluggableDevice] Make graph_buf argument in TF_NewFunctionLibraryDefinition constant", "body": "`TF_NewFunctionLibraryDefinition` doesn't modify anything in `graph_buf`, and having it as non-const makes it impossible to use the graph buffer passed to the `Optimize` function without making a copy.", "comments": ["@penpornk Can you please review this PR ? Thank you!"]}, {"number": 55542, "title": "Converter for LogicalNot operation", "body": "- Converter for LogicalNot operation is using the same base class (ConvertUnaryImpl) as other Unary operations.\r\n- The special converter for Rsqrt was removed and now for this operation regular Unary Op converter is used.\r\n- New templated class implemented for testing ConvertUnary, ConvertBooleanUnary, ConvertActivation.\r\n- A new check and corresponding subtests were added to Validate: at least 1 dimension is required for input of any Unary and UnaryBoolean operation. (Similar subtest for ConvertActivation operations is blocked and after refactoring of ConvertActivation it will be activated)\r\n", "comments": ["Replacement for [PR#55428](https://github.com/tensorflow/tensorflow/pull/55428)", "still seeing the same error:\r\nIn file included from [third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:16](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc?l=16&ws=tap-prod-presubmit/181146118&snapshot=2):\r\nIn file included from [./third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h:26](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.h?l=26&ws=tap-prod-presubmit/181146118&snapshot=2):\r\nIn file included from [./third_party/tensorflow/compiler/tf2tensorrt/convert/op_converter.h:25](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/op_converter.h?l=25&ws=tap-prod-presubmit/181146118&snapshot=2):\r\nIn file included from [./third_party/tensorflow/compiler/tf2tensorrt/convert/weights.h:22](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/weights.h?l=22&ws=tap-prod-presubmit/181146118&snapshot=2):\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:98](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h?l=98&ws=tap-prod-presubmit/181146118&snapshot=2):30: error: no matching function for call to 'DebugString'\r\n    StrAppend(&tmp_s, StrCat(DebugString(el), \", \"));\r\n                             ^~~~~~~~~~~\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:1706](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc?l=1706&ws=tap-prod-presubmit/181146118&snapshot=2):58: note: in instantiation of function template specialization 'tensorflow::tensorrt::DebugString<bool>' requested here\r\n                       << \", Received Input Tensor: \" << DebugString(values);\r\n                                                         ^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:1888](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc?l=1888&ws=tap-prod-presubmit/181146118&snapshot=2):7: note: in instantiation of function template specialization 'tensorflow::tensorrt::convert::ParameterizedOpConverterTestBase::AddTestTensor<bool>' requested here\r\n      AddTestTensor(\"input\", p.input_dims, input_tf_type, input_values);\r\n      ^\r\n[third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc:7057](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc?l=7057&ws=tap-prod-presubmit/181146118&snapshot=2):3: note: in instantiation of function template specialization 'tensorflow::tensorrt::convert::OpConverter_UnaryTest<bool>::RunTests<nvinfer1::UnaryOperation>' requested here\r\n  RunTests(\"LogicalUnary\", ops_to_test, *UnaryBooleanOperationMap(), op_map,\r\n  ^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/trt_parameters.h:46](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/trt_parameters.h?l=46&ws=tap-prod-presubmit/181146118&snapshot=2):8: note: candidate function not viable: no known conversion from 'const std::__bit_iterator<std::vector<bool>, true, 0>::reference' (aka 'const std::__bit_const_reference<std::vector<bool>>') to 'const tensorflow::tensorrt::TrtPrecisionMode' for 1st argument\r\nstring DebugString(const TrtPrecisionMode mode);\r\n       ^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:95](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h?l=95&ws=tap-prod-presubmit/181146118&snapshot=2):8: note: candidate template ignored: could not match 'vector' against '__bit_const_reference'\r\nstring DebugString(const std::vector<CType>& vector) {\r\n       ^\r\n[./third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h:86](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/compiler/tf2tensorrt/convert/utils.h?l=86&ws=tap-prod-presubmit/181146118&snapshot=2):8: note: candidate template ignored: requirement 'std::is_arithmetic<std::__bit_const_reference<std::vector<bool, std::allocator<bool>>>>::value' was not satisfied [with CType = std::__bit_const_reference<std::vector<bool>>]\r\nstring DebugString(const CType& el) {\r\n       ^\r\n1 error generated.", "@bixia1 : It looks like **bool**'s are represented differently by the compilers we are using. After replacement **bool** by **int** the compilation should be fine.", "I am fine with change bool in the test or having another print method.", "Test failure:\r\n[----------] 3 tests from OpConvTestInstantiation/OpConverter_BOOL_Test\r\n[ RUN      ] OpConvTestInstantiation/OpConverter_BOOL_Test.ConvertBoolean/0\r\nF0418 09:07:39.785729    2094 convert_nodes_test.cc:1202] Cannot create tensor with type bool\r\n*** Check failure stack trace: ***\r\n    @     0x560d281e6d38  absl::log_internal::LogMessage::SendToLog()\r\n    @     0x560d281e6602  absl::log_internal::LogMessage::Flush()\r\n    @     0x560d281e6fc9  absl::log_internal::LogMessageFatal::~LogMessageFatal()\r\n    @     0x560d1c9e4f49  tensorflow::tensorrt::convert::OpConverterTest::AsTensor<>()\r\n    @     0x560d1c967556  tensorflow::tensorrt::convert::OpConverterTest::AddTestWeights<>()\r\n    @     0x560d1ca11c03  tensorflow::tensorrt::convert::OpConverter_UnaryTest<>::runExpectedToFailTest()\r\n    @     0x560d1c9b39a7  tensorflow::tensorrt::convert::OpConverter_UnaryTest<>::RunTests<>()\r\n    @     0x560d1c9b334d  tensorflow::tensorrt::convert::OpConverter_BOOL_Test_ConvertBoolean_Test::TestBody()\r\n    @     0x560d1cb684a2  testing::Test::Run()\r\n    @     0x560d1cb6972f  testing::TestInfo::Run()\r\n    @     0x560d1cb6a495  testing::TestSuite::Run()\r\n    @     0x560d1cb7b69c  testing::internal::UnitTestImpl::RunAllTests()\r\n    @     0x560d1cb7abfb  testing::UnitTest::Run()\r\n    @     0x560d1cb3a8d3  main\r\n    @     0x7f75d3dbd8d3  __libc_start_main\r\n    @     0x560d1c93002a  [../sysdeps/x86_64/start.S:120](https://cs.corp.google.com/piper///depot/sysdeps/x86_64/start.S?l=120&ws=tap-prod-presubmit/181733079&snapshot=2) _start", "@bixia1: What version of TRT are you using? It looks like DT_BOOL is not fully supported for TRT < 8.2. \r\nI added\r\n```\r\n#if IS_TRT_VERSION_GE(8, 2, 0, 0)\r\n```\r\nstatements in two places (in `ConverBooleanUnary::Validate()` and in `ConvertBoolean` test). I hope this will solve the problem."]}, {"number": 55536, "title": "Adding Select Tf Ops to Cmake", "body": "**System information**\r\n- TensorFlow version (you are using): any\r\n- Are you willing to contribute it (Yes/No): no\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.** \r\nCurrently one has to decide between using Select TF ops with bazel or GPU support with CMake.\r\nIt would be nice if either bazel has an option to support GPU on any OpenCL system or CMake can build with TF Ops.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nEverybody who wants to use GPU acceleration and TF Ops.\r\n\r\n", "comments": []}, {"number": 55534, "title": "xla cpu backend enhancements", "body": "This PR implements \r\n1. batch dot emitter with eigen runtime\r\n2. ACL (Arm Compute Library) runtime for dot and conv emitters", "comments": ["@snadampal Can you please resolve conflicts? Thank you!", "Hi @gbaned , thanks for the review! As you know, the conflict in the xla.proto file is for the enum definitions, It will be a one line change to use the last available value for the new definitions. Since this file is being changed in lot of other PRs as well, depending on which PR gets merged first, all other need to update theirs. I'm thinking it may be good to address it at the end, close to when the PR is ready for merge.  If you think this is required to proceed with CI tests, I will update it right away."]}, {"number": 55530, "title": "Test fail on r2.8: core:__tensorflow_core_lib_math_math_util_test   ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v2.8.0-2-ge994fb9c3ad 2.8.0\r\n- Python version: 3.8.3\r\n- Bazel version (if compiling from source): 0.25.2\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\ntest fails\r\n\r\n**Describe the expected behavior**\r\n\r\ntest passes\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): yes\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\nThis code https://github.com/tensorflow/tensorflow/blob/37541479e953b40a2ab5d45fa5cf4c54740820df/tensorflow/core/platform/default/logging.h#L402 appears to return an aliased/out of scope value when called from https://github.com/tensorflow/tensorflow/blob/37541479e953b40a2ab5d45fa5cf4c54740820df/tensorflow/core/lib/math/math_util.h#L100 from (commenting out this line causes the test to pass) https://github.com/tensorflow/tensorflow/blob/37541479e953b40a2ab5d45fa5cf4c54740820df/tensorflow/core/lib/math/math_util_test.cc#L193\r\n\r\nEvidence for 'aliased/out of scope value' above: the following change in `math_util.h` causes the test to pass:\r\n```\r\n<<<\r\n  DCHECK_NE(0, denominator) << \"Division by zero is not supported.\";\r\n===\r\n  IntegralType dZero = 0;\r\n  DCHECK_NE(dZero, denominator) << \"Division by zero is not supported.\";\r\n>>>\r\n```\r\n\r\nBut we might prefer either:\r\n - The above\r\n - Debug whatever is uint64-specific about https://github.com/tensorflow/tensorflow/blob/37541479e953b40a2ab5d45fa5cf4c54740820df/tensorflow/core/platform/default/logging.h#L280, but I can't see that atm\r\n - Test on a newer gcc, and deprecate 7.5.0 if that passes\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n$ git checkout r2.8\r\n$ bazel  --host_jvm_args=-Xmx32g test --jobs=12  --config=dbg --verbose_failures -k //tensorflow/core:__tensorflow_core_lib_math_math_util_test   \r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[test.log](https://github.com/tensorflow/tensorflow/files/8443281/t.log)\r\n\r\n", "comments": ["Update: \r\ntest still fails on commit `55645ca964508507890529a71591f51a344a6356` April 9\r\n\r\ntest passes with ``--config=opt``", "@awf\r\nPlease let us know if this issue is resolved for you in a recent commit ?Thanks!", "Still present in 55645c from 2 days ago.\r\nI'll take a look, but I don't see anything more recent that might have fixed it.\r\n", "Confirmed on\r\n```\r\ncommit c44d14f2194cf4c4b4060fd4141194ee62792ca8 (HEAD -> master, upstream/master, origin/master, origin/HEAD)\r\nDate:   Mon Apr 11 05:36:24 2022 -0700\r\n```", "Note I'm happy to submit a PR, but there are a few options, as listed above, so seeking guidance on which one to implement."]}, {"number": 55528, "title": "how to build v2.8.0 with verbs under centos7.8?", "body": null, "comments": ["@zhucan \r\nIn order to expedite the trouble-shooting process here,Could you please fill the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose), \r\nPlease refer to this[ link](https://www.tensorflow.org/install/source) and let us know if it helps?Thanks!", "This link only for ubuntu and mac. @sushreebarsa ", "@zhucan Could you please confirm if it is a duplicate ticket of this issue #55524 ?\r\nThanks!", "Yes, because of I had compiled the tensorflow2.0 with verbs successfully. @sushreebarsa ", "@zhucan Could you please move this issue to closed status if it is resolved?\r\nThanks!", "But with the v2.8.0, I hadn't fixed @sushreebarsa ", "[Tensorflow2.0.md](https://github.com/tensorflow/tensorflow/files/8510195/Tensorflow2.0.md)\r\n", "Chinese: https://github.com/zhucan/kubernetes-study/blob/master/Tensorflow2.0.pdf"]}, {"number": 55527, "title": "I would like to ask how to save the personalization model during training on device", "body": "I would like to ask how to save the personalization model during training on device. If this personalization model can be used for transmission, it may be possible to implement federated learning for user privacy and security, and I would like to ask if this feature is supported or will be supported soon.", "comments": ["@YourPeer ,\r\n In order to expedite the trouble-shooting process, I request you to please provide a minimal code snippet and the TensorFlow version you are using.Thanks!\r\n\r\nAlso can you please have a look at this [document](https://blog.tensorflow.org/2019/03/introducing-tensorflow-privacy-learning.html).It delivers information that federated learning for user privacy and security has been implemented as TensorFlow Privacy.Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 55525, "title": "XNNPack delegate support for quantized models, no latency improvement", "body": "\r\n\r\nI built tensorflow with  --define tflite_with_xnnpack=true --define xnn_enable_qs8=true to have acceleration for quantized models. But I don't gain any improvement in the latency of the quantized models compared to when I build tensorflow with --define tflite_with_xnnpack=true alone. I tested both in windows and linux. I am testing on desktop with intel CPU.  Is there anything I am doing wrong? I tested with trained models too. I used the following code to quantize the model.:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import *\r\n\r\nfrom os import listdir\r\nfrom os.path import isfile, join\r\n\r\ndef representative_dataset():\r\n    folder = '.\\\\test'\r\n    onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f))]  #you can use random arrays I guess\r\n    for file in onlyfiles:\r\n        data = np.load(join(folder, file))\r\n        data = np.expand_dims(data, axis=0)\r\n        yield [tf.dtypes.cast(data, tf.float32)]\r\n\r\ndef SOC(input_tensor, classNumer=1, epsilonBN=1e-3):\r\n    a = Input(shape=input_tensor)\r\n    x = Conv2D(16, (3, 3), padding='same', use_bias=False, name=\"Conv_1\")(a)\r\n    x = BatchNormalization(epsilon=epsilonBN, name=\"BN_1\")(x)\r\n    x=Activation('relu')(x)\r\n    x = MaxPooling2D(pool_size=(2, 2), name=\"MaxPool_1\")(x)\r\n\r\n    x = Conv2D(32, (3, 3), padding='same', use_bias=False, name=\"Conv_2\")(x)\r\n    x = BatchNormalization(epsilon=epsilonBN, name=\"BN_2\")(x)\r\n    x=Activation('relu')(x)\r\n    x = MaxPooling2D(pool_size=(2, 2), name=\"MaxPool_2\")(x)\r\n\r\n    x = Conv2D(64, (3, 3), padding='same', use_bias=False, name=\"Conv_3\")(x)\r\n    x = BatchNormalization(epsilon=epsilonBN, name=\"BN_3\")(x)\r\n    x=Activation('relu')(x) \r\n    x = MaxPooling2D(pool_size=(2, 2), name=\"MaxPool_3\")(x)\r\n\r\n    x = Conv2D(128, (3, 3), padding='same', use_bias=False, name=\"Conv_4\")(x)\r\n    x = BatchNormalization(epsilon=epsilonBN, name=\"BN_4\")(x)\r\n    x = Activation('relu')(x)\r\n    x = MaxPooling2D(pool_size=(2, 2), name=\"MaxPool_4\")(x)\r\n\r\n    x = Flatten()(x)\r\n    x = Dense(classNumer, kernel_initializer='uniform', name=\"Dense\")(x)\r\n    model = tf.keras.Model(inputs=a, outputs=x)\r\n    return model\r\n\r\nnew_model = SOC((32,32,5))\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(new_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset\r\ntflite_quant_model = converter.convert()\r\n\r\nwith open('qmodel.tflite', 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n````\r\n\r\n**System information**\r\n- OS: Windows 10 Pro\r\n- Desktop, Intel(R) Core(TM) i7-9700K CPU \r\n- Tensorflow 2.9 built from source (tested with tensoflow 2.8 built from source on linux too)\r\n- python 3.10\r\n- Bazel 5.1.0\r\n- Did not include gpu support on the tensorflow build in windows\r\n\r\n", "comments": ["Hi @Soleimani64 ! According to this [document](https://blog.tensorflow.org/2021/09/faster-quantized-inference-with-xnnpack.html#:~:text=server%2Dclass%20processors.-,How%20can%20you%20use%20it%3F,-Quantized%20XNNPACK%20inference), Limited support for operators with asymmetric quantization is available via the --define xnn_enable_qu8=true Bazel option. So **\"-define tflite_with_xnnpack=true --define xnn_enable_qs8=true\"** is  correct command to build with bazel here .\r\n\r\nAttaching relevant [thread](https://www.tensorflow.org/model_optimization/guide/pruning/pruning_for_on_device_inference) for reference.\r\nThanks!", "Hi @mohantym \r\nThanks for the reply. but I used  \"**-define tflite_with_xnnpack=true --define xnn_enable_qs8=true**\" to build tensorflow. I did **not** use --define xnn_enable_qu8=true", "@mohantym\r\nTo make it clear, the problem here is that I was expecting to see lower latency for quantized models when I use \"--define xnn_enable_qs8=true\" option. But the latency doesn't change at all. I was wondering if you can give me an example when \"--define xnn_enable_qs8=true\" actually reduces the latency of a quantized model.\r\n I assumed that with this option we can use fixed-point operations on a desktop cpu. Is that correct?", "Sorry @Soleimani64 ! According to this [document](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/README.md), **BatchNormalization** is not supported operator yet in XNNPack yet. I think you have to stay with **-define tflite_with_xnnpack=true** for now to use the default XNNPack engine . Thanks!", "Thanks @mohantym \r\nI tried without BatchNormalization, the latency became a little bit smaller with --define xnn_enable_qs8=true, but the difference is very small. I will try with bigger models. However, if I don't quantize the model, the latency is always much smaller than the quantized model!\r\nHow about my other question: does tensorflow do 8-bit operations when we activate --define xnn_enable_qs8=true, or it still uses the floating point (I mean on a desktop cpu)?\r\n", "Hi @sachinprasadhs ! Could you please look at this issue?", "@Soleimani64 use `--define tflite_with_xnnpack_qs8=true` and/or `--define tflite_with_xnnpack_qu8=true` Bazel flags.\r\nSee [XNNPack delegate documentation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack#quantized-operators) for details.", "@Maratyszcza I did that, it doesn't improve the latency.\r\n", "Do you use the latest version of TensorFlow Lite?\r\nDo you see \"Created TensorFlow Lite XNNPACK delegate for CPU.\" message when you run the model?", "@Maratyszcza , yes to both questions. \r\nI can see improvement in the latency of both tf32 and int8 models when comparing XNN pack with regular tensorflow. but I don't see any difference in the latency of the int8 models if I use --define tflite_with_xnnpack_qs8=true and --define tflite_with_xnnpack_qu8=true compared to when I don't use them.\r\n"]}, {"number": 55524, "title": "Run \"bazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\" failed", "body": "**System information**\r\n- CentOS Linux release 7.8.2003 (Core)\r\n\r\n\r\n**Describe the current behavior**\r\nbazel build --config=mkl --config=opt //tensorflow/tools/pip_package:build_pip_package --verbose_failures\r\n\r\n\r\nlogs:\r\n![image](https://user-images.githubusercontent.com/13997135/162117884-3aa73da3-d0bc-4a37-b991-c7d49f91697e.png)\r\n", "comments": ["@zhucan,\r\nHi, Thanks for reporting this issue. \r\nCould you provide the configuration  details like Bazel, Tensorflow version. Recommend to provide complete Traceback instead of screenshot. \r\n\r\nRun `bazel clean --expunge`, forcing bazel to re-download and build each dependency to avoid issue. Also sync the bazel build `bazel sync`. Thanks!", "@gadagashwini Thanks for your response. it's ok for me. But there is no \"build doc\" for centos7.8. And there is no docs build tensorflow(v2.8.0) with verbs?", "For tensorflow(v2.8.0) on the centos7.8, like this:\r\n\r\n1. sudo yum install python3-devel python3-pip\r\n2. pip install -U --user pip numpy wheel\r\n3. pip install -U --user keras_preprocessing --no-deps\r\n4. ./configure\r\n5. bazel build --distdir=/home/bazel_package //tensorflow/tools/pip_package:build_pip_package\r\nit's ok for me, but I don't know how to build the tensorflow with verbs. There is no config for it.\r\n![image](https://user-images.githubusercontent.com/13997135/162182353-b2ce9f8c-821d-4c40-a5e9-dedcff3aabd6.png)\r\n\r\n![image](https://user-images.githubusercontent.com/13997135/162182527-af81bfbf-1ec1-4c85-8e73-e92f9d61dc7d.png)\r\n\r\n", "@gadagashwini Can you help me to look at this issue.https://github.com/tensorflow/networking/issues/40", "ERROR: /home/deeproute/workspace/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:432:15: Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops_a_m.cc failed: (Exit 4): gcc failed: error executing command \r\n  (cd /root/.cache/bazel/_bazel_root/b0f1c7b7aacab844a054d97356b0d4dd/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    PATH=/root/.cargo/bin:/sbin:/bin:/usr/sbin:/usr/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/lib/python3.6/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  /bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops_a_m/tf_ops_a_m.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops_a_m/tf_ops_a_m.pic.o' -fPIC '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT=\".so\"' '-DLLVM_PLUGIN_EXT=\".so\"' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_GETSPECIFIC=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_LSEEK64=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=\"X86\"' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=\"x86_64-unknown-linux-gnu\"' '-DLLVM_DEFAULT_TARGET_TRIPLE=\"x86_64-unknown-linux-gnu\"' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote. -iquotebazel-out/k8-opt/bin -iquoteexternal/llvm-project -iquotebazel-out/k8-opt/bin/external/llvm-project -iquoteexternal/llvm_terminfo -iquotebazel-out/k8-opt/bin/external/llvm_terminfo -iquoteexternal/llvm_zlib -iquotebazel-out/k8-opt/bin/external/llvm_zlib -iquoteexternal/com_google_absl -iquotebazel-out/k8-opt/bin/external/com_google_absl -iquoteexternal/nsync -iquotebazel-out/k8-opt/bin/external/nsync -iquoteexternal/eigen_archive -iquotebazel-out/k8-opt/bin/external/eigen_archive -iquoteexternal/gif -iquotebazel-out/k8-opt/bin/external/gif -iquoteexternal/libjpeg_turbo -iquotebazel-out/k8-opt/bin/external/libjpeg_turbo -iquoteexternal/com_google_protobuf -iquotebazel-out/k8-opt/bin/external/com_google_protobuf -iquoteexternal/com_googlesource_code_re2 -iquotebazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquoteexternal/farmhash_archive -iquotebazel-out/k8-opt/bin/external/farmhash_archive -iquoteexternal/fft2d -iquotebazel-out/k8-opt/bin/external/fft2d -iquoteexternal/highwayhash -iquotebazel-out/k8-opt/bin/external/highwayhash -iquoteexternal/zlib -iquotebazel-out/k8-opt/bin/external/zlib -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypeInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SubElementInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticBaseIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticCanonicalizationIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithmeticOpsIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -w -DAUTOLOAD_DYNAMIC_KERNELS '-std=c++14' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c tensorflow/compiler/mlir/tensorflow/ir/tf_ops_a_m.cc -o bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops_a_m/tf_ops_a_m.pic.o)\r\nExecution platform: @local_execution_config_platform//:platform\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <http://bugzilla.redhat.com/bugzilla> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 1871.483s, Critical Path: 251.24s\r\nINFO: 1847 processes: 299 internal, 1548 local.\r\nFAILED: Build did NOT complete successfully\r\n@gadagashwini ", "@zhucan, For Cent OS, Tensorflow build instructions mentioned for [Ubuntu](https://www.tensorflow.org/install/source#ubuntu) can be used. \r\nYou can build Tensorflow with VERBS config\r\n`--config=verbs # Build with libverbs support.`. Thanks!", "The \"--config=verbs\" is not suitable for tensorflow 2.8.0 . @gadagashwini \r\n", "@zhucan, \r\n\r\nTensorflow 2.0 build supports `--config=verbs`.\r\n`config_info_line('verbs', 'Build with libverbs support.')` \r\nFor more information take a look [here](https://github.com/tensorflow/tensorflow/blob/r2.0/configure.py#L1581). Thanks!", "@gadagashwini I mean tensorflow2.8 not 2.0\r\n", "https://github.com/tensorflow/tensorflow/blob/r2.8/configure.py#L1581 @gadagashwini no verbs config.", "@zhucan, Tensorflow v2.8  has no `--config==verbs`.\r\nTensorflow v2.0 supports `--config==verbs`", "I know, so I want to know how to compile tensorflow2.8.0 with verbs..... @gadagashwini "]}, {"number": 55522, "title": "tflite gpu_delegate \"undefined symbol: glFenceSync\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 22.04 arm64\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.8.0\r\n- Python version: 3.10\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: Mali G72\r\n\r\n**Describe the current behavior**\r\ni compiled tflite from source to test the gpu / gl delegate in python with\r\n`bazel build -s -c opt --copt \"-DEGL_NO_X11\" --copt=\"-DMESA_EGL_NO_X11_HEADERS\" tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so`\r\n\r\ntflite inference and training works without loading the delegate, uses xnnpack delegate for cpu\r\n\r\nif loading the gpu delegate, i get following error:\r\nOSError: /home/linux/Documents/libtensorflowlite_gpu_delegate.so: undefined symbol: glFenceSync\r\n**UPDATE: the loading takes places via following code:\r\n`delegate = tf.lite.experimental.load_delegate('./libtensorflowlite_gpu_delegate.so')`\r\nthe error is exactly following:\r\nOSError                                   Traceback (most recent call last)\r\n/tmp/ipykernel_24874/1769046455.py in <module>\r\n----> 1 delegate = tf.lite.experimental.load_delegate('./libtensorflowlite_gpu_delegate.so')\r\n      2 model = tf.lite.Interpreter(model_content=tflite_model, experimental_delegates=[delegate])\r\n      3 \r\n      4 model.allocate_tensors()\r\n      5 infer = generator_lite.get_signature_runner(\"infer\")\r\n\r\n~/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py in load_delegate(library, options)\r\n    173   \"\"\"\r\n    174   try:\r\n--> 175     delegate = Delegate(library, options)\r\n    176   except ValueError as e:\r\n    177     raise ValueError('Failed to load delegate from {}\\n{}'.format(\r\n\r\n~/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py in __init__(self, library, options)\r\n     81                          'due to missing immediate reference counting.')\r\n     82 \r\n---> 83     self._library = ctypes.pydll.LoadLibrary(library)\r\n     84     self._library.tflite_plugin_create_delegate.argtypes = [\r\n     85         ctypes.POINTER(ctypes.c_char_p),\r\n\r\n/usr/lib/python3.10/ctypes/__init__.py in LoadLibrary(self, name)\r\n    450 \r\n    451     def LoadLibrary(self, name):\r\n--> 452         return self._dlltype(name)\r\n    453 \r\n    454     __class_getitem__ = classmethod(_types.GenericAlias)\r\n\r\n/usr/lib/python3.10/ctypes/__init__.py in __init__(self, name, mode, handle, use_errno, use_last_error, winmode)\r\n    372 \r\n    373         if handle is None:\r\n--> 374             self._handle = _dlopen(self._name, mode)\r\n    375         else:\r\n    376             self._handle = handle\r\n\r\n`OSError: ./libtensorflowlite_gpu_delegate.so: undefined symbol: glFenceSync`\r\n--> This is on Ubuntu 22.04 arm64\r\n--> on Ubuntu 22.04 x64 the Error is:\r\n`OSError: ./libtensorflowlite_gpu_delegate.so: undefined symbol: glDeleteBuffers` \r\n**\r\n\r\n**Describe the expected behavior**\r\nshould work with gpu delegate?\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["https://github.com/tensorflow/tensorflow/issues/45953", "Hi @BenjaminWegener ! \r\nSorry for the late response.  I see the issue is happening when you are loading libtensorflowlite_gpu_delegate.so (built using bazel in linux system).  Can you try below steps and let us know ?\r\n1. Install opengl dependencies in terminal \r\n```\r\nsudo apt-get install mesa-common-dev libegl1-mesa-dev libgles2-mesa-dev\r\nsudo apt-get install mesa-utilsglxinfo | grep -i opengl\r\n\r\n```\r\n2. Rebuild the gpu_delegate file  (Removed .so as you are not doing inference in C++ ,added monolthic as your are on pc )\r\n3. \r\n`bazel build  --config=monolithic -c opt --copt \"-DEGL_NO_X11\" --copt=\"-DMESA_EGL_NO_X11_HEADERS\" tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate`\r\n\r\n4. put the full path of the library containing libtensorflowlite_gpu_delegate.so in this line ( you can test this first as it is an OS error. put the .so file built with the location of library it contains)\r\n\r\n`delegate = tf.lite.experimental.load_delegate('./libtensorflowlite_gpu_delegate)`\r\n\r\nAttaching relevant[ thread](https://review.mlplatform.org/plugins/gitiles/ml/armnn/+/86723e67a940ed9f288c3cb59c120b9a10a1d590/delegate/IntegrateDelegateIntoPython.md) for reference. Thanks!", "Thanks for your response, that thread you attached looks promising.\r\nleaving the \".so\" from the command line yields following:\r\n`INFO: Found applicable config definition build:dynamic_kernels in file /home/linux/Documents/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nERROR: Skipping 'tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate': no such target '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate': target 'libtensorflowlite_gpu_delegate' not declared in package 'tensorflow/lite/delegates/gpu' (did you mean 'libtensorflowlite_gpu_delegate.so'?) defined by /home/linux/Documents/tensorflow/tensorflow/lite/delegates/gpu/BUILD\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such target '//tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate': target 'libtensorflowlite_gpu_delegate' not declared in package 'tensorflow/lite/delegates/gpu' (did you mean 'libtensorflowlite_gpu_delegate.so'?) defined by /home/linux/Documents/tensorflow/tensorflow/lite/delegates/gpu/BUILD\r\nINFO: Elapsed time: 0.370s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n`", "Ok @BenjaminWegener ! Please switch to the third step directly and let us know once you put the .so file with the library it contains. Thanks!", "step 3 produces the output mentioned in https://github.com/tensorflow/tensorflow/issues/55522#issuecomment-1091672405", "Hi @sachinprasadhs ! Could you please look at this issue?", "I don't think you can drop \".so\".  You need to build \"tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so\" AFAIK", "So how to build and import a GPU delegate in Python in Linux? Is it possible?"]}, {"number": 55520, "title": "Serialize TF graph after XLA compilation ", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.8.0\r\n\r\n**Describe the feature and the current behavior/state.**\r\nNot available\r\n**Will this change the current api? How?**\r\nPotentially\r\n\r\nHi, I was wondering if there is a way to serialize a TF graph after compiling with XLA. Typically we can serialize a graph as a proto file and load it afterwards. However, the graph execution is slow the first time we run it after loading presumably due to JIT compilation. Is there a way to store the graph so that there is no performance overhead during the first run after loading it? \r\n\r\nConsider the following example: \r\n[https://gist.github.com/pemoi1982jpm/bf3e6afe90551a824d0f9e9baa42d041](https://gist.github.com/pemoi1982jpm/bf3e6afe90551a824d0f9e9baa42d041)\r\n\r\nSo the question:  Is there a way to serialize the TF graph in a way that the first run is also fast after restoring it?  \r\n\r\nIn principle the graph was already compiled using XLA, Can we store it in some format so that XLA doesn\u2019t have to compile it again? \r\n\r\nAny help or guidance would be really helpful. \r\n", "comments": ["Is there any update on this issue?\r\nThank you"]}, {"number": 55519, "title": "Tensorflow for M1 mac", "body": "I have a macbook air m1. I am trying to use tensorflow (for darkflow) in my mac. I need to get tensorflow 1.13.2 but brew and conda always installs the latest version. Now it gives me error ' tensorflow.contrib' not found as it was removed in 1.14. \r\n\r\nI am importing this: 'from darkflow.net.build import TFNet'\r\n\r\nCould someone please explain what commands to replace for the new version.", "comments": ["@akshatmanohar21,\r\n\r\nYou can follow commands below to install Tensorflow=1.13.2\r\n\r\n```\r\nbrew install python3\r\npython3 -m vent tf\r\nsource tf/bin/activate\r\npython3 -m pip install -U pip\r\npip install tensorflow==1.13.2\r\n```", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}]