[{"number": 29539, "title": "[XLA][ROCm] Provide non default address space for Rocm global symbols", "body": "Since ROCm use address space 1 for global symbols, provide for a utility so targets can query for the address space that global symbols should reside in. \r\n", "comments": ["@jlebar Thanks for the comments! Please let me know if there are any more suggestions. "]}, {"number": 29538, "title": "Conv1D Example", "body": "Add Conv1D example with simple Sequential Model and input_shape", "comments": ["> Add line to import Sequential and Conv1D Layer\r\n\r\nI compared this to other examples, where it wasn't used. Is using imports canonical then?"]}, {"number": 29537, "title": "Conv1D example", "body": "Add Conv1D example with simple Sequential Model and input_shape", "comments": []}, {"number": 29536, "title": "TF2 Nightly-20190607 broken import", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf_nightly_2.0_preview-2.0.0.dev20190607\r\n- Python version:  Python2 / Python3\r\n\r\n**Describe the current behavior**\r\nTF2 nightly breaks with this error:\r\n```\r\nFile \"/usr/lib/python2.7/site-packages/tensorflow/__init__.py\", line 93, in <module>\r\n    from tensorflow_core import *\r\nAttributeError: 'module' object has no attribute 'compiler\r\n```\r\n\r\n**Code to reproduce the issue**\r\n```\r\npip install tf-nightly-2.0-preview\r\npython -c \"import tensorflow as tf\"\r\n```\r\n**Other info / logs**\r\nProbably related to the commit mentioned in https://github.com/tensorflow/tensorflow/issues/29532 but not positive.", "comments": ["Sorry about that, I'll fix today", "This is now fixed, I have 3 more other issues caused by the virtual pip that needs fixing but I'll keep an eye.\r\n\r\nPlease let me know if this still doesn't work.", "Hello I am getting this issue from TF 2.0 in Google Colab. The code was running previously, but all of a sudden, it started outputting this error as shown below.\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-e5f1f8798c12> in <module>()\r\n     11 #from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials\r\n     12 #import python_utils\r\n---> 13 import tensorflow as tf\r\n     14 import tensorflow_probability as tfp\r\n     15 tfd = tfp.distributions\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py in <module>()\r\n     91 \r\n     92 # We still need all the names that are toplevel on tensorflow_core\r\n---> 93 from tensorflow_core import *\r\n     94 \r\n     95 # We also need to bring in keras if available in tensorflow_core\r\n\r\nAttributeError: module 'tensorflow_core' has no attribute 'compiler'\r\n", "Hi @EziamaUgonna, install yesterday's nightly with `pip install tf-nightly-2.0-preview==2.0.0.dev20190606` or alternatively it should be fixed on tomorrow's version."]}, {"number": 29535, "title": "model.trainable=False does nothing in tensorflow keras", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.13\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1.105\r\n- GPU model and memory:m60,8gb\r\n\r\n\r\nIt seems setting `model.trainable=False` in tensorflow keras does nothing except for to print a wrong model.summary(). Here is the code to reproduce the issue:\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nIMG_SHAPE = (160, 160, 3)\r\n\r\n# Create the base model from the pre-trained model MobileNet V2\r\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n                                               include_top=False, \r\n                                               weights='imagenet')\r\nbase_model.trainable = False\r\n# for layer in base_model.layers:\r\n#     layer.trainable=False\r\nbc=[] #before compile\r\nac=[] #after compile\r\nfor layer in base_model.layers:\r\n    bc.append(layer.trainable)\r\nprint(np.all(bc)) #True\r\nprint(base_model.summary()) ##this changes to show no trainable parameters but that  is wrong given the output to previous np.all(bc)\r\nbase_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), \r\n              loss='categorical_crossentropy', \r\n              metrics=['accuracy'])\r\nfor layer in base_model.layers:\r\n    ac.append(layer.trainable)\r\nprint(np.all(ac)) #True\r\nprint(base_model.summary()) #this changes to show no trainable parameters but that  is wrong given the output to previous np.all(ac)\r\n```\r\n", "comments": ["tf.keras.models.Model documentation does not mention this attribute.\r\n\r\n```python\r\n>>> hasattr(base_model, 'trainable')\r\nTrue\r\n```\r\ntf.keras.models.Model inherits attribute from Network.\r\nNetwork inherits the attribute from Layer\r\nAttribute seems to be defined here:\r\nhttps://github.com/tensorflow/tensorflow/blob/9699b90e16b3538168dde330e1ac8745fe8b73b8/tensorflow/python/keras/engine/network.py#L194-L195\r\n\r\nand here:\r\nhttps://github.com/tensorflow/tensorflow/blob/c572ea2806176d036e4d81b5de22f7251449d371/tensorflow/python/keras/engine/base_layer.py#L164-L166", "Setting the `trainable` attribute to False affects the models' `trainable_weights` attribute.", "https://github.com/tensorflow/tensorflow/blob/c572ea2806176d036e4d81b5de22f7251449d371/tensorflow/python/keras/engine/base_layer.py#L766-L772\r\n\r\nThis Property seems to be the cause", "When trainable is set to false, the trainable_weights property resolve to empty list. This causes the layer summary to display the parameter count to display incorrectly.\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/9699b90e16b3538168dde330e1ac8745fe8b73b8/tensorflow/python/keras/utils/layer_utils.py#L234-L239", "setting `model.trainable=False` is the suggested way in even the Tensorflow documentation for transfer learning https://www.tensorflow.org/tutorials/images/transfer_learning", "> When trainable is set to false, the trainable_weights property resolve to empty list. This causes the layer summary to display the parameter count to display incorrectly.\r\n> It is unclear to me how trainable_weights being set to an empty list makes layer summary to display the parameter count incorrectly. Can you explain a bit more please?\r\n> https://github.com/tensorflow/tensorflow/blob/9699b90e16b3538168dde330e1ac8745fe8b73b8/tensorflow/python/keras/utils/layer_utils.py#L234-L239\r\n\r\n", "I think the problem is actually that the individual layers in the model stay trainable when the model is set to be non trainable.\r\n\r\n```python\r\n>>> import tensorflow as tf\r\n>>> base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\r\n                                               include_top=False, \r\n                                               weights='imagenet')\r\n>>> base_model.trainable = False\r\n>>> base_model.layers[0].trainable\r\nTrue\r\n```", "i mentioned that in the code bit i posted as minimal reproducible example. All layers remain trainable both before and after compiling the model", "Tried on Colab with TF 1.13.1 and was able to get mentioned model.summary with trainable params as 0.", "I can confirm that all three `model.trainable=False`, `layer.trainable=False for layer in model.layers` and `model.trainable=True` result in different behavior", "https://github.com/tensorflow/tensorflow/blob/c572ea2806176d036e4d81b5de22f7251449d371/tensorflow/python/keras/engine/base_layer.py#L750-L754\r\n\r\nThis observer pattern does not seem to be functioning as expected.", "Any update on this?", "This is fixed in 1.14, can you update the version and try it again?", "@Nitinsiwach Please check a gist with 1.14 [here](https://colab.sandbox.google.com/gist/jvishnuvardhan/936cbc0184d3ae96bdc682f34a32d9a2/tf29535_model_tranables.ipynb). Thanks!", "Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29535\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29535\">No</a>\n", "Encountered this error in tensorflow 1.14 from a colab gpu runtime with the following code:\r\n\r\n`import tensorflow as tf\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import (Dense, Activation, BatchNormalization, \r\n                                     Reshape, Conv2D, LeakyReLU, Input)\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nprint(tf.__version__)\r\n\r\nnetwork = Sequential([\r\n          Conv2D(64, 4, strides=(2, 2), padding='same', input_shape=(28,28,1)),\r\n          LeakyReLU(),\r\n          Conv2D(128, 4, strides=(2, 2), padding='same'),\r\n          BatchNormalization(axis=-1),\r\n          LeakyReLU(),\r\n          Reshape((7*7*128,)),\r\n          Dense(1024),\r\n          BatchNormalization(axis=-1),\r\n          LeakyReLU(),\r\n          Dense(1),\r\n          Activation('sigmoid')\r\n      ])\r\n\r\nnetwork.compile(loss='binary_crossentropy', optimizer=Adam(2e-4, beta_1=0.5))\r\nnetwork.summary()\r\nnetwork.trainable = False\r\nnetwork.summary()`", "@aladoro Please create a new issue by providing your platform details and the standalone code. Thanks!", "```\r\nfrom keras.models import Model\r\nfrom keras.applications.vgg16 import VGG16\r\n\r\nmodel = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\r\nmodel.trainable = False\r\n\r\nmodel.summary()\r\n\r\nmodel = Model(inputs=model.input, outputs=model.output)\r\n\r\nmodel.summary()\r\n```\r\nThis first summary shows there is no trainable weights. Wheras second summary shows all weights are trainable. \r\n\r\n```\r\nfor layer in model.layers:\r\n        layer.trainable = False\r\n```\r\nIf doing it this way, both summaries show no trainable weights.", "@don-tpanic Please create a new issue with more details on the issue and also provide platform details, version used etc. Thanks!", "> ```\r\n> from keras.models import Model\r\n> from keras.applications.vgg16 import VGG16\r\n> \r\n> model = VGG16(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\r\n> model.trainable = False\r\n> \r\n> model.summary()\r\n> \r\n> model = Model(inputs=model.input, outputs=model.output)\r\n> \r\n> model.summary()\r\n> ```\r\n> \r\n> This first summary shows there is no trainable weights. Wheras second summary shows all weights are trainable.\r\n> \r\n> ```\r\n> for layer in model.layers:\r\n>         layer.trainable = False\r\n> ```\r\n> \r\n> If doing it this way, both summaries show no trainable weights.\r\n\r\nyes this is because confusingly, the behavior of model.trainable is different in keras vs tf.keras: in keras it only impacts the variable of the model layer, without impacting the variable of all the sub layers, contrary to what happens in tf.keras. (Is it expected to have such different behavior between keras and tf.keras??)\r\nIn the second summary, you effectively creates a new model which set model.trainable = True by default, which explains the different behavior.\r\nI find setting the trainable flag of the whole model very confusing, so I now only set it per sub layers. For example in tf.keras, if you do model.trainable = False and then model.layers[-4].trainable = True, the latter instruction has no effect as it does not change the trainable flag at the top of the model.", "@babaish: Thank you, good point, useful to know!"]}, {"number": 29534, "title": "1.11 and above: optimizers function incorrectly with datasets with unbalanced labels", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.10 (16.04 has the same issue)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): `('v1.11.0-0-gc19e29306c', '1.11.0')`, `('v1.12.0-0-ga6d8ffae09', '1.12.0')`, `('v1.13.1-0-g6612da8951', '1.13.1')`\r\n- Python version: 2.7, 3.5, 3.6, 3.7\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: - (reproducable with GPU explicitly disabled)\r\n\r\n**Describe the current behavior**\r\n- Reference version: `('v1.10.0-0-g656e7a2b34', '1.10.0')`\r\n\r\nWe are experiencing the issue on our models, that used to be trained using Tensorflow 1.10, where there was no issue, but since we upgraded to 1.13, it appeared.\r\nI have constructed an artificial, as simple as possible example, showcasing the issue.\r\n\r\nThe example in the attached Zip file is using `imports85` data. The \"True\" label is `price < PRICE_CUTOFF`, with `PRICE_CUTOFF` set so that the resulting label mean is `0.0146`, making it significantly unbalanced.\r\n\r\nThe FTRL optimizer in the example code has bizarre coefficients set to demonstrate the issue more graphically. In our models we have them in reasonable range and still get the wrong behavior.\r\n\r\nIf the model is trained and evaluated using *1.10*, the results (given all the constants as set in the code) are the following:\r\n```\r\nResulting means:\r\n  Label: 0.0146341463415\r\n  Prediction (no optimizer): 0.023864556\r\n  Prediction (optimizer): 0.026022965\r\n```\r\n\r\nNow, the results above are pretty bad, but it's due to really small dataset, used to train the model, but it's not significant from the standpoint of showcasing wrong behavior: in our models, the predictions, done with 1.10-trained model are pretty on-point.\r\n\r\nWhen the model is trained and evaluated using *1.13*, the model *WITH* optimizer applied starts to show unreasonably high predictions:\r\n```\r\nResulting means:\r\n  Label: 0.0146341463415\r\n  Prediction (no optimizer): 0.02390607\r\n  Prediction (optimizer): 0.044271074\r\n``` \r\nThese results above are consistent with what we see in our models, where predictions became overwhelmingly positive once 1.13 was used for the model training (since we do use optimizers).\r\n\r\nThis does not seem to be an issue with FTRL optimizer itself, since we have tried several of them, all showing significant skewing of predicted results towards one or other end, if the label in the dataset is not balanced.\r\n\r\nThe example code should allow easy reproduction of the issue. Zip file also contains two requirements files: `requirements_1.10.txt` and `requirements_1.13.txt` to streamline setting up the virtual environments to test the issue.\r\n\r\n**Code to reproduce the issue**\r\nSee attached Zip\r\n[optimizers_unbalanced_label_bug.zip](https://github.com/tensorflow/tensorflow/files/3266358/optimizers_unbalanced_label_bug.zip)\r\n", "comments": ["I could test with the other TF versions to better identify, where stuff went south - and it seems to be `1.11` (I have tried `('v1.11.0-0-gc19e29306c', '1.11.0')` specifically) version. For it, the results are much like for 1.13:\r\n```\r\nResulting means:\r\n  Label: 0.0146341463415\r\n  Prediction (no optimizer): 0.023945015\r\n  Prediction (optimizer): 0.044300616\r\n```\r\nWasn't it a version, where some new graph optimization mechanisms were introduced?\r\n\r\n\r\n", "...and the same in 1.12 (`('v1.12.0-0-ga6d8ffae09', '1.12.0')`):\r\n```\r\nResulting means:\r\n  Label: 0.0146341463415\r\n  Prediction (no optimizer): 0.023909163\r\n  Prediction (optimizer): 0.044271577\r\n```\r\n\r\nApparently, something was introduced in 1.11 and broke stuff.", "@VOvchinnikov I tried reproducing issue with the provided code ,but i am unable to reproduce .Will it be possible to provide minimal code snippet which can reproduce the reported issue.Thanks ", "Closing the issue, because the observed effect has been caused by this: https://github.com/tensorflow/tensorflow/issues/18317\r\n\r\nThe deviation of prediction mean, when using other optimizers, was caused by my mistake during setup of the experiment.\r\n"]}, {"number": 29533, "title": "[ROCm] Adding ROCm support for tensor array ops", "body": "This PR adds ROCm support for tensor array ops\r\n\r\nThe changes in this PR are trivial, please review and merge...thanks.\r\n\r\n----------------------------------------\r\n\r\n@tatianashp @whchung ", "comments": []}, {"number": 29532, "title": "Virtual TF package breaks tf.sysconfig based include path", "body": "As f1ffa0225ae19870c1473b1d70faf0ceaeea9862 landed yesterday, the framework library is now moved from `tensorflow` to `tensorflow_core` and the include path also changed to `tensorflow_core/include`.\r\n\r\n`tf.sysconfig.get_include` and `tf.sysconfig.get_lib` should be changed accordingly but they are not. This breaks downstream packages such as horovod and tensorflow_io that build custom ops and link to TF. It also means any binary distributions of these packages will break binary backward compatibility with this change.\r\n\r\nSee https://github.com/horovod/horovod/issues/1129 for a reproducing procedure.\r\n\r\nPing @mihaimaruseac @yifeif ", "comments": ["Sorry about that, will fix today.", "I think this should be fixed now, but let me know if not and I missed something."]}, {"number": 29531, "title": "model.trainable=False and layer.trainable=False for each layer giving very different performance", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version (use command below):1.3\r\n- Python version:2.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1.105\r\n- GPU model and memory:m60, 8gb\r\n\r\n`for layer in base_model_seq.layers:`\r\n `    layer.trainable=False` \r\nand \r\n`base_model_seq.trainable=False` \r\nare giving very different results. To my understanding both are the ways to achieve same behavior\r\n\r\nData pipeline: This remains exactly same in both the cases. Providing the code for reference\r\n```\r\nimport tensorflow as tf\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\nimg_height = 224\r\nimg_width = 224\r\nfrom random_eraser import get_random_eraser\r\nIMAGE_DIR = \"./data/images\"\r\ntraincsv = pd.read_csv('./data/train.csv')\r\ntestcsv = pd.read_csv('./data/test_ApKoW4T.csv')\r\ntraincsv['category'] = traincsv['category'].astype(str)\r\ntrain_batch_size=32\r\nval_batch_size = 32\r\ntest_batch_size = 32\r\nseed = 43\r\ntraincsv = pd.read_csv('./data/train.csv')\r\ntestcsv = pd.read_csv('./data/test_ApKoW4T.csv')\r\ntraincsv['category'] = traincsv['category'].astype(str)\r\ndf_train,df_val = train_test_split(traincsv, stratify = traincsv.category, random_state=42, test_size=.1, shuffle=True)\r\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,width_shift_range=0.3, height_shift_range=0.3, \r\n                                                            rescale=1./255, shear_range=0.0,zoom_range=0.3, horizontal_flip=True, fill_mode='nearest',\r\n                                                            preprocessing_function=get_random_eraser(pixel_level=True,s_h=0.4))\r\n\r\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\r\n\r\ntrain_generator = train_datagen.flow_from_dataframe(df_train,directory = IMAGE_DIR,x_col='image',y_col='category',\r\n           class_mode=\"categorical\", target_size=(img_height, img_width), batch_size=train_batch_size,shuffle=True)\r\n\r\nvalidation_generator = val_datagen.flow_from_dataframe(df_val,directory=IMAGE_DIR,x_col='image',y_col='category',\r\n           class_mode=\"categorical\", target_size=(img_height, img_width),batch_size=val_batch_size,shuffle=True)\r\n\r\nsteps_per_epoch = train_generator.n // 32\r\nvalidation_steps =  validation_generator.n //32\r\n```\r\nNow the piece followed by ######... is the only part i change in running the two experiments but i get drastically different results\r\n```\r\nimport tensorflow as tf\r\n\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nprint(\"TensorFlow version is \", tf.__version__)\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.model_selection import StratifiedKFold\r\nfrom tensorflow.keras.applications import MobileNet, InceptionResNetV2\r\nfrom tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\r\nfrom tensorflow.python.keras.applications import densenet_rootexposed\r\nimport tensorflow as tf\r\n\r\nIMAGE_DIR = \"./data/images\"\r\nimg_height =224\r\nimg_width = 224\r\nIMG_SHAPE = (img_height,img_width, 3)\r\ndef step_decay(epoch, lr):\r\n    print(lr)\r\n    drop = 0.96\r\n    return lr*drop\r\nlrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\r\ncallbacks = [lrate]\r\nbase_model_seq = keras.applications.densenet.DenseNet201(input_shape=IMG_SHAPE,\r\n                                              include_top=False, \r\n                                                weights='imagenet')\r\n#######################################This is the only part i change while training the models\r\nbase_model_seq.trainable=False\r\n# for layer in base_model_seq.layers:\r\n#     layer.trainable=False\r\n#######################################\r\nmodel_seq = tf.keras.Sequential([\r\n  base_model_seq,\r\n  keras.layers.Flatten(),\r\n  keras.layers.Dropout(0.5),\r\n  keras.layers.Dense(2048, activation='relu'),\r\n  keras.layers.Dropout(0.5),\r\n  keras.layers.BatchNormalization(),\r\n  keras.layers.Dense(5, activation='softmax')\r\n])\r\n\r\n\r\nfor layer in base_model_seq.layers:\r\n    layer._name = layer._name + str(\"_1729\")\r\nprint(len(model_seq.trainable_variables))\r\n\r\n\r\nmodel_seq.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), \r\n              loss='categorical_crossentropy', \r\n              metrics=['accuracy'])\r\n\r\nprint('************************')\r\nprint(model_seq.summary())\r\nepochs = 100\r\n\r\n```\r\n\r\nwhen i run it by freezing the model with `base_model_seq.trainable=False` the model starts with .8 val accuracy and rises up to .91 in first 20 epochs but when i run it by freezing the model with\r\n` for layer in base_model_seq.layers:\r\n     layer.trainable=False` \r\nThe validation accuracy starts with .72 and reaches .82 in 20 epochs and then stalls there. I have repeated the experiment enough number of times to ensure this cannot be attributed to chance initialization. Please point me to the source of inconsistency in the approaches i have describes", "comments": ["@Nitinsiwach Is this resolved? I know you had opened another similar issue https://github.com/tensorflow/tensorflow/issues/29535. If it was not resolved, please provide a standalone code to reproduce the issue. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29531\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29531\">No</a>\n"]}, {"number": 29530, "title": "tf.keras.Model serialization in tf1.14.0rc0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): pip \r\n- TensorFlow version (use command below):  1.14.0rc0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI am encountering serialization issues when trying to dump the `config` from a `tf.keras.Model` object without complex things like custom layers (or even Lambdas...)\r\n\r\nThe code worked well with tf 1.13.1 however in tf 1.1.4, json/yaml serialization fails, and `to_yaml` and `model_from_yaml` fails as well\r\n\r\n**Describe the expected behavior**\r\n\r\nWe should be able to serialize the config from a `tf.keras.Model` into json/yaml and load it back, especially without using custom layers\r\n\r\nI checked the changelog for 1.14 and did not see any documented changes on model serialization for tf 1.14\r\n\r\nIs serializing just the config file something that should not be done in 1.14 / 2.0 ?\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n# %%\r\nimport tensorflow as tf\r\nimport json\r\nfrom ruamel import yaml\r\n\r\n\r\n# %%\r\ndef mini_cnn(input_shape, num_classes):\r\n    inputs = tf.keras.layers.Input(shape=input_shape)\r\n\r\n    x = tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", use_bias=False)(inputs)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(64, use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.Dropout(0.25)(x)\r\n\r\n    logits = tf.keras.layers.Dense(num_classes, use_bias=True, name=\"logits\")(x)\r\n    probas = tf.keras.layers.Activation(activation=\"softmax\", name=\"probas\")(logits)\r\n\r\n    model = tf.keras.Model(inputs=inputs, outputs=probas, name=\"model\")\r\n\r\n    return model\r\n\r\n\r\n# %%\r\nm = mini_cnn((64, 64, 3), 2)\r\n\r\n## %%\r\ntry:\r\n    c = m.get_config()\r\n    m2 = tf.keras.models.model_from_config(c)\r\nexcept:\r\n    print(\"c error\")\r\n\r\ntry:\r\n    y = m.to_yaml()\r\n    m2 = tf.keras.models.model_from_yaml(y)\r\nexcept:\r\n    print(\"y error\")\r\n\r\ntry:\r\n    j = m.to_json()\r\n    m2 = tf.keras.models.model_from_json(j)\r\nexcept:\r\n    print(\"j error\")\r\n```\r\n\r\n**Other info / logs**\r\n\r\nYAML traceback (seems to come from batchnorm)\r\n```\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 49, in <module>\r\n    m2 = tf.keras.models.model_from_yaml(y)\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py\", line 78, in model_from_yaml\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 89, in deserialize\r\n    printable_module_name='layer')\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 192, in deserialize_keras_object\r\n    list(custom_objects.items())))\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1131, in from_config\r\n    process_node(layer, node_data)\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\", line 1087, in process_node\r\n    layer(flat_input_tensors[0], **kwargs)\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 591, in __call__\r\n    self._maybe_build(inputs)\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\", line 1881, in _maybe_build\r\n    self.build(input_shapes)\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/layers/normalization.py\", line 286, in build\r\n    raise ValueError('Duplicate axis: %s' % self.axis)\r\nValueError: Duplicate axis: ListWrapper([3, 3])\r\nNone\r\n```\r\n\r\nconfig error\r\n```\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 42, in <module>\r\n    m2 = tf.keras.models.model_from_config(c)\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py\", line 55, in model_from_config\r\n    return deserialize(config, custom_objects=custom_objects)\r\n  File \"/my-env/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py\", line 81, in deserialize\r\n    layer_class_name = config['class_name']\r\nKeyError: 'class_name'\r\nNone\r\n\r\n```\r\n\r\nhere is the yaml file that was generated:\r\n\r\n```yaml\r\nbackend: tensorflow\r\nclass_name: Model\r\nconfig:\r\n  input_layers:\r\n  - [input_1, 0, 0]\r\n  layers:\r\n  - class_name: InputLayer\r\n    config:\r\n      batch_input_shape: !!python/tuple [null, 64, 64, 3]\r\n      dtype: float32\r\n      name: input_1\r\n      sparse: false\r\n    inbound_nodes: []\r\n    name: input_1\r\n  - class_name: Conv2D\r\n    config:\r\n      activation: linear\r\n      activity_regularizer: null\r\n      bias_constraint: null\r\n      bias_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      bias_regularizer: null\r\n      data_format: channels_last\r\n      dilation_rate: &id001 !!python/tuple [1, 1]\r\n      dtype: float32\r\n      filters: 16\r\n      kernel_constraint: null\r\n      kernel_initializer:\r\n        class_name: GlorotUniform\r\n        config: {dtype: float32, seed: null}\r\n      kernel_regularizer: null\r\n      kernel_size: !!python/tuple [3, 3]\r\n      name: conv2d\r\n      padding: same\r\n      strides: &id002 !!python/tuple [1, 1]\r\n      trainable: true\r\n      use_bias: false\r\n    inbound_nodes:\r\n    - - - input_1\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: conv2d\r\n  - class_name: BatchNormalization\r\n    config:\r\n      axis: !!python/object/new:tensorflow.python.training.tracking.data_structures._ListWrapper\r\n        listitems: [3]\r\n        state:\r\n          _external_modification: false\r\n          _last_wrapped_list_snapshot: [3]\r\n          _non_append_mutation: false\r\n          _self_extra_variables: []\r\n          _self_trainable: true\r\n          _storage: [3]\r\n      beta_constraint: null\r\n      beta_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      beta_regularizer: null\r\n      center: true\r\n      dtype: float32\r\n      epsilon: 0.001\r\n      gamma_constraint: null\r\n      gamma_initializer:\r\n        class_name: Ones\r\n        config: {dtype: float32}\r\n      gamma_regularizer: null\r\n      momentum: 0.99\r\n      moving_mean_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      moving_variance_initializer:\r\n        class_name: Ones\r\n        config: {dtype: float32}\r\n      name: batch_normalization\r\n      scale: true\r\n      trainable: true\r\n    inbound_nodes:\r\n    - - - conv2d\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: batch_normalization\r\n  - class_name: Activation\r\n    config: {activation: relu, dtype: float32, name: activation, trainable: true}\r\n    inbound_nodes:\r\n    - - - batch_normalization\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: activation\r\n  - class_name: MaxPooling2D\r\n    config:\r\n      data_format: channels_last\r\n      dtype: float32\r\n      name: max_pooling2d\r\n      padding: valid\r\n      pool_size: !!python/tuple [2, 2]\r\n      strides: !!python/tuple [2, 2]\r\n      trainable: true\r\n    inbound_nodes:\r\n    - - - activation\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: max_pooling2d\r\n  - class_name: Conv2D\r\n    config:\r\n      activation: linear\r\n      activity_regularizer: null\r\n      bias_constraint: null\r\n      bias_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      bias_regularizer: null\r\n      data_format: channels_last\r\n      dilation_rate: *id001\r\n      dtype: float32\r\n      filters: 32\r\n      kernel_constraint: null\r\n      kernel_initializer:\r\n        class_name: GlorotUniform\r\n        config: {dtype: float32, seed: null}\r\n      kernel_regularizer: null\r\n      kernel_size: !!python/tuple [3, 3]\r\n      name: conv2d_1\r\n      padding: same\r\n      strides: *id002\r\n      trainable: true\r\n      use_bias: false\r\n    inbound_nodes:\r\n    - - - max_pooling2d\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: conv2d_1\r\n  - class_name: BatchNormalization\r\n    config:\r\n      axis: !!python/object/new:tensorflow.python.training.tracking.data_structures._ListWrapper\r\n        listitems: [3]\r\n        state:\r\n          _external_modification: false\r\n          _last_wrapped_list_snapshot: [3]\r\n          _non_append_mutation: false\r\n          _self_extra_variables: []\r\n          _self_trainable: true\r\n          _storage: [3]\r\n      beta_constraint: null\r\n      beta_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      beta_regularizer: null\r\n      center: true\r\n      dtype: float32\r\n      epsilon: 0.001\r\n      gamma_constraint: null\r\n      gamma_initializer:\r\n        class_name: Ones\r\n        config: {dtype: float32}\r\n      gamma_regularizer: null\r\n      momentum: 0.99\r\n      moving_mean_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      moving_variance_initializer:\r\n        class_name: Ones\r\n        config: {dtype: float32}\r\n      name: batch_normalization_1\r\n      scale: true\r\n      trainable: true\r\n    inbound_nodes:\r\n    - - - conv2d_1\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: batch_normalization_1\r\n  - class_name: Activation\r\n    config: {activation: relu, dtype: float32, name: activation_1, trainable: true}\r\n    inbound_nodes:\r\n    - - - batch_normalization_1\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: activation_1\r\n  - class_name: MaxPooling2D\r\n    config:\r\n      data_format: channels_last\r\n      dtype: float32\r\n      name: max_pooling2d_1\r\n      padding: valid\r\n      pool_size: !!python/tuple [2, 2]\r\n      strides: !!python/tuple [2, 2]\r\n      trainable: true\r\n    inbound_nodes:\r\n    - - - activation_1\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: max_pooling2d_1\r\n  - class_name: Conv2D\r\n    config:\r\n      activation: linear\r\n      activity_regularizer: null\r\n      bias_constraint: null\r\n      bias_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      bias_regularizer: null\r\n      data_format: channels_last\r\n      dilation_rate: *id001\r\n      dtype: float32\r\n      filters: 64\r\n      kernel_constraint: null\r\n      kernel_initializer:\r\n        class_name: GlorotUniform\r\n        config: {dtype: float32, seed: null}\r\n      kernel_regularizer: null\r\n      kernel_size: !!python/tuple [3, 3]\r\n      name: conv2d_2\r\n      padding: same\r\n      strides: *id002\r\n      trainable: true\r\n      use_bias: false\r\n    inbound_nodes:\r\n    - - - max_pooling2d_1\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: conv2d_2\r\n  - class_name: BatchNormalization\r\n    config:\r\n      axis: !!python/object/new:tensorflow.python.training.tracking.data_structures._ListWrapper\r\n        listitems: [3]\r\n        state:\r\n          _external_modification: false\r\n          _last_wrapped_list_snapshot: [3]\r\n          _non_append_mutation: false\r\n          _self_extra_variables: []\r\n          _self_trainable: true\r\n          _storage: [3]\r\n      beta_constraint: null\r\n      beta_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      beta_regularizer: null\r\n      center: true\r\n      dtype: float32\r\n      epsilon: 0.001\r\n      gamma_constraint: null\r\n      gamma_initializer:\r\n        class_name: Ones\r\n        config: {dtype: float32}\r\n      gamma_regularizer: null\r\n      momentum: 0.99\r\n      moving_mean_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      moving_variance_initializer:\r\n        class_name: Ones\r\n        config: {dtype: float32}\r\n      name: batch_normalization_2\r\n      scale: true\r\n      trainable: true\r\n    inbound_nodes:\r\n    - - - conv2d_2\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: batch_normalization_2\r\n  - class_name: Activation\r\n    config: {activation: relu, dtype: float32, name: activation_2, trainable: true}\r\n    inbound_nodes:\r\n    - - - batch_normalization_2\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: activation_2\r\n  - class_name: MaxPooling2D\r\n    config:\r\n      data_format: channels_last\r\n      dtype: float32\r\n      name: max_pooling2d_2\r\n      padding: valid\r\n      pool_size: !!python/tuple [2, 2]\r\n      strides: !!python/tuple [2, 2]\r\n      trainable: true\r\n    inbound_nodes:\r\n    - - - activation_2\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: max_pooling2d_2\r\n  - class_name: Flatten\r\n    config: {data_format: channels_last, dtype: float32, name: flatten, trainable: true}\r\n    inbound_nodes:\r\n    - - - max_pooling2d_2\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: flatten\r\n  - class_name: Dense\r\n    config:\r\n      activation: linear\r\n      activity_regularizer: null\r\n      bias_constraint: null\r\n      bias_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      bias_regularizer: null\r\n      dtype: float32\r\n      kernel_constraint: null\r\n      kernel_initializer:\r\n        class_name: GlorotUniform\r\n        config: {dtype: float32, seed: null}\r\n      kernel_regularizer: null\r\n      name: dense\r\n      trainable: true\r\n      units: 64\r\n      use_bias: false\r\n    inbound_nodes:\r\n    - - - flatten\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: dense\r\n  - class_name: BatchNormalization\r\n    config:\r\n      axis: !!python/object/new:tensorflow.python.training.tracking.data_structures._ListWrapper\r\n        listitems: [1]\r\n        state:\r\n          _external_modification: false\r\n          _last_wrapped_list_snapshot: [1]\r\n          _non_append_mutation: false\r\n          _self_extra_variables: []\r\n          _self_trainable: true\r\n          _storage: [1]\r\n      beta_constraint: null\r\n      beta_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      beta_regularizer: null\r\n      center: true\r\n      dtype: float32\r\n      epsilon: 0.001\r\n      gamma_constraint: null\r\n      gamma_initializer:\r\n        class_name: Ones\r\n        config: {dtype: float32}\r\n      gamma_regularizer: null\r\n      momentum: 0.99\r\n      moving_mean_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      moving_variance_initializer:\r\n        class_name: Ones\r\n        config: {dtype: float32}\r\n      name: batch_normalization_3\r\n      scale: true\r\n      trainable: true\r\n    inbound_nodes:\r\n    - - - dense\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: batch_normalization_3\r\n  - class_name: Activation\r\n    config: {activation: relu, dtype: float32, name: activation_3, trainable: true}\r\n    inbound_nodes:\r\n    - - - batch_normalization_3\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: activation_3\r\n  - class_name: Dropout\r\n    config: {dtype: float32, name: dropout, noise_shape: null, rate: 0.25, seed: null,\r\n      trainable: true}\r\n    inbound_nodes:\r\n    - - - activation_3\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: dropout\r\n  - class_name: Dense\r\n    config:\r\n      activation: linear\r\n      activity_regularizer: null\r\n      bias_constraint: null\r\n      bias_initializer:\r\n        class_name: Zeros\r\n        config: {dtype: float32}\r\n      bias_regularizer: null\r\n      dtype: float32\r\n      kernel_constraint: null\r\n      kernel_initializer:\r\n        class_name: GlorotUniform\r\n        config: {dtype: float32, seed: null}\r\n      kernel_regularizer: null\r\n      name: logits\r\n      trainable: true\r\n      units: 2\r\n      use_bias: true\r\n    inbound_nodes:\r\n    - - - dropout\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: logits\r\n  - class_name: Activation\r\n    config: {activation: softmax, dtype: float32, name: probas, trainable: true}\r\n    inbound_nodes:\r\n    - - - logits\r\n        - 0\r\n        - 0\r\n        - {}\r\n    name: probas\r\n  name: model\r\n  output_layers:\r\n  - [probas, 0, 0]\r\nkeras_version: 2.2.4-tf\r\n```\r\n\r\nOne can see the error with ListWrapper\r\n\r\nNote:\r\n\r\nLambda layers serialization seem to fail, whereas they worked in tf1.13 as well", "comments": ["@fchouteau I tried reproducing the issue with Tensorflow 1.14.0-rc0 but i did not receive error. I got the following output:\r\n```\r\nc error\r\ny error\r\n```\r\nLet us know if that is expected behavior. Thanks! ", "Hello !\r\n\r\nI forgot to include the traceback in the reproducibility script\r\n\r\n```python\r\n# %%\r\nimport traceback\r\n\r\nimport tensorflow as tf\r\n\r\n\r\n# %%\r\ndef mini_cnn(input_shape, num_classes):\r\n    inputs = tf.keras.layers.Input(shape=input_shape)\r\n\r\n    x = tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", use_bias=False)(inputs)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(64, use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.Dropout(0.25)(x)\r\n\r\n    logits = tf.keras.layers.Dense(num_classes, use_bias=True, name=\"logits\")(x)\r\n    probas = tf.keras.layers.Activation(activation=\"softmax\", name=\"probas\")(logits)\r\n\r\n    model = tf.keras.Model(inputs=inputs, outputs=probas, name=\"model\")\r\n\r\n    return model\r\n\r\n\r\ndef mini_cnn_with_lambda(input_shape, num_classes):\r\n    inputs = tf.keras.layers.Input(shape=input_shape)\r\n\r\n    x = tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", use_bias=False)(inputs)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Conv2D(32, (3, 3), padding=\"same\", use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.MaxPooling2D((2, 2), strides=2)(x)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tf.keras.layers.Dense(64, use_bias=False)(x)\r\n    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    x = tf.keras.layers.Dropout(0.25)(x)\r\n    x = tf.keras.layers.Lambda(lambda x: 2. * x)(x)\r\n\r\n    logits = tf.keras.layers.Dense(num_classes, use_bias=True, name=\"logits\")(x)\r\n    probas = tf.keras.layers.Activation(activation=\"softmax\", name=\"probas\")(logits)\r\n\r\n    model = tf.keras.Model(inputs=inputs, outputs=probas, name=\"model\")\r\n\r\n    return model\r\n\r\n\r\n# %%\r\nm = mini_cnn_with_lambda((64, 64, 3), 2)\r\n\r\n## %%\r\ntry:\r\n    c = m.get_config()\r\n    m2 = tf.keras.models.model_from_config(c)\r\n    error_c = \"Success on dict loading\"\r\nexcept:\r\n    error_c = \"Error on loading model from config\\n\"\r\n    error_c += traceback.format_exc()\r\n\r\ntry:\r\n    y = m.to_yaml()\r\n    m2 = tf.keras.models.model_from_yaml(y)\r\n    error_y = \"Success on YAML loading\"\r\nexcept:\r\n    error_y = \"Error on loading model from yaml\\n\"\r\n    error_y += traceback.format_exc()\r\n\r\ntry:\r\n    j = m.to_json()\r\n    m2 = tf.keras.models.model_from_json(j)\r\n    error_j = \"Success on json loading\"\r\nexcept:\r\n    error_j = \"Error on loading model from json\\n\"\r\n    error_j += traceback.format_exc()\r\n\r\n# %%\r\n\r\nprint(\"-- Status & traceback from config\")\r\nprint(error_c)\r\n\r\nprint(\"-- Status & traceback from yaml\")\r\nprint(error_y)\r\n\r\nprint(\"-- Status & traceback from json\")\r\nprint(error_j)\r\n```\r\n\r\nSo the y error you got signified that the script failed on importing the yaml config file...\r\n\r\nHowever, i tried to re-run everything this morning on 1.13, 1.14rc0 and 1.14rc1 and both json loading and yaml loading are working fine,\r\n\r\nThe only error I get on the three versions is a `ValueError: Improper config format` on     \r\n```python\r\nc = m.get_config()\r\nm2 = tf.keras.models.model_from_config(c) \r\n```\r\n\r\nHowever, since this error is also present in 1.13.1, I guess I am using `from_config` wrong ?", "@fchouteau This is duplicate of this [#26707](https://github.com/tensorflow/tensorflow/issues/26707). ", "Yes, for the model_from_config it appears to be duplicated indeed. I think the issue can be closed.", "Thanks!"]}, {"number": 29529, "title": "Support RaggedTensor in table lookups", "body": "**System information**\r\n- TensorFlow version (you are using): 2.0.0-alpha0 \r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMake Table lookups (`tf.lookup.StaticHashTable` for example) support Ragged Tensor. Currently it only supports Tensor & Sparse Tensor.\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nPretty much any NLP practitioner using this awesome library :)\r\n\r\n**Any Other info.**\r\n", "comments": ["Why don't you use tf.ragged.map_flat_values?  raggedTensor.values has very long range flat value which is normal tensor. ", "By the way, I have noticed that at least in TF 2.2 it does not seem possible to have ragged tensors stored as hash values either (or at least not when using KeyValueTensorInitializers). It raises an exception about ragged tensors not having len().\r\n\r\nIs there any particular reason for this? It feels like there's nothing really preventing this to work.\r\n- The first dimension of any ragged tensor is uniform, implying that while len(tensor) might fail, tensor.shape[0] should provide the value this is looking for.\r\n- While I ignore the details of how this is exactly implemented, shouldn't the values of a hash table only require being comparable for equality, as they are compared with the rest of elements in the same bucket?\r\n- The traceback suggests that the exception comes from an attempt to convert the ragged tensor into a constant tensor, even a constant ragged tensor is provided.", "@Zoltrix We are checking to see if you still need help on this issue. please check with the latest stable version of TF which  is`TF2.7` and also check https://github.com/tensorflow/tensorflow/issues/29529#issuecomment-635916245", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 29528, "title": "Minor fix in makefile for micro vision example", "body": "Change-Id: Iff1172ce5379b21f7ec7fae9202fa6a6ab79d83a", "comments": ["Already fixed"]}, {"number": 29527, "title": "Change byte odering of `image` array in case of big endian and dtype float32", "body": "Creating PR with reference to open issue https://github.com/tensorflow/tensorflow/issues/28704\r\n\r\nThis change is added to change byte ordering of NumPy array(image) in case of Big endian with dtype float32. \r\n\r\nFixes #28704.\r\n\r\n", "comments": []}, {"number": 29526, "title": "tf.scatter_add does not throw an error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.5\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nThe documentation of [tf.scatter_add](https://www.tensorflow.org/api_docs/python/tf/scatter_add) states that it requires the following:\r\n```\r\nupdates.shape = indices.shape + ref.shape[1:]\r\n```\r\nHowever, no exception is thrown when I run the code below. In this code, the terms in the condition above have below values.\r\n```\r\nupdates.shape = (204671, 100)\r\nindices.shape = (204671, 2)\r\nref.shape[1:] = (80, 100)\r\n```\r\nThis makes it harder to understand the behavior of the function ```tf.scatter_add``` in case ```indices``` is a matrix. Specifically, what is the difference between ```tf.scatter_add``` and ```tf.scatter_nd``` when ```indices``` is a matrix.\r\n\r\n**Describe the expected behavior**\r\nAn exception should be thrown when the condition in ```tf.scatter_add``` is not met.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow_probability.python.distributions import Bernoulli, Normal\r\n\r\nif __name__ == '__main__':\r\n    tf.enable_eager_execution()\r\n\r\n    event_dist = Bernoulli(probs=0.2)\r\n    kernel_dist = Normal(loc=0, scale=1)\r\n\r\n    events = event_dist.sample(sample_shape=(128, 80, 100), seed=0)\r\n    kernel = kernel_dist.sample(sample_shape=(100, 100), seed=0)\r\n\r\n    event_idx = tf.where(events)\r\n    event_effect = tf.gather(kernel, event_idx[:, 2])  # PSP due to each spike\r\n    comp = tf.get_variable(name='v1', shape=(128, 80, 100), initializer=tf.zeros_initializer())\r\n    tf.scatter_add(comp, event_idx[:, :2], event_effect)\r\n```\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 29525, "title": "[TF 2.0] constant folding failed: invalid argument: unsupported type: 21", "body": "**System information**\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview 2.0.0.dev20190606\r\n\r\n- Python version: 3.6.5\r\n\r\n**Code to reproduce the issue**\r\n<pre>\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nclass Model(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.dense = tf.keras.layers.Dense(10)\r\n\r\n    def call(self, inputs):\r\n        return self.dense(inputs)\r\n\r\n\r\nmodel = Model()\r\n\r\n\r\ndef forward(x):\r\n    batch_size = x.shape[0]\r\n    ys = tf.TensorArray(tf.float32, size=batch_size)\r\n    for i in tf.range(batch_size):\r\n        y = model(x[i][tf.newaxis, :])\r\n        ys = ys.write(i, y)\r\n    return ys.stack()\r\n\r\n\r\ndef train(x, forward_func):\r\n    with tf.GradientTape() as tape:\r\n        ys = forward_func(x)\r\n        loss = tf.reduce_mean(ys)\r\n    grads = tape.gradient(loss, model.trainable_weights)\r\n    return grads\r\n\r\n\r\ndef big_train(x):\r\n    with tf.GradientTape() as tape:\r\n        batch_size = x.shape[0]\r\n        ys = tf.TensorArray(tf.float32, size=batch_size)\r\n        for i in tf.range(batch_size):\r\n            y = model(x[i][tf.newaxis, :])\r\n            ys = ys.write(i, y)\r\n        ys = ys.stack()\r\n        loss = tf.reduce_mean(ys)\r\n    grads = tape.gradient(loss, model.trainable_weights)\r\n    return grads\r\n\r\n\r\nx = np.random.rand(10, 5).astype(np.float32)\r\n\r\ncodes_buggy = [\r\n    \"tf.function(train)(x, forward)\",\r\n    \"tf.function(big_train)(x)\"\r\n]\r\n\r\ncodes_normal = [\r\n    \"tf.function(train)(x, tf.function(forward))\",\r\n    \"train(x, tf.function(forward))\",\r\n    \"train(x, forward)\",\r\n    \"big_train(x)\"\r\n]\r\n\r\n\r\ndef test(code):\r\n    tf.print(\"==========================\")\r\n    tf.print(f\"{code}:\")\r\n    exec(code)\r\n\r\n\r\ntest(codes_buggy[0])\r\ntest(codes_buggy[1])\r\n\r\ntest(codes_normal[0])\r\ntest(codes_normal[1])\r\ntest(codes_normal[2])\r\ntest(codes_normal[3])\r\n</pre>\r\n\r\n**Other info / logs**\r\nPrint:\r\n<pre>\r\n==========================\r\ntf.function(train)(x, forward):\r\n2019-06-07 16:46:23.314712: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-06-07 16:46:23.357137: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-06-07 16:46:23.460568: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\r\n==========================\r\ntf.function(big_train)(x):\r\n2019-06-07 16:46:24.139754: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-06-07 16:46:24.180814: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n==========================\r\ntf.function(train)(x, tf.function(forward)):\r\n==========================\r\ntrain(x, tf.function(forward)):\r\n==========================\r\ntrain(x, forward):\r\n==========================\r\nbig_train(x):\r\n</pre>\r\n\r\nRelated to #28626 .", "comments": ["Have the same issue on a TF2.0 GPU beta0. It really influences performance.", "Hi @vejvarm What kind of performance do you mean? Training speed or accuracy?", "Hi @llan-ml,\r\n\r\nsorry for not ellaborating on that. By performance I mean the training speed. If I remember correctly, with the warning it took about 2 seconds/batch  while without it I'm at 2 to 4 batches/second. So roughly 4 to 8 times slowdown with the warning. Not really sure about a specific number, but it was significant.\r\n\r\nAs of accuracy, I haven't had the time to run the model for long enough to see if it has some inpact on that.", "@llan-ml I tried to reproducing the issue on colab with latest tf-nightly-gpu-2.0-preview but i did not get any error. Can you try once and let us know if that still an issue. Thanks!", "> \r\n> \r\n> @llan-ml I tried to reproducing the issue on colab with latest tf-nightly-gpu-2.0-preview but i did not get any error. Can you try once and let us know if that still an issue. Thanks!\r\n\r\nJust tried it and to my knowledge it is still there as of **2.0.0.dev20190614**. It's just not written dirrectly to the cell output as it is not an error but a warning. It can be found in the runtime logs of the notebook:\r\n\r\n```\r\nWARNING | 2019-06-14  10:21:43.847057: E  tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant  folding failed: Invalid argument: Unsupported type: 21\r\n-- | --\r\n```\r\n\r\n\r\n", "@gadagashwini I tested with `2.0.0.dev20190615`, and the error still appears.", "same issue on tf-gpu 1.14 now", "@rmlarsen this looks like a grappler issue, can you triage?", "I am having a similar issue, also on TensorFlow 2.0 beta with GPU enabled.\r\n\r\nInterestingly, hiding the GPU away from Tensorflow (using `export CUDA_VISIBLE_DEVICES=-1` before running the script) enables the code to run (but still prints out the error message this Issue is about, and feels slower than it should), while using the GPU results in a memory leakage that end up with the system crashing due to the GPU memory being saturated.\r\n\r\n**System information**\r\n- OS Platform and Distribution: Linux Mint 19.1\r\n- TensorFlow installed from: binary (using pip)\r\n- TensorFlow version: v2.0.0-beta0-16-g1d91213fe7\r\n- Python version: 3.6.8\r\n- CUDA/cuDNN version: 10.0 / 7\r\n- GPU model and memory: QUADRO P-1000 with 4 GB of dedicated RAM (+ 16 GB of system RAM)\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef build_autoencoder(input_dim, embed_dim=100): \r\n    \"\"\"Set up an auto-encoder model made of two BiLSTM layers.\"\"\" \r\n    # Set up input tensors.\r\n    inputs = tf.keras.Input((None, input_dim), dtype=tf.float32) \r\n    mask = tf.keras.Input((None,), dtype=tf.bool) \r\n    # Set up encoder and decoder BiLSTM layers.\r\n    encoder = tf.keras.layers.Bidirectional( \r\n        tf.keras.layers.LSTM(embed_dim, return_sequences=True),\r\n        merge_mode='sum' \r\n    ) \r\n    decoder = tf.keras.layers.Bidirectional( \r\n        tf.keras.layers.LSTM(input_dim, return_sequences=True),\r\n        merge_mode='sum' \r\n    ) \r\n    # Build the outputs tensor.\r\n    outputs = decoder(encoder(inputs, mask=mask), mask=mask) \r\n    # Set up, compile and return the model.\r\n    model = tf.keras.Model(inputs=[inputs, mask], outputs=outputs) \r\n    model.compile('adam', tf.keras.losses.mse) \r\n    return model\r\n\r\n\r\ndef build_mock_data(dim, nsamples, maxlen, seed=0):\r\n    \"\"\"Build some mock data for bug demonstration purposes.\r\n\r\n    Return an array of zero-padded sequences of random\r\n    actual length, and an associated boolean mask Tensor.\r\n    \r\n    Use a random seed for reproducibility.\r\n    \"\"\"\r\n    np.random.seed(seed)\r\n    sizes = np.random.choice(maxlen, size=nsamples)\r\n    inputs = np.random.normal(size=(nsamples, max(sizes), dim))\r\n    for i, size in enumerate(sizes):\r\n        inputs[i, size:] = 0.\r\n    mask = tf.sequence_mask(sizes, dtype=tf.bool)\r\n    return inputs.astype(np.float32), mask\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Generate the mock data. Instantiate the mdoel.\r\n    inputs, mask = build_mock_data(dim=100, nsamples=64, maxlen=500, seed=0)\r\n    model = build_autoencoder(input_dim=100, embed_dim=50)\r\n\r\n    # This works fine.\r\n    model.predict([inputs, mask])\r\n\r\n    # This also works.\r\n    model.evaluate([inputs, mask], inputs)\r\n\r\n    # This is where things go wrong.\r\n    model.fit([inputs, mask], inputs)\r\n```\r\n\r\n**Error with GPU enabled**\r\n```\r\nTrain on 64 samples\r\n2019-06-28 17:11:29.014129: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-06-28 17:11:29.778881: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-06-28 17:11:42.317560: E tensorflow/stream_executor/cuda/cuda_driver.cc:890] failed to alloc 8589934592 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-06-28 17:11:42.317583: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 8589934592\r\n2019-06-28 17:11:42.317604: E tensorflow/stream_executor/cuda/cuda_driver.cc:890] failed to alloc 7730940928 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-06-28 17:11:42.317609: W ./tensorflow/core/common_runtime/gpu/gpu_host_allocator.h:44] could not allocate pinned host memory of size: 7730940928\r\n2019-06-28 17:11:53.241866: W tensorflow/core/common_runtime/bfc_allocator.cc:314] Allocator (GPU_0_bfc) ran out of memory trying to allocate 12.5KiB (rounded to 12800).  Current allocation summary follows.\r\nKilled\r\n```\r\n\r\n**Error with GPU disabled**\r\n```\r\nTrain on 64 samples\r\n2019-06-28 17:20:25.088606: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n2019-06-28 17:20:25.709958: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21\r\n64/64 [==============================] - 3s 44ms/sample - loss: 0.4860\r\n```", "Hi,\r\n\r\nI did some additional  testing based on my previous bug-yielding example and would like to report on it, in hope that it may help track down, and ultimately fix, the issue at stake.\r\n\r\n**Setting and consequences**\r\n\r\nWhat I did was _getting rid of sequences masking for the BiLSTM layers_, thus using a less-general model expecting batches of same-length sequences. In this case, I _no longer encounter GPU memory leakage_ (at least, not something that would make my computer crash on the first run of fitting the model), however an _optimization warning_ is raised - and I have no idea whether it relates to the initial issue or not. It shows up _both with and without enabling the use of the GPU_, and for each use of the model (not just for the fitting process).\r\n\r\n**Warning message**\r\n```\r\n2019-07-01 09:22:25.637712: W tensorflow/core/grappler/optimizers/implementation_selector.cc:199] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_860_1038' and '__inference___backward_cudnn_lstm_860_1038_specialized_for_Adam_gradients_encoder_StatefulPartitionedCall_1_grad_StatefulPartitionedCall_at___inference_keras_scratch_graph_5563' both implement 'lstm_7a1d4064-50de-41c0-86d3-5a99f303e8d7' but their signatures do not match.\r\n```\r\n\r\n**Code**\r\n\r\nIn the code below, I allow distinct batches to contain sequences of different length, however I also made a test using a strict parameter (i.e. setting the `InputLayer`'s shape to `(length, input_dim)` with `length` an integer instead of `None`), which yields exactly the same error message.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef build_autoencoder(input_dim, embed_dim=100): \r\n    \"\"\"Set up an auto-encoder model made of two BiLSTM layers.\"\"\" \r\n    # Set up the input tensor.\r\n    inputs = tf.keras.Input((None, input_dim), dtype=tf.float32) \r\n    # Set up encoder and decoder BiLSTM layers.\r\n    encoder = tf.keras.layers.Bidirectional( \r\n        tf.keras.layers.LSTM(embed_dim, return_sequences=True),\r\n        merge_mode='sum', name='encoder'\r\n    ) \r\n    decoder = tf.keras.layers.Bidirectional( \r\n        tf.keras.layers.LSTM(input_dim, return_sequences=True),\r\n        merge_mode='sum', name='decoder'\r\n    ) \r\n    # Build the outputs tensor.\r\n    outputs = decoder(encoder(inputs)) \r\n    # Set up, compile and return the model.\r\n    model = tf.keras.Model(inputs=inputs, outputs=outputs) \r\n    model.compile('adam', tf.keras.losses.mse) \r\n    return model\r\n\r\n\r\ndef build_mock_data(dim, nsamples, length, seed=0):\r\n    \"\"\"Build some mock data for bug demonstration purposes.\r\n\r\n    Return an array of shape (nsamples, length, dim) filled\r\n    with random normally-distributed data.\r\n    \r\n    Use a random seed for reproducibility.\r\n    \"\"\"\r\n    np.random.seed(seed)\r\n    return np.random.normal(size=(nsamples, length, dim))\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Generate the mock data. Instantiate the mdoel.\r\n    inputs = build_mock_data(dim=100, nsamples=64, length=500, seed=0)\r\n    model = build_autoencoder(input_dim=100, embed_dim=50)\r\n\r\n    # This works but prints the error warning.\r\n    model.predict(inputs)\r\n\r\n    # Same thing here.\r\n    model.evaluate(inputs, inputs)\r\n\r\n    # Same thing here.\r\n    model.fit(inputs, inputs)\r\n```\r\n\r\nI hope this helps solving the initial issue. Please let me know if there is any additional info I can provide or test I can run to help. At the moment, not being able to fit models with LSTM layers using properly-masked variable-length sequences is quite an issue to put code into production under TensorFlow 2.0. I know this is the whole point of a beta release (having a not-yet-quite-stable version out to identify issued that need solving before the actual release), but the programming logic has been so greatly altered as compared with TF 1.x that it would also be unpractical not to start taking the step (getting used to Eager execution demands an important effort, after having extensively used the low-level placeholder / session API)...", "Note: this issue is quite similar to the newly-opened #30263", "Additional test/results (sorry for the multiplication of messages - I really want to provide as much info as possible, hoping it can help solve the issue):\r\n\r\n* Changing my code to feed the model with a numpy array containing the batched sequences' lengths (_then_ converting it to a sequence mask Tensor using `tf.sequence_mask` _within_ the model) partly fixes the issue.\r\n\r\n* `E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] constant folding failed: Invalid argument: Unsupported type: 21` still shows up twice when first calling the `fit` method of the model built (both when using GPU or CPU-only)\r\n\r\n* The model can be run and fit, both with and without the GPU. After the first call to fit, the error message no longer shows up, and I can see the loss decreasing along the iterations (up to some point).\r\n\r\n**Code**:\r\n```\r\ndef build_autoencoder(input_dim, embed_dim=100): \r\n    \"\"\"Set up an auto-encoder model made of two BiLSTM layers.\"\"\" \r\n    # Set up the input tensors.\r\n    inputs = tf.keras.Input((None, input_dim), dtype=tf.float32)\r\n    sizes = tf.keras.Input((), dtype=tf.int32)\r\n    # Set up encoder and decoder BiLSTM layers.\r\n    encoder = tf.keras.layers.Bidirectional( \r\n        tf.keras.layers.LSTM(embed_dim, return_sequences=True),\r\n        merge_mode='sum', name='encoder'\r\n    ) \r\n    decoder = tf.keras.layers.Bidirectional( \r\n        tf.keras.layers.LSTM(input_dim, return_sequences=True),\r\n        merge_mode='sum', name='decoder'\r\n    ) \r\n    # Build the outputs tensor.\r\n    mask = tf.sequence_mask(sizes, maxlen=tf.shape(inputs)[1])\r\n    outputs = decoder(encoder(inputs, mask=mask), mask=mask) \r\n    # Set up, compile and return the model.\r\n    model = tf.keras.Model(inputs=[inputs, sizes], outputs=outputs) \r\n    model.compile('adam', tf.keras.losses.mse) \r\n    return model\r\n\r\ndef build_mock_data(dim, nsamples, maxlen, seed=0):\r\n    \"\"\"Build some mock data for bug demonstration purposes.\r\n\r\n    Return an array of zero-padded sequences of random\r\n    actual length, and an array containing those lengths.\r\n    \r\n    Use a random seed for reproducibility.\r\n    \"\"\"\r\n    np.random.seed(seed)\r\n    sizes = np.random.choice(maxlen, size=nsamples)\r\n    inputs = np.random.normal(size=(nsamples, max(sizes), dim))\r\n    for i, size in enumerate(sizes):\r\n        inputs[i, size:] = 0.\r\n    return inputs.astype(np.float32), sizes\r\n\r\nif __name__ == '__main__':\r\n    # Generate the mock data. Instantiate the mdoel.\r\n    inputs, sizes = build_mock_data(dim=100, nsamples=64, maxlen=500, seed=0)\r\n    model = build_autoencoder(input_dim=100, embed_dim=50)\r\n\r\n    # This works fine.\r\n    model.predict([inputs, sizes])\r\n\r\n    # This also works.\r\n    model.evaluate([inputs, sizes], inputs)\r\n\r\n    # This prints out the error messages, but works.\r\n    model.fit([inputs, sizes], inputs)\r\n\r\n    # Further calls no longer print errors, and the loss decreases.\r\n    model.fit([inputs, sizes], inputs)\r\n    model.fit([inputs, sizes], inputs)\r\n    model.fit([inputs, sizes], inputs)\r\n```\r\n\r\n**Conclusion**:\r\n\r\n* So, I guess the initial issue (the error showing up) is not solved.\r\n\r\n* There is, additionally, the issue previously pointed out (and also object of issue #30263) of an optimization error (and apparent failure to fit models) when using fixed-length sequences.\r\n\r\n* However, the GPU memory issue I was personally encountering seems to have been related to the use of a Tensor (instead of a numpy array) as input to my model. I don't know whether this is by design (in which case it might be worth it to add warnings when users do that?) or a separate issue, but I was able to fix it with better code design.", "I also run into this issue when using masking on a GRU/LSTM layer, though running on CPU does not prevent the memory from blowing up and crashing the machine. In fact, even when running on GPU, system memory maxes out, though it looks as though the printed errors imply that GPU memory has been completely filled as well. Removing the masking, however allows training to occur without issue, though the \"constant folding failed: Invalid argument: Unsupported type: 21\" message still occurs.", "Hi,\r\nCould anyone from the TF team confirm that this issue is being researched / worked out? It appears that it does not show in every setting (thus the difficulty to pass \"front-row\" issues screeners, as in the newly opened #30533), but it causes major performance issues to people who are confronted to it (see my performance tests on issue #30263). It should also be noted that this not only affects TF 2.0, but also 1.14 when Eager execution is enabled...", "> Hi, I can confirm that this is not inherent to the LSTM implementation. I have just reproduced the same error with GRU. Maybe it has something to do with the gpu optimized CuDNN implementations?\r\n\r\nThank you for sharing this. The issue seems to be at the grappler level, which if I am not mistaken is indeed the mechanism that chooses the backend kernel to use, which can be a CuDNN one...", "Interestingly, I am encountering this issue in TF 1.14, in TF 2.0b1 installed through pip, but not in TF 2.0b1 installed from source using the r2.0 branch, and not always in TF 2.0b1 installed from source using yesterday's state of the master branch.\r\n\r\nUsing this issue's code, on the latter installation, I have a distinct bug, namely repeated prints similar to `W tensorflow/core/grappler/costs/virtual_scheduler.cc:794] Output node [ gradients/while_grad/while_grad/gradients/zeros_1_switch/_43 ] has alread seen this input node [ gradients/while_grad/while_grad/merge/_25 -- possibly due to Swith-Merge in previous nodes. Skip to increment num_inputs_ready.`", "Edit: I should note that I am running on the gpu nightly pip build as of the time stamp on this comment.\r\n\r\nAnother interesting piece of narrowing information. In the piece of code below, everything runs without a hitch if the for loop (tf.while_loop behind the scenes) is removed. That is...\r\n\r\nWithout for loop: tf function routine runs twice, code runs ad infinitum\r\n\r\nWith for loop: tf function routine runs twice, graph placement issue and and code breaks\r\n\r\nHere's the code:\r\n\r\nhttps://github.com/jkamalu/tensorflow_bugs/blob/master/LSTMGraphPlacement.py\r\n\r\nAnother thing worth noting is that this issue appears even without the while loop with tensorflow GPU distributed strategies as seen https://github.com/tensorflow/tensorflow/issues/29189\r\n\r\n@pandrey-fr a note: if the cudnn implementation is not important to you (I don't know why it wouldn't be, but just in case), you can wrap the LSTMCell layer in the RNN layer and it works fine... another hint that this error might be in the optimized implementation.\r\n", "The warning should go away in the next nightly. I'm looking into the original issue with unsupported types in constant folding.", "The issue is that the error handling in many places in Grappler is much too conservative. In this case we bail completely out of folding because we fail to convert a constant of an unknown type early. I'll work on making the code more robust in this sense.", "The particular error in this case was due to ZerosLike being overloaded for DT_VARIANT types: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/constant_op.cc#L267\r\n\r\nI am submitting a fix now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29525\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29525\">No</a>\n", "Fix submitted: https://github.com/tensorflow/tensorflow/commit/24174643a75e819b8ce01fd70d45d03616e50071", "Great, thank you @rmlarsen!", "As announced by @rmlarsen, the fix (which is now included in the nightly build) removes the error message ; however it appears (at least in my case) that LSTM layers with masking still won't be moved to the GPU (when Eager is enabled at least - I am still trying to figure out whether it is the case with Eager disabled), which is somehow confusing. Do you have any idea why this is the case?", "Do you mean they won't be moved to the GPU or that the graph won't be built with the CuDNN implementation? My bootleg LSTM layers (see below) exist on the GPU with the standard implementation (I verify this by watching nvidia-smi). I use masking (right-padding, so TF v2.0 CuDNN compatible), but end up having to use RNN-wrapped LSTMCell instances, which don't use the CuDNN implementation.\r\n\r\nIt should be noted that in a while loop for dynamic decoding, the GPU enabled, CuDNN compatible tf.keras.layers.LSTM implementation does not function, nor does this specific setup work (even without the while loop) on multiple GPUs via a distributed strategy.", "To be honest I am not quite sure... What I did was using a `tf.keras.callbacks.TensorBoard` callback to trace the fitting of my model (in Eager mode), and I found out that on TensorBoard the LSTM unit is represented in a different color than the other bits (when I set the visualization parameter to \"device used\"), with the other bits' color being labeled \"GPU:0\". I also verified that when I use custom layers of mine that make use of masking, they are clearly drawn to have been placed on the GPU.\r\n\r\nIf you have any advice as to how to properly keep track of where operations are being performed (maybe also when Eager execution is disabled), I would be glad to use them!", "Hi @rmlarsen, I just wanted to let you know that errors resembling this decrease in speed were reintroduced by later nightly builds. This isn't a request for a fix (I downgraded to the July 24 nightly and everything works fine now), but I thought you might like to know just in case it's a simple thing.\r\n\r\nWith the **_same code_** (multi-gpu setting on TF v2 with LSTM) ...\r\n\r\nOn the July 24 build... model trains quickly on all GPUs and is correct and gives spurious error messages\r\n\r\n> \r\n> 2019-08-13 11:31:15.551518: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] implementation_selector failed: Invalid argument: Invalid format of input node name:  Expected: {forward_node_name}:{index}\r\n> 2019-08-13 11:31:32.258063: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference_cudnn_lstm_with_fallback_186209' and '__inference_standard_lstm_185862_specialized_for_model_lstm_2_StatefulPartitionedCall_at___inference_step_311955' both implement 'lstm_03256996-2770-4288-91ed-338407bd3cc3' but their signatures do not match.\r\n\r\nOn the August 12 build... model trains on all GPUs and is correct but takes ~50 times more time. Not an exaggeration.\r\n\r\n> \r\n> 2019-08-13 09:39:40.107723: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_1/TensorListPushBack_42 was passed float from se_q3/seq_encoder/while/body/_1/decoder_c/lstm_3/StatefulPartitionedCall:9 incompatible with expected variant.\r\n> 2019-08-13 09:39:53.596733: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] function_optimizer failed: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_1/TensorListPushBack_42 was passed float from se_q3/seq_encoder/while/body/_1/decoder_c/lstm_3/StatefulPartitionedCall:9 incompatible with expected variant.\r\n> 2019-08-13 09:39:58.604309: W tensorflow/core/common_runtime/process_function_library_runtime.cc:686] Ignoring multi-device function optimization failure: Invalid argument: Input 1 of node se_q3/seq_encoder/while/body/_1/TensorListPushBack_69 was passed float from se_q3/seq_encoder/while/body/_1/decoder_c/lstm_2/StatefulPartitionedCall:9 incompatible with expected variant.\r\n> \r\n"]}, {"number": 29524, "title": "Error when running make for Tensorflow Lite Micro", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: bfc8733ffb\r\n- GCC/Compiler version (if compiling from source): arm-none-eabi-g++ 8.2.1\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhen running\r\n\r\n$ make -f tensorflow/lite/experimental/micro/tools/Makefile generate_projects\r\n\r\nit results in the following error:\r\n\r\ntensorflow/lite/experimental/micro/examples/micro_vision/Makefile.inc:22: *** missing separator.  Stop.\r\n\r\nReverting commit d77ccda7569 removes that error, but leads to the following error:\r\n\r\nmake: *** No rule to make target 'tensorflow/lite/experimental/micro/tools/make/gen/linux_x86_64/prj/micro_vision_test/make/tensorflow/lite/experimental/micro/examples/micro_vision/no_person_image_data.cc', needed by 'generate_micro_vision_test_make_project'.  Stop.", "comments": ["I believe that this issue has been fixed. Please try again.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29524\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29524\">No</a>\n", "Hello. I'm having a similar issue.\r\n\r\nI'm trying to generate a Keil project for the microcontroller I'm developing. When I followed procedures in the following link, it gave me the error:\r\n\r\nhttps://www.tensorflow.org/lite/microcontrollers/library\u00a0\r\n\r\nSystem information\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.2 (19C57)\r\nTensorFlow installed from (source or binary): source\r\nTensorFlow version: 1.13.1 (8ac4b20)\r\nGCC/Compiler version (if compiling from source): x86_64-apple-darwin19.2.0\r\nDescribe the problem\r\nWhen running\r\n\r\n$ gmake -f tensorflow/lite/experimental/micro/tools/Makefile generate_projects\r\n\r\nit results in the following error:\r\n\r\ngmake: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/osx_x86_64/prj/micro_speech/make/tensorflow/lite/experimental/micro/examples/micro_speech/simple_features/simple_model_settings.h', needed by 'generate_micro_speech_make_project'.  Stop.\r\n", "Any update to this? I am also facing the \r\n\r\n> find: \u2018tensorflow/lite/tools/make/downloads/absl/absl/\u2019: No such file or directory\r\n> make: *** No rule to make target `generate_projects'.  Stop.\r\n\r\n\r\nerror when running the \r\n\r\n> make -f tensorflow/lite/micro/tools/make/Makefile generate_projects\r\n\r\nas directed in the documentation.\r\n\r\nhttps://www.tensorflow.org/lite/microcontrollers/library\r\n\r\nIs this just a case of missing \"absl\" or something else?\r\n", "I am experiencing this issue too. Can someone please respond? ", "@mazurick As this is an old issue, can you please open a new issue with a standalone code to reproduce the issue? Thanks!", "@mazurick There is a slight change in the new version. `Makefile` is found at: `tensorflow/lite/micro/tools/make/Makefile` instead of `tensorflow/lite/experimental/micro/tools/make/Makefile`", "Hello i have problem with the hello_wold_test i have the make file in the location tensorflow/tensorflow/lite/micro/tools/make/Makefile\r\nBefore this i clone it from the github .\r\nThe version that i used the tensorflow is the 0.4.0 and i used ubuntu 16.04 and i want to run it on my ubuntu for start and next in apollo3 blue.\r\n#44122 ", "Hi, I am facing the same problem too when i tried running \r\nmake -f tensorflow/lite/micro/tools/make/Makefile TARGET=esp generate_hello_world_esp_project\r\nTensorflow was directly cloned from github.\r\nAny ideas?? ", "Hello I have the exactly same problem while runnig the command make -f tensorflow/lite/micro/tools/make/Makefile generate_projects\r\n\r\nDid anyone find a solution to it yet ?\r\n", "Yes. Take the EDX course\r\n\r\nSent from my iPhone\r\n\r\nOn Mar 17, 2022, at 12:12 PM, RickForRescue ***@***.***> wrote:\r\n\r\n\ufeff\r\n\r\nHello I have the exactly same problem while runnig the command make -f tensorflow/lite/micro/tools/make/Makefile generate_projects\r\n\r\nDid anyone find a solution to it yet ?\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/29524#issuecomment-1071031115>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEDQ2XZTLSIV7FJTT6RBH3LVANKYNANCNFSM4HVTZ7YA>.\r\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n", "> \r\n\r\nI am sorry, I couldnt follow. Can you elaborate please?", "I took the EDX micro purse on TinyML and it flushed out any problems I was having.\r\n\r\nOn Mar 21, 2022, at 6:42 AM, RickForRescue ***@***.***> wrote:\r\n\r\n\ufeff\r\n\r\nI am sorry, I couldnt follow. Can you elaborate please?\r\n\r\n\u2014\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/29524#issuecomment-1073741408>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEDQ2X25GSLTNMUXBCOMSKDVBBHA5ANCNFSM4HVTZ7YA>.\r\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n"]}, {"number": 29523, "title": "LICENSE: Restore original Apache license", "body": "* Remove copyright notice from first line. Apache's instruction: \r\n\r\n> Each original source document (code and documentation, but **excluding the LICENSE** and NOTICE files) SHOULD include a short license header at the top. \r\n\r\nRef: http://www.apache.org/dev/apply-license.html#new\r\n\r\n* The Apache 2.0 license includes an Appendix about how to apply the license terms to files. We should not modify the license file. \r\nReference: https://github.com/kubernetes/kubernetes/commit/d30db1f9a915aa95402e1190461469a1889d92be", "comments": ["Thanks, I'm going to check this PR with our open source counsel.\r\n", "This is the same situation as encountered in https://github.com/tensorflow/tensorflow/pull/26554 -- this change doesn't make any difference, so in the name of caution about changing the long-standing license doc, I'm closing this PR.\r\n", "Thanks, @martinwicke. Updated!"]}, {"number": 29522, "title": "LICENSE: Restore original Apache license", "body": "* Remove copyright notice from first line. Apache's instruction: \r\n\r\n> Each original source document (code and documentation, but **excluding the LICENSE** and NOTICE files) SHOULD include a short license header at the top. \r\n\r\nRef: http://www.apache.org/dev/apply-license.html#new\r\n\r\n* The Apache 2.0 license includes an Appendix about how to apply the license terms to files. We should not modify the license file. \r\nReference: https://github.com/kubernetes/kubernetes/commit/d30db1f9a915aa95402e1190461469a1889d92be", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29522) for more info**.\n\n<!-- need_sender_cla -->"]}, {"number": 29521, "title": "Revert \"Fix an important performance regression for LSTM and GRU in t\u2026", "body": "\u2026f 2.0\"\r\n\r\nThis reverts commit 4f39bd9ce899dc0a22089eedd1c90c394a54dd68.", "comments": []}, {"number": 29520, "title": "Add python syntax formatting for code samples, returns value and errors raised", "body": "Part of the Tensorflow DocSprint", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29520) for more info**.\n\n<!-- need_author_cla -->", "@eddie-zhou Spelling mistake fixed", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F29520) for more info**.\n\n<!-- cla_yes -->", "@eddie-zhou @ymodak  What is needed for my branch to be merged into master?"]}, {"number": 29519, "title": "Update ag_logging.py with clarity and reference", "body": "* fixes typo\r\n* adds clarity to text\r\n* adds reference to absl's logging doc page\r\n\r\nref #29250 #29251", "comments": ["@ghchinoy  Could you please address the reviewer comments. Thanks!"]}, {"number": 29518, "title": "add compatibility check badges to README", "body": "Hello google package maintainer,\r\nThe badges being added to the README in this PR will indicate your compatibility with other google packages. This addresses a set of user bugs which have happened when a user depends on two Cloud libraries (or runtimes that bundle libraries), say A and B, which both depend on library C. If the two libraries require different versions of C, the users can run into issues both when they pip install the libraries, and when they deploy their code. Our compatibility server checks that all libraries we make including this one are self and pairwise compatible as well as not having any deprecated dependencies. The two badges will mark the build for your project green when the latest version available on PyPI and github HEAD respectively meet all compatibility checks with itself and all other libraries. The badge target will link to a details page that elaborates on the current status. This should help you fix issues pre-release, to avoid user surprises. For more information, please take a look at our project charter at go/python-cloud-dependencies-project-charter and the badging PRD https://docs.google.com/document/d/1GYRFrfUou2ssY71AtnLkc8Sg1SD4dxqN4GzlatGHHyI/edit?ts=5c6f031d", "comments": ["Can one of the admins verify this patch?", "@ylil93  Can you please resolve conflicts? Thanks!", "I do not think this is the right place to add that badge. I would recommend having this near the documentation badges at the top.\r\n\r\n@ewilderj are we aware of this? Do you think we should add this badge?", "@ewilderj Any update on this PR, please. Thanks!", "@ylil93 Can you please resolve conflicts? Thanks!", "I would worry about about the badge proliferation after these.\r\n\r\n> This should help you fix issues pre-release, to avoid user surprises.\r\n\r\nIs the expectation that the TF team will use these badges as a tool before cutting a release?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "Looks like we do now wish to add this badge. Closing the PR."]}, {"number": 29517, "title": "Fix an important performance regression for LSTM and GRU in tf 2.0", "body": "The issue was caused by auto inline the tf function in eager context,\r\nwhich cause the grappler not able to do the swap the optimization.\r\n\r\nPiperOrigin-RevId: 251945251", "comments": []}, {"number": 29516, "title": "\"Failed to load the native TensorFlow runtime\" Win7-x64", "body": "**System information**\r\n- OS Platform and Distribution: Microsoft Windows 7 x64\r\n- Mobile device: N/A\r\n- TensorFlow installed from: Binary\r\n- TensorFlow version: 1.13\r\n- Python version: 3.7.3\r\n- Installed using: PIP\r\n- Bazel version: N/A\r\n- GCC/Compiler version: N/A\r\n- CUDA/cuDNN version: CUDA 10.1.1 / cuDNN 7.6\r\n- GPU model and memory: NVIDIA GT 720 2GB\r\n\r\n**Describe the problem**\r\nI have installed everything as outlined by the installation instructions and have not gotten any errors, but I get this error at run. I have updated my environment variables and tried many solutions already posted.\r\n\r\n**Logs**\r\nFile \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\",\r\n line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\",\r\n line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nE:\\Videowelder\\Desktop\\srgan-master>python train.py\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_\r\nhelper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\",\r\n line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\",\r\n line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 7, in <module>\r\n    from time import localtime, strftime\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-im\r\nport\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-pac\r\nkages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_\r\nhelper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript\r\nion)\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\",\r\n line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Videowelder\\AppData\\Local\\Programs\\Python\\Python37\\lib\\imp.py\",\r\n line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed: The specified module could not be found.", "comments": ["Please lower your cuda version to 10.0\r\nhttps://www.tensorflow.org/install/gpu#software_requirements"]}, {"number": 29515, "title": "Update build_def.bzl", "body": "To resolve the following two issues on a linux_aarch64 system:\r\n1. prohibited conversions between vectors (https://github.com/tensorflow/tensorflow/issues/26731#issue-421396111)\r\n2. prohibited explicit use of frame pointer register x29 in asm (https://github.com/tensorflow/tensorflow/issues/26731#issuecomment-499361715)", "comments": ["Hi petewarden, could you review the code change at your convenience?", "It looks like the underlying #26731 is fixed, so I'm assuming we can close this now?"]}, {"number": 29514, "title": "[TF 2.0 API Docs] Added examples to docs for is_jpeg() and is_png()", "body": "Examples were missing for the tf.io functions `is_jpeg()` and `is_png()`", "comments": ["I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29513, "title": "Failed to convert object of type <class 'dict'> to Tensor", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nI'm writing some code to create a tf.estimator.BoostedTreeRegressor. I'm loading my data through pandas, so both my features and labels are pd frames. Due to this, my training function for the estimator is \r\n```python\r\ntf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,shuffle= True, batch_size = batch_size)\r\n```\r\nand the testing function is \r\n```python\r\ntf.estimator.inputs.pandas_input_fn(x=X_test,y=y_test, shuffle = False, batch_size = batch_size)\r\n```\r\nWhenever I run the train function, I get the following error:\r\n```python\r\nTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'PP': <tf.Tensor 'random_shuffle_queue_DequeueUpTo:11' shape=(None,) dtype=float64>}. Consider casting elements to a supported type.\r\n```\r\n\r\n**Other info / logs**\r\n```python\r\nWARNING: Logging before flag parsing goes to stderr.\r\nW0606 15:20:02.134926 11092 estimator.py:1799] Using temporary folder as model directory: C:\\Users\\CRISTI~1\\AppData\\Local\\Temp\\tmpeez4al44\r\nW0606 15:20:02.150548 11092 deprecation.py:323] From C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:238: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nW0606 15:20:02.166169 11092 deprecation.py:323] From C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0606 15:20:02.166169 11092 deprecation.py:323] From C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nW0606 15:20:02.181790 11092 deprecation.py:323] From C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nW0606 15:20:02.181790 11092 deprecation.py:323] From C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:2758: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.cast` instead.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 559, in make_tensor_proto\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 559, in <listcomp>\r\n    str_values = [compat.as_bytes(x) for x in proto_values]\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\", line 61, in as_bytes\r\n    (bytes_or_text,))\r\nTypeError: Expected binary or unicode string, got {'PP': <tf.Tensor 'random_shuffle_queue_DequeueUpTo:11' shape=(None,) dtype=float64>}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/CristianVives/Documents/translation/scratch/driver.py\", line 51, in <module>\r\n    model.train(train_fun)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 359, in train\r\n    loss = self._train_model(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1139, in _train_model\r\n    return self._train_model_default(input_fn, hooks, saving_listeners)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1169, in _train_model_default\r\n    features, labels, ModeKeys.TRAIN, self.config)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator.py\", line 1127, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py\", line 1924, in _model_fn\r\n    train_in_memory=train_in_memory)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\boosted_trees.py\", line 1087, in _bt_model_fn\r\n    batch_size = array_ops.shape(labels)[0]\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 313, in shape\r\n    return shape_internal(input, name, optimize=True, out_type=out_type)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 337, in shape_internal\r\n    input_tensor = ops.convert_to_tensor(input)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1050, in convert_to_tensor\r\n    return convert_to_tensor_v2(value, dtype, preferred_dtype, name)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1108, in convert_to_tensor_v2\r\n    as_ref=False)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1186, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 304, in _constant_tensor_conversion_function\r\n    return constant(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 245, in constant\r\n    allow_broadcast=True)\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 283, in _constant_impl\r\n    allow_broadcast=allow_broadcast))\r\n  File \"C:\\Users\\CristianVives\\.conda\\envs\\ml\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\", line 563, in make_tensor_proto\r\n    \"supported type.\" % (type(values), values))\r\nTypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'PP': <tf.Tensor 'random_shuffle_queue_DequeueUpTo:11' shape=(None,) dtype=float64>}. Consider casting elements to a supported type.\r\n```", "comments": ["This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow-tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there.\r\n"]}, {"number": 29512, "title": "[TF 2.0 API Docs] tf.io.decode_image", "body": "Added a usage example in decode_image under image_ops_impl.py. Issue is raised on the link https://github.com/tensorflow/tensorflow/issues/29511", "comments": ["I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 29511, "title": "[TF 2.0 API Docs] tf.io.decode_image", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io/decode_image\r\nhttps://github.com/tensorflow/tensorflow/edit/master/tensorflow/python/ops/image_ops_impl.py\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Usage example\r\n\r\nNo usage example given\r\n\r\n### Submit a pull request?\r\n\r\nYes\r\nhttps://github.com/tensorflow/tensorflow/pull/29512", "comments": ["@imransalam,\r\nThe `Used in the notebooks` section has been added in the latest version of the documentation for [tf.io.decode_image](https://www.tensorflow.org/api_docs/python/tf/io/decode_image).\r\n\r\nClosing the issue as it is resolved. Please feel free to re-open if mistaken. Thanks!"]}, {"number": 29510, "title": "ImportError on Pycharm", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: extracted from tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl and placed in python path \r\n- TensorFlow version: 1.13\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: venv\r\n- Bazel version (if compiling from source): na\r\n- GCC/Compiler version (if compiling from source): na\r\n- CUDA/cuDNN version: CUDA Version 10.0.166\r\n- GPU model: 128-core NVIDIA Maxwell @ 921MHz  \r\n- Memory: 4GB 64-bit LPDDR4 @ 1600MHz | with swapfile and SD card with 64GB\r\n\r\n\r\n**Describe the problem**\r\n\r\nWhen attempting to import tensorflow (`import tensorflow`) on pycharm the following error occurs: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/tensorflow/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ihub/Documents/PycharmProjects/first/HelloWorld.py\", line 15, in <module>\r\n    import tensorflow\r\n  File \"/usr/local/lib/tensorflow/__init__.py\", line 27, in <module>\r\n    from tensorflow._api.v2 import audio\r\n  File \"/usr/local/lib/tensorflow/_api/v2/audio/__init__.py\", line 8, in <module>\r\n    from tensorflow.python.ops.gen_audio_ops import decode_wav\r\n  File \"/usr/local/lib/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/usr/local/lib/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"/usr/local/lib/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"/usr/local/lib/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"/usr/local/lib/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\r\n    return _load(spec)\r\nImportError: /usr/local/lib/tensorflow/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\nthe part that confuses me most is that the main error is about `_pywrap_tensorflow_internal.so` \"not existing\" yet it is in the same place as the rest of the fils, the exact path where it says it isnt't", "comments": []}]