[{"number": 43701, "title": "RTX3080 install RuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid", "body": "**System information**\r\nubuntu18.04.5 LTS\r\nGeForce RTX 3080 \r\nNVIDIA driver 455.23.04 + CUDA10.1 + Cudnn7.6.5\r\nAnaconda3\r\nenvs: tensorflow-gpu==2.3.1\r\n**Describe the problem**\r\n```\r\nimport tensorflow as tf\r\ntf.test.is_gpu_available()\r\n```\r\n\r\n>...\r\n 2020-10-01 17:18:08.728031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1\r\n2020-10-01 17:18:08.728126: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/librazxc/.conda/envs/tf23/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/librazxc/.conda/envs/tf23/lib/python3.7/site-packages/tensorflow/python/framework/test_util.py\", line 1563, in is_gpu_available\r\n    for local_device in device_lib.list_local_devices():\r\n  File \"/home/librazxc/.conda/envs/tf23/lib/python3.7/site-packages/tensorflow/python/client/device_lib.py\", line 43, in list_local_devices\r\n    _convert(s) for s in _pywrap_device_lib.list_devices(serialized_config)\r\nRuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\r\n\r\n**Any other info / logs**\r\ntensorflow-gpu==2.2 or 2.1 is work well\r\n", "comments": ["@PureHing \r\nPlease refer this issue and let us know if it helps: #43588", "@Saduf2019 it cannot help me.", "@PureHing \r\nCould you please try this once and let us know : [link](https://github.com/tensorflow/tensorflow/issues/41990#issuecomment-686853339) and this [link1](https://github.com/tensorflow/tensorflow/issues/43174#issuecomment-702236129)", "Have same issue with 3090", "> @PureHing\r\n> Could you please try this once and let us know : [link](https://github.com/tensorflow/tensorflow/issues/41990#issuecomment-686853339) and this [link1](https://github.com/tensorflow/tensorflow/issues/43174#issuecomment-702236129)\r\n\r\nare you suggesting we should go with cuda 11 and give it a try?", "Same issue for 3090: issue here: https://github.com/tensorflow/tensorflow/issues/43718", "@PureHing\r\nCould you Please update.", "3080 and 3090 does not work with CUDA 10.\r\nYou have to use CUDA 11.0 or higher.\r\nRight now, the only way to do so is by installing tf-nightly or building yourself.", "Please refer to [this link](https://github.com/tensorflow/tensorflow/issues/43629#issuecomment-702851560) and move the issue to closed status if resolved.", "@Saduf2019 Tensorflow can be used depending on the following environment:\r\nubuntu18.04.5 LTS\r\nGeForce RTX 3080*2\r\nNVIDIA driver 455.23.04 + CUDA11.1 + Cudnn8.0.4\r\nAnaconda3\r\nenvs: tf-nightly-gpu==2.4.0-dev20201008\r\n\r\nBTW,how can i use this [demo](https://github.com/tensorflow/docs-l10n/blob/master/site/zh-cn/tutorials/distribute/custom_training.ipynb) in tf-nightly-gpu?\r\nerror: AttributeError: 'MirroredStrategy' object has no attribute 'experimental_run_v2'\r\n", "@PureHing Looks like this is a know issue with GeForce RTX. Similar issues was reported in [this issue](https://github.com/tensorflow/tensorflow/issues/41990). Please check some of the workarounds and root-causes that could help you resolve your issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43701\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43701\">No</a>\n", "I also have this issue and I am using RTX 3070.\r\nI followed @jaentrouble instruction to use tf.nightly builds and it works perfectly after that.", "I am using RTX 3080, ubuntu 20.04, kernel 5.4.0.58\r\nI find something ...\r\n\r\n```\r\nif CUDA ==11.0:\r\n    if tensorflow < 2.3:\r\n        if cudnn == 8.x:\r\n            tensorflow error: can't find so.7\r\n        elif cudnn == 7.x:\r\n            tensorflow raise: Your CUDA software stack is old.(GPU not working for DL job.)\r\n```\r\nNow, I am trying to change the Nvidia driver version to 11.2.", "@IMYin \r\nPlease change TensorFlow to version 2.5.0-dev. it works for me!"]}, {"number": 43700, "title": " The issue about unexpected inference result (output class and probability-value) through quantized tflite model in more than a certain number of classes", "body": "**System information**\r\n- Model training (VGG16) environment\r\n    - OS Platform and Distribution\r\n        - Windows 10\r\n    - GPU\r\n        - NVIDIA GTX1080\r\n    - Cuda Toolkit 10.0.130\r\n    - Python 3.7\r\n    - Keras 2.2.4\r\n    - TensorFlow 1.14.0\r\n\r\n- Model training (MobileNetV2) environment    \r\n    - OS Platform and Distribution\r\n        - Ubuntu 18.04.4 LTS (GNU/Linux 4.15.0-99-generic x86_64)\r\n    - GPU\r\n        - NVIDIA Quadro P5000\r\n    - Cuda Toolkit 10.1.243\r\n    - Python 3.7\r\n    - Keras 2.2.4\r\n    - TensorFlow 1.14.0\r\n\r\n- Model convert environment\r\n    - OS Platform and Distribution\r\n        - Mac OS X 10.14.5 (18F132)\r\n    - Python 3.6.7\r\n    - Keras 2.2.4\r\n    - TensorFlow 1.14.0\r\n\r\n- Target mobile device\r\n    - Pixel 3 XL\r\n        - Android 9 (build PQ3A.1980801.002)\r\n\r\n**Trained model information**\r\n- VGG16 and MobileNetV2 model (transfer learning)\r\n- The number of classes: **10,996**\r\n    - This model is for classification of  the huge classes of the images of a certain category.\r\n\r\n**Abstract of the issue**\r\n- In case did the tflite conversion with enable quantization option for this model, the output class (1st rank) for the input test data(about 1000-2000 jpg images files) to 10,996 class model was completely different all from the class of input test data, and I got the unexpected output (the output class did not match in all test data and the probability-value in 1st rank is about 0.4-0.5) compared with the output for 10,806 class model.\r\n- But in disabled quantization option, the output class matched the class of the input test data, the probability-value in 1st rank is 0.9 and more, and the classification accuracy for the same test data was 90% and more.\r\n- **This issue did not occur in the model of less 10,806 classes**\r\n    - I'm investigating if this issue occurs between 10,806 and 10,996 classes.\r\n- This issue occurred on mobile device (Pixel 3 XL) but similarly reproduced on Mac too.\r\n- The method of the quantization is dynamic range quantization.\r\n\r\n**Output example**\r\n- Classes\r\n    - [ a_0, a_1, ... , a_10995 ]\r\n- Input test data class (expected output class)\r\n    - a_1234\r\n- Output class and probability-values **(Quantized)**\r\n    ```\r\n    a_4952 0.49650624\r\n    a_5831 0.044097286\r\n    a_6422 0.032513995\r\n    ...\r\n    ```\r\n-  Output class and probability-values **(Not-quantized)**\r\n    ```\r\n    a_1234 0.92802435\r\n    a_1235 0.004674452\r\n    a_13731 0.0012423205\r\n    ...\r\n    ```\r\n\r\n**Question**\r\n- Is there something that is considered to be the cause of this issue?\r\n- In case a model has the huge number of classes, in internal calculation of a model, overflow or zero-division occurred by applying dynamic range quantization to a trained model, and as a result, is there a possibility that the output result is something wrong?\r\n\r\n- A similar issue occurred in CoreML:\r\n    - In CoreML, when a model has the model more than 9,000 classes, this resulted in overflow with Neural Engine mode and GPU mode, but I could avoid this issue with enable CPU mode. \r\n- In Android, I couldn't avoid this issue with the same method.\r\n\r\n**Source code for model conversion**\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nfrom keras.models import load_model, model_from_json\r\nfrom keras.optimizers import SGD\r\n\r\ndef convert(input_path, \r\n            weights_file,\r\n            output_dir, \r\n            output_filename, \r\n            initial_lr=2e-4, \r\n            momentum=0.9, \r\n            is_quantized=False):\r\n\r\n    model = model_from_json(open(os.path.join(input_path, 'model', 'model.json')).read())\r\n    model.load_weights(os.path.join(input_path, 'model', weights_file))\r\n    model.summary()\r\n\r\n    opt = SGD(initial_lr, momentum=momentum)\r\n    model.compile(\r\n        optimizer=opt,\r\n        loss='categorical_crossentropy'\r\n    )\r\n\r\n    output_modeldir = os.path.split(input_path)[-1]\r\n    output_dir = os.path.join(output_dir, 'models', output_modeldir)\r\n\r\n    os.makedirs(output_dir, exist_ok=True)\r\n\r\n    keras_file = os.path.join(output_dir, 'weights_temp.h5')\r\n    model_ = model.save(keras_file)\r\n\r\n    converter = tf.lite.TFLiteConverter.from_keras_model_file(keras_file)\r\n\r\n    if is_quantized:\r\n        converter.post_training_quantize=True\r\n        output_filename = f'{output_filename}_quantization.tflite'\r\n    else:\r\n        output_filename = f'{output_filename}.tflite'\r\n\r\n    tflite_model = converter.convert()\r\n\r\n    tflite_file = os.path.join(output_dir, f'{output_filename}')\r\n    with open(tflite_file, 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\n    print(f'Saved [{tflite_file}]')\r\n```\r\n\r\n**Output logs**\r\n\r\n```\r\nUsing TensorFlow backend.\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\r\n\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\r\n\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\r\n\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\r\n\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\r\n\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\r\n\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n2020-10-01 11:34:43.751852: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\r\nTo enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-10-01 11:34:43.752602: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 6. Tune using inter_op_parallelism_threads for best performance.\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nimage_input (InputLayer)     (None, 128, 128, 3)       0\r\n_________________________________________________________________\r\nblock1_conv1 (Conv2D)        (None, 128, 128, 64)      1792\r\n_________________________________________________________________\r\nblock1_conv2 (Conv2D)        (None, 128, 128, 64)      36928\r\n_________________________________________________________________\r\nblock1_pool (MaxPooling2D)   (None, 64, 64, 64)        0\r\n_________________________________________________________________\r\nblock2_conv1 (Conv2D)        (None, 64, 64, 128)       73856\r\n_________________________________________________________________\r\nblock2_conv2 (Conv2D)        (None, 64, 64, 128)       147584\r\n_________________________________________________________________\r\nblock2_pool (MaxPooling2D)   (None, 32, 32, 128)       0\r\n_________________________________________________________________\r\nblock3_conv1 (Conv2D)        (None, 32, 32, 256)       295168\r\n_________________________________________________________________\r\nblock3_conv2 (Conv2D)        (None, 32, 32, 256)       590080\r\n_________________________________________________________________\r\nblock3_conv3 (Conv2D)        (None, 32, 32, 256)       590080\r\n_________________________________________________________________\r\nblock3_pool (MaxPooling2D)   (None, 16, 16, 256)       0\r\n_________________________________________________________________\r\nblock4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160\r\n_________________________________________________________________\r\nblock4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808\r\n_________________________________________________________________\r\nblock4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808\r\n_________________________________________________________________\r\nblock4_pool (MaxPooling2D)   (None, 8, 8, 512)         0\r\n_________________________________________________________________\r\nblock5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808\r\n_________________________________________________________________\r\nblock5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808\r\n_________________________________________________________________\r\nblock5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808\r\n_________________________________________________________________\r\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0\r\n_________________________________________________________________\r\ngap_vgg16gap (GlobalAverageP (None, 512)               0\r\n_________________________________________________________________\r\nfc1_vgg16gap (Dense)         (None, 4096)              2097152\r\n_________________________________________________________________\r\nbn1_vgg16gap (BatchNormaliza (None, 4096)              16384\r\n_________________________________________________________________\r\nrelu1_vgg16gap (Activation)  (None, 4096)              0\r\n_________________________________________________________________\r\ndo1_vgg16gap (Dropout)       (None, 4096)              0\r\n_________________________________________________________________\r\nfc2_vgg16gap (Dense)         (None, 4096)              16777216\r\n_________________________________________________________________\r\nbn2_vgg16gap (BatchNormaliza (None, 4096)              16384\r\n_________________________________________________________________\r\nrelu2_vgg16gap (Activation)  (None, 4096)              0\r\n_________________________________________________________________\r\ndo2_vgg16gap (Dropout)       (None, 4096)              0\r\n_________________________________________________________________\r\npredictions_vgg16gap (Dense) (None, 10996)             45050612\r\n=================================================================\r\nTotal params: 78,672,436\r\nTrainable params: 71,020,788\r\nNon-trainable params: 7,651,648\r\n_________________________________________________________________\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\r\n\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\n`normal` is a deprecated alias for `truncated_normal`\r\n2020-10-01 11:34:49.269923: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-10-01 11:34:49.270019: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2020-10-01 11:34:49.759721: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize\r\n2020-10-01 11:34:49.759743: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.007ms.\r\n2020-10-01 11:34:49.759750: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   function_optimizer: function_optimizer did nothing. time = 0.004ms.\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/lite/python/util.py:238: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.convert_variables_to_constants`\r\nWARNING:tensorflow:From /anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `tf.compat.v1.graph_util.extract_sub_graph`\r\n/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/lite/python/lite.py:769: UserWarning: Property post_training_quantize is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\r\n  \" instead.\" % name)\r\n2020-10-01 11:34:50.957910: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-10-01 11:34:50.958019: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session\r\n2020-10-01 11:34:51.559693: E tensorflow/core/grappler/grappler_item_builder.cc:637] Init node block1_conv1/kernel/Assign doesn't exist in graph\r\nSaved [./weights_quantization.tflite]\r\n```\r\n\r\n", "comments": [" Hi @jaeyoo , Is there an anything update or a progressing about a solution for this issue? \r\nThanks.", "Hi @pdkz I am still working on it. Please stay tuned. thanks!", "@pdkz Hi pdkz, Sorry for belated reply. Could you please check the conversion with tf-nightly >= 2.4.0-dev20201001? I am still investigating, but I want to check it in parallel on both sides. Thanks.", "Hi @jaeyoo Never mind, I'm grateful for your support. \r\nOkay, I'll try to convert the model using tf-nightly 2.4.0 greater. I'd like to be helpful to you even just a little.\r\nThanks.", "Hi @jaeyoo\r\nI tried to do the conversion in tf-nightly 2.5.0.\r\nAt the moment, it seems that I get the expected output from the converted model  when I checked the accuracy. Thanks for your advices and supports!\r\nBut it's not clear what's caused it, so I'm also investigating the source code to find out the cause for this issue.\r\nIs this issue known or latent bug in TF 1.x ? (It seems I couldn't find posts about it in issue tracker. )\r\nPlease tell me if you find out the cause or the hint. Thanks", "Hi @pdkz Thank you for sharing good news! Yes... honestly speaking, I couldn't reproduce the result at the latest TF as you saw, and I tried to dig into where the cause was, but I couldn't find it yet. also, it wasn't known issue, so I was surprised at the first glance at this issue. Please let me inform you if I can find it. Thank you!"]}, {"number": 43698, "title": "TF2 TensorArray with multi-dimensional tensor wrote in it will be stacked into None shape tensor in Autograph mode", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux lz 5.4.0-48-generic #52~18.04.1-Ubuntu`\r\n- TensorFlow installed from (source or binary):  `(in conda env) pip install tensorflow-gpu -i https://pypi.tuna.tsinghua.edu.cn/simple`\r\n- TensorFlow version (use command below): `v2.3.0-rc2-23-gb36436b087 2.3.0`\r\n- Python version: `3.8`\r\n- CUDA/cuDNN version: `conda install cudatoolkit=10.1 cudnn=7.6.5`\r\n- GPU model and memory:  ` 4 x Titan xp 12GB`\r\n\r\n**Describe the current behavior**\r\n\r\nI use `tf.TensorArray` to dynamically store the specified number of multi-dimensional Tensor, and try to convert the TensorArray into Tensor which will be fed into the next network layer. I use `@tf.function` decorator to convert pythonic code into TF's AutoGraph mode to accelerate the training process. Specifically, take the following code as an example:\r\n``` python\r\nimport tensorflow as tf\r\n\r\nclass TestModel(tf.keras.Model):\r\n\r\n    def __init__(self, N):\r\n        super(TestModel, self).__init__()\r\n        self.conv_first = tf.keras.layers.Conv2D(4, (3, 3))\r\n        self.nframe = N\r\n\r\n    def __call__(self, x):\r\n\r\n        aligned_fea = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n\r\n        def cond(i, N, fea_col):\r\n            return i < N\r\n\r\n        def body(i, N, fea_col):\r\n            fea_col = fea_col.write(i, x)\r\n            i = tf.add(i, 1)\r\n            return i, N, fea_col\r\n\r\n        _, _, aligned_fea = tf.while_loop(cond, body, [0, self.nframe, aligned_fea])\r\n\r\n        tf.print(\"aliged_fea shape:\", aligned_fea.size())\r\n\r\n        t = aligned_fea.stack()\r\n        tf.print(\"t shape:\", t.shape)\r\n\r\n        # without these two lines of reshaping the stacked tensor coercively, the t will have no first dimension\r\n        tt = tf.reshape(t,[self.nframe, 8, 4, 6,3])\r\n        tf.print(\"tt shape:\", tt.shape)\r\n\r\n        return t\r\n\r\n@tf.function\r\ndef foo(tm):\r\n    x = tf.ones([8,4,6,3], dtype=tf.float32)\r\n    output = tm(x)\r\n\r\nnframe = 10\r\ntm = TestModel(nframe)\r\nfoo(tm)\r\n```\r\nThe output of the above code is:\r\n```\r\naliged_fea shape: 10\r\nt shape: TensorShape([None, 8, 4, 6, 3])\r\ntt shape: TensorShape([10, 8, 4, 6, 3])\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nI have tested the code in Eager mode, that is to remove the decorator `@tf.function`. Everything goes well. But error will be thrown when the Autograph mode starts, since the output tensor doesn't have got the first dimension of itself shape, and can not be feed into the next layer of the network.\r\n\r\nMoreover, if I write scalar into TensorArray, whether I use Eager or Autograph mode, the output of the shape of Tensor will be fine with true numbers.\r\n\r\nMy problem is the TensorArray, whatever it contains scalar or multi-dimensional tensor, should automatically detect the size of itself and stack into the tensor with full shape in both Eager and Graph mode,  and the output should be as follows so to speak:\r\n```\r\naliged_fea shape: 10\r\nt shape: TensorShape([10, 8, 4, 6, 3])\r\ntt shape: TensorShape([10, 8, 4, 6, 3])\r\n```\r\n\r\nOne workaround to deal with this problem is just to reshape the stacked tensor after `TensorArray.stack()` but it is not robust for my codes. Thus, I wonder if this is a bug or just the feature to fit something unknown.\r\n\r\nI am struggling with this for a long time. Appreciate your reply in advance.", "comments": ["I am able to replicate this issue, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/62222f48460bdd79381394fd274bfb33/untitled418.ipynb)", "Could you try using a fixed size TensorArray `tf.TensorArray(dtype=tf.float32, size=self.nframe)`", "@saxenasaurabh Oh yes! TensorArray with fixed size does work! Thanks for your suggestion. But it is still confusing the reason why TensorArray could not be stacked into the Tensor with explicit dimensions.", "Because the size of the TensorArray depends on the algorithm in the while loop and it is hard to statically infer the size of the TA after the loop.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43698\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43698\">No</a>\n", "@saxenasaurabh Hi what do you think of this problem? [#4403](https://github.com/tensorflow/tensorflow/issues/44073#issuecomment-713113752)\r\nThe problem of TensorArray still exists.", "Possibly a shape inference issue. Once we isolate a minimal repro I can look into a fix."]}, {"number": 43695, "title": "[Draft] Adding Multi device iterator Serialization and De-serialization", "body": "Context: \r\nThe idea is to be able to save and restore iterators (The iterators that can be saved) for multi worker training. This will allow for training to be resumed mid-epoch instead. This reduces wasted compute cycles when restarting training jobs.\r\nWith this change the Backup and restore keras callback can include the iterator (if supported), and actually restore the keras model to a state mid epoch also.\r\n\r\nDescription:\r\nAdding a serialization and de-serialization to multi device iterator.\r\nAdded Save and Restore in both the Multi Device Resource and the Multi Device Buffer.\r\n\r\nDesign.\r\nNew operators added for Multi Device Iterators to serialize and de-serialize\r\nMulti device buffer will save its buffer and host_iterator.\r\nMulti device Resource will restore by creating a new buffer because the host iterator is const.\r\nIt will then call the multi device buffer to restore its own buffer.\r\n\r\nTest.\r\nUsing a Range Dataset(10)\r\nSave after 0-5, then restore in a new session, and verify 6-9\r\n", "comments": ["@sboshin  This PR is in draft, any update on this? Please. Thanks!", "@sboshin  Any update on this PR? Please. Thanks!", "It has been 16 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@sboshin Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 43694, "title": "[Intel MKL] Suppressing auto_mixed_precision_test for non-avx512f machine (broadwell)", "body": "[Intel MKL] Suppressing auto_mixed_precision_test for non-avx512f machine like broadwell.", "comments": ["@noim210  Can you please check @penpornk's comments and keep us posted ? Thanks!", "@noim210  Any update on this PR? Please. Thanks!\r\n", "@noim210  Can you please resolve conflicts? Thanks!", "(Changing the tag back to awaiting_response since I already replied in a comment.)", "It looks like somebody refractor the code and move some parts from python/xx folder to python/util folder. That's why \"`//tensorflow/python/BUILD`\" file has some changes. \r\nThe current changes what you are seeing, that are not my change, I just make sure current-master file and my branch's \"`//tensorflow/python/BUILD`\" file is same. That's why those changes are necessary (again to match with master).\r\n", "The changes you have don't make the file up-to-date, though. They are undoing the newer changes to the file. For example, https://github.com/tensorflow/tensorflow/blob/2f6535b09c258f021b7bff50266348c0d1dbfb13/tensorflow/python/BUILD#L1594\r\nThis line was introduced yesterday and is still there in the most recent commit. Your commit is deleting it. That's why I think your file is not up-to-date.", "That's interesting. My master is saying those files need to be deleted. Anyway, thank you for sharing the file link. I will make sure my file is same as your link.\r\n", "I fixed the problem. Actually, my master might be older or I forget to merge my master. \r\nNow I pull master and updated it accordingly.\r\nThank you for helping me fix this."]}, {"number": 43692, "title": "Update README.md", "body": "update readme", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43692) for more info**.\n\n<!-- need_sender_cla -->", "@AnkitSharma29  Can you please sign CLA. Thanks!"]}, {"number": 43691, "title": "[tf.data service] extend data service ops tests and sanitise api calls", "body": "This PR addresses the following:\r\n\r\n- Sanitize the test cases by utilizing the newly created test base.\r\n- Fix the parameter name from `dispatcher` to `cluster` in the `make_distributed_range_dataset` method as per API calls.\r\n- Extend test cases in `data_service_ops_ft_test`\r\n\r\ncc: @aaudiber ", "comments": ["@aaudiber  thanks for the review. Also, I observed that when the dispatchers are restarted without fault tolerance, the new dispatcher is unable to find the `dataset_id` of the already created dataset and thus perpetually throws a `warn` message until it times out. Should the test base handle this by making a pre-check for fault-tolerance and immediately throwing an error in case the restart is called without any `work_dir`? Please let me know your opinion on this. Thanks.", "@kvignesh1420 Ideally we could improve handling for non-fault-tolerant dispatcher restarts, so that the error doesn't occur in the first place. I think this could be done by having dispatchers generate uuids when they are started and sharing them with registered workers, so that we can identify when the dispatcher restarts without fault tolerance. When the dispatcher identifies that a heartbeating worker was previously registered with a different dispatcher, the dispatcher can tell the worker to drop all of its outstanding tasks."]}, {"number": 43690, "title": "InvalidArgumentError with `Conv1D` which has the same `dilation_rate` as the batch size, using `tf.keras.Model.fit`, on TPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n- Accelerator: TPU\r\n\r\n**Describe the current behavior**\r\nWhen I optimized `Conv1D`, which has the same `dilation_rate` as the batch size, using `tf.keras.Model.fit`, I encountered the following error.\r\n\r\n```\r\nInvalidArgumentError: 9 root error(s) found.\r\n  (0) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=\"BatchToSpaceND\" op_name=\"dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND\"}. Constraint: multiple_of: 8, stride: 1\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]\r\n\t [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_197]]\r\n  (1) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=\"BatchToSpaceND\" op_name=\"dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND\"}. Constraint: multiple_of: 8, stride: 1\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]\r\n\t [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_157]]\r\n  (2) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=\"BatchToSpaceND\" op_name=\"dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND\"}. Constraint: multiple_of: 8, stride: 1\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]\r\n\t [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_137]]\r\n  (3) ...\r\n```\r\nThis error does not occur if I forward using `strategy.run`.\r\nA detailed error log is attached below.\r\n\r\n**Describe the expected behavior**\r\nI want to optimize it without errors.\r\n\r\n**Standalone code to reproduce the issue**\r\nColab https://colab.research.google.com/drive/1W-eM2EwzYSMhHyN4GDUd22nKSLrrka3v?usp=sharing\r\ngist https://gist.github.com/Hiroshiba/9c1914dbf57228204a4ed1f661168d36\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n<details>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-5-5265a4b37c17> in <module>()\r\n     13     steps_per_epoch=5,\r\n     14     epochs=1,\r\n---> 15     shuffle=False,\r\n     16 )\r\n\r\n14 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n    106   def _method_wrapper(self, *args, **kwargs):\r\n    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n--> 108       return method(self, *args, **kwargs)\r\n    109 \r\n    110     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n   1101               logs = tmp_logs  # No error, now safe to assign to logs.\r\n   1102               end_step = step + data_handler.step_increment\r\n-> 1103               callbacks.on_train_batch_end(end_step, logs)\r\n   1104         epoch_logs = copy.copy(logs)\r\n   1105 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)\r\n    438     \"\"\"\r\n    439     if self._should_call_train_batch_hooks:\r\n--> 440       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)\r\n    441 \r\n    442   def on_test_batch_begin(self, batch, logs=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)\r\n    287       self._call_batch_begin_hook(mode, batch, logs)\r\n    288     elif hook == 'end':\r\n--> 289       self._call_batch_end_hook(mode, batch, logs)\r\n    290     else:\r\n    291       raise ValueError('Unrecognized hook: {}'.format(hook))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs)\r\n    307       batch_time = time.time() - self._batch_start_time\r\n    308 \r\n--> 309     self._call_batch_hook_helper(hook_name, batch, logs)\r\n    310 \r\n    311     if self._check_timing:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)\r\n    340       hook = getattr(callback, hook_name)\r\n    341       if getattr(callback, '_supports_tf_logs', False):\r\n--> 342         hook(batch, logs)\r\n    343       else:\r\n    344         if numpy_logs is None:  # Only convert once.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)\r\n    959 \r\n    960   def on_train_batch_end(self, batch, logs=None):\r\n--> 961     self._batch_update_progbar(batch, logs)\r\n    962 \r\n    963   def on_test_batch_end(self, batch, logs=None):\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs)\r\n   1014     if self.verbose == 1:\r\n   1015       # Only block async when verbose = 1.\r\n-> 1016       logs = tf_utils.to_numpy_or_python_type(logs)\r\n   1017       self.progbar.update(self.seen, list(logs.items()), finalize=False)\r\n   1018 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)\r\n    535     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n    536 \r\n--> 537   return nest.map_structure(_to_single_numpy_or_python_type, tensors)\r\n    538 \r\n    539 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)\r\n    633 \r\n    634   return pack_sequence_as(\r\n--> 635       structure[0], [func(*x) for x in entries],\r\n    636       expand_composites=expand_composites)\r\n    637 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)\r\n    633 \r\n    634   return pack_sequence_as(\r\n--> 635       structure[0], [func(*x) for x in entries],\r\n    636       expand_composites=expand_composites)\r\n    637 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)\r\n    531   def _to_single_numpy_or_python_type(t):\r\n    532     if isinstance(t, ops.Tensor):\r\n--> 533       x = t.numpy()\r\n    534       return x.item() if np.ndim(x) == 0 else x\r\n    535     return t  # Don't turn ragged or sparse tensors to NumPy.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)\r\n   1061     \"\"\"\r\n   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\r\n-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access\r\n   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr\r\n   1065 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)\r\n   1029       return self._numpy_internal()\r\n   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access\r\n-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access\r\n   1032 \r\n   1033   @property\r\n\r\n/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: 9 root error(s) found.\r\n  (0) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=\"BatchToSpaceND\" op_name=\"dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND\"}. Constraint: multiple_of: 8, stride: 1\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]\r\n\t [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_197]]\r\n  (1) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=\"BatchToSpaceND\" op_name=\"dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND\"}. Constraint: multiple_of: 8, stride: 1\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]\r\n\t [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_157]]\r\n  (2) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=\"BatchToSpaceND\" op_name=\"dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND\"}. Constraint: multiple_of: 8, stride: 1\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]\r\n\t [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_137]]\r\n  (3) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=\"BatchToSpaceND\" op_name=\"dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND\"}. Constraint: multiple_of: 8, stride: 1\r\n\tTPU compilation failed\r\n\t [[{{node tpu_compile_succeeded_assert/_4759666286058538557/_4}}]]\r\n\t [[tpu_compile_succeeded_assert/_4759666286058538557/_4/_187]]\r\n  (4) Invalid argument: {{function_node __inference_train_function_5549}} Compilation failure: Reshape's input dynamic dimension is decomposed into multiple output dynamic dimensions, but the constraint is ambiguous and XLA can't infer the output dimension %reshape.40 = f32[8,8,2047,128]{3,2,1,0} reshape(f32[<=64,2047,128]{2,1,0} %reshape.39), metadata={op_type=\"BatchToSpaceND\" op_name=\"dilated_conv1d_1/conv1d_1/conv1d/BatchToSpaceND\"}. Constraint: multiple_of: 8, stride: 1\r\n\tTPU compilation failed\r\n\t [[{{ ... [truncated]\r\n```\r\n\r\n</details>", "comments": ["I have tried in colab with TF version 2.3 and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/09a9efcf14b3111df6a41add407b2a4d/untitled408.ipynb).However i am seeing different error message in TF nightly versions(`2.4.0-dev20200930`).Please find the gist [here.](https://colab.research.google.com/gist/ravikyram/9e5e9c9a9d8f5835890ec4b14fda32f5/untitled409.ipynb).Thanks!", "Removing assignment as I'm no longer working on Python TensorFlow. Sorry!", "BatchToSpaceND is not longer called on TPU. I think we should fix this gist reported by @ravikyram instead: https://colab.research.google.com/gist/ravikyram/9e5e9c9a9d8f5835890ec4b14fda32f5/untitled409.ipynb Is there anyone from TF team can help? ", "That error is most likely due to a version mismatch between the TF client and the backend. Let's check back in after the next TensorFlow release when it's on Cloud TPUs.", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/adccbe43b6b16976bab8a00287641888/43690.ipynb). Thanks!", "Hi @sachinprasadhs, the problem should have been fixed by @yunxing. Could you try TF nightly?", "@rxsang \r\nI am able to replicate the issue on tf [2.6 as well], please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e5d80e3f840a4231194db960c92b14b0/untitled627.ipynb)", "Any updates here? Conv1D with delation still not working. I tryed tf2.6 and tf-nightly [2.7.0.dev20210831-cp37-cp37m-manylinux2010_x86_64]", "> @rxsang\r\n> I am able to replicate the issue on tf [2.6 as well], please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e5d80e3f840a4231194db960c92b14b0/untitled627.ipynb)\r\n\r\nHello, I found the temporary solution:\r\nYou should set drop_remainder to \"True\" to hold the size of the batch.\r\n\r\n> `dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(buffer_size=batch_size).repeat().batch(batch_size, **drop_remainder=True**)`", "Hi @Hiroshiba !\r\nWe are checking to see whether you still need help in this issue . Did you check with above [suggestion](https://github.com/tensorflow/tensorflow/issues/43690#issuecomment-911569137) yet? Thanks!", "I don't know, but let's close it!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43690\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43690\">No</a>\n"]}, {"number": 43689, "title": "No library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: \r\n- Python version: 3.8.0\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1\r\n- GPU model and memory:\r\n\r\n```bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package```\r\n\r\n```Repository command failed\r\nNo library found under: /usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudart.so.11.1\r\nINFO: Elapsed time: 0.344s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package```", "comments": ["I may be wrong, but won't you need to use `bazel build --config=cuda` for cuda support?", "@summa-code,\r\nCould you please provide the exact sequence of commands / steps that you executed before running into the problem?\r\n\r\nAlso, instead of CUDA 11.1 could you please try building TensorFlow using CUDA 11.0 and cuDNN 8, and check if it works. Thanks!", "That is the command that i listed above, it works with 11.0 but Nvidia recently released 11.1 and also does a symblink to 11.0, but Bazel script is looking for 11.1. So this is a bug in Bazel script.\r\n\r\n@pmeckoni That is why we use \"configure\" command before building", "This seems to be blocking Jax builds on CUDA 11.1 as well (by way of the TF Bazel config). ", "AFAIK, we don't currently officially support building with CUDA 11.1. @sanjoy is that right?", "Yes, CUDA 11.1 is not supported yet.", "Closing, i think the project moved on and we have 11.2 and i dont see any issues", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43689\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43689\">No</a>\n"]}, {"number": 43686, "title": "kernel changes will break arc", "body": "@tensorflow/micro\r\n\r\nOnce #43682 is merged, the building with TARGET=`arc` and `TAGS=arc_mli` will fail. This is mostly as a result of namespace changes.\r\n\r\nTagging @dzakhar  and @JaccovG.\r\n", "comments": ["Hi @advaitjain, \r\nThank you for warning us. We're looking at it. Hopefully we'll make changes soon"]}, {"number": 43685, "title": "[INTEL MKL] MKL DNN 0.x code cleanup - AddN op", "body": "DNN 0.x cleanup of MklAddN Op:\r\n\r\n(1) Remove all DNN 0.x related code\r\n\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 43684, "title": "Kernel changes break xtensa_hifimini", "body": "@tensorflow/micro\r\n\r\nOnce #43682 is merged, the building with `TARGET=xtensa_hifimini` and TAGS=`xtensa_hifimini` will fail. This is mostly as a result of namespace changes. ", "comments": []}, {"number": 43683, "title": "[ROCm] Fix for ROCM CSB breakage - 200930", "body": "The following commit introduces a new subtest that is failing on the ROCm platform\r\n\r\nhttps://github.com/tensorflow/tensorflow/commit/326a4b419c07141eeb82a8f1087dfe55fbec9187#diff-d56271856aeccc4bf59c34b80b40bafa\r\n\r\nThe failure we see is\r\n\r\n```\r\n======================================================================\r\nERROR: testFoo(False) (__main__.FunctionInlineControlTest)\r\ntestFoo(False) (__main__.FunctionInlineControlTest)\r\ndecorated(False)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/function_test_gpu.runfiles/absl_py/absl/testing/parameterized.py\", line 267, in bound_param_test\r\n    test_method(self, testcase_params)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1799, in decorated\r\n    return func(self, *args, **kwargs)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/function_test.py\", line 1654, in testFoo\r\n    self.assertEqual(noinline, Cell.definition.attr[\"_noinline\"].b)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/function.py\", line 327, in definition\r\n    context.add_function(self._c_func.func)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py\", line 2361, in add_function\r\n    context().add_function(fdef)\r\n  File \"/root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py\", line 1126, in add_function\r\n    pywrap_tfe.TFE_ContextAddFunction(self._handle, fn)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Attempting to add a duplicate function with name: Cell_CACqFJ4l9ho where the previous and current definitions differ. Previous definiton: signature {\r\n  name: \"Cell_CACqFJ4l9ho\"\r\n...\r\n...\r\nattr {\r\n  key: \"_noinline\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\n...\r\n...\r\n and current definition: signature {\r\n  name: \"Cell_CACqFJ4l9ho\"\r\n...\r\n...\r\nattr {\r\n  key: \"_noinline\"\r\n  value {\r\n    b: false\r\n  }\r\n}\r\n...\r\n...\r\n```\r\n\r\nThe difference in the value of \"_noinline\" is to be expected.\r\n\r\nI do not understand enough about the testcase / underlying functionality to do root cause analysis here.\r\n\r\nThis error does not seem to be ROCM specific, but it is causing ROCm CSB to fail, and hence need to skip it for ROCm.\r\n\r\n\r\n--------------------------------------\r\n\r\n/cc @cheshire @chsigg @nvining-work ", "comments": ["@cheshire @chsigg gentle ping"]}, {"number": 43682, "title": "Add ability to register particular kernel variants for fully_connected.", "body": "This is the first rollout of a new API that will enable the registration of\r\nspecific kernel variants if an application wants to reduce binary size.\r\n\r\nHigh level summary of the changes:\r\n * Kernel implementations can define custom TfLiteRegistration structs with\r\n   pointers to functions that have only a subset of the functionality.\r\n\r\n * By using these custom TfLiteRegistration structs via the\r\n   MicroMutableOpResolver, the application can achieve more control over the\r\n   binary size.\r\n\r\n * This change modifies the keyword_benchmark and fully_connected kernel (CMSIS\r\n   implementation) to demonstrate what this would look like.\r\n\r\n * We are also moving to a flat tflite namespace (additional rationale in\r\n   https://abseil.io/tips/130) and this change is an incremental step in that\r\n   direction.\r\n\r\n * It is not entirely clear why, but for the linker to be able to drop the\r\n unused functions, the kernel.cc file must use a global TfLiteRegistration\r\n struct as opposed to returning a temporary. We do not know exactly why that is\r\n the case but will follow that pattern for now.\r\n\r\n * Lastly, note that this change intentionally does not attempt to refactor some\r\n   of the name-aliasing in the kernel implementations (e.g. Eval, EvalInt8,\r\n   EvalQuantized ... all exist in the same kernels/cmsis-nn/fully_connected.cc).\r\n   This cleanup will be the subject of future changes.\r\n\r\nBreaking change:\r\n This change breaks the build for other optimized kernels that have a\r\n specialized fully_connected implementation (e.g. xtensa_hifimini\r\n and arc_mli)\r\n\r\n\r\nSize difference as a result of this change:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 keyword_benchmark TARGET=sparkfun_edge TAGS=cmsis-nn\r\nsize tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/keyword_benchmark\r\n```\r\n\r\n|                |    text   | data |     bss   |    dec \r\n| ----          |   ----:    |  ----: |     ----:    |    ----:\r\n| default    |  95836 |  124 | 104432 | 200392\r\n| int8-only |  93996 |  124 | 104432 | 198552\r\n\r\nApprox 2KB saved in the code size.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "@njeffrie @nkreeger look at changes in kernels/fully_connected.cc and kernels/cmsis-nn/fully_connected.cc for now. Once #43571 is merged, a lot of the other changes should go away.\r\n", "@advaitjain  Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 43681, "title": "tf.function expects to return a Tensor error, which is what I'm returning as well", "body": "```python\r\n# @tf.function # does not work if we uncomment this line\r\ndef only_specific_companies(example: Dict[str, tf.Tensor], valid_prefixes=[\"Google\", \"Apple\", \"LinkedIn\", \"Facebook\"]):\r\n    curr_company = example['company']\r\n    example['company'] = \"Other\"\r\n    \r\n    def company_setter_fn_gen(m):\r\n        def set_company_fn():\r\n            example['company'] = m\r\n        return set_company_fn\r\n    \r\n    set_company_fn = None\r\n    for m in valid_prefixes:\r\n        if tf.strings.regex_full_match(curr_company, f\"^{m}.*\"):\r\n            set_company_fn = company_setter_fn_gen(m)\r\n\r\n    if set_company_fn:\r\n        set_company_fn()\r\n\r\n    tf.print(example)\r\n    return example\r\n\r\nonly_specific_companies({\"company\": tf.constant(\"Google Inc\")})\r\n```\r\nReproduced at https://colab.research.google.com/drive/1Sp3PSl16LUJ0NbsniWtVaJitnm7TdaBX?usp=sharing\r\n\r\n", "comments": ["Sorry, my bad. Got confused with the error message. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43681\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43681\">No</a>\n"]}, {"number": 43680, "title": "Expose TF_RegisterFilesystemPlugin C API", "body": "This PR is part of the effort for modular file system.\r\n\r\nIn modular file system `RegisterFilesystemPlugin` was a C++ API and there is no directly way to call this API through python.\r\n\r\nThis PR exposes C API `TF_RegisterFilesystemPlugin`, so that it can be used in python bindings.\r\n\r\nIn addition, an experimental API `tf.experimental.register_filesystem_plugin` has been setup\r\nso that it is possible to register a modular file system plugin with:\r\n```\r\ntf.experimental.register_filesystem_plugin(plugin_path)\r\n```\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["This seems to have a lot of breakages, can you take a look please?", "Thanks @mihaimaruseac, I have updated the PR and all tests seem pass now. (I did an update to api_golden to cover the `tf.experimental.register_filesystem_plugin(plugin_path)`).\r\n\r\nPlease take a look and let me know if there are any other issues.", "@mihaimaruseac I noticed that GitHub is showing merge conflict. Do I need to rebase and update the PR?", "It should be good for now, it got imported internally and it seems most tests are passing. We'll need to wait for API owners approval, that would in ~24 hours.", "Thanks @mihaimaruseac! Let me know if there is anything else needs to be done.", "@yongtang  Can you please resolve conflicts? Thanks!", "@mihaimaruseac Wondering if there is any update on this PR or do you see if rebase and resolve the conflict is needed?", "Did not get approval yet from TF API owners, I think that due to team changes we will have to wait for the next week. Sorry for the delay.\r\n\r\nThere is no need to solve conflicts, I solved them internally already"]}, {"number": 43679, "title": "relaxing numpy requirement", "body": "Numpy is fixed to `<1.19` in current main branch. Justification in [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L64) is that it was waiting for https://github.com/numpy/numpy/pull/15355 to be merged. But it seems to be merged and deployed. Is there any reason to maintain the current fixed version of numpy?\r\n", "comments": ["The main issue is that the numpy PR breaks TF compilation. So we introduced the numpy bound.", "Is there any plans to fix it from either Tensorflow or Numpy side?", "Yes, See #40728 for example.", "@mihaimaruseac Are there commits in a new branch that we can merge? This is blocking our internal stack and we want to rebuild tensorflow 2.3 with the right patches applied.", "#40728 is the only public branch at this time, unfortunately.", "@mihaimaruseac according to https://github.com/tensorflow/tensorflow/pull/40728#issuecomment-698217353 the issue seems to be fixed in 2 commits already ? Looking at the dates they should have made it into 2.3.\r\n\r\nAm I missing something ?", "Neither of those two commits has been made on the `r2.3` branch (that is, they have not been made before branch was cut, nor have they been cherry-picked to the branch). They are included in nightly builds and will be included in TF 2.4.", "@mihaimaruseac Do they fix the numpy issue or are they unrelated ? because the setup.py in master branch still forces numpy < 1.19 (I am assuming nightlies also have this limitation).\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py", "I think we need a different PR to relax `setup.py`.", "Thanks for the info! This helps us in building a custom wheel to use internally.", "Note that `setup.py` is only used to determine which `numpy` wheel gets installed on your system. The API breakage is caused by the Numpy C++ API and only affects compiling TF. If you don't need to compile TF but only use a newer numpy, you can just ignore the bound in `setup.py`.", "Fixed by aafe25d", "is there any plans to add a release any time soon to include this?", "There is going to be the 2.4 release", "is there any approximate date for the 2.4 release? Mostly wondering as we currently had to build our own wheel to unlock upgrading wheel and it would be nice to have it synced w OSS.", "Yes, [branch cut is next week](https://groups.google.com/a/tensorflow.org/g/developers/c/jEaxXK6-C5k) and then we start the release process which can take 2 weeks if all goes well"]}, {"number": 43678, "title": "add file_paths attr to image dataset", "body": "Adding an attribute (file_paths) to the dataset returned from image_dataset_from_directory to enable access to the image filepaths.\r\nResolves tensorflow/tensorflow/#40203", "comments": []}, {"number": 43677, "title": "My code is much slower in TF2 than TF1", "body": "**System information**\r\n- Colab\r\n\r\n\r\n**Primary issue:** My code is much slower on TF2 compared to TF1\r\n\r\nYou can verify this by running each notebook and seeing the difference in elapsed times.\r\n\r\nI suspect it's a **tf.data** issue, because when you increase the value of ntopics (in TF2), the code slows down even more. The TF1 code is unaffected by changing ntopics. This parameter changes the size of the simulation dataset.\r\n\r\n\r\n**Secondary issue:** My TF1 code is slower on GPU than CPU.\r\n\r\nYou can verify this by changing the TF1 code runtime and seeing the elapsed time goes up.\r\n\r\nThis problem has not (yet) been reproduced on TF2 because of the above primary issue.\r\n\r\nI don't know if this is possible to fix because there are a lot of sequential operations in the main calculateloss function. But if you see anything that could be optimized in any way (preferably in TF2) please point it out!\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n- [TF1 colab notebook](https://colab.research.google.com/drive/1YVxJ7-HMBctlI8rR6KsaQRDqYjPDSQIe?usp=sharing)\r\n\r\n- [TF2 colab notebook](https://colab.research.google.com/drive/1-gF4IZi4wSsnAt8hFJpcVc0ALQcWAJni?usp=sharing)\r\n\r\nThank you", "comments": ["I am able to replicate this issue, please find the gist for [tf 1](https://colab.research.google.com/gist/Saduf2019/d131da06187a4522e327c06b0dea922c/untitled423.ipynb) and [tf2](https://colab.research.google.com/gist/Saduf2019/5252c588d4e8a0d7c65f322748b7fb4b/untitled418.ipynb)", "Hi @osotsia, there is a lot of custom code here which makes it difficult to debug. You might want to try Stack Overflow where there's a larger community that can help you to better optimize your TF2 code.\r\n\r\nHowever, a few things to keep in mind when using tf.function is that it works best with TensorFlow ops, as numpy and Python calls are converted to constants. If you prefer numpy you might want to [check out the TF numpy API](https://www.tensorflow.org/guide/tf_numpy). You should also check to see that you aren't doing any unnecessary retracing, as that can also cause a slowdown. You can find out more about common tf.function pitfalls [in the guide.](https://www.tensorflow.org/guide/function#setup)\r\n\r\nAs for the GPU concern, we recently [released a guide](https://www.tensorflow.org/guide/gpu_performance_analysis) on how to debug your performance on GPUs. It's targeted to TF2, but you might still find it helpful.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43676, "title": "Converter fails when using a channel first model and batchnorm", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 18:04\r\n- TensorFlow installed from: binary (mini conda)\r\n- TensorFlow version: 2.2\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n```\r\n#  A VERY SIMPLE TWO LAYER MODEL WHICH FAILS ONLY WHEN CHANNELS FIRST\r\n\r\nDATA_FORMAT = \"channels_first\"\r\nbackend.set_image_data_format(DATA_FORMAT)\r\nimg_inputs = keras.Input(shape=(3, 384, 384))\r\nx = layers.Conv2D(8, kernel_size=3, strides=1, padding='same', use_bias=False, data_format=DATA_FORMAT,\r\n                  activation=None, name='conv')(img_inputs)\r\nx = layers.BatchNormalization(axis=1, epsilon=1e-3, momentum=0.999, name='BN')(x)\r\nmodel = keras.Model(inputs=img_inputs, outputs=x, name=\"channel_first_test\")\r\nmodel.summary()\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n# fails with and without these next options, (but without even just Conv2D also fails with channels first)\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.allow_custom_ops=True\r\nconverter.experimental_new_converter =True\r\n\r\ntflite_model = converter.convert()\r\n\r\n```\r\n\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n\r\n2020-09-30 17:26:45.007381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-09-30 17:26:45.037941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.038216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-09-30 17:26:45.038315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-30 17:26:45.039124: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-09-30 17:26:45.039929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-09-30 17:26:45.040053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-09-30 17:26:45.040893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-30 17:26:45.041375: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-30 17:26:45.043242: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-30 17:26:45.043302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.043595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.043841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-09-30 17:26:45.044038: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-09-30 17:26:45.047755: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3999980000 Hz\r\n2020-09-30 17:26:45.048058: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557076b6a340 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-30 17:26:45.048068: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-30 17:26:45.048152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.048409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-09-30 17:26:45.048426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-30 17:26:45.048433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-09-30 17:26:45.048439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-09-30 17:26:45.048445: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-09-30 17:26:45.048451: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-30 17:26:45.048456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-30 17:26:45.048462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-30 17:26:45.048488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.048749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.048988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-09-30 17:26:45.049004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-30 17:26:45.104115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-30 17:26:45.104128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-09-30 17:26:45.104132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-09-30 17:26:45.104216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.104484: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.104733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.104974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9708 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-09-30 17:26:45.106002: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55707a4518d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-30 17:26:45.106009: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\nModel: \"channel_first_test\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(None, 3, 384, 384)]     0         \r\n_________________________________________________________________\r\nconv (Conv2D)                (None, 8, 384, 384)       216       \r\n_________________________________________________________________\r\nBN (BatchNormalization)      (None, 8, 384, 384)       32        \r\n=================================================================\r\nTotal params: 248\r\nTrainable params: 232\r\nNon-trainable params: 16\r\n_________________________________________________________________\r\n2020-09-30 17:26:45.464048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.464203: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-09-30 17:26:45.464263: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-09-30 17:26:45.464463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.464599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-09-30 17:26:45.464618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-30 17:26:45.464625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-09-30 17:26:45.464631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-09-30 17:26:45.464637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-09-30 17:26:45.464643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-30 17:26:45.464649: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-30 17:26:45.464655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-30 17:26:45.464680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.464824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.464947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-09-30 17:26:45.464961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-30 17:26:45.464964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-09-30 17:26:45.464966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-09-30 17:26:45.465005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.465150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.465279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9708 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-09-30 17:26:45.466142: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-09-30 17:26:45.466150: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.002ms.\r\n2020-09-30 17:26:45.466152: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.\r\n2020-09-30 17:26:45.474700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.474845: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2020-09-30 17:26:45.474873: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-09-30 17:26:45.475055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.475192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-09-30 17:26:45.475208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-30 17:26:45.475214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-09-30 17:26:45.475220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-09-30 17:26:45.475225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-09-30 17:26:45.475231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-30 17:26:45.475237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-30 17:26:45.475243: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-30 17:26:45.475267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.475409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.475530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-09-30 17:26:45.475541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-30 17:26:45.475544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-09-30 17:26:45.475546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-09-30 17:26:45.475583: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.475725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:45.475852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9708 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-09-30 17:26:45.477438: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize\r\n2020-09-30 17:26:45.477447: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 9 nodes (-5), 8 edges (-5), time = 0.361ms.\r\n2020-09-30 17:26:45.477450: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 9 nodes (0), 8 edges (0), time = 0.108ms.\r\nTraceback (most recent call last):\r\n  File \"/home/m/PycharmProjects/test/model/tflite_test.py\", line 43, in <module>\r\n    tflite_model = converter.convert()\r\n  File \"/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 514, in convert\r\n    result = _toco_convert_impl(\r\n  File \"/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 491, in toco_convert_impl\r\n    data = toco_convert_protos(\r\n  File \"/home/mn/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\", line 227, in toco_convert_protos\r\n    raise ConverterError(\"See console for info.\\n%s\\n%s\\n\" % (stdout, stderr))\r\ntensorflow.lite.python.convert.ConverterError: See console for info.\r\n2020-09-30 17:26:46.253082: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.\r\n2020-09-30 17:26:46.253096: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.\r\n2020-09-30 17:26:46.257083: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-09-30 17:26:46.279406: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3999980000 Hz\r\n2020-09-30 17:26:46.279736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c77ff6240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-09-30 17:26:46.279747: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-09-30 17:26:46.280153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\r\n2020-09-30 17:26:46.282831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:46.282979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5\r\ncoreClock: 1.65GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s\r\n2020-09-30 17:26:46.283066: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-30 17:26:46.283843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\r\n2020-09-30 17:26:46.284635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\r\n2020-09-30 17:26:46.284757: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\r\n2020-09-30 17:26:46.285563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-30 17:26:46.286029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-30 17:26:46.287805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\r\n2020-09-30 17:26:46.287859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:46.288029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:46.288155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\r\n2020-09-30 17:26:46.288174: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-30 17:26:46.323961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-09-30 17:26:46.323977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \r\n2020-09-30 17:26:46.323981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \r\n2020-09-30 17:26:46.324065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:46.324234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:46.324384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-09-30 17:26:46.324525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 105 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\n2020-09-30 17:26:46.325513: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560c78d5dc30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-09-30 17:26:46.325522: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5\r\nFatal Python error: Segmentation fault\r\n\r\nCurrent thread 0x00007f70cdd84740 (most recent call first):\r\n  File \"/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 50 in execute\r\n  File \"/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/absl/app.py\", line 251 in _run_main\r\n  File \"/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/absl/app.py\", line 300 in run\r\n  File \"/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/platform/app.py\", line 40 in run\r\n  File \"/home/m/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/lite/toco/python/toco_from_protos.py\", line 93 in main\r\n  File \"/home/m/miniconda3/envs/tf/bin/toco_from_protos\", line 11 in <module>\r\nSegmentation fault (core dumped)\r\n\r\n\r\n\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Put link here or attach to the issue.\r\n```\r\n\r\n**Failure details**\r\nExpect the tflite converter to work on channel-first models \r\n\r\n\r\n**RNN conversion support**\r\nIf converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.\r\n\r\n**Any other info / logs**\r\nSame two layer test model successfully converts when set up as _channel last_ \r\nRemoving the batchnorm layer succesfully converts with _channel first_.\r\nHowever, even just a lone conv2d layer fails with _channel first_ if I do not include the extra 'converter.' parameters\r\n\r\n", "comments": ["@PoseAI When I used `tf-nightly`, I was not able to reproduce the issue. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/59797706487551888b539cc0519816a6/untitled51.ipynb).\r\n\r\nPlease verify once and close the issue if this was resolved for you. Thanks!", "hi, i confirm current tf-nightly resolves the issue.  hopefully conda upgrades soon as i try to avoid nightly builds.\r\n\r\nthis may be useful for anybody else who had an error converting to TF from ONNX  (or from Pytorch via ONNX) as the \"required\" TF version is 2.2 and I suspect this is what made my conversion fail.  \r\nI am closing thanks\r\n\r\n"]}, {"number": 43675, "title": "Adding README.md to TFLite evaluation/tasks", "body": "PR related to #43294. \r\n\r\nAdding documentation under `evaluation/tasks` to guide users who are directed from the [TFLite documentation guide](https://www.tensorflow.org/lite/performance/post_training_quantization#model_accuracy).", "comments": ["@bhack any suggestions?", "@gbaned requesting reviewers", "Thanks for the review. I added snippets of information from [Tools for Evaluation](https://www.tensorflow.org/lite/performance/delegates#tools_for_evaluation). For the most part, I tried to keep it short and redirected people to the TFLite guide site to keep it consistent."]}, {"number": 43674, "title": "Wrong maximum value passed in the positional encoding for the Transformer model", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/tutorials/text/transformer\r\n\r\n## Description of issue (what needs changing):\r\nWhen creating positional encoding vectors set we specify maximum_position_encoding which should be equal to the maximum possible length of the sequence fed into the model. So it must be equal to MAX_LENGTH (which is 40). In your sample it's equal to the vocabulary length (or 10000).\r\n\r\n### Clear description\r\nWhen max length is so big, the difference between neighboring elements pos encoding is very subtle and it's harder for the model to grasp it. And logically the value is incorrect.\r\n\r\n### Correct links\r\n\r\nNot applicable.\r\n\r\n### Parameters defined\r\n\r\nNot applicable.\r\n\r\n### Returns defined\r\n\r\nNot applicable.\r\n\r\n### Raises listed and defined\r\n\r\nNo error. Just typo in the sample code.\r\n\r\n### Usage example\r\n\r\nNo.\r\n\r\n### Request visuals, if applicable\r\n\r\nNo.\r\n\r\n### Submit a pull request?\r\n\r\nNo.", "comments": ["If the issue is open, Can I work on this?\ud83d\ude80", "@adithyaakrishna Feel free to open up a PR", "Okay, Thanks! @Harsh188 :)", "> MAX_LENGTH (which is 40).\r\n\r\nYeah, that's only in the `translate` class. During training I see sequences up to 400 tokens. \r\nThis aspect of the code could be clearer.\r\n\r\nI'm sending a fix.\r\n\r\n> the difference between neighboring elements pos encoding is very subtle and it's harder for the model to grasp it.\r\n\r\nI disagree on this point. Each `(sin,cos)` pair has a fixed rotation rate. Spinning them for longer doesn't make them more or less different from time-step to time-step.\r\n\r\n"]}, {"number": 43673, "title": "[XLA] Make generated LLVM code more readable.", "body": "Small PR that makes the XLA generated LLVM code more readable.\r\n\r\n@cheshire ", "comments": []}, {"number": 43672, "title": "Issue in installing the package using pip", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 64 bit \r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version:\r\n- Python version:3.8.2\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nTried to install tensorflow using pip but showed, **Could not find a version that satisfies the requirement tensorflow (from versions: )\r\nNo matching distribution found for tensorflow**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["@Yuvraj15101999 \r\nAs you are using python version 3.8, ensure the tf version is 2.2 or higher and python is 64 bits.\r\nPlease post the output of pip3 --version?\r\nPlease refer to #43048, #43244, #42977 and let us know.", " the output of pip3 --version\r\npip 20.2.3", "Can you run in python:\r\n```\r\nimport sys\r\nprint(sys.maxsize > 2**32)\r\n```", "Yes I'm currently running python 32 bit, I am trying to run 64 bit", " @Yuvraj15101999 \r\nPlease update if the issue is resolved, with the correct python.", "Done happy to close this issue with TensorFlow installed", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43672\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43672\">No</a>\n", "@Yuvraj15101999 \r\nGlad it worked for you, thanks for the update."]}, {"number": 43671, "title": "Single threaded TensorFlow", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nTensorFlow should generate the specified number of threads, irrespective of the available number of CPUs or CPU cores. \r\nconfig.set_inter_op_parallelism_threads(1);\r\nconfig.set_intra_op_parallelism_threads(1); is not enough to restrict the total number of generated threads.\r\nSome related issues.\r\nhttps://github.com/tensorflow/tensorflow/issues/33627\r\nhttps://github.com/tensorflow/tensorflow/issues/42510\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\nPeople who discuss it at the following issues:\r\nhttps://github.com/tensorflow/tensorflow/issues/33627\r\nhttps://github.com/tensorflow/tensorflow/issues/42510\r\n\r\n**Any Other info.**\r\n", "comments": ["@fisakhan \r\n`config.set_inter_op_parallelism_threads(1);\r\nconfig.set_intra_op_parallelism_threads(1);` can't work well because there would be many python libs to be executed in a AI script. It's possible other libs will enable multiple threads.\r\n\r\nAbove two parameters only limit the tensroflow train/infer.\r\n\r\nThere is simple method to limit one thread, like:\r\n`numactl -C 1 python train.py`\r\n\r\nIt sets the python running on the No.1 CPU core only.\r\nIf you use 'htop', you will see only No.1 CPU core is busy.\r\n\r\n", "@NeoZhangJianyu both of your suggestions are not working and have been properly discussed on https://github.com/tensorflow/tensorflow/issues/33627#issuecomment-675661637\r\nnumactl doesn't work because another C++ program (from another party) is calling my code and I have to control the number of thread from within my code.", "If I build TensorFlow from source then what code should I change to disable threading by TensorFlow?", "@fisakhan Have you tried to control also `OMP_THREAD_NUM`? ", "@bhack I use system(export OMP_NUM_THREADS=1)\" in my C++ code but it looks like TensorFlow overwrites this setting. Can you please suggest how and where to use OMP_NUM_THREADS to force TensorFlow not to generate threads?\r\n. ", "On master you can try to take a look at\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/threadpool_device.cc#L45:L69", "@fisakhan OK, I understand your case.\r\n\r\nCurrently, the method to control thread number by external is only for process, instead of library (your case).\r\nThe current env variables/parameters are only impact the AI operation part. Like OMP_NUM_THREADS and inter_op_parallelism_threads, intra_op_parallelism_threads.\r\n\r\nI think they are working for AI operation of Tensorflow. But they won't impact the other part of tensorflow to use single thread or multiple threads. For example, load dataset.\r\n\r\nYour case need whole tensorflow use single thread.\r\nIt's a little hard: even if the source code of tensorflow is changed to use single thread, but it's hard to make sure the 3rd party libs used by tensorflow to use single thread.\r\n\r\nI have two alternatives:\r\n1. Modify the source code of tensorflow to set the thread number variables to 1 by hard code.\r\nIt's  big work, and would be fault, because it can't control the 3rd party lib.\r\n\r\n2. Refactor your software structure:\r\nSperate the software into two processes. The function call is changed to IPC.\r\nYour code is running as child process and receive the parameters/return result by IPC.\r\nSo the thread number can be controlled by 'numactrl'.\r\n", "@fisakhan \r\nIs there any feedback?", "I'm still struggling with that.", "@fisakhan \r\n\r\nGood lucky!\r\n\r\nDo you still need any support from Tensorflow with MKL view?", "@fisakhan \r\n\r\nIf you don't know the support, could you close this issue?", "This is not possible with tensorflow. I wonder how did some people control the tensorflow threads [here](https://github.com/usnistgov/frvt/issues/12).", "@fisakhan Were you able to disable multi-threading in Tensorflow C API?\r\nI am also working on the same problem.", "@suraj-maniyar No I couldn't. I still don't know how to do that.", "@fisakhan @suraj-maniyar can you try building tensorflow 2.4 with --config=mkl_threadpool instead of --config=mkl to use Eigen threadpool. this way you don't have to use omp threadpool and control through just setting  config.set_inter_op_parallelism_threads(1)\r\nconfig.set_intra_op_parallelism_threads(1) and see if that solves the problem", "@preethivenkatesh try it and then be confident in providing answer. Try...if..., try...if... doesn't work.", "@preethivenkatesh I got build errors when using --config=mkl_threadpool.\r\nCould you please elaborate what all steps you followed to build Tensorflow 2.4?", "@suraj-maniyar \r\nbazel:3.1.0, \r\ngcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n\r\nHere is the build command I used for c api\r\n`bazel build --config=mkl_threadpool  //tensorflow/tools/lib_package:libtensorflow`\r\n\r\n For sanity check purpose, I tested the installer on Python for the  single-threaded behavior and was able to do it by setting the threads for all sessions in the script [here](https://github.com/IntelAI/models/blob/271256623b6dca4663cad409081af79c5d374b10/models/image_recognition/tensorflow/resnet50/inference/eval_image_classifier_inference.py). you can try to use this as a ref point to compare if your setting the sessions and configuring it to use 1 thread. \r\n\r\n   infer_config = tf.compat.v1.ConfigProto()\r\n   infer_config.intra_op_parallelism_threads = 1\r\n   infer_config.inter_op_parallelism_threads = 1\r\n   infer_config.use_per_session_threads = 1\r\n\r\n\r\n\r\n", "@fisakhan @suraj-maniyar\r\nAs my understanding of your issue, you want to a program which call TF to work only use one thread.\r\nThe program's work include AI inference and other functions (like pre/post handle dataset).\r\n\r\nThe parameters above will only impact the TF inference behavior: use 1 thread to infer.\r\nBut the parameters can't control the program do other jobs with 1 thread.\r\n\r\nPlease check and find the code/function to use multiple threads in your case.\r\nIf TF AI inference code use multiple threads, we could continue check.\r\nElse, it's another topic. Like load images by multiple workers, which are multiple threads. They need to be refactored to use single thread."]}, {"number": 43670, "title": "(venv) (base) razafimandimby@razafimandimby-Lenovo-G50-30:~$ python3 -c 'import tensorflow as tf; print(tf._version_)' Instruction non permise (core dumped)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["can you help me please ?\r\n", "Are you on 64bit or 32bit machine? Please fill you template as it is empty.", "I'm on 64bit machine. What kind of template should I fill in ?  I'am a beginner on this platform and on IT in general. Thanks for your understanding.", "This one https://github.com/tensorflow/tensorflow/issues/43670#issue-711899548. You haven't filled any info.", "System information\r\n\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04.1 TLS\r\n    Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Laptop Lenovo G50-30\r\n    TensorFlow installed from (source or binary): TensorFlow installed from source \r\n    TensorFlow version: 2.3.1\r\n    Python version: 3.8.2\r\n    Installed using virtualenv? pip?9 conda?: virtualenv \r\n    Bazel version (if compiling from source):\r\n    GCC/Compiler version (if compiling from source):\r\n    CUDA/cuDNN version:\r\n    GPU model and memory: Graphic Pilote Nvidia for Windows 8.1 (64 bits) - Lenovo G50-30\r\n\r\nI'm sorry, I don't know for the rest.", "Can you run in python:\r\n```\r\nimport sys\r\nprint(sys.maxsize > 2**32)\r\n```", "(venv) (base) razafimandimby@razafimandimby-Lenovo-G50-30:~$ import sys\r\n\r\nLa commande \u00ab\u00a0import\u00a0\u00bb n'a pas \u00e9t\u00e9 trouv\u00e9e, mais peut \u00eatre install\u00e9e avec\u00a0:\r\n\r\nsudo apt install graphicsmagick-imagemagick-compat  # version 1.4+really1.3.35-1, or\r\nsudo apt install imagemagick-6.q16                  # version 8:6.9.10.23+dfsg-2.1ubuntu11.1\r\nsudo apt install imagemagick-6.q16hdri              # version 8:6.9.10.23+dfsg-2.1ubuntu11.1\r\n\r\n", "this is the error message that i have \r\n", "It Is python code", "\r\n(venv) (base) razafimandimby@razafimandimby-Lenovo-G50-30:~$ sudo apt install imagemagick-6.q16hdri\r\n(venv) (base) razafimandimby@razafimandimby-Lenovo-G50-30:~$ import sys\r\nimport-im6.q16hdri: attempt to perform an operation not allowed by the security policy `PS' @ error/constitute.c/IsCoderAuthorized/408.\r\nThis is the last command that i do and the error message ", "can you take my laptop with TeamViewer please ?", "It is not a bug it is just a support request. \r\nI think that you can close this ticket and reformulate as a support question at https://stackoverflow.com/questions/tagged/tensorflow", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43670\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43670\">No</a>\n"]}, {"number": 43669, "title": "add cuda compute capability 8 to profiler hardware type utils", "body": "- Add case for CUDA compute capability 8.0 and 8.6 ", "comments": []}, {"number": 43668, "title": "add cuda compute capability 8 to profiler hardware type utils", "body": "- Add case for CUDA compute capability 8.0 and 8.6", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43668) for more info**.\n\n<!-- need_sender_cla -->", "> @googlebot I signed it!\r\n\r\n", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43668) for more info**.\n\n<!-- need_author_cla -->", "> We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors. If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)? If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\r\n> In order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\r\n> \r\n> \u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43668) for more info**.\r\n\r\n@googlebot I fixed it."]}, {"number": 43666, "title": "Compiling client code with -mavx2 leads to Eigen errors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS 10.14**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **n/a**\r\n- TensorFlow installed from (source or binary): **binary** (compiled in CI, transferred to client system)\r\n- TensorFlow version (use command below): **v2.3.0**\r\n- Python version: **n/a**\r\n- Bazel version (if compiling from source): **3.1.0**\r\n- GCC/Compiler version (if compiling from source): Apple Clang 11.0.0 for the client code (not sure which compiler did the tf build, but likely the same or 10.0.0)\r\n- CUDA/cuDNN version: **n/a**\r\n- GPU model and memory: **n/a**\r\n\r\n**Describe the current behavior**\r\n\r\nWe have compiled the C++ library from 2.3.0 and are now using it from our Code. A line such as \r\n```\r\n#include <tensorflow/cc/saved_model/loader.h>\r\n```\r\nLeads to a compilation failure when `-mavx2` is used:\r\n\r\n```\r\nIn file included from /build/<ycode>.hpp:20:\r\nIn file included from /build/libs/tensorflow/include/tensorflow/cc/saved_model/loader.h:27:\r\nIn file included from /build/libs/tensorflow/include/tensorflow/core/public/session.h:24:\r\nIn file included from /build/libs/tensorflow/include/tensorflow/core/framework/tensor.h:23:\r\nIn file included from /build/libs/tensorflow/include/tensorflow/core/framework/allocator.h:26:\r\nIn file included from /build/libs/tensorflow/include/tensorflow/core/framework/numeric_types.h:24:\r\nIn file included from /build/libs/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:\r\n/build/libs/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:9: error: no template named 'eigen_packet_wrapper'\r\ntypedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;\r\n```\r\n\r\nInspecting that file shows that the template is indeed not defined. Other Eigen headers for Neon or SSE (consider `/usr/local/include/eigen3/Eigen/src/Core/arch/SSE/PacketMath.h` in Eigen 3.3.7) do include this template.\r\n\r\n**Describe the expected behavior**\r\n\r\nClient C++ program compiles against TF 2.3.0 when `-mavx2` or-march=native` are enabled.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n# test.cpp\r\n#include <tensorflow/cc/saved_model/loader.h>\r\n\r\nint main(int argc, char *argv[])\r\n{\r\n    return 0;\r\n}\r\n```\r\nThis works:\r\n```\r\nclang -std=c++14 -L libs/tensorflow/lib/ -isystem /usr/local/include/eigen3/ -isystem libs/tensorflow/include/third_party/eigen3/ -isystem libs/tensorflow/include/ -ltensorflow_cc -lstdc++ test.cpp\r\n```\r\nThis does not work:\r\n```\r\nclang -mavx2 -std=c++14 -L libs/tensorflow/lib/ -isystem /usr/local/include/eigen3/ -isystem libs/tensorflow/include/third_party/eigen3/ -isystem libs/tensorflow/include/ -ltensorflow_cc -lstdc++ test.cpp\r\n```\r\n\r\nTensorflow directory:\r\n```\r\ntree -L 2 libs/tensorflow/\r\nlibs/tensorflow/\r\n\u251c\u2500\u2500 include\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 absl\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 external\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tensorflow\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 third_party\r\n\u2514\u2500\u2500 lib\r\n    \u251c\u2500\u2500 libtensorflow_cc.so\r\n    \u2514\u2500\u2500 libtensorflow_cc.so.2 -> libtensorflow_cc.so\r\n```\r\n\r\nThis problem for instance prevents me from using some libraries who's CMake configuration brings the `march=native` or `mavx2` flags into the build.", "comments": ["I have confirmed that upgrading the system eigen to current master (30960d485ec7e45b095d3ad206b2dbcc8bc835ba) resolves this problem.", "Tensorflow is currently on this hash https://github.com/tensorflow/tensorflow/blob/ed63e8a52c49702e06fc5c4f87dc240dd638f627/tensorflow/workspace.bzl#L234-L244\r\n\r\n/cc @ezhulenev", "Maybe there should be a mention of the required Eigen version in some piece of the documentation (which is sparse anyway for C++).", "Can you close this and open an new Doc type issue?", "TensorFlow tracks the head of the dev branch of Eigen quite closely, and\nhas not been tested with the stable 3.3.x branch for several years.\n\nOn Wed, Sep 30, 2020 at 10:17 AM bhack <notifications@github.com> wrote:\n\n> Can you close this and open an new Doc type issue?\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/43666#issuecomment-701524149>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEA72DVM22NSUBUEM6S7ZUTSINRZTANCNFSM4R6ZQEYQ>\n> .\n>\n", "Solution: Use the same/newer Eigen version as the one TF uses.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43666\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43666\">No</a>\n"]}, {"number": 43664, "title": "Unable to build tf-master", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version: master\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.5.0\r\n- GCC/Compiler version (if compiling from source): MSVC 2017\r\n- CUDA/cuDNN version: 11.1 / 7.6.0\r\n- GPU model and memory: RTX 2080 TI\r\n\r\n**Describe the problem**\r\n\r\nunable to build \r\n\r\n**Any other info / logs**\r\n\r\n```\r\n[1,694 / 2,066] Compiling tensorflow/core/framework/graph.pb.cc; 2s local ... (4\r\n actions, 3 running)\r\nERROR: E:/dp/tools/tensorflow/tensorflow-master/tensorflow/core/platform/windows\r\n/BUILD:151:11: C++ compilation of rule '//tensorflow/core/platform/windows:platf\r\norm_port' failed (Exit 2): python.exe failed: error executing command\r\n  cd E:/dp/_internal/_e/up/_bazel_administrator/n2u4av42/execroot/org_tensorflow\r\n\r\n  SET INCLUDE=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\T\r\nools\\MSVC\\14.16.27023\\ATLMFC\\include;C:\\Program Files (x86)\\Microsoft Visual Stu\r\ndio\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\include;C:\\Program Files (x86)\\Wind\r\nows Kits\\NETFXSDK\\4.6.1\\include\\um;C:\\Program Files (x86)\\Windows Kits\\10\\includ\r\ne\\10.0.17763.0\\ucrt;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\\r\nshared;C:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um;C:\\Program\r\n Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\winrt;C:\\Program Files (x86)\\W\r\nindows Kits\\10\\include\\10.0.17763.0\\cppwinrt\r\n    SET LIB=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Too\r\nls\\MSVC\\14.16.27023\\ATLMFC\\lib\\x64;C:\\Program Files (x86)\\Microsoft Visual Studi\r\no\\2017\\Community\\VC\\Tools\\MSVC\\14.16.27023\\lib\\x64;C:\\Program Files (x86)\\Window\r\ns Kits\\NETFXSDK\\4.6.1\\lib\\um\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0\r\n.17763.0\\ucrt\\x64;C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.17763.0\\um\\x64\r\n;\r\n    SET PATH=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\To\r\nols\\MSVC\\14.16.27023\\bin\\HostX64\\x64;C:\\Program Files (x86)\\Microsoft Visual Stu\r\ndio\\2017\\Community\\Common7\\IDE\\VC\\VCPackages;C:\\Program Files (x86)\\Microsoft SD\r\nKs\\TypeScript\\3.1;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\\r\nCommon7\\IDE\\CommonExtensions\\Microsoft\\TestWindow;C:\\Program Files (x86)\\Microso\r\nft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFound\r\nation\\Team Explorer;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Communit\r\ny\\MSBuild\\15.0\\bin\\Roslyn;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Co\r\nmmunity\\Team Tools\\Performance Tools\\x64;C:\\Program Files (x86)\\Microsoft Visual\r\n Studio\\2017\\Community\\Team Tools\\Performance Tools;C:\\Program Files (x86)\\Micro\r\nsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\\\x64;C:\\Program Files (x8\r\n6)\\Microsoft Visual Studio\\Shared\\Common\\VSPerfCollectionTools\\;C:\\Program Files\r\n (x86)\\Microsoft SDKs\\Windows\\v10.0A\\bin\\NETFX 4.6.1 Tools\\x64\\;C:\\Program Files\r\n (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Micro\r\nsoft\\FSharp\\;C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.17763.0\\x64;C:\\Prog\r\nram Files (x86)\\Windows Kits\\10\\bin\\x64;C:\\Program Files (x86)\\Microsoft Visual\r\nStudio\\2017\\Community\\\\MSBuild\\15.0\\bin;C:\\Windows\\Microsoft.NET\\Framework64\\v4.\r\n0.30319;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\ID\r\nE\\;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\Tools\\;\r\n;C:\\Windows\\system32;C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Communi\r\nty\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\CMake\\bin;C:\\Program Files (x86)\r\n\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\C\r\nMake\\Ninja\r\n    SET PWD=/proc/self/cwd\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TEMP=E:\\DP\\_internal\\_e\\_t\r\n    SET TMP=E:\\DP\\_internal\\_e\\_t\r\n  E:/DP/python/3.6.8/python.exe -B external/local_config_cuda/crosstool/windows/\r\nmsvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x060\r\n0 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DE\r\nPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd49\r\n96 /I. /Ibazel-out/x64_windows-opt-exec-50AE0418/bin /Iexternal/eigen_archive /I\r\nbazel-out/x64_windows-opt-exec-50AE0418/bin/external/eigen_archive /Iexternal/co\r\nm_google_absl /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/com_google_\r\nabsl /Iexternal/snappy /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/external/sn\r\nappy /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt-exec-50AE0418/bin/exte\r\nrnal/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE\r\n_TRAITS=0 /D__CLANG_SUPPORT_DYN_ANNOTATION__ /DTF_USE_SNAPPY /showIncludes /MD /\r\nO2 /DNDEBUG /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:prep\r\nrocessor /std:c++14 -DGOOGLE_CUDA=1 -DTENSORFLOW_USE_NVCC=1 -DTENSORFLOW_MONOLIT\r\nHIC_BUILD /DPLATFORM_WINDOWS /DEIGEN_HAS_C99_MATH /DTENSORFLOW_USE_EIGEN_THREADP\r\nOOL /DEIGEN_AVOID_STL_ARRAY /Iexternal/gemmlowp /wd4018 /wd4577 /DNOGDI /DTF_COM\r\nPILE_LIBRARY /Fobazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/core/plat\r\nform/windows/_objs/platform_port/port.obj /c tensorflow/core/platform/windows/po\r\nrt.cc\r\nExecution platform: @local_execution_config_platform//:platform\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\stdio.h(378): w\r\narning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\stdio.h(2437):\r\nwarning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\corecrt_search.\r\nh(188): warning C5105: macro expansion producing 'defined' has undefined behavio\r\nr\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\stdlib.h(79): w\r\narning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\stdlib.h(1286):\r\n warning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\corecrt_memory.\r\nh(76): warning C5105: macro expansion producing 'defined' has undefined behavior\r\n\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\corecrt_wstring\r\n.h(573): warning C5105: macro expansion producing 'defined' has undefined behavi\r\nor\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\string.h(531):\r\nwarning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\corecrt_math.h(\r\n44): warning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\corecrt_math.h(\r\n963): warning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\malloc.h(173):\r\nwarning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\float.h(328): w\r\narning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\sys/stat.h(87):\r\n warning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\sys/stat.h(120)\r\n: warning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\sys/stat.h(217)\r\n: warning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\ucrt\\ctype.h(241): w\r\narning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\winbase.h(9254):\r\nwarning C5105: macro expansion producing 'defined' has undefined behavior\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\oaidl.h(487): war\r\nning C5103: pasting '/' and '/' does not result in a valid preprocessing token\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared\\wtypes.h(745)\r\n: note: in expansion of macro '_VARIANT_BOOL'\r\n\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\oaidl.h(487): err\r\nor C2059: syntax error: '/'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\oaidl.h(487): err\r\nor C2238: unexpected token(s) preceding ';'\r\n\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\oaidl.h(502): war\r\nning C5103: pasting '/' and '/' does not result in a valid preprocessing token\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared\\wtypes.h(745)\r\n: note: in expansion of macro '_VARIANT_BOOL'\r\n\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\oaidl.h(502): err\r\nor C2059: syntax error: '/'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\oaidl.h(502): err\r\nor C2238: unexpected token(s) preceding ';'\r\n\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\propidlbase.h(319\r\n): warning C5103: pasting '/' and '/' does not result in a valid preprocessing t\r\noken\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\shared\\wtypes.h(745)\r\n: note: in expansion of macro '_VARIANT_BOOL'\r\n\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\propidlbase.h(319\r\n): error C2059: syntax error: '/'\r\nC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.17763.0\\um\\propidlbase.h(319\r\n): error C2238: unexpected token(s) preceding ';'\r\n\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 481.272s, Critical Path: 17.16s\r\nINFO: 458 processes: 458 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\nThe system cannot find the path specified.\r\n```\r\n\r\nseems like _VARIANT_BOOL is replaced thus generates error in windows kits library.", "comments": ["@iperov \r\nPlease share the steps followed before you encounter this issue, please follow [this guide ](https://www.tensorflow.org/install/source_windows) and let us know.\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43664\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43664\">No</a>\n", "Solution: update your Windows SDK to 10.0.19041.0, because there is a bug in 10.0.17763.0 version headers", "I can't understand how @iperov could have closed this issue months before someone gave the solution.\r\nThis shows that tf dev team does not care at all about tf C++ usage, and even less about tf used on Windows.\r\nBecause yeah, I want to use tf in C++ on Windows, how crazy am I, right ??\r\n\r\nThank you @gloriouskilka , your answer is perfectly correct and helped me resolve the issue I also had.\r\nNow I need to face the 60 000 DLL symbols limitation on Windows.\r\n\r\nI take the opportunity to say that the tf compiling process is a shame for a framework this big and serious. If everybody made compiling that hard for their projects, the world would definitely be a worse place.", "I am migrating to pytorch.\r\nTF is dead and it was a wrong choice.", "@YoannNicod You're welcome!\r\nPartially agree, I'd like to rephrase, if you please: if everyone would close issues without providing the solution... :)\r\n\r\nSpeaking of this particular problem, there should be pretty neat Windows CI build check infrastructure to detect such weird compiling error (What? Windows SDK version related bug? Seriously?), I doubt that could be predicted or/and automatically checked.", "@gloriouskilka \r\nI am not talking _only_ about this issue, but about the entire tensorflow compiling process which is really a pain in the ***.\r\nThough I agree with you, Win SDK related bug is unexpected, it shows that tf is almost never compiled on windows for C++.\r\n\r\nBy the way, I gave up compiling tensorflow, the generated dll doesn't expose most of the usefull symbols (because of the 64k symbols dll windows limitation, apparently). I am NOT going to fix tensorflow code myself as suggested in a 3 years old tf issue (https://github.com/tensorflow/tensorflow/issues/23542) (I tried it anyway and it doesn't even work).\r\n\r\nI have defended Tensorflow long enough, I think it is time that I too migrate to PyTorch.", "@YoannNicod \r\nthere are a lot of problems with basic usage of TF, not only with compilation. \r\nI wrote several issues , but nothing were fixed.\r\nEspecially with RTX 3x. \r\nI had huge headache when RTX 3k was released and DeepFaceLab had to be upgraded to work with it.\r\nMy new project will work on amazing pytorch, that just works.\r\n\r\nI think the biggest mistake of google, was to start upgrade tf to pytorch-like eager-execution, which is produced tons of bugs and anal pain of developers. Graph system was fine and better optimized.\r\n\r\nTF - R.I.P. \r\npress F", "@iperov\r\nyeah that is sad really. It is surprising from Google which usually makes really good stuff. I was very hyped by tf 2, but went from disappointment in disappointment.\r\nF"]}]