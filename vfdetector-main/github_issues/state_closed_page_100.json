[{"number": 52116, "title": "Update release notes for TensorFlow 2.8.0", "body": null, "comments": []}, {"number": 52115, "title": "Inconsistent broadcast behavior when using model.fit vs custom training loop", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linunx ? Repro on colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6\r\n- Python version: 3.7 ? \r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/a\r\n\r\n**Describe the current behavior**\r\n\r\nbroadcast behavior different between `keras.Model.fit` v/s custom training loop\r\n\r\n**Describe the expected behavior**\r\n\r\nbroadcast behavior is consistent.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\nN/A\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://gist.github.com/pavanky/95d6ae0a1e875d2cd9e60e24e9d3e1b6\r\n", "comments": ["Note: The behavior of custom training loop is consistent with eager mode execution (without keras), and numpy. However this inconsistent behavior may cause unnecessary debugging overhead.", "Hi @sanatmpa1 ,Could you please look at this issue ? It is replicating in TF [2.5](https://colab.research.google.com/gist/mohantym/896dcd25d8288ced6e216debd88518f4/github_52115.ipynb#scrollTo=GeJIUD2Dg_c6),[2.6 ](https://colab.research.google.com/gist/mohantym/63b366893cf0417a93734fdc3b4156c5/github_52115.ipynb#scrollTo=1ZcpdRU8hH3C) and showing different error in Model1.fit() operation in  [2.7.0dev].(https://colab.research.google.com/gist/mohantym/278bda43f794d9ea3f881196361d82b9/github_52115.ipynb#scrollTo=fOMZqEGxg8Td)", "@pavanky Thanks for opening this issue. Development of keras moved to separate repository https://github.com/keras-team/keras/issues\r\n\r\nPlease post this issue on keras-team/keras repo.\r\nTo know more see;\r\nhttps://discuss.tensorflow.org/t/keras-project-moved-to-new-repository-in-https-github-com-keras-team-keras/1999\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52115\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52115\">No</a>\n"]}, {"number": 52114, "title": " Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, float32, float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution : Ubuntu 20.4\r\n- TensorFlow installation : Tensorflow 2.6\r\n\r\n\r\n### 2. Code\r\n\r\n   # Arrange output as [N, (y1, x1, y2, x2, class_id, score)]\r\n    # Coordinates are normalized.\r\n    detections = tf.concat([\r\n        tf.gather(refined_rois, keep),\r\n       # tf.to_float(tf.gather(class_ids, keep))[..., tf.newaxis],\r\n        tf.cast(tf.gather(class_ids, keep), dtype=tf.float32)[..., tf.newaxis],\r\n        tf.gather(class_scores, keep),[..., tf.newaxis]\r\n        ], axis=1)\r\n\r\n### 5. (optional) Any other info / logs\r\n\r\nWARNING:tensorflow:From /home/nika/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse fn_output_signature instead\r\n\r\n Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, float32, float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.", "comments": ["\r\nI had this error while completing the demo.ipynb file of Mask_RCNN, please I need help", "@kdja90 \r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "I had this error when i run the run the demo.ipynb\r\n\r\nWARNING:tensorflow:From /home/nika/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:617: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse fn_output_signature instead\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-138183a2a830> in <module>\r\n      1 # Create model object in inference mode.\r\n----> 2 model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\r\n      3 \r\n      4 # Load weights trained on MS-COCO\r\n      5 model.load_weights(COCO_MODEL_PATH, by_name=True)\r\n\r\n~/Documents/modules/Mask_RCNN-master/mrcnn/model.py in __init__(self, mode, config, model_dir)\r\n   1860         self.model_dir = model_dir\r\n   1861         self.set_log_dir()\r\n-> 1862         self.keras_model = self.build(mode=mode, config=config)\r\n   1863 \r\n   1864     def build(self, mode, config):\r\n\r\n~/Documents/modules/Mask_RCNN-master/mrcnn/model.py in build(self, mode, config)\r\n   2067             # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in\r\n   2068             # normalized coordinates\r\n-> 2069             detections = DetectionLayer(config, name=\"mrcnn_detection\")(\r\n   2070                 [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\r\n   2071 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    974     # >> model = tf.keras.Model(inputs, outputs)\r\n    975     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n--> 976       return self._functional_construction_call(inputs, args, kwargs,\r\n    977                                                 input_list)\r\n    978 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1112         layer=self, inputs=inputs, build_graph=True, training=training_value):\r\n   1113       # Check input assumptions set after layer building, e.g. input shape.\r\n-> 1114       outputs = self._keras_tensor_symbolic_call(\r\n   1115           inputs, input_masks, args, kwargs)\r\n   1116 \r\n\r\n~/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)\r\n    846       return tf.nest.map_structure(keras_tensor.KerasTensor, output_signature)\r\n    847     else:\r\n--> 848       return self._infer_output_signature(inputs, args, kwargs, input_masks)\r\n    849 \r\n    850   def _infer_output_signature(self, inputs, args, kwargs, input_masks):\r\n\r\n~/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)\r\n    886           self._maybe_build(inputs)\r\n    887           inputs = self._maybe_cast_inputs(inputs)\r\n--> 888           outputs = call_fn(inputs, *args, **kwargs)\r\n    889 \r\n    890         self._handle_activity_regularization(inputs, outputs)\r\n\r\n~/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    693       except Exception as e:  # pylint:disable=broad-except\r\n    694         if hasattr(e, 'ag_error_metadata'):\r\n--> 695           raise e.ag_error_metadata.to_exception(e)\r\n    696         else:\r\n    697           raise\r\n\r\nTypeError: in user code:\r\n\r\n    /home/nika/Documents/modules/Mask_RCNN-master/mrcnn/model.py:830 call  *\r\n        detections_batch = utils.batch_slice(\r\n    /home/nika/Documents/modules/Mask_RCNN-master/mrcnn/utils.py:820 batch_slice  *\r\n        output_slice = graph_fn(*inputs_slice)\r\n    /home/nika/Documents/modules/Mask_RCNN-master/mrcnn/model.py:782 refine_detections_graph  *\r\n        detections = tf.concat([\r\n    /home/nika/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\r\n        return target(*args, **kwargs)\r\n    /home/nika/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:1769 concat\r\n        return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n    /home/nika/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py:1227 concat_v2\r\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n    /home/nika/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:466 _apply_op_helper\r\n        raise TypeError(\"%s that don't all match.\" % prefix)\r\n\r\n    TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [float32, float32, float32, <NOT CONVERTIBLE TO TENSOR>] that don't all match.\r\n", "please i'm waiting for help!", "@kdja90 Can you please share a simple standalone code to reproduce the issue? or send me the link to `demo.ipynb` file. Did you get the file from tnesorflow webpage? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52114\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52114\">No</a>\n"]}, {"number": 52113, "title": "libtensorflowlite with Select Ops linking failure", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): from source\r\n- TensorFlow version: 2.6.0\r\n- Python version: python 3.8.10\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): bazel 3.7.2\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: no cuda\r\n- GPU model and memory: no\r\n\r\n**Describe the problem**\r\n\r\nI'm trying to build from source Tensorflow Lite with Select Tensorflow Operation for Ubuntu 20.04 using Bazel build system and I get the compilation error.\r\n\r\n```\r\nERROR: /home/anastasiia/tensorflow/tensorflow/lite/BUILD:1033:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates::GetDelegates(int) const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::GetDelegates(int) const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here\r\n```\r\n\r\nFor Select TF operations I used tutorial: [ops_select](https://www.tensorflow.org/lite/guide/ops_select)\r\nWithout Select TF ops everything is ok. (without dependency `//tensorflow/lite/delegates/flex:delegate` )\r\nWithout buildin ops everything is ok. (without dependency `//tensorflow/lite/kernels:builtin_ops_all_linked`)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n1. git clone https://github.com/tensorflow/tensorflow.git\r\n2. Add the TensorFlow ops delegate library dependency to the build dependencies. In the file tensorflow/lite/BUILD for target libtensorflowlite add dependency : `tensorflow/lite/delegates/flex:delegate`.\r\n3. bazel build --config=monolithic -c opt //tensorflow/lite:libtensorflowlite.so\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```\r\nERROR: /home/anastasiia/tensorflow/tensorflow/lite/BUILD:1033:24: Linking of rule '//tensorflow/lite:libtensorflowlite.so' failed (Exit 1): gcc failed: error executing command /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/libtensorflowlite.so-2.params\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates::GetDelegates(int) const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver()'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here\r\n/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops/register.pic.o: multiple definition of 'tflite::ops::builtin::BuiltinOpResolver::GetDelegates(int) const'\r\n/usr/bin/ld.gold: bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_ops_all_linked/register.pic.o: previous definition here\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow/lite:libtensorflowlite.so failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 9.240s, Critical Path: 6.93s\r\nINFO: 6 processes: 3 internal, 3 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n```", "comments": ["Hi again,\r\nI solved this issue myself.\r\nYou need to change dependency in target 'libtensorflowlite':\r\n`//tensorflow/lite/kernels:builtin_ops_all_linked`  ->` //tensorflow/lite/kernels:builtin_ops`\r\n\r\nIt would be nice to configure it automatically or write about it in the documentation. If you have a more correct way of doing it, please let me know.\r\n\r\nBest regards,\r\nAnastasiia", "@AnastGerus \r\nSure we will work on that, can you please share the link to this documentation where the change is requested to be made.", "Hi @Saduf2019 ,\r\n\r\nI suppose it can be done here: https://www.tensorflow.org/lite/guide/ops_select#c\r\nBut I'm not sure if it's a good way to change only documentation. I hope someone can check a proposed solution and automate it in Bazel target's code.\r\n\r\nBest regards,\r\nAnastasiia", "@AnastGerus , Good to know that your issue s resolved, however regarding details in document it is mentioned on a high level [here](https://www.tensorflow.org/lite/guide/ops_select#known_limitations).\r\nDo you need more specific details to be added, since it may be limited to few hardware configurations and may not address the whole community.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52113\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52113\">No</a>\n"]}, {"number": 52112, "title": "tensorflow 2.6 does not detect ptxas.exe", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.6\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): \r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: GTX 2060 6GB\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI'm currently having problems with Tensorflow not detecting the ptxas.exe inside the cuda folders as seen here:\r\n\r\n`2021-09-23 23:36:12.332220: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\r\n2021-09-23 23:36:12.941230: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2\r\n2021-09-23 23:36:12.944774: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2\r\n2021-09-23 23:36:12.947274: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas.exe --version\r\n2021-09-23 23:36:12.951640: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2\r\n2021-09-23 23:36:12.953523: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\r\nRelying on driver to perform ptx compilation.\r\nModify $PATH to customize ptxas location.`\r\n\r\nBut I can confirm that the ptxas.exe does exist.\r\n\r\nI'm currently new to this kind of stuff, so if possible elaborate more so I could understand better, thank you.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n``import os\r\nfrom pickle import TRUE\r\nos.add_dll_directory(\"C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.4\\\\bin\")\r\nimport keras\r\nfrom keras.datasets import mnist\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Flatten\r\nfrom keras.layers import Conv2D, MaxPooling2D\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 12\r\n\r\nimg_rows, img_cols = 28, 28\r\n\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n\r\nx_train = x_train.reshape(60000,28,28,1)\r\nx_test = x_test.reshape(10000,28,28,1)\r\n\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\ny_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\ny_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = Sequential()\r\nmodel.add(Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=(28,28,1)))\r\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(Dropout(0.25))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(128, activation='relu'))\r\nmodel.add(Dropout(0.5))\r\nmodel.add(Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=tf.keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\n\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])``\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nHere is the complete output:\r\n`x_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\n2021-09-23 23:36:10.447932: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-23 23:36:10.895610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3967 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.52021-09-23 23:36:11.489872: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\nEpoch 1/12\r\n2021-09-23 23:36:12.332220: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\r\n2021-09-23 23:36:12.941230: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2\r\n2021-09-23 23:36:12.944774: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2\r\n2021-09-23 23:36:12.947274: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas.exe --version\r\n2021-09-23 23:36:12.951640: E tensorflow/core/platform/windows/subprocess.cc:287] Call to CreateProcess failed. Error code: 2\r\n2021-09-23 23:36:12.953523: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\r\nRelying on driver to perform ptx compilation.\r\nModify $PATH to customize ptxas location.\r\nThis message will be only logged once.\r\n469/469 [==============================] - 7s 9ms/step - loss: 29.2319 - accuracy: 0.1548 - val_loss: 3.7147 - val_accuracy: 0.4812\r\nEpoch 2/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 12.0754 - accuracy: 0.2700 - val_loss: 1.5613 - val_accuracy: 0.6339\r\nEpoch 3/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 6.1234 - accuracy: 0.3334 - val_loss: 1.2056 - val_accuracy: 0.6057\r\nEpoch 4/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 3.6699 - accuracy: 0.3449 - val_loss: 1.4396 - val_accuracy: 0.5644\r\nEpoch 5/12\r\nEpoch 6/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 2.2604 - accuracy: 0.3577 - val_loss: 1.6565 - val_accuracy: 0.5153\r\nEpoch 7/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 2.0842 - accuracy: 0.3743 - val_loss: 1.6186 - val_accuracy: 0.5310\r\nEpoch 8/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 1.9903 - accuracy: 0.3897 - val_loss: 1.5678 - val_accuracy: 0.5573\r\nEpoch 9/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 1.9153 - accuracy: 0.4106 - val_loss: 1.4787 - val_accuracy: 0.5954\r\nEpoch 10/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 1.8446 - accuracy: 0.4291 - val_loss: 1.4024 - val_accuracy: 0.6281\r\nEpoch 11/12\r\n469/469 [==============================] - 4s 9ms/step - loss: 1.7916 - accuracy: 0.4465 - val_loss: 1.3342 - val_accuracy: 0.6566\r\nEpoch 12/12\r\n469/469 [==============================] - 4s 10ms/step - loss: 1.7438 - accuracy: 0.4593 - val_loss: 1.2915 - val_accuracy: 0.6798\r\nTest loss: 1.2914707660675049\r\nTest accuracy: 0.6797999739646912`", "comments": ["Hi @MJDragoon , The above code ran successfully after removing this line . \r\n`os.add_dll_directory(\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\bin\")`\r\n\r\n It seems you are trying to add path for CUDA in code itself. \r\nCan you try again after setting it as environment path from environment variables instead. \r\n\r\nAttaching [GIST ](https://colab.research.google.com/gist/mohantym/e5f00a636a87792d0f878975ed46bbb4/github_52122.ipynb) in 2.6 for reference", "> Hi @MJDragoon , The above code ran successfully after removing this line .\r\n> `os.add_dll_directory(\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\bin\")`\r\n> \r\n> It seems you are trying to add path for CUDA in code itself.\r\n> Can you try again after setting it as environment path from environment variables instead.\r\n> \r\n> Attaching [GIST ](https://colab.research.google.com/gist/mohantym/e5f00a636a87792d0f878975ed46bbb4/github_52122.ipynb) in 2.6 for reference\r\n\r\nHi @mohantym, thanks for the reply but unfortunately, if I removed the line you suggested I would get these errors\r\n\r\n`2021-09-24 08:57:06.355128: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-09-24 08:57:06.357962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\r\n2021-09-24 08:57:06.361187: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\r\n2021-09-24 08:57:06.363529: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\r\n2021-09-24 08:57:06.366463: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\r\n2021-09-24 08:57:06.368716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\r\n2021-09-24 08:57:06.371322: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\r\n2021-09-24 08:57:06.375071: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found`", "Hi @MJDragoon ,Can you look at these threads for answer. [link1](https://github.com/tensorflow/tensorflow/issues/46606#issuecomment-765516590), [setting environment variable for cuda](https://docs.nvidia.com/gameworks/content/developertools/desktop/environment_variables.htm). Thanks!", "Hi @mohantym, thank you for the reply.\r\n\r\nI have checked the links you posted and was sure I added the path in the environment variables when I installed it, nonetheless, I still went through and did the solutions in those links but unfortunately it did not fix the problem. I still needed the os.add_dll_directory in order to access the dll's it didn't find, and it did not detect the ptxas.exe within CUDA.", "Hi @MJDragoon  ,Could you please try again with CUDA 11.2 , cuDNN8.1 and Python 3.8 ?\r\nReference:\r\nhttps://www.tensorflow.org/install/gpu", "Hi @mohantym.\r\n\r\nThankfully that did the trick! but I still needed the os.add_dll_directory otherwise errors will appear, other than that no errors seem to occur! thank you for your time.", "Ok @MJDragoon,  If the above steps worked , Can you try again after adding  **os.add_dll_directory()** operation \r\nto see if error occurs ? ", "Hi @mohantym,\r\n\r\nAfter adding the os.add_dll_directory() into the code, no ptxas.exe error occurred, but I did get a cudart64_110.dll error. I am currently reinstalling everything that I have downloaded during this project in order to at least fix that issue.", "Hi @sanatmpa1! \r\nCould you please look at this issue! Issue is not replicating in Colab ,when commenting out this line .\r\n>  os.add_dll_directory(\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.4\\bin\")\r\n\r\nAttaching GIST for reference in [2.5](https://colab.research.google.com/gist/mohantym/ab6a4daf4454ec6ee6c42880fcd49767/52112.ipynb#scrollTo=KXnXiymmrI7U),[2.6](https://colab.research.google.com/gist/mohantym/166ce616e3aa6899c443da0f4939d193/52112.ipynb#scrollTo=KXnXiymmrI7U) and [2.7.0dev(nightly)](https://colab.research.google.com/gist/mohantym/fbf32de1df79ec8415f1788a2d90a996/52112.ipynb#scrollTo=KXnXiymmrI7U)\r\n", "@MJDragoon,\r\n\r\nFor the error related to cudart64_110.dll , you can refer to the similar [issue1](cudart64_110.dll ), [issue2](https://github.com/tensorflow/tensorflow/issues/43193). As you said you can try to do a fresh install and make sure you have the tested build configurations as per this [guide](https://www.tensorflow.org/install/source_windows#gpu) while installing. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52112\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52112\">No</a>\n"]}, {"number": 52111, "title": "Fix: indentation of block in the `__add__` docstring", "body": "currently renders as\r\n![image](https://user-images.githubusercontent.com/5219726/134537284-e7ac0543-8f7d-40e0-bf99-8d45bf7d8498.png)\r\non this URL: https://www.tensorflow.org/api_docs/python/tf/Tensor#__add__", "comments": ["I didn't introduce this lint failure, but fixed it in 04c740a80629f468f7d6df854b14df1db90b8c50", "Got introduced in 88d40fca9a0bd529b50ee33d98060f93a7dcf66d"]}, {"number": 52110, "title": "TF Lite kernel tests cross-compilation enablement using CMake", "body": "This PR implements the enablement of the **TF Lite kernel tests CMake cross-compilation** using the natively built _flatc_ compiler (for details please see the updated build_cmake.md file).\r\n\r\nAs it currently stands in the _master_ branch, the native CMake compilation produces malformed TF Lite kernel tests. Therefore, for testing purposes, this PR also encompasses changes implemented by the currently reviewed [PR](https://github.com/tensorflow/tensorflow/pull/51406) which fixes it.", "comments": []}, {"number": 52109, "title": "A bug for tf.keras.layers.TextVectorization when built from saved configs and weights", "body": "I have tried writing a python program to save tf.keras.layers.TextVectorization to disk and load it with the answer of https://stackoverflow.com/questions/65103526/how-to-save-textvectorization-to-disk-in-tensorflow.\r\nThe TextVectorization layer built from saved configs outputs a vector with wrong length when the arg output_sequence_length is not None and output_mode='int'.\r\nFor example, if I set output_sequence_length= 10, and output_mode='int', it is expected that given a text, TextVectorization should output a vector with length of 10, see vectorizer and new_v2 in the code below.\r\nHowever, if TextVectorization's arg output_mode='int' is set from saved configs, it doesn't output a vector with length of 10(actually it is 9, the real length of the sentence. It seems like output_sequence_length is not set successfully). See the object new_v1 in the code below.\r\nThe interesting thing is, I have compared from_disk['config']['output_mode'] and 'int', they equal to each other.\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\nimport pickle\r\n# In[]\r\nmax_len = 10  # Sequence length to pad the outputs to.\r\ntext_dataset = tf.data.Dataset.from_tensor_slices([\r\n                                                   \"I like natural language processing\",\r\n                                                   \"You like computer vision\",\r\n                                                   \"I like computer games and computer science\"])\r\n# Fit a TextVectorization layer\r\nVOCAB_SIZE = 10  # Maximum vocab size.\r\nvectorizer = tf.keras.layers.TextVectorization(\r\n        max_tokens=None,\r\n        standardize=\"lower_and_strip_punctuation\",\r\n        split=\"whitespace\",\r\n        output_mode='int',\r\n        output_sequence_length=max_len\r\n        )\r\nvectorizer.adapt(text_dataset.batch(64))\r\n# In[]\r\n#print(vectorizer.get_vocabulary())\r\n#print(vectorizer.get_config())\r\n#print(vectorizer.get_weights())\r\n# In[]\r\n\r\n\r\n# Pickle the config and weights\r\npickle.dump({'config': vectorizer.get_config(),\r\n             'weights': vectorizer.get_weights()}\r\n            , open(\"./models/tv_layer.pkl\", \"wb\"))\r\n\r\n\r\n# Later you can unpickle and use\r\n# `config` to create object and\r\n# `weights` to load the trained weights.\r\n\r\nfrom_disk = pickle.load(open(\"./models/tv_layer.pkl\", \"rb\"))\r\n\r\nnew_v1 = tf.keras.layers.TextVectorization(\r\n        max_tokens=None,\r\n        standardize=\"lower_and_strip_punctuation\",\r\n        split=\"whitespace\",\r\n        output_mode=from_disk['config']['output_mode'],\r\n        output_sequence_length=from_disk['config']['output_sequence_length'],\r\n        )\r\n# You have to call `adapt` with some dummy data (BUG in Keras)\r\nnew_v1.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\r\nnew_v1.set_weights(from_disk['weights'])\r\nnew_v2 = tf.keras.layers.TextVectorization(\r\n        max_tokens=None,\r\n        standardize=\"lower_and_strip_punctuation\",\r\n        split=\"whitespace\",\r\n        output_mode='int',\r\n        output_sequence_length=from_disk['config']['output_sequence_length'],\r\n        )\r\n\r\n# You have to call `adapt` with some dummy data (BUG in Keras)\r\nnew_v2.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\r\nnew_v2.set_weights(from_disk['weights'])\r\nprint (\"*\"*10)\r\n# In[]\r\ntest_sentence=\"Jack likes computer scinece, computer games, and foreign language\"\r\n\r\nprint(vectorizer(test_sentence))\r\nprint (new_v1(test_sentence))\r\nprint (new_v2(test_sentence))\r\nprint(from_disk['config']['output_mode']=='int')\r\n```\r\nHere are the print() outputs:\r\n```\r\n**********\r\ntf.Tensor([ 1  1  3  1  3 11 12  1 10  0], shape=(10,), dtype=int64)\r\ntf.Tensor([ 1  1  3  1  3 11 12  1 10], shape=(9,), dtype=int64)\r\ntf.Tensor([ 1  1  3  1  3 11 12  1 10  0], shape=(10,), dtype=int64)\r\nTrue\r\n```\r\nDoes anyone know why?\r\nI have also raised a same issue as this in the repo of Keras https://github.com/keras-team/keras/issues/15382 \r\n", "comments": ["@lankuohsing We see that the issue is posted in Keras repo and PR [**15422**](https://github.com/keras-team/keras/pull/15422) is merged for this issue .Please let us know if we can close this ticket here ?Thanks!", "> @lankuohsing We see that the issue is posted in Keras repo and PR [**15422**](https://github.com/keras-team/keras/pull/15422) is merged for this issue .Please let us know if we can close this ticket here ?Thanks!\r\n\r\n@sushreebarsa  The solution in that PR is useful! Thanks! You can close the issue.", "@lankuohsing \r\nThank you for your update,  moving this issue to closed status as it is resolved.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52109\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52109\">No</a>\n"]}, {"number": 52107, "title": "ValueError: Layer conv2d_18 expects 1 inputs, but it received 2 input tensors. ", "body": "```\r\n\r\ndef unet_model(input_size=(96, 128, 3), n_filters=32, n_classes=23):\r\n    \"\"\"\r\n    Unet model\r\n    \r\n    Arguments:\r\n        input_size -- Input shape \r\n        n_filters -- Number of filters for the convolutional layers\r\n        n_classes -- Number of output classes\r\n    Returns: \r\n        model -- tf.keras.Model\r\n    \"\"\"\r\n    \r\n    inputs = Input(input_size)\r\n    \r\n    # Contracting Path (encoding)\r\n    # Add a conv_block with the inputs of the unet_ model and n_filters\r\n    ### START CODE HERE\r\n    \r\n    cblock1 = conv_block(inputs,n_filters)\r\n    \r\n    # Chain the first element of the output of each block to be the input of the next conv_block. \r\n    # Double the number of filters at each new step\r\n    cblock2 = conv_block(inputs,max_pooling=True) #inputs=None, n_filters=32, dropout_prob=0, max_pooling=True\r\n    cblock3 = conv_block(cblock2,max_pooling=True)\r\n    cblock4 = conv_block(cblock3,dropout=0.3,max_pooling=True) # Include a dropout_prob of 0.3 for this layer\r\n    # Include a dropout_prob of 0.3 for this layer, and avoid the max_pooling layer\r\n    cblock5 = conv_block(cblock4,128, dropout=0.3, max_pooling=False) \r\n    ### END CODE HERE\r\n    \r\n    # Expanding Path (decoding)\r\n    # Add the first upsampling_block.\r\n    # Use the cblock5[0] as expansive_input and cblock4[1] as contractive_input and n_filters * 8\r\n    ### START CODE HERE\r\n    ublock6 = upsampling_block(expansive_input ,contractive_input, n_filters * 8) #upsampling_block(expansive_input, contractive_input, n_filters=32)\r\n    \r\n    # Chain the output of the previous block as expansive_input and the corresponding contractive block output.\r\n    # Note that you must use the second element of the contractive block i.e before the maxpooling layer. \r\n    # At each step, use half the number of filters of the previous block \r\n    \r\n    ublock7 = upsampling_block(ublock6,contractive_input, n_filter*4)\r\n    ublock8 = upsampling_block(ublock5,contractive_input, n_filter*2)\r\n    ublock9 = upsampling_block(ublock9,contractive_input,  n_filter*1)\r\n    \r\n    ### END CODE HERE\r\n\r\n    conv9 = Conv2D(n_filters,\r\n                 3,\r\n                 activation='relu',\r\n                 padding='same',\r\n                 kernel_initializer='he_normal')(ublock9)\r\n\r\n    # Add a Conv2D layer with n_classes filter, kernel size of 1 and a 'same' padding\r\n    ### START CODE HERE\r\n    conv10 = Conv2D(n_classes,1, padding='same')(conv9)\r\n    ### END CODE HERE\r\n    \r\n    model = tf.keras.Model(inputs=inputs, outputs=conv10)\r\n\r\n    return model\r\n\r\n\r\n\r\n```\r\n\r\n![2](https://user-images.githubusercontent.com/26819449/134488571-b8c36108-bd89-437d-a63c-ab4d71a03c93.JPG)\r\n\r\n![1](https://user-images.githubusercontent.com/26819449/134488612-c3db09a3-94c7-4759-a7e0-44f844e500a9.JPG)\r\n\r\nCan you tell me why I am getting this error I am trying to Implement U-net respectively?\r\nThank You.\r\n\r\n\r\n", "comments": ["Hi @starboyvarun , Can you please share the complete stand alone code as Colab gist to reproduce this issue  ?It will help expedite the issue.But  Feel free to visit [this thread](https://stackoverflow.com/questions/61586981/valueerror-layer-sequential-20-expects-1-inputs-but-it-received-2-input-tensor) for answer.", "I can't create a gist. Is it possible you can help me like this? @mohantym \r\nThank You.", "Hi @starboyvarun ! I checked the given code  Earlier , Functions like conv_block, upsample_block are missing .  It would be a great aid if we can get a complete stand alone code with documents you are referring .Here is an article on how  to use and share your[ colab notebook](https://medium.datadriveninvestor.com/google-colaboratory-notebooks-and-sharing-it-on-github-a01b51ec6da8) for reference . Thanks!", "Thanks!\r\n@mohantym "]}, {"number": 52106, "title": "Pricing details for Tensorflow ", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\nHi Team,\r\nI am interested in knowing the pricing details and the storage availability as I want to use this Tensorflow software in my project. I also would want to know any new and important feature the latest version has come up with. Appreciating the response at the latest.\r\nThanking you.\r\n\r\nRegards,\r\nAditi G", "comments": ["@AditiG27     **`TensorFlow`** is an **Open-source platform** for creating ML-enabled applications. To know more on Tensorflow please visit to  our [official site of Tensorflow](https://www.tensorflow.org/overview/) and refer to the documentation .Please let us know if it helps ? Thank you!", "Does this has any subscription or package ? I couldnt find these pricing and storage details. Please provide me these details.", "@AditiG27 Tensorflow is a `free` and `open source` Software library for Machine learning and Artificial Intelligence. This framework is free to use. Please have a look at the [link ](https://www.tensorflow.org/install) to install Tensorflow and let us know if it helps ?Thanks!", "Any professional support services charges included or required ?", "@AditiG27 There is no support  service charges included for using Tensorflow .It is free and open source platform .Thanks!", "@AditiG27 Could you please let us know if we can close this ticket as it has been resolved ?Thank you!", "https://cloud.google.com/blog/products/ai-machine-learning/introducing-tensorflow-enterprise-supported-scalable-and-seamless-tensorflow-in-the-cloud", "Can you provide the infrastructure requirement for Tensorfllow setup along with its security features ?", "TF Enterprise is provided via GCP. Pricing details should be obtained via Google Cloud sales.\r\n\r\nTF itself is open source, free to use.\r\n\r\nGitHub is not a good place to discuss this. This space is for issues related to the code, bugs, feature requests, etc."]}, {"number": 52104, "title": "Use \"pluggable_device_host_bfc\" instead of \"cpu\" allocator when `_Send` to (`_Recv` on) host memory", "body": "1. Use \"pluggable_device_host_bfc\" instead of \"cpu\" allocator when _Send to (_Recv on) host memory\r\n2. format logs in pluggable device", "comments": []}, {"number": 52102, "title": "[INTEL oneDNN] Upgrade pip version ", "body": "Upgrade pip version in mkl docker file", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52102) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent", "@angerson, @penpornk Can you please review this PR ? Thanks!", "@angerson, @penpornk Can you please review this PR ? Thanks!", "@angerson, @penpornk Can you please review this PR ? Thanks!", "no longer need.  Close.", "no longer needed. "]}, {"number": 52101, "title": "Error compilation NVCC for   TF 2.5 from source using Clang-12 ", "body": "Hi all!\r\n**System information**\r\n- OS  -  Linux Debian 10 debian 4.19.0-17-amd64\r\n- TensorFlow: try install from source\r\n- TensorFlow version:  TF 2.5.1 nightly\r\n- Python version: 3.8.12 build from source Clang-12\r\n- Try compiled in the separate venv 3.8.12 \r\n- Bazel version: 3.7.2 \r\n- GCC/Compiler version:  gcc/g++ -  Debian 8.3.0-6\r\n- Clang compiler: complete  bundle of  LLVM-12 from https://apt.llvm.org/  \r\n- CUDA/cuDNN version:  Cuda_11.2.2_460.32.03 installed from `run` file / CUDNN 8.1.1.3\r\n- GPU model and memory:  GT1050 2GB RAM single\r\n\r\n**Describe the problem**\r\nI tried several times to compile from source TF 2.5 from sources using clang as CUDA compiler.\r\nBefore I successful   compiled TF 2.5.1 from source using  system GCC 8 with warning about unsporting  MLIR.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nFirst step: run configure.py with separate venv\r\n\r\n```\r\n(tf25) mvg@debian:~/tensorflow$ ./configure\r\nYou have bazel 3.7.2 installed.\r\nPlease specify the location of python. [Default is /home/mvg/tf25/bin/python3]: \r\nFound possible Python library paths:\r\n  /home/mvg/tf25/lib/python3.8/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/mvg/tf25/lib/python3.8/site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: n\r\nNo TensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 11.2 in:\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/lib\r\n    /usr/local/cuda-11.2/targets/x86_64-linux/include\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 6.1]: \r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: y\r\nClang will be used as CUDA compiler.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify which clang should be used as device and host compiler. [Default is /usr/lib/llvm-12/bin/clang]: \r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: \r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=mkl_aarch64 \t# Build with oneDNN and Compute Library for the Arm Architecture (ACL).\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\n\t--config=numa        \t# Build with NUMA support.\r\n\t--config=dynamic_kernels\t# (Experimental) Build kernels into separate shared objects.\r\n\t--config=v2          \t# Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n\t--config=noaws       \t# Disable AWS S3 filesystem support.\r\n\t--config=nogcp       \t# Disable GCP support.\r\n\t--config=nohdfs      \t# Disable HDFS support.\r\n\t--config=nonccl      \t# Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n\r\nStep 2: compilation log\r\n```\r\n(tf251) mvg@debian:~/tensorflow$ bazel build -c opt --copt=-msse3  --copt=-msse4.1 --copt=-msse4.2  --copt=-mavx --copt=-mfma --copt=-mfma4 --config=cuda --jobs=5 //tensorflow/tools/pip_package:build_pip_package\r\nWARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=80\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/mvg/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/mvg/tf251/bin/python3 --action_env PYTHON_LIB_PATH=/home/mvg/tf251/lib/python3.8/site-packages --python_path=/home/mvg/tf251/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.1 --action_env LD_LIBRARY_PATH=/home/mvg/python3.8.12/lib/ --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-12/bin/clang --config=cuda_clang\r\nINFO: Found applicable config definition build:short_logs in file /home/mvg/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/mvg/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda_clang in file /home/mvg/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:cuda in file /home/mvg/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:linux in file /home/mvg/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/mvg/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /home/mvg/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /home/mvg/tensorflow/tensorflow/workspace0.bzl:105:34: in workspace\r\n  /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (246 packages loaded, 33167 targets configured).\r\nINFO: Found 1 target...\r\nERROR: /home/mvg/.cache/bazel/_bazel_mvg/196ae8161680bc9ae310a4b3aee3f0d6/external/nccl_archive/BUILD.bazel:77:13: C++ compilation of rule '@nccl_archive//:nccl' failed (Exit 1): clang failed: error executing command /usr/lib/llvm-12/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/nccl/bootstrap.pic.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/nccl/bootstrap.pic.o' -iquote ... (remaining 53 argument(s) skipped)\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:431:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:23:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bcopy (const void *__src, void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:38:13: note: previous declaration is here\r\nextern void bcopy (const void *__src, void *__dest, size_t __n)\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:431:\r\nIn file included from /usr/include/strings.h:144:\r\n/usr/include/x86_64-linux-gnu/bits/strings_fortified.h:29:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/strings.h:42:13: note: previous declaration is here\r\nextern void bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:42:14: note: previous declaration is here\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:38:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memmove (void *__dest, const void *__src, size_t __len))\r\n       ^\r\n/usr/include/string.h:46:14: note: previous declaration is here\r\nextern void *memmove (void *__dest, const void *__src, size_t __n)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:45:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (mempcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\n/usr/include/string.h:377:14: note: previous declaration is here\r\nextern void *mempcpy (void *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\n/usr/include/string.h:60:14: note: previous declaration is here\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:81:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (explicit_bzero (void *__dest, size_t __len))\r\n       ^\r\n/usr/include/string.h:435:13: note: previous declaration is here\r\nextern void explicit_bzero (void *__s, size_t __n) __THROW __nonnull ((1));\r\n            ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:88:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:121:14: note: previous declaration is here\r\nextern char *strcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:95:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpcpy (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:451:14: note: previous declaration is here\r\nextern char *stpcpy (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:103:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncpy (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:124:14: note: previous declaration is here\r\nextern char *strncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:116:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (stpncpy (char *__dest, const char *__src, size_t __n))\r\n       ^\r\n/usr/include/string.h:459:14: note: previous declaration is here\r\nextern char *stpncpy (char *__restrict __dest,\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:126:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strcat (char *__restrict __dest, const char *__restrict __src))\r\n       ^\r\n/usr/include/string.h:129:14: note: previous declaration is here\r\nextern char *strcat (char *__restrict __dest, const char *__restrict __src)\r\n             ^\r\nIn file included from <built-in>:1:\r\nIn file included from /usr/lib/llvm-12/lib/clang/12.0.1/include/__clang_cuda_runtime_wrapper.h:211:\r\nIn file included from /usr/include/string.h:494:\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:133:8: error: exception specification in declaration does not match previous declaration\r\n__NTH (strncat (char *__restrict __dest, const char *__restrict __src,\r\n       ^\r\n/usr/include/string.h:132:14: note: previous declaration is here\r\nextern char *strncat (char *__restrict __dest, const char *__restrict __src,\r\n             ^\r\nIn file included from external/nccl_archive/src/bootstrap.cc:12:\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:117:7: error: call to 'memcpy' is ambiguous\r\n      memcpy(addrs+found, interface->ifa_addr, salen);\r\n      ^~~~~~\r\n/usr/include/string.h:42:14: note: candidate function\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\nIn file included from external/nccl_archive/src/bootstrap.cc:12:\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:192:5: error: call to 'memcpy' is ambiguous\r\n    memcpy(localAddrs+found, interface->ifa_addr, salen);\r\n    ^~~~~~\r\n/usr/include/string.h:42:14: note: candidate function\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\nIn file included from external/nccl_archive/src/bootstrap.cc:12:\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:227:5: error: call to 'memset' is ambiguous\r\n    memset(&hints, 0, sizeof(hints));\r\n    ^~~~~~\r\n/usr/include/string.h:60:14: note: candidate function\r\nextern void *memset (void *__s, int __c, size_t __n) __THROW __nonnull ((1));\r\n             ^\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:59:8: note: candidate function\r\n__NTH (memset (void *__dest, int __ch, size_t __len))\r\n       ^\r\nIn file included from external/nccl_archive/src/bootstrap.cc:12:\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:239:7: error: call to 'memcpy' is ambiguous\r\n      memcpy(&sin, p->ai_addr, sizeof(struct sockaddr_in));\r\n      ^~~~~~\r\n/usr/include/string.h:42:14: note: candidate function\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\nIn file included from external/nccl_archive/src/bootstrap.cc:12:\r\nbazel-out/host/bin/external/nccl_archive/_virtual_includes/include_hdrs/socket.h:245:7: error: call to 'memcpy' is ambiguous\r\n      memcpy(&sin6, p->ai_addr, sizeof(struct sockaddr_in6));\r\n      ^~~~~~\r\n/usr/include/string.h:42:14: note: candidate function\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\nexternal/nccl_archive/src/bootstrap.cc:135:5: error: call to 'memcpy' is ambiguous\r\n    memcpy(rankAddressesRoot+info.rank, &info.extAddressListenRoot, sizeof(union socketAddress));\r\n    ^~~~~~\r\n/usr/include/string.h:42:14: note: candidate function\r\nextern void *memcpy (void *__restrict __dest, const void *__restrict __src,\r\n             ^\r\n/usr/include/x86_64-linux-gnu/bits/string_fortified.h:31:8: note: candidate function\r\n__NTH (memcpy (void *__restrict __dest, const void *__restrict __src,\r\n       ^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated when compiling for sm_61.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/mvg/tensorflow/tensorflow/lite/python/BUILD:59:10 C++ compilation of rule '@nccl_archive//:nccl' failed (Exit 1): clang failed: error executing command /usr/lib/llvm-12/bin/clang -MD -MF bazel-out/host/bin/external/nccl_archive/_objs/nccl/bootstrap.pic.d '-frandom-seed=bazel-out/host/bin/external/nccl_archive/_objs/nccl/bootstrap.pic.o' -iquote ... (remaining 53 argument(s) skipped)\r\nINFO: Elapsed time: 8973.744s, Critical Path: 178.02s\r\nINFO: 16630 processes: 4349 internal, 12281 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nas see above compilation error rising when compiling section NCCL which turn off by configure script.\r\n`.bazelrc` has link for Ubuntu repo NCCL\r\n```\r\n434: build:rbe_linux_cuda11.2_nvcc_base --repo_env=TF_NCCL_CONFIG_REPO=\"@ubuntu18.04-gcc7_manylinux2010-cuda11.2-cudnn8.1-tensorrt7.2_config_nccl\"\r\n460: build:rbe_linux_cuda_clang_base --repo_env=TF_NCCL_CONFIG_REPO=\"@ubuntu18.04-clang_manylinux2010-cuda11.2-cudnn8.1-tensorrt7.2_config_nccl\"\r\n```\r\nMay be problem is here?\r\n\r\n", "comments": ["@Vadim-Maklakov Could you please have a look at the similar issue[ link](https://github.com/tensorflow/tensorflow/issues/36213) and try to use latest stable version of TF 2.6.0 from [source](https://www.tensorflow.org/install/source) ? Please let us know if it helps? Thanks!", "Hi @ sushreebarsa!\r\nI figured out what was going on, it was my mistake that I did not carefully read the documentation for CUDA 11.2.2 and tried to compile nvcc with Clang-12 instead of the required Clang-11. Please close this ticket, my apologies for unnecessary trouble.\r\n In my case in the '/usr/local/cuda-11-2/include/crt/host_config.h` explicitly stated:\r\n\r\n```\r\n#if (__clang_major__ >= 12) || (__clang_major__ < 3) || ((__clang_major__ == 3) &&  (__clang_minor__ < 3))\r\n#error -- unsupported clang version! clang version must be less than 12 and greater than 3.2 . The nvcc flag '-allow-unsupported-compiler' can be used to override this version check; however, using an unsupported host compiler may cause compilation failure or incorrect run time execution. Use at your own risk.\r\n\r\n\r\n```\r\nThank you very much.", "@Vadim-Maklakov Thank you for the update! Closing this issue as it has been resolved for you.Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52101\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52101\">No</a>\n"]}, {"number": 52100, "title": "enable cholesky_op (#1449)", "body": "This enables the cholesky op on ROCM", "comments": []}, {"number": 52099, "title": "[ROCM] enable lu_op (#1433)", "body": "This prs the lu_op on rocm. ", "comments": ["@bloops @gbaned Is there anything I should change in this PR?", "@cheshire @gbaned can we get an update on this PR? Been open for 22 days. ", "@cheshire @chsigg gentle ping", "@cheshire @chsigg The failure on MAC OS is unrelated to the changes that we made. It seems to be a ci issue. \r\n\r\n``` Error: Could not create the Java Virtual Machine.```\r\n", "@cheshire @gbaned can we get an update on the status of this PR?", "@cheshire @gbaned Issues have been addressed.", "@cheshire Can you please review this PR ? Thanks!", "@cheshire @gbaned Is there anything we can do?", "@cheshire Can you please review this PR ? Thanks!", "@cheshire Any updates on this", "@cheshire What should we do about the ci failures? There is no link for more details for the failures. Is it unrelated to this pr?\r\n"]}, {"number": 52098, "title": "Build libtensorflow_cc.dll x86 error on Window", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.6.0\r\n- Python version: Python 3.9.7\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): MSVC 2019\r\n- CUDA/cuDNN version:  11.2\r\n- GPU model and memory: NVIDIA GeForce GTX 1650\r\n\r\n\r\nAs title, I got an error when try to create libtensorflow_cc.lib and libtensorflow_cc.dll which can be used with MSVC2019_32bit compiler.\r\n\r\nI used below command:\r\n`bazel build --cpu=x86 --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings --local_ram_resources=2048 tensorflow:tensorflow_cc`\r\nBut I got below error:\r\n` The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=225\r\nINFO: Reading rc options for 'build' from c:\\users\\1111\\desktop\\work\\tensorflow\\.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Options provided by the client:\r\n  'build' options: --python_path=C:/Users/1111/AppData/Local/Programs/Python/Python39/python.exe\r\nINFO: Reading rc options for 'build' from c:\\users\\1111\\desktop\\work\\tensorflow\\.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true\r\nINFO: Reading rc options for 'build' from c:\\users\\1111\\desktop\\work\\tensorflow\\.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/1111/AppData/Local/Programs/Python/Python39/python.exe --action_env PYTHON_LIB_PATH=C:/Users/1111/AppData/Local/Programs/Python/Python39/lib/site-packages --python_path=C:/Users/1111/AppData/Local/Programs/Python/Python39/python.exe --action_env TF_CUDA_VERSION=11.2 --action_env TF_CUDNN_VERSION=8 --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --config=cuda --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true\r\nINFO: Found applicable config definition build:short_logs in file c:\\users\\1111\\desktop\\work\\tensorflow\\.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file c:\\users\\1111\\desktop\\work\\tensorflow\\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\1111\\desktop\\work\\tensorflow\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:opt in file c:\\users\\1111\\desktop\\work\\tensorflow\\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX\r\nINFO: Found applicable config definition build:cuda in file c:\\users\\1111\\desktop\\work\\tensorflow\\.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda\r\nINFO: Found applicable config definition build:windows in file c:\\users\\1111\\desktop\\work\\tensorflow\\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false\r\nINFO: Found applicable config definition build:monolithic in file c:\\users\\1111\\desktop\\work\\tensorflow\\.bazelrc: --define framework_shared_object=false\r\nDEBUG: C:/users/1111/_bazel_1111/ejl7ywpf/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.\r\nERROR: C:/users/1111/_bazel_1111/ejl7ywpf/external/local_config_cuda/crosstool/BUILD:24:19: in cc_toolchain_suite rule @local_config_cuda//crosstool:toolchain: cc_toolchain_suite '@local_config_cuda//crosstool:toolchain' does not contain a toolchain for cpu 'x86'\r\nERROR: Analysis of target '//tensorflow:tensorflow_cc' failed; build aborted: Analysis of target '@local_config_cuda//crosstool:toolchain' failed\r\nINFO: Elapsed time: 1.352s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (35 packages loaded, 175 targets configured)`\r\n\r\nIs there any way to fix this?\r\nIf yes, please share with me.\r\nThanks", "comments": ["@tilakrayal \r\nCould you please help me check this issue?\r\nThanks", "@VNTamao \r\nThis may be due to conflicts in dependencies.Always it would be better to create an virtual environment and install tensorflow .\r\n\r\nYou might be facing this issue because of the following reasons\r\n\r\nYou are running 32-bit Python or 32-bit OS\r\nYou have not installed the [Microsoft Visual C++ Redistributable](https://support.microsoft.com/en-us/topic/the-latest-supported-visual-c-downloads-2647da03-1eea-4433-9aff-95f26a218cc0) package\r\nYour CPU does not support AVX instructions.\r\nPlease take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.\r\n\r\n\r\nThanks!", "@Saduf2019 \r\nCan we create a library 32 bit to using with Desktop applition?\r\nI'm developing a Desktop application, use Qt IDE and MSVC 2019 32bit.\r\nI tried with MSVC 2019 64bit and it's ok. But when I tried change to MSVC 2019 32bit, I cannot create tensorflow_cc library.\r\nDoes tensorflow support to create library using for MSVC 2019 32bit?\r\nThanks", "No, as explained in most of the issues, there is no support for 32 bits. Please use 64 bit its tested and works as expected. You may move this issue to closed status as the issue reported is addressed.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52098\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52098\">No</a>\n"]}, {"number": 52097, "title": "Update curl to 7.79.1", "body": "This PR updates curl from 7.78 to 7.79.1.\r\n\r\nThe reason of update is to fix the following vulnerabilities in 7.78:\r\n- CVE-2021-22947: STARTTLS protocol injection via MITM\r\n- CVE-2021-22946: Protocol downgrade required TLS bypassed\r\n- CVE-2021-22945: UAF and double-free in MQTT sending\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Is it possible to update to 7.81.0 ?", "@RamanSggs  Added PR #53799 for 7.81.0 upgrade."]}, {"number": 52096, "title": "training with model_main_tf2.py to mobilenetv2 quantized_300x300 cause error", "body": "Hello \r\nI've been training my model with \r\nssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\r\nbut it's very slow with raspberry pi, \r\nso I think I should try with new pretrained model : ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03\r\nwith new model, \r\nwhen I start training, it cause error \r\n`\r\nValueError: ssd_mobilenet_v2 is not supported. See `model_builder.py` for features extractors compatible with different versions of Tensorflow`\r\n\r\nI saw #46970 and  \r\nsaidRaiss recommend to change type :\"ssd_mobilenet_v2\" to ssd_mobilenet_v2_keras\r\nit cause other error saying \r\n```\r\n\r\n2021-09-22 20:51:48.499444: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\pedro\\TFODCourse\\Tensorflow\\models\\research\\object_detection\\model_main_tf2.py\", line 115, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"C:\\Users\\pedro\\anaconda3\\envs\\tensorflow39\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\Users\\pedro\\anaconda3\\envs\\tensorflow39\\lib\\site-packages\\absl\\app.py\", line 312, in run\r\n    _run_main(main, args)\r\n  File \"C:\\Users\\pedro\\anaconda3\\envs\\tensorflow39\\lib\\site-packages\\absl\\app.py\", line 258, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"C:\\Users\\pedro\\TFODCourse\\Tensorflow\\models\\research\\object_detection\\model_main_tf2.py\", line 106, in main\r\n    model_lib_v2.train_loop(\r\n  File \"C:\\Users\\pedro\\anaconda3\\envs\\tensorflow39\\lib\\site-packages\\object_detection-0.1-py3.9.egg\\object_detection\\model_lib_v2.py\", line 599, in train_loop\r\n    load_fine_tune_checkpoint(\r\n  File \"C:\\Users\\pedro\\anaconda3\\envs\\tensorflow39\\lib\\site-packages\\object_detection-0.1-py3.9.egg\\object_detection\\model_lib_v2.py\", line 389, in load_fine_tune_checkpoint\r\n    raise IOError('Checkpoint is expected to be an object-based checkpoint.')\r\nOSError: Checkpoint is expected to be an object-based checkpoint.\r\n```\r\n\r\nmy pre-trainedmodel pipeline.config is as below\r\n\r\n```\r\n\r\nmodel {\r\n  ssd {\r\n    num_classes: 90\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 300\r\n        width: 300\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: \"ssd_mobilenet_v2_keras\"\r\n      depth_multiplier: 1.0\r\n      min_depth: 16\r\n      conv_hyperparams {\r\n        regularizer {\r\n          l2_regularizer {\r\n            weight: 3.99999989895e-05\r\n          }\r\n        }\r\n        initializer {\r\n          truncated_normal_initializer {\r\n            mean: 0.0\r\n            stddev: 0.0299999993294\r\n          }\r\n        }\r\n        activation: RELU_6\r\n        batch_norm {\r\n          decay: 0.999700009823\r\n          center: true\r\n          scale: true\r\n          epsilon: 0.0010000000475\r\n          train: true\r\n        }\r\n      }\r\n    }\r\n    box_coder {\r\n      faster_rcnn_box_coder {\r\n        y_scale: 10.0\r\n        x_scale: 10.0\r\n        height_scale: 5.0\r\n        width_scale: 5.0\r\n      }\r\n    }\r\n    matcher {\r\n      argmax_matcher {\r\n        matched_threshold: 0.5\r\n        unmatched_threshold: 0.5\r\n        ignore_thresholds: false\r\n        negatives_lower_than_unmatched: true\r\n        force_match_for_each_row: true\r\n      }\r\n    }\r\n    similarity_calculator {\r\n      iou_similarity {\r\n      }\r\n    }\r\n    box_predictor {\r\n      convolutional_box_predictor {\r\n        conv_hyperparams {\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 3.99999989895e-05\r\n            }\r\n          }\r\n          initializer {\r\n            truncated_normal_initializer {\r\n              mean: 0.0\r\n              stddev: 0.0299999993294\r\n            }\r\n          }\r\n          activation: RELU_6\r\n          batch_norm {\r\n            decay: 0.999700009823\r\n            center: true\r\n            scale: true\r\n            epsilon: 0.0010000000475\r\n            train: true\r\n          }\r\n        }\r\n        min_depth: 0\r\n        max_depth: 0\r\n        num_layers_before_predictor: 0\r\n        use_dropout: false\r\n        dropout_keep_probability: 0.800000011921\r\n        kernel_size: 1\r\n        box_code_size: 4\r\n        apply_sigmoid_to_scores: false\r\n      }\r\n    }\r\n    anchor_generator {\r\n      ssd_anchor_generator {\r\n        num_layers: 6\r\n        min_scale: 0.20000000298\r\n        max_scale: 0.949999988079\r\n        aspect_ratios: 1.0\r\n        aspect_ratios: 2.0\r\n        aspect_ratios: 0.5\r\n        aspect_ratios: 3.0\r\n        aspect_ratios: 0.333299994469\r\n      }\r\n    }\r\n    post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 9.99999993923e-09\r\n        iou_threshold: 0.600000023842\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SIGMOID\r\n    }\r\n    normalize_loss_by_num_matches: true\r\n    loss {\r\n      localization_loss {\r\n        weighted_smooth_l1 {\r\n        }\r\n      }\r\n      classification_loss {\r\n        weighted_sigmoid {\r\n        }\r\n      }\r\n      hard_example_miner {\r\n        num_hard_examples: 3000\r\n        iou_threshold: 0.990000009537\r\n        loss_type: CLASSIFICATION\r\n        max_negatives_per_positive: 3\r\n        min_negatives_per_image: 3\r\n      }\r\n      classification_weight: 1.0\r\n      localization_weight: 1.0\r\n    }\r\n  }\r\n}\r\ntrain_config {\r\n  batch_size: 24\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n  data_augmentation_options {\r\n    ssd_random_crop {\r\n    }\r\n  }\r\n  optimizer {\r\n    rms_prop_optimizer {\r\n      learning_rate {\r\n        exponential_decay_learning_rate {\r\n          initial_learning_rate: 0.00400000018999\r\n          decay_steps: 800720\r\n          decay_factor: 0.949999988079\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.899999976158\r\n      decay: 0.899999976158\r\n      epsilon: 1.0\r\n    }\r\n  }\r\n  fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\"\r\n  from_detection_checkpoint: true\r\n  num_steps: 20000000\r\n}\r\ntrain_input_reader {\r\n  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\r\n  tf_record_input_reader {\r\n    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100\"\r\n  }\r\n}\r\neval_config {\r\n  num_examples: 8000\r\n  metrics_set: \"coco_detection_metrics\"\r\n  use_moving_averages: true\r\n  include_metrics_per_category: true\r\n}\r\neval_input_reader {\r\n  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\r\n  shuffle: false\r\n  num_readers: 1\r\n  tf_record_input_reader {\r\n    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010\"\r\n  }\r\n}\r\ngraph_rewriter {\r\n  quantization {\r\n    delay: 48000\r\n    weight_bits: 8\r\n    activation_bits: 8\r\n  }\r\n}\r\n\r\n```", "comments": ["Hi @kotran88, Could you please look at the answers of this threads  [1](https://stackoverflow.com/questions/64464315/train-my-model-in-tensorflow-2-checkpoint-is-expected-to-be-an-object-based-chec),[2](https://github.com/tensorflow/tensorflow/issues/46970) .Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52095, "title": "Update device_setter guide for v1 compatibility", "body": "I came across a problem from the #48684 issue. Updating the doc to fix the example.", "comments": []}, {"number": 52094, "title": "Couldn't import 'tensorflow.tools.graph_transforms' in tensorflow 2.5.0", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary pip install\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.9.5\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n```\r\nfrom tensorflow.tools.graph_transforms import TransformGraph\r\n```\r\n\r\nError message:\r\n\r\n```\r\nModuleNotFoundError: No module named 'tensorflow.tools.graph_transforms'\r\n```\r\n\r\nTensorflow version:\r\n```\r\n> pip show tensorflow\r\n\r\nName: tensorflow\r\nVersion: 2.5.0\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https://www.tensorflow.org/\r\nAuthor: Google Inc.\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: ..\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\r\nRequires: tensorboard, wheel, keras-nightly, grpcio, wrapt, six, astunparse, gast, protobuf, typing-extensions, google-pasta, numpy, tensorflow-estimator, h5py, flatbuffers, opt-einsum, absl-py, keras-preprocessing, termcolor\r\nRequired-by:\r\n```\r\n\r\n**Describe the expected behavior**\r\nSuccessfully import the graph_transforms module.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing): N/A\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\nfrom tensorflow.tools.graph_transforms import TransformGraph\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nSource code: https://replit.com/@Amit-KumarKuma4/objectdetection", "comments": ["@amitkumardatta \r\nPlease refer to these same error issues: [link](https://github.com/tensorflow/tensorflow/issues/33352), [link1](https://stackoverflow.com/questions/59825444/modulenotfounderror-no-module-named-tensorflow-tools-graph-transforms-when-us)", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52094\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52094\">No</a>\n"]}, {"number": 52093, "title": "[MHLO]: refine delete redundant specialization function", "body": "using c++ SFINAE auto select map template paramater", "comments": ["Should I update my code base\uff1f", "TensorFlow is for now still limited to C++14 unfortunately, and our linter flagged there:\r\n```\r\n  /third_party/tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/transforms/map_lmhlo_to_scalar_op.h:249 ClangTidy: template template parameter using 'typename' is incompatible with C++ standards before C++17\r\n  /third_party/tensorflow/compiler/mlir/hlo/include/mlir-hlo/Dialect/mhlo/transforms/map_lmhlo_to_scalar_op.h:254 ClangTidy: template template parameter using 'typename' is incompatible with C++ standards before C++17\r\n```", "Thank you for reminding me. I'll fix it later.", "> using c++ SFINAE auto select map template paramater\r\n\r\n"]}, {"number": 52092, "title": "Undeclared identifier when building from source v2.3.2", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.3.2\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): msvc v142\r\n- CUDA/cuDNN version: CUDA 10.2 / cuDNN 7.6\r\n- GPU model and memory: Nvidia GeForce RTX 2060 / 14 GB\r\n\r\n\r\n\r\n**Describe the problem**\r\nUndeclared identifier error when building from source v2.3.2. \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n**Configuration**\r\n> $ python configure.py\r\n> You have bazel 3.1.0 installed.\r\n> Please specify the location of python. [Default is C:\\python\\python.exe]:\r\n> \r\n> \r\n> Found possible Python library paths:\r\n>   C:\\python\\lib\\site-packages\r\n> Please input the desired Python library path to use.  Default is [C:\\python\\lib\\site-packages]\r\n> \r\n> Do you wish to build TensorFlow with ROCm support? [y/N]: n\r\n> No ROCm support will be enabled for TensorFlow.\r\n> \r\n> Do you wish to build TensorFlow with CUDA support? [y/N]: y\r\n> CUDA support will be enabled for TensorFlow.\r\n> \r\n> Found CUDA 10.2 in:\r\n>     C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64\r\n>     C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include\r\n> Found cuDNN 7 in:\r\n>     C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/lib/x64\r\n>     C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.2/include\r\n> \r\n> \r\n> Please specify a list of comma-separated CUDA compute capabilities you want to build with.\r\n> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\n> Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]:\r\n> \r\n> \r\n> Please specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n> \r\n> \r\n> Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: n\r\n> Not overriding eigen strong inline, some compilations could take more than 20 mins.\r\n> \r\n> Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:\r\n> Not configuring the WORKSPACE for Android builds.\r\n> \r\n> Preconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n>         --config=mkl            # Build with MKL support.\r\n>         --config=monolithic     # Config for mostly static monolithic build.\r\n>         --config=ngraph         # Build with Intel nGraph support.\r\n>         --config=numa           # Build with NUMA support.\r\n>         --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n>         --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\n> Preconfigured Bazel build configs to DISABLE default on features:\r\n>         --config=noaws          # Disable AWS S3 filesystem support.\r\n>         --config=nogcp          # Disable GCP support.\r\n>         --config=nohdfs         # Disable HDFS support.\r\n>         --config=nonccl         # Disable NVIDIA NCCL support.\r\n> \r\n**Build command:**\r\n`bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package` \r\n\r\n**Any other info / logs**\r\n`external/com_google_absl\\absl/time/clock.h(70): error C2065: 'Duration': undeclared identifier\r\nexternal/com_google_absl\\absl/time/clock.h(70): error C2146: syntax error: missing ')' before identifier 'duration'\r\nexternal/com_google_absl\\absl/time/clock.h(70): error C2143: syntax error: missing ';' before '{'\r\nexternal/com_google_absl\\absl/time/clock.h(70): error C2447: '{': missing function header (old-style formal list?)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 899.084s, Critical Path: 504.33s\r\nINFO: 1352 processes: 1352 local.\r\nFAILED: Build did NOT complete successfully` \r\n\r\nThe issue has been reported already [#38316](https://github.com/tensorflow/tensorflow/issues/38316). But the solution is not proper.\r\n", "comments": ["@Ramsonjehu ,\r\nCan you please try installing TensorFlow v2.3 with CUDA 10.1 and cuDNN 7.6 and check if you are facing the same error.TF v2.3 is little older version.Suggesting to upgrade to latest stable tf v2.6. For more information please take a look at the tested [build configurations](https://www.tensorflow.org/install/source_windows#gpu) Thanks!\r\n\r\n\r\n\r\n", "@tilakrayal \r\n1. I'm Trying to use NXP's eIQ portal application which requires TF v2.3.2. so I can't upgrade to TF v2.6. \r\n2. Does this error has anything to do with CUDA 10.2? because all the other build configuration are same as in the link you provided. ", "Hi @tilakrayal ,\r\nAs you said i tried building with CUDA 10.1. But im facing a new issue as mentioned below.\r\n\r\n`DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1556410077 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule git_repository defined at:\r\n  C:/users/surya/_bazel_surya/h5eisutx/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18: in <toplevel>\r\nINFO: Repository local_config_cuda instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule cuda_configure defined at:\r\n  F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl:1399:18: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cuda':\r\n   Traceback (most recent call last):\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1214, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 363, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 363, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n\r\nERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1214, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 363, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 363, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n\r\nWARNING: Target pattern parsing failed.\r\nERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1369\r\n                _create_local_cuda_repository(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1213, in _create_local_cuda_repository\r\n                to_list_of_strings(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 1214, in to_list_of_strings\r\n                _cuda_include_path(<2 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 363, in _cuda_include_path\r\n                inc_entries.append(<1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/gpus/cuda_configure.bzl\", line 363, in inc_entries.append\r\n                realpath(repository_ctx, <1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/remote_config/common.bzl\", line 268, in realpath\r\n                execute(repository_ctx, <1 more arguments>)\r\n        File \"F:/ramson/tensorflow_source/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\n\r\nINFO: Elapsed time: 0.968s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n    currently loading: tensorflow/tools/pip_package`", "@Ramsonjehu ,\r\nCan you please refer this [comment](https://github.com/tensorflow/tensorflow/issues/44251#issuecomment-724863274) from issue with the similar error.It helps.Thanks!", "@tilakrayal , In the [comment](https://github.com/tensorflow/tensorflow/issues/44251#issuecomment-724863274) you mentioned, its been told to check the line which causes the error. \r\n \r\n\r\n    def _cuda_autoconf_impl(repository_ctx):\r\n    \"\"\"Implementation of the cuda_autoconf repository rule.\"\"\"\r\n     if not enable_cuda(repository_ctx):\r\n        _create_dummy_repository(repository_ctx)\r\n    elif get_host_environ(repository_ctx, _TF_CUDA_CONFIG_REPO) != None:\r\n        has_cuda_version = get_host_environ(repository_ctx, _TF_CUDA_VERSION) != None\r\n        has_cudnn_version = get_host_environ(repository_ctx, _TF_CUDNN_VERSION) != None\r\n        if not has_cuda_version or not has_cudnn_version:\r\n            auto_configure_fail(\"%s and %s must also be set if %s is specified\" %\r\n                                (_TF_CUDA_VERSION, _TF_CUDNN_VERSION, _TF_CUDA_CONFIG_REPO))\r\n        _create_remote_cuda_repository(\r\n            repository_ctx,\r\n            get_host_environ(repository_ctx, _TF_CUDA_CONFIG_REPO),\r\n        )\r\n    else:\r\n        _create_local_cuda_repository(repository_ctx) // **This is line number 1369 mentioned in the error**\r\n\r\nAs you can see **_repository_ctx_** is the argument to the function, im unable to find from where the function is called, so i can't validate if the path exists or not.  Can you please suggest any specific solution ?", "@Ramsonjehu \r\nIs there any specific reason for using 2.3.2, as many bugs are fixed is later versions would you want to try on later versions.\r\n\r\nAlso can you try:\r\nCould you please try and let us know:\r\n$ bazel clean\r\n$ ./configure # make sure to choose gpu support\r\n$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer\r\n\r\nIf you still face error please follow [this comment](https://github.com/tensorflow/tensorflow/issues/50943#issuecomment-912322300).\r\n", "Hi @Saduf2019 ,\r\n**_Is there any specific reason for using 2.3.2?_** \r\nYes, as i mentioned before I'm Trying to use NXP's eIQ portal application which requires TF v2.3.2. so I can't upgrade to TF v2.6.\r\n\r\n**_Also can you try:\r\nCould you please try and let us know:\r\n$ bazel clean\r\n$ ./configure # make sure to choose gpu support**\r\nI couldn't run ./configure, instead i tried running python configure.py and choose gpu support \r\n**$ bazel build -c opt --config=cuda //tensorflow/cc:tutorials_example_trainer** \r\nwhen running the above command, I get the following error:\r\n`ERROR: While parsing option -c -opt: Not a valid compilation mode: '-opt' (should be fastbuild, dbg or opt)`", "@Ramsonjehu \r\n Did you try : If you still face error please follow this comment. above, do let us know.", "@Ramsonjehu, Does [this reference](https://github.com/tensorflow/tensorflow/issues/50943#issuecomment-912322300) helps you?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52092\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52092\">No</a>\n"]}, {"number": 52091, "title": "Cannot load saved tf model (AttributeError: '_UserObject' object has no attribute 'add_slot')", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.9\r\n- CUDA/cuDNN version: 11.3/8.2.1\r\n- GPU model and memory: RTX 8000, 48 GB\r\n\r\n**Describe the current behavior**\r\nI'm loading our upsample model and saving it as a saved model:\r\n```py\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\nfrom tensorflow_addons.layers import InstanceNormalization, SpectralNormalization\r\n\r\nmodel = load_model('upsample---[_20]---[______3171]---[_____63420].h5', custom_objects={'InstanceNormalization': InstanceNormalization, 'SpectralNormalization': SpectralNormalization})\r\n\r\ntf.saved_model.save(model, 'upsample_saved_model')\r\n```\r\nThen, when I'm trying to load it (do not use the same runtime if running in Collab):\r\n```py\r\nimport tensorflow as tf\r\n\r\nmodel = tf.saved_model.load('upsample_saved_model')\r\n```\r\nIt errors with:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/daniel/trt/2_find_layer_names.py\", line 4, in <module>\r\n    model = tf.saved_model.load('upsample_saved_model')\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 864, in load\r\n    result = load_internal(export_dir, tags, options)[\"root\"]\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 902, in load_internal\r\n    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 162, in __init__\r\n    self._load_all()\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 259, in _load_all\r\n    self._load_nodes()\r\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/saved_model/load.py\", line 448, in _load_nodes\r\n    slot_variable = optimizer_object.add_slot(\r\nAttributeError: '_UserObject' object has no attribute 'add_slot'\r\n```\r\n\r\nThe same code works just fine in TensorFlow 2.5.\r\n\r\nIf you need the model to test: https://github.com/Sentdex/GANTheftAuto/tree/main/trained_models (it's the upsample one).\r\n\r\n**Describe the expected behavior**\r\nThis should not error and load the model like in TensorFlow 2.5\r\n\r\n**Standalone code to reproduce the issue**\r\nAs described above.\r\n\r\nAdditionally, I mentioned this issue here: https://github.com/tensorflow/tensorflow/issues/52013#issuecomment-920475505 and \r\nmohantym created a notebook and ran it in Collab: https://colab.research.google.com/gist/mohantym/187d7a2dc387998237211d7c590c6b1a/github_52013_sentdex.ipynb - you can replicate the issue in this notebook too, but after downloading/installing all the dependencies, then loading the model and saving it as the saved model, use Runtime -> Restart runtime before attempting to load saved model, otherwise this bug is not going to be triggered.", "comments": ["Hi @daniel-kukiela !Could you please try again  Python 3.7/3.8? Tried to replicate in [2.5](https://colab.research.google.com/gist/mohantym/de39207fdedadde6861bf60eae50f9b5/github_52013_sentdex.ipynb#scrollTo=AwimBoSxRdv8),[2.6](https://colab.research.google.com/gist/mohantym/187d7a2dc387998237211d7c590c6b1a/github_52013_sentdex.ipynb#scrollTo=x0Jbyu8RaJpP) and [2.7](https://colab.research.google.com/gist/mohantym/6d58b25b5dc4fa20239b9729806d77cc/github_52013_sentdex.ipynb#scrollTo=J-mTHl22UhJv) .Thanks!", "@mohantym I mentioned twice (one in this other issue and once here) that you cannot load the save modes in the current runtime. You need to load it in fresh runtime (for example by invoking Runtime -> Restart runtime). When you are running it in the same runtime that you used to save the model (like if you run it all in a single script), this bug does not trigger. By restartting runtime you are loading a model like from a separate script, and then this bug occurs.\r\n\r\nI ran your TF 2.6 notebook in Collab, but restarted runtime before loading the model (as in my steps to reproduce in Collab):\r\n![image](https://user-images.githubusercontent.com/4561245/134351979-31581182-fa1f-4b0f-8732-72832b65effd.png)\r\n\r\nCan you please follow my steps to replicate it under TF 2.6? TF 2.5 works ok.\r\n\r\nThank you for your help", "Hmm, I checked your TF 2.7 nightly notebook and the issue seems to be gone there. It is possible that someone already found this bug and fixed it then.\r\n\r\nSounds like we should consider it fixed in TF 2.7 then?\r\n\r\n\r\nThank you", "Thanks @daniel-kukiela for confirming the same .Feel free to close the issue if it is fixed in nightly.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52091\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52091\">No</a>\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 52090, "title": "Unable to run the exe that has wheel generated by Rosetta terminal on MacOs Mojave, error: Symbol not found: ___darwin_check_fd_set_overflow", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Mojave 10.14.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not relevant\r\n- TensorFlow installed from (source or binary): private wheel\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: virtualenv\r\n- Bazel version (if compiling from source): 0.21.0\r\n- GCC/Compiler version (if compiling from source): not relevant \r\n- CUDA/cuDNN version: not relevant\r\n- GPU model and memory: not relevant\r\n\r\n\r\n\r\n**Describe the problem**\r\nFollowing the instructions in forum: https://github.com/tensorflow/tensorflow/issues/46044\r\nI compiled tensorflow 1.13.1 using Bazel 0.21.1 under Bigsur using rosetta terminal in order to run this wheel under Rosettna emulator over arm/intel cpu, the generated wheel works fine on Bigsur and Catalina but it didn't work on Mojave I am getting error appeared in the logs section.\r\nHowever the public wheel works fine on Mojave 10.14.6.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. Use Big Sir with arm processor m1 since it has Terminal for Rosetta\r\n\r\n2. open Terminal Get Info and enable \"Open using Rosetta\" (duplicate Terminal app for this and open Rosetta terminal)\r\n\r\n3. git clone https://github.com/tensorflow/tensorflow.git\r\n\r\n4. cd tensorflow\r\n\r\n5. git checkout branch_name (for us: v1.13.1)\r\n\r\n6. install bazel version 0.21.0\r\n\r\n7. generate virtual environment of python3.6 that has in the requirements tensorflow==1.13.1 and other packages related to our project\r\n\r\n8. ./configure (say N to everything except setting the python virtual environment created in 7 here)\r\n\r\n9. bazel build //tensorflow/tools/pip_package:build_pip_package\r\n\r\n10.\u00a0bazel-bin/tensorflow/tools/pip_package/build_pip_package/tmp/tensorflow_pkg\r\n\r\n11. rename .whl file to disable OS restrictions (us: tensorflow-1.13.1-py3-none-any.whl)\r\n\r\n12. Take this wheel that support rosetta and set it in the virtual environment (instead of downloading public wheel), in requirements: ./wheels/tensorflow-1.13.1-py3-none-any.whl \r\n\r\n13. Build the exe using pyinstaller version 3.4 with this virtual environment that has private tensorflow wheel.\r\n\r\n14. Run the exe from 13 on Moajve and you will get the error attached in logs \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n2021-09-20 14:10:58,992 INFO Traceback (most recent call last):\r\n  File \"tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n  File \"imp.py\", line 297, in find_module\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\r\n  File \"/p4client/ProAudio/dev_mainV12/ProAudio/XPlatform/Apps/WavesPluginServer/pythonVirtualEnv/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 627, in exec_module\r\n  File \"tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\r\n  File \"tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\nImportError: dlopen(/Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so, 6): Symbol not found: ___darwin_check_fd_set_overflow\r\n  Referenced from: /Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so (which was built for Mac OS X 11.0)\r\n  Expected in: /usr/lib/libSystem.B.dylib\r\n in /Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@Nizarazo \r\nWe see that you are using old version of tensorflow(`1.13`) which is not actively supported. We recommend that you upgrade to `2.6.0` and let us know if the issue still persists in newer versions. Please have a look at the [link](https://www.tensorflow.org/install/source#macos) to `Build from source` and let us know if it helps ?Thanks!", "Hi @sushreebarsa @Saduf2019 \r\n\r\nThe issue is not related to tensorflow version 1.13 because the public wheel of tensorflow 1.13 is working fine on Mojave, the issue is the private wheel generated using rosetta terminal under BigSur M1 it does not run on Mojave so I guess it is something related to the way I build the private wheel under BigSir M1 rosetta terminal.\r\nIs there any configuration I need to add while building in BigSur M1 using Bazel for example \"minimum macos version\" so that it can support Mojave?\r\nI wrote the 14 steps how I build the wheel above please have a look and let me know what step is wrong/missing.\r\n\r\nThanks ", "@Nizarazo \r\nThe Apple TF on M1 chips is a private fork of TF owned by Apple. We don't have access to fixing code issues there.\r\n\r\nFor more information, please go through issue #44751. Thanks!", "@Saduf2019 I am not using Apple TF but I am using your tensorflow by building it from source using rosetta terminal under Big Sur as explained in thread #46044 so the generated wheel runs fine over BigSur M1 using Rosetta emulator and it runs fine also on catalina but it doesn't run on Mojave I am getting this:\r\n2021-09-20 14:10:58,992 INFO Traceback (most recent call last):\r\nFile \"tensorflow/python/pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\nFile \"imp.py\", line 297, in find_module\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile \"tensorflow/python/pywrap_tensorflow.py\", line 58, in\r\nFile \"/p4client/ProAudio/dev_mainV12/ProAudio/XPlatform/Apps/WavesPluginServer/pythonVirtualEnv/lib/python3.6/site-packages/PyInstaller/loader/pyimod03_importers.py\", line 627, in exec_module\r\nFile \"tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in\r\nFile \"tensorflow/python/pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\nImportError: dlopen(/Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so, 6): Symbol not found: ___darwin_check_fd_set_overflow\r\nReferenced from: /Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so (which was built for Mac OS X 11.0)\r\nExpected in: /usr/lib/libSystem.B.dylib\r\nin /Library/Application Support/Waves/WavesPluginServer/WavesPluginServerV13.1.bundle/Contents/MacOS/_pywrap_tensorflow_internal.so\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions. Include the entire stack trace\r\nabove this error message when asking for help.", "@Nizarazo \r\nFor the error, can you refer to same error issues and let us know: \"ImportError: No module named '_pywrap_tensorflow_internal'\"\r\n[link](https://github.com/tensorflow/tensorflow/issues/40459),[link1](https://github.com/tensorflow/tensorflow/issues/37906),[link2](https://stackoverflow.com/questions/44080677/no-module-named-pywrap-tensorflow-internal),[link](https://github.com/tensorflow/tensorflow/issues/11571)", "@Saduf2019\r\nThese links are all for Windows while the issue I am facing is in MacOs Mojave any similar issues in MacOs?\r\n\r\nBy the way I see that the file _pywrap_tensorflow_internal.so exists but seems that the symbol ___darwin_check_fd_set_overflow is missing.", "Fixed by building Tensorflow using older macOS Catalina instead of Big Sur seems sadly it is not backward compatible.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52090\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52090\">No</a>\n"]}, {"number": 52089, "title": "tensor flow resource exhausted error on Kaggle notebooks", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.0\r\n- Python version: 3.9\r\n- Bazel version (if compiling from source): no\r\n- GCC/Compiler version (if compiling from source): NIL\r\n- CUDA/cuDNN version: NIL\r\n- GPU model and memory: kaggle notebook\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: \r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**this is the error i have been displayed**:\r\nResource exhausted: OOM when allocating tensor with\r\n shape[800000,32,30,62] and type float on\r\n /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc    \r\n [[{{node conv2d_1/convolution}}]]\r\n\r\n**Describe the expected behavior**:\r\n\r\nthe expected behavior was that it runs the code and outputs the desired result. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nnone\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nnone\r\n", "comments": ["@VivanVatsa ,\r\nWe see you are using tf v2.0. Could you please update TensorFlow to the latest stable version v2.6 and let us know if you are facing the same error. Thanks!", "was running the latest tensorflow2.6 on Kaggle notebooks, still no luck\r\n\r\ntried installing TensorFlow for m1 chips!", "@VivanVatsa ,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code and dataset to reproduce the issue reported here.Thanks!"]}, {"number": 52088, "title": "How to reduce libtensorflowlite.so size with C++ api", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.5.1\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 12.0.5\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: android GPU\r\n\r\n\r\n\r\n**Describe the problem**\r\nHello , I want to build libtensorflowlite.so for android with c++ api, I want to build the selective so with a tflite model  .I follow the guide in https://github.com/tensorflow/tensorflow/issues/50946\r\nI add tflite_custom_cc_library like this:\r\n\r\nload(\"//tensorflow/lite:build_def.bzl\", \"tflite_cc_shared_object\", \"tflite_copts\", \"tflite_copts_warnings\", \"tflite_custom_cc_library\")\r\ntflite_custom_cc_library(\r\n    name = \"custom_tflite_lib\",\r\n    models = [\r\n        \"test.tflite\",\r\n    ],\r\n)\r\nand the trained tflite model for selective build. And the the command is :\r\nbazel build -c opt --config android_arm64 tensorflow/lite:custom_tflite_lib\r\n\r\nIt built successfully.And the  so size reduced from 3.7M to 380K. But when I link the so, I found that it only contains the kernels and so on ,it doesn't contain the flatbuffer and other functions such as \"TfLiteDelegateCreate\" and so on.I have also read the issue https://github.com/tensorflow/tensorflow/issues/50946 \r\nbut the so built is only for c api, I want to use the android gpu to inference with opencl, so how should I reduce the libtensorflowlite.so for c++ api with other necessary parts?\r\n\r\nThanks.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nbazel build -c opt --config android_arm64 tensorflow/lite:custom_tflite_lib\r\n\r\n", "comments": ["Hi @jvishnuvardhan ,Could you please look at this issue!", "That cc_library is for using as dependencies of other bazel rule.\r\nfor cc shared object, you can try: in tensorflow/lite/BUILD:\r\n- remove \":framework\"  and \"//third_party/tensorflow/lite/kernels:builtin_ops_all_linked\" in the deps field\r\n- add your new custom cc_library there\r\nThen you can try to build the //tensorflow/lite:tensorflowlite.", "@Liupengshuaige, Can you please try as described [in the comment](https://github.com/tensorflow/tensorflow/issues/52088#issuecomment-930701228) and let us know?", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52088\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52088\">No</a>\n"]}, {"number": 52084, "title": "fix a document typo in registration.h", "body": "i found a little typo in the explanation comment of \"TF_NEW_ID_FOR_INIT\"  in registration.h.\r\nline 144:   //   #define M(a, b) TF_NEW_ID_FOR_INIT(M, a, b)\r\n          ->   //   #define M(a, b) TF_NEW_ID_FOR_INIT(M_IMPL, a, b)\r\nlower one is correct. thank you.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F52084) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!"]}, {"number": 52083, "title": "Bug", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no):\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52083\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/52083\">No</a>\n"]}, {"number": 52082, "title": "[XLA] Make EmitFullWarpShuffleDownLoopForReduce test the required condition.", "body": "@cheshire \r\n\r\nThis check was recently removed by commit 3b9f82ecd122352c65a26b9b8943e8da73f3e77c. As it is very hard to detect an error, it is better to keep it.", "comments": ["@nouiz  Can you please resolve conflicts? Thanks!\r\n", "rebased."]}, {"number": 52081, "title": "Removed README.md", "body": "Since the URL is showing 404 error and alternative URL is not present in the repo for README file removed URL https://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md from the file.", "comments": []}]