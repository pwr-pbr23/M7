[{"number": 5904, "title": "slim/preprocess missing", "body": "I would like to use slim.preprocess (as featured in slim/README.md).\r\n\r\nThanks,\r\nPhilip", "comments": ["There are some preprocessing functions for different nets here, https://github.com/tensorflow/models/tree/master/slim/preprocessing.\r\n\r\nI don't know where `slim.preprocess` went.", "slim.preprocess was deprecated and removed.   I have opened a bug to update the documentation."]}, {"number": 5903, "title": "typo `Conv` to `conv2d`", "body": "", "comments": ["Can one of the admins verify this patch?"]}, {"number": 5902, "title": "Input tensor on GPU in C++ API", "body": "I am trying to feed a Tensor (using the C++ API) that has memory allocated on the GPU (using GPUBFCAllocator) into a network.\r\nNow, the placeholder in the network is on the GPU (I checked this in Tensorboard), and the memory allocated for the input tensor is on the GPU, but whenever I run the network, `nvprof --print-gpu-trace` shows me `[CUDA memcpy HtoD]` and `[CUDA memcpy DtoH]`, before the computations (i.e. convolutions etc.) start.\r\nThis suggests to me that the input tensor is being copied to CPU memory, and then back to GPU memory.\r\n\r\nWhile debugging this, I found multiple hints in the source that seem to suggest the CPU is always used as device to feed tensors from.\r\nSee e.g.:\r\nhttps://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/core/common_runtime/direct_session.cc#L264\r\nhttps://github.com/tensorflow/tensorflow/blob/429905c6283d3182a816487807e97e592849ce19/tensorflow/core/common_runtime/graph_runner.cc#L109\r\n\r\n1. Is this analysis correct?\r\n2. How can one feed in a tensor that has memory allocated on GPU memory, without copying back and forth to CPU memory? If this is currently not possible, then I think this would be a good feature to add.\r\nEspecially when one wants to combine Tensorflow input/output with other algorithms (not in TF), one might want to keep data on the GPU, to avoid host to device and device to host transfers.\r\n\r\nThanks in advance.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04.1 LTS\r\n\r\nInstalled version of CUDA and cuDNN: CUDA 8.0, cuDNN 5.1.5\r\nOutput of `ls -l /usr/local/cuda/lib64/libcud*`:\r\n```\r\n-rw-r--r-- 1 root root 558720 Sep 15 00:02 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root     16 Sep 15 00:05 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root     19 Sep 15 00:05 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 415432 Sep 15 00:02 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root 775162 Sep 15 00:02 /usr/local/cuda/lib64/libcudart_static.a\r\nlrwxrwxrwx 1 root root     43 Oct  3 17:03 /usr/local/cuda/lib64/libcudnn.so -> /usr/local/cudnn-8.0-v5.1/lib64/libcudnn.so\r\nlrwxrwxrwx 1 root root     45 Oct  3 17:03 /usr/local/cuda/lib64/libcudnn.so.5 -> /usr/local/cudnn-8.0-v5.1/lib64/libcudnn.so.5\r\nlrwxrwxrwx 1 root root     49 Oct  3 17:03 /usr/local/cuda/lib64/libcudnn.so.5.1.5 -> /usr/local/cudnn-8.0-v5.1/lib64/libcudnn.so.5.1.5\r\nlrwxrwxrwx 1 root root     49 Oct  3 17:03 /usr/local/cuda/lib64/libcudnn_static.a -> /usr/local/cudnn-8.0-v5.1/lib64/libcudnn_static.a\r\n```\r\nTensorflow installed from source:\r\n1. The commit hash (`git rev-parse HEAD`): a5074383617a9947f248a0ddd56b94f9fb0f970b\r\n2. The output of `bazel version`:\r\n```\r\nBuild label: 0.4.0\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nThis should give the general idea.\r\n```cpp\r\ntensorflow::GPUBFCAllocator* allocator = new tensorflow::GPUBFCAllocator(0, sizeof(float) * height * width * 3);\r\ntensorflow::Tensor input_tensor = tensorflow::Tensor(allocator, tensorflow::DataType::DT_FLOAT, tensorflow::TensorShape( { 1, height, width, 3 }));\r\nstd::vector<tensorflow::Tensor>* outputs = new std::vector<tensorflow::Tensor>;\r\n<copy some data into the allocated space>\r\n<create a new session, load graph etc.> // Note: the \"input_layer\" is on the GPU\r\nsession->Run( { { \"input_layer\", input_tensor } }, { \"output_layer\" }, { }, outputs);\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n\r\nPartial output of nvprof --print-gpu-trace:\r\n```\r\n   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput           Device   Context    Stream  Name\r\n249.54ms  4.7360us                    -               -         -         -         -  1.0039KB  207.01MB/s  GeForce GTX 107         1         7  [CUDA memset]\r\n932.67ms  44.799us                    -               -         -         -         -  384.00KB  8.1745GB/s  GeForce GTX 107         1         7  [CUDA memcpy HtoD]\r\n933.22ms  15.584us            (12 32 1)        (32 8 1)        20        0B        0B         -           -  GeForce GTX 107         1         7  void cv::cudev::grid_transform_detail::transformSmart<int=4, unsigned char, float, cv::cudev::saturate_cast_func<unsigned char, float>, cv::cudev::WithOutMask>(cv::cudev::GlobPtr<unsigned char>, cv::cudev::grid_transform_detail::transformSmart<int=4, unsigned char, float, cv::cudev::saturate_cast_func<unsigned char, float, float>, cv::cudev::WithOutMask>, unsigned char, float, int, int) [158]\r\n933.29ms  121.12us                    -               -         -         -         -  1.5000MB  12.094GB/s  GeForce GTX 107         1         7  [CUDA memcpy DtoH]\r\n1.24468s  2.1120us                    -               -         -         -         -        4B  1.8062MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24492s     992ns                    -               -         -         -         -        4B  3.8455MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24522s     992ns                    -               -         -         -         -        4B  3.8455MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24553s     992ns                    -               -         -         -         -        4B  3.8455MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24594s  1.0880us                    -               -         -         -         -        4B  3.5062MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24632s     992ns                    -               -         -         -         -        4B  3.8455MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24800s     992ns                    -               -         -         -         -        8B  7.6909MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24816s  1.1200us                    -               -         -         -         -      512B  435.97MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24830s  38.879us                    -               -         -         -         -  288.00KB  7.0644GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24846s  1.0880us                    -               -         -         -         -      512B  448.79MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24858s  1.1200us                    -               -         -         -         -      512B  435.97MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24868s  1.1200us                    -               -         -         -         -      512B  435.97MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24874s  1.1200us                    -               -         -         -         -      512B  435.97MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24963s  106.62us                    -               -         -         -         -  1.1250MB  10.304GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24981s  1.4080us                    -               -         -         -         -  1.0000KB  693.58MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.24987s  1.1840us                    -               -         -         -         -  1.0000KB  824.80MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25153s  196.60us                    -               -         -         -         -  2.2500MB  11.176GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25189s  1.4080us                    -               -         -         -         -  1.0000KB  693.58MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25196s  1.2800us                    -               -         -         -         -  1.0000KB  762.94MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25250s  180.92us                    -               -         -         -         -  2.2500MB  12.145GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25276s  1.4080us                    -               -         -         -         -  1.0000KB  693.58MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25281s  1.1840us                    -               -         -         -         -  1.0000KB  824.80MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25332s  180.67us                    -               -         -         -         -  2.2500MB  12.162GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25355s  1.4400us                    -               -         -         -         -  1.0000KB  678.17MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25359s  1.1520us                    -               -         -         -         -  1.0000KB  847.71MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25406s  191.36us                    -               -         -         -         -  2.2500MB  11.483GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25431s  1.4400us                    -               -         -         -         -  1.0000KB  678.17MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25435s  1.1520us                    -               -         -         -         -  1.0000KB  847.71MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25461s  90.910us                    -               -         -         -         -  1.1250MB  12.085GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25477s  1.3120us                    -               -         -         -         -  1.0000KB  744.33MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25482s  1.1520us                    -               -         -         -         -  1.0000KB  847.71MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25486s  1.0560us                    -               -         -         -         -       68B  61.411MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25490s  4.1280us                    -               -         -         -         -  38.250KB  8.8367GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25500s  1.1840us                    -               -         -         -         -      256B  206.20MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25506s  1.0560us                    -               -         -         -         -      256B  231.19MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25511s  12.288us                    -               -         -         -         -  144.00KB  11.176GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25523s  1.0880us                    -               -         -         -         -      256B  224.39MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25526s  1.0560us                    -               -         -         -         -      256B  231.19MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25532s  12.320us                    -               -         -         -         -  144.00KB  11.147GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25542s  1.0880us                    -               -         -         -         -      256B  224.39MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25545s  1.0880us                    -               -         -         -         -      256B  224.39MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25549s  1.6640us                    -               -         -         -         -  6.7500KB  3.8686GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25552s  1.0880us                    -               -         -         -         -      256B  224.39MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25555s  1.1840us                    -               -         -         -         -      256B  206.20MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25559s  1.0880us                    -               -         -         -         -      256B  224.39MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25562s  1.0880us                    -               -         -         -         -      256B  224.39MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25570s  23.487us                    -               -         -         -         -  288.00KB  11.694GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25580s  1.1200us                    -               -         -         -         -      512B  435.97MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25584s  1.1200us                    -               -         -         -         -      512B  435.97MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25597s  45.919us                    -               -         -         -         -  576.00KB  11.963GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25607s  1.1200us                    -               -         -         -         -      512B  435.97MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25610s  1.0880us                    -               -         -         -         -      512B  448.79MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25624s  45.919us                    -               -         -         -         -  576.00KB  11.963GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25634s  1.0880us                    -               -         -         -         -      512B  448.79MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25638s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25641s  1.4720us                    -               -         -         -         -  2.0000KB  1.2958GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25645s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25649s  1.1520us                    -               -         -         -         -  1.0000KB  847.71MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25652s  1.1840us                    -               -         -         -         -  1.0000KB  824.80MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25656s  1.1520us                    -               -         -         -         -  1.0000KB  847.71MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25659s  1.0880us                    -               -         -         -         -      512B  448.79MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25662s  1.1200us                    -               -         -         -         -      512B  435.97MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25666s  1.0880us                    -               -         -         -         -      256B  224.39MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25669s  1.0560us                    -               -         -         -         -      256B  231.19MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25673s  1.1520us                    -               -         -         -         -  1.0000KB  847.71MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25676s  1.1840us                    -               -         -         -         -  1.0000KB  824.80MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25680s  1.1520us                    -               -         -         -         -  1.0000KB  847.71MB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.25965s  360.89us                    -               -         -         -         -  4.5000MB  12.177GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.26011s  1.5360us                    -               -         -         -         -  2.0000KB  1.2418GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.26015s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.26543s  719.60us                    -               -         -         -         -  9.0000MB  12.214GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.26626s  1.8240us                    -               -         -         -         -  2.0000KB  1.0457GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.26630s  1.2480us                    -               -         -         -         -  2.0000KB  1.5283GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.26788s  727.98us                    -               -         -         -         -  9.0000MB  12.073GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.26872s  1.5040us                    -               -         -         -         -  2.0000KB  1.2682GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.26876s  1.2480us                    -               -         -         -         -  2.0000KB  1.5283GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27030s  718.67us                    -               -         -         -         -  9.0000MB  12.230GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27109s  1.5680us                    -               -         -         -         -  2.0000KB  1.2164GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27112s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27268s  718.58us                    -               -         -         -         -  9.0000MB  12.231GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27349s  1.5040us                    -               -         -         -         -  2.0000KB  1.2682GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27359s  1.6000us                    -               -         -         -         -  2.0000KB  1.1921GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27437s  360.12us                    -               -         -         -         -  4.5000MB  12.203GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27479s  1.5360us                    -               -         -         -         -  2.0000KB  1.2418GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27484s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27487s  1.2480us                    -               -         -         -         -  2.0000KB  1.5283GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27491s  1.3120us                    -               -         -         -         -  2.0000KB  1.4538GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27495s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27651s  719.02us                    -               -         -         -         -  9.0000MB  12.224GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27732s  1.5040us                    -               -         -         -         -  2.0000KB  1.2682GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27736s  1.3440us                    -               -         -         -         -  2.0000KB  1.4192GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27890s  719.02us                    -               -         -         -         -  9.0000MB  12.224GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27970s  1.5350us                    -               -         -         -         -  2.0000KB  1.2426GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.27976s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28131s  719.09us                    -               -         -         -         -  9.0000MB  12.223GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28212s  1.5360us                    -               -         -         -         -  2.0000KB  1.2418GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28222s  1.2480us                    -               -         -         -         -  2.0000KB  1.5283GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28377s  718.06us                    -               -         -         -         -  9.0000MB  12.240GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28463s  1.6320us                    -               -         -         -         -  2.0000KB  1.1687GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28468s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28619s  719.86us                    -               -         -         -         -  9.0000MB  12.209GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28703s  1.5040us                    -               -         -         -         -  2.0000KB  1.2682GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28707s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28863s  719.28us                    -               -         -         -         -  9.0000MB  12.219GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28946s  1.5040us                    -               -         -         -         -  2.0000KB  1.2682GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28951s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28955s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28960s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.28964s  1.2800us                    -               -         -         -         -  2.0000KB  1.4901GB/s  GeForce GTX 107         1        14  [CUDA memcpy HtoD]\r\n1.29034s  20.768us                    -               -         -         -         -  1.5000MB  70.534GB/s  GeForce GTX 107         1        14  [CUDA memcpy DtoD]\r\n1.29047s  30.015us             (16 1 1)      (1024 1 1)        25        0B        0B         -           -  GeForce GTX 107         1        13  void tensorflow::functor::SwapDimension1And2InTensor3<float>(int, float const *, tensorflow::functor::Dimension<int=3>, tensorflow::functor::SwapDimension1And2InTensor3<float>*) [936]\r\n1.29050s  3.7120us              (2 1 1)      (1024 1 1)        27        0B        0B         -           -  GeForce GTX 107         1        13  void tensorflow::functor::SwapDimension0And2InTensor3<float>(int, float const *, tensorflow::functor::Dimension<int=3>, tensorflow::functor::SwapDimension0And2InTensor3<float>*) [942]\r\n1.59580s     864ns                    -               -         -         -         -      112B  123.62MB/s  GeForce GTX 107         1         7  [CUDA memcpy HtoD]\r\n<computations start>\r\n```\r\n", "comments": ["Thanks for your clear and detailed issue report.\r\n\r\nYour analysis is correct.  GPU tensors are always fed by establishing a CPU tensor and copying its value to the GPU.  In the other direction, when reading the output of a graph computation, if the value you want to read is produced on the GPU, it must be copied back to the CPU before producing I/O e.g. writing to a file or window.\r\n\r\nThe basic reason is that the GPU is simply an auxillary processor with very limited capability to engage in  I/O operations.   It is not feasible to e.g. read from local disk directly into GPU memory without first staging through CPU memory.  In principle this should not pose a performance problem, so long as you're able to to buffer input into CPU RAM ahead of it's being needed on the GPU.\r\n\r\nIf you want to combine a TF program with a non-TF program that both run on GPU, without copying data back and forth to CPU RAM, I think you will need to arrange for input/output values to be in Vars that are local to the GPU, and somehow make them accessible to the non-TF program.  Alternatively, maybe you can package your non-TF program as a single TF Op.\r\n\r\n\r\n\r\n\r\n", "qq", "Thanks @poxvoculi for your quick reply and detailed explanation.\r\n\r\nI understand that reading from/to disk etc. into GPU memory without passing through CPU memory is not feasible. However, that is not what I am trying to do. I am trying to combine a TF progam with a non-TF program that both run on GPU, as you mention.\r\n\r\nYour last suggestion (using Vars) seems feasible, but it also seems like a bit of a workaround. Especially in high-performance environments, I can imagine that I would not be the only one using this functionality.\r\n\r\nThe fact that data is copied to CPU memory and then back to GPU memory when running a graph with an input tensor on GPU memory, suggests to me that TF is detecting that this tensor is on GPU memory.\r\nSo wouldn't it be easier to directly work from an input tensor if it is on GPU memory, and transfer it CPU -> GPU if it is on CPU memory?\r\nSimilarly there could be an option I think to specify whether output tensors should be copied back to CPU memory or not.\r\nThis would give the flexibility needed I think.\r\n\r\nThanks and looking forward to hear your thoughts.", "OK, if your real question is \"How do I get a GPU-using TensorFlow program to cooperate effectively with a non-TF GPU program?\" that's a whole collection of difficult issues.  \r\n\r\nThe short, easy answer is to pipeline them, with the inputs/output staged through CPU RAM, as it sounds like you're doing.  Anything beyond that gets into tricky stuff that I probably shouldn't even suggest. \r\n\r\nJust for illustrative purposes,  here are some thoughts about some of the problems.\r\n\r\nSuppose you're willing to invoke the programs as separate processes, and just want to avoid the CPU RAM staging of values passed between them.  TensorFlow allocates all GPU RAM by default, because that allows us to do memory management more efficiently than by using cudaMalloc.  Via an option one can request that TF only allocate part of the total.  TF can only apply operations to Tensor typed memory regions that it has allocated.  It should be possible to declare a Var to be resident to a GPU, which provides a long-term storage with a static location, but there's no way to specify an address at which that Var should allocate.  Once the Var has been established, a TF graph program can read and store its value.  With considerably hackery on your part, it might be possible to modify the TF runtime to capture the GPU RAM address of an allocated Var in which you're interested, and make it available to another process.  While the TF program is still live, holding the allocated memory, but idle, it *might* be possible to start another program which uses the GPU and can read/write a location it hasn't allocated itself via pointer.  (GPUs are not (yet) virtualizable, so I think a second process will see the same memory contents and address space as the first.)  So this might point to a crude way of alternating the actions of two separate GPU-using programs on some shared memory, so long as only one of them is a TF program.\r\n\r\nAlternatively, you might want to call a non-TF program in the middle of a TF graph, e.g. by wrapping it in a new Op.  This could work, if you're willing and able to rewrite that non-TF program to obey the TF GPU runtime assumptions about use of stream executor contexts, memory allocators, etc. which are pretty non-standard in the CUDA world.  Simply trying to link and call a pre-existing GPU utility is unlikely to work.\r\n\r\nHope this clarifies things.\r\n", "Thank you again for your quick and detailed reply.\r\n\r\nThat is exactly the question. My problem does not relate to calling a non-TF program in the middle of a TF graph.\r\n\r\nLet's say program A runs on the GPU (non-TF) and I want to feed its output (which is in GPU memory) into a TF network (program B, which uses the TF C++ API).\r\nI understand that the easiest way is to do: copy output of A back to CPU memory -> run program B. TF will then internally copy the data to GPU memory and start running the graph.\r\n\r\nNow, I'm looking for a way to bypass the process of staging it through CPU RAM, because from a performance point of view this seems to be unnecessary. It would be more efficient if we could do a memory copy from GPU memory to GPU memory, i.e. copying the output of program A to the allocated memory for the input tensor of program B, both living on GPU memory. \r\n\r\nAs you mention there are some problems there.\r\nI understand that TensorFlow cannot operate on memory that it didn't allocate, but I am using a TF allocator:\r\n```cpp\r\ntensorflow::GPUBFCAllocator* allocator = new tensorflow::GPUBFCAllocator(0, sizeof(float) * height * width * 3);\r\ntensorflow::Tensor input_tensor = tensorflow::Tensor(allocator, tensorflow::DataType::DT_FLOAT, tensorflow::TensorShape( { 1, height, width, 3 }));\r\n<copy output data from program A into the GPU memory allocated by input_tensor using a GPU->GPU copy>\r\n```\r\nSo TF allocated a Tensor on GPU memory, holding our input data (i.e. the output of program A), and I do not see why TF will now still insist on staging this through CPU RAM. All of this we can do at runtime.\r\n\r\nI think it would be a good feature addition to skip staging through CPU RAM if the input tensor has its memory allocated on the GPU using a TF GPU allocator.\r\nSimilarly, there could be a flag to keep output tensors on GPU memory and not stage them back to CPU RAM. It would then be the users responsibility of course to deal with this, but it will allow users to not waste any time copying back and forth to CPU RAM in performance-critical environments.\r\n\r\nThe solution using Vars could work I think, but I think it is a bit of a workaround and making this a feature could be useful to more users who are trying to optimize setups where TF is getting input/output from other programs operating with GPU memory.\r\n\r\nWhat are your thoughts on this?", "The session->Run() function computes the the subgraph closure induced by the specified outputs, in other words, it computes forward from consts, Vars or fed Nodes only along paths that lead to the requested outputs.   Given this situation, if you already have figured out how to copy the values computed by A into a TF allocated tensor, why not copy them into a GPU resident Var, instead of into some other kind of tensor, and skip the explicit feed argument to Run()?  If you're working in C++, you can use DMAHelper to get the address of the backing buffer for the memcpy.\r\n\r\nI agree it could be handy to have Session::Run() recognize that a feed tensor may already be GPU resident.  I guess that whoever wrote that code assumed it would never happen, so it could take a while to discover everything that needs to be fixed.", "Yes, using Vars would indeed be a solution for now, but I still think (as you mention) that support for using feed tensors for this would be welcome.\r\nDo you think this could be flagged as 'enhancement'?\r\n\r\nThank you for the pointer to DMAHelper.", "Is this support being considered for a future release? This could help improve performance for tasks that need to eke out every bit of performance.", "@asimshankar Is the c-api going to support auto-recognition of GPU-resident tensors supplied to Session::Run()?", "A proper implementation would likely involve some interface changes as well (perhaps the `Tensor` object in C++ should also point to the device/allocator that backs its `TensorBuffer`). There is nobody actively working on this at this time. If anyone would like to contribute a change, we'd be welcoming of that (perhaps make sense to discuss the plan before sending a PR though).\r\n\r\nA hacky workaround might be to create dummy input/output operations that avoid the copying. For example, the `Tensor` object provided to `Session::Run` might just contain enough metadata for the kernel to materialize a `Tensor` object referencing the GPU memory of interest.", "larsmennen did solved this issue? i have the same problem \r\n", "I would also be interested in knowing how to solve this issue. From my understanding, tensorflow::ops::Variable does not expose the tensor so we can memcpy into it. Is there any other way to get the cuda pointer from the Variable?", "Same problem here, I'm working with video in GPU memory and for me copying data to CPU and back to GPU again is a problem, I'm working in realtime and copies kill performance. Is there any update on this issue?", "Update: We do have an experimental means of feeding/fetching the GPU memory of Tensors, added in commit a1d6179adb1ca6208281ed955860c319525edf75\r\n\r\nThis is subject to change, but something y'all may want to try.\r\nSee the unittest https://github.com/tensorflow/tensorflow/blob/f3ed7f7e836da4f0ca1cb04cadce938744932b72/tensorflow/core/common_runtime/direct_session_test.cc#L1897\r\nfor an example.\r\n\r\nThis is all in the runtime implementation, there are no current plans to surface this into higher level APIs (or Python) at this time.", "+1 on this feature, especially for python.\r\n\r\nLet me give a definitive use case, for same data different parameter / architecture jobs, we have the same data-set being used to train and evaluate over and over again. If we have 100 different combinations to try, this means we will copy from disk -> cpu mem -> gpu mem 100 times. ALL nvidia documentation suggests limiting data transfer between device and host as the primary optimisation.\r\n\r\nCurrently TF is a blocker for us using in certain production use cases because of this.", "+1 too, need to feed from GPU memory", "Tried @asimshankar 's suggested experimental code, it works, thank you! Any time frame for when this will become an API not likely to change at any moment?"]}, {"number": 5901, "title": "Im2txt: \"No variables to save\" error at the initial training stage", "body": "Hi,\r\n\r\nI'm trying to train im2txt (show & tell) model with mscoco dataset using tensorflow-0.11.\r\n\r\nI successfully completed the downloaded & preprocess steps described in the guide (https://github.com/tensorflow/models/tree/master/im2txt).\r\n\r\nI'm using the inception_v3.ckpt model for the initial training process.\r\n\r\nBut I'm getting \"no variables to save\" error @ show_and_tell_model.py in function setup_inception_initializer at line \r\n saver = tf.train.Saver(self.inception_variables)\r\nI print out self.inception_variables on the line above and I see that it is an empty list.\r\n\r\nBelow is the complete dump of the error.\r\n\r\nAny help would be appreciated.\r\n\r\nThanks.\r\n\r\npython train.py \\\r\n\t--input_file_pattern=\"/usr/local/tensorflow-models/im2txt/im2txt/data/mscoco/train-?????-of-00256\" \\\r\n\t--inception_checkpoint_file=\"/usr/local/tensorflow-models/im2txt/im2txt/data/inception_v3.ckpt\" \\\r\n\t--train_dir=\"/usr/local/tensorflow-models/im2txt/im2txt/model/train\" \\\r\n\t--train_inception=false \\\r\n\t--number_of_steps=1000000\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so.5.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so.8.0 locally\r\nINFO:tensorflow:Prefetching values from 256 files matching /usr/local/tensorflow-models/im2txt/im2txt/data/mscoco/train-?????-of-00256\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 117, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 43, in run\r\n    sys.exit(main(sys.argv[:1] + flags_passthrough))\r\n  File \"train.py\", line 67, in main\r\n    model.build()\r\n  File \"/usr/local/tensorflow-models/im2txt/im2txt/show_and_tell_model.py\", line 361, in build\r\n    self.setup_inception_initializer()\r\n  File \"/usr/local/tensorflow-models/im2txt/im2txt/show_and_tell_model.py\", line 335, in setup_inception_initializer\r\n    saver = tf.train.Saver(self.inception_variables)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1021, in build\r\n    raise ValueError(\"No variables to save\")\r\nValueError: No variables to save\r\n\r\n", "comments": ["@ilblackdragon Looks like there was a recent change to show_and_tell_model.py that renamed tf.GraphKeys.VARIABLES to tf.GraphKeys.GLOBAL_VARIABLES.  Could that be causing this problem?", "Thank you Paul!\r\n\r\nI changed the statement tf.GraphKeys.VARIABLES to tf.GraphKeys.GLOBAL_VARIABLES in function build_image_embeddings as well as in function setup_global_step. The training process started without any other problem.\r\n\r\nThank you for your help. :)\r\n\r\nBest,\r\nSoner"]}, {"number": 5900, "title": "Fix sandboxed build", "body": "There are a number of missing dependency declarations, which break the sandboxed bazel build. This PR adds a few, hopefully all.", "comments": ["@jart, I think the proto rules are easier to use this way, but what I did may be unidiomatic. If there's a better way let me know.", "Thanks! The GPU failure looks unrelated. ", "Sorry @danmane in advance for the pain you'll have syncing this."]}, {"number": 5899, "title": "Make placement of constants follow consumers if they are all on the same device", "body": "A generator node (for instant a constant) will now go to the same device as its consumers, when those consumers are all on the same device.   Previously a constant would only follow its consumer when there was only a single consumer.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I am in the process of getting the CLA signed and set up.", "I signed it!", "The company's name is Graphcore.   Signed yesterday.  ", "So I can see GraphCore is in our corp CLA list, and you've signed your commits with your company email.  Can you make sure your email is part of the googlegroup registered with us when you sign up for the corp CLA? That way our bot will come back happy.", "Is the problem that the google group contains davidjpnorman@gmail.com, and not davidjpnorman@googlemail.com (my GitHub email)?   I have tried adding davidjpnorman@googlemail.com, but googlegroups won't have it, recognizing that one is an alias of the other.\r\n\r\nI think I will transfer ownership of the group, remove davidjpnorman@gmail.com, and add davidjpnorman@googlemail.com, and see if that works.", "Hmm.  It seems that the email address is converted from @googlemail.com into @gmail.com by the google groups system.\r\n\r\nMuch as I hate to do it, I will see if I can alter the email address in the github account.\r\n", "I signed it!\r\n", "ok - i changed the GitHub email address to davidjpnorman@gmail.com - hopefully that will be ok\r\n", "So I think the following need to match:\r\n\r\n1) git commit email on all your commits\r\n2) email must be added to your github account\r\n3) email must be registered on the internal list your company has to associate emails with the corp CLA account.\r\n\r\nIf those don't match, our CLA bot can't connect all of the pieces together.  Can you give this one more try?  If it doesn't work, I'll rope in one of the CLA people at Google to help :)", "Ok.\r\n\r\nso perhaps the problem is that the original commit comes from davidn@graphcore.ai, while the merge request is by the github account (now davidjpnorman@gmail.com).  \r\n\r\nalthough both of these addresses are in the google group, perhaps they need to be the same.  Can you confirm?\r\n", "Or - i guess that I might have to redo the merge request, because maybe it is tagged with the previous github account email address (davidjpnorman@googlemail.com), rather than the new one.   I'll investigate that.", "Ok - i'll redo this whole request to see what happens.", "I tried creating another:  https://github.com/tensorflow/tensorflow/pull/6615\r\n\r\nhowever it didn't work either.  I will look through the pages that it is referring to in the error message"]}, {"number": 5898, "title": "pool efficiency on cpu", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI have searched about cpu usage and try everything I can. But it's still pool efficiency usage, **only use 30% of my cpu**\r\nI thought it maybe the train-dataset read speed limitation?\r\n\r\n### Environment info\r\nOperating System:\r\ncentos6.5 with latest version of tensorflow \r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nfull code here\r\nhttps://github.com/tobegit3hub/deep_recommend_system/blob/master/a8a_classifier.py\r\n```\r\n# Read TFRecords files for training\r\nfilename_queue = tf.train.string_input_producer(\r\n    tf.train.match_filenames_once(\"data/a8a_train.libsvm.tfrecords\"),\r\n    num_epochs=epoch_number)\r\nserialized_example = read_and_decode(filename_queue)\r\nbatch_serialized_example = tf.train.shuffle_batch(\r\n    [serialized_example],\r\n    batch_size=batch_size,\r\n    num_threads=thread_number,\r\n    capacity=capacity,\r\n    min_after_dequeue=min_after_dequeue)\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\ntry the specify the thread number when init session , the shuffle_batch num_threads etc.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nhere is a tensorboard screenshot\r\n\r\n<img width=\"750\" alt=\"screen shot 2016-11-28 at 4 15 15 pm\" src=\"https://cloud.githubusercontent.com/assets/918889/20661115/0014e198-b588-11e6-8c33-b692c32568cd.png\">\r\n\r\n", "comments": ["I don't understand what problem you're trying to describe.  I'm guessing that you have some performance measurement claiming that your CPU is no more than 30% utilized, and you'd like to figure out how to speed up your program by raising that utilization rate.  \r\n\r\nThis forum is for bug reports and feature requests.  For general model performance questions stackoverflow is a more appropriate forum.  If I guessed correctly about  your problem, please post it there."]}, {"number": 5897, "title": "Fix params docstring of embedding_lookup", "body": "The first parameter of embedding_lookup actually accept a single tensor.\r\n```py\r\n  if not isinstance(params, list):\r\n    params = [params]\r\n```\r\nHowever, in the docstring it requires a list which is confusing.\r\nThis PR fix and unify the doc.", "comments": ["Can one of the admins verify this patch?", "Thanks!"]}, {"number": 5896, "title": "VocabularyProcessor does not work with Chinese", "body": "In order to train a model with tensorflow, try to use tf.contrib.learn.preprocessing.VocabularyProcessor to get a vocabulary vector. \r\n\r\n```\r\ndef create_vocab(input_iter, min_frequency):\r\n  \"\"\"\r\n  Creates and returns a VocabularyProcessor object with the vocabulary\r\n  for the input iterator.\r\n  \"\"\"\r\n  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\r\n      FLAGS.max_sentence_len,\r\n      min_frequency=min_frequency,\r\n      tokenizer_fn=tokenizer_fn)\r\n  vocab_processor.fit(input_iter)\r\n  return vocab_processor\r\n\r\n```\r\n\r\nIt works well with English. But all the Chinese words are not parsed. They are treated as a single word.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/3538629/20660678/cc49a12a-b585-11e6-9018-8f8dc0d628b3.png)\r\n\r\n\r\n### Environment info\r\nOperating System: Mac OSX, CPU Only\r\nTensorFlow: Version - **0.11.0rc1**\r\n\r\n### What other attempted solutions have you tried?\r\nNot yet. \r\n", "comments": ["Get a work around http://acepor.github.io/2015/12/17/General-Pipelines/\r\n```\r\nbash -x segment.sh ctb test.csv UTF-8 0 > test.segmenter.csv\r\n```\r\n\r\nThen, Chinese words are split with space.\r\n", "Maybe this might help too:\r\n\r\nhttps://github.com/tensorflow/models/blob/master/syntaxnet/syntaxnet/models/parsey_universal/tokenize_zh.sh", "@poxvoculi Thanks, I have found several solutions to process Chinese, there are a lot tools.\r\nhttp://eng.snaplingo.net/use-stanford-word-segmenter-to-process-chinese-data/"]}, {"number": 5895, "title": "Cannot import tensorflow using simple Pip installation instructions", "body": "This is the error im getting. I saw the other posts for conda and windows computers, but I have a mac and im not using conda...\r\n\r\n$ python\r\nPython 2.7.10 (default, Jul 30 2016, 18:31:42) \r\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\ndyld: warning, LC_RPATH $ORIGIN/../../_solib_darwin/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Slib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\r\ndyld: warning, LC_RPATH ../local_config_cuda/cuda/lib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\r\ndyld: warning, LC_RPATH ../local_config_cuda/cuda/extras/CUPTI/lib in /Library/Python/2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so being ignored in restricted program because it is a relative path\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/__init__.py\", line 23, in <module>\r\n    from tensorflow.python import *\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 21, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Library/Python/2.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow')\r\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/importlib/__init__.py\", line 37, in import_module\r\n    __import__(name)\r\nImportError: No module named _pywrap_tensorflow\r\n>>> exit()\r\n", "comments": ["How did you install tensorflow? \r\n\r\nIf you used pip on a Mac, you very likely want to install tensorflow into a virtualenv. MacOS is very protective of its system directories, so pip doesn't always work properly even if used as root.", "Yes, I only used pip on a Mac. Should I uninstall anything before trying installing tensorflow with virtualenv then? Also it seems weird they would have that instruction set for pip on mac if it didnt work...", "It is at least somewhat confusing, since usually the failure more is that tensorflow doesn't install properly due to permission problems. \r\n\r\nYou don't have to uninstall anything, just make a virtualenv and install tensorflow in there.", "Hi, try to import _pywrap_tensorflow.so in  [PYTHON_INSTALL_DIR]/site-packages/tensorflow/python/_pywrap_tensorflow.so this most of the time provides more specific on why it is not loading. Most of the time this happens is because its not able to link to a dynamic library somewhere in your environment, or a minor version changed from a dynamic lib. After that you can take it from there."]}, {"number": 5894, "title": "Does the random regression forest works with multi-variable in labels?", "body": "For example, each label is a vector of floats.\r\n", "comments": ["This appears to be a usage question, rather than a bug report or feature request, hence it is better asked on stackoverflow."]}, {"number": 5893, "title": "Update installation instructions for gpu binaries.", "body": "", "comments": []}, {"number": 5892, "title": "Huge network traffic between parameter server and worker", "body": "hi, we found huge network traffic between parameter server and worker during distributed dnn training. our model has one embedding layer with 40M sparse features, those features are spitted into 20 groups and each group full-connected to one 5-dimension vector, so for sparse part, total parameter count is 200M. we used tf.nn.embedding_lookup_sparse to retrieve sparse parameters. however, with one parameter server one worker, parameter will send about 700M to worker for one batch, this traffic is very close to whole parameter size.\r\nhere is our code for sparse part:\r\n\r\n```\r\nW_embeddings = []\r\nfor i in range(1, FLAGS.feature_group_count):\r\n    W_embeddings.append(tf.get_variable(\"W_embedding_%d\" % (i-1), [feature_dimensions[i], FLAGS.embedding_count], partitioner=tf.min_max_variable_partitioner(ps_count)))\r\n\r\nembedding_tensors = []\r\nfor i in range(len(W_embeddings)):\r\n    embedding_tensors.append(tf.nn.embedding_lookup_sparse(W_embeddings[i],sp_ids=feature_groups[i+1],sp_weights=None,combiner=\"sum\"))\r\n```\r\n\r\npython version is Ubuntu 3.4.3, tensorflow 0.11.0 is downloaded following official site using pip3.\r\nI searched and found no one reported this kind of issue before, i am not sure whether we are using tensorflow incorrectly or something wrong with our environment, any idea about how to fix this?\r\n", "comments": ["i reinstalled tensorflow from source code at master branch, then this issue is gone! i think python3.4 pip package is something wrong.", "however, after installing latest version, my another script can't run, here is the error message:\r\n```\r\n  File \"lrnn_distTrain_feeding.py\", line 290, in main                                                                                                                                                                               [99/1919]\r\n    train_dist()\r\n  File \"lrnn_distTrain_feeding.py\", line 237, in train_dist\r\n    with sv.prepare_or_wait_for_session(master=server.target) as sess:\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/supervisor.py\", line 720, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/session_manager.py\", line 245, in prepare_session\r\n    is_ready, msg = self._model_ready(sess)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/session_manager.py\", line 435, in _model_ready\r\n    return self._ready(self._ready_op, sess, \"Model not ready\")\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/training/session_manager.py\", line 404, in _ready\r\n    ready_value = sess.run(op)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'Tshape' not in Op<name=Reshape; signature=tensor:T, shape:int32 -> output:T; attr=T:type>; NodeDef: report_uninitialized_variables/boolean_mask/Reshape\r\n= Reshape[T=DT_STRING, Tshape=DT_INT32, _device=\"/job:ps/replica:0/task:0/cpu:0\"](report_uninitialized_variables/Const, report_uninitialized_variables/boolean_mask/concat)\r\n```\r\n\r\nseems that tensorflow is accessing Tshape attr, what exactly tensorflow is doing? how to fix this?", "core/ops/ops.pbtxt defines some expected attributes for the Reshape Op, which includes one called \"Tshape\".  When the Op executes, it expects to find the desired output shape in its corresponding graph Node, but it's not there.  It appears that the graph has been constructed without it, or somehow it got stripped prior to execution. Perhaps with this hint you can figure out what's going wrong.", "@fesun The reason of huge network traffic is in r0.11 embedding_lookup_sparse does not support PartitionedVariable correctly. For details, you can read this: http://stackoverflow.com/questions/40628977/how-to-scale-tf-nn-embedding-lookup-sparse", "@poxvoculi @llhe thanks for your explanations, problem solved now.\r\n"]}, {"number": 5891, "title": "Typo Fix (intialized to initialized)", "body": "Just a basic typo fix I stumbled upon.", "comments": ["Can one of the admins verify this patch?", "@mvpatel2000, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @tensorflower-gardener and @danmane to be potential reviewers."]}, {"number": 5890, "title": "catch22 situation with tf.nn.sampled_softmax_loss and tf.nn.softmax", "body": "Hi,\r\nreferring to this: \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard1/tf.nn.sampled_softmax_loss.md\r\nand this:\r\nhttps://github.com/tensorflow/tensorflow/issues/4138\r\n\r\nto make samlped_softmax efficient we need to invert w and w_t to take transpose out of the loss function.\r\nbut then it becomes an issue at test / inference time when you need to use the full softmax with the tranpose inside.\r\n\r\nie: either training is slow or testing is slow.\r\n\r\nwhat's the solution ?", "comments": ["From https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5889, "title": "Variable initialization method changed in tensorflow 0.11.0", "body": "otherwise, variables not initialized", "comments": ["Can one of the admins verify this patch?", "@tangshuran, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ilblackdragon, @vincentvanhoucke and @vrv to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tangshuran It is similar to the issue #5874, `tf.global_variables_initializer()` is the new API for TF, and `initialize_all_variables` probably to be deprecated soon. So I think it is better to keep the new API. \r\n\r\nBy the way, if you would like to use the new API, you can install tensorflow via nightly binary. ", "Thanks~I thought it was old~"]}, {"number": 5888, "title": "No gradient defined for operation 'Round' (op type: Round)", "body": "1. A link to the pip package you installed:\r\nhttps://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=gpu-linux/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow_gpu-0.11.0-cp27-cp27mu-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.head\r\n\r\n\r\n`tf.round` used to have gradients. but now it gives error.\r\n```python\r\na = tf.get_variable('a', shape=[1])\r\nx = tf.round(a)\r\ng = tf.gradients(x, a)\r\n```", "comments": ["Looks like `tf.round` switched from using `Floor` to `Round` recently.\r\n@itsmeolivia  Any idea why the gradient might be missing?", "Seems to be fixed already.", "I'm still having this issue on windows.", "are you on the latest version of TF?", "I'm on 1.0.0 Build #88 GPU Windows", "This is still not working. No gradient if you use tf.round @ppwwyyxx", "Works for me.", "Hi,\r\n\r\nHow do you check if a function has a gradient or not in TensorFlow?\r\nI have a few lines of code and I'm getting the same error. How can I find the line causing the problem?\r\n\r\nSincerely\r\n", "This appears to be broken. Is there a workaround? I can reproduce it with the following code in v1.3.0 running through Docker on Linux.\r\n```\r\nimport tensorflow as tf\r\na = tf.get_variable('a', shape=[1])\r\nx = tf.round(a)\r\ng = tf.gradients(x, a)\r\nprint 'gradient =', g\r\nprint 'version =', tf.__version__\r\n```\r\nOutput:\r\n```\r\ngradient = [None]\r\nversion = 1.3.0\r\n```", "@tolsonMDA \"None\" is exactly what the gradient is supposed to be.", "@ppwwyyxx Thanks for taking a look. Can you elaborate? I have the same problem using any of `tf.round`, `tf.floor`, `tf.ceil`, and `tf.cast(a, tf.int32)`, but the gradient is fine (i.e. not `None`) when I cast it to a floating type. Why shouldn't it work when casting it to an integer?", "\"None\" means the gradient is zero. And the gradient is [indeed zero](http://www.wolframalpha.com/input/?i=derivative+round(x)).\r\nSame for others.", "Ah, guess I'm being dense today. That makes sense. Thanks!"]}, {"number": 5887, "title": "It's not possible to read csv without TextLineReader?", "body": "`def input_data(start_index, amount, shuffle):\r\n    \r\n    data_folder = '/media/sf_vb-shared/data/'\r\n    range_queue = tf.train.range_input_producer(amount, shuffle = shuffle)\r\n    range_value = range_queue.dequeue()\r\n\r\n    abs_index = tf.add(range_value, tf.constant(start_index))    \r\n    abs_index_str = tf.as_string(abs_index, width = 9, fill = '0')\r\n    \r\n    png_file_name = tf.string_join([tf.constant(data_folder), tf.constant('data'), abs_index_str, tf.constant('.png')])\r\n    csv_file_name = tf.string_join([tf.constant(data_folder), tf.constant('data'), abs_index_str, tf.constant('.csv')])\r\n\r\n    csv_data = tf.read_file(csv_file_name)\r\n    csv_data = tf.Print(csv_data, [csv_data], message = \"This is csv_data: \")\r\n    label_defaults = [[0.0] for x in range(n_classes)]   \r\n    unpacked_labels = tf.decode_csv(csv_data, record_defaults = label_defaults)\r\n    labels = tf.pack(unpacked_labels)\r\n    labels = tf.Print(labels, [labels], message = \"These are labels: \")`\r\n\r\nThe output is \"This is csv_data: [0,0,0,1,0\\n]\", so\r\ncsv_data is read ok, but \"These are labels\" never gets printed out...\r\nIs it possible to feed decode_csv with read_file output?", "comments": ["https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5886, "title": "Crash with dynamic tf.reshape \"Check failed: dtype() == expected_dtype (9 vs. 3)\"", "body": "```py\r\nimport tensorflow as tf\r\n\r\nqueue = tf.train.string_input_producer(['data.tfrecord'])\r\n\r\nreader = tf.TFRecordReader()\r\n_, example = reader.read(queue)\r\n\r\nfeatures = tf.parse_single_example(example,features={\r\n    'mr_image': tf.FixedLenFeature([], tf.string),\r\n    'us_image': tf.FixedLenFeature([], tf.string),\r\n    'mr_shape': tf.FixedLenFeature([2], tf.int64),\r\n    'us_shape': tf.FixedLenFeature([2], tf.int64),\r\n})\r\n\r\nmr_image = tf.decode_raw(features['mr_image'], tf.int16)\r\nus_image = tf.decode_raw(features['us_image'], tf.uint8)\r\n\r\nmr_shape = tf.cast(features['mr_shape'], tf.int64)\r\nus_shape = tf.cast(features['us_shape'], tf.int64)\r\n\r\nwith tf.Session() as sess:\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n    \r\n    #mr = tf.reshape(mr_image, mr_shape)\r\n    #us = tf.reshape(us_image, us_shape)\r\n\r\n    print(len(sess.run(mr_image)),len(sess.run(us_image)))\r\n    print(sess.run([mr_shape, us_shape]))\r\n    #print(sess.run(tf.reshape(mr_image, mr_shape)))\r\n    \r\n    coord.request_stop()\r\n    coord.join(threads)\r\n```\r\n\r\nWill print:\r\n```\r\n183604 118218\r\n[array([466, 394]), array([366, 323])]\r\n```\r\n\r\nHowever `print(sess.run(tf.reshape(mr_image, mr_shape)))` will cause a crash with:\r\n```sh\r\nF tensorflow/core/framework/tensor.cc:441] Check failed: dtype() == expected_dtype (9 vs. 3)\r\n```\r\n\r\nAlso `print(sess.run(tf.reshape(mr_image, [466, 394])))` works as expected.\r\n\r\n- macOS 10.12.1\r\n- Python 3.5.2\r\n- tensorflow 0.11", "comments": ["Thanks for reporting the issue. It looks from your report that this is a runtime failure, rather than a Python exception.\r\n\r\nThe docs for `tf.reshape` state that the new shape is allowed to be int32 or int64:\r\nhttps://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#reshape\r\nand the op is correctly registered:\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/core/ops/array_ops.cc#L1155\r\n\r\nHowever, unless I'm missing something it looks like the implementation of the OpKernel assumes that the `sizes` are `int32`:\r\nhttps://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/core/kernels/reshape_op.h#L48\r\n\r\nThis seems to have been the case for a *long* time!  \r\n\r\nAs a workaround, you could simple use int32 shapes, but either the documentation or implementation should probably be fixed.\r\n", "@prb12 Yes, correct if I change to:\r\n\r\n```py\r\nmr_shape = tf.cast(features['mr_shape'], tf.int32)\r\nus_shape = tf.cast(features['us_shape'], tf.int32)\r\n```\r\n\r\nit works!\r\n\r\nIf it makes sense I could add a type check to `tf.reshape` would just require some details where to place it best.", "The best thing to do is add a \r\n```\r\nTypeConstraint<int32>(\"Tshape\")\r\n```\r\n to all of the kernel regisgtrations here: https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/core/kernels/reshape_op.cc#L21\r\n\r\nsince right now the implementation assumes int32 for the Tshape input.  Do you want to make that change?", "This seems to be already the case in [later releases](https://github.com/tensorflow/tensorflow/blob/c8a45a8e236776bed1d14fd71f3b6755bd63cc58/tensorflow/core/kernels/reshape_op.cc). Should I still add this change to the `r0.11` branch?\r\n\r\nAre there any tests for this?", "Oh hm, I *thought* I fixed that weeks ago too, glad to hear I didn't miss this one.  We don't have a lot of 'not implemented' tests, which is what would be needed to test this case -- one could register the kernels for Tshape<int64> if desired, we just haven't gotten around to adding them + tests.\r\n\r\nWe haven't backported a lot of changes into 0.x releases, but I suspect we'll do so once we hit 1.0.", "@prb12 Thanks.", "Potentially related issue: https://github.com/tensorflow/tensorflow/issues/9836"]}, {"number": 5885, "title": "decode_csv result shape", "body": "", "comments": []}, {"number": 5884, "title": "Tensor slice is too large to serialize", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI have read the realted isssue  https://github.com/tensorflow/tensorflow/issues/4291 , but it's not solved.\r\n\r\n### Environment info\r\ncentos with cpu support and tensorflow latest version\r\n\r\nno cuda or cudnn\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI am build a 4 layer network (128*64*64*32), input_unit feature size 20000000\uff08sparse feature size\uff09,  when train the model ,it errors that \"Tensor slice is too large to serialize\".\r\n\r\n\r\nmy training code is like this https://github.com/tobegit3hub/deep_recommend_system/blob/master/a8a_classifier.py\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nnothing , it's seems that a protobuf limitation , but I want to know how to solve this with changes in my training code \r\n\r\n### Logs or other output that would be helpful\r\n\"Tensor slice is too large to serialize\"\r\n\r\n\r\n", "comments": ["Please can you provide the information requested in the issues reporting template.", "@prb12  done ", "In the related thread you cite, there is a suggestion from @concretevitamin:\r\n\" a quick workaround is to use, for variables, tf.get_variable(..., partitioner=..), and for non-ref tensors, tf.split(). \"\r\n\r\nIs this not feasible for your model?", "+1 to @poxvoculi.  Also, if you use the unofficial 0.12 branch (the official release is coming soon), the Saver and the default checkpoint format have been updated to handle this case.", "thanks @poxvoculi , I tried ,but cannot find any documents about the partitioner usage.", " @concretevitamin  with 0.12 , it's support large network parameter?  without any modification of my code?   I'll try this version :) ", "@concretevitamin  thanks, it's worked in 0.12rc ", "@ericyue Great to know it's working now!  (Re your previous question: `tf.get_variable()` has a `partitioner` field.)"]}, {"number": 5883, "title": "Android TF Error: Not found: Op type not registered 'Const'", "body": "Hello,\r\n\r\nI'm trying to run a dummy inference model I built in TF with python.\r\nThe model takes an input vector (size 3x200) and multiply (component-wise) it with a variable vector of the same size whose values were randomized. so output_node = input_node * variable_node\r\nThen there is a reduce_sum to a final value.\r\nThe model was generated with this procedure:\r\n`  \r\n    checkpoint_prefix = \"mygraph.ckpt\"\r\n    checkpoint_state_name = \"mygraph.ckpt\"\r\n    input_graph_name = \"input_graph.pb\"\r\n    output_graph_name = \"output_graph.pb\"\r\n\r\n    def dummy_model(input):\r\n        with tf.Graph().as_default():\r\n            sess = tf.Session()\r\n            with sess.as_default():\r\n\r\n            variable_node = tf.Variable(tf.random_normal([3, 200], stddev=0.35), name=\"variable_node\")\r\n            input_node = tf.Variable(input, name=\"input_node\")\r\n            output_node = tf.reduce_sum(tf.mul(variable_node, input_node, name=\"output_node\"), axis=1)\r\n            init_op = tf.global_variables_initializer()\r\n\r\n            #init = tf.global_variables_initializer()\r\n            sess.run(init_op)\r\n \r\n            saver = tf.train.Saver(write_version=2)\r\n            checkpoint_path = saver.save(sess, checkpoint_prefix, global_step=0, latest_filename=checkpoint_state_name)\r\n            tf.train.write_graph(sess.graph, \"./\", input_graph_name)`\r\n   \r\n\r\nAfter generating the model files (and renaming them) I made the process of freezing and optimizing the graph for inference on android using the script in the python\\tools folder (freeze_graph.py, optimize_for_inference.py, print_selective_registration_header.py). In the end of this process I get the optimized graph 'my_optimized_graph.pb' and the ops_to_register.h' header which is used to tell tensorflow (bazel) what kernels to register when you add the \"SELECTIVE_REGISTRATION\" flag to the build rule.\r\n\r\nMy problem is, when I load the model in android I get the following errors occurred after the session->create() call:\r\n\r\n> E/native: op_kernel.cc:925 OpKernel ('op: \"_Send\" device_type: \"GPU\"') for unknown op: _Send\r\nE/native: op_kernel.cc:925 OpKernel ('op: \"_Recv\" device_type: \"GPU\"') for unknown op: _Recv\r\nE/native: op_kernel.cc:925 OpKernel ('op: \"_Send\" device_type: \"CPU\"') for unknown op: _Send\r\nE/native: op_kernel.cc:925 OpKernel ('op: \"NoOp\" device_type: \"GPU\"') for unknown op: NoOp\r\nE/native: op_kernel.cc:925 OpKernel ('op: \"_Recv\" device_type: \"CPU\"') for unknown op: _Recv\r\nE/native: op_kernel.cc:925 OpKernel ('op: \"NoOp\" device_type: \"CPU\"') for unknown op: NoOp\r\nE/native: op_kernel.cc:925 OpKernel ('op: \"Placeholder\" device_type: \"GPU\"') for unknown op: Placeholder\r\nE/native: op_kernel.cc:925 OpKernel ('op: \"Placeholder\" device_type: \"CPU\"') for unknown op: Placeholder\r\nE/native: op_kernel.cc:925 OpKernel ('op: \"Const\" device_type: \"CPU\"') for unknown op: Const\r\nE/native: tensorflow_inference_jni.cc:138 Could not create Tensorflow Graph: Not found: Op type not registered 'Const'\r\n\r\nIt seems like the kernels did registered but the ops are unknown.\r\nI searched a lot the forum and couldn't find solution for this issue.\r\n### Did anyone succeed in compiling TF on Android with SELECTIVE_REGISTRATION and running a custom graph model other than the \"inception\" model which used in the demo?", "comments": ["Can you post the generated ops_to_register.h file?", "Closing due to inactivity. Provided new information, I will re-open."]}, {"number": 5882, "title": "TF sometimes hangs indefinitely when initializing variables", "body": "When doing a hyperoptimization TF hangs sometimes (once every 30 times) when initializing variables.\r\nThere are no error messages and it stalls for hours so I have to kill it manually.\r\n\r\nI have included information below. Please let me know if there's anything else you need.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/2788\r\nhttps://github.com/tensorflow/tensorflow/issues/5448\r\n\r\n### Environment info\r\nOperating System:\r\nDebian 8.6\r\nKernel: Linux 3.16.0-4-amd64\r\n\r\n### Installed version of CUDA and cuDNN: \r\n/usr/local/cuda/lib64/libcudadevrt.a    /usr/local/cuda/lib64/libcudart.so.8.0.44  /usr/local/cuda/lib64/libcudnn.so.5\r\n/usr/local/cuda/lib64/libcudart.so      /usr/local/cuda/lib64/libcudart_static.a   /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n/usr/local/cuda/lib64/libcudart.so.8.0  /usr/local/cuda/lib64/libcudnn.so          /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp34-cp34m-linux_x86_64.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n0.11.0\r\n\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\n# clean up from previous trial\r\nself.sess.close()\r\ntf.reset_default_graph()\r\n\r\n# initialize model\r\ntf_config = tf.ConfigProto()\r\ntf_config.gpu_options.per_process_gpu_memory_fraction = 0.45\r\nself.sess = tf.Session(config=tf_config)\r\n\r\n*** create model ***\r\n\r\nself.sess.run(tf.initialize_all_variables())\r\n``` \r\n<--- HANGS HERE\r\n\r\n\r\n### What other attempted solutions have you tried?\r\ninitialize variables one by one --> hangs at first variable\r\nconfig.operation_timeout_in_ms=60000 (as a temporary workaround)\r\n\r\n### Logs or other output that would be helpful\r\n\r\n```\r\ntail -n 2 nohup.log:\r\n\r\n2016-11-26 23:48:13.170 |  INFO |         create_model | tf_model.py:277 | 47.9 | Initializing all variables...\r\nInitializing: learning_rate:0\r\n\r\n```\r\n\r\n\r\n```\r\nstrace -p 15477 --> \r\n\r\nProcess 15477 attached\r\nfutex(0x7ffc1aabd91c, FUTEX_WAIT_PRIVATE, 1, NULL\r\n\r\n```\r\n```\r\nnvidia-smi -->\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 980 Ti  Off  | 0000:01:00.0     Off |                  N/A |\r\n|  1%   60C    P2    44W / 275W |   2901MiB /  6077MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0     15477    C   python                                        2899MiB |\r\n+-----------------------------------------------------------------------------+\r\n```", "comments": ["@mrry @suharshs This looks like some sort of issue with session state?", "I'm guessing this is a GPU driver/allocator initialization race. Can you answer a few questions to help us redirect the issue?\r\n\r\n1. Does the program ever hang if you disable the GPU (e.g. using `export CUDA_VISIBLE_DEVICES= python my_script.py`? \r\n2. Are you running more than one TensorFlow process on the same GPU? (The line `tf_config.gpu_options.per_process_gpu_memory_fraction = 0.45` implies that you might be.) Does the hang ever happen when running a single TensorFlow process in isolation?\r\n3. Can you try attaching to the hanging process to get the stacks of all threads (using `thread apply all bt`)?", "@mrry Thanks, I'm gonna try the above steps. Keep u posted.", "I also ran into that problem. Like mrry said, I'm running two process on one GPU, since we have only one GPU and we think the GPU memory is enough for two process. When we ran two process on one GPU using caffe, no such problem happens. Could you please offer some suggestions?@mrry", "@mariolew the first TF process tries to allocate all of GPU memory did you try solution here? -- http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory", "@yaroslavvb Yeah, I've tried this. But when we ran two processes, it's easy to get stuck for 5~6 hours and then it start working for a few minutes and then stuck again. The Volatile GPU-util is 0 when stucking.", "@bboonz did the steps @mrry gave you work? Also want to note that @mariolew is also reporting this, so I'm not going to close due to inactivity. ", "I've encountered a similar issue. I'm trying to run code which uses TensorFlow on a new installation and I get a large delay (1-2 min) on the initialization stage. Disabling GPU by using `export CUDA_VISIBLE_DEVICES=`  avoids this delay. There's only 1 GPU in the machine. (ubuntu 16.04, GTX 1080, nVidia 375.26, cuda 8.0.44, tf 0.12.1).\r\nOn the new installation, this delay happens every run.\r\n\r\nOn two different machines, one with a GTX 980 and the other with two GTX 1080 the code runs smoothly, though each running with different tf/nvidia/cuda 8 version (both with ubuntu 16.04).\r\n\r\nWhat more information can I gather to help resolving this issue?", "Closing due to lack of activity.  If somebody can provide the information requested by @mrry then please feel free to reopen.", "@mickeyil Your issue doesn't 'hang indefinitely'.  I'd prefer that issues with different symptoms weren't confused in the same thread.  If you are still having problems please open a separate issue.  ", "Hi, has anyone been able to find a solution for this? Currently facing it with Cuda 8.0 (V8.0.44), CuDNN 7, Ubuntu 16.04, Tensorflow 1.2.1 and mono-gpu (GeForce GTX 1060).\r\nThanks!\r\n\r\nEDIT: I removed CuDNN 7 and went back to CuDNN 5.1. There's a relative improvement but it's still super slow to load.\r\n\r\nEDIT2: fixed it by re-installing completely the Nvidia drivers, CUDA (8.0) and CuDNN (6).", "@RomainSabathe cuDNN (6) is what I needed instead of 5.1 :)"]}, {"number": 5881, "title": "Fix typo", "body": "", "comments": ["Can one of the admins verify this patch?", "@newge, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @vrv and @martinwicke to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please."]}, {"number": 5880, "title": "Android custom classifier importing error", "body": "Hi\r\n\r\nI am having trouble importing a graph that I made in python into the Android demo app and running it. I built a CNN for character recognition. I am having problems finding the input and output nodes so I kept trying random logical combinations. Also I froze the graph and ran it through inference.\r\nThere seems to be no documentation at all for importing your own classifier into Android.\r\nBoth the data set and the ipython notebook are at: https://drive.google.com/drive/folders/0By50gGWStkpFZ1ZsajI0WHpFN0E?usp=sharing\r\n\r\nIn the ipython noteboook, the CNN at the bottom of the file is what I am trying to import to Android, the one with Dropout and MaxPooling.\r\n\r\n", "comments": ["https://www.tensorflow.org/versions/r0.11/resources/index.html\r\n> For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n"]}, {"number": 5879, "title": "Upload installation instructions.", "body": "", "comments": ["@yifeif, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @keveman and @vrv to be potential reviewers.", "Oops - I merged the PR before I noticed your new comment, @gunan @yifeif, maybe you can do another PR to mention the reason why tensorflow_gpu wheels are not available via PyPi?"]}, {"number": 5878, "title": "server used by model reset connection", "body": "\r\n### tensorflow official tutorial link with problem\r\n<a href=\"https://www.tensorflow.org/versions/r0.11/tutorials/image_recognition/index.html\">https://www.tensorflow.org/versions/r0.11/tutorials/image_recognition/index.html</a>\r\n\r\n### Environment info\r\nUbuntu 14 64bit\r\n\r\n### Installation\r\nInstalled with pip to virtualenv\r\npip wheel used: https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\r\n\r\n### Execution of model\r\npython tensorflow/models/image/imagenet/classify_image.py\r\n\r\n### Output\r\n```\r\npython classify_image.py\r\n>> Downloading inception-2015-12-05.tgz 100.0%Traceback (most recent call last):\r\n  File \"classify_image.py\", line 212, in <module>\r\n    tf.app.run()\r\n  File \"/home/user/Documents/virt_env/python-tensorflow-test/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv))\r\n  File \"classify_image.py\", line 205, in main\r\n    maybe_download_and_extract()\r\n  File \"classify_image.py\", line 197, in maybe_download_and_extract\r\n    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\r\n  File \"/usr/lib/python2.7/urllib.py\", line 94, in urlretrieve\r\n    return _urlopener.retrieve(url, filename, reporthook, data)\r\n  File \"/usr/lib/python2.7/urllib.py\", line 268, in retrieve\r\n    block = fp.read(bs)\r\n  File \"/usr/lib/python2.7/socket.py\", line 380, in read\r\n    data = self._sock.recv(left)\r\nsocket.error: [Errno 104] Connection reset by peer\r\n```", "comments": ["The URL in question (i.e. the imagenet dataset download) seems to be working fine for me now.\r\nPerhaps a transient error?  \r\n\r\nCould you please try again?  And also try opening the same URL in a web browser:\r\n```\r\nhttp://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\r\n```", "Hey, The link seems to be working now but I don't have time to try it in tensorflow. I can download through the browser to the folder as a bypass if this problem persists for me. It shouldn't fail the CRC check then hopefully. \r\n\r\nThanks for your help.\r\n- James"]}, {"number": 5877, "title": "GraphKeys.VARIABLES deprecated early", "body": "`GraphKeys.VARIABLES` was intended to be deprecated next march according to the code.\r\n\r\nBut [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L4067) it is set as a property. Therefore back-compatibility of `GraphKeys.VARIABLES` is already broken now because one has to use `GraphKeys().VARIABLES` instead.\r\n\r\nIt should something like a classproperty instead.", "comments": ["I believe this was fixed by @ilblackdragon on 11/03.  I'm not sure why it hasn't made it to the public tree yet?", "Actually, seems like there was a change from working `@classproperty` to `@property`, and as we changed tests we didn't catch this. Let me fix it up back.", "Sorry about that. I overlooked it and didn't notice any test failures. @ilblackdragon "]}, {"number": 5876, "title": "Error in hessians() and _hessian_vector_product() for tf.nn.sparse_softmax_cross_entropy_with_logits()", "body": "On a simple model that implements logistic regression, constructing the loss using tf.nn.sparse_softmax_cross_entropy_with_logits() makes both hessians() and _hessian_vector_product() return identically zero vectors, which is incorrect. If I instead write the loss function manually using tf.log, tf.sigmoid, etc., hessians() and _hessian_vector_product return the correct answer. These two versions of the loss function agree on their values and their gradients; however, the Hessian output is different. \r\n\r\nHere is some sample output:\r\n\r\n```\r\nUsing sparse_softmax_cross_entropy_with_logits:\r\nLoss before first step: 0.686726\r\nLoss after first step : 0.686181\r\nActual diff in grad:\r\n[ 0.000122    0.00014928]\r\nPredicted diff in grad using _hessian_vector_product:\r\n[array([ 0.,  0.], dtype=float32)]\r\nHessian:\r\n[array([[ 0.,  0.],\r\n       [ 0.,  0.]], dtype=float32)]\r\n\r\nUsing custom loss function:\r\nLoss before first step: 0.686726\r\nLoss after first step : 0.686181\r\nActual diff in grad:\r\n[ 0.00012201  0.00014931]\r\nPredicted diff in grad using _hessian_vector_product:\r\n[array([ 0.00012199,  0.0001493 ], dtype=float32)]\r\nHessian:\r\n[array([[ 0.08229966,  0.        ],\r\n       [ 0.        ,  0.08278375]], dtype=float32)]\r\n```\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone that I can find. The code below uses hessians() and _hessian_vector_product() from https://github.com/tensorflow/tensorflow/blob/a4c8df209d7413068f4ed3e71c43eb798fbd5580/tensorflow/python/ops/gradients_impl.py\r\n\r\nHere is the PR that implemented hessians(): https://github.com/tensorflow/tensorflow/pull/5329\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n-rw-r--r-- 1 root root   558720 Oct  1 00:18 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Oct  1 00:18 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Oct  1 00:18 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 78065952 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn.so.5.0.5\r\n-rw-r--r-- 1 root root 68709594 Oct  1 16:19 /usr/local/cuda/lib64/libcudnn_static.a\r\n```\r\nThe same behavior occurs when running on CPU only.\r\n\r\nInstalled from: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\r\nv0.11.0\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```\r\ntf.set_random_seed(0)\r\n\r\n### Setup toy data and weights\r\nimages_placeholder = tf.placeholder(tf.float32, shape=(3, 2))\r\nlabels_placeholder = tf.placeholder(tf.int32, shape=(3))\r\nfeed_dict = {\r\n    images_placeholder: np.array([[0, 0], [0, 1], [1, 0]]),\r\n    labels_placeholder: np.array([0, 1, 1]),\r\n}\r\n  \r\nweights = tf.Variable(\r\n  tf.truncated_normal([2],\r\n                      stddev=1.0 / math.sqrt(float(2))),\r\n  name='weights')\r\n\r\n### Calculate loss using built-in TF function\r\nweights_with_zeros = tf.pack([tf.zeros([2]), weights], axis=1)\r\nlogits = tf.matmul(images_placeholder, weights_with_zeros)\r\ncross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels_placeholder)\r\nloss = tf.reduce_mean(cross_entropy)\r\n\r\n### Calculate loss using manually constructed TF function\r\nlogits2 = tf.matmul(images_placeholder, tf.reshape(weights, [2, 1]))\r\nlabels2 = (tf.to_float(labels_placeholder) * 2) - 1\r\nlogits_mul_labels = tf.mul(tf.reshape(logits2, [-1]), tf.reshape(labels2, [-1]))\r\ncross_entropy2 = - tf.log(tf.sigmoid(logits_mul_labels))\r\nloss2 = tf.reduce_mean(cross_entropy2)\r\n\r\n### Create train_op\r\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\r\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\r\ntrain_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n### Calculate gradients, Hessians, and Hessian-vector products for both versions of loss\r\ngrad = tf.gradients(loss, [weights])\r\ngrad2 = tf.gradients(loss2, [weights])\r\nv_placeholder = tf.placeholder(tf.float32, shape=weights.get_shape())\r\nhessian_vector = _hessian_vector_product(loss, [weights], [v_placeholder])\r\nhessian_vector2 = _hessian_vector_product(loss2, [weights], [v_placeholder])\r\nhessian = hessians(loss, [weights])\r\nhessian2 = hessians(loss2, [weights])\r\n\r\n### Run training for a single step to get the parameters to change.\r\ninit = tf.initialize_all_variables()\r\nsess = tf.Session()\r\nsess.run(init)\r\n\r\nold_weights_val, old_loss_val, old_grad_val, old_loss2_val, old_grad2_val= sess.run(\r\n  [weights, loss, grad, loss2, grad2], \r\n  feed_dict=feed_dict)\r\n\r\n_ = sess.run(train_op, feed_dict=feed_dict)\r\n\r\nnew_weights_val, new_loss_val, new_grad_val, new_loss2_val, new_grad2_val = sess.run(\r\n  [weights, loss, grad, loss2, grad2], \r\n  feed_dict=feed_dict)\r\n\r\nhessian_val, hessian2_val = sess.run(\r\n  [hessian, hessian2], \r\n  feed_dict=feed_dict)\r\n\r\n### Calculate the actual difference in gradients before and after the train step,\r\n### and compare with the predicted difference in gradients based on the Hessian.\r\ndiff_in_weights = new_weights_val - old_weights_val\r\nactual_diff_in_grad = new_grad_val[0] - old_grad_val[0]\r\nactual_diff_in_grad2 = new_grad2_val[0] - old_grad2_val[0]\r\n\r\nfeed_dict[v_placeholder] = diff_in_weights\r\npredicted_diff_in_grad = sess.run(hessian_vector, feed_dict=feed_dict)\r\npredicted_diff_in_grad2 = sess.run(hessian_vector2, feed_dict=feed_dict)\r\n\r\nprint('Diff in weights:\\n%s' % diff_in_weights)\r\n\r\nprint('\\nUsing sparse_softmax_cross_entropy_with_logits:')\r\nprint('Loss before first step: %s' % old_loss_val)\r\nprint('Loss after first step : %s' % new_loss_val)\r\nprint('Actual diff in grad:\\n%s' % actual_diff_in_grad)\r\nprint('Predicted diff in grad using _hessian_vector_product:\\n%s' % predicted_diff_in_grad)\r\nprint('Hessian:\\n%s' % hessian_val)\r\n\r\nprint('\\nUsing custom loss function:')\r\nprint('Loss before first step: %s' % old_loss2_val)\r\nprint('Loss after first step : %s' % new_loss2_val)\r\nprint('Actual diff in grad:\\n%s' % actual_diff_in_grad2)\r\nprint('Predicted diff in grad using _hessian_vector_product:\\n%s' % predicted_diff_in_grad2)\r\nprint('Hessian:\\n%s' % hessian2_val)\r\n\r\nsess.close()\r\n\r\n```\r\n### What other attempted solutions have you tried?\r\nRunning in CPU or GPU makes no difference.\r\n\r\nUsing more complicated networks (i.e., adding some non-linear hidden layers before the linear softmax step) makes the Hessian returned from sparse_softmax_cross_entropy_with_logits() non-zero, but the returned value is still wrong in the sense that it does not match the empirical values. In contrast, using the same custom loss function above returns the correct Hessians.  \r\n\r\nThe same problem occurs when using \"real\" data (e.g., MNIST) or with more examples.\r\n\r\n### Logs or other output that would be helpful\r\nFull output when using CPU:\r\n```\r\nDiff in weights:\r\n[ 0.00148226  0.0018035 ]\r\n\r\nUsing sparse_softmax_cross_entropy_with_logits:\r\nLoss before first step: 0.686726\r\nLoss after first step : 0.686181\r\nActual diff in grad:\r\n[ 0.000122    0.00014928]\r\nPredicted diff in grad using _hessian_vector_product:\r\n[array([ 0.,  0.], dtype=float32)]\r\nHessian:\r\n[array([[ 0.,  0.],\r\n       [ 0.,  0.]], dtype=float32)]\r\n\r\nUsing custom loss function:\r\nLoss before first step: 0.686726\r\nLoss after first step : 0.686181\r\nActual diff in grad:\r\n[ 0.00012201  0.00014931]\r\nPredicted diff in grad using _hessian_vector_product:\r\n[array([ 0.00012199,  0.0001493 ], dtype=float32)]\r\nHessian:\r\n[array([[ 0.08229966,  0.        ],\r\n       [ 0.        ,  0.08278375]], dtype=float32)]\r\n```", "comments": ["@goodfeli, @tillahoffmann, @vrv This is possibly related to #5329 which you were all involved with.  Could somebody please comment on the above?", "Will have a look tomorrow.", "I won't have time to dig into this properly for a while but here's a hunch: The `sparse_softmax_cross_entropy_with_logits` presumably indexes the tensor of logits and some of the indexing operations don't have gradients defined (cf. #206). It may be possible to compute the first derivative of a function but the second derivative may not yet be implemented. @benoitsteiner and @yuefengz probably have a better understanding of the [implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1392).", "I agree with @tillahoffmann that this sounds like the gradient of `sparse_softmax_cross_entropy_with_logits` does not implement the gradient correctly.\r\n\r\nIf this is indeed caused by an op failing to *implement* the gradient, it seems like tf.gradients ought to raise a NotImplementedError or issue some other error message, rather than silently returning numerically incorrect values.\r\n\r\nVincent Dumoulin and Alex Kurakin have both told me they have had trouble with ops silently returning zero as their second derivative in the past.", "Thanks for looking into this! On further testing, it seems like the problem is not localized to `sparse_softmax_cross_entropy_with_logits`. Implementing the same example using `softmax_cross_entropy_with_logits` (i.e., changing the labels from `[0, 1, 1]` to `[[1, 0], [0, 1], [0, 1]]`) has the same issue of a zero second derivative. So the problem might be in the softmax function itself and not the sparse indexing.", "The softmax function shouldn't actually be involved in the implementation of either of those costs (the numerically stable way to implement them involves simplifying `log softmax x` to `x - log sum exp x`), so I doubt that it's a problem with the softmax op. ", "Oh, right. Sorry, I was sloppy with that statement. I meant to say that the problem is probably in the (presumably shared) `softmax_cross_entropy_with_logits` part of the function and not in the additional indexing operations that `sparse_softmax_cross_entropy_with_logits` does on top of that, which @tillahoffmann suggested was the culprit.", "Hm, I'm computing Hessians of cross entropy losses in my work and don't seem to have a problem with them. Will have a look into a minimum working example and get back to you.", "According to the 0.12 release notes, there are some ops whose second gradients have been fixed, so it might be worth upgrading to 0.12rc0 (just released last night) and see if the problem persists", "Thanks for the tip! I just tested the above example in 0.12rc0 but unfortunately the behavior is unchanged (zeros for Hessians).", "This is unfortunately a known bug and is caused by the fact that the cross_entropy_loss ops are fused: they calculate fw and bprop values at the same time and return them together (the bprop is hidden from the user but is used only during tf.gradients call).  This doesn't work well with TF's gradient registration mechanism when you want the second derivative.  Solutions may be forthcoming, but no promises on the timeline.", "Ok. Thank you for the update!", "@ebrevdo Do you have a link to the github issue for this? More generally, is there a way to at least cause a NotImplementedError rather than returning the wrong numbers?", "@goodfeli this is the only github issue afaik (the other one is internal and doesn't have much more info).\r\n\r\nI have one idea about how to get this to fail loudly when differentiating twice, I'll talk to Eugene today and see if it's possible in the short-term.  One workaround for now is to not use the fused cross entropy functions and use their stable, primitive-op expansion.  It will be slower but at least your results will be more correct.", "Okay, with https://github.com/tensorflow/tensorflow/commit/df52532ccb9a6084ed8877a5b11698fa451a8762 we now fail loudly when you try to take second derivatives of ops that can't be done that way.  I think we're just going to rely on removing these fused ops via XLA one day as the eventual solution.", "Cool. Thanks!"]}, {"number": 5875, "title": "[TF-Slim]The latest version has `write_version=saver_pb2.SaverDef.V2` as default set\u2026", "body": "\u2026ting. And V2 is more efficient than V1.", "comments": ["@zuoxingdong, thanks for your PR! By analyzing the history of the files in this pull request, we identified @concretevitamin, @tensorflower-gardener and @nathansilberman to be potential reviewers.", "Can one of the admins verify this patch?", "@tensorflow-jenkins, test this please", "The test failure in //tensorflow/python/kernel_tests:rnn_test in the Linux GPU build is unrelated. Merging this PR now.\r\n\r\nThanks again, @zuoxingdong !", "/cc @sguada @nathansilberman "]}]