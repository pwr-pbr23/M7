[{"number": 7296, "title": "Seperate gradients in tf.gradients", "body": "based on [documentation](https://www.tensorflow.org/api_docs/python/train/gradient_computation#gradients) `tf.gradients` return the `sum(dy/dx)`. For our problem, **sum** doesn't make sense. Is it possible to get the list of gradients instead? ", "comments": ["Generally not possible, you need to do `n` calls to `tf.gradients` or use a trick from ian here: https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-253056458", "Closing to keep things in one place (http://stackoverflow.com/questions/42058690/separate-gradients-in-tf-gradients)"]}, {"number": 7295, "title": "tf.get_collection to extract variables of one scope ", "body": "Hi,\r\n\r\nI have `n`(e.g: n=3) scopes and `x` (e.g: x=4) no of Variables defined in each scope. \r\nThe scopes are:\r\n\r\n        model/generator_0\r\n        model/generator_1\r\n        model/generator_2\r\n\r\nOnce I compute the loss, I want to extract and provide all the variables from only one of the scope based on a criteria during run-time. Hence the index of the scope `idx` that I select is an argmin tensor cast into `int32` \r\n\r\n        <tf.Tensor 'model/Cast:0' shape=() dtype=int32>\r\n\r\nI have already tried: \r\n\r\n        train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'model/generator_'+tf.cast(idx, tf.string)) \r\n\r\nwhich obviously did not work. \r\nIs there any way to get all the `x` Variables belonging to that particular scope using `idx` to pass into the optimizer.\r\nForgive me if this question doesnt fit into tensorflow issues. \r\n\r\nThanks in advance!\r\nVignesh Srinivasan", "comments": ["indeed this answer doesn't fit into TF issues, but it fits very well into stackoverflow issues, I have a solution in mind if you post it there", "@yaroslavvb : I have posted it there now. \r\nhttp://stackoverflow.com/questions/42073239/tf-get-collection-to-extract-variables-of-one-scope", "Thanks @yaroslavvb!"]}, {"number": 7294, "title": "Any reason to not publish wheels for Mac OS < 10.11.x on PyPI?", "body": "The tensorflow wheels for Mac OS on PyPI are tagged with a platform of 10.11 and thus aren't picked up by pip on Mac OS 10.10.x or earlier. Google hosts wheels that work on any version of Mac OS (e.g., https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.12.1-py2-none-any.whl).\r\n\r\nCan these be uploaded to PyPI or can a wheel be published there that supports older versions of Mac OS?", "comments": ["The infra team would need to setup older MacOS machines and test wheels in that environment, and fix/document \"older MacOS/tensorflow\" snags in a regular cycle before publishing an official wheel....how much impact would such releases have?", "The pip package you linked is simply renamed to `none-any` to avoid any issues that may be caused by older pip versions. That is not actually built for older versions.\r\nAs @yaroslavvb pointed out, we wont be able to test the pip packages for older versions, and have as good support for it. This combined with Apple's policy of quickly sending updates to all devices as soon as possible, we decided to keep our official support close to the latest version of MacOS.\r\n\r\n@dwyatte Does the above answer your question?\r\n\r\n", "Helpful context -- thanks guys. Even if this is a \"won't fix\", it's useful to have it here in GitHub for others who experience the same issue.\r\n\r\nAnyway, to answer @yaroslavvb's question about impact: I think this is pretty low impact. Most people using tensorflow seriously will be using it on Linux with a GPU. Of the small subset of Mac users, only a few will be on old versions of the OS.\r\n\r\nAs the project continues to mature, it might worth noting these OS requirements in the documentation, but the discussion here is a sufficient answer for me."]}, {"number": 7293, "title": "How to do k-max pooling with proper dim ?", "body": "Hi folks,\r\n\r\nDoes anybody have this same issue as me ? I want to apply a k-max pooling to this var, however it's is not in the proper dimension yet:\r\n\r\nFor ex:\r\nh = tf.Variable(batch_size,1,18,512)     # This is the obligatory dim's result I got when I apply Deep CNN in NLP\r\n\r\nI want to have the result of k-max pooling like this:\r\nh_kmaxpooling with dim (batch_size,1,**8**,512) \r\n\r\nIf I use tf.nn.top_k(h, 8), it will result in (batch_size,1,18,**8**) instead.\r\nI've tried **tf.transpose(x, perm=[batch_size, 1, 512, 18])** but it always give error in batch_size since it doesn't know how many examples are. Or may be I've set wrong tf.transpose in this case ?\r\n ", "comments": ["This question would be better asked on StackOverflow.  Github issues are for bug reports and feature requests."]}, {"number": 7292, "title": "F c:\\tf_jenkins\\home\\workspace\\release-win\\device\\gpu\\os\\windows\\tensorflow\\stream_executor\\cuda\\cuda_blas.cc:268] Check failed: s.ok() could not find cublasCreate_v2 in cuBLAS DSO; dlerror: cublasCreate_v2 not found", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:win10\r\n\r\nInstalled version of CUDA and cuDNN: \r\ncuda8.0 cudnn 5\r\n\r\nIf installed from binary pip package, provide:\r\npip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-win_amd64.whl\r\n\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["@mrry Want this lovely Windows GPU issue?", "Not without much more information, e.g. including what other logs are printed. If I had to guess, I'd say the problem is an incompatible version of the cuBLAS DLL in the `%PATH%`. The logs should contain additional information about what DLL was loaded.", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}, {"number": 7291, "title": "Fix gradle task dependencies", "body": "Without the `afterEvaluate` closure, gradle can\u2019t find the `assembleDebug` task.\r\n\r\nSee: https://code.google.com/p/android/issues/detail?id=219732#c32", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Should have to be submitted with the update of gradle tool version (to 2.2.x) at the same time!", "Why that Issue is Closed? Code is not committed and still missed in master brunch: tensorflow/tensorflow/examples/android/build.gradle\r\n\r\n@andrewharp have a look, please.\r\n\r\nPS Got the same problem and found the same solution.", "@ArtsiomCh We actually have a CL in the pipeline internally that fixes this and a couple of other things, should be public in a day or two.", "@ArtsiomCh @andrewharp Actually it's not a real issue for previous gradle tool version (pre-2.2), so I withdrew it."]}, {"number": 7290, "title": "\"SAME\" padding for ConvNet doesn't work as expected !", "body": "Hi folks,\r\nWhen I want to reimplement the paper [VDCNN for Text Classification](https://arxiv.org/abs/1606.01781), I need to do padding=1 with filter_kernel=3 to keep the same input's dimension between ConvNet in the model.  When I use padding 'SAME', the result isn't well expected. There's a problem in the height_size_output. It seems doesn't do element-wise product of block convnet 72x3 between the input & the filter. \r\nBy the way, there's no problem of height_size_output using the padding 'VALID'. It seems that it did. However, its width_size_output is reduced by 2. \r\nIn my intuition, padding 'VALID' behave much more properly like in the paper, but how can I pad 1 to both the left & right of the 2D input before doing ConvNet ?\r\n\r\n-----------------------------------------\r\nimport tensorflow as tf\r\n\r\ndata = tf.placeholder(\"float\", shape=[1, 72, 1014, 1])\r\n#\r\nfilter_shape = [72, 3, 1, 64]\t\r\nW = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.05), name=\"W\")\r\n#\r\nconv = tf.nn.conv2d(data, W, padding='SAME')\r\nprint(conv.get_shape())  # prints (1, **72**, 1014, 64), but should be (1, **1**, 1014, 64)\r\n\r\nconv = tf.nn.conv2d(data, W, padding='VALID')\r\nprint(conv.get_shape())  # prints (1, **1**, **1012**, 64), but should be (1, **1**, **1014**, 64)", "comments": ["I found the answer by using tf.pad() in this post, so I will close this topic. \r\nhttp://stackoverflow.com/questions/37659538/custom-padding-for-convolutions-in-tensorflow\r\n\r\nHowever, it should be mentioned in the doc this issue because the actual tensorflow's guide is not clear enough at this point ! Thanks"]}, {"number": 7289, "title": "tensorflow/core/platform/setround.cc:27:4: error: 'fesetround' is not a member of 'std'", "body": "```\r\nERROR: /local/tmp/tensorflow/tensorflow/core/BUILD:1036:1: C++ compilation of rule '//tensorflow/core:lib_internal' failed: gcc failed: error executing command /local2/tmp/brew/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/local2/tmp/brew/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 ... (remaining 104 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\ntensorflow/core/platform/setround.cc: In constructor 'tensorflow::port::ScopedSetRound::ScopedSetRound()':\r\ntensorflow/core/platform/setround.cc:27:4: error: 'fesetround' is not a member of 'std'\r\n    std::fesetround(FE_TONEAREST);\r\n    ^\r\ntensorflow/core/platform/setround.cc:27:4: note: suggested alternative:\r\nIn file included from tensorflow/core/platform/setround.cc:19:0:\r\n/local2/tmp/brew/include/fenv.h:88:12: note:   'fesetround'\r\n extern int fesetround (int __rounding_direction) __THROW;\r\n            ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\n```\r\n\r\nLooks like the issue is that this file is being compiled with gcc instead of g++", "comments": ["gcc vs. g++ is highly unlikely to be the reason.  What platform are you on, and what compiler are you using?", "@girving FYI: I'm fixing this internally and will get it pushed to github. port::ScopedSetRound was kind of broken in the sense that it didn't reset the rounding mode in the destructor, as stated in the header file. It also might as well take the desired rounding mode as an argument. It didn't matter since it is currently only used to set rounding mode upon thread startup.", "@carlorosati FYI: The fix was submitted internally at Google and should appear on Github in a day or two."]}, {"number": 7288, "title": "Are linkopts propagated from copts and/or deps ?", "body": "The linker complains about not finding `-lpthread`, while I didn't add this flag to linkopts.\r\n\r\nI've checked the executed command, and in fact there is extra flags on it `-lz  -lpthread ...`.\r\n\r\nWhere did they came from ? Is there a workaround for this ?\r\n\r\n\r\nMore details\r\n====\r\n\r\nBUILD file\r\n---\r\n\r\n```\r\ncc_binary(\r\n    name = \"libfoo.so\",\r\n    srcs = glob([\r\n         \"jni/**/*.cc\",\r\n         \"jni/**/*.h\",\r\n    ]),\r\n    copts = [ \"-fexceptions\", \"-DEIGEN_AVOID_STL_ARRAY\",\r\n              \"-mfpu=neon\", \"-std=c++11\",\r\n              \"-DMIN_LOG_LEVEL=0\", \"-DTF_LEAN_BINARY\",\r\n              \"-O2\", ],\r\n    linkopts = [\r\n        \"-llog\",\r\n        \"-lm\",\r\n    ],\r\n    linkshared = 1,\r\n    deps = [\r\n        \"@org_tensorflow//tensorflow/core:android_tensorflow_lib\",\r\n        \"@boringssl//:crypto\",\r\n    ],\r\n)\r\n```\r\n\r\nCommand\r\n---\r\n`bazel build -c opt //:libfoo.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures --sandbox_debug --strategy=CppLink=standalone`\r\n\r\n\r\nFull error\r\n---\r\n\r\n```\r\n...\r\n  external/bazel_tools/tools/cpp/link_dynamic_library.sh no ignored ignored ignored external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/arm-linux-androideabi-gcc -shared -o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/libfoo.so -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/org_tensorflow/tensorflow/core/libandroid_tensorflow_lib.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/org_tensorflow/tensorflow/core/kernels/libandroid_tensorflow_kernels.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/org_tensorflow/tensorflow/core/libandroid_tensorflow_lib_lite.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/org_tensorflow/tensorflow/core/libprotos_all_cc.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/protobuf/libprotobuf.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/protobuf/libprotobuf_lite.a -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/boringssl/libcrypto.a -Wl,-no-whole-archive external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libgnustl_static.a external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/libsupc++.a -llog -lm -lz -lpthread -static-libgcc -no-canonical-prefixes '-march=armv7-a' -Wl,--fix-cortex-a8 '--sysroot=external/androidndk/ndk/platforms/android-21/arch-arm'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\nexternal/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread\r\ncollect2: error: ld returned 1 exit status\r\nTarget //:libfoo.so failed to build\r\n\r\n```\r\n", "comments": ["```\r\n$ cd tensorflow\r\n$ git log -1\r\ncommit ac352bc807410c8a863d0b95202cceeb62ed0f10\r\nAuthor: gunan <gunan@google.com>\r\nDate:   Sun Feb 5 22:19:11 2017 -0800\r\n\r\n    Simplify cuDNN library handling in configure script. (#7269)\r\n    \r\n    cuda_configure.bzl now handles the more complicated operations.\r\n```", "@jart Have you seen this before?", "Since copts and linkopts are viral and propagate to dependencies, `-lpthread` is most likely being inherited from `@boringssl//:crypto`. See: https://github.com/google/boringssl/blob/bbcaa15b0647816b9a1a9b9e0d209cd6712f0105/BUILD#L91\r\n\r\nIf linking against BoringSSL for an Android build is genuinely a good thing to do, then this could be a bug. Hey @petewarden what's the correct way to get SSL functionality for Android code? Should our friend be using a different library?\r\n\r\n**Unrelated suggestions:**\r\n\r\n> name = \"libfoo.so\",\r\n\r\nJust name it `\"foo\"` because when Bazel builds this, it will prefix `lib` and append `.so` by default.\r\n\r\n>    copts = [ \"-fexceptions\", \"-DEIGEN_AVOID_STL_ARRAY\",\r\n>              \"-mfpu=neon\", \"-std=c++11\",\r\n>              \"-DMIN_LOG_LEVEL=0\", \"-DTF_LEAN_BINARY\",\r\n>              \"-O2\", ],\r\n\r\nMany of these flags might already be inherited from the `@org_tensorflow//tensorflow/core:android_tensorflow_lib` dependency.\r\n\r\nThe `-O2` (or maybe `-O3`) flag is set by default when you say `bazel build -c opt`.\r\n\r\nBazel should already pass flags like `-std=c++11` by default.\r\n\r\nAnother thing to note is that `bazel build -s //blah` is good for troubleshooting.", "@jart Thank you for your answer.\r\n\r\nI think a similar question is discussed [here](https://github.com/google/protobuf/issues/1373#issuecomment-204619831). but I don't understand proposed solution, can you help me ?\r\n\r\n> Just name it \"foo\" because when Bazel builds this, it will prefix lib and append .so by default.\r\n\r\n`BUILD:40:18: in linkshared attribute of cc_binary rule //:foo: 'linkshared' used in non-shared library.`\r\n\r\n> Many of these flags might already be inherited from the @org_tensorflow//tensorflow/core:android_tensorflow_lib dependency.\r\n\r\nYou are right, only `-std=c++11` is required here.\r\n", "@riless @jart\r\n-lpthread is not necessary or possible on Android, so it sounds like the solution would be to add another condition for the select statement as in the linked commit https://github.com/google/protobuf/pull/1386:\r\n\r\n```\r\n    linkopts = select({\r\n        \":mac_x86_64\": [],\r\n        \":android\": [],\r\n        \"//conditions:default\": [\"-lpthread\"],\r\n    }),\r\n```\r\n\r\nwhich would necessitate adding a config setting like so:\r\n\r\n```\r\nconfig_setting(\r\n    name = \"android\",\r\n    values = {\r\n        \"crosstool_top\": \"//external:android/crosstool\",\r\n    },\r\n    visibility = [\"//visibility:public\"],\r\n)\r\n```\r\n\r\nThe only other workaround I know of that doesn't require editing the other repository is to create a dummy libpthread.so target, but that's pretty hacky.", "I'd like to go with the _dummy libpthread.so target_ solution.\r\n\r\nI don't know how to do this. I've tried to add a target like this.\r\n\r\n```\r\ncc_library(\r\n    name = \"pthread\",\r\n    srcs = [\"third_party/empty.cc\"],\r\n    visibility = [\"//visibility:public\"],\r\n    linkstatic = 1\r\n)\r\n```\r\n\r\nAnd added it to deps of libfoo.so\r\n\r\n```\r\ncc_binary(\r\n    name = \"libfoo.so\",\r\n   ...\r\n   deps = [\r\n        \":pthread\",\r\n        ...\r\n  ]\r\n)\r\n```\r\n\r\nbut I still have the `cannot find -lpthread` error.\r\n\r\n\r\n", "@riless See https://github.com/tensorflow/tensorflow/commit/cd53f3c3302c9312c1840389a9988a879b8b9dd5#diff-06b90d704990e9b4f5adadb18c837b64\r\n\r\nThis has since been removed, as obviously for a long-term solution it was preferable to fix the protobuf source instead. If we need boringssl on Android builds we should do the same for that as well.", "@andrewharp  it works now, thank you.\r\n\r\nI'll still go with this solution as I want boringssl to be downloaded on build."]}, {"number": 7287, "title": "SKFLOW/TFLearn SKCompat does not properly implement SKLearn predict and predict_proba", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nNone seem to apply, this is a new issue just introduced with 1.0.0-rc1\r\n\r\n### Environment info\r\nOperating System:\r\nMac\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n\r\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.0rc1-py3-none-any.whl\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n1.0.0-rc1\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nTFLearn has incorrect implementations of ```predict``` and ```predict_proba```, per the [sklearn developer's guide](http://scikit-learn.org/stable/developers/contributing.html#coding-guidelines).  The current (v1.0.0-rc1) version's ```predict``` actually performs as would be expected for ```predict_proba``` and ```SKCompat``` does not include a ```predict_proba```.\r\n\r\nSKFLOW/TFLearn's compatibility with sklearn has been broken in various ways since when v0.8 (which worked) was refactored into SKCompat (see #6584).  \r\n\r\nThe following code shows how the accuracy function fails due to this issue.  This could be fixed with an argmax call, but the core problem is that SKCompat does not properly implement the sklearn functions.\r\n\r\nNote, this code is based on the Contrib TF Learn example here:  [https://www.tensorflow.org/tutorials/tflearn/](https://www.tensorflow.org/tutorials/tflearn/)\r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\nfrom sklearn import cross_validation\r\nfrom sklearn import metrics\r\nimport tensorflow as tf\r\n\r\ndef main(unused_argv):\r\n  # Load dataset.\r\n  iris = tf.contrib.learn.datasets.load_dataset('iris')\r\n  x_train, x_test, y_train, y_test = cross_validation.train_test_split(\r\n      iris.data, iris.target, test_size=0.2, random_state=42)\r\n\r\n  # Build 3 layer DNN with 10, 20, 10 units respectively.\r\n  feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(\r\n      x_train)\r\n  classifier = tf.contrib.learn.DNNClassifier(\r\n      feature_columns=feature_columns, hidden_units=[10, 20, 10], n_classes=3)\r\n  classifier = tf.contrib.learn.SKCompat(classifier)\r\n\r\n  # Fit and predict.\r\n  classifier.fit(x_train, y_train, steps=200)\r\n\r\n  # This outputs something with shape [30,3] should output single class predictions\r\n  # See http://scikit-learn.org/stable/developers/contributing.html#coding-guidelines\r\n  # See predict vs predict_proba\r\n  #predictions = classifier.predict(x_test)\r\n  #score = metrics.accuracy_score(y_test, predictions)\r\n  #print('Accuracy: {0:f}'.format(score))\r\n\r\n  # predict_proba does not work either:\r\n  classifier.predict_proba(x_test)\r\n  # Generates a: AttributeError: 'SKCompat' object has no attribute 'predict_proba'\r\n\r\nif __name__ == '__main__':\r\n  tf.app.run()\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nhttp://scikit-learn.org/stable/developers/contributing.html#coding-guidelines\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\n/Users/jeff/anaconda/envs/tf-latest/bin/python /Users/jeff/Dropbox/school/teaching/wustl/classes/T81_558_deep_learning/code/test3.py\r\n/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\r\n  \"This module will be removed in 0.20.\", DeprecationWarning)\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/bs/_w74fpx157v3vs82q0fwnflr0000gn/T/tmpl7bwdnzy\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nWARNING:tensorflow:From /Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\r\nInstructions for updating:\r\nPlease switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nWARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\r\nTraceback (most recent call last):\r\n  File \"/Users/jeff/Dropbox/school/teaching/wustl/classes/T81_558_deep_learning/code/test3.py\", line 52, in <module>\r\n    tf.app.run()\r\n  File \"/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/jeff/Dropbox/school/teaching/wustl/classes/T81_558_deep_learning/code/test3.py\", line 44, in main\r\n    score = metrics.accuracy_score(y_test, predictions)\r\n  File \"/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/sklearn/metrics/classification.py\", line 172, in accuracy_score\r\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\r\n  File \"/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/sklearn/metrics/classification.py\", line 72, in _check_targets\r\n    check_consistent_length(y_true, y_pred)\r\n  File \"/Users/jeff/anaconda/envs/tf-latest/lib/python3.5/site-packages/sklearn/utils/validation.py\", line 181, in check_consistent_length\r\n    \" samples: %r\" % [int(l) for l in lengths])\r\nValueError: Found input variables with inconsistent numbers of samples: [30, 3]\r\n\r\nProcess finished with exit code 1\r\n```", "comments": ["Does `SKCompat` still exist in Tensorflow 1.0?\r\n\r\n```\r\n>>> import tensorflow as tf\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n>>> tf.contrib.learn.SKCompat\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'SKCompat'\r\n```", "Did TensorFlow drop support for scikit-learn? (that would be unfortunate)  Did it get renamed to something else?  Wasn't scikit-learn comparability the main point of skflow?", "Hi,\r\n\r\nHere is what I currently have `SKCompat` in TF 1.0:\r\n```\r\nIn [1]: import tensorflow as tf\r\ntf.contrib.learn.SKC\r\nIn [2]: tf.contrib.learn.SKCompat\r\nOut[2]: tensorflow.contrib.learn.python.learn.estimators.estimator.SKCompat\r\n\r\nIn [3]: tf.__version__\r\nOut[3]: '1.0.0-rc2'\r\n```\r\n\r\nIndeed we don't have `predict_proba` at this point implemented in `SKCompat` because the way probabilities can be surfaced from the underlaying `Estimator` depends on your implementation of `model_fn`. You can use `.predict` method and select `outputs` that return probabilities.", "If you are not going to support predict_proba, I suppose that is okay.  But every scikit-learn model that I've ever seen returns a scalar class index per prediction when predict is called on a model.  Not a vector of probabilities like tflearn is currently doing. You can easily get to the prediction class using an argmax, but it makes it difficult to make TF models interchangeable with other scikit-learn models since the shape of a TF classification is different than any other scikit-learn model. ", "@ilblackdragon I am not showing SKCompat as part of TensorFlow 1.0.  Was scikit-learn compatibility dropped?  I tried your same instructions on TF 1.0 (you used rc2) and I get:\r\n\r\n```\r\n>>> import tensorflow as tf\r\n>>> tf.contrib.learn.SKCompat\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow.contrib.learn' has no attribute 'SKCompat'\r\n>>> tf.__version__\r\n'1.0.0'\r\n>>>\r\n```\r\n\r\nYet, in 1.0.0 I still get warnings telling me to use SKCompat.  Very confused.\r\n\r\n```\r\nEstimator is decoupled from Scikit Learn interface by moving into\r\nseparate class SKCompat. Arguments x, y and batch_size are only\r\navailable in the SKCompat class, Estimator will only accept input_fn.\r\nExample conversion:\r\n  est = Estimator(...) -> est = SKCompat(Estimator(...))\r\n```\r\n\r\nI am using it as follows, let me know if this is not correct.  I am following the instructions from the warning above:\r\n\r\n```\r\n    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\r\n    model = learn.DNNRegressor(\r\n        model_dir=model_dir,\r\n        feature_columns=feature_columns,\r\n        hidden_units=HIDDEN_UNITS)\r\n\r\n    # Make TensorFlow model behave like scikit-learn\r\n    model = learn.SKCompat(model)\r\n```", "I upgraded to 1.0.1 and I have the same problem:\r\n>>> import tensorflow as tf\r\n>>> tf.contrib.learn.SKCompat\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nAttributeError: 'module' object has no attribute 'SKCompat'\r\n>>> tf.__version__\r\n'1.0.1'\r\n>>> \r\n", "I made a PR fixing this. It should appear in 1.1, soon.", "And by \"this\" I mean the fact that SKCompat is hidden. It does not change the behavior of SKCompat that is part of this issue. I'll adjust the PR description.", "We can probably make predict_proba and predict work as expected on comforming classification models, and for simple regression models (which won't support predict_proba). \r\n\r\nHowever, many (maybe most) actual models have several outputs. What should happen in this case?", "@martinwicke , In answer to the question, \"However, many (maybe most) actual models have several outputs. What should happen in this case?\" \r\n\r\nFor models with several outputs, you would probably want to follow the way other scikit-learn models handle this.  If you are going for scikit-learn compatibility.\r\n\r\nhttp://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html\r\n\r\nAlso, it seems like SKFLOW/TFLearn currently does not support models with more than one output (for regression).  Please see #6849 ", "Scikit Learn only supports heterogeneous outputs, so can pack them into one\nmatrix. In Tnesorflow you can have out output is float, one a number and\nanother a string. And this is fairly regular case (like one classification\nthat returns class index, probability and class string).\n Due to this we made decision to return different type output.\n\nAlternatively we can make that predict on SKCompat will work into for few\nsimple cases and if they have multioutput model one will be not be using it.\n\nOn Mar 11, 2017 5:09 PM, \"Jeff Heaton\" <notifications@github.com> wrote:\n\n> For models with several outputs, you would probably want to follow the way\n> other scikit-learn models handle this. If you are going for scikit-learn\n> compatibility.\n>\n> http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.\n> MultiOutputRegressor.html\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7287#issuecomment-285913197>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAKtfvre0Vt26RxMWk6oOCdBMOUWqgSqks5rk0WwgaJpZM4L4IZt>\n> .\n>\n", "@ilblackdragon I do see your point.  I had assumed the goal of SKCompat was to be compatible with scikit-learn, but I can see that is not the case.  Its easy enough for me to just wrap it myself when I need the to use TensorFlow models with Scikit-Learn operations that expect standard calls (e.g. grids and ensembles).  Thanks for the information! ", "@jeffheaton I'm having the same issue with DNNClassifier and SKCompat. It would help if the tf.contrib.learn Quickstart tutorial were updated to reflect the use of SKcompat rather than the deprecated version that it currently uses (see https://www.tensorflow.org/get_started/tflearn).\r\n", "FYI, I switched to Keras.  It actually implements sklearn in a way that works with sklearn."]}, {"number": 7286, "title": "Upload sdists to PyPI", "body": "Hi.\r\n\r\nIt appears that PyPI only contains wheels for tensorflow:\r\n\r\nhttps://pypi.org/project/tensorflow/#files\r\n\r\nThanks for uploading those! Wheels are great.\r\n\r\nBut sdists are good to have too -- specifically, I'm not sure whether tensorflow works on PyPy (I'm about to try it out -- EDIT: now I've seen #252, which I might try to help on) but not having sdists uploaded makes that harder to try without hunting down this repository.", "comments": ["@caisq Can you comment?  I'm unfamiliar with sdists.", "I assume by sdist you assume source distribution to be built during `pip install`?\r\nThat might be tricky, as we rely on bazel. Bazel is quite heavy, and has limited os support, so it still wont work on many OSs.", "https://github.com/tensorflow/tensorflow/issues/6540 may be relevant", "#6540 appears mostly to be about eggs, which yeah I don't think tensorflow should have.\r\n\r\n@gunan I'm still trying to understand tensorflow's logistics (I'm mostly focused on directly hitting #252 but I've certainly managed to build it via bazel by now) but my first assumption would have been that an sdist would have invoked the requisite bazel command to build itself (under the assumption that bazel is already installed, which tends to happen for sdists, you rely on the fact that the user has already done all the external non-Python work)\r\n\r\nBasically, I would have imagined it to work the same as it does right now, just with one less manual step of having to clone the tensorflow source to get at the setup.py, ideally I'd just install bazel, then run `pip install`, and if tensorflow didn't upload a wheel compatible with my system, pip (setuptools) should invoke whatever tool tensorflow uses to build one (and if it succeeds, great).", "Unfortunately I think this is too hard at present given the complexities of bazel.  Closing for now."]}, {"number": 7285, "title": "AttributeError: module 'tensorflow' has no attribute 'Variable'", "body": "I run project from here=> https://github.com/david-gpu/deep-makeover\r\nI installed tensorflow by using Anaconda, then I run python3 dm_main.py --run train \r\n\r\nThen I have this problem: \r\nFile \"dm_main.py\", line 18, in <module>\r\n    import dm_model\r\n  File \"/Users/oteldanagul/Downloads/deep-makeover-master/dm_model.py\", line 5, in <module>\r\n    import dm_arch\r\n  File \"/Users/oteldanagul/Downloads/deep-makeover-master/dm_arch.py\", line 10, in <module>\r\n    _glbl_is_training = tf.Variable(initial_value=True, trainable=False, name='glbl_is_training')\r\nAttributeError: module 'tensorflow' has no attribute 'Variable'\r\n", "comments": ["What version of TensorFlow are you using?  What's `dir(tf)`?", "@girving r0.12\r\n@girving \r\nI imported tensorflow as tf\r\ndir(tf)\r\n['__doc__', '__loader__', '__name__', '__package__', '__path__', '__spec__']", "How are you installing TensorFlow?  Somehow you've managed to get a TF distribution with nothing in it.", "I remember things like this happening in windows before.\r\nWhat operating system are you using?\r\nWhat command did you use to install?", "@gunan  @girving  thanks guys, I solved my problem by reinstalling using by pip install tensorflow )\r\nI use OSX", "Hi guys, \r\nI've just installed tensorflow using the next command:\r\n\r\npython -m pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.1-cp35-cp35m-win_amd64.whl\r\n\r\nMy python:\r\nPython 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32\r\n\r\nOS: Windows 10 Pro\r\n\r\nMy code:\r\nimport tensorflow as tf\r\nprint(dir(tf));\r\nW = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\r\n\r\nResult:\r\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'tf']\r\nTraceback (most recent call last):\r\n  File \"C:/Users/Ros/Desktop/Tmp/tensorflow.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:/Users/Ros/Desktop/Tmp\\tensorflow.py\", line 3, in <module>\r\n    W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\r\nAttributeError: module 'tensorflow' has no attribute 'Variable'", "@gunan What extra information would you need to debug this?", "Calling in windows experts:\r\n@mrry @guschmue @vit-stepanovs \r\n\r\n@rsaliy What does the installation command output look like?\r\nOn windows, we have seen cases where pip/conda commands succeeded, but the logs told the installations were failed. \r\nWhen installing, were you in a terminal that has admin rights?\r\nWithout that, the installation may fail because you wont be able to write to system folders.\r\n\r\nAlso, to avoid some pip issues, could you first run:\r\n```\r\npython -m pip install --upgrade --ignore_installed pip setuptools\r\n```\r\n\r\nYou can also try using the flag `--ignore_installed` when installing tensorflow to see if it helps.", "@gunan \r\n>What does the installation command output look like?\r\n\r\nI've lost it, because started to train model:\r\npython -m tensorflow.models.image.mnist.convolutional\r\n\r\nI don't remember nothing special.\r\nModel was successfully trained\r\n\r\nMinibatch loss: 1.595, learning rate: 0.006302\r\nMinibatch error: 0.0%\r\nValidation error: 0.8%\r\nStep 8500 (epoch 9.89), 354.0 ms\r\nMinibatch loss: 1.616, learning rate: 0.006302\r\nMinibatch error: 1.6%\r\nValidation error: 0.9%\r\nTest error: 0.8%\r\n\r\n>When installing, were you in a terminal that has admin rights?\r\n>Without that, the installation may fail because you wont be able to write to system folders.\r\n\r\nI wasn't, but python is installed in C:\\Users\\Ros\\AppData\\Local\\Programs\\Python\\Python35 which doesn't require admin rights.", "Rename your script from tensorflow.py to example.py ... 'import tensorflow' will import your own script, not tensorflow.\r\n\r\nC:\\temp>python tensorflow.py\r\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'tf']\r\nTraceback (most recent call last):\r\n  File \"tensorflow.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\temp\\tensorflow.py\", line 3, in <module>\r\n    W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\r\nAttributeError: module 'tensorflow' has no attribute 'Variable'\r\n\r\nC:\\temp>ren tensorflow.py example.py\r\n\r\nC:\\temp>python example.py\r\n['AggregationMethod', 'Assert', 'AttrValue', 'COMPILER_VERSION', 'ConditionalAccumulator', 'ConditionalAccumulatorBase', 'ConfigProto', 'DType', 'DeviceSpec', 'Dimension', 'Event', 'FIFOQueue', 'FixedLenFeature', 'FixedLenSequenceFeature', 'FixedLengthRecordReader', 'GIT_VERSION', 'GPUOptions', 'GRAPH_DEF_VERSION', 'GRAPH_DEF_VERSION_MIN_CONSUMER', 'GRAPH_DEF_VERSION_MIN_PRODUCER', 'Graph', 'GraphDef', 'GraphKeys', 'GraphOptions', 'HistogramProto', 'IdentityReader', 'IndexedSlices', 'InteractiveSession', 'LogMessage', 'NameAttrList', 'NoGradient', 'NodeDef', 'NotDifferentiable', 'OpError',\r\n", "@guschmue \r\nThanks, fixed :)", "@rsaliy  could you tell me how to fix it,I meet the same problem with you.", "Not sure if this is related but I am getting \r\n\r\n```python\r\nimport tensorflow.python.keras as keras\r\n## AttributeError: module 'tensorflow' has no attribute 'python'\r\n```\r\n\r\nbut importing specific objects does work:\r\n\r\n```\r\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\r\n```\r\n\r\nThis is ubuntu with tf 1.4.0 compiled from source. Any pointers? ", "The following should work for me on 1.4:\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom keras.layers import Input, Dense\r\n```\r\nPer 1.4 it was\r\n```\r\nfrom tensorflow.contrib import keras\r\n```", "In my case the solution was:\nRename your script from tensorflow.py to example.py ... 'import tensorflow'\nwill import your own script, not tensorflow.\n\nOn Thu, Dec 21, 2017 at 6:05 PM, Guenther Schmuelling <\nnotifications@github.com> wrote:\n\n> The following should work for me on 1.4:\n>\n> import tensorflow as tf\n> from tensorflow import keras\n> from keras.layers import Input, Dense\n>\n> Per 1.4 it was\n>\n> from tensorflow.contrib import keras\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7285#issuecomment-353388283>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ARseJ_UR7t_BmfMTE6ZmRBH0Dxvhg_6fks5tCoG3gaJpZM4L4A_d>\n> .\n>\n", "Thanks. `python` and `core` are [explicitly deleted](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/__init__.py#L40) from the tensorflow namespace. Not sure why that was needed but it is surely unexpected.", "@rsaliy Same here!! Thanks", "Weights_11 = tf.Variable(tf.random.normal[1,10])\r\nbiases_L1 = tf.Variable(tf.zeros(1,10))\r\nWx_plus_b_L1 = tf.matmul(x,Weights_11) + biases_L1\r\nL1 = tf.nn.tanh(Wx_plus_b_L1)\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"G:/workspace/PycharmProjects/tensorflow/py_05_linear_regresion.py\", line 14, in <module>\r\n    Weights_11 = tf.Variable(tf.random.normal[1,10])\r\nAttributeError: module 'tensorflow' has no attribute 'random'", "I think it is ```tf.random_normal()```", "Hi, I installed \r\ntensorflow 1.5.0 \r\ntenseorflow-gpu 1.5.0\r\nCUDA 9\r\ncudnn 7\r\npython 3.6\r\npip 18\r\nBut when i try to import tensorflow i found this errors :\r\n\r\n(base) C:\\Users\\Mouna Nouira>py\r\nPython 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nAttributeError: module 'tensorflow' has no attribute '__version__'\r\n\r\ncan you help me to resolved this problem please\r\n\r\n![capturetensorflow](https://user-images.githubusercontent.com/44030099/48544136-47764480-e8c3-11e8-9154-87841373e097.PNG)\r\n", "> On windows, we have seen cases where pip/conda commands succeeded, but the logs told the installations were failed.\r\n\r\nThis was the problem for me! Thanks a lot for the advice. The tensorflow-gpu module was installed with pip showing no error messages, but did not work to invoke any methods in the module.\r\n\r\nI reinstalled with Anaconda Promt (as administrator) and the tensorflow module is now usuable as expected. \r\n\r\n@m6hichri thas looks like my problem. Try to reinstall as administrator!", "thanks,it's fixed\r\n\r\n", "> \r\n> \r\n> @gunan @girving thanks guys, I solved my problem by reinstalling using by **pip install tensorflow** )\r\n> I use OSX\r\n\r\nWorked for me . I'm on windows 10", "> In my case the solution was: Rename your script from tensorflow.py to example.py ... 'import tensorflow' will import your own script, not tensorflow.\r\n> [\u2026](#)\r\n> On Thu, Dec 21, 2017 at 6:05 PM, Guenther Schmuelling < ***@***.***> wrote: The following should work for me on 1.4: import tensorflow as tf from tensorflow import keras from keras.layers import Input, Dense Per 1.4 it was from tensorflow.contrib import keras \u2014 You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub <[#7285 (comment)](https://github.com/tensorflow/tensorflow/issues/7285#issuecomment-353388283)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ARseJ_UR7t_BmfMTE6ZmRBH0Dxvhg_6fks5tCoG3gaJpZM4L4A_d> .\r\n\r\nthis works for me", "For me this error was caused by using tf V2 with a code written in tf V1\r\nI replaced ``` tf.get_variable```  with ```tf.compat.v1.get_variable``` and it worked", "> I replaced ` tf.get_variable` with `tf.compat.v1.get_variable` and it worked\r\n\r\nThis was the issue for me too. I found this useful https://stackoverflow.com/a/55872941/8205650\r\n\r\n", "I am facing this error please help me\r\n  decoder_embeddings_matrix = tf.variable(tf.random_uniform([questions_num_words +1, decoder_embedding_size], 0, 1))\r\n\r\nAttributeError: module 'tensorflow' has no attribute 'variable'", "Add a capital letter"]}, {"number": 7284, "title": "Any clear way to restore saved network and parameters?", "body": "I'm using tensorflow-gpu0.12.1 on a linux-minit18 machine.\r\n\r\nI found it is very hard to restore the saved network and its parameters. The closest answer is \r\n\r\nhttp://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python\r\n\r\nand \r\n\r\nhttps://www.tensorflow.org/api_docs/python/state_ops/exporting_and_importing_meta_graphs\r\n\r\nI followed the example in the above link using tf.train.import_meta_graph, but it didn't work.\r\n\r\n```\r\nwith tf.Session() as sess:\r\n  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')\r\n  new_saver.restore(sess, 'my-save-dir/my-model-10000')\r\n```\r\n\r\n\r\n\r\nAll I want is something like:\r\n\r\ntraining ...\r\nsave_model(model_file)\r\nsave_parameters(par_file)\r\n...\r\n\r\npredicting...\r\nmodel = load_model(model_file)\r\nmodel.load_parameters(par_file)\r\n...\r\n\r\nI found similar implementations in Keras or Tensorlayer, etc. I like them because they split the training and predicting tasks and make them independant. In predicting task, I don't have to find and call the same def_network function in training task, which is boring and error prone. \r\n\r\n@yaroslavvb says it is on the roadmap, please make it faster. I'd like to test it ASAP!\r\n\r\nMany thanks!\r\nBen\r\n\r\n", "comments": ["Please ask another question on Stackoverflow if the previous questions do not suffice.  The structure you want is doable in current TensorFlow, but Github issues are only for bug reports and feature requests.", "Any link so I can have a try?\r\n\r\nThanks!", "The metagraph importing is working as far as I know. You need to give more information in the SO question. @benwu232 ", "I wrote two examples \r\n[tf_save.py](https://gist.github.com/benwu232/6b90d6571cc19c5ebb5294919d9f677d) and [tf_restore.py](https://gist.github.com/benwu232/73d95d930c5e1f1555acc91de2a27bf8). \r\n\r\nIn tf_save.py, collection is used to save the useful variables. And in tf_restore.py, it is used to restore the useful varibales, such as inputs, outputs, etc.\r\n\r\nConclusion is the apis are right, but you need to use something like collection to save the variables you need to restore, such as inputs and outputs. It took me quite a time to understand this. Hope this will help others.\r\n\r\nThanks!\r\n", "@benwu232 For future reference, it would probably have helped others more if you'd asked and then asked this question on StackOverflow."]}, {"number": 7283, "title": "\u201cmodule has no attribute 'learn' \u201d", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nhttps://github.com/tensorflow/tensorflow/issues/6607\r\n\r\n### Environment info\r\nOperating System:\r\n Windows 7 Enterprise\r\n\r\n### If possible, provide a minimal reproducible example \r\n\r\n1. I have download and installed the windows docker with:\r\ndocker run -p 8888:8888 -e \"PASSWORD=****\" -d --name tensorflow b.gcr.io/tensorflow-udacity/assignments\r\n\r\n2. I run the docker with a jupyter notebook:\r\ndocker start tensorflow (and then navigate: 192.168.99.100:8888)\r\n\r\n3. The code fails at the beggining:\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport datetime\r\n**from tensorflow.contrib import learn**\r\n\r\n---------------------------------------------------------------------------\r\n**ImportError**                               Traceback (most recent call last)\r\n<ipython-input-6-9bbc1f0d7c04> in <module>()\r\n      8 import datetime\r\n      9 # import tensorflow.contrib as tfc\r\n---> 10 from tensorflow.contrib import learn\r\n     11 \r\n     12 # Parameters\r\n\r\n**ImportError: cannot import name learn**\r\n\r\n### What other attempted solutions have you tried?\r\nReinstalling everything and changing tensorflow.contrib by tf.contrib.\r\n\r\nWhat can I do?\r\nHelp is appreciated.\r\n", "comments": ["If you import just `tensorflow as tf`, does `tf.contrib.learn` work?", "It doesn\u00b4t:\r\n\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-2-2c24d9475067> in <module>()\r\n     51 # Build vocabulary\r\n     52 max_document_length = max([len(x.split(\" \")) for x in x_text])\r\n---> 53 vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)\r\n     54 x = np.array(list(vocab_processor.fit_transform(x_text)))\r\n     55 \r\n\r\n**AttributeError: 'module' object has no attribute 'learn'**\r\n", "@martinwicke Thoughts?  This is pretty weird.  Could it be related to this even weirder issue?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/7285", "contrib is lazy-loaded to avoid some contrib project breaking your TF installation. However, \r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.contrib.learn)\r\n```\r\n\r\nshould definitely work. \r\n\r\n@pitercius, what code generates the error above?", "It should work but it doesn\u00b4t:\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-efeec648fb66> in <module>()\r\n      1 import tensorflow as tf\r\n----> 2 print(tf.contrib.learn)\r\n\r\n**AttributeError: 'module' object has no attribute 'learn'**\r\n\r\n@martinwicke , the failing part of the code is:\r\n\r\n```\r\n####################################################################################################\r\n## Train\r\n####################################################################################################\r\nimport tensorflow as tf\r\n# print(tf.contrib.learn)\r\nimport numpy as np\r\nimport os\r\nimport time\r\nimport datetime\r\n# from tensorflow.contrib import learn\r\n\r\n# Parameters\r\n# ==================================================\r\n\r\n# Data loading params\r\ntf.flags.DEFINE_float(\"dev_sample_percentage\", 0.1, \"Percentage of the training data to use for validation\")\r\ntf.flags.DEFINE_string(\"data_file\", \"./DAT_data_file.txt\", \"Data source for the data.\")\r\ntf.flags.DEFINE_string(\"target_file\", \"./DAT_target_file.txt\", \"Data source for the target.\")\r\n\r\n# Model Hyperparameters\r\ntf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\r\ntf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\r\ntf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\r\ntf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\r\ntf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\r\n\r\n# Training parameters\r\ntf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\r\ntf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\r\ntf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\r\ntf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\r\ntf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\r\n# Misc Parameters\r\ntf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\r\ntf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\r\n\r\nFLAGS = tf.flags.FLAGS\r\nFLAGS._parse_flags()\r\nprint(\"\\nParameters:\")\r\nfor attr, value in sorted(FLAGS.__flags.items()):\r\n    print(\"{}={}\".format(attr.upper(), value))\r\nprint(\"\")\r\n\r\n\r\n# Data Preparation\r\n# ==================================================\r\n\r\n# Load data\r\nprint(\"Loading data...\")\r\nx_text, y = load_data_and_labels(FLAGS.data_file, FLAGS.target_file)\r\n\r\n# Build vocabulary\r\nmax_document_length = max([len(x.split(\" \")) for x in x_text])\r\nvocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_document_length)\r\nx = np.array(list(vocab_processor.fit_transform(x_text)))\r\n```\r\n\r\nIn fact, the code is essentially identical to https://github.com/dennybritz/cnn-text-classification-tf/blob/master/train.py  (where dependencies, like load_data_and_labels, are)\r\n\r\nIf anything else is needed, don\u00b4t hesitate to ask", "Which version of Tensorflow is that?\u200b\n", "I get it with:\r\ndocker run -p 8888:8888 -e \"PASSWORD=****\" -d --name tensorflow b.gcr.io/tensorflow-udacity/assignments \r\n\r\nCould be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/Dockerfile ?\r\n\r\n", "@vincentvanhoucke, I don't know much about that docker file, is it up to date?", "@martinwicke no, it hasn't been regenerated in a while. I'll put it on my queue. We need to do it after all the API changes.", "Looks like the current tf Docker image is 1.12.1, which doesn't have the latest API changes which have been incorporated in the assignments already. I'm going to wait until a newer one is created.", "I pushed a version based on 1.0.0 a few days ago."]}, {"number": 7282, "title": "Typo in TF-slim README", "body": "Looks like a s/metrics_to_values/names_to_values/g was missed when taking this from the metrics README :).", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it.", "CLAs look good, thanks!\n\n<!-- ok -->", "PR merged. Thanks, @bradyz "]}, {"number": 7281, "title": "Feature Request : Latent Dirichlet Allocation impementation using tensorflow(skflow)", "body": "I am trying to Topic Modelling (LDA) using tensorflow. But I could find any existing LDA implementation in TF learn (skflow) package. \r\n\r\nIs there any other way to implement Topic Modelling using tensorflow?\r\n\r\nIt would be helpful if LDA support is given in TF Learn (skflow) similar to the scikit-learn Latent Dirichlet Implementation.", "comments": ["LDA is probably not in scope for `tf.learn`, but I'll wait for @ilblackdragon to comment. ", "Even if there is any suitable example / implementation of LDA using tensorflow , it would be very helpful", "Since Keras model can be constructed in Tensorflow, you can refer Matthew Honnibal's keras model in below gihub repository:\r\nhttps://github.com/explosion/spaCy", "@gusdoe if you are talking about [this keras model](https://github.com/explosion/spaCy/blob/a5538d93d0cf00cdaa4ca0f61dd08cc817af8e0e/examples/deep_learning_keras.py) - it's not LDA, but LSTM model for sentiment analysis [which we have examples in TF/TF.Learn].\r\n\r\n@mahatosourav91 Probably the answer is late, I haven't seen implementation for TF.\r\nHere is implementation in Python that one can convert into TF: https://github.com/hannawallach/python-lda/blob/master/src/lda.py\r\n\r\nOne of the main issues is that TF likes to operate in batches, but LDA is mostly done on the whole corpus at the same time. If you corpus-level optimization, you can just implement the code from above in TF apis. Otherwise one need to play around to get it to work with batches.\r\n\r\n@girving We would still accept a PR if somebody implemented a good version of LDA for TF.Learn and contributed it, wouldn't we?", "@ilblackdragon I think it might be a better fit for a separate repository or models.  As you say: if it's unbatched, it has a different API.  If it's batched, it's new research that we don't want to support in core yet."]}, {"number": 7280, "title": "Split up saver test as it frequently times out.", "body": "@sherrym @concretevitamin Could you take a look to see if the split makes sense?", "comments": ["@gunan is there a particular reason why this is sent as a PR, not as an internal change first?", "It will be faster to get the PR into the release branch.", "@gunan, I don't feel that the current way of dividing the tests into two files based on test name is very clear. Some test class do not have the name \"Checkpoint\" in them, but they still use and/or test checkpoints. @sherrym and @concretevitamin may have better input on how best to do the division.", "Will bump the size of saver_test instead, have to push the release out today.\r\nSomeone else will have to revisit splitting up this test later."]}, {"number": 7279, "title": "resolve #6762 on ldconfig only available on root PATH", "body": "See Linux man page for [ldconfig(8)](https://linux.die.net/man/8/ldconfig), synopsis section:\r\n\r\n```bash\r\n/sbin/ldconfig [ -nNvXV ] [ -f conf ] [ -C cache ] [ -r root ] directory ...\r\n/sbin/ldconfig -l [ -v ] library ...\r\n/sbin/ldconfig -p\r\n```\r\n\r\nFormer pull request #6848 has been closed as @jowagner has not signed a CLA.", "comments": ["Can one of the admins verify this patch?", "This patch may break on some platforms. The FHS states that ldconfig is \"usually located in /usr/sbin\". The alternative location \"/sbin\" is described as optional.", "openSUSE and Debian: ok (/sbin)", "Can someone review this PR? Current master with CUDA still breaks on latest Debian stable.", "@tensorflow-jenkins test this please\r\n\r\nIf this passes all of our tested / supported platforms, it seems okay, but will let Gunhan review when he is free (he is out of the office for a bit though).  Or maybe @caisq  if he feels trongly", "@gunan Thanks for your help. I've changed the lines as suggested.", "@tensorflow-jenkins test this please", "An error message \"configure: line xyz: type: ldconfig: not found\" is printed on stderr if the command is not found in the line starting with \"if ! type ldconfig\".\r\n\r\nThere's a simple solution but as I cannot sign the CLA at this time, I cannot submit it. Hint: 5 characters.\r\n\r\nI can confirm, however, that the variable LDCONFIG_BIN is initialised as intended to \"/sbin/ldconfig\" for my login without ldconfig in PATH.\r\n\r\nTested on openSUSE Leap 42.2.", "@jowagner Thanks for the heads up. Not sure why some checks failed though.", "Jenkins, test this please.", "gpu build passed, so I'm calling this a win.", "That's my first PR merged into TF! Thank you guys a lot @vrv @gunan @jowagner for your guide and suggestion. \r\n\r\nReally remarkable moment to me and I hope I could submit more patches in the future.", "Thank you for incrementally coming up with a solution that is substantially different to my CLA-less PR and for taking the time to bring this over the finishing line.\r\n\r\n"]}, {"number": 7278, "title": "Feature request: separable convolutions in 3D", "body": "I would like a `tf.nn.separable_conv3d` identical to `tf.nn.separable_conv2d` except with separability between dimentions [1,2,3] and 4, for use with 3D CNNs.\r\n\r\nRationale: separable convolutions perform very well in 2D (see: Xception architecture https://arxiv.org/abs/1610.02357 ).  In 3D, the number of parameters grows even faster for non-separable convolutions, so the reduction in parameters from using separable convolutions would be relatively even bigger.  This is one of the reasons 3D networks tend to have a simpler architecture than 2D.  \r\n\r\nNecessity: There doesn't seem to be any way to implement this other than in the TF core. (Suggestions?)", "comments": ["@aizvorski TensorFlow's 2-D separable convs are built out of normal convs and depthwise convs.  The first bit already exists in 3-D.  The second bit can be implemented in Python by splitting, but it's slower than a native op (this is how separable convs in 2D used to work, I think).  We'd be happy to accept a native 3-D version of depthwise conv, but I don't know of plans to do it ourselves.  I also be happy to give some more detail on the no-new-op version of separable conv if you want, though of course it'll be less performant than a native version.", "@girving Thank you!  Yes I would really like some detail on how to do the no-new-op version of separable conv.  The depthwise part seems easy, just with a 1x1x1xN kernel; I couldn't figure out a good way to do the pure spatial conv. I can try to do a native version later if this shows good performance.", "@aizvorski Here's the commit that turned the depthwise conv native: https://github.com/tensorflow/tensorflow/commit/975094c8382ee15dad434278804cb096f9bac84f.  You should be able to do an analogous thing to the 2D case to make a non-native depthwise conv for 3D, and then a separable conv is just depthwise followed by normal conv.", "I second that, separable 3D convs are vital to export Xception type architectures to video and medical imaging. What bits of code would need to change to implement a native 3D version? \r\n\r\nAre there any plans to create a nn_ops.depthwise_conv3d_native in the near future?\r\n\r\nThanks in advance!", "@alexvicegrab I don't know of any plans, but we'd be happy to accept contributions.", "@alexvicegrab Also, as I noted above, it's possible to write a slower version of separable convolutions in pure Python.", "@girving True, but making a slower version (how much slower?) to some extent defeats the point of implementing separable convolutions, I was hoping to use (or create) a fast version that can take advantage of hardware acceleration, but I'm not sure how to go about it, since I can't seem access the native code of the nn_ops.depthwise_conv2d_native (so as to have an example of how this is implemented in 2D to guide me), unless I've been looking in the wrong places. Otherwise I'd be happy to have a go at trying to implement it :)", "Here's the native op: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/depthwise_conv_op.cc", "@chsigg Do you plan to implement separable conv3d or depthwise conv3d?", "Hey! Is anybody working on this?\r\n", "@AnishShah I'm not working on that project anymore, so I am not. Still think it's an important thing to do, but don't have the time or the nitty-gritty expertise to address it. But looks like you have made some leeway :)", "I also wish a separable convolution for 3D image. Signals in 3D is more difficult to learn, and training is slow. \r\n", "I'm attempting to continue what @AnishShah started. We'll see how it goes...\r\nhttps://github.com/nio1814/tensorflow/tree/depthwise_conv3d", "Hi @nio1814, can you tell us how it goes with this implementation?", "@nio1814 Do you plan to open a PR with your code?", "I think this will be quite strategic. Check CVPR 2018 accepted https://github.com/kenshohara/3D-ResNets-PyTorch/", "Google @s9xie and @murphyk have also a related work [Rethinking Spatiotemporal Feature Learning For Video Understanding](https://arxiv.org/abs/1712.04851)", "Any news about the implementation @nio1814?", "At first I was going to basically adapt the 2D code to 3D, but say in #12940 that the 2D version wasn't very fast. Maybe I'll try doing some benchmarks with r1.7 to see what the current status of the 2D version is.", "@nio1814 What about https://github.com/tensorflow/tensorflow/pull/17961#issuecomment-381127268? /cc @chsigg ", "I have found a temporally solution to this using tf.scan to iterate over the feature dimension.\r\nI think that this is not as efficient as a native function, but low level programming of tensorflow\r\nis too hard for me :-)\r\nI have checked it using the 2d depth wise convolution first. The extension to 3d is inmediate:\r\n\r\n```python\r\ndef tf_scan_depthwise_conv2d():\r\n#First check that we get the same results as with  tf.nn.depthwise_conv2d\r\n#using tf.nn.conv2d and tf.scan\r\n\r\n    def f( old,input):\r\n        x_sample = input[0]\r\n        kernel_sample = input[1]\r\n\r\n        y_sample = tf.nn.conv2d(x_sample,kernel_sample,[1,1,1,1],\"SAME\")\r\n        return y_sample\r\n    x_values = np.random.randn(4,5,5,3).astype('float32')\r\n    kernel_values = np.random.randn(2,2,3,2).astype('float32')\r\n\r\n    x = tf.constant(x_values)\r\n    kernel = tf.constant(kernel_values)\r\n\r\n    y = tf.nn.depthwise_conv2d(x,kernel,[1,1,1,1],\"SAME\")\r\n\r\n    kernel_scan = tf.transpose(kernel,[2,0,1,3])\r\n    kernel_scan = tf.expand_dims(kernel_scan,axis=3)\r\n    x_scan = tf.transpose(x,[3,0,1,2])\r\n    x_scan = tf.expand_dims(x_scan,axis=-1)\r\n\r\n    c = tf.scan(f, (x_scan,kernel_scan) ,initializer = tf.zeros((4,5,5,2)))\r\n\r\n    c = tf.transpose(c,[1,2,3,0,4])\r\n    c = tf.reshape(c,[4,5,5,6])\r\n    with tf.Session() as sess:\r\n         y_values,kernel_scan_values,x_scan_values,c_values  = sess.run([y,kernel_scan,x_scan,c])\r\n\r\n    #check input shapes for the scanned version\r\n    print(kernel_scan_values.shape)\r\n    print(x_scan_values.shape)\r\n    #check output shapes\r\n    print(c_values.shape)\r\n    print(y_values.shape)\r\n    #check values using depthwise_conv2d or the scanned version\r\n    print(y_values[0,0,0,:])\r\n    print(c_values[0,0,0,:])\r\n\r\ndef tf_scan_depthwise_conv3d():\r\n#Now extend the results to 3D\r\n    def f( old,input):\r\n        x_sample = input[0]\r\n        kernel_sample = input[1]\r\n\r\n        y_sample = tf.nn.conv3d(x_sample,kernel_sample,[1,1,1,1,1],\"SAME\")\r\n        return y_sample\r\n\r\n    x_values = np.random.randn(4,5,5,5,3).astype('float32')\r\n    kernel_values = np.random.randn(2,2,2,3,2).astype('float32')\r\n\r\n    x = tf.constant(x_values)\r\n    kernel = tf.constant(kernel_values)\r\n\r\n    kernel_scan = tf.transpose(kernel,[3,0,1,2,4])\r\n    kernel_scan = tf.expand_dims(kernel_scan,axis=4)\r\n    x_scan = tf.transpose(x,[4,0,1,2,3])\r\n    x_scan = tf.expand_dims(x_scan,axis=-1)\r\n    c = tf.scan(f, (x_scan,kernel_scan) ,initializer = tf.zeros((4,5,5,5,2)))\r\n    c = tf.transpose(c,[1,2,3,4,0,5])\r\n    c = tf.reshape(c,[4,5,5,5,6])\r\n\r\n    with tf.Session() as sess:\r\n         kernel_scan_values,x_scan_values,c_values  = sess.run([kernel_scan,x_scan,c])\r\n\r\n\r\n    print(kernel_scan_values.shape)\r\n    print(x_scan_values.shape)\r\n    print(c_values.shape)\r\n    print(c_values[0,0,0,0,:])\r\n```\r\n\r\n\r\n\r\n", "@nio1814 Check the performance with https://github.com/tensorflow/tensorflow/pull/17961#issuecomment-388372269 ", "Any updates for this? I test the code by @alalbiol and the run time results are shown as below:\r\n\r\n`normal conv2d 0.3061046600341797ms native time 0.29351091384887695ms scan time 1.5845508575439453ms`\r\n\r\nWas running on tensorflow version 1.13.0-dev20181017 on GTX 1080 TI, cuda 9.0, cudnn 7.", "Yes, any updates? Would be super useful to have a tf.nn.separable_conv3d function!", "For anyone using Keras, this implementation addresses this issue \r\n https://github.com/alexandrosstergiou/keras-DepthwiseConv3D", "> \r\n> \r\n> For anyone using Keras, this implementation addresses this issue\r\n> https://github.com/alexandrosstergiou/keras-DepthwiseConv3D\r\n\r\nNo, it doesn't. It implements just the first step of the Depthwise Separable Convolution.", "I have opened a PR for native 3D Group Convolutions which enable Depth-wise 3D Convolutions natively as well  #31492 ", "My commit was accepted. \r\n\r\nUsing the benchmark code [here](https://github.com/tensorflow/tensorflow/pull/31492#issuecomment-520063961)  and replacing group number to the channel number, you get about 2x speedup over manually concatenating the channels over 256 channels.\r\n\r\n```\r\n// With manual concatenation\r\nLoop=True, Forward: 15.444471641506786 itr/s\r\nLoop=True, Backward: 4.057799918417722 itr/s\r\n// Native group convolution\r\nLoop=False, Forward: 29.1905996890655 itr/s\r\nLoop=False, Backward: 8.75012744363749 itr/s\r\n```\r\nMaking it separable would mean executing another convolution over the channels.\r\n", "Cool. These are benchmarks on GPU or CPU?", "On a 2060 mobile RTX GPU ", "@danganea are you providing just the conv3d operation for using depthwise3d or a tf keras layer. \r\n\r\n\r\n> I have opened a PR for native 3D Group Convolutions which enable Depth-wise 3D Convolutions natively as well #31492\r\n\r\nAlso how would you use the depthwise3d portion?", "The tensorflow commit does not include a keras layer.\r\n\r\nThe separable operation is a depthwise part + a pointwise part. Meaning what you'd do is have a filter of shape (D,H,W,1,input_depth * depth_multiplier) which you use in a initial tf.nn.conv3d operation. This is the group convolution that does the depthwise part. There will be input_depth * depth_multiplier groups.  Generally it's easy to reason about if depth_multiplier == 1, where there will be as many groups as input channels.\r\n\r\nAfter this depthwise part, you'd do a pointwise kernel to get the desired final depth.\r\n\r\nIn any case, here's a preliminary keras layer that uses my operation, as a branch of an existing CPU implementation:\r\nhttps://github.com/danganea/keras-DepthwiseConv3D\r\n\r\nAlong with a gist of it in action: https://gist.github.com/danganea/01756783499ab3b9524bb495f0d1203b", "I tried your keras layer implimentation, but it seems to fail on simple example.\r\n```\r\nimport keras\r\nx = keras.layers.Input([64,224,224,3])\r\nkernel_size=3\r\nstrides=1\r\no = DepthwiseConv3D(kernel_size,\r\n                        strides=strides,\r\n                        padding='same',\r\n                        use_bias=False)(x)\r\n``` \r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1863   try:\r\n-> 1864     c_op = c_api.TF_FinishOperation(op_desc)\r\n   1865   except errors.InvalidArgumentError as e:\r\n\r\nInvalidArgumentError: Dimensions must be equal, but are 3 and 1 for 'depthwise_conv3d_3/Conv3D' (op: 'Conv3D') with input shapes: [?,64,224,224,3], [3,3,3,1,3].\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n9 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs)\r\n   1865   except errors.InvalidArgumentError as e:\r\n   1866     # Convert to ValueError for backwards compatibility.\r\n-> 1867     raise ValueError(str(e))\r\n   1868 \r\n   1869   return c_op\r\n\r\nValueError: Dimensions must be equal, but are 3 and 1 for 'depthwise_conv3d_3/Conv3D' (op: 'Conv3D') with input shapes: [?,64,224,224,3], [3,3,3,1,3].\r\n```\r\n", "@hollowgalaxy   looks like you're not using the version of Tensorflow that includes my commit. Are you?\r\n\r\nThis is precisely the error you get prior to my commit getting accepted. It complains that the filter input dimension (1) is different than the input channel number(3).", "I see that makes sense. Is there a nightly version that this commit is featured in?", "Hey! I also tried the keras layer implementation, but it gives the same error 'Dimensions must be equal, but are 13 and 32 for 'depthwise_conv3d_1/Conv3D''. I'm using tensorflow 1.6.0. Should I get another version? ", "@AbirAffes @hollowgalaxy  Unfortunately I don't know all the tensorflow branches in which this commit is integrated. \r\n\r\nI see the merge request and my name can be seen in this contributor list for this 1.15 version:\r\nhttps://github.com/tensorflow/tensorflow/releases/tag/v1.15.0-rc0 . I haven't tested that but it seems safe to say that this version at least should have it.\r\n\r\nA safe thing to make sure you are using the right version is to just build tensorflow yourself with the latest changes from master branch. \r\n\r\nOther than building Tensorflow directly yourself, I believe there might be some \"tf-nightly\" version that has my commit, but I haven't yet looked into it, as I just build  Tensorflow directly.", "Closing this issue, as the feature seems to have been implemented for both tf.keras and TensorFlow core. Thanks for the suggestion, @aizvorski, and for the add, @danganea! \ud83d\udc4d ", "> Closing this issue, as the feature seems to have been implemented for both tf.keras and TensorFlow core. Thanks for the suggestion, @aizvorski, and for the add, @danganea! \ud83d\udc4d\r\n\r\nWhich Keras and Tensorflow version do I need for this feature?", "> My commit was accepted.\r\n> \r\n> Using the benchmark code [here](https://github.com/tensorflow/tensorflow/pull/31492#issuecomment-520063961) and replacing group number to the channel number, you get about 2x speedup over manually concatenating the channels over 256 channels.\r\n> \r\n> ```\r\n> // With manual concatenation\r\n> Loop=True, Forward: 15.444471641506786 itr/s\r\n> Loop=True, Backward: 4.057799918417722 itr/s\r\n> // Native group convolution\r\n> Loop=False, Forward: 29.1905996890655 itr/s\r\n> Loop=False, Backward: 8.75012744363749 itr/s\r\n> ```\r\n> \r\n> Making it separable would mean executing another convolution over the channels.\r\n\r\nI am getting this error:\r\n\r\nValueError: Dimensions must be equal, but are 8 and 1 for 'depthwise_conv3d_4/Conv3D' (op: 'Conv3D') with input shapes: [?,14,14,14,8], [3,3,3,1,8].\r\n\r\n\r\nI am using:\r\nKeras:  2.3.0\r\nTensorflow:  2.0.0", "Can someobdy confirm that the separable_conv3d feature is implemented in the latest release of Tensorflow 2.1 ? I can't find it in the Tensorflow API docs (there is SeparableConv1D and SeparableConv2D, but not SeparableConv3D). Thanks !", "Can't find it either.", "@dynamicwebpaige can you reopen, as this feature seems to be missing in Tensorflow 2.1?", "In Keras, you can do depthwise separable convolution (i.e. what a `DepthwiseConv3D` layer would do, if implemented), by using the `Conv3D` layer with `groups=filters`, where `filters` is the number of channels in your input tensor (which equals the number of channels in your output tensor, too). The only thing that's slightly annoying is that you need to know your number of input channels when you create the layer, rather than when the model is built.\r\n\r\nTo achieve `SeparableConv3D`, you then just have to put a pointwise convolution layer after this depthwise separable convolution, for example `Conv3D` with a kernel size of `(1, 1, 1)` (though it might be more performant to wrap a `Conv2D` layer with kernel size `(1, 1)` in a `TimeDistributed` layer; both approaches do exactly the same thing).\r\n\r\nIn pure tensorflow can do depthwise separable convolution with `tf.nn.convolution`. Just pass a kernel that has 1 for the input channel dimension size. This is what `Conv3D` with `groups=filters` does under the hood.", "Thank you @hunse . That will have to do as a fix for now.\r\nI too would like this to be re opened as a feature request. I think both DepthwiseConv3D and SeparableConv3D are missing in tensorflow."]}, {"number": 7277, "title": "Improve docs for incompatible protobuf error", "body": "On Mac OS X, importing tensorflow can result in the error: \"TypeError: `__init__()` got an unexpected keyword argument 'syntax'\". As explained in the doc, this is caused by an incompatible protobuf version. The suggested fix, \"pip install --upgrade protobuf\", doesn't resolve the issue if the user has installed protobuf via Homebrew. This commit adds an example command to fix the error in this case.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "Signed the CLA", "CLAs look good, thanks!\n\n<!-- ok -->", "Looks like homebrew only has protobuf up to 3.0\r\nhttp://brewformulas.org/Protobuf\r\nOur pip packages require protobuf 3.1, and we are upgrading to 3.2.\r\nMaybe we should recommend against using homebrew installed protobuf?\r\n\r\n@jhseu wdyt?", "Homebrew actually has protobuf 3.2.0. That page is outdated. Tested on my mac:\r\nhttps://github.com/Homebrew/homebrew-core/blob/master/Formula/protobuf.rb\r\n\r\nThis change seems reasonable to me.", "Great, thanks for verifying!\r\nMerging."]}, {"number": 7276, "title": "Weights are nan, loss is computed fine and gradients are too small", "body": "I have been researching about this over the INTERNET for a very long time. But I still do not know where my problem lies. I am finetuning a siamese fashioned vgg. And the problem is that even after first iteration distance calcualted is fine, loss calculated is fine but the gradients are too small(which is a problem) and the weights that are updated are nan. I do not understand what the problem is. \r\nIn the network, I have removed fully connected layers and the output from the last pooling layer is the input to the distance function. Here is my code. I have only attached important snippets of the code which I hope help understand the architecture. \r\nPS: stackoverflow was of no help that is why I am posting it here. \r\nPPS: I have attached snapshot of gradients as well\r\n![screenshot from 2017-02-06 02-00-10](https://cloud.githubusercontent.com/assets/25512256/22630753/60a8760a-ec10-11e6-9a1f-d75ba573842b.png)\r\n\r\n\r\n[code.txt](https://github.com/tensorflow/tensorflow/files/753375/code.txt)\r\n\r\n\r\n", "comments": ["BTW, this list is for bugs and feature requests in TensorFlow. It is common to get NaN's during normal operation. There was probably no response on stackoverflow because such problems are often due to a subtle bug in network design or training, and requires significant effort to track down. If you think this is a bug in TensorFlow, please more specific details of which functionality of TF is misbehaving", "Makes sense. The problem is with the first two conv layers only. Latter ones are updated fine. Thank you for your reply, Ill take a look into design. "]}, {"number": 7275, "title": "Fixed extraction step when using data_root", "body": "This fixes an issues introduced in a81c968d7484b04ef6bffe6aef235d1d00c8a81a", "comments": ["Can one of the admins verify this patch?", "@mlucool I believe this is conflicting with https://github.com/tensorflow/tensorflow/pull/7366.", "Ah someone else fixed it before me \ud83d\udc4d "]}, {"number": 7274, "title": "Is it possible to call TensorBoard smooth function manually?", "body": "I have two arrays X and Y. \r\n\r\nIs there a function I can call in tensorboard to do the smooth? \r\n\r\nRight now I can do an alternative way in python like:\r\n```python\r\n    sav_smoooth = savgol_filter(Y, 51, 3)\r\n    plt.plot(X, Y)\r\n```\r\nBut I didn't get a similar results as shown in tensorboard by doing so. Is there a function I can call?  Could you provide an API that allow this kind of operation? \r\n\r\nThanks. ", "comments": ["@KaixiangLin I'm suspecting this function is implemented in javascript logic of TB so it would be hard to make available, but cc @dandelionmane just in case", "@yaroslavvb  Thanks for the comment.  \r\n\r\nIf directly call the function is not available right now, Could you briefly explain the method how tensorboard smooth the curve? @dandelionmane \r\n\r\nThanks a lot!", "Please ask questions like these on Stackoverflow (or scicomp).  Github issues are for feature requests and comments.", "@KaixiangLin You can find the smoothing logic [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/components/vz_line_chart/vz-line-chart.ts#L432)", "Thank you. "]}, {"number": 7273, "title": "fix docs formatting of tf.while_loop", "body": "The [docs](https://www.tensorflow.org/api_docs/python/control_flow_ops/control_flow_operations) of tf.while_loop were wrongly formatted:\r\n![0205-23 11 37](https://cloud.githubusercontent.com/assets/1381301/22627166/24fd63aa-eb83-11e6-8170-a535a2a8a882.png).\r\n\r\nBy looking at similar docs in `tf.cond`, this seems to be the fix.", "comments": ["@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}, {"number": 7272, "title": "CUDA_ERROR_OUT_OF_MEMORY", "body": "i tried to excute chabot application (DeepQA). but i found out error.\r\nthis is the problem screen.....\r\n\r\nE tensorflow/core/common_runtime/direct_session.cc:135] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY; total memory reported: 18446744073648275456\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 29, in <module>\r\n    chatbot.main()\r\n  File \"/home/deepl/Desktop/chatbot/DeepQA-master/chatbot/chatbot.py\", line 179, in main\r\n    log_device_placement=False)  # Too verbose ?\r\n  File \"/home/deepl/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1186, in __init__\r\n    super(Session, self).__init__(target, graph, config=config)\r\n  File \"/home/deepl/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 551, in __init__\r\n    self._session = tf_session.TF_NewDeprecatedSession(opts, status)\r\n  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\r\n    next(self.gen)\r\n  File \"/home/deepl/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 469, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.InternalError: Failed to create session.\r\n\r\n\r\ni don't know how to solve this problem.. \r\nif you know to solve this problem, please comment for me. thank you for reading.", "comments": ["It sounds like your network is requiring more memory than is available. This list is for bugs/features in TensorFlow itself, and your issue sounds like something on user end", "Today i executed this program. then, it worked very well.. I think that this program concerned about network temporarily. (case by case).  Now it work well, but I'm afraid that it doesn't work again..\r\nI'm gonna notice you that promgram's ouput. thank you for your comment! :)", "Closing since it sounds like the problem evaporated."]}, {"number": 7271, "title": "Fix build error, where nccl requires -lrt link option", "body": "/usr/bin/ld: bazel-out/local_linux-opt/bin/external/nccl_archive/libnccl.a(core.cu.o): undefined reference to symbol 'shm_unlink@@GLIBC_2.2.5'\r\n/usr/lib64/librt.so.1: error adding symbols: DSO missing from command line\r\n\r\nBuilding meets the error, that requires -lrt as link option for libnccl", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please", "What build environment is this for?", "The mac build failed.  I think on mac, you need to not use that flag.  So something like this for linkopts;\r\n\r\nlinkopts = select({\r\n        \"//tensorflow:darwin\": [ ],\r\n        \"//conditions:default\": [\"-lrt\"],\r\n    }", "Friendly ping for @buaaliyi ", "updated", "Jenkins, test this please", "Jenkins, test this please", "Fixed the mistake, can now handling different architectures.", "Jenkins, test this please", "Jenkins, test this please.", "Jenkins, test this please", "Fix a typo in curl.BUILD, which was introduced by merging and then caused build error.", "Jenkins, test this please", "The build failure of windows cmake tests due to the main upstream code.\r\n\r\n![image](https://cloud.githubusercontent.com/assets/876295/23053119/fc4b6bbc-f511-11e6-84c9-f29ce270ac28.png)\r\n", "Yup, existing failure.  @cwhipkey let me know if this is good to merge by approving", "Done\n\nOn Thu, Feb 16, 2017 at 9:15 PM, Vijay Vasudevan <notifications@github.com>\nwrote:\n\n> Yup, existing failure. @cwhipkey <https://github.com/cwhipkey> let me\n> know if this is good to merge by approving\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/7271#issuecomment-280554848>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AQw4weZ3JM1ccUwraeKchLFbBzGP3KwUks5rdSzpgaJpZM4L3chJ>\n> .\n>\n"]}, {"number": 7270, "title": "Failure in \"Linking CXX shared library libpywrap_tensorflow.so\" , with error info \"additional relocation overflows omitted from the output\"", "body": "I am using Cmake 3.5.1 to compile sourcecode in Ubuntu 16.04 with a ppc64le machine ( Power64 little endian ).  Python 2.7.12.  The compile is failed in  \"Linking CXX shared library libpywrap_tensorflow.so\". The error info is as bellows.   I try compiling source code both 0.10.0 and 0.12.0,  and get the same error.\r\nCould you give me a help please ? Thanks\r\n\r\n\r\n[ 98%] Running SWIG to generate Python wrappers\r\n[ 98%] Building CXX object CMakeFiles/pywrap_tensorflow.dir/pywrap_tensorflow.cc.o\r\n[ 98%] Linking CXX shared library libpywrap_tensorflow.so\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> > >::get_deleter()':\r\ntensor_array_ops.cc:(.text._ZNSt10unique_ptrIN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEESt14default_deleteIS6_EE11get_deleterEv[_ZNSt10unique_ptrIN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEESt14default_deleteIS6_EE11get_deleterEv]+0x28): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `std::tuple_element<1ul, std::tuple<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>*, std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> > > >::type& std::get<1ul, Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>*, std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> > >(std::tuple<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>*, std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> > >&)' defined in .text._ZSt3getILm1EJPN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEESt14default_deleteIS6_EEERNSt13tuple_elementIXT_ESt5tupleIJDpT0_EEE4typeERSE_[_ZSt3getILm1EJPN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEESt14default_deleteIS6_EEERNSt13tuple_elementIXT_ESt5tupleIJDpT0_EEE4typeERSE_] section in CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<bool const, 2, 1, long>, 16, Eigen::MakePointer>*) const':\r\ntensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKbLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<std::complex<double> const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<std::complex<double> const, 2, 1, long>, 16, Eigen::MakePointer>*) const':\r\ntensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKSt7complexIdELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS8_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKSt7complexIdELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS8_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<std::complex<float> const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<std::complex<float> const, 2, 1, long>, 16, Eigen::MakePointer>*) const':\r\ntensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKSt7complexIfELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS8_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKSt7complexIfELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS8_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> > >::_M_head(std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> > >&)':\r\ntensor_array_ops.cc:(.text._ZNSt11_Tuple_implILm1EJSt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKdLi2ELi1ElEELi16ENS1_11MakePointerEEEEEE7_M_headERS9_[_ZNSt11_Tuple_implILm1EJSt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKdLi2ELi1ElEELi16ENS1_11MakePointerEEEEEE7_M_headERS9_]+0x24): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `std::_Head_base<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> >, true>::_M_head(std::_Head_base<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> >, true>&)' defined in .text._ZNSt10_Head_baseILm1ESt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKdLi2ELi1ElEELi16ENS1_11MakePointerEEEELb1EE7_M_headERS9_[_ZNSt10_Head_baseILm1ESt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKdLi2ELi1ElEELi16ENS1_11MakePointerEEEELb1EE7_M_headERS9_] section in CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer>*) const':\r\ntensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKdLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKdLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer>*) const':\r\ntensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKfLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKfLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer>*) const':\r\ntensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKNS0_4halfELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS7_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKNS0_4halfELi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS7_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> >& std::__get_helper<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> >>(std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> >>&)':\r\ntensor_array_ops.cc:(.text._ZSt12__get_helperILm1ESt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKaLi2ELi1ElEELi16ENS1_11MakePointerEEEEJEERT0_RSt11_Tuple_implIXT_EJS9_DpT1_EE[_ZSt12__get_helperILm1ESt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKaLi2ELi1ElEELi16ENS1_11MakePointerEEEEJEERT0_RSt11_Tuple_implIXT_EJS9_DpT1_EE]+0x24): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> > >::_M_head(std::_Tuple_impl<1ul, std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> > >&)' defined in .text._ZNSt11_Tuple_implILm1EJSt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKaLi2ELi1ElEELi16ENS1_11MakePointerEEEEEE7_M_headERS9_[_ZNSt11_Tuple_implILm1EJSt14default_deleteIN5Eigen9TensorMapINS1_6TensorIKaLi2ELi1ElEELi16ENS1_11MakePointerEEEEEE7_M_headERS9_] section in CMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `std::default_delete<Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer> >::operator()(Eigen::TensorMap<Eigen::Tensor<signed char const, 2, 1, long>, 16, Eigen::MakePointer>*) const':\r\ntensor_array_ops.cc:(.text._ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKaLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_[_ZNKSt14default_deleteIN5Eigen9TensorMapINS0_6TensorIKaLi2ELi1ElEELi16ENS0_11MakePointerEEEEclEPS6_]+0x2c): relocation truncated to fit: R_PPC64_REL24 (stub) against symbol `operator delete(void*, unsigned long)@@CXXABI_1.3.9' defined in .text section in /usr/lib/gcc/powerpc64le-linux-gnu/5/libstdc++.so\r\nCMakeFiles/tf_core_kernels.dir/home/gaoshan/software/pkgbuild/tensorflow/tensorflow/tensorflow/core/kernels/tensor_array_ops.cc.o: In function `void std::_Construct<std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> > >, std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> > > >(std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> > >*, std::unique_ptr<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer>, std::default_delete<Eigen::TensorMap<Eigen::Tensor<unsigned char const, 2, 1, long>, 16, Eigen::MakePointer> > >&&)':\r\ntensor_array_ops.cc:(.text._ZSt10_ConstructISt10unique_ptrIN5Eigen9TensorMapINS1_6TensorIKhLi2ELi1ElEELi16ENS1_11MakePointerEEESt14default_deleteIS7_EEJSA_EEvPT_DpOT0_[_ZSt10_ConstructISt10unique_ptrIN5Eigen9TensorMapINS1_6TensorIKhLi2ELi1ElEELi16ENS1_11MakePointerEEESt14default_deleteIS7_EEJSA_EEvPT_DpOT0_]+0x60): **additional relocation overflows omitted from the output\r\ncollect2: error: ld returned 1 exit status\r\nCMakeFiles/pywrap_tensorflow.dir/build.make:1711: recipe for target 'libpywrap_tensorflow.so' failed\r\nmake[2]: *** [libpywrap_tensorflow.so] Error 1\r\nCMakeFiles/Makefile2:6759: recipe for target 'CMakeFiles/pywrap_tensorflow.dir/all' failed\r\nmake[1]: *** [CMakeFiles/pywrap_tensorflow.dir/all] Error 2\r\nMakefile:83: recipe for target 'all' failed**", "comments": ["@mrry Can you comment on this cmake issue?", "I don't think this error is specific to CMake, but rather the unsupported build platform. Unfortunately I don't have a `ppc64le` machine to reproduce the error, and it's not one of our supported platforms, so I'll have to mark this as \"community support\".\r\n\r\nIn case it's helpful, here are a couple of random suggestions:\r\n\r\n* Have you tried building with Bazel? (I'm not sure if Bazel is supported on this platform either.) Or the (simpler) Makefile build?\r\n* Can you try adding the `-mcmodel=large` compilation option? (as suggested [here](https://www.ibm.com/developerworks/community/wikis/home?lang=en#!/wiki/W51a7ffcf4dfd_4b40_9d82_446ebc23c550/page/IBM%20Advance%20Toolchain%20for%20PowerLinux%20Documentation))\r\n\r\n/cc @namrata-ibm, who has been building for PPC, in case this error looks familiar.", "I had not faced this error. As @mrry suggests, you could try building with Bazel(which has to be built first).\r\nAlso what is the gcc version being used?", "-mcmodel=large may not work. Try \"-Os\" option or try both. With the two options, it should work.", "Please try the suggestions and reopen if this is still an issue. Thanks."]}, {"number": 7269, "title": "Simplify cuDNN library handling in configure script.", "body": "cuda_configure.bzl now handles the more complicated operations.\r\nAlso, we were running into bugs with failure to handle filenames \"cudnn.dylib\" and \"cudnn.5.dylib\"\r\ncuda_configure can do that without any issues.\r\n\r\nAlso locally verified that local_config_cuda repository references libcudnn with version string in the name, thus rendering the removed code redundant.", "comments": ["@gunan, can you verify this CL with an experimental Mac GPU build?", "http://ci.tensorflow.org/job/tensorflow-pull-requests-mac-gpu/4/console\r\n\r\nlooks like it timed out, will adjust timeout and rerun.\r\nAs @vrv added this code, just want to get his final OK on the code, too.", "I don't think I wrote this code, but it's removing and simplifying a lot of code, so either this works for everyone, or we'll get a bug filed against us saying that we broke some corner case that we don't have the ability to test :(.", "mac gpu build going good so far:\r\nhttp://ci.tensorflow.org/job/tensorflow-pull-requests-mac-gpu/5/console\r\nWill handle saver_test flakiness now.\r\n\r\nJenkins, test this please."]}, {"number": 7268, "title": "C++ compilation of rule '@jemalloc//:jemalloc' failed:", "body": "Trying to install tensor flow on cluster from source. I have installed bazel\r\n[bazel release 0.4.4- (@non-git)], and I am using python 2.7.13 with pyenv. Upon trying to build the tensorflow pip wheel I am getting a compilation error:\r\n\r\nERROR: ~/.cache/bazel/_bazel/e924d9c3ba75314415252c6f4f93bb86/external/jemalloc/BUILD:10:1: C++ compilation of rule '@jemalloc//:jemalloc' failed: gcc failed: error executing command /opt/apps/compilers/gcc/4.8.2/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/opt/apps/compilers/gcc/4.8.2/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 38 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\r\n\r\nHas anyone experienced this? Is this a consequence of the warning I get from bazel for being on an NFS:\r\nWARNING: Output base '~/.cache/bazel/_bazel/e924d9c3ba75314415252c6f4f93bb86' is on NFS. This may lead to surprising failures and undetermined behaviour.\r\n\r\n\r\nMy gcc version is:\r\ngcc (GCC) 4.8.2\r\nCopyright (C) 2013 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n", "comments": ["Ultimately I disabled jemalloc when prompted during configuration by the tensorflow/configure file. TensorFlow seems to have compiled fine from there on.", "cc @jhseu who added jemalloc. @Montmorency can you specify your linux/bazel versions?", "@Montmorency Possibly unrelated, but I've met random problems using bazel build on NFS hosted home directory.\r\n\r\n```bash\r\nexport TEST_TMPDIR=\"/tmp\" # or some local directory that is not hosted on NFS\r\n```\r\n\r\nThis might help.", "@jhseu Can you comment?", "@Montmorency I tested against gcc 4.8.4 and it works for me.\r\n\r\nCan you copy the compilation error that you get before that error message? Also, can you try removing NFS as a potential issue by doing what @byronyi mentioned?", "Thanks for response!\r\nBazel\r\n```\r\n [bazel release 0.4.4- (@non-git)]\r\n```\r\nDistro\r\n```\r\n$cat /etc/issue\r\n  Scientific Linux release 6.6 (Carbon)\r\n  Kernel \\r on an \\m\r\n$uname -r\r\n  2.6.32-431.17.1.el6.x86_64\r\n$uname -i\r\n  x86_64\r\n$gcc --version\r\ngcc (GCC) 6.2.0\r\nCopyright (C) 2016 Free Software Foundation, Inc.\r\n\r\n```\r\nUpdate: \r\nI had a closer look and started using a more recent compiler. The NFS is not an issue. Closer inspection of the output pointed to failure to do with JEMALLOC_THP and some undefined variable(MADV_NOHUGEPAGE). I think the cluster has an older linux kernel. I manually removed these flags from the jemalloc/BUILD file and compilation completed without any further issues:\r\ndeleted:\r\n\"#undef JEMALLOC_THP\": \"#define JEMALLOC_THP\",\r\n\"#undef JEMALLOC_HAVE_SECURE_GETENV\": \"#define JEMALLOC_HAVE_SECURE_GETENV\",\r\n\r\nI removed the second flag when the build failed at linking stage I think this is related to an older version of openssl on the cluster which configure didn't seem to detect (getenv rather than secure_getenv). Training the models is fine with this build but I do still a strange glibc error when running model.predict_proba():\r\n\r\n*** glibc detected *** python: double free or corruption (!prev): 0x0000000001375da0 ***\r\n======= Backtrace: =========\r\n/lib64/libc.so.6[0x3168e75e66]\r\n/lib64/libc.so.6[0x3168e789b3]\r\n/lib64/ld-linux-x86-64.so.2(_dl_deallocate_tls+0x67)[0x31686112f7]\r\n/lib64/libpthread.so.0[0x316920675d]\r\n/lib64/libpthread.so.0[0x31692078ea]\r\n/lib64/libpthread.so.0(pthread_join+0xd4)[0x31692081f4]\r\n/opt/apps/compilers/gcc/6.2.0/lib64/libstdc++.so.6(_ZNSt6thread4joinEv+0x27)[0x7fbc4ea24a77]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x248c310)[0x7fbc51186310]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow6thread10ThreadPool4ImplD0Ev+0x144)[0x7fbc51160594]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow10FileSystem16GetMatchingPathsERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPSt6vectorIS6_SaIS6_EE+0x5ce)[0x7fbc511830de]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(_ZN10tensorflow3Env16GetMatchingPathsERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEPSt6vectorIS6_SaIS6_EE+0xab)[0x7fbc5117f7eb]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x986bfb)[0x7fbc4f680bfb]\r\n/users/k1511981/.pyenv/versions/t2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so(+0x9877c4)[0x7fbc4f6817c4]\r\n", "Ah yeah, that kernel is really old (from 2009!). I'm not sure we should try to support jemalloc with that kernel when there's a reasonable workaround.\r\n\r\nThe double free issue is unrelated because it happens even when disabling jemalloc (from your other comment on #6968). Also, it's in code that's unaffected by jemalloc. Looking at the stack trace, it's crashing in pthread_join in deallocating thread-local storage. Seems unlikely to be a TensorFlow issue (possibly also related to the old linux kernel?)\r\n\r\nClosing the issue out as intended behavior.", "I haven't tested, but some searching indicates that jemalloc should build as long as the Linux kernel version is >= 2.6.38, otherwise it needs to be disabled.", "I removed bazel cache, did bazel clean, disabled jemalloc via configure and still get the following error (and yes my kernel is 2.6.32)\r\n\r\nERROR: /home/ebice/.cache/bazel/_bazel_ebice/975e0509e630426b34ea61d02aa8b898/ex                                                                                                             ternal/jemalloc/BUILD:10:1: C++ compilation of rule '@jemalloc//:jemalloc' faile                                                                                                             d: gcc failed: error executing command /opt/rh/devtoolset-6/root/usr/bin/gcc -U_                                                                                                             FORTIFY_SOURCE -fstack-protector -Wall -B/opt/rh/devtoolset-6/root/usr/bin -B/us                                                                                                             r/bin -Wunused-but-set-parameter -Wno-free-nonheap-object ... (remaining 38 argu                                                                                                             ment(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Pr                                                                                                             ocess exited with status 1.\r\nexternal/jemalloc/src/pages.c: In function 'je_pages_huge':\r\nexternal/jemalloc/src/pages.c:203:30: error: 'MADV_HUGEPAGE' undeclared (first u                                                                                                             se in this function)\r\n  return (madvise(addr, size, MADV_HUGEPAGE) != 0);", "@edi-bice Unfortunately, I don't have access to any machine with such old Linux kernels to test. That shouldn't happen if jemalloc is really disabled as far as I can tell.", "\"really disabled\" is the keyword. Apparently bazel clean did not really clean everything. In addition to .bazelrc the file .tf_configure.bazelrc remained even after a bazel clean and inside there jemalloc=true despite configure output stating \"jemalloc disabled\".", "Did you rerun ./configure? That file is deleted and updated again here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/configure#L167"]}, {"number": 7267, "title": "Switch to use tf.losses.softmax_cross_entropy in examples", "body": "`tf.contrib.losses` is deprecated. This only updates the examples. Leaving the actual removal of the`tf.contrib.losses` module to you guys internally. ", "comments": []}]