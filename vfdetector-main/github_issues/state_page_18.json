[{"number": 52323, "title": "TFlite model is too slow when Output Buffer is Direct().", "body": "**System information**\r\n- tensorflow-lite-gpu:2.7.0-rc0\r\n- Are you willing to contribute it- Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI'm trying to use output buffer as```output =  ByteBuffer.allocateDirect(4 * LARGE_DIMENSIONS)\r\n                .let {\r\n                    it.order(ByteOrder.nativeOrder())\r\n                    it.asFloatBuffer()\r\n                }```\r\n               with a buffer of large dimensions. The problem is that using direct allocation make the model run half of the velocity. I believe this because current tflite api is only able to use `copyTo`  instead of what happens with direct memory allocation in `setTo`.\r\nCan you confirm that this is the expected behavior? \r\n", "comments": ["@filipeferreiradsr ,\r\nCan you please share a reproducible code that supports your statement so that the issue can be easily understood? Thanks!", "```\r\nclass InferencerTFLite internal constructor(\r\n    private val modelConfig: ModelConfig,\r\n    options: Interpreter.Options,\r\n    modelBytes: ByteBuffer\r\n) : Inferencer<Mat, FloatBuffer> {\r\n\r\n\r\n    private val interpreter: Interpreter\r\n\r\n    private val output: FloatBuffer\r\n\r\n    private val input: FloatBuffer\r\n\r\n    init {\r\n        interpreter = Interpreter(modelBytes, options)\r\n\r\n        input = ByteBuffer.allocateDirect(\r\n            modelConfig.input.width * modelConfig.input.height * modelConfig.input.channels * modelConfig.input.channelSize\r\n        )\r\n            .let {\r\n                it.order(ByteOrder.nativeOrder())\r\n                it.asFloatBuffer()\r\n            }\r\n\r\n        output =\r\n            ByteBuffer.allocateDirect(4 * modelConfig.output.height * modelConfig.output.width * modelConfig.output.channels)\r\n                .let {\r\n                    it.order(ByteOrder.nativeOrder())\r\n                    it.asFloatBuffer()\r\n                }\r\n\r\n    }\r\n\r\n    override fun inference(inputFrame: Mat): FloatBuffer {\r\n        interpreter.run(\r\n            createInput(inputFrame),\r\n            createOutput()\r\n        )\r\n        return output\r\n    }\r\n\r\n    override fun close() {\r\n        interpreter.close()\r\n    }\r\n\r\n    private fun createInput(inputFrame: Mat): FloatBuffer =\r\n        input.apply {\r\n            rewind()\r\n            put(inputFrame.toFloatArray())\r\n        }\r\n\r\n    private fun createOutput(): FloatBuffer =\r\n        output.apply {\r\n            rewind()\r\n        }\r\n\r\n}\r\n```\r\n\r\nIf use \r\n\r\n`\r\noutput = FloatBuffer.allocate(modelConfig.output.height * modelConfig.output.width * modelConfig.output.channels) `\r\n\r\nThe model runs 2x the velocity"]}, {"number": 52310, "title": "copy_to_device too slow", "body": "now the order of my dataset:\r\n```TFRecordDataset->batch->map->copy_to_device```\r\nmap will output lots of small tensor, small tensor memcpyH2D is too slow. data by nsys:\r\n![image](https://user-images.githubusercontent.com/33950866/136652365-9639e001-c0b2-4ce0-9d3f-e02e73139d2a.png)\r\n\r\n```TFRecordDataset->batch->map->merge->copy_to_device->split``` maybe have a good performance.\r\n\r\nI can contribute this part of the code if necessary\r\n\r\n\r\n", "comments": ["In same data H2D test:\r\nsmall H2D:\r\n![image](https://user-images.githubusercontent.com/33950866/136657933-3abd4917-1fce-4433-bb1c-b256472b52cd.png)\r\n\r\nBig H2D:\r\n![image](https://user-images.githubusercontent.com/33950866/136657794-c833ec8f-2695-48a1-abd3-b61fca1f250a.png)\r\n\r\nH2D performance: BigH2D speed:18.8862 GB/s(13%) SmallH2D speed: 16.6752GB/s.\r\n"]}, {"number": 52298, "title": "ModifyGraphWithDelegate crashes when calling with XNNPack delegate", "body": "Hello, \r\n\r\nI want to run TF an a arm64 device with XNNPack enabled. I am using the C++ API. When calling `ModifyGraphWithDelegate` it segfaults.\r\n\r\nThis is how I load the interpreter and delegate:\r\n\r\n```\r\n    model_ = (tflite::FlatBufferModel::BuildFromBuffer(buffer_.get(), bufSize));\r\n    \r\n    if (!model_) {\r\n        std::cout << \"Could not load model.\";\r\n        return false;\r\n    }\r\n    \r\n    tflite::ops::builtin::BuiltinOpResolver resolver;\r\n    \r\n    TfLiteStatus status = tflite::InterpreterBuilder(*model_, resolver)(&tfinterpreter_);\r\n\r\n    TfLiteXNNPackDelegateOptions options = TfLiteXNNPackDelegateOptionsDefault();\r\n    tflite::Interpreter::TfLiteDelegatePtr delegate(\r\n      TfLiteXNNPackDelegateCreate(&options),\r\n         [](TfLiteDelegate* delegate) { TfLiteXNNPackDelegateDelete(delegate); });\r\n\r\n    \r\n    if (tfinterpreter_->ModifyGraphWithDelegate(std:move(delegate)) != kTfLiteOk) {\r\n        std::cout << \"XNNPACK Loading failed\";\r\n        return false;\r\n    }\r\n```\r\n\r\n`ModifyGraphWithDelegate` never returns. The crashlog does not tell me much:\r\n\r\n```\r\n10-08 11:46:31.049 17616 17616 F libc    : Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0 in tid 17616 (server), pid 17616 (server)\r\n10-08 11:46:31.106 17619 17619 E crash_dump64: unknown process state: t\r\n10-08 11:46:31.120 17619 17619 I crash_dump64: obtaining output fd from tombstoned, type: kDebuggerdTombstone\r\n10-08 11:46:31.122  1040  1040 I /system/bin/tombstoned: received crash request for pid 17616\r\n10-08 11:46:31.124 17619 17619 I crash_dump64: performing dump of process 17616 (target tid = 17616)\r\n10-08 11:46:31.126  2950  2950 D io_stats: !@   8,0 r 773089 29031960 w 2641492 116829328 d 176910 191244240 f 718246 1038251 iot 3371860 2240840 th 51200 51200 39664 pt 3530 inp 0 0 638962.480\r\n10-08 11:46:31.136 17619 17619 F DEBUG   : *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***\r\n10-08 11:46:31.136 17619 17619 F DEBUG   : Build fingerprint: 'samsung/a70qeea/a70q:10/QP1A.190711.020/A705FNXXU5BTF1:user/release-keys'\r\n10-08 11:46:31.136 17619 17619 F DEBUG   : Revision: '14'\r\n10-08 11:46:31.136 17619 17619 F DEBUG   : ABI: 'arm64'\r\n10-08 11:46:31.138 17619 17619 F DEBUG   : Timestamp: 2021-10-08 11:46:31+0200\r\n10-08 11:46:31.138 17619 17619 F DEBUG   : pid: 17616, tid: 17616, name: server  >>> /data/local/tmp/erver/server <<<\r\n10-08 11:46:31.138 17619 17619 F DEBUG   : uid: 2000\r\n10-08 11:46:31.138 17619 17619 F DEBUG   : signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0\r\n10-08 11:46:31.139 17619 17619 F DEBUG   : Cause: null pointer dereference\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     x0  0000000000000001  x1  0000007f1aa58ca8  x2  0000000000000002  x3  0000000000000300\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     x4  000000000000000f  x5  0000007fc6afa0e8  x6  0000007fc6af9fc0  x7  0000000000000001\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     x8  ffffffffffffffff  x9  0000000000000040  x10 0000007f1ab1e5f8  x11 0000007f1ab19990\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     x12 0000007f19480a00  x13 0000000000000000  x14 0000000000000000  x15 0000007f9da1316c\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     x16 0000000000000000  x17 0000007f1abbe8f8  x18 0000000000000080  x19 000000000000000f\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     x20 0000000000000000  x21 0000000000000000  x22 0000007fc6af9fc0  x23 000000000000007e\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     x24 0000000000000010  x25 0000007f1a54a238  x26 0000007fc6afa0e8  x27 0000007fa24de020\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     x28 000000000000000f  x29 0000007fc6af9ee0\r\n10-08 11:46:31.139 17619 17619 F DEBUG   :     sp  0000007fc6af9db0  lr  0000007f9d7499a0  pc  0000007f9d74da94\r\n10-08 11:46:31.143 17619 17619 F DEBUG   : \r\n10-08 11:46:31.143 17619 17619 F DEBUG   : backtrace:\r\n10-08 11:46:31.143 17619 17619 F DEBUG   :     NOTE: Function names and BuildId information is missing for some frames due\r\n10-08 11:46:31.144 17619 17619 F DEBUG   :     NOTE: to unreadable libraries. For unwinds of apps, only shared libraries\r\n10-08 11:46:31.144 17619 17619 F DEBUG   :     NOTE: found under the lib/ directory are readable.\r\n10-08 11:46:31.144 17619 17619 F DEBUG   :       #00 pc 000000000149fa94  /data/local/tmp/al_server/libserver.so\r\n```\r\n\r\n\r\nI compiled TF 2.4.3 with cmake:\r\n\r\n```\r\ncmake -DCMAKE_TOOLCHAIN_FILE=~/android-ndk-r19c/build/cmake/android.toolchain.cmake \\\r\n  -DANDROID_ABI=arm64-v8a -DTFLITE_ENABLE_XNNPACK=ON -DCMAKE_ANDROID_ARM_NEON=ON -DANDROID_ARM_NEON=ON ../tensorflow/lite\r\n```\r\n\r\n\r\nAny help would be much appreciated,\r\nThanks", "comments": []}, {"number": 52287, "title": "[TF-TRT] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Jetson AGX Xavier Jetpack Version 4.5.1 ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): nvidia container install\r\n- TensorFlow version (use command below): 7.1.3\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: CUDA 10.2 cuDNN 8.0 \r\n- GPU model and memory: Jetson AGX Xavier, 16GB\r\n\r\n<br>\r\n\r\n**Describe the current behavior**\r\n\r\nHi, while using TF-TRT, always I faced on an error `2021-10-04 00:54:11.478005: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.` when I called `converter.build(input_fn=my_input_fn)` method. (either the error raised at runtime when **not** call build() method for pre-build engine.)\r\n\r\n(image : trials, parameter tuning `minimum_segment_size` and so on.)\r\n![image](https://user-images.githubusercontent.com/46595649/136313733-2ab4dea4-7847-494e-a466-20bfc7701b1e.png)\r\n\r\nBut any of them couldn't remove that error message.\r\n\r\nMy entire source code wasn't different with any other example source codes.\r\n\r\n```python3\r\n# memory growing mode (already tried fixed gpu memory mode either.)\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(gpus[0], True)\r\n\r\n#https://github.com/tensorflow/tensorrt/issues/195\r\n#https://github.com/tensorflow/tensorrt/issues/200\r\nenv = os.environ.get('TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE')\r\nnew_env = os.environ['TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE'] = \"1\"\r\nprint(f'Setting environment variable... TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE={new_env}... (before {env})')\r\n\r\n# Conversion Parameters \r\nconversion_params = trt.TrtConversionParams(\r\n    precision_mode=QUANTIZATION_MODE, # 'INT8' or 'FP16'\r\n    max_workspace_size_bytes=MAX_GPU_TRTENGINE_SIZE_MB<<20, # 2048 ... etc.\r\n    minimum_segment_size=MINIMUM_SEGMENT_SIZE, # 10 ... etc.\r\n    maximum_cached_engines=MAXIMUM_CACHED_ENGINE, # 20 ... etc.\r\n    use_calibration=True # This argument is ignored if precision_mode is not INT8.\r\n    )\r\n\r\nconverter = trt.TrtGraphConverterV2(\r\n    input_saved_model_dir=SAVED_MODEL_DIR,\r\n    input_saved_model_tags=SAVED_MODEL_TAG_SET,\r\n    input_saved_model_signature_key=SAVED_MODEL_SIGNATURE_DEF,\r\n    conversion_params=conversion_params\r\n    )\r\n\r\nif QUANTIZATION_MODE.lower() == 'int8':\r\n    print('Convert with calibration')\r\n    converter.convert(\r\n        calibration_input_fn=lambda:input_fn(args) # args : my command line input\r\n        # input_fn returns numpy array [1, 320, 320, 3] shaped.\r\n        ) # Maybe converter.convert() works fine...?\r\nelse:\r\n    print('Convert without calibration')\r\n    converter.convert()\r\n\r\nprint('[Info] Convert complete.')\r\nprint('[Info] Now prebuild trt-engine.')\r\nconverter.build(input_fn=lambda:input_fn(args)) # here raises error message, but process doesn't shut down.\r\n\r\noptimized_saved_model_fulldir = parse_tftrt_path(args)\r\n\r\n# Save the model to the disk\r\nconverter.save(optimized_saved_model_fulldir)\r\nprint('[Info] writing savedmodel complete.')\r\n```\r\n\r\nI found that optimized model does not improving model inference speed, so I guess the error is not just a warning. Maybe It's a bug.\r\n\r\nThanks!\r\n\r\n<br>\r\n\r\n**Describe the expected behavior**\r\n\r\n- no warning\r\n- inference speed improvement\r\n\r\n<br>\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nCOLAB's TF-TRT is unstable.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nmodel signature\r\n```\r\n==============================\r\n2021-10-07 12:58:34.474165: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2\r\nThe given SavedModel contains the following tag-sets:\r\n'serve'\r\n\r\n2021-10-07 12:59:08.535907: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2\r\nThe given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\r\nSignatureDef key: \"__saved_model_init_op\"\r\nSignatureDef key: \"serving_default\"\r\n\r\n2021-10-07 12:59:41.622920: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2\r\nThe given SavedModel SignatureDef contains the following input(s):\r\n  inputs['input_1'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: (-1, -1, -1, 3)\r\n      name: serving_default_input_1:0\r\nThe given SavedModel SignatureDef contains the following output(s):\r\n  outputs['softmax'] tensor_info:\r\n      dtype: DT_FLOAT\r\n      shape: (-1, -1, -1, 9)\r\n      name: StatefulPartitionedCall:0\r\nMethod name is: tensorflow/serving/predict\r\n==============================\r\n```\r\n\r\nEntire program log description below\r\n```\r\n2021-10-07 12:58:32.487612: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\r\n2021-10-07 12:58:32.513042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 12:58:32.513711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: \r\npciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2\r\ncoreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s\r\n2021-10-07 12:58:32.514587: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2\r\n2021-10-07 12:58:32.515218: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.10\r\n2021-10-07 12:58:32.515752: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.10\r\n2021-10-07 12:58:32.516480: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\r\n2021-10-07 12:58:32.522466: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\r\n2021-10-07 12:58:32.535133: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\r\n2021-10-07 12:58:32.543363: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.10\r\n2021-10-07 12:58:32.546243: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-10-07 12:58:32.546781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 12:58:32.547194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 12:58:32.547471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0\r\n\r\nSetting environment variable... TF_DEBUG_TRT_ALLOW_INEFFICIENT_TRANSPOSE=1... (before None)\r\n2021-10-07 13:00:15.205272: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libnvinfer.so.7\r\nConvert with calibration\r\n2021-10-07 13:01:31.023349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:01:31.023722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: \r\npciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2\r\ncoreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s\r\n2021-10-07 13:01:31.024083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:01:31.024338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:01:31.024470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0\r\n2021-10-07 13:01:31.025038: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2\r\n2021-10-07 13:01:36.768804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-10-07 13:01:36.768996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-10-07 13:01:36.769069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n2021-10-07 13:01:36.769398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:01:36.769747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:01:36.769976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:01:36.770180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 18830 MB memory) -> physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2)\r\n2021-10-07 13:06:42.865817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:06:42.866173: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2021-10-07 13:06:42.866754: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\r\n2021-10-07 13:06:42.870918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:06:42.871261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: \r\npciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2\r\ncoreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s\r\n2021-10-07 13:06:42.871593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:06:42.871875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:06:42.872000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0\r\n2021-10-07 13:06:42.872134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-10-07 13:06:42.872198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-10-07 13:06:42.872245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n2021-10-07 13:06:42.872483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:06:42.872836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:06:42.873158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 18830 MB memory) -> physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2)\r\n2021-10-07 13:06:42.874915: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 31250000 Hz\r\n2021-10-07 13:06:47.236124: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1171] Optimization results for grappler item: graph_to_optimize\r\n  function_optimizer: Graph size after: 1216 nodes (901), 1824 edges (1509), time = 176.228ms.\r\n  function_optimizer: function_optimizer did nothing. time = 2.022ms.\r\n\r\n2021-10-07 13:07:39.201871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:07:39.202299: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\r\n2021-10-07 13:07:39.202699: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\r\n2021-10-07 13:07:39.210304: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:07:39.210650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1734] Found device 0 with properties: \r\npciBusID: 0000:00:00.0 name: Xavier computeCapability: 7.2\r\ncoreClock: 1.377GHz coreCount: 8 deviceMemorySize: 31.17GiB deviceMemoryBandwidth: 82.08GiB/s\r\n2021-10-07 13:07:39.211093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:07:39.211425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:07:39.211551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1872] Adding visible gpu devices: 0\r\n2021-10-07 13:07:39.211696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-10-07 13:07:39.211766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \r\n2021-10-07 13:07:39.211813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \r\n2021-10-07 13:07:39.212127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:07:39.212488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1001] ARM64 does not support NUMA - returning NUMA node zero\r\n2021-10-07 13:07:39.212722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 18830 MB memory) -> physical GPU (device: 0, name: Xavier, pci bus id: 0000:00:00.0, compute capability: 7.2)\r\n2021-10-07 13:07:45.495807: I tensorflow/compiler/tf2tensorrt/segment/segment.cc:790] There are 30 ops of 8 different types in the graph that are not converted to TensorRT: Identity, ResizeNearestNeighbor, Placeholder, NoOp, Mul, Shape, DataFormatVecPermute, StridedSlice, (For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops).\r\n2021-10-07 13:07:46.085000: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:759] Number of TensorRT candidate segments: 6\r\n2021-10-07 13:07:46.227369: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 0 consisting of 424 nodes by TRTEngineOp_0_0.\r\n2021-10-07 13:07:46.232840: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 1 consisting of 22 nodes by StatefulPartitionedCall/model/TRTEngineOp_0_1.\r\n2021-10-07 13:07:46.234022: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 2 consisting of 22 nodes by StatefulPartitionedCall/model/TRTEngineOp_0_2.\r\n2021-10-07 13:07:46.234935: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 3 consisting of 22 nodes by StatefulPartitionedCall/model/TRTEngineOp_0_3.\r\n2021-10-07 13:07:46.236717: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 4 consisting of 22 nodes by StatefulPartitionedCall/model/TRTEngineOp_0_4.\r\n2021-10-07 13:07:46.237427: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:853] Replaced segment 5 consisting of 27 nodes by TRTEngineOp_0_5.\r\n2021-10-07 13:07:47.073017: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1171] Optimization results for grappler item: tf_graph\r\n  constant_folding: Graph size after: 558 nodes (-624), 1200 edges (-624), time = 554.18ms.\r\n  layout: Graph size after: 595 nodes (37), 1237 edges (37), time = 328.333ms.\r\n  constant_folding: Graph size after: 587 nodes (-8), 1229 edges (-8), time = 230.304ms.\r\n  TensorRTOptimizer: Graph size after: 54 nodes (-533), 62 edges (-1167), time = 975.276ms.\r\n  constant_folding: Graph size after: 54 nodes (0), 62 edges (0), time = 21.947ms.\r\nOptimization results for grappler item: StatefulPartitionedCall/model/TRTEngineOp_0_1_native_segment\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 26.112ms.\r\n  layout: Graph size after: 26 nodes (0), 25 edges (0), time = 42.376ms.\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 31.746ms.\r\n  TensorRTOptimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 5.574ms.\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 22.56ms.\r\nOptimization results for grappler item: StatefulPartitionedCall/model/TRTEngineOp_0_2_native_segment\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 6.176ms.\r\n  layout: Graph size after: 26 nodes (0), 25 edges (0), time = 9.286ms.\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 8.636ms.\r\n  TensorRTOptimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.877ms.\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 6.713ms.\r\nOptimization results for grappler item: StatefulPartitionedCall/model/TRTEngineOp_0_4_native_segment\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.232ms.\r\n  layout: Graph size after: 26 nodes (0), 25 edges (0), time = 4.541ms.\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.265ms.\r\n  TensorRTOptimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.405ms.\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.41ms.\r\nOptimization results for grappler item: TRTEngineOp_0_5_native_segment\r\n  constant_folding: Graph size after: 29 nodes (0), 40 edges (0), time = 4.414ms.\r\n  layout: Graph size after: 29 nodes (0), 40 edges (0), time = 4.662ms.\r\n  constant_folding: Graph size after: 29 nodes (0), 40 edges (0), time = 4.751ms.\r\n  TensorRTOptimizer: Graph size after: 29 nodes (0), 40 edges (0), time = 0.367ms.\r\n  constant_folding: Graph size after: 29 nodes (0), 40 edges (0), time = 4.544ms.\r\nOptimization results for grappler item: StatefulPartitionedCall/model/TRTEngineOp_0_3_native_segment\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.687ms.\r\n  layout: Graph size after: 26 nodes (0), 25 edges (0), time = 7.218ms.\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.93ms.\r\n  TensorRTOptimizer: Graph size after: 26 nodes (0), 25 edges (0), time = 0.535ms.\r\n  constant_folding: Graph size after: 26 nodes (0), 25 edges (0), time = 4.577ms.\r\nOptimization results for grappler item: TRTEngineOp_0_0_native_segment\r\n  constant_folding: Graph size after: 431 nodes (0), 440 edges (0), time = 57.452ms.\r\n  layout: Graph size after: 431 nodes (0), 440 edges (0), time = 88.399ms.\r\n  constant_folding: Graph size after: 431 nodes (0), 440 edges (0), time = 70.943ms.\r\n  TensorRTOptimizer: Graph size after: 431 nodes (0), 440 edges (0), time = 8.772ms.\r\n  constant_folding: Graph size after: 431 nodes (0), 440 edges (0), time = 66.01ms.\r\n\r\nFound 30 files belonging to 1 classes.\r\n2021-10-07 13:40:34.221298: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\r\nstep 1/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 13:40:35.773429: I tensorflow/compiler/tf2tensorrt/common/utils.cc:58] Linked TensorRT version: 7.1.3\r\n2021-10-07 13:40:35.773914: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libnvinfer.so.7\r\n2021-10-07 13:40:35.773981: I tensorflow/compiler/tf2tensorrt/common/utils.cc:60] Loaded TensorRT version: 7.1.3\r\n2021-10-07 13:40:35.785002: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libnvinfer_plugin.so.7\r\n2021-10-07 13:41:56.956338: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\r\n2021-10-07 13:41:56.964140: I tensorflow/stream_executor/cuda/cuda_dnn.cc:380] Loaded cuDNN version 8000\r\n2021-10-07 13:42:03.715476: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.10\r\nstep 2/30, image shape : (1, 320, 320, 3)\r\nstep 3/30, image shape : (1, 320, 320, 3)\r\nstep 4/30, image shape : (1, 320, 320, 3)\r\nstep 5/30, image shape : (1, 320, 320, 3)\r\nstep 6/30, image shape : (1, 320, 320, 3)\r\nstep 7/30, image shape : (1, 320, 320, 3)\r\nstep 8/30, image shape : (1, 320, 320, 3)\r\nstep 9/30, image shape : (1, 320, 320, 3)\r\nstep 10/30, image shape : (1, 320, 320, 3)\r\nstep 11/30, image shape : (1, 320, 320, 3)\r\nstep 12/30, image shape : (1, 320, 320, 3)\r\nstep 13/30, image shape : (1, 320, 320, 3)\r\nstep 14/30, image shape : (1, 320, 320, 3)\r\nstep 15/30, image shape : (1, 320, 320, 3)\r\nstep 16/30, image shape : (1, 320, 320, 3)\r\nstep 17/30, image shape : (1, 320, 320, 3)\r\nstep 18/30, image shape : (1, 320, 320, 3)\r\nstep 19/30, image shape : (1, 320, 320, 3)\r\nstep 20/30, image shape : (1, 320, 320, 3)\r\nstep 21/30, image shape : (1, 320, 320, 3)\r\nstep 22/30, image shape : (1, 320, 320, 3)\r\nstep 23/30, image shape : (1, 320, 320, 3)\r\nstep 24/30, image shape : (1, 320, 320, 3)\r\nstep 25/30, image shape : (1, 320, 320, 3)\r\nstep 26/30, image shape : (1, 320, 320, 3)\r\nstep 27/30, image shape : (1, 320, 320, 3)\r\nstep 28/30, image shape : (1, 320, 320, 3)\r\nstep 29/30, image shape : (1, 320, 320, 3)\r\nstep 30/30, image shape : (1, 320, 320, 3)\r\n[Info] Convert complete.\r\n[Info] Now prebuild trt-engine.\r\nFound 30 files belonging to 1 classes.\r\nstep 1/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.501273: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.518895: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.520264: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.521836: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.523049: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.525338: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 2/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.553969: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.555540: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.556665: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.558038: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.559088: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.559983: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 3/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.594004: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.597923: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.599552: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.601033: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.602305: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.604312: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 4/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.634213: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.638674: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.640178: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.642291: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.643825: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.649495: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 5/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.668430: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.671339: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.672554: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.678498: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.679625: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.681399: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 6/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.699913: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.703150: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.705529: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.708076: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.711813: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.713090: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 7/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.748968: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.750285: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.751345: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.752453: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.753522: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.754501: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 8/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.773218: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.774877: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.776059: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.777162: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.778034: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.779149: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 9/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.794973: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.796168: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.797143: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.797970: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.798687: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.799433: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 10/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.848015: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.849429: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.850419: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.851397: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.852383: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.853300: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 11/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.890036: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.892058: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.905496: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.906838: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.907802: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.908979: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 12/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.931153: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.933730: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.935208: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.936988: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.938206: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.945270: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 13/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:54.996327: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.997990: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:54.999318: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.000395: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.001443: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.002548: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 14/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.026953: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.028467: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.029830: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.030897: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.033084: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.034140: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 15/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.049957: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.051390: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.052484: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.053757: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.054732: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.055674: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 16/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.072601: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.075016: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.077017: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.078947: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.080625: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.081977: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 17/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.122032: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.123231: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.124173: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.125291: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.126088: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.126915: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 18/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.142622: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.144997: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.146100: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.147124: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.147956: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.148818: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 19/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.163239: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.165280: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.167056: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.170588: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.173496: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.174742: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 20/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.191675: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.193147: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.195249: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.196687: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.197986: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.198907: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 21/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.223086: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.224427: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.225536: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.226596: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.227501: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.228680: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 22/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.243688: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.245667: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.247469: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.253290: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.255746: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.258155: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 23/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.272156: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.273568: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.274988: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.276136: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.277656: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.278708: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 24/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.299802: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.301459: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.303199: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.304482: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.305795: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.306835: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 25/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.322027: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.324808: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.326183: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.327298: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.328234: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.329430: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 26/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.347076: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.348356: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.351511: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.354776: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.357402: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.358476: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 27/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.378031: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.383370: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.386487: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.387798: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.393018: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.395249: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 28/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.426818: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.431675: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.434095: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.436594: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.437868: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.438853: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 29/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.454118: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.455327: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.456274: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.460399: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.461846: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.462862: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\nstep 30/30, image shape : (1, 320, 320, 3)\r\n2021-10-07 14:21:55.475968: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.477193: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.478811: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.479840: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.480687: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n2021-10-07 14:21:55.481721: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:42] DefaultLogger INVALID_ARGUMENT: Cannot set empty memory.\r\n[Info] writing savedmodel complete.\r\n```", "comments": []}, {"number": 52277, "title": "`tf.random.shuffle` is deterministic when XLA is used", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): Binary (I assume that's what `pip install` does, at least)\r\n- TensorFlow version (use command below): `v2.6.0-rc2-32-g919f693420e 2.6.0`\r\n- Python version: Python 3.6.9\r\n- CUDA/cuDNN version: N/A, program runs on CPU\r\n- GPU model and memory: N/A, program runs on CPU\r\n\r\n**Describe the current behavior**\r\n\r\nIf I call `tf.random.shuffle` on a 1D tensor of float-32 values of size 10 (for example) inside a JIT-compiled `tf.function`, the shuffled tensor is the same every time I run the program.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe shuffled tensor should usually be different between runs.\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): No\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function(jit_compile=True)\r\ndef shuffle_me(some_tensor):\r\n    output = tf.random.shuffle(some_tensor)\r\n    return output\r\n\r\ninput_ = [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]\r\ninput_ = tf.constant(input_, dtype=tf.float32)\r\n\r\nshuffled = shuffle_me(input_)\r\nprint(shuffled)\r\n```\r\n\r\nI ran this program 20 times, and the output was always `tf.Tensor([0. 2. 5. 9. 3. 4. 8. 7. 6. 1.], shape=(10,), dtype=float32)`. The probability of this happening at random is 1 in (10!)^19, or about 1 in 4.3 x 10^124. If you comment out the line `@tf.function(jit_compile=True)`, the outputs are random.\r\n\r\nIf you are running on Linux, you may find it easiest to do something like:\r\n\r\n```bash\r\nfor N in $(seq 1 20); do python test_shuffle_xla.py; done 2> /dev/null\r\n```\r\n\r\nThis will filter out all of the warnings and just show each of the shuffled tensors.\r\n\r\n**Other info / logs**\r\n\r\nI do not have any other information or logs to share at this time, but I would be happy to generate any logs that the TensorFlow team might find useful.\r\n\r\nThank you for your time!", "comments": ["@callumm-graphcore,\r\n\r\nI am able to reproduce the similar output in colab and here's the [gist](https://colab.research.google.com/gist/sanatmpa1/6efddfa6fbfd4c0d9e83f0eee1d41872/52277.ipynb). Can you take a look at this [link](https://www.tensorflow.org/api_docs/python/tf/xla/experimental/compile?hl=ko#known_issues) which mentions that if we don't set the seed manually, the result from tf.random with XLA will be similar everytime you run it.\r\n\r\n> when a seed is not specified, running the program multiple times will generate the same numbers", "Hi @sanatmpa1,\r\n\r\nThank you for pointing that out, my bad. Based on the sentence below, I understand that my reproducer above will always produce the same output:\r\n> Second, when a seed is not specified, running the program multiple times will generate the same numbers.\r\n\r\nHowever, this only says that _when a seed is not specified_ the same program will generate the same numbers every time. I wrote a script to test what happens when a seed is specified, see below:\r\n\r\n```python\r\nimport argparse\r\nimport tensorflow as tf\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument('--tf-seed', default=0, type=int,\r\n                    help=\"If not 0, set tf.random seed to this value\")\r\nparser.add_argument('--shuffle-seed', default=0, type=int,\r\n                    help=\"If not 0, set tf.random.shuffle seed to this value\")\r\nargs = parser.parse_args()\r\n\r\nif args.tf_seed:\r\n    tf.random.set_seed(args.tf_seed)\r\n\r\n@tf.function(experimental_compile=True)\r\ndef shuffle_me(some_tensor):\r\n    if args.shuffle_seed:\r\n        output = tf.random.shuffle(some_tensor, seed=args.shuffle_seed)\r\n    else:\r\n        output = tf.random.shuffle(some_tensor)\r\n    return output\r\n\r\ninput_ = [0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]\r\ninput_ = tf.constant(input_, dtype=tf.float32)\r\n\r\nshuffled = shuffle_me(input_)\r\nprint(shuffled)\r\n```\r\n\r\nThis also always gives the same output, regardless of what combination of flags is used. I used the Bash command lines below to test this:\r\n\r\n```bash\r\nfor N in $(seq 1 20); do python test_shuffle_xla_set_seed.py --tf-seed $N --shuffle-seed $N 2> /dev/null ; done\r\nfor N in $(seq 1 20); do python test_shuffle_xla_set_seed.py --shuffle-seed $N 2> /dev/null ; done\r\nfor N in $(seq 1 20); do python test_shuffle_xla_set_seed.py --tf-seed $N 2> /dev/null ; done\r\nfor N in $(seq 1 20); do python test_shuffle_xla_set_seed.py 2> /dev/null ; done\r\n```\r\n\r\n(I know that changing the seed shouldn't do anything, I just realised I had changed it after I'd run my tests. Also, you shouldn't need to run 20 tests for each combination yourself to be reasonably convinced that there's no randomness.)\r\n\r\nShould I also always expect that the values are the same even if I have set the seed? Is there something I need to do in order to generate different values?\r\n\r\nThanks again for looking into this.\r\n", "@callumm-graphcore,\r\n\r\nThanks for the update and I agree with you. Even when I was trying with different seed values, I was getting the same result for the first run i.e `tf.Tensor([0. 2. 5. 9. 3. 4. 8. 7. 6. 1.], shape=(10,), dtype=float32)`, which shows that `tf.random with XLA` is deterministic irrespective of the seed values that we provide.", "https://www.tensorflow.org/xla/known_issues#random_number_generation_ignores_tf_seed", "Hi @cheshire, according to this link, \"XLA will behave as if the compilation was seeded with a new unique seed at each run.\". With a \"new unique seed\", I would expect a different result, but I always see the same result. Is there something I'm misunderstanding here?", "This is the seeding behavior: https://cs.opensource.google/tensorflow/tensorflow/+/master:tensorflow/compiler/tf2xla/tf2xla_util.cc;l=547-556;drc=3381da37560d64c7cb62b53879a0a931ff9036c4\r\n\r\nit looks like it will generate a new seed each time you run it within a single execution, ignoring TF seed. Which behavior would you expect?", "We can seed the first calculation with the TF seed, let me see if there are any known blockers.", "For RNG in general, `tf.random.stateless_*` and `tf.random.Generator` are recommended over `tf.random.shuffle` and alike. Please see https://www.tensorflow.org/guide/random_numbers . We don't have `stateless_shuffle` or `Generator.shuffle` yet. We are working on adding the former (#53584).", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Hi @cheshire, thank you for looking into this. I would still expect to be able to make the first call to `tf.random.shuffle` produce different results across multiple runs, even if that meant changing the seed each time. It'd be great if we could use the TF random seed for this purpose.\r\n\r\nThank you for pointing me to the stateless RNG ops and the RNG generators, @wangpengmit. I will use those when possible in the future."]}, {"number": 52272, "title": "Build r2.6 fails when build ppc64le machine", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):**source build**\r\n- TensorFlow version: **2.6(with gpu)**\r\n- Python version: **python3.6**\r\n- Installed using virtualenv? pip? conda?: \r\n- Bazel version (if compiling from source):**bazel 3.7.2- (@non-git)**\r\n- GCC/Compiler version (if compiling from source):gcc 5.5.0(but gcc7 is also the same err is happened)\r\n- CUDA/cuDNN version: cuda 10.2 cuDNN7.6\r\n- GPU model and memory: RTX 2070 super(8gb gpu memory)\r\n\r\nwhen build blow error is happen and abend build...\r\n\r\nERROR: /home/sueoka/gitrepo/tensorflow/tensorflow/core/platform/default/BUILD:147:11: C++ compilation of rule '//tensorflow/core/platform/default:env_time' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/ppc-opt/bin/tensorflow/core/platform/default/_objs/env_time/env_time.d ... (remaining 43 argument(s) skipped)\r\nIn file included from external/eigen_archive/Eigen/Core:210:0,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from ./tensorflow/core/platform/bfloat16.h:21,\r\n                 from ./tensorflow/core/platform/types.h:21,\r\n                 from ./tensorflow/core/platform/env_time.h:20,\r\n                 from tensorflow/core/platform/default/env_time.cc:19:\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = __vector(8) short int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:801:122: error: invalid parameter combination for AltiVec intrinsic\r\n template<> EIGEN_STRONG_INLINE Packet8s   pmul<Packet8s>  (const Packet8s&   a, const Packet8s&   b) { return vec_mul(a,b); }\r\n                                                                                                                          ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = __vector(8) short unsigned int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:802:122: error: invalid parameter combination for AltiVec intrinsic\r\n template<> EIGEN_STRONG_INLINE Packet8us  pmul<Packet8us> (const Packet8us&  a, const Packet8us&  b) { return vec_mul(a,b); }\r\n                                                                                                                          ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = __vector(16) signed char]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:803:122: error: invalid parameter combination for AltiVec intrinsic\r\n template<> EIGEN_STRONG_INLINE Packet16c  pmul<Packet16c> (const Packet16c&  a, const Packet16c&  b) { return vec_mul(a,b); }\r\n                                                                                                                          ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmul(const Packet&, const Packet&) [with Packet = __vector(16) unsigned char]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:804:122: error: invalid parameter combination for AltiVec intrinsic\r\n template<> EIGEN_STRONG_INLINE Packet16uc pmul<Packet16uc>(const Packet16uc& a, const Packet16uc& b) { return vec_mul(a,b); }\r\n                                                                                                                          ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmadd(const Packet&, const Packet&, const Packet&) [with Packet = __vector(8) short int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:833:127: error: invalid parameter combination for AltiVec intrinsic\r\n template<> EIGEN_STRONG_INLINE Packet8s pmadd(const Packet8s& a, const Packet8s& b, const Packet8s& c) { return vec_madd(a,b,c); }\r\n                                                                                                                               ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Packet Eigen::internal::pmadd(const Packet&, const Packet&, const Packet&) [with Packet = __vector(8) short unsigned int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:834:131: error: invalid parameter combination for AltiVec intrinsic\r\n template<> EIGEN_STRONG_INLINE Packet8us pmadd(const Packet8us& a, const Packet8us& b, const Packet8us& c) { return vec_madd(a,b,c); }\r\n                                                                                                                                   ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'Eigen::internal::Packet8bf Eigen::internal::F32ToBf16(Eigen::internal::Packet4f)':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1267:89: error: 'vec_cmpne' was not declared in this scope\r\n   Packet4bi is_mant_not_zero = vec_cmpne(mantissa, reinterpret_cast<Packet4ui>(p4i_ZERO));\r\n                                                                                         ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux_mul(const Packet&) [with Packet = __vector(8) short int; typename Eigen::internal::unpacket_traits<T>::type = short int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1546:37: error: invalid parameter combination for AltiVec intrinsic\r\n   pair = vec_mul(a, vec_sld(a, a, 8));\r\n                                     ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1547:46: error: invalid parameter combination for AltiVec intrinsic\r\n   quad = vec_mul(pair, vec_sld(pair, pair, 4));\r\n                                              ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1548:46: error: invalid parameter combination for AltiVec intrinsic\r\n   octo = vec_mul(quad, vec_sld(quad, quad, 2));\r\n                                              ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux_mul(const Packet&) [with Packet = __vector(8) short unsigned int; typename Eigen::internal::unpacket_traits<T>::type = short unsigned int]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1557:37: error: invalid parameter combination for AltiVec intrinsic\r\n   pair = vec_mul(a, vec_sld(a, a, 8));\r\n                                     ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1558:46: error: invalid parameter combination for AltiVec intrinsic\r\n   quad = vec_mul(pair, vec_sld(pair, pair, 4));\r\n                                              ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1559:46: error: invalid parameter combination for AltiVec intrinsic\r\n   octo = vec_mul(quad, vec_sld(quad, quad, 2));\r\n                                              ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux_mul(const Packet&) [with Packet = __vector(16) signed char; typename Eigen::internal::unpacket_traits<T>::type = signed char]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1577:37: error: invalid parameter combination for AltiVec intrinsic\r\n   pair = vec_mul(a, vec_sld(a, a, 8));\r\n                                     ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1578:46: error: invalid parameter combination for AltiVec intrinsic\r\n   quad = vec_mul(pair, vec_sld(pair, pair, 4));\r\n                                              ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1579:46: error: invalid parameter combination for AltiVec intrinsic\r\n   octo = vec_mul(quad, vec_sld(quad, quad, 2));\r\n                                              ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1580:48: error: invalid parameter combination for AltiVec intrinsic\r\n   result = vec_mul(octo, vec_sld(octo, octo, 1));\r\n                                                ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h: In function 'typename Eigen::internal::unpacket_traits<T>::type Eigen::internal::predux_mul(const Packet&) [with Packet = __vector(16) unsigned char; typename Eigen::internal::unpacket_traits<T>::type = unsigned char]':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1589:37: error: invalid parameter combination for AltiVec intrinsic\r\n   pair = vec_mul(a, vec_sld(a, a, 8));\r\n                                     ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1590:46: error: invalid parameter combination for AltiVec intrinsic\r\n   quad = vec_mul(pair, vec_sld(pair, pair, 4));\r\n                                              ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1591:46: error: invalid parameter combination for AltiVec intrinsic\r\n   octo = vec_mul(quad, vec_sld(quad, quad, 2));\r\n                                              ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/PacketMath.h:1592:48: error: invalid parameter combination for AltiVec intrinsic\r\n   result = vec_mul(octo, vec_sld(octo, octo, 1));\r\n                                                ^\r\nIn file included from external/eigen_archive/Eigen/Core:212:0,\r\n                 from ./third_party/eigen3/Eigen/Core:1,\r\n                 from ./tensorflow/core/platform/bfloat16.h:21,\r\n                 from ./tensorflow/core/platform/types.h:21,\r\n                 from ./tensorflow/core/platform/env_time.h:20,\r\n                 from tensorflow/core/platform/default/env_time.cc:19:\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/Complex.h: In member function 'Eigen::internal::Packet2cf Eigen::internal::Packet2cf::operator-() const':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/Complex.h:77:31: error: 'vec_neg' was not declared in this scope\r\n     return Packet2cf(vec_neg(v));\r\n                               ^\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/Complex.h: In member function 'Eigen::internal::Packet1cd Eigen::internal::Packet1cd::operator-() const':\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/Complex.h:330:31: error: 'vec_neg' was not declared in this scope\r\n     return Packet1cd(vec_neg(v));\r\n                               ^\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 76.801s, Critical Path: 11.73s\r\nINFO: 221 processes: 7 internal, 214 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["Could you please refer the answer provided in this [link](https://stackoverflow.com/questions/35330084/) and let us know the outcome. Thanks.", "I check provided link, \r\nI already install Anaconda env and install PowerAI repo.\r\nBut PowerAI repo is provided blow TF v2.2.\r\n\r\nI want to use a new version of TensorflowTTS, so TF2.6 is required(IF TensorflowTTS can use lower then TF2.2, my trouble is resolve...)\r\n\r\nReturn to first question. How to build r2.6 of TF to build on IBM Power LE machine...", "I try cherry-pick blow error happens\r\n\r\n$ git cherry-pick 9b6215a691a2eebaadb8253bd0cf706f2309a0b8\r\n\r\nwarning: inexact rename detection was skipped due to too many files.\r\nwarning: you may want to set your merge.renamelimit variable to at least 22911 and retry the command.\r\nerror: 9b6215a691a\u3092\u9069\u7528\u3067\u304d\u307e\u305b\u3093\u3067\u3057\u305f... third_party/tensorflow/core: Use macro __linux__ instead of __linux\r\n\u30d2\u30f3\u30c8: after resolving the conflicts, mark the corrected paths\r\n\u30d2\u30f3\u30c8: with 'git add <paths>' or 'git rm <paths>'\r\n\u30d2\u30f3\u30c8: and commit the result with 'git commit'\r\n\r\nand rebuild it but above error still happens", "I retry to fit below cherry-picks\r\n ce70f6cf842a46296119337247c24d307e279fa0\r\n f1acb3bd828a73b15670fc8019f06a5cd51bd564\r\n 9b6215a691a2eebaadb8253bd0cf706f2309a0b8\r\n \r\n and It success no error\r\n\r\nbut when I build, the same error is occur..."]}, {"number": 52266, "title": "TFLite: How to mix floating-point kernels with quantized kernels for different parts of the graph.", "body": "## URL(s) with the issue:\r\nhttps://www.tensorflow.org/lite/performance/post_training_quant\r\n\r\n## Description of issue (what needs changing):\r\nOn this page it says:\r\n\r\n> TFLite supports on the fly quantization and dequantization of activations to allow for:\r\n>\r\n> 1. Using quantized kernels for faster implementation when available.\r\n> **2. Mixing of floating-point kernels with quantized kernels for different parts of the graph.**\r\n\r\nWhere is it documented how this can be done?\r\n\r\n### Clear description\r\n\r\nI would like to create a TFLite model where the first and last layer use floating-point kernels and the intermediate layers use use quantized kernels. From the bold section above it seems possible.\r\n\r\nThe TFLite version of my model has high accuracy using float32. But it has really poor accuracy with int8. I've narrowed the issue down to my convolution layer which is used to pre and post process my audio signal. These layers have a very low parameter count compared to the rest of my model.\r\n\r\nIf I could quantized the only the big, middle portion of my model then I could get a high accuracy model with much faster accuracy and latency.\r\n\r\nIf there's another solution to this I'd love to hear about it.\r\n\r\n### Correct links\r\n\r\nYes\r\n\r\n### Parameters defined\r\n\r\nYes\r\n\r\n### Returns defined\r\n\r\nYes\r\n\r\n### Raises listed and defined\r\n\r\nYes\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nNo\r\n\r\n### Request visuals, if applicable\r\n\r\nNo\r\n\r\n### Submit a pull request?\r\n\r\nNo", "comments": ["Perhaps this isn't actually possible since it's on the TFLite Roadmap:\r\n\r\n> Selective post-training quantization to exclude certain layers from quantization.\r\n[1] https://www.tensorflow.org/lite/guide/roadmap \r\n[2] https://github.com/tensorflow/tensorflow/issues/21596", "@shariq-audiofocus ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled, could you please do so as it helps us analyse the issue.Thanks!", "I don't see how the rest of the template is relevant but I filled it out as best I could.", "Assigning to TF Model Optimization for review."]}, {"number": 52265, "title": "TensorFlowLite with XNNPack error with dynamic shapes (while loop in model appeared after TFLite conversion)", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- Mobile device if the issue happens on mobile device: ARM64\r\n- TensorFlow installed from (source or binary): from source\r\n- TensorFlow version (use command below): nightly 2.8.0-dev20211004  and 2.6.0\r\n- Python version: python 3.8.10\r\n- Bazel version (if compiling from source): bazel 3.7.2., installed using bazelisk\r\n- GCC/Compiler version (if compiling from source): 9.3.0\r\n- CUDA/cuDNN version: no cuda\r\n- GPU model and memory: no\r\n\r\n**Describe the current behavior**\r\n1. I've converted my model from the ONNX file to TFLite.\r\n2. I've built TensorFlow Lite for ARM64 device using XNNpack with `bazel build --config=elinux_aarch64 --define tflite_with_xnnpack=true -c opt //tensorflow/lite:libtensorflowlite.so`. (used TF [guide](https://www.tensorflow.org/lite/guide/build_arm))\r\nWhen I run it I see:\r\n```\r\nERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\nFailed to modify graph!\r\n```\r\nEverything works correctly without XNNpack, but I need it so desperately.\r\nAlso, I've tried using the TensorFlow delegation mechanism for the [ARM NN](https://github.com/ARM-software/armnn/tree/branches/armnn_20_11/delegate) library, but it has the same issue.\r\n\r\n**Describe the expected behavior**\r\nI expect it to process my model. \r\nI've been exploring my model after conversion. All tensors shapes are defined, no `None` s are present. But it seems to me that the conversion process created some `while` loop and it can be the cause of the issue. Please correct me if I'm wrong. I also don't know why it's created. Maybe it is possible just to find a workaround in the converter.\r\n\r\n**Standalone code to reproduce the issue**\r\nI'm adding here my ONNX model and TFLite model archived. Please put them on the `some_path `variable. [model.zip](https://github.com/tensorflow/tensorflow/files/7288640/model.zip)\r\nConverter code:\r\n```\r\nonnx_model = onnx.load(some_path)   # load onnx model\r\ntf_rep = prepare(onnx_model, 'CPU')   # ONNX -> TensorFlowRep representation (computational graph)\r\ntf_rep.export_graph(some_path) \r\nconverter = tf.lite.TFLiteConverter.from_saved_model(some_path)\r\nconverter.experimental_enable_resource_variables = True\r\ntflite_model = converter.convert()\r\nwith open(os.path.join(some_path,'model.tflite'), 'wb') as f:\r\n  f.write(tflite_model)    \r\n```   \r\n\r\n**Other info / logs**\r\nINFO: Created TensorFlow Lite delegate for select TF ops.\r\nINFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 32 nodes with 1 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 4 nodes with 1 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 3 nodes with 0 partitions.\r\nINFO: TfLiteFlexDelegate delegate: 4 nodes delegated out of 29 nodes with 1 partitions.\r\nINFO: TfLiteArmnnDelegate: Created TfLite ArmNN delegate.\r\nERROR: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.\r\nFailed to modify graph!\r\n\r\n", "comments": ["Hi @AnastGerus ! Could you look at similar issues for reference  . [Link1 ](https://github.com/tensorflow/tensorflow/issues/38036),[Link2](https://stackoverflow.com/questions/66789210/converting-tf-keras-model-to-tflite-model-is-slow-and-doesnt-work-with-xnn-pac),[Link3](https://www.tensorflow.org/lite/performance/nnapi)", "Hi @mohantym,\r\nYes, I've seen these links. I would also benefit from [this ](https://github.com/tensorflow/tensorflow/issues/42491) request, but it doesn't look like something that will be added soon.\r\nIn other ones, the root of the issue is dynamic shapes of input (resizing basically), but I don't get where it can come from in my model. I have fixed input size, no resize ops in the model, all ops are supported by TFLite builtins. The only suspicious thing after conversion is the 'while' loop that came from nowhere. \r\nWhere it came from in the conversion time? How can I do a workaround for it? Does it make sense to wait until XNNPACK with dynamic shapes of tensors support will be implemented?\r\n\r\nThanks for your attention,\r\nBest regards,\r\nAnastasiia\r\n", "Hi @AnastGerus @sachinprasadhs ! I  edited above code using below reference ans was getting Assertion error . Can you please checkout  this code base. Attaching [Gist](https://colab.research.google.com/gist/mohantym/a4800e625b86a36171cff0d0cdb89cd7/github_52265.ipynb#scrollTo=ZEVrkAmCpzIW) for Reference.\r\n[Reference:](https://colab.research.google.com/github/joshuacwnewton/ONNX-to-TFLite/blob/master/onnx_to_tflite.ipynb#scrollTo=oXwuHawWvXmk)", "@multiverse-tf possible to take a look at this? (XNNPack error)", "This is a known issue with XNNPACK delegate (and other TFLite delegates) as it doesn't support model graphs with dynamic tensors. Note that a tensor could be marked dynamic at TFLite runtime when there's a control-flow op (like if, while etc.) prepared. In other words, even when the model graph itself doesn't have any tensors of dynamic shapes statically, at runtime, a model could have dynamic tensors.\r\n\r\nWe are putting resources internally to resolve such issues as soon as possible, hopefully by the next TF release."]}, {"number": 52259, "title": "The difference in the return order of object detection results in tf2.5-cpu and tf2.6-gpu", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\nhttps://www.tensorflow.org/lite/tutorials/model_maker_object_detection#optional_test_the_tflite_model_on_your_image\r\n\r\n\r\n## Description of issue (what needs changing):\r\nthe return order in tf2.5-cpu in google colab is \r\n```\r\n  boxes = get_output_tensor(interpreter, 0)\r\n  classes = get_output_tensor(interpreter, 1)\r\n  scores = get_output_tensor(interpreter, 2)\r\n  count = int(get_output_tensor(interpreter, 3))\r\n```\r\n\r\nbut in tf2.6-gpu the return order is\r\n```\r\n    scores = get_output_tensor(interpreter, 0)\r\n    print(scores)\r\n    boxes = get_output_tensor(interpreter, 1)\r\n    print(boxes)\r\n    count = int(get_output_tensor(interpreter, 2))\r\n    print(count)\r\n    classes = get_output_tensor(interpreter, 3)\r\n    print(classes)\r\n```\r\n\r\nthe link to my script is: https://github.com/MaoXianXin/Tensorflow_tutorial/blob/tf_hub/TFLite_prac/object_detection_lite.py\r\nthe detect result is in the Clear description\r\n\r\n### Clear description\r\n![2021-10-05-19-58-43](https://user-images.githubusercontent.com/22192610/136018975-a943ff2b-3698-44c7-bd32-d5ab4d156f33.png)\r\nthe following is my result:\r\n![result](https://user-images.githubusercontent.com/22192610/136019965-c69c2071-149b-49fa-8cee-b41318e41012.png)\r\n\r\nif i use the code in docs the error is result:\r\n![2021-10-05-20-13-48](https://user-images.githubusercontent.com/22192610/136020174-4ff542e1-37a3-4adb-96b7-0894d43f6e49.png)\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["find the object_detector_spec.py in anaconda3/envs/tf2.5/lib/python3.6/site-packages/tensorflow_examples/lite/model_maker/core/task/model_spec, then change nms_boxes, nms_classes, nms_scores, _ = lite_runner.run(images) ----> nms_scores, nms_boxes, nms_count, nms_classes = lite_runner.run(images), should address the error in tf2.6-gpu", "@MaoXianXin \r\nCan you please share a colab gist with the difference in result in cpu and gpu.", "![2021-10-14-22-01-39](https://user-images.githubusercontent.com/22192610/137337349-48509eef-727a-4fdc-8543-6ff3b4c27375.png)\r\n", "![2021-10-14-22-21-35](https://user-images.githubusercontent.com/22192610/137337391-d535f39b-f33f-46ce-835d-e94ef0420937.png)\r\n![2021-10-14-22-24-08](https://user-images.githubusercontent.com/22192610/137337403-5e1e57bd-1c61-46be-b696-dd2d25e28a52.png)\r\n\r\n", "@Saduf2019   The results are shown above, and my script is in https://github.com/MaoXianXin/Tensorflow_tutorial/blob/tf_hub/TFLite_prac/object_detection_lite.py", "the dataset need to train is here: https://drive.google.com/file/d/1qj0pMtE-ORnSPAf0A3JHPXhWmVzi1Zo6/view?usp=sharing", "@miaout17, There seems to be difference in order of interpreter for CPU and GPU, could you please take a look into this. Thanks."]}, {"number": 52258, "title": "Building tflite with xnnpack on windows 10 ", "body": "hello team,\r\n \r\ni wanted to use xnnpack with tflite on my windows 10 machine. found out that we need to build tf from source with tflite_with_xnnpack=true flag. i was not able to build successfully ,and I am not sure I am following the correct steps or not as I didn't get proper installation guide for building tf with xnnpack on windows. It would be helpful if I get any documentation or resource to build and use tflite with XNN pack", "comments": ["@amoghht ,\r\nWe see that the issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose) has not been filled and the tensorflow version you are using, could you please do so as it helps us analyse the issue.Thanks!", "\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 \r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version:2.3.0 and 2.5.0\r\n- Python version:- Installed using virtualenv? pip? conda?: conda python 3.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n\r\n\r\n\r\n\r\n**Describe the problem**\r\nIs there any documentation for building tflite with xnnpack on windows 10??\r\n\r\n\r\n\r\n", "@amoghht ,\r\nCan you please take a look at this [link](https://github.com/tensorflow/tensorflow/issues/38852) with the similar issue.It helps.Thanks!", "@tilakrayal ,\r\nthank you,will try and update soon", "I am following the below given blog to build tensorflow from source with xnnpack.\r\n\r\nhttps://medium.com/vitrox-publication/deep-learning-frameworks-tensorflow-build-from-source-on-windows-python-c-cpu-gpu-d3aa4d0772d8\r\n\r\nsystem details:\r\nProcessor : Intel(R) Core(TM) i7-4600U CPU @ 2.10GHz, 2701 Mhz, 2 Core(s), 4 Logical Processor(s)\r\nSystem Type\tx64-based PC\r\nos: windows 10 \r\ntensorflow version: 2.2.0\r\nbazel version: 2.0.0\r\npython version: 3.8\r\n\r\ncommand: bazel build --config=opt --define tflite_with_xnnpack=true //tensorflow/tools/pip_package:build_pip_package\r\n\r\ni am getting this error:\r\n\r\nERROR: E:/tensorflow/tensorflow/python/BUILD:437:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 2)\r\ntensorflow/python/lib/core/bfloat16.cc(635): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(635): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()'\r\ntensorflow/python/lib/core/bfloat16.cc(639): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(639): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()'\r\ntensorflow/python/lib/core/bfloat16.cc(643): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(643): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()'\r\ntensorflow/python/lib/core/bfloat16.cc(646): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(646): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()'\r\ntensorflow/python/lib/core/bfloat16.cc(650): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(650): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()'\r\ntensorflow/python/lib/core/bfloat16.cc(654): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *)' to 'PyUFuncGenericFunction'\r\ntensorflow/python/lib/core/bfloat16.cc(654): note: None of the functions with this name in scope match the target type\r\ntensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_5e9d6c6041ef47d930e8efd674a02f59>::operator ()'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nINFO: Elapsed time: 14.813s, Critical Path: 5.48s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully\r\n\r\n"]}, {"number": 52244, "title": "XLA dumps are not generated after setting XLA_FLAGS=\"--xla_dump_to=/some/path\"", "body": "I was running the unit test ```//tensorflow/compiler/tests:categorical_op_test_gpu``` and wanted to see the XLA dumps for this unit test. I set the --xla_dump_to to a correct path, but dump files were never generated (the setup is correct as for other tests, It can generate XLA dumps). \r\n\r\nI tried to investigate this issue  further and I found that [```detailed_logging```](https://github.com/tensorflow/tensorflow/blob/70ba657dcc0aed3593562931df16090218ab616b/tensorflow/compiler/jit/xla_compile_on_demand_op.cc#L138) has been hard coded to false in this location. This value is being used later on in [here](https://github.com/tensorflow/tensorflow/blob/70ba657dcc0aed3593562931df16090218ab616b/tensorflow/compiler/xla/service/dump.cc#L76) to reset the ```dump_to``` path. \r\n\r\nWith this empty value for ```dump_to```, it will never generate the files even though ```xla_dump_to``` has been set. This behavior is contradictive to the XLA documentation described [here](https://www.tensorflow.org/xla)", "comments": []}, {"number": 52223, "title": "OP_REQUIRES failed at conv_ops.cc:1276 : Not found: No algorithm worked!", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4 / 8.2.4.15\r\n- GPU model and memory: NVIDIA GeForce RTX 2070 \r\n\r\n**Describe the current behavior**\r\n\r\nI get the following error on GPU:\r\n```\r\n2021-10-01 23:05:27.951528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6173 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id:0000:09:00.0, compute capability: 7.5\r\n2021-10-01 23:05:28.331213: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\r\n2021-10-01 23:05:28.866860: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at conv_ops.cc:1276 : Not found: No algorithm worked!\r\nTraceback (most recent call last):\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1359, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1451, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\r\n  (0) Not found: No algorithm worked!\r\n         [[{{node convolution}}]]\r\n  (1) Not found: No algorithm worked!\r\n         [[{{node convolution}}]]\r\n         [[convolution/_5]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt is expected that I get some error here. But it should be about some wrong filter shape. The exception I get is very misleading.\r\n\r\nOn CPU, the exception looks much different, and is more like what I would expect.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    with tf.compat.v1.Session(graph=graph) as session:\r\n        x = tf.compat.v1.placeholder(tf.float32, (None, None, 1, 40))  # [B,T,1,40]\r\n        filters = tf.compat.v1.placeholder(tf.float32, (3, 3, None, 32))\r\n        y = tf.compat.v1.nn.convolution(x, filter=filters, padding=\"SAME\")\r\n\r\n        session.run(\r\n            y,\r\n            feed_dict={\r\n                x: numpy.zeros((3, 4, 1, 40)),\r\n                filters: numpy.zeros((3, 3, 1, 32)),\r\n                })\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis problem was originally reported here: https://github.com/rwth-i6/returnn/issues/703\r\n\r\nThere are a couple of the same error also reported here:\r\n* https://github.com/tensorflow/tensorflow/issues/43174\r\n* https://github.com/tensorflow/tensorflow/issues/45044\r\n* https://github.com/tensorflow/tensorflow/issues/48117\r\n\r\nIn many of these cases, it seems to be caused by too less GPU memory. However, not in the case here. So probably these are not duplicates. Although it's not totally clear.\r\n\r\nI stumbled upon this problem due to a wrong model checkpoint, and the model checkpoint loading ignored the different shape of the filter, which is another bug (https://github.com/tensorflow/tensorflow/issues/52220).\r\n", "comments": ["Hi @sanatmpa1! Could you please look at this issue , Its not replicating when **filters** shape is set to (3, 3, None, 40)  (i.e even numbers as Errors Indicate). Attaching GIST in TF [2.5](https://colab.research.google.com/gist/mohantym/ffbf33fdcf3d2c0283bbb28c431409ff/github_52223.ipynb#scrollTo=Wy2RfCPWoIQ_) ,[2.6](https://colab.research.google.com/gist/mohantym/09940eadf48201d9c29d4e35a7102701/github_52223.ipynb#scrollTo=A-_E5WOSHZiu) and [2.7](https://colab.research.google.com/gist/mohantym/b11ef7f34908d5979b36a2cbe334a4eb/github_52223.ipynb#scrollTo=KujDgU3aHVIa) for reference."]}, {"number": 52220, "title": "Model checkpoint load ignores wrong shape", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 11.6 + Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below):  v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.9.6 + 3.8\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nSaving a variable of shape (3,3,40,32) into a checkpoint, and then loading the variable with shape (3,3,1,32) from this checkpoint does not cause an error. Instead, it loads fine. The variable shape still claims to be (3,3,1,32), but when evaluated, one can see that it is indeed (3,3,40,32).\r\n\r\nSo there does not seem to be any validation of the shape during checkpoint loading.\r\n\r\n**Describe the expected behavior**\r\n\r\nIn general, if a tensor or variable claims to be of some static shape, I think it should never be possible that its actual shape when evaluated is different.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nprint(\"TF:\", tf.__version__)\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nfilename = \"test-ckpt-diff-shape.model\"\r\n\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    with tf.compat.v1.Session(graph=graph) as session:\r\n        shape1 = (3,3,40,32)\r\n        v = tf.compat.v1.get_variable(name=\"W\", shape=shape1)\r\n        print(v)\r\n        saver = tf.compat.v1.train.Saver(var_list=[v])\r\n        session.run(tf.compat.v1.global_variables_initializer())\r\n        saver.save(sess=session, save_path=filename)\r\n\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    with tf.compat.v1.Session(graph=graph) as session:\r\n        shape2 = (3,3,1,32)\r\n        v = tf.compat.v1.get_variable(name=\"W\", shape=shape2)\r\n        print(v)\r\n        saver = tf.compat.v1.train.Saver(var_list=[v])\r\n        saver.restore(sess=session, save_path=filename)\r\n        v_raw = session.run(v)\r\n        print(v)\r\n        print(v_raw.shape)\r\n        assert v.shape.as_list() == list(shape2)\r\n        assert v.shape.as_list() == list(v_raw.shape)\r\n```\r\n\r\n**Other info / logs**\r\n\r\nI stumbled upon this problem because it causes a seemingly unrelated error for some following 2D convolution where this variable is used as a kernel:\r\n```\r\n2021-09-30 12:52:25.768174: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:1\r\n115 : Not found: No algorithm worked!\r\nTensorFlow exception: 2 root error(s) found.\r\n  (0) Not found: No algorithm worked!\r\n         [[node conv0/convolution (defined at u/schmitt/src/returnn/returnn/tf/layers/basic.py:4061) ]]\r\n         [[output/rec/while/Switch_14/_553]]\r\n  (1) Not found: No algorithm worked!\r\n         [[node conv0/convolution (defined at u/schmitt/src/returnn/returnn/tf/layers/basic.py:4061) ]]\r\n0 successful operations.\r\n```\r\nThis was reported here: https://github.com/rwth-i6/returnn/issues/703\r\n\r\nThere are a couple of these errors also reported here:\r\n* https://github.com/tensorflow/tensorflow/issues/43174\r\n* https://github.com/tensorflow/tensorflow/issues/45044\r\n* https://github.com/tensorflow/tensorflow/issues/48117\r\n\r\nI wonder if some of them have a similar source.\r\n\r\nBut anyway, this issue is not really about the convolution error, but about the checkpoint loading behavior, and the variable shape.\r\n", "comments": ["Hi @Albert-Z-Guo! ,Tried to replicate this in 2.6 ! Assertion Error in **second Assertion** might be  raising due to **mismatch in new session shape (3,3,40,32) and  old session shape (3,3,1,32)** ,while  **first Assertion** new session shape is **being matched to declared shape (3,3,1,32)** .In Attaching [GIST](https://colab.research.google.com/gist/mohantym/1d70ba29c7ea96c84773e1e62fc0df9a/github_52220.ipynb) for reference.", "Yes and this is wrong. You always must have `v.shape.as_list() == list(session.run(v).shape)`.", "It's also wrong that the model checkpoint loading was done without error. There should have been an exception there.", "Hi @Saduf2019 , Could you please look into this issue . It is replicating in [2.5 ](https://colab.research.google.com/gist/mohantym/b68163371f9e8a869b0244b103ab0572/github_52220.ipynb#scrollTo=S8zOfkH3z28T),[2.6 ](https://colab.research.google.com/gist/mohantym/1d70ba29c7ea96c84773e1e62fc0df9a/github_52220.ipynb#scrollTo=YGsXUWna1zP_)and [nightly.](https://colab.research.google.com/gist/mohantym/fc67428fd6b301bc309c953991dee235/github_52220.ipynb#scrollTo=S8zOfkH3z28T)"]}, {"number": 52217, "title": "fix moments float16 cast bug", "body": "Related https://github.com/tensorflow/addons/issues/2550.\r\nReproduce example:\r\n```python\r\ntf.debugging.enable_check_numerics()\r\ntf.nn.moments(tf.constant([[[[[ -741., 353.2, 1099., -1807. ,502.8, -83.4, 333.5, -130.9]]]]], dtype=tf.float16), [1, 2, 4], True)\r\n```", "comments": ["Still waiting for merging this fix", "@mihaimaruseac What issue we have internally?", "Just a reminder: we don't have copybara visibility, any feedback here?"]}, {"number": 52203, "title": "macOS debug build issue", "body": "**System information**\r\n- OS Platform and Distribution: macOS Big Sur 11.6\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): Apple clang via Xcode 12.5.1.12E507\r\n- CUDA/cuDNN version: CPU only\r\n- GPU model and memory: CPU only\r\n\r\n**Describe the problem**\r\n\r\nReincarnation of #46566 (`truncated or malformed object (LC_SEGMENT_64 command 0 fileoff field plus filesize field extends past the end of the file)` during linkage of debug build) but with latest versions (tensorflow 2.6, bazel 3.7.2, Xcode 12.5.1, macOS 11.6). I'm also fine with re-opening #46566 (I can't, permission denied).\r\n\r\nI even tried bazel 4.2.1 (in case this was fixed there but not backported to v3.x) by increasing the max-allowed-version beyond 3.99.0 in tensorflow/configure.py. However this didn't change anything.\r\n\r\nSeems that the size of the debug symbols grew beyond some maximum supported threshold.", "comments": ["In case this may help, the failing command is:\r\n```\r\nSUBCOMMAND: # //tensorflow:libtensorflow.2.6.0.dylib [action 'Linking tensorflow/libtensorflow.2.6.0.dylib', configuration: f01429eecad250eb96e9476d19c311b40478752688e2996f9bb665f374d2d0a6, execution platform: @local_execution_config_platform//:platform]\r\n(cd /Users/joachim/Documents/vcpkg/buildtrees/tensorflow/.bzl/c9862ac12f4d7c0fe77e29ecf9744e73/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=MacOSX \\\r\n    APPLE_SDK_VERSION_OVERRIDE=11.3 \\\r\n    PATH=/usr/bin:/usr/bin:/Users/joachim/Documents/vcpkg/downloads/tools/bazel/4.2.1-darwin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin:/Applications/Postgres.app/Contents/Versions/latest/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    XCODE_VERSION_OVERRIDE=12.5.1.12E507 \\\r\n  external/local_config_cc/cc_wrapper.sh @bazel-out/darwin-dbg/bin/tensorflow/libtensorflow.2.6.0.dylib-2.params)\r\n[11,425 / 11,428] Linking tensorflow/libtensorflow.2.6.0.dylib; 283s local\r\n[11,425 / 11,428] Linking tensorflow/libtensorflow.2.6.0.dylib; 1310s local\r\n[11,425 / 11,428] Linking tensorflow/libtensorflow.2.6.0.dylib; 2517s local\r\n[11,425 / 11,428] Linking tensorflow/libtensorflow.2.6.0.dylib; 3901s local\r\nERROR: /Users/joachim/Documents/vcpkg/buildtrees/tensorflow/x64-osx-dbg/tensorflow/BUILD:984:20: Linking tensorflow/libtensorflow.2.6.0.dylib failed: (Exit 1): cc_wrapper.sh failed: error executing command \r\n  (cd /Users/joachim/Documents/vcpkg/buildtrees/tensorflow/.bzl/c9862ac12f4d7c0fe77e29ecf9744e73/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    APPLE_SDK_PLATFORM=MacOSX \\\r\n    APPLE_SDK_VERSION_OVERRIDE=11.3 \\\r\n    PATH=/usr/bin:/usr/bin:/Users/joachim/Documents/vcpkg/downloads/tools/bazel/4.2.1-darwin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin:/Applications/Postgres.app/Contents/Versions/latest/bin \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    XCODE_VERSION_OVERRIDE=12.5.1.12E507 \\\r\n  external/local_config_cc/cc_wrapper.sh @bazel-out/darwin-dbg/bin/tensorflow/libtensorflow.2.6.0.dylib-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/install_name_tool: object: bazel-out/darwin-dbg/bin/tensorflow/libtensorflow.2.6.0.dylib truncated or malformed object (LC_SEGMENT_64 command 0 fileoff field plus filesize field extends past the end of the file)\r\nINFO: Elapsed time: 10295.555s, Critical Path: 4215.89s\r\nINFO: 11426 processes: 2317 internal, 9109 local.\r\nFAILED: Build did NOT complete successfully\r\nFAILED: Build did NOT complete successfully\r\n```\r\nSteps to reproduce:\r\n```\r\n$ git clone https://github.com/microsoft/vcpkg\r\n$ ./vcpkg/bootstrap-vcpkg.sh\r\n```\r\nRemove the work-around I added in microsoft/vcpkg/pull/20015 in file `./vcpkg/ports/tensorflow-common/tensorflow-common.cmake` at line 190\r\n```\r\n\t\telseif(VCPKG_TARGET_IS_OSX)\r\n\t\t\tset(BUILD_OPTS --compilation_mode=fastbuild) # debug build on macOS currently broken\r\n```\r\nby reverting back to `--compilation_mode=dbg`. Then start the build:\r\n```\r\n$ ./vcpkg/vcpkg install tensorflow\r\n```", "This is a scenario that happens regardless of platform. TF build is too large to fully build it with debug symbols.\r\n\r\nYou can try using per file copts to only include debug symbols on some files", "> This is a scenario that happens regardless of platform. TF build is too large to fully build it with debug symbols.\r\n\r\nOn Linux, it still seems to succeed. On Windows, I was aware that we technically simply have no chance once the file size grows beyond 4GB as the internals can only handle 32-bit offsets. On macOS, I was hoping that it would still succeed even beyond 4GB as Apple made the `.dylib` file format 64-bit ready; and from the error message we have proof that the 64-bit extensions are actually used (`LC_SEGMENT_64`); however, Apple seems to have a bug in `otool` (and maybe further tools) so that `.dylib` files beyond 4GB can't be loaded even if they were created correctly: in the `otool` sources I found checks like `if(fileoff + filesize > object_size)` where `fileoff` and `filesize` are `uint64_t` but `object_size` is only `uint32_t`..."]}, {"number": 52200, "title": "No registered 'Const' OpKernel for GPU devices with constant folding", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.4 / 8.2.4.15\r\n- GPU model and memory: NVIDIA GeForce RTX 2070 \r\n\r\n**Describe the current behavior**\r\n\r\nThe code below fails with an exception.\r\nThis is the full output:\r\n```\r\nTF: 2.6.0\r\n2021-09-30 15:52:24.159169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.162278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.162637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.163155: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-30 15:52:24.163754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.164103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.164431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.456691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.457036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.457342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.457640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5732 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:09:00.0, compute capability: 7.5\r\n2021-09-30 15:52:24.466132: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'Const' OpKernel for GPU devices compatible with node {{node ConstantFolding/Const_enter}}\r\n         (OpKernel was found, but attributes didn't match) Requested Attributes: dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"\r\n        .  Registered:  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_VARIANT]\r\n  device='DEFAULT'; dtype in [DT_BOOL]\r\n  device='DEFAULT'; dtype in [DT_QUINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT32]\r\n  device='DEFAULT'; dtype in [DT_QUINT8]\r\n  device='DEFAULT'; dtype in [DT_QINT8]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX128]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX64]\r\n  device='DEFAULT'; dtype in [DT_INT8]\r\n  device='DEFAULT'; dtype in [DT_UINT8]\r\n  device='DEFAULT'; dtype in [DT_INT16]\r\n  device='DEFAULT'; dtype in [DT_UINT16]\r\n  device='DEFAULT'; dtype in [DT_UINT32]\r\n  device='DEFAULT'; dtype in [DT_INT64]\r\n  device='DEFAULT'; dtype in [DT_UINT64]\r\n  device='DEFAULT'; dtype in [DT_DOUBLE]\r\n  device='DEFAULT'; dtype in [DT_FLOAT]\r\n  device='DEFAULT'; dtype in [DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_HALF]\r\n  device='DEFAULT'; dtype in [DT_INT32]\r\n  device='CPU'\r\n  device='TPU_SYSTEM'\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_UINT64]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_QINT32]\r\n  device='GPU'; dtype in [DT_UINT32]\r\n  device='GPU'; dtype in [DT_QUINT16]\r\n  device='GPU'; dtype in [DT_QINT16]\r\n  device='GPU'; dtype in [DT_INT16]\r\n  device='GPU'; dtype in [DT_UINT16]\r\n  device='GPU'; dtype in [DT_QINT8]\r\n  device='GPU'; dtype in [DT_INT8]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_BFLOAT16]\r\n  device='GPU'; dtype in [DT_HALF]\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1359, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1451, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for 'GPU' devices compatible with node {{node ConstantFolding/Const_enter}}\r\n         (OpKernel was found, but attributes didn't match) Requested Attributes: _XlaHasReferenceVars=false, dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"\r\n        .  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_VARIANT]\r\n  device='DEFAULT'; dtype in [DT_BOOL]\r\n  device='DEFAULT'; dtype in [DT_QUINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT32]\r\n  device='DEFAULT'; dtype in [DT_QUINT8]\r\n  device='DEFAULT'; dtype in [DT_QINT8]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX128]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX64]\r\n  device='DEFAULT'; dtype in [DT_INT8]\r\n  device='DEFAULT'; dtype in [DT_UINT8]\r\n  device='DEFAULT'; dtype in [DT_INT16]\r\n  device='DEFAULT'; dtype in [DT_UINT16]\r\n  device='DEFAULT'; dtype in [DT_UINT32]\r\n  device='DEFAULT'; dtype in [DT_INT64]\r\n  device='DEFAULT'; dtype in [DT_UINT64]\r\n  device='DEFAULT'; dtype in [DT_DOUBLE]\r\n  device='DEFAULT'; dtype in [DT_FLOAT]\r\n  device='DEFAULT'; dtype in [DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_HALF]\r\n  device='DEFAULT'; dtype in [DT_INT32]\r\n  device='CPU'\r\n  device='TPU_SYSTEM'\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_UINT64]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_QINT32]\r\n  device='GPU'; dtype in [DT_UINT32]\r\n  device='GPU'; dtype in [DT_QUINT16]\r\n  device='GPU'; dtype in [DT_QINT16]\r\n  device='GPU'; dtype in [DT_INT16]\r\n  device='GPU'; dtype in [DT_UINT16]\r\n  device='GPU'; dtype in [DT_QINT8]\r\n  device='GPU'; dtype in [DT_INT8]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_BFLOAT16]\r\n  device='GPU'; dtype in [DT_HALF]\r\n\r\n         [[ConstantFolding/Const_enter]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf-const-gpu.py\", line 18, in <module>\r\n    session.run(n)\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 967, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1190, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1368, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1394, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for 'GPU' devices compatible with node {{node ConstantFolding/Const_enter}}\r\n         (OpKernel was found, but attributes didn't match) Requested Attributes: _XlaHasReferenceVars=false, dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"\r\n        .  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_VARIANT]\r\n  device='DEFAULT'; dtype in [DT_BOOL]\r\n  device='DEFAULT'; dtype in [DT_QUINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT32]\r\n  device='DEFAULT'; dtype in [DT_QUINT8]\r\n  device='DEFAULT'; dtype in [DT_QINT8]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX128]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX64]\r\n  device='DEFAULT'; dtype in [DT_INT8]\r\n  device='DEFAULT'; dtype in [DT_UINT8]\r\n  device='DEFAULT'; dtype in [DT_INT16]\r\n  device='DEFAULT'; dtype in [DT_UINT16]\r\n  device='DEFAULT'; dtype in [DT_UINT32]\r\n  device='DEFAULT'; dtype in [DT_INT64]\r\n  device='DEFAULT'; dtype in [DT_UINT64]\r\n  device='DEFAULT'; dtype in [DT_DOUBLE]\r\n  device='DEFAULT'; dtype in [DT_FLOAT]\r\n  device='DEFAULT'; dtype in [DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_HALF]\r\n  device='DEFAULT'; dtype in [DT_INT32]\r\n  device='CPU'\r\n  device='TPU_SYSTEM'\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_UINT64]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_QINT32]\r\n  device='GPU'; dtype in [DT_UINT32]\r\n  device='GPU'; dtype in [DT_QUINT16]\r\n  device='GPU'; dtype in [DT_QINT16]\r\n  device='GPU'; dtype in [DT_INT16]\r\n  device='GPU'; dtype in [DT_UINT16]\r\n  device='GPU'; dtype in [DT_QINT8]\r\n  device='GPU'; dtype in [DT_INT8]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_BFLOAT16]\r\n  device='GPU'; dtype in [DT_HALF]\r\n\r\n         [[ConstantFolding/Const_enter]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code below should work without error on a GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nprint(\"TF:\", tf.__version__)\r\ntf.compat.v1.disable_eager_execution()\r\ntf.compat.v1.disable_control_flow_v2()\r\n\r\n\r\nwith tf.compat.v1.Session() as session:\r\n  x = tf.constant(\"foo\")\r\n\r\n  def body(i):\r\n    with tf.control_dependencies([tf.print(x)]):\r\n      return i + 1\r\n\r\n  n = tf.while_loop(cond=lambda i: tf.less(i, 1), body=body, loop_vars=[0])\r\n  session.run(n)\r\n```\r\n", "comments": ["@sachinprasadhs ,\r\nI was able to reproduce the issue in tf v2.5,v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/6996de1c84dbdf1431043370d1c9ea08/52200.ipynb)."]}, {"number": 52197, "title": "tf.distribute.MultiWorkerStrategy - Don't mirror models on workers.", "body": "I am exploring multi-worker training with Tensorflow and I'm at the point where I would need a `tf.distribute.MultiWorkerStrategy`. Compare `tf.distribute.Strategy` with the `tf.distribute.MirroredStrategy`.\r\n\r\nI do not want the model to get mirrored on each GPU of each worker, instead I want the model to get mirrored on each worker.\r\n\r\nIs there a way I can to that? Would be possible to implement tf.distribute.MultiWorkerStrategy` by myself (without too much effort)?", "comments": ["@stefan-falk ,\r\nCan you please elaborate about your Feature. Also, please specify the Use Cases for this feature. Thanks!", "@tilakrayal Of course. \r\n\r\nAfter looking into this though I came to the conclusion that what I want is probably not possible (yet). The reason for this is the fact that all gradients are computed on one device at the moment and it does not seem to be a trivial task to distribute these computations over multiple devices.\r\n\r\nBut let me explain where I am coming from. Maybe there's a simpler solution to my problem already available.\r\n\r\n----\r\n\r\nNow, usually when we use the classic `MirroredStrategy` we'd have the following (e.g. for 4 GPUs on one machine):\r\n\r\n```none\r\ndevice |\u00a0stores\r\n---------------\r\ngpu:0  |\u00a0model, gradients\r\ngpu:1  |\u00a0model\r\ngpu:2  |\u00a0model \r\ngpu:3  |\u00a0model  \r\n```\r\n\r\nIn my case this is a problem because I'll run into OOM exceptions on `/gpu:0` rather quickly once the model is large enough. \r\n\r\n### Scenario 1\r\n\r\nOne solution here would be to remove the `model` from `/gpu:0` s.t. we can increase the model or batch-size:\r\n\r\n```none\r\ndevice |\u00a0stores\r\n---------------\r\ngpu:0  | gradients\r\ngpu:1  |\u00a0model\r\ngpu:2  |\u00a0model \r\ngpu:3  |\u00a0model  \r\n```\r\n\r\nI am not sure how I could achieve such a setup locally / on a single worker.\r\n\r\n### Scenario 2\r\n\r\nHowever, even if scenario 1 was possible, I'd already loose one model instance and the model is still very limited by the VRAM of just one device. This is where I realized that I could distribute the model across multiple GPUs. E.g. we could split the model encoders over `/gpu:0` and `/gpu:1` and its decoders over `/gpu:2` and `/gpu:3`:\r\n\r\n```none\r\ndevice |\u00a0stores\r\n---------------\r\ngpu:0  | encoder_layers[0:10]  & gradients[0]\r\ngpu:1  |\u00a0encoder_layers[10:20] & gradients[1]\r\ngpu:2  |\u00a0decoder_layers[0:10]  & gradients[2]\r\ngpu:3  |\u00a0decoder_layers[10:20] & gradients[3]\r\n```\r\n\r\nIf this was possible, it would be useful to have the suggested `tf.distribute.MultiWorkerStrategy` s.t. this setup gets exactly replicated on each worker/machine.\r\n\r\nHowever, the problem here are the gradients. It appears to be (almost) impossible to distribute the computations for the gradients across multiple GPUs at this point. For this reason, scenario 2 is probably off the table I guess?\r\n\r\n\r\n### Scenario 3\r\n\r\nThe last solution I'd see, which is not ideal but better than nothing, would be to use scenario 1 over multiple workers.\r\n\r\n```none\r\ndevice |\u00a0stores\r\n---------------\r\nMachine / Worker 1:\r\ngpu:0  | gradients\r\ngpu:1  |\u00a0model\r\ngpu:2  |\u00a0model \r\ngpu:3  |\u00a0model  \r\nMachine / Worker 2:\r\ngpu:0  | gradients\r\ngpu:1  |\u00a0model\r\ngpu:2  |\u00a0model \r\ngpu:3  |\u00a0model  \r\n```\r\n\r\nOr, if this is not possible, at least something like:\r\n\r\n```none\r\ndevice |\u00a0stores\r\n---------------\r\nMachine 1 / Worker 1:\r\ngpu:0  | gradients\r\ngpu:1  |\u00a0model\r\nMachine 1 / Worker 2:\r\ngpu:2  |\u00a0gradients \r\ngpu:3  |\u00a0model  \r\n````\r\n\r\nThis last option would also require a `tf.distribute.MultiWorkerStrategy` in order to avoid having the model and the gradients being stored on one device again.", "Have you checked [ParameterServerStrategy](https://www.tensorflow.org/guide/distributed_training#parameterserverstrategy) which may help your case, more details can be found in [this](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/ClusterCoordinator) document with the example. Let us know if this helps you. Thank you!", "@sachinprasadhs thanks for helping me out here.\r\n\r\nIt looks like the right strategy in order to mirror the distribution on one machine to other machines. The only missing thing imo is the question about the gradients.\r\n\r\nAs mentioned, as I distributed the model over some GPUs, I still had the issue that all gradients where left on one (usually GPU:0) device. That would bottleneck the model size.\r\n\r\nWould the `ParameterServerStrategy` deal with this issue?", "Hello @stefan-falk thanks for the question! I'd suggest visiting https://www.tensorflow.org/guide/distributed_training and see if either [`ParameterServerStrategy`](https://www.tensorflow.org/tutorials/distribute/parameter_server_training) or [`MultiWorkerMirroredStrategy`](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) suits your need. I would give them a try.", "@rchao I'll try to run an example and see if it supports what I need here. Thanks for now :)", "Sounds good. Thanks!"]}, {"number": 52195, "title": "Support torch.nn.fold in tensorflow?", "body": "This function is popular in Vision Models. \r\n\r\nI don't find any api function in tf that is equivalent with [torch.nn.fold](https://pytorch.org/docs/stable/generated/torch.nn.Fold.html?highlight=fold#).\r\n\r\nSo does Tensowflow already have such a function or is there a plan to do so?\r\n", "comments": ["You can refer [this](https://github.com/tensorflow/fold/blob/master/tensorflow_fold/g3doc/index.md) document on Tensorflow Fold.", "hi @sachinprasadhs, thanks for your reply.\r\nAccording to the official documentation,\r\n> The *Tensorflow Fold* is  a library for creating TensorFlow models that consume structured data\r\n\r\nSo it's a library used for Dynamic Computation Graphs, which is different with the Pytorch function [torch.nn.fold](https://pytorch.org/docs/stable/generated/torch.nn.Fold.html?highlight=fold#).", "Thanks for the feature request, we don't have a plan at this moment but contributions welcome. (probably https://www.tensorflow.org/addons is a better place initially)", "How i can contribute\r\nplease guide me\r\nI like to solve this problem", "@rohan100jain fyi"]}, {"number": 52194, "title": "Object Detection Android App Crash", "body": "I am trying to convert my custom `MobileNet Single Shot Detector (v2)`  TF 1.x to `tflite` using [Roboflow tutorial colab](https://colab.research.google.com/drive/1qXn9q6m5ug7EWJsJov6mHaotHhCUY-wG?usp=sharing). After Conversion I deploy it on [Tensorflow android app demo ](https://github.com/tensorflow/examples.git). When I run the app it always crashes after lunch immediately showing the following error:\r\n'''\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\nProcess: org.tensorflow.lite.examples.detection, PID: 10743\r\njava.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.\r\nat org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithByteBuffer(Native Method)\r\nat org.tensorflow.lite.task.vision.detector.ObjectDetector.access$100(ObjectDetector.java:86)\r\nat org.tensorflow.lite.task.vision.detector.ObjectDetector$3.createHandle(ObjectDetector.java:211)\r\nat org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)\r\nat org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromBufferAndOptions(ObjectDetector.java:207)\r\nat org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.(TFLiteObjectDetectionAPIModel.java:87)\r\nat org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:81)\r\nat org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\r\nat org.tensorflow.lite.examples.detection.CameraActivity$7.onPreviewSizeChosen(CameraActivity.java:446)\r\nat org.tensorflow.lite.examples.detection.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:357)\r\nat org.tensorflow.lite.examples.detection.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:362)\r\nat org.tensorflow.lite.examples.detection.CameraConnectionFragment.access$300(CameraConnectionFragment.java:66)\r\nat org.tensorflow.lite.examples.detection.CameraConnectionFragment$3.onSurfaceTextureAvailable(CameraConnectionFragment.java:171)\r\nat android.view.TextureView.getTextureLayer(TextureView.java:415)\r\nat android.view.TextureView.draw(TextureView.java:360)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21389)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21380)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21380)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.draw(View.java:22538)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21389)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat androidx.coordinatorlayout.widget.CoordinatorLayout.drawChild(CoordinatorLayout.java:1277)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.draw(View.java:22538)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21389)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21380)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21380)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21380)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21380)\r\nat android.view.View.draw(View.java:22254)\r\nat android.view.ViewGroup.drawChild(ViewGroup.java:4541)\r\nat android.view.ViewGroup.dispatchDraw(ViewGroup.java:4302)\r\nat android.view.View.draw(View.java:22538)\r\nat com.android.internal.policy.DecorView.draw(DecorView.java:848)\r\nat android.view.View.updateDisplayListIfDirty(View.java:21389)\r\nat android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:559)\r\nat android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:565)\r\nat android.view.ThreadedRenderer.draw(ThreadedRenderer.java:647)\r\nat android.view.ViewRootImpl.draw(ViewRootImpl.java:4417)\r\nat android.view.ViewRootImpl.performDraw(ViewRootImpl.java:4144)\r\nat android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:3391)\r\nat android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:2182)\r\nat android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:8730)\r\nat android.view.Choreographer$CallbackRecord.run(Choreographer.java:1352)\r\nat android.view.Choreographer.doCallbacks(Choreographer.java:1149)\r\nat android.view.Choreographer.doFrame(Choreographer.java:1049)\r\nat android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:1333)\r\nat android.os.Handler.handleCallback(Handler.java:938)\r\nat android.os.Handler.dispatchMessage(Handler.java:99)\r\nat android.os.Looper.loop(Looper.java:233)\r\nat android.app.ActivityThread.main(ActivityThread.java:8010)\r\nat java.lang.reflect.Method.invoke(Native Method)\r\nat com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:631)\r\nat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:978)\r\nI/Process: Sending signal. PID: 10743 SIG: 9\r\n'''", "comments": ["@tilakrayal Could you assist with this problem\r\n", "@myasser63 ,\r\nLooks like this is duplicate of issue #52183.Can you please close this issue, since it is already being tracked there? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "@myasser63 did you fix the problem?\r\n", "> @myasser63 did you fix the problem?\r\n\r\nNot yet, I have tried many times with different models and still getting the same issue\r\n\r\nDo you have a solution?", "> > @myasser63 did you fix the problem?\r\n> \r\n> Not yet, I have tried many times with different models and still getting the same issue\r\n> \r\n> Do you have a solution?\r\n\r\nUnfortunately no. ", "@tilakrayal could you help us on this, please?", "@myasser63 ,\r\nCan you please let us know the tensorflow version you are using.Thanks!", "I used tensorflow object detection with TF 1.x\r\n", "I used tensorflow obj detection with TF 2.x", "@myasser63 ,\r\nWe see that you are using tf version 1.x which is not actively supported, please update to latest stable v2.6 and let us know if you are using same issue.", "@tilakrayal on my case I used tf 2.6. what do you think about that?\r\n", "@myasser63 I have solved my problem by downgrading tensor version from 2.6 to 2.5. and everything worked for me\r\n", "@mirjalal Thanks I will give it a try. May I know which object detection model you used\r\n", "it is custom model but pre-trained ssd mobilnetx640 model was used\r\n", "which version of tf are you used?", "@sachinprasadhs I tried again converting `SSD_mobilenetv2` to `tflite` using TF v2.6 and got the same error as the previous error using TF1.x. \r\n\r\nThe error I got when I run the app:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: main\r\n    Process: org.tensorflow.lite.examples.detection, PID: 2960\r\n    java.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithByteBuffer(Native Method)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.access$100(ObjectDetector.java:86)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector$3.createHandle(ObjectDetector.java:211)\r\n        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromBufferAndOptions(ObjectDetector.java:207)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.<init>(TFLiteObjectDetectionAPIModel.java:87)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:81)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:99)\r\n        at org.tensorflow.lite.examples.detection.CameraActivity.onPreviewFrame(CameraActivity.java:200)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1227)\r\n        at android.os.Handler.dispatchMessage(Handler.java:106)\r\n        at android.os.Looper.loop(Looper.java:223)\r\n        at android.app.ActivityThread.main(ActivityThread.java:7656)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:592)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:947)", "@myasser63 did you try with 2.5 version as @mirjalal claimed that it worked for him after downgrading the Tensorflow version, also can you try the same with tf-nightly version as well. Thank you.", "I have tried TF v2.5.0 and the error still persists", "I am getting now this error:\r\n\r\nE/AndroidRuntime: FATAL EXCEPTION: DefaultDispatcher-worker-1\r\n    Process: org.tensorflow.codelabs.objectdetection, PID: 13744\r\n    java.lang.AssertionError: Error occurred when initializing ObjectDetector: Output tensor at index 0 is expected to have 3 dimensions, found 2.\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.initJniWithModelFdAndOptions(Native Method)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.access$000(ObjectDetector.java:86)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector$1.createHandle(ObjectDetector.java:152)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector$1.createHandle(ObjectDetector.java:145)\r\n        at org.tensorflow.lite.task.core.TaskJniUtils$1.createHandle(TaskJniUtils.java:70)\r\n        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromLibrary(TaskJniUtils.java:91)\r\n        at org.tensorflow.lite.task.core.TaskJniUtils.createHandleFromFdAndOptions(TaskJniUtils.java:66)\r\n        at org.tensorflow.lite.task.vision.detector.ObjectDetector.createFromFileAndOptions(ObjectDetector.java:143)\r\n        at org.tensorflow.codelabs.objectdetection.MainActivity.runObjectDetection(MainActivity.kt:127)\r\n        at org.tensorflow.codelabs.objectdetection.MainActivity.access$runObjectDetection(MainActivity.kt:48)\r\n        at org.tensorflow.codelabs.objectdetection.MainActivity$setViewAndDetect$1.invokeSuspend(MainActivity.kt:183)\r\n        at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:33)\r\n        at kotlinx.coroutines.DispatchedTask.run(DispatchedTask.kt:106)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely(CoroutineScheduler.kt:571)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.executeTask(CoroutineScheduler.kt:750)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.runWorker(CoroutineScheduler.kt:678)\r\n        at kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run(CoroutineScheduler.kt:665)\r\n", "@sachinprasadhs I tried Tensorflow model maker with TF V2.5 and V2.6 and I got the same error above when I launch the app in android studio.", "@lu-wang-g Can you please have a look if something missing when using the example app.\r\n\r\nThanks", "The first issue, \"java.lang.AssertionError: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.\", was due to lack of metadata. To be more specific, floating models require metadata information of NormalizationOptions. You can add metadata using [MetadataWriter API for ObjectDetector](https://www.tensorflow.org/lite/convert/metadata_writer_tutorial#object_detectors).\r\n\r\nThe second issue, \"Output tensor at index 0 is expected to have 3 dimensions, found 2.\" is due an update in TF 2.6 that the order of the output tensor may get changed. Model Maker and Task Library have fixed the issue in the nightly build. In the reference app, you need to depend on the nightly build of Task library,  i.e. changing [this](https://github.com/tensorflow/examples/blob/f9dfe1d72d3a2e4ae9673eddf0e416c3c6ddaeaf/lite/examples/object_detection/android/lib_task_api/build.gradle#L42) to\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite-task-vision:0.0.0-nightly-SNAPSHOT'\r\n```\r\nOr, you can roll back Model Maker, TF, Task Library to some old version."]}, {"number": 52193, "title": "API to explicitly label python input arguments in ``tf.function``", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf 2.5.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nIn Jax, ``tf.function``'s equivalent ``jax.jit`` has an argument ``static_argnums=(1, )``, which explicitly tells the compiler that the second(1) argument of the function to be jitted is a python object instead of a tensor, and the function should be retraced for each different argument in ``static_argnums``.\r\n\r\nIt seems to me that, ``tf.function`` has no corresponding API. Though, in general case, the function wrapped by ``tf.function`` can automatically determine whether the input is a tensor or a python object. Such automatic control fails when the function jitted is a function with ``tf.custom_gradient``. Please see the demo below.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass TensorWrapper:\r\n    def __init__(self, tensor):\r\n        self._tensor = tensor  # suppose we have no way to directly access the tensor\r\n\r\n    def plus(self, other):\r\n        # only public API\r\n        return other + self._tensor\r\n\r\n    def __str__(self):\r\n        return \"TensorWrapper(\" + self._tensor.__str__() + \")\"\r\n\r\n    __repr__ = __str__\r\n\r\n\r\nTW_list_tf = list(map(TensorWrapper, [tf.zeros([2, 2]), tf.ones([2, 2])]))\r\n\r\nprint(TW_list_tf)\r\n\r\n\r\ndef u_tf(a, b):\r\n    return tf.reduce_sum(TW_list_tf[b].plus(a))\r\n\r\nprint(\"u_tf \", u_tf(tf.zeros([2, 2]), 1))\r\n\r\nu_tf_jit = tf.function(u_tf)\r\n\r\nprint(\"u_tf_jit\", u_tf_jit(tf.zeros([2, 2]), 1))\r\n\r\na = tf.zeros([2, 2])\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(a)\r\n    loss = u_tf(a, 1)\r\nprint(\"u_tf grad \", tape.gradient(loss, a))\r\n\r\na = tf.zeros([2, 2])\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(a)\r\n    loss = u_tf_jit(a, 1)\r\nprint(\"u_tf_jit grad \", loss, tape.gradient(loss, a))\r\n\r\n@tf.custom_gradient\r\ndef u_tf_grad_v2(a, b):\r\n    r = u_tf(a, b)\r\n\r\n    def grad(dr):\r\n        return 2.0 * dr * tf.ones_like(a), tf.zeros_like(b)\r\n\r\n    return r, grad\r\n\r\na = tf.zeros([2, 2])\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(a)\r\n    loss = u_tf_grad_v2(a, 1)\r\nprint(\"u_tf_grad_v2 grad \", tape.gradient(loss, a))\r\n\r\nu_tf_grad_v2_jit = tf.function(u_tf_grad_v2)\r\n\r\ntry:\r\n    a = tf.zeros([2, 2])\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(a)\r\n        loss = u_tf_grad_v2_jit(a, 1)\r\n    print(loss, tape.gradient(loss, a))\r\nexcept Exception as e:\r\n    print(\"u_tf_grad_v2_jit grad:\", e)\r\n# list indices must be integers or slices, not Tensor since int b is compiled to tensor\r\n```\r\n\r\nThere are two ways to solve the final exception. Add ``nondiff_argnums`` arguments in ``tf.custom_gradient`` API, or better add ``static_argnums`` arguments in ``tf.function`` API. Or is there currently any workaround for this ``tf.custom_gradient`` + ``tf.function`` + non tensor input issue?\r\n\r\n\r\n**Will this change the current api? How?**\r\nYes, a new arguments ``static_argnums=`` should be added in ``tf.function()`` API.\r\n\r\n**Who will benefit with this feature?**\r\nEveryone wants better control on the jit behavior in tf and enjoys writting highly customized code with tf.\r\n\r\n**Any Other info.**\r\n", "comments": ["Hi @sanatmpa1!Could you please look at this feature request!", "`tf.function` lets you explicitly treat arguments as tensors in two ways:\r\n\r\n1. Add an `input_signature` - that lets you specify that `b` is a `Tensor`\r\n2. Add type annotations, e.g. `u_tf(a: tf.Tensor, b: tf.Tensor)` and then use `tf.function(experimental_follow_type_hints=True)(u_tf)` (side note: for more complete type annotations, you should use `tf.types.experimental.TensorLike`)\r\n\r\nDo any of those work for you?", "@mdanatg , thanks for your reply. But the problem is that I want to label explicitly some arguments of the function as **non-Tensor** instead of ``tf.tensor``.", "I see, what you seem to need is really a @custom_gradient with non-Tensor arguments. That seems to be supported when running eagerly, but inside tf.function, it seems to [convert everything to a tensor](https://github.com/tensorflow/tensorflow/blob/5d8492ab786724ed811588183c93b1623f02c3c5/tensorflow/python/ops/custom_gradient.py#L410). That looks like a bug. I'm also not sure if custom_gradient interacts well with tf.function. This is what is being done more commonly, and is what should work in your case:\r\n\r\n```\r\n@tf.custom_gradient\r\ndef u_tf_grad_v2(a, b):\r\n    r = u_tf_jit(a, b)\r\n\r\n    @tf.function\r\n    def grad(dr):\r\n        return 2.0 * dr * tf.ones_like(a), tf.zeros_like(b)\r\n\r\n    return r, grad\r\n```\r\n\r\nThat said, calling this custom gradient from within another tf.function won't work until the custom_gradient bug is fixed.", "> I see, what you seem to need is really a @custom_gradient with non-Tensor arguments.\r\n\r\nYes, this is exactly the issue.\r\n\r\n> it seems to convert everything to a tensor\r\n\r\nI believe the linked code is exactly the source of the problem\r\n\r\n> That said, calling this custom gradient from within another tf.function won't work until the custom_gradient bug is fixed.\r\n\r\nYes, and an outside ``tf.function`` is often needed.", "In this example, there is a way to avoid the bug, even inside a tf.function, if you can avoid passing the non-Tensor argument to the @custom_gradient. You can do that by closing over it. Things should be fine so long as b is still an argument of an enclosing tf.function. Like for example:\r\n\r\n```\r\n@tf.function\r\ndef call_with_custom_gradient(a, b):\r\n  @tf.custom_gradient\r\n  def u_tf_grad_v2(a):\r\n      r = u_tf_jit(a, b)  # close over b\r\n\r\n      def grad(dr):\r\n          return 2.0 * dr * tf.ones_like(a)  # don't calculate grads wrt b (it doesn't make sense anyway)\r\n\r\n      return r, grad\r\n  return u_tf_grad_v2(a)\r\n\r\n\r\na = tf.zeros([2, 2])\r\nwith tf.GradientTape() as tape:\r\n    tape.watch(a)\r\n    loss = call_with_custom_gradient(a, 1)\r\nprint(\"u_tf_grad_v2 grad \", tape.gradient(loss, a))\r\n```", "@mdanatg , wow, thank you! This closure idea is brilliant!"]}, {"number": 52167, "title": "tf.image.resize() behaves differently when wrapped in a tf.data.Dataset.map()", "body": "`tf.image.resize()` gives different results if used as is (plain) or if wrapped in a `tf.data.Dataset.map()`, however, I except the output to be the same in both cases. The behaviour is shown in the output of the snipped below.\r\n\r\nSpecifically, this issue seems related to the specific interpolation method `tf.image.ResizeMethod.BILINEAR` (default) with uses `tensorflow.python.ops.gen_image_ops.resize_bilinear` under-the-hood. Changing the interp method (e.g. to `tf.image.ResizeMethod.BICUBIC`, or setting bilinear + `antialias=True`, which uses a different method) no longer show any differences.\r\n\r\nEnvironment:\r\n- Tensorflow 2.6.0\r\n- Python 3.8.6\r\n- Linux 20.04\r\n- Driver Version: 460.32.03\r\n- CUDA Version: 11.2\r\n- cuDNN 8.1.0\r\n- NVIDIA T4 GPU (16GB)\r\n\r\nReproducible script:\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nprint(tf.__version__)\r\n\r\n@tf.function\r\ndef tf_read_and_resize_image(filename):\r\n    image_string = tf.io.read_file(filename)\r\n    image = tf.image.decode_jpeg(image_string, channels=3)\r\n    image=tf.cast(image, tf.float32)\r\n    image = tf.image.resize(image, (224, 224), method=tf.image.ResizeMethod.BILINEAR)\r\n    return image\r\n\r\n# create random uint8 image\r\nnp.random.seed(42)\r\nim = np.random.randint(low=0, high=255, size=(100, 100, 3), dtype=np.uint8)\r\nim_filename = \"/tmp/test_image.jpg\"\r\nim = Image.fromarray(im).save(im_filename)\r\n\r\n# plain function\r\nplain_tensor = tf_read_and_resize_image(im_filename)\r\n\r\n# wrapped in tf.data map()\r\nds = tf.data.Dataset.from_tensor_slices([im_filename])\r\nds = ds.map(tf_read_and_resize_image)\r\nds_tensor = next(ds.as_numpy_iterator())\r\n\r\nassert plain_tensor.dtype == ds_tensor.dtype == \"float32\"\r\nassert plain_tensor.shape == ds_tensor.shape == (224, 224, 3)\r\nnp.testing.assert_array_equal(plain_tensor, ds_tensor) # fails with ~11% diff.\r\n```\r\n\r\nOutput:\r\n```\r\n(tf_issue) root@f1f0235af85c:/srv/2D3Dreg/deeplearning# python test.py \r\n2.6.0\r\n2021-09-28 14:10:08.987749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:08.997055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:08.997667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:08.998427: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-28 14:10:08.999018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:08.999644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:09.000204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:09.584861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:09.585487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:09.586072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-28 14:10:09.586636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13666 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\n2021-09-28 14:10:09.596141: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 30, in <module>\r\n    np.testing.assert_array_equal(input_tensor_read, ds_read) # fails with ~11% diff.\r\n  File \"/srv/2D3Dreg/deeplearning/tf_issue/lib/python3.8/site-packages/numpy/testing/_private/utils.py\", line 930, in assert_array_equal\r\n    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\r\n  File \"/srv/2D3Dreg/deeplearning/tf_issue/lib/python3.8/site-packages/numpy/testing/_private/utils.py\", line 840, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\nMismatched elements: 16381 / 150528 (10.9%)\r\nMax absolute difference: 0.00051498\r\nMax relative difference: 1.0005011e-05\r\n x: array([[[164.      , 208.      , 131.      ],\r\n        [154.83928 , 198.83928 , 124.55357 ],\r\n        [130.73215 , 174.73215 , 107.58929 ],...\r\n y: array([[[164.      , 208.      , 131.      ],\r\n        [154.83928 , 198.83928 , 124.55357 ],\r\n        [130.73215 , 174.73215 , 107.58929 ],...\r\n```", "comments": ["Hi @goncinious! , I tried to replicate this issue with  the code  you shared  in Colab ,Could not find the above Error Stack Trace . Can you please share the same as Colab Gist? Attaching [GIST](https://colab.research.google.com/gist/mohantym/09fd474860213fb96cecb108e3403d59/github_52167.ipynb) for reference.", "Thanks @mohamedadaly,\r\n\r\nI wasn't able to reproduce the result in the Colab environment either (with or without GPU support), but I was able to reproduce it using the official [Tensorflow Docker images](https://www.tensorflow.org/install/docker#download_a_tensorflow_docker_image), which is similar to my setup (also running from within a Docker container).\r\n\r\nEnvironment:\r\n- GPU: NVIDIA Quadro T2000 (also tested on a NVIDIA T4)\r\n- Docker version 20.10.7, build f0df350\r\n- host OS: Ubuntu 20.04\r\n- Driver Version: 460.91.03\r\n\r\n\r\nHere are the steps to reproduce this issue:\r\n\r\n1. Pull official Docker image with GPU support \r\n```\r\ndocker pull tensorflow/tensorflow:latest-gpu\r\n```\r\n\r\n2. Run docker command with GPU support, mounting `test.py` (see [GIST](https://colab.research.google.com/gist/goncinious/c326400ecf1c860ffe69f106c18fefc3/tf_image_resize_debug.ipynb)) script to `/srv`\r\n```\r\ndocker run --network host --gpus all -v /home/goncalo/test.py:/srv/test.py -it tensorflow/tensorflow:latest-gpu /bin/bash\r\n```\r\n\r\n3. Install `PIL` dependency\r\n```\r\npip install Pillow\r\n```\r\n4. `cd` to `/srv` and run test script\r\n```\r\ncd /srv\r\npython test.py\r\n```\r\n\r\nHere's the obtained output:\r\n```\r\nroot@goncalo-laptop:/srv# python test.py\r\n2.6.0\r\n3.6.9 (default, Jan 26 2021, 15:33:00) \r\n[GCC 8.4.0]\r\n2021-09-29 16:34:10.381327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-29 16:34:10.385631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-29 16:34:10.386060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\r\n2021-09-29 16:34:10.443339: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-29 16:34:10.444913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-29 16:34:10.445410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-29 16:34:10.445940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-29 16:34:10.797101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-29 16:34:10.797604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-29 16:34:10.798019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-29 16:34:10.798349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1690 MB memory:  -> device: 0, name: Quadro T2000, pci bus id: 0000:01:00.0, compute capability: 7.5\r\n2021-09-29 16:34:10.806638: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 34, in <module>\r\n    np.testing.assert_array_equal(plain_tensor, ds_tensor) # fails with ~11% diff.\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/testing/_private/utils.py\", line 931, in assert_array_equal\r\n    verbose=verbose, header='Arrays are not equal')\r\n  File \"/usr/local/lib/python3.6/dist-packages/numpy/testing/_private/utils.py\", line 840, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\nMismatched elements: 16381 / 150528 (10.9%)\r\nMax absolute difference: 0.00051498\r\nMax relative difference: 1.0005011e-05\r\n x: array([[[164.      , 208.      , 131.      ],\r\n        [154.83928 , 198.83928 , 124.55357 ],\r\n        [130.73215 , 174.73215 , 107.58929 ],...\r\n y: array([[[164.      , 208.      , 131.      ],\r\n        [154.83928 , 198.83928 , 124.55357 ],\r\n        [130.73215 , 174.73215 , 107.58929 ],...\r\n```\r\n\r\nThe issue appears GPU-related, as the test pass fine without GPU support:\r\n\r\n```\r\nroot@goncalo-laptop:/srv# CUDA_VISIBLE_DEVICES=\"\" python test.py\r\n\r\n2.6.0\r\n3.6.9 (default, Jan 26 2021, 15:33:00) \r\n[GCC 8.4.0]\r\n2021-09-29 16:34:44.403763: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-09-29 16:34:44.403789: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: goncalo-laptop\r\n2021-09-29 16:34:44.403793: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: goncalo-laptop\r\n2021-09-29 16:34:44.403860: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.91.3\r\n2021-09-29 16:34:44.403889: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.91.3\r\n2021-09-29 16:34:44.403915: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.91.3\r\n[]\r\n2021-09-29 16:34:44.463146: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-29 16:34:44.471553: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\r\npassed!\r\n```\r\n\r\n\r\n", "Hi @sanatmpa1! Could you please look at this issue! Issue is not replicating in Colab environment.Providing GIST in [2.5,](https://colab.research.google.com/drive/1cbtFq7U_EDthDCn4AnwhUa-kLdACbcET?resourcekey=0-1DdAxOrBl4XFZqdNx_e1pA#scrollTo=1kwpXalNBgrn),[2.6 ](https://colab.research.google.com/gist/mohantym/09fd474860213fb96cecb108e3403d59/github_52167.ipynb#scrollTo=1kwpXalNBgrn)and [2.7](https://colab.research.google.com/gist/mohantym/e62ed7d4dd5196fee732c2b9b8ee32a3/github_52167.ipynb#scrollTo=1kwpXalNBgrn)  for reference. ", "Solution is always simple I will explain with example.\r\n\r\n* Let's say you are using tensorflow, numpy, pandas, or any other library which contains the function of seeds.\r\n* This packages have their own way to define seeds method.\r\n* So, **Solution is that try to creat the function which is generate the seeds from this all package. this way you can produce same results everytime.**\r\n\r\nExample Function\r\n```python\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef random_seed(seed):\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    tf.random.set_seed(seed)\r\n\r\nrseed = random_seed(2022)\r\n\r\ntf.keras.Dropout(0.25, seed=rseed)\r\n```\r\n\r\nI hope this helps...!!! Feel free to share your thought on this.\r\n\r\n ", "Thanks @ashishpatel26 for your reply.\r\n\r\nFixing the seeds didn't fix the issue described. I can also confirm that the issue persists on the latest TF Docker image (with TF 2.7.0). (see [previous comment](https://github.com/tensorflow/tensorflow/issues/52167#issuecomment-930357745) for steps to reproduce).\r\n\r\nI also tried setting `TF_DETERMINIC_OPS` and `num_parallel_calls=1` in `map()` (see [GIST](https://colab.research.google.com/drive/1IWl9M2yIVxs8_JTCZMShSyNPu-lZkU6I?usp=sharing)), but I continue to get different results using resize + BILINEAR interp method, inside vs. outside of the map() method.\r\n\r\nFinally, I tried the [`nightly-gpu`](https://hub.docker.com/layers/tensorflow/tensorflow/nightly-gpu/images/sha256-cf86db348de66caa93a4248020c41fd783d8a83a9e9dacb42f870a6999547f74?context=explore) image (TF 2.9.0-dev20220124) and used the recently introduced [tf.config.experimental.enable_op_determinism](https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism) function - made no difference.", "@goncinious Is this still an issue. I couldn't reproduce the issue when I ran your code with `tf-nightly` and the following two line at the start of your code. [Here](https://colab.research.google.com/gist/jvishnuvardhan/3624a43bfa923f8d30ddec597bc560b0/tf_image_resize_bilinear_issue.ipynb) is a gist for reference. Thanks!\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.keras.utils.set_random_seed(1)\r\ntf.config.experimental.enable_op_determinism()\r\n```\r\n\r\nCan you please verify once and close the issue if this was resolved for you. Thanks!", "I can confirm that this is still an issue even when the two lines are included at the top of the code. @jvishnuvardhan, have you tested o a machine with GPU? As mentioned in https://github.com/tensorflow/tensorflow/issues/52167#issuecomment-930357745 (see bottom of the comment), this issue only seems to occur when running on GPU.\r\n\r\nI've just tested the latest `nightly-gpu` TensorFlow container (`sha256:bcf90aa34b1dd1322c981c780bf7ee8f6546ecc27b2175b3ed28ab883173cb29`) and the issue persists when running on NVIDIA A10G or Tesla T4 GPUs.", "@goncinious Looks like team is working on updating ops to work as deterministic. Thanks! ", "Thanks for the update, @jvishnuvardhan! Could you please also update the tag `2.6.0` -> `2.8.0`, since this concerns the latest version of TF as well?"]}, {"number": 52160, "title": "Make libTensorFlow C++ library work for the latest M1 Mac", "body": "The M1 Mac with it's own non-Intel processor was released ten months ago but is still not supported by the libTensorFlow C++ library **https://www.tensorflow.org/install/lang_c**.  It would be good to get that done so that we can release apps for the Mac.  As far as I know TensorFlow does work for the M1 Mac in Python.\r\n\r\nYou can test the library file compatibility by running xcrun as follows:-\r\nRun the XCode command: xcrun lipo -info \"libtensorflow.dylib\" and it returns \"libtensorflow.dylib is architecture x86_64\".  \r\n\r\nFor it to support the Intel Mac and the M1 Mac it should return \"libtensorflow.dylib are: arm64 x86_64\" (ie. both formats so that the compiler can build the app for Intel, Arm or both (the default)).", "comments": ["Hi @Eric2016Lv ! \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code/colab gist to reproduce the issue faced].Thanks", "Sorry, can't see an \"issue template\".  Sorry don't use colab.  The issue is quite clear:-  The C++ library ( **https://www.tensorflow.org/install/lang_c** ) doesn't work for the new Mac released almost a year ago.", "Hi @EricTheRed20! ,Sorry for the late response. \r\nCould you please look at the answer of these threads[ thread1](https://stackoverflow.com/questions/63609132/what-is-a-proper-command-to-build-tensorflow-lite-c-api-for-macos) ,[thread2](https://github.com/karthickai/tflite) for now. It suggests building Tensorflow C++ using Bazel .", "Thanks for that.  Neither of those links give a solution.  They are both for TensorFlow \"Lite\" which is a cut down version of the full library.  Unfortunately TensorFlow \"Lite\" doesn't have the full functionality required and the full [libTensorFlow C++ library](https://www.tensorflow.org/install/lang_c) is needed.", "Hi @jvishnuvardhan ! Could you please look into this issue!", "@EricTheRed20 I didn't try it. But I guess it should work. With `bazel 4.2.x`,\r\n```\r\nbazel build --config opt --cpu=darwin_arm64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n```\r\nThat's what I used to build `//tensorflow/tools/pip_package:build_pip_package` on my M1 machine.", "Ouch.  Yes, you are right.  It might work.  I've been down that track before - of building from source and it was a nightmare.  I think that it a good idea for someone who knows what they are doing to update the library for the Mac so that it works in XCode for both Intel and the M1 processor.   As you say they have done it for pip.  They don't however ask people to build from scratch for pip.  Can someone build it for the Mac and update the [library page](https://www.tensorflow.org/install/lang_c) so that it really does work for \"macOS, Version 10.12.6 (Sierra) or higher\".\r\n\r\nThe last build was about a month ago.  Could you guys ask the person that built it to build it for the Intel processor AND the M1 processor and then update the [library page](https://www.tensorflow.org/install/lang_c).  \r\n\r\nThe new M1 processor has been out for about ten months and we are hanging out to be able to ship to our M1 Mac customers.", "I can confirm that `bazel build --config opt --cpu=darwin_arm64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow` with bazel 4.2.1 works on my M1 machine.\r\n\r\n```\r\nfreedom@myway-m1 ~ % uname -a\r\nDarwin myway-m1.local 21.1.0 Darwin Kernel Version 21.1.0: Thu Sep 23 21:17:12 PDT 2021; root:xnu-8019.40.86.181.1~2/RELEASE_ARM64_T8101 arm64\r\nfreedom@myway-m1 ~ % /tmp/hello_tf  \r\nHello from TensorFlow C library version 2.8.0\r\nfreedom@myway-m1 ~ % file /tmp/hello_tf\r\n/tmp/hello_tf: Mach-O 64-bit executable arm64\r\n```\r\n\r\nI don't work for TensorFlow or Google. I guess there is no darwin_arm64 binary build available because tf master branch is still with bazel 3.7.2 which doesn't support darwin_arm64 well. \r\n", "Thanks for doing that.  It is good news that it compiles for the M1 Mac.   Did it build all six .dylib files.\r\n\r\nThe ideal would be to have one library (or set of libraries) that supports both the Intel Mac and the M1 Mac.  I have a library that does that and it returns the value \"libtensorflow.dylib are: arm64 x86_64\" when you run the command above.  You can then include it in an XCode project and XCode will build an executable that works for all current Macs - Intel and M1.  Magic.  It avoids the pain of getting Mac users to install the right version because for our customers they are pretty computer illiterate and we need to step them through the most basic steps.\r\n\r\nIs it possible to build one library (or set of libraries) that supports both processors?", "We don't have access to M1 devices to build and test on so the only way this can progress is if someone from community owns the build and sends the relevant PRs.", "> Thanks for doing that.  It is good news that it compiles for the M1 Mac.   Did it build all six .dylib files.\r\n> \r\n> \r\n> \r\n> The ideal would be to have one library (or set of libraries) that supports both the Intel Mac and the M1 Mac.  I have a library that does that and it returns the value \"libtensorflow.dylib are: arm64 x86_64\" when you run the command above.  You can then include it in an XCode project and XCode will build an executable that works for all current Macs - Intel and M1.  Magic.  It avoids the pain of getting Mac users to install the right version because for our customers they are pretty computer illiterate and we need to step them through the most basic steps.\r\n> \r\n> \r\n> \r\n> Is it possible to build one library (or set of libraries) that supports both processors?\r\n\r\nDunno if there is a way to build fat/universal binaries with current tensorflow and bazel settings. At least, you can build binaries for both architectures separately and use tools, such as lipo, to create universal binaries. E.g., with\r\n```\r\nbazel build --config opt --cpu=darwin_x86_64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n```\r\nyou should be able to get x86_64.\r\n\r\nYou may want to check apple's [universal binary doc](https://developer.apple.com/documentation/apple-silicon/building-a-universal-macos-binary)", "To have the .dylib files have the code for both types of processor would be the best.  It makes building a \"Universal Binary\" straight forward because it is the default in XCode.   Everything just runs on both types of processor and it is \"Magic\".  I don't know how it would work having two .dylib files for each library.  Would be messy at best.\r\n\r\nIs it Ok to offer money to get someone to do a build which supports both processors (in one set of .dylib files) so that a Universal Binary is easy to make.  Or if I offer money, will I be hanged by the neck at dawn outside Googles head office for violating some terms and conditions that I don't know about yet.\r\n\r\nI really think that TensorFlow should support the creation of Universal Binaries on a Mac for their [C++ library](https://www.tensorflow.org/install/lang_c).", "I followed the steps for [building a pip from source](https://www.tensorflow.org/install/source#macos) but using your \"bazel build\" command.  I'm getting the error \"ERROR: The project you're trying to build requires Bazel 3.7.2 (specified in /Users/Eric/.../tensorflow/.bazelversion), but it wasn't found in /opt/homebrew/Cellar/bazel/4.2.1_1/libexec/bin.\"\r\n\r\nWhy is it asking me to use an old version of Bazel?\r\n\r\nIt would be good if they would update the [C++ Library](https://www.tensorflow.org/install/lang_c) page so that they support universal binaries for the Mac rather than only supporting old Macs.   It shouldn't be our job to build from source.", "Currently TF does not build with Bazel 4.", "@freedomtan I tried what you posted here above, however, this does not work. This is what I did\r\n1. Clone TF  @ commit https://github.com/tensorflow/tensorflow/commit/274df9b02330b790aa8de1cee164b70f72b9b244]\r\n2. Modify .bazelversion to 4.2.1\r\n3. Install Bazel 4.2.1 for arm64\r\n4. `bazel build --config opt --cpu=darwin_arm64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow `[Note, with --host_cpu, not --host-cpu as in your instructions]\r\n\r\nThis leads to:\r\n\r\n```\r\nERROR: /private/var/tmp/_bazel_kgoderis/faf5712d1547963f5eabb3daf14d653d/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\n```\r\n\r\n[Edit/Silly me : wondering if I don't need Xcode here for the toolchain, still in the process of setting up my new M1 machine]", "> This leads to:\r\n> \r\n> ```\r\n> ERROR: /private/var/tmp/_bazel_kgoderis/faf5712d1547963f5eabb3daf14d653d/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\n> ERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\n> ```\r\n\r\n@kgoderis \r\nDid you ever figure this out? If not, I had the same issue which persisted even after installing a complete version of XCode (version < 13 since I'm still on Big Sur), and the solution ended up being that I had to run it with `sudo`... \ud83e\udd26 ", "@TeoZosa No, I never thought of running that as superuser. I does goes past the error for the time being, fingers crossed. ", "I confirm this works, although it generates a monster file ;-)\r\n\r\n```\r\n(base) kgoderis@Karels-M1-MacBook-Pro tensorflow % file libtensorflow.2.8.0.dylib\r\nlibtensorflow.2.8.0.dylib: Mach-O 64-bit dynamically linked shared library arm64\r\n(base) kgoderis@Karels-M1-MacBook-Pro tensorflow % file libtensorflow_framework.2.8.0.dylib\r\nlibtensorflow_framework.2.8.0.dylib: Mach-O 64-bit dynamically linked shared library arm64\r\n\r\n```", "Hi guys!\r\nIn case you are looking for a compiled arm64 TensorFlow C library for M1 Apple silicon:\r\nhttps://github.com/vodianyk/libtensorflow-cpu-darwin-arm64", "@vodianyk greatly appreciated! Could you share a script or a list of commands that enabled you to build it?", "Hey @janhartman \r\n\r\nHere you go:\r\n1. `git clone https://github.com/tensorflow/tensorflow.git`\r\n2. `cd tensorflow`\r\n3. `vim .bazelversion // change bazel version (in my case 3.7.2 to 4.2.1)`\r\n4. `bazel build -c opt //tensorflow/tools/lib_package:libtensorflow`\r\n5. .. wait for the build to finish\r\n6. It will produce an archive at bazel-bin/tensorflow/tools/lib_package/libtensorflow.tar.gz\r\n", "> Hey @janhartman\r\n> \r\n> Here you go:\r\n> \r\n> 1. `git clone https://github.com/tensorflow/tensorflow.git`\r\n> 2. `cd tensorflow`\r\n> 3. `vim .bazelversion // change bazel version (in my case 3.7.2 to 4.2.1)`\r\n> 4. `bazel build -c opt //tensorflow/tools/lib_package:libtensorflow`\r\n> 5. .. wait for the build to finish\r\n> 6. It will produce an archive at bazel-bin/tensorflow/tools/lib_package/libtensorflow.tar.gz\r\n\r\nHi, could you possibly upload a `2.3.1` version build? I don't have enough experience on compiling libraries like these. It would be greatly appreciated if you could do this.\r\n\r\nUpdate;\r\n\r\nI've attempted to do this myself and keep receiving errors left and right on `v2.3.1`. I've followed everything given here.", "None of the above worked for me, but this https://github.com/bazelbuild/bazel/issues/13514#issuecomment-847917936 did.", "@vodianyk repo has been the only thing that worked for me, although if possible is there a solution to get the full C++ api rather than pure C?\r\n\r\nfollowing some @davidxia answer, this solved one issue and caused another with protobuff which I haven't been able to solve on my own M1. \r\n\r\n@vodianyk do you have any ideas for the full C++ api? \r\n\r\nhappy to run M1 tests if people want to try approaches but do not have access to the architecture", "> ```\r\n> bazel build --config opt --cpu=darwin_arm64 --host-cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n> ```\r\n\r\nERROR: --host-cpu=darwin_arm64 :: Unrecognized option: --host-cpu=darwin_arm64\r\n", "> > ```\r\n> > bazel build --config opt --cpu=darwin_arm64 --host-cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n> > ```\r\n> \r\n> \r\n> ERROR: --host-cpu=darwin_arm64 :: Unrecognized option: --host-cpu=darwin_arm64\r\n\r\nSame result", "@juancrescente @Mblakey Did you install Xcode?", "@janhartman  yes, tried with both standalone command line tools and full Xcode install, error persists ", "What happens if you remove the `host-cpu` param? I didn't need it (built from `r2.7` branch with Bazel 4.2.1)", "@janhartman \r\n\r\n```\r\nIn file included from external/com_google_protobuf/src/google/protobuf/any_lite.cc:36:\r\nIn file included from external/com_google_protobuf/src/google/protobuf/stubs/strutil.h:41:\r\nexternal/com_google_protobuf/src/google/protobuf/port_def.inc:74:2: error: PROTOBUF_DEPRECATED was previously defined\r\n#error PROTOBUF_DEPRECATED was previously defined\r\n ^\r\nexternal/com_google_protobuf/src/google/protobuf/any_lite.cc:60:19: error: out-of-line definition of 'InternalPackFrom' does not match any declaration in 'google::protobuf::internal::AnyMetadata'\r\nvoid AnyMetadata::InternalPackFrom(const MessageLite& message,\r\n                  ^~~~~~~~~~~~~~~~\r\nexternal/com_google_protobuf/src/google/protobuf/any_lite.cc:69:19: error: out-of-line definition of 'InternalUnpackTo' does not match any declaration in 'google::protobuf::internal::AnyMetadata'\r\nbool AnyMetadata::InternalUnpackTo(StringPiece type_name,\r\n                  ^~~~~~~~~~~~~~~~\r\nexternal/com_google_protobuf/src/google/protobuf/any_lite.cc:71:19: error: no viable conversion from 'google::protobuf::StringPiece' to 'const google::protobuf::Descriptor *'\r\n  if (!InternalIs(type_name)) {\r\n                  ^~~~~~~~~\r\n/usr/local/include/google/protobuf/stubs/stringpiece.h:316:3: note: candidate function\r\n  operator string() const {\r\n  ^\r\n/usr/local/include/google/protobuf/any.h:79:37: note: passing argument to parameter 'message' here\r\n  bool InternalIs(const Descriptor* message) const;\r\n                                    ^\r\nexternal/com_google_protobuf/src/google/protobuf/any_lite.cc:97:19: error: out-of-line definition of 'InternalIs' does not match any declaration in 'google::protobuf::internal::AnyMetadata'\r\nbool AnyMetadata::InternalIs(StringPiece type_name) const {\r\n                  ^~~~~~~~~~\r\n4 warnings and 5 errors generated.\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 5.401s, Critical Path: 1.02s\r\nINFO: 11 processes: 9 internal, 2 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "@janhartman above is without the host tag\r\n", "> > ```\r\n> > bazel build --config opt --cpu=darwin_arm64 --host-cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n> > ```\r\n> \r\n> ERROR: --host-cpu=darwin_arm64 :: Unrecognized option: --host-cpu=darwin_arm64\r\n\r\n`host-cpu` was my typo. should be `host_cpu`. However, it was for older master and older bazel. For recent master, with native JDK and native `bazel 5.0.0` , you should be able build the library for native format (either x86_64 or m1) with\r\n```\r\nbazel build --config opt  //tensorflow/tools/lib_package:libtensorflow\r\n```", "@freedomtan \r\n```\r\n\r\n(tf) \u279c  tensorflow git:(master) \u2717 bazel build --config opt  //tensorflow/tools/lib_package:libtensorflow\r\n\r\n~/miniforge3/envs/tf/bin ~/Documents/lib/tensorflow\r\n~/Documents/lib/tensorflow\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=170\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --host_java_toolchain=//tensorflow/tools/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/user/miniforge3/envs/tf/bin/python3 --action_env PYTHON_LIB_PATH=/Users/user/miniforge3/envs/tf/lib/python3.9/site-packages --python_path=/Users/user/miniforge3/envs/tf/bin/python3\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /Users/user/Documents/lib/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/user/Documents/lib/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:macos in file /Users/user/Documents/lib/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nINFO: Build options --copt, --cpu, and --host_copt have changed, discarding analysis cache.\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace0.bzl:110:34: in workspace\r\n  /Users/user/miniforge3/envs/tf/share/bazel/7d4fd1924864d2d6fce922e4404d7479/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /Users/user/miniforge3/envs/tf/share/bazel/7d4fd1924864d2d6fce922e4404d7479/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nERROR: /Users/user/miniforge3/envs/tf/share/bazel/7d4fd1924864d2d6fce922e4404d7479/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nINFO: Repository nsync instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:878:21: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:459:20: in _tf_repositories\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository com_google_absl instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:871:28: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:59:9: in _initialize_third_party\r\n  /Users/user/Documents/lib/tensorflow/third_party/absl/workspace.bzl:39:20: in repo\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 0.636s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (18 packages loaded, 361 targets configured)\r\n```\r\n", "The following steps worked for me. thank you @freedomtan & @janhartman \r\n\r\n```\r\n1. Fresh install of Xcode 13.2.0\r\n2. install command line tools3. \r\n```\r\n* command line tools first and then Xcode install DOES NOT WORK\r\n\r\n```\r\nbrew install bazelisk \r\nbrew install autoconf automake libtool cmake\r\n```\r\n* libtool and autoconf cause issues with protobuf, fresh brew install solves\r\n\r\n```\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\ngit checkout r2.8 \r\n./configure\r\n```\r\nbazel version should be 4.2.0  \r\n\r\n```\r\nbazel build --config opt  //tensorflow/tools/lib_package:libtensorflow\r\n``` \r\n\r\nwill post on installing the cc headers globally ", "> ```\r\n> bazel build --config opt  //tensorflow/tools/lib_package:libtensorflow\r\n> ```\r\n\r\nNot working for me:\r\n```\r\n\r\n\r\nbazel build --config opt  //tensorflow/tools/lib_package:libtensorflow\r\n\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=190\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/opt/homebrew/opt/python@3.9/bin/python3.9 --action_env PYTHON_LIB_PATH=/opt/homebrew/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages --python_path=/opt/homebrew/opt/python@3.9/bin/python3.9\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /Users/user/Documents/lib/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/user/Documents/lib/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:macos in file /Users/user/Documents/lib/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/c3e082762b7664bbc7ffd2c39e86464928e27c0c.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nDEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = \"1596824487 -0400\"\r\nDEBUG: Repository io_bazel_rules_docker instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:23:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace0.bzl:108:34: in workspace\r\n  /private/var/tmp/_bazel_user/7d4fd1924864d2d6fce922e4404d7479/external/bazel_toolchains/repositories/repositories.bzl:35:23: in repositories\r\nRepository rule git_repository defined at:\r\n  /private/var/tmp/_bazel_user/7d4fd1924864d2d6fce922e4404d7479/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>\r\nERROR: /private/var/tmp/_bazel_user/7d4fd1924864d2d6fce922e4404d7479/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nINFO: Repository icu instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:881:28: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:71:8: in _initialize_third_party\r\n  /Users/user/Documents/lib/tensorflow/third_party/icu/workspace.bzl:8:20: in repo\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository sobol_data instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:881:28: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:81:15: in _initialize_third_party\r\n  /Users/user/Documents/lib/tensorflow/third_party/sobol_data/workspace.bzl:6:20: in repo\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository 'icu' used the following cache hits instead of downloading the corresponding file.\r\n * Hash '3144e17a612dda145aa0e4acb3caa27a5dae4e26edced64bc351c43d5004af53' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-69-1.zip\r\n * Hash '3144e17a612dda145aa0e4acb3caa27a5dae4e26edced64bc351c43d5004af53' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-69-1.zip\r\nIf the definition of 'icu' was updated, verify that the hashes were also updated.\r\nINFO: Repository com_google_absl instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:881:28: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:59:9: in _initialize_third_party\r\n  /Users/user/Documents/lib/tensorflow/third_party/absl/workspace.bzl:14:20: in repo\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 152.726s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (144 packages loaded, 2691 targets configured)\r\n    Fetching ...4d2d6fce922e4404d7479/external/icu; Extracting /private/var/tmp/_bazel_user/7d4fd1924864d2d6fce922e4404d7479/external/icu/temp7808244371439966139/release-69-1.zip\r\n\r\n```", "I can confirm that installing bazel@4.2.1 with brew, checking out branch r2.8 while having XCode installed builds the TF successfully ", "@lmiklosko my output using bazel 4.2.1\r\n\r\n```\r\nbazel version                                                         \r\n~/miniforge3/envs/tf/bin ~/Documents/lib/tensorflow\r\n~/Documents/lib/tensorflow\r\nStarting local Bazel server and connecting to it...\r\nBuild label: 4.2.1- (@non-git)\r\nBuild target: bazel-out/darwin_arm64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Sep 8 08:37:41 2021 (1631090261)\r\nBuild timestamp: 1631090261\r\nBuild timestamp as int: 1631090261\r\n\r\n```\r\n```\r\n\r\n(tf) \u279c  tensorflow git:(r2.8) \u2717 bazel build --config opt  //tensorflow/tools/lib_package:libtensorflow\r\n\r\n~/miniforge3/envs/tf/bin ~/Documents/lib/tensorflow\r\n~/Documents/lib/tensorflow\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=139\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/user/miniforge3/envs/tf/bin/python3 --action_env PYTHON_LIB_PATH=/Users/user/miniforge3/envs/tf/lib/python3.9/site-packages --python_path=/Users/user/miniforge3/envs/tf/bin/python3\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /Users/user/Documents/lib/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/user/Documents/lib/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:macos in file /Users/user/Documents/lib/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nERROR: /Users/user/miniforge3/envs/tf/share/bazel/7d4fd1924864d2d6fce922e4404d7479/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nINFO: Repository com_google_absl instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:881:28: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:59:9: in _initialize_third_party\r\n  /Users/user/Documents/lib/tensorflow/third_party/absl/workspace.bzl:14:20: in repo\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 2.048s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (64 packages loaded, 347 targets configured)\r\n    currently loading: tensorflow/compiler/mlir/tensorflow\r\n    Fetching @llvm-project; Restarting.\r\n\r\n```\r\n\r\n\r\nAnd with bazel 4.2.0\r\n\r\n\r\n```\r\n(tf) \u279c  tensorflow git:(r2.8) \u2717 bazel build --config opt  //tensorflow/tools/lib_package:libtensorflow\r\n\r\n~/miniforge3/envs/tf/bin ~/Documents/lib/tensorflow\r\n~/Documents/lib/tensorflow\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=139\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/user/miniforge3/envs/tf/bin/python3 --action_env PYTHON_LIB_PATH=/Users/user/miniforge3/envs/tf/lib/python3.9/site-packages --python_path=/Users/user/miniforge3/envs/tf/bin/python3\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /Users/user/Documents/lib/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/user/Documents/lib/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:macos in file /Users/user/Documents/lib/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nWARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/c3e082762b7664bbc7ffd2c39e86464928e27c0c.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nLoading: 0 packages loaded\r\nLoading: 0 packages loaded\r\n    Fetching @llvm-raw; fetching 62s\r\nERROR: /Users/user/miniforge3/envs/tf/share/bazel/7d4fd1924864d2d6fce922e4404d7479/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nINFO: Repository lmdb instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:888:21: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:537:20: in _tf_repositories\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository icu instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:881:28: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:71:8: in _initialize_third_party\r\n  /Users/user/Documents/lib/tensorflow/third_party/icu/workspace.bzl:8:20: in repo\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository eigen_archive instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:881:28: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:64:11: in _initialize_third_party\r\n  /Users/user/Documents/lib/tensorflow/third_party/eigen3/workspace.bzl:14:20: in repo\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository gif instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:888:21: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:308:20: in _tf_repositories\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository double_conversion instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:888:21: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:705:20: in _tf_repositories\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository com_github_googlecloudplatform_google_cloud_cpp instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:888:21: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:259:20: in _tf_repositories\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository boringssl instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:888:21: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:554:20: in _tf_repositories\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository curl instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:888:21: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:490:20: in _tf_repositories\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nINFO: Repository zlib instantiated at:\r\n  /Users/user/Documents/lib/tensorflow/WORKSPACE:15:14: in <toplevel>\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:888:21: in workspace\r\n  /Users/user/Documents/lib/tensorflow/tensorflow/workspace2.bzl:562:20: in _tf_repositories\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:128:21: in tf_http_archive\r\nRepository rule _tf_http_archive defined at:\r\n  /Users/user/Documents/lib/tensorflow/third_party/repo.bzl:81:35: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 101.929s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (71 packages loaded, 312 targets configured)\r\n\r\n```", "@juancrescente I had exactly the same issue, until I reinstalled bazel using brew to 4.2.1 and fresh installing XCode through AppStore. Try to do `bazel clean` before reinstalling bazel though.And I re-did the configure step as well!", "@lmiklosko how did you exactly installed bazel 4.2.1 with brew? I could do it only with conda", "> @lmiklosko how did you exactly installed bazel 4.2.1 with brew? I could do it only with conda\r\n\r\nHad the same issue. So I contributed [this](https://github.com/bazelbuild/homebrew-tap#installing-bazel).", "@davidxia thanks\r\nbazel 4.2.1 is failing on my M1, following your instructions \r\n\r\n```\r\n\r\nbrew install bazel@4.2.1\r\n==> Downloading https://ghcr.io/v2/homebrew/core/python/3.10/manifests/3.10.2\r\n######################################################################## 100.0%\r\n==> Downloading https://ghcr.io/v2/homebrew/core/python/3.10/blobs/sha256:51d2cddb9e1b29141b832f1a4b0c95bb6b603cb7805b7a0bc682331798509919\r\n==> Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sha256:51d2cddb9e1b29141b832f1a4b0c95bb6b603cb7805b7a0bc68233\r\n######################################################################## 100.0%\r\n==> Downloading https://github.com/bazelbuild/bazel/releases/download/4.2.1/bazel-4.2.1-dist.zip\r\n==> Downloading from https://objects.githubusercontent.com/github-production-release-asset-2e65be/20773773/02b98e27-3834-408b-b50d-8c3b7873\r\n######################################################################## 100.0%\r\n==> Installing bazel@4.2.1 from bazelbuild/tap\r\n==> Installing dependencies for bazelbuild/tap/bazel@4.2.1: python@3.10\r\n==> Installing bazelbuild/tap/bazel@4.2.1 dependency: python@3.10\r\n==> Pouring python@3.10--3.10.2.arm64_monterey.bottle.tar.gz\r\n==> /opt/homebrew/Cellar/python@3.10/3.10.2/bin/python3 -m ensurepip\r\n==> /opt/homebrew/Cellar/python@3.10/3.10.2/bin/python3 -m pip install -v --no-deps --no-index --upgrade --isolated --target=/opt/homebrew/\r\nTarget //src:bazel_nojdk failed to build\r\nINFO: Elapsed time: 63.765s, Critical Path: 8.80s\r\nINFO: 1684 processes: 374 internal, 1310 local.\r\nFAILED: Build did NOT complete successfully\r\n\r\nERROR: Could not build Bazel\r\n\r\n```", "Adding more info:\r\n\r\nInstalled bazel 4.2.1 with conda\r\nInstalled xcode dev tools from apple store\r\nStill getting:\r\n\r\n```\r\n\r\nbazel build --config opt  //tensorflow/tools/lib_package:libtensorflow\r\n\r\n~/miniforge3/envs/tf/bin ~/Documents/lib/tensorflow\r\n~/Documents/lib/tensorflow\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=139\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/Users/user/miniforge3/envs/tf/bin/python3 --action_env PYTHON_LIB_PATH=/Users/user/miniforge3/envs/tf/lib/python3.9/site-packages --python_path=/Users/user/miniforge3/envs/tf/bin/python3\r\nINFO: Reading rc options for 'build' from /Users/user/Documents/lib/tensorflow/.bazelrc:\r\n  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils\r\nINFO: Found applicable config definition build:short_logs in file /Users/user/Documents/lib/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /Users/user/Documents/lib/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:opt in file /Users/user/Documents/lib/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:macos in file /Users/user/Documents/lib/tensorflow/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\r\nERROR: /Users/user/miniforge3/envs/tf/share/bazel/7d4fd1924864d2d6fce922e4404d7479/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'\r\nERROR: Analysis of target '//tensorflow/tools/lib_package:libtensorflow' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed\r\nINFO: Elapsed time: 0.161s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 0 targets configured)\r\n\r\n```\r\n", "@juancrescente why are you installing bazel via conda? Surely easier to have brew install bazelisk and let that take care of bazel versions? \r\n\r\nYou need full Xcode with the command tools ", "I was finally able to build it like this:\r\n\r\nInstall xcode from app store\r\nInstall xcode command line tools\r\nInstall bazelink (used bazel 5.0.0)\r\n\r\nRun this:\r\n`sudo xcode-select -s /Library/Developer/CommandLineTools\r\n`\r\nAnd make sure to add this to the build command\r\n`bazel build --config opt  //tensorflow/tools/lib_package:libtensorflow -c opt --macos_sdk_version=12.1`\r\n\r\nNote that I got this version of macos_sdk_version from here:\r\n\r\n```\r\nls /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.1.sdk  \r\n/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX12.1.sdk\r\n\r\n```\r\nAdded LD to the path:\r\n\r\n\r\n```\r\nexport DYLD_LIBRARY_PATH=\"$DYLD_LIBRARY_PATH:/Users/user/Documents/lib/tensorflow/bazel-bin/tensorflow\"\r\n```\r\n\r\nNow I still cannot make it work with golang, I'm dealing with this:\r\n\r\n\r\n```\r\nmake build                                                                                                        \r\n# github.com/tensorflow/tensorflow/tensorflow/go\r\nld: library not found for -ltensorflow\r\nclang: error: linker command failed with exit code 1 (use\r\n```\r\n\r\nThis is the content of the generated bins\r\n\r\n```\r\n\r\nls bazel-bin/tensorflow \r\nc                                            libtensorflow.2.8.0.dylib-2.params           libtensorflow_framework.2.dylib\r\ncc                                           libtensorflow.2.9.0.dylib-2.params           lite\r\ncompiler                                     libtensorflow.so.2.8.0-2.params              stream_executor\r\ncore                                         libtensorflow_framework.2.8.0.dylib          tools\r\ndistribute                                   libtensorflow_framework.2.8.0.dylib-2.params\r\nlibtensorflow.2.8.0.dylib                    libtensorflow_framework.2.9.0.dylib-2.params\r\n\r\n```\r\n\r\nBut hey, one step further", "I'm gonna add an update since it's related with the build of tensorflow. I can build but I cannot use the libraries:\r\n\r\n```\r\nbazel build --config opt  //tensorflow/tools/lib_package:libtensorflow -c opt --macos_sdk_version=12.1 --config=opt //tensorflow:libtensorflow.so\r\nsudo cp bazel-bin-tensorflow/libtensorflow* /usr/local/lib/.\r\n```\r\n\r\n\r\n```\r\nmake build\r\nmkdir -p dist\r\n# github.com/tensorflow/tensorflow/tensorflow/go\r\nld: warning: ignoring file /usr/local/lib/libtensorflow.so, building for macOS-x86_64 but attempting to link with file built for macOS-arm64\r\nUndefined symbols for architecture x86_64:\r\n  \"_TFE_ContextListDevices\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_ContextListDevices in _x003.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_ContextListDevices)\r\n  \"_TFE_ContextOptionsSetAsync\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_ContextOptionsSetAsync in _x003.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_ContextOptionsSetAsync)\r\n  \"_TFE_ContextOptionsSetConfig\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_ContextOptionsSetConfig in _x003.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_ContextOptionsSetConfig)\r\n  \"_TFE_DeleteContext\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_DeleteContext in _x003.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_DeleteContextOptions, __cgo_5439f5b57bf2_Cfunc_TFE_DeleteContext )\r\n  \"_TFE_DeleteContextOptions\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_DeleteContextOptions in _x003.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_DeleteContextOptions)\r\n  \"_TFE_DeleteTensorHandle\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_DeleteTensorHandle in _x012.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_DeleteTensorHandle)\r\n  \"_TFE_NewContext\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_NewContext in _x003.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_NewContextOptions, __cgo_5439f5b57bf2_Cfunc_TFE_NewContext )\r\n  \"_TFE_NewContextOptions\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_NewContextOptions in _x003.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_NewContextOptions)\r\n  \"_TFE_NewTensorHandle\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_NewTensorHandle in _x012.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_NewTensorHandle)\r\n  \"_TFE_TensorHandleCopyToDevice\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleCopyToDevice in _x012.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleCopyToDevice)\r\n  \"_TFE_TensorHandleDataType\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleDataType in _x012.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleDataType)\r\n  \"_TFE_TensorHandleDeviceName\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleDeviceName in _x012.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleDeviceName)\r\n  \"_TFE_TensorHandleDim\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleDim in _x012.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleDim)\r\n  \"_TFE_TensorHandleNumDims\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleNumDims in _x012.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleNumDims)\r\n  \"_TFE_TensorHandleResolve\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleResolve in _x012.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TFE_TensorHandleResolve)\r\n  \"_TF_AddControlInput\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_AddControlInput in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_AddControlInput)\r\n  \"_TF_AddGradientsWithPrefix\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_AddGradientsWithPrefix in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_AddGradientsWithPrefix)\r\n  \"_TF_AddInput\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_AddInput in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_AddInputList, __cgo_5439f5b57bf2_Cfunc_TF_AddInput )\r\n  \"_TF_AddInputList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_AddInputList in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_AddInputList)\r\n  \"_TF_AllocateTensor\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_AllocateTensor in _x011.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_AllocateTensor)\r\n  \"_TF_CloseSession\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_CloseSession in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_CloseSession)\r\n  \"_TF_DeleteBuffer\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteBuffer in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteBuffer)\r\n  \"_TF_DeleteDeviceList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteDeviceList in _x003.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteDeviceList)\r\n  \"_TF_DeleteGraph\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteGraph in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteGraph)\r\n  \"_TF_DeleteImportGraphDefOptions\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteImportGraphDefOptions in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteImportGraphDefOptions)\r\n  \"_TF_DeleteLibraryHandle\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteLibraryHandle in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteLibraryHandle)\r\n  \"_TF_DeletePRunHandle\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeletePRunHandle in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeletePRunHandle)\r\n  \"_TF_DeleteSession\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteSession in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteSessionOptions, __cgo_5439f5b57bf2_Cfunc_TF_DeleteSession )\r\n  \"_TF_DeleteSessionOptions\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteSessionOptions in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteSessionOptions)\r\n  \"_TF_DeleteStatus\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteStatus in _x010.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteStatus)\r\n  \"_TF_DeleteTensor\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeleteTensor in _x011.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeleteTensor)\r\n  \"_TF_DeviceListCount\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeviceListCount in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeviceListCount)\r\n  \"_TF_DeviceListMemoryBytes\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeviceListMemoryBytes in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeviceListMemoryBytes)\r\n  \"_TF_DeviceListName\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeviceListName in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeviceListName)\r\n  \"_TF_DeviceListType\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_DeviceListType in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_DeviceListType)\r\n  \"_TF_Dim\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_Dim in _x011.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_Dim)\r\n  \"_TF_FinishOperation\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_FinishOperation in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_FinishOperation)\r\n  \"_TF_GetCode\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_GetCode in _x010.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_GetCode)\r\n  \"_TF_GraphGetTensorNumDims\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_GraphGetTensorNumDims in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_GraphGetTensorNumDims)\r\n  \"_TF_GraphGetTensorShape\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_GraphGetTensorShape in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_GraphGetTensorShape)\r\n  \"_TF_GraphImportGraphDef\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_GraphImportGraphDef in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_GraphImportGraphDef)\r\n  \"_TF_GraphNextOperation\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_GraphNextOperation in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_GraphNextOperation)\r\n  \"_TF_GraphOperationByName\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_GraphOperationByName in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_GraphOperationByName)\r\n  \"_TF_GraphToGraphDef\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_GraphToGraphDef in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_GraphToGraphDef)\r\n  \"_TF_ImportGraphDefOptionsAddInputMapping\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_ImportGraphDefOptionsAddInputMapping in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_ImportGraphDefOptionsAddInputMapping)\r\n  \"_TF_ImportGraphDefOptionsSetDefaultDevice\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_ImportGraphDefOptionsSetDefaultDevice in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_ImportGraphDefOptionsSetDefaultDevice)\r\n  \"_TF_ImportGraphDefOptionsSetPrefix\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_ImportGraphDefOptionsSetPrefix in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_ImportGraphDefOptionsSetPrefix)\r\n  \"_TF_LoadLibrary\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_LoadLibrary in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_LoadLibrary)\r\n  \"_TF_LoadSessionFromSavedModel\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_LoadSessionFromSavedModel in _x007.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_LoadSessionFromSavedModel)\r\n  \"_TF_Message\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_Message in _x010.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_Message)\r\n  \"_TF_NewBuffer\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_NewBuffer in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_NewBuffer)\r\n  \"_TF_NewGraph\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_NewGraph in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_NewGraph)\r\n  \"_TF_NewImportGraphDefOptions\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_NewImportGraphDefOptions in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_NewImportGraphDefOptions)\r\n  \"_TF_NewOperation\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_NewOperation in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_NewOperation)\r\n  \"_TF_NewSession\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_NewSession in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_NewSession, __cgo_5439f5b57bf2_Cfunc_TF_NewSessionOptions )\r\n  \"_TF_NewSessionOptions\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_NewSessionOptions in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_NewSessionOptions)\r\n  \"_TF_NewStatus\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_NewStatus in _x010.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_NewStatus)\r\n  \"_TF_NumDims\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_NumDims in _x011.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_NumDims)\r\n  \"_TF_OperationDevice\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationDevice in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationDevice)\r\n  \"_TF_OperationGetAttrBool\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrBool in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrBoolList, __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrBool )\r\n  \"_TF_OperationGetAttrBoolList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrBoolList in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrBoolList)\r\n  \"_TF_OperationGetAttrFloat\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrFloat in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrFloatList, __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrFloat )\r\n  \"_TF_OperationGetAttrFloatList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrFloatList in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrFloatList)\r\n  \"_TF_OperationGetAttrInt\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrInt in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrIntList, __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrInt )\r\n  \"_TF_OperationGetAttrIntList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrIntList in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrIntList)\r\n  \"_TF_OperationGetAttrMetadata\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrMetadata in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrMetadata)\r\n  \"_TF_OperationGetAttrShape\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrShape in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrShapeList, __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrShape )\r\n  \"_TF_OperationGetAttrShapeList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrShapeList in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrShapeList)\r\n  \"_TF_OperationGetAttrString\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrString in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrString, __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrStringList )\r\n  \"_TF_OperationGetAttrStringList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrStringList in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrStringList)\r\n  \"_TF_OperationGetAttrTensor\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrTensor in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrTensorList, __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrTensor )\r\n  \"_TF_OperationGetAttrTensorList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrTensorList in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrTensorList)\r\n  \"_TF_OperationGetAttrType\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrType in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrType, __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrTypeList )\r\n  \"_TF_OperationGetAttrTypeList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrTypeList in _x002.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationGetAttrTypeList)\r\n  \"_TF_OperationInput\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationInput in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationInputType, __cgo_5439f5b57bf2_Cfunc_TF_OperationInput )\r\n  \"_TF_OperationInputType\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationInputType in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationInputType)\r\n  \"_TF_OperationName\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationName in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationName)\r\n  \"_TF_OperationNumInputs\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationNumInputs in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationNumInputs)\r\n  \"_TF_OperationNumOutputs\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationNumOutputs in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationNumOutputs)\r\n  \"_TF_OperationOpType\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationOpType in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationOpType)\r\n  \"_TF_OperationOutputConsumers\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationOutputConsumers in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationOutputConsumers)\r\n  \"_TF_OperationOutputListLength\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationOutputListLength in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationOutputListLength)\r\n  \"_TF_OperationOutputNumConsumers\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationOutputNumConsumers in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationOutputNumConsumers)\r\n  \"_TF_OperationOutputType\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_OperationOutputType in _x006.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_OperationOutputType)\r\n  \"_TF_SessionListDevices\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SessionListDevices in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SessionListDevices)\r\n  \"_TF_SessionPRun\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SessionPRun in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SessionPRun, __cgo_5439f5b57bf2_Cfunc_TF_SessionPRunSetup )\r\n  \"_TF_SessionPRunSetup\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SessionPRunSetup in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SessionPRunSetup)\r\n  \"_TF_SessionRun\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SessionRun in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SessionRun)\r\n  \"_TF_SetAttrBool\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrBool in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrBoolList, __cgo_5439f5b57bf2_Cfunc_TF_SetAttrBool )\r\n  \"_TF_SetAttrBoolList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrBoolList in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrBoolList)\r\n  \"_TF_SetAttrFloat\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrFloat in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrFloat, __cgo_5439f5b57bf2_Cfunc_TF_SetAttrFloatList )\r\n  \"_TF_SetAttrFloatList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrFloatList in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrFloatList)\r\n  \"_TF_SetAttrInt\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrInt in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrInt, __cgo_5439f5b57bf2_Cfunc_TF_SetAttrIntList )\r\n  \"_TF_SetAttrIntList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrIntList in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrIntList)\r\n  \"_TF_SetAttrShape\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrShape in _x004.o\r\n     (maybe you meant: _TF_SetAttrShapeList_Helper, __cgo_5439f5b57bf2_Cfunc_TF_SetAttrShapeList_Helper , __cgo_5439f5b57bf2_Cfunc_TF_SetAttrShapeList , __cgo_5439f5b57bf2_Cfunc_TF_SetAttrShape )\r\n  \"_TF_SetAttrShapeList\", referenced from:\r\n      _TF_SetAttrShapeList_Helper in _x004.o\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrShapeList in _x004.o\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrShapeList_Helper in _x004.o\r\n     (maybe you meant: _TF_SetAttrShapeList_Helper, __cgo_5439f5b57bf2_Cfunc_TF_SetAttrShapeList_Helper , __cgo_5439f5b57bf2_Cfunc_TF_SetAttrShapeList )\r\n  \"_TF_SetAttrString\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrString in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrStringList, __cgo_5439f5b57bf2_Cfunc_TF_SetAttrString )\r\n  \"_TF_SetAttrStringList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrStringList in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrStringList)\r\n  \"_TF_SetAttrTensor\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrTensor in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrTensor, __cgo_5439f5b57bf2_Cfunc_TF_SetAttrTensorList )\r\n  \"_TF_SetAttrTensorList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrTensorList in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrTensorList)\r\n  \"_TF_SetAttrType\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrType in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrTypeList, __cgo_5439f5b57bf2_Cfunc_TF_SetAttrType )\r\n  \"_TF_SetAttrTypeList\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetAttrTypeList in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetAttrTypeList)\r\n  \"_TF_SetConfig\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetConfig in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetConfig)\r\n  \"_TF_SetDevice\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetDevice in _x004.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetDevice)\r\n  \"_TF_SetTarget\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_SetTarget in _x008.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_SetTarget)\r\n  \"_TF_TensorBitcastFrom\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_TensorBitcastFrom in _x011.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_TensorBitcastFrom)\r\n  \"_TF_TensorByteSize\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_TensorByteSize in _x011.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_TensorByteSize)\r\n  \"_TF_TensorData\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_TensorData in _x011.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_TensorData)\r\n  \"_TF_TensorType\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_TensorType in _x011.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_TensorType)\r\n  \"_TF_Version\", referenced from:\r\n      __cgo_5439f5b57bf2_Cfunc_TF_Version in _x013.o\r\n     (maybe you meant: __cgo_5439f5b57bf2_Cfunc_TF_Version)\r\nld: symbol(s) not found for architecture x86_64\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake: *** [build/binaries] Error 2\r\n```\r\nIt's complaining cause the .so generated file is not for x86_64\r\n", "@juancrescente what's your environment (jdk, and bazel) and tensorflow version?\r\n\r\nWith native jdk (arm64) and bazel (arm64) and recent tensorflow master on m1 machines, \r\n```\r\nbazel build --config opt  //tensorflow/tools/lib_package:libtensorflow \r\n```\r\nshould give you arm64 binaries as expected.\r\n\r\nOn x86 machines, with native jdk (x86_64) and bazel (x86_64) and recent tensorflow master, you should get x86 binaries.\r\n\r\nIf you want to build x86_64 binaries on m1 machines,\r\n```\r\nbazel build --config opt --cpu=darwin_x86_64 --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n```\r\nshould work.\r\n\r\nif you use x86_64 jdk on m1 machines, \r\n```\r\nbazel build --config opt --cpu={darwin_arm64, darwin_x86_64} --host_cpu=darwin_arm64 //tensorflow/tools/lib_package:libtensorflow\r\n```\r\nshould work.", "@freedomtan \r\ntensorflow is branch r2.8 (master fails)\r\nbazel is :\r\n```\r\n(tf) \u279c  tensorflow git:(r2.8) \u2717 bazel version\r\n~/miniforge3/envs/tf/bin ~/Documents/lib/tensorflow\r\n~/Documents/lib/tensorflow\r\nAnother command (pid=99894) is running. Waiting for it to complete on the server (server_pid=99478)...\r\nBuild label: 4.2.1- (@non-git)\r\nBuild target: bazel-out/darwin_arm64-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Sep 8 08:37:41 2021 (1631090261)\r\nBuild timestamp: 1631090261\r\nBuild timestamp as int: 1631090261\r\n\r\n```\r\n\r\nJava:\r\n```\r\n\r\njava --version\r\nopenjdk 11.0.9.1 2020-11-04 LTS\r\nOpenJDK Runtime Environment Zulu11.43+1007-CA (build 11.0.9.1+1-LTS)\r\nOpenJDK 64-Bit Server VM Zulu11.43+1007-CA (build 11.0.9.1+1-LTS, mixed mode)\r\n```\r\n\r\nThis is the error I'm getting:\r\n\r\n```\r\ngo run\r\ndyld[554]: Library not loaded: @rpath/libtensorflow_framework.2.dylib\r\n  Referenced from: /usr/local/lib/libtensorflow.2.dylib\r\n  Reason: tried: '/usr/local/lib/../_solib_darwin_x86_64/_U_S_Stensorflow_Clibtensorflow.2.7.0.dylib___Utensorflow/libtensorflow_framework.2.dylib' (no such file), '/usr/local/lib/libtensorflow_framework.2.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/local/lib/libtensorflow_framework.2.dylib' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/usr/lib/libtensorflow_framework.2.dylib' (no such file)\r\n```\r\n", "@EricTheRed20,\r\n\r\nPlease take a look at the [Note](https://www.tensorflow.org/install/pip#macos_1) \r\n\r\n> Note: For users of Apple M1 computers, to get native performance, you'll want to follow the instructions found [here](https://developer.apple.com/metal/tensorflow-plugin/). Conda has shown to have the smoothest install. Some TensorFlow binaries (specifically, ones with custom C++ extensions like TensorFlow Decision Forests, object detection, and TFX) are not compiled against M1 targets. If you need those libraries, you will have to use TensorFlow with x86 emulation and Rosetta.", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 52157, "title": "[PluggableDevice] Add `TF_OpKernelConstruction_GetNodeDef`", "body": "Although the API has functions like `TF_OpKernelConstruction_GetAttrInt32`, it lacks an abstract to iterate through all the attributes and get their names as well as their value. Since some device backends have relatively slow kernel creation times, being able to access the `NodeDef` allows them to uniquely identify kernels for caching purposes.", "comments": ["@jpienaar Can you please review this PR ? Thanks!", "We already have taken some steps to get rid of most iteration over attributes towards enabling the backing to not be NodeDef.\r\n\r\nAdditionally exposing the NodeDef results in exposing unregistered attributes which are in the UB part of the world. But I don't know what the policy is PluggableDevice side.", "@jpienaar I can certainly change this PR provide ways to iterate through attributes instead of exposing the NodeDef itself. The ultimate goal here is to be able to cache kernels that have identical attributes to avoid creating kernels that have a long initialization cost multiple times. Can you point me towards the steps taken so far to decouple attributes from the NodeDef?", "@jpienaar  Can you please assist on above comments from @PatriceVignola. Thanks!", "@jpienaar Can you please assist on above comments from @PatriceVignola. Thanks!"]}, {"number": 52151, "title": "mkl_fused_batch_norm_op_test failing ", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0 / 2.6.0\r\n- Python version: 3.8\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): 10.3\r\n\r\n**Describe the current behavior**\r\n\r\nAccording to https://groups.google.com/a/tensorflow.org/g/build/c/RZhgZst-fgQ we build without `--config=mkl` and  when running the tests of the build the test //tensorflow/core/kernels/mkl:mkl_fused_batch_norm_op_test fails:\r\n\r\n```\r\n[==========] 5 tests from 1 test suite ran. (197 ms total)\r\n[  PASSED  ] 0 tests.\r\n[  FAILED  ] 5 tests, listed below:\r\n[  FAILED  ] Test/FusedBatchNormOpTest/0.Training, where TypeParam = float\r\n[  FAILED  ] Test/FusedBatchNormOpTest/0.TrainingRunningMean, where TypeParam = float\r\n[  FAILED  ] Test/FusedBatchNormOpTest/0.Inference, where TypeParam = float\r\n[  FAILED  ] Test/FusedBatchNormOpTest/0.InferenceIgnoreAvgFactor, where TypeParam = float\r\n[  FAILED  ] Test/FusedBatchNormOpTest/0.FusedBatchNormGradV3, where TypeParam = float\r\n```\r\n\r\nThe last one (`FusedBatchNormGradV3`) seemingly succeeds on TF 2.6 while the other 4 fail on 2.5 and 2.6.\r\nThe tests succeed when using `--config=mkl` and on other systems. It seems the AMD Epyc CPUs are affected, another Intel node works fine. So that might be related although the Intel CPUs are a bit older (broadwell) and we use `-march=native`.\r\n\r\n**Other info / logs**\r\n\r\nTest log: [test.log](https://github.com/tensorflow/tensorflow/files/7236226/test.log)\r\n\r\nCommand used: `CC_OPT_FLAGS=\"-O3 -march=native -fno-math-errno -fPIC\" bazel  test --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --subcommands --verbose_failures --jobs=64 --copt=\"-fPIC\" --action_env=PYTHONPATH --action_env=EBPYTHONPREFIXES --action_env=PYTHONNOUSERSITE=1 --distinct_host_configuration=false --test_output=errors --build_tests_only --local_test_jobs=64 --test_env=CUDA_VISIBLE_DEVICES='-1' --test_timeout=3600 -- //tensorflow/core/kernels/mkl:mkl_fused_batch_norm_op_test`", "comments": ["@TensorFlow-MKL ", "Some more test results for `//tensorflow/core/kernels/mkl:mkl_fused_batch_norm_op_test` where I checked `Test/FusedBatchNormOpTest/0.Training` and `Test/FusedBatchNormOpTest/0.TrainingRunningMean` and printed the first element of the `output` and `mkl_output` tensors for various configs. The input seems to be random, but the same (checked the first 2 elements if the input tensor)\r\n\r\n`--config=mkl` is equal to --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true`\r\n\r\nSo I did a full range scan over all possible combinations. Note that all values in a column for the results should be the same and for the `output` part are indeed (noted in the header), just the mkl output varies wildly\r\n\r\n| build with mkl | enable mkl | tensorflow mkldnn contraction kernel | build with openmp | TF ENABLE ONEDNN OPTS | Training 0.647915  | TrainingRunningMean -0.821899 |\r\n| -------------------- |-------------- |----------------------------------|-------------------|---------------------------|---------------|---------------------------------|\r\n| true | true | 0 | true | 0 | 0.647915 | -0.821899 |\r\n| true | true | 0 | true | 1 | 0.647915 | -0.821899 |\r\n| true | true | 0 | false | 0 | 0 | 0.0025533 |\r\n| true | true | 0 | false | 1 | 1.55481 | 0 |\r\n| true | true | 1 | true | 0 | 0.647915 | -0.821899 |\r\n| true | true | 1 | true | 1 | 0.647915 | -0.821899 |\r\n| true | true | 1 | false | 0 | -9.09574e+23 | 0 |\r\n| true | true | 1 | false | 1 | -0.488759 | 0 |\r\n| true | false | 0 | true | 0 |  |  |\r\n| true | false | 0 | true | 1 |  |  |\r\n| true | false | 0 | false | 0 | 1.55481 | -1.94947e+25 |\r\n| true | false | 0 | false | 1 | -1.43031e-17 | 1.53355e-35 |\r\n| true | false | 1 | true | 0 |  |  |\r\n| true | false | 1 | true | 1 |  |  |\r\n| true | false | 1 | false | 0 | 0 | -4.68903e+24 |\r\n| true | false | 1 | false | 1 | 0 | 7.27895e+24 |\r\n| false | true | 0 | true | 0 | 0 | -6.97298e+14 |\r\n| false | true | 0 | true | 1 | 1.09406 | 0 |\r\n| false | true | 0 | false | 0 | 0 | 0.872275 |\r\n| false | true | 0 | false | 1 | 0 | -1.87957e+35 |\r\n| false | true | 1 | true | 0 | -5.79435e-30 | 0 |\r\n| false | true | 1 | true | 1 | -5.24765e+22 | -5.24765e+22 |\r\n| false | true | 1 | false | 0 | 1437.87 | 4.23253e-35 |\r\n| false | true | 1 | false | 1 | -1.32684e+24 | -1.32684e+24 |\r\n| false | false | 0 | true | 0 | 0 | 1.44743e+09 |\r\n| false | false | 0 | true | 1 | 1.09406 | 2.43139e-35 |\r\n| false | false | 0 | false | 0 | 0 | -8.40301e-23 |\r\n| false | false | 0 | false | 1 | 1.09406 | -0.706766 |\r\n| false | false | 1 | true | 0 | 6578.88 | -3.87551e+15 |\r\n| false | false | 1 | true | 1 | -195440 | -195440 |\r\n| false | false | 1 | false | 0 | -6.85091e-32 | 3.93441e-35 |\r\n| false | false | 1 | false | 1 | 5.69273e+15 | 0 |", "hi, we are working on reproducing this issue. We will get back to you shortly", "Hi @Flamefire , I tried to run the test on the TF master branch and all test passed. Can you please kindly try the same test on the `master` branch? ", "@shailensobhee Any more information on the context of your test with `master`? What type of CPUs/GPUs, etc.?", "I can confirm I see the same failing tests on AMD EPYC systems (both Rome and Milan), but not on Intel systems (neither Skylake nor Cascade Lake), with both TensorFlow 2.5.0 and 2.6.0 (the tests are not failing with TensorFlow 2.4.1); see also test reports in https://github.com/easybuilders/easybuild-easyblocks/pull/2583 .\r\n\r\nIf this issue was fixed recently, would it be possible to point our a specific (set of) commits, or a pull request, in which this problem was fixed (so we can consider backporting the fix to TensorFlow 2.5.0 + 2.6.0)?", "> @shailensobhee Any more information on the context of your test with `master`? What type of CPUs/GPUs, etc.?\r\n\r\nHi @boegel I tested on CPU (my system sports a 2x Xeon 6252). "]}, {"number": 52150, "title": "Add full parameter to SSIM", "body": "**System information**\r\n- TensorFlow version (you are using): 2.6.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, `tf.image.ssim` and also `tf.image.ssim_multiscale` return the mean ssim value of the images (across height and width dimensions), but returning the full ssim image would be highly useful for multiple applications (reconstruction autoencoders, etc).\r\n\r\n**Will this change the current api? How?**\r\n\r\nYes, both functions (`tf.image.ssim` and `tf.image.ssim_multiscale`) will include a new parameter `full` and will return a tensor containing the full ssim image if `full` is True. In fact, this ressembles the api of [scikit-image structural_similarity](https://scikit-image.org/docs/dev/api/skimage.metrics.html?highlight=structural_similarity#skimage.metrics.structural_similarity).\r\n\r\n**Any Other info.**\r\n\r\nThe implementation for `tf.image.ssim` is straightforward.  It consists of removing the `reduce_mean` operation if `full` is True, or conversely `reduce_mean` for no axes (`axes=[]`).\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/bdc6a138403e8257841e8dff6d6b9322bb65053a/tensorflow/python/ops/image_ops_impl.py#L4227\r\n\r\nI am not sure about the implementation for `tf.image.ssim_multiscale`, but it should be similar (or even the same).", "comments": ["Hi @Saduf2019! \r\nCould you please look at this feature request!"]}, {"number": 52148, "title": "tf.matmul returns wrong results if called within tf.vectorized_map", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- macOS Big Sure, Version 11.6\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n`tf.matmul(a,b, transpose_a=False, transpose_b=True)` returns wrong result for `a,b` of complex\r\ndtypes if wrapped with `tf.vectorized_map`. In fact, `tf.matmul(a,b, transpose_a=False, transpose_b=True)` returns the result expected from the call `tf.matmul(a,b, adjoint_a=False, adjoint_b=True)`, and vice versa.\r\n**Describe the expected behavior**\r\n`tf.matmul(a,b, transpose_a=False, transpose_b=True)` should return `a@b.T` if wrapped in `tf.vectorized_map`\r\n\r\n**[Contributing](https://www.tensorflow.org/community/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes/no): no\r\n\r\n**Standalone code to reproduce the issue**\r\n```python \r\nimport tensorflow as tf\r\nimport numpy as np\r\ndef matmul_adjoint(args):\r\n    t1, t2 = args\r\n    return tf.matmul(t1, t2, adjoint_a=False, adjoint_b=True)\r\n\r\n\r\ndef matmul_transpose(args):\r\n    t1, t2 = args\r\n    return tf.matmul(t1, t2, transpose_a=False, transpose_b=True)\r\n\r\ndef test_vectorized_matmul(dtype):\r\n    rdtype= np.ones(0, dtype).real.dtype\r\n    if dtype in (np.complex64, np.complex128):\r\n        a = np.random.rand(2,4,4).astype(rdtype) + 1j * np.random.rand(2,4,4).astype(rdtype)\r\n        b = np.random.rand(2,4,4).astype(rdtype) + 1j * np.random.rand(2,4,4).astype(rdtype)\r\n    else:\r\n        a = np.random.rand(2,4,4).astype(rdtype) \r\n        b = np.random.rand(2,4,4).astype(rdtype)\r\n    A = tf.convert_to_tensor(a, dtype=dtype)\r\n    B = tf.convert_to_tensor(b, dtype=dtype)\r\n    result_adjoint = tf.vectorized_map(matmul_adjoint,(A,B))\r\n    result_transpose = tf.vectorized_map(matmul_transpose,(A,B))\r\n    expected_transpose = []\r\n    for n in range(2):\r\n        expected_transpose.append(a[n] @ b[n].T)\r\n    expected_transpose = np.stack(expected_transpose)\r\n    expected_adjoint = []\r\n    for n in range(2):\r\n        expected_adjoint.append(a[n] @ (b[n].T.conj()))\r\n    expected_adjoint = np.stack(expected_adjoint)\r\n    eps = np.finfo(rdtype).eps\r\n    try:\r\n        np.testing.assert_allclose(result_adjoint, expected_adjoint, \r\n                                   atol=10*eps, rtol=10*eps)\r\n    except AssertionError as err:\r\n        print(''.join(['#']*60))\r\n        print(f\"matmul adjoint test failed failed for dtype {dtype} with error {err}\")\r\n    try:\r\n        np.testing.assert_allclose(result_transpose, expected_transpose, atol=10*eps, rtol=10*eps)\r\n    except AssertionError as err:\r\n        print(''.join(['#']*60))\r\n        print(f\"matmul_transpose failed for dtype {dtype} with error {err}\")\r\n      \r\n    # the following passes, so it seems that the transpose and adjoint arguments\r\n    # to tf.matmul need to be swapped\r\n    np.testing.assert_allclose(result_adjoint, expected_transpose, \r\n                               atol=10*eps, rtol=10*eps)\r\n    \r\n    np.testing.assert_allclose(result_transpose, expected_adjoint, \r\n                               atol=10*eps, rtol=10*eps)\r\n    \r\ntest_vectorized_matmul(np.float32)\r\ntest_vectorized_matmul(np.complex64)\r\n\r\n```\r\n", "comments": ["@sanatmpa1 ,\r\nI was able to reproduce the issue in tf v2.5, v2.6 and nightly.Please find the gist of it [here](https://colab.research.google.com/gist/tilakrayal/dc2cc8f88a4ed682904f6a05123202ec/52148.ipynb)."]}, {"number": 52140, "title": "More than 20% of video memory missing both Linux and Windows [RTX 3080]", "body": "### **System information**\r\n**- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\nThis is the same for my custom code and for simple scripts while setting GPU in TensorFlow. \r\n**- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\nThis was tested on Windows 10 and Linux Ubuntu 20.04\r\n**- TensorFlow installed from (source or binary):**\r\nTensorFlow installed from binary.\r\n**- TensorFlow version (use command below):**\r\nThis was tested on 2.5.0, 2.5.1 and 2.6.0\r\n**- Python version:**\r\nPython 3.8.10\r\n**- CUDA/cuDNN version:**\r\nTested on \r\nCuda Toolkit: 11.2, 11.4\r\nCuDNN: 8.1.0, 8.1.1, 8.2.4\r\n**- GPU model and memory:**\r\nMSI RTX 3080 10GB memory\r\n\r\n### **Describe the current behavior**\r\nI have a 10GB 3080RTX GPU, NVidia-smi reports 10014MiB memory, Tensorflow reports:\r\n> Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7591 MB memory\r\n\r\nAfter initial research I was convinced that this is related to Windows 10 OS limitations, so I installed Ubuntu 20.04 in dual boot. It didn't change anything, I tried various versions of Tensorflow, Cuda, Cudnn.\r\nI tried using:\r\n\r\n```\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\nfor gpu_instance in physical_devices:\r\n    tf.config.experimental.set_memory_growth(gpu_instance, True)\r\n\r\n```\r\n\r\nIt didn't fix the problem. Also, I tried :\r\n\r\n```\r\nfrom tensorflow.compat.v1 import ConfigProto\r\nfrom tensorflow.compat.v1 import InteractiveSession\r\n\r\nconfig = ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 1.0\r\nsession = InteractiveSession(config=config)\r\n\r\n```\r\nAnd indeed, TensorFlow started to report proper full 10GB of memory in 'Created device' message, so tf should see the memory properly. With this method I was able to push memory to almost 8GB and it even allowed me to run slightly higher batch size. But, if I specify fraction of more than 0.8 (it may slightly vary from run-to-run) than i have:\r\n> 2021-09-26 12:48:26.691479: F tensorflow/core/util/cuda_solvers.cc:115] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.\r\n\r\nOne important thing to note, while TensorFlow is reporting a device with ~7.5GB, in nvidia-smi it is reporting more than 9GB by /usr/bin/python3! I am not running any other Python script in parallel. \r\n\r\n![Screenshot from 2021-09-26 12-53-00](https://user-images.githubusercontent.com/13711526/134804804-3dced212-188e-4e49-881e-d19e7e6c8d90.png)\r\n\r\nSo, the memory usage in reality is reaching its limits while I am able to use only 7.5GB, which is even less than known 81% limitation for Windows 10 users! Why am I being allocated almost extra 2GB on top which I can't even use for training?\r\n\r\nI was trying to fix it for a long time and really don't have any idea what to do now. Other people's problems with missing tf memory that I found on Internet were related to Windows OS, mine is not. Am I missing something? I would really appreciate any idea on what is going on.\r\nThank you in advance.\r\n\r\n", "comments": ["Have you tried:\r\n\r\n```\r\ngpus = tensorflow.config.experimental.list_physical_devices('GPU')\r\nfor gpu in gpus:\r\n    tensorflow.config.experimental.set_virtual_device_configuration(gpu, [tensorflow.config.experimental.VirtualDeviceConfiguration(memory_limit=10000)])\r\n```", "> Have you tried:\r\n> \r\n> ```\r\n> gpus = tensorflow.config.experimental.list_physical_devices('GPU')\r\n> for gpu in gpus:\r\n>     tensorflow.config.experimental.set_virtual_device_configuration(gpu, [tensorflow.config.experimental.VirtualDeviceConfiguration(memory_limit=10000)])\r\n> ```\r\n\r\nI tried this and it is having similar effect to\r\n\r\n`config.gpu_options.per_process_gpu_memory_fraction = 1.0\r\n`\r\n\r\nBasically, if I specify something slightly higher like 8500 limit I get:\r\n\r\n> 2021-09-26 14:03:44.463333: F tensorflow/core/util/cuda_solvers.cc:115] Check failed: cusolverDnCreate(&cusolver_dn_handle) == CUSOLVER_STATUS_SUCCESS Failed to create cuSolverDN instance.\r\n\r\nIf I try to allocate almost the totality of VRAM by setting 10000 for _VirtualDeviceConfiguration_ or 1.0 for _per_process_gpu_memory_fraction_ than I get something like:\r\n\r\n> 2021-09-26 14:07:49.020930: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 9.78G (10501423104 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n> ...\r\n> 2021-09-26 14:07:52.205567: W tensorflow/stream_executor/stream.cc:1455] attempting to perform BLAS operation using StreamExecutor without BLAS support\r\n> ...\r\n> tensorflow.python.framework.errors_impl.InternalError: Blas xGEMM launch failed : a.shape=[1,1,512], b.shape=[1,512,512], m=1, n=512, k=512 [Op:MatMul]", "**Small Update:**\r\nWhen I install tf 2.4.x I get \r\n\r\n> Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8703 MB memory\r\n\r\nThis is looking much more acceptable, but I can't train anything because of:\r\n\r\n> 2021-09-27 02:05:29.061057: E tensorflow/stream_executor/cuda/cuda_dnn.cc:336] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\n> ...\r\n> tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\r\n\r\nMay be the compatibility issue with my cuda version, but I can't downgrade it much because of rtx3000 card support. I have a feeling that something in cuda/tensorflow allocates almost 2GB for some compatibility thing with rtx3000, but does it really need that much? I would appreciate if owners of those cards would reach out with their status. \r\n\r\nAlso, this topic might be related to my problem, but no solution proposed:\r\nhttps://www.reddit.com/r/tensorflow/comments/k3vcwj/tensorflow_with_rtx_3000_seems_to_use_more_vram/\r\n\r\n", "**Update 2:**\r\n\r\nI tested some networks on PyTorch 1.9.1 + CUDA 11.1 and facing similar issue. For example, on Windows right before running the script I have  520MiB / 10240MiB allocated:\r\n\r\n![image](https://user-images.githubusercontent.com/13711526/135730824-6d366c9b-fb46-4711-b04e-f3b5fdb05b4f.png)\r\n\r\nAfter I run the training I get:\r\n\r\n> RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.00 GiB total capacity; 7.39 GiB already allocated; 0 bytes free; 7.44 GiB reserved in total by PyTorch)\r\n\r\nnvidia-smi shows that almos all available memory is allocated:\r\n\r\n![image](https://user-images.githubusercontent.com/13711526/135730828-e51d63a0-0514-4aa1-8c25-af7e9174481e.png)\r\n\r\nPyTorch info:\r\n![image](https://user-images.githubusercontent.com/13711526/135730831-7dc52bff-02a9-49b9-b495-d84a27eb05a1.png)\r\nSo again very similar issue. Pytorch allocated  7.39 GiB, nvidia-smi shows memory usage increased by ~9.5GiB after I launched the script, extra 2GB is taken for unknown reason, as with TensorFlow. I wrote to Nvidia but they didn't give me definitive answer by now, maybe people who work on TensorFlow+CUDA side of things know what to say about this?"]}, {"number": 52125, "title": "'pip install tensorflow' does not work on FreeBSD", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): FreeBSD 12.2\r\n- TensorFlow installed from (source or binary): trying to install binary using `pip`\r\n- TensorFlow version: any\r\n- Python version: 3.8\r\n\r\n**Describe the problem**\r\n\r\n`pip install tensorflow` does not work on FreeBSD. Apparently there is no precompiled package for FreeBSD. Why?\r\n\r\n```\r\nFreeBSD% ./spleeter/bin/pip install tensorflow     \r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\n\r\n**Any other info / logs**\r\nWas pointed to this issue tracker from https://github.com/pypa/packaging-problems/issues/547.", "comments": ["Same error ", "#52143 ", "@probonopd ,\r\nIn order to expedite the trouble-shooting process, could you please provide the following information\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nTensorFlow installed from (source or binary):\r\nTensorFlow version:\r\nPython version:\r\nInstalled using virtualenv? pip? conda?:\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source):\r\nCUDA/cuDNN version:\r\nGPU model and memory:\r\n \r\nand the exact sequence of commands / steps that you executed before running into the problem\r\n", "> In order to expedite the trouble-shooting process, could you please provide the following information\r\n\r\nI think I already provided the relevant information above but here we go again:\r\n\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): __`FreeBSD 12.2`__\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: __Not a mobile device__\r\nTensorFlow installed from (source or binary): __trying to install binary using `pip`__\r\nTensorFlow version: __any__\r\nPython version: __3.8.10__\r\nInstalled using virtualenv? pip? conda?: __trying to install binary using `pip`__\r\nBazel version (if compiling from source): __n/a__\r\nGCC/Compiler version (if compiling from source): __n/a__\r\nCUDA/cuDNN version: __n/a__\r\nGPU model and memory: __n/a__\r\n\r\n> and the exact sequence of commands / steps that you executed before running into the problem\r\n\r\n```\r\npip install tensorflow\r\n```\r\n\r\nThe result is:\r\n\r\n```\r\nERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\r\nERROR: No matching distribution found for tensorflow\r\n```\r\n\r\nThis is probably because __FreeBSD builds are missing__ on https://www.tensorflow.org/install/pip#other? There is only Windows, macOS, and Linux but not FreeBSD, and this is what this issue is about.", "@probonopd \r\nCan you please refer to similar error [issues](https://github.com/tensorflow/tensorflow/issues/42482#issuecomment-680855973) and let us know: #44615, #47205, [link](https://stackoverflow.com/questions/38896424/tensorflow-not-found-using-pip)\r\n\r\nPlease, uninstall everything and then install in new environment.\r\n\r\nStep 1: pip install -U pip\r\nStep 2: pip install -U setuptools\r\nStep 3: pip install tensorflow\r\nTensorFlow 2 packages require a pip version >19.0. Could you please upgrade pip using the below command and let us know if it works.", "I have pip 21", "@omxpro \r\nCan you please refer to : #37345, #50766, [link](https://githubmemory.com/repo/pypa/packaging-problems/issues/547) ", "Those seem to refer to source builds whereas I am interested in a binary installation like on the Mac, on Windows, and on Linux.\r\n\r\nThe fact is that there is not FreeBSD builds:\r\n\r\nhttps://storage.googleapis.com/tensorflow lists binaries for Mac (`darwin`), Windows (`windows`), and Linux (`linux`) but *not* for FreeBSD.", "The team responsible for the official Google TF builds has very few people and cannot manage the large space of `{operating system} x {python version} x {dependency versions} x {CPU/GPU} x {C/Python/C++/Java/etc.} x {TF/TFLite/etc}`. The team really has to limit amount of support offered.\r\n\r\nCustom builds are available via SIG Build (https://github.com/tensorflow/build), you can also join the monthly meeting to discuss support for the custom builds there (bit.ly/tf-sig-build-notes)", "I understand that the team responsible for the official Google TF builds cannot do builds for everything.\r\n\r\nSo would it be possible to provide source distributions on PyPI and let https://www.pypa.io/ do the building?\r\n\r\n---\r\n\r\nhttps://github.com/pypa/packaging-problems/issues/547#issuecomment-926882371\r\n\r\n> I think the answer to your problem is that TensorFlow doesn't distribute any source distributions on PyPI ", "Wouldn't that result in timeouts? A current builds takes hours.", "We could attempt to provide one though", "There might be an issue here in that TF builds with Bazel (no cmake support) and pypa might not be able to run these.", "Can you install Bazel through PyPI? If so, then just add it to the pyproject.toml, and it will \"just work\" (in theory) for any recent pip. Providing a PyPI package for bazel shouldn't be that hard, see how we provide [cmake](https://github.com/scikit-build/cmake-python-distributions) & [ninja](https://github.com/scikit-build/ninja-python-distributions), or [clang-format](https://github.com/ssciwr/clang-format-wheel) is another good example.\r\n\r\nAlso, PyPA doesn't build anything, but we provide cibuildwheel, which makes it easy to use CI (or other) services to build wheels. Adding an SDist would be nice, and would help package this for FreeBSD, I would think. (Not familiar with FreeBSD, though). I'm not sure there are redistributable binaries for FreeBSD, though, that can be uploaded to PyPA - it's not manylinux/musllinux/etc. I think everything would need to be able to be built from SDist (including bazel unless it was pre-installed).\r\n\r\nTL;DR: adding bazel as a PyPI package would be great, and having a nice SDist + build environment would be useful. But for FreeBSD, an SDist + whatever is done to package things is likely required; probably needing manual install steps like for Bazel.", "But that is no longer TF issue, Bazel is a separate team", "Yes, I'm making two suggestions, and one is not really targeting the TensorFlow team (though maybe a use case coming from the TF team would help motivate the Bazel team to try it? :) ). I'm not aware of pre-built binaries[^1] for FreeBSD, so I don't think that would actually help that much in this case. The other is that an SDist of TensorFlow would likely still help (even if it requires prerequisites, not ideal, but better than no SDist and not that uncommon). I expect that SDist would then need to be used by FreeBSD packagers (?) to make whatever they need for a package.\r\n\r\n[^1]: for wheels, that is. So the OP's exact request is not currently possible. But having an SDist would likely solve it, `pytoport` and tools like that would then work.", "CC @perfinion, based on the SIG Build discussion ", "@probonopd\r\nTensorFlow GPU edition exist only for Windows and Linux because only this OS have CUDA and CUDNN support .\r\nFreeBSD doesn't have CUDA and CUDNN drivers.\r\nIn the case of FreeBSD only one possible way - try compile CPU edition TF from source, but is very pain and time consumption  - but somebody [did  it using Clang](https://forums.freebsd.org/threads/howto-install-tensorflow-on-freebsd.59479/)\r\nOr change FreeBSD to  Linux Distro like Debian, Ubuntu or Centos. Debiand and Ubuntu support RootZFS. :)", "Formally can turn on Linuxator in the FreeBSD, compile inside Linuxator python with Linux support from source with gcc  but I don't sure  in the full binary comparability with native Linux. After update FreeBSD Lunuxator port all  repair again... \r\nnext line in the [ .bazelrc](https://github.com/tensorflow/tensorflow/blob/master/.bazelrc) show all supported OSes:\r\n```\r\n# Suppress C++ compiler warnings, otherwise build logs become 10s of MBs.\r\n288  build:android --copt=-w\r\n289 build:ios --copt=-w\r\n290 build:linux --copt=-w\r\n291 build:linux --host_copt=-w\r\n292 build:macos --copt=-w\r\n293 build:windows --copt=/W0\r\n```\r\nThe simple and easy way - migrate to Linux or try compilation with 146% probability to fail wit clang "]}, {"number": 52121, "title": "Saving customized RNN model, got error of \"Tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError\"", "body": "**System information**\r\n- I have written the customized simple RNN model based on TF layers:\r\n- OS Platform and Distribution: Windows 10 \r\n- TensorFlow installed from (source or binary): TF2.6 \r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: Python 3.7.11\r\n- CUDA/cuDNN version: 11.4\r\n- GPU model and memory: NVIDIA TITAN RTX 24 G\r\n\r\n\r\n**Describe the current behavior**\r\nv2.6.0-rc2-32-g919f693420e 2.6.0\r\n\r\n**Describe the expected behavior**\r\nI have implemented a simple RNN model by using customized layers. The model is running ok with the eager execution and graph execution in the stage of the model building. However, while I tried to save the generated dummy model, I got the following error: \r\n\r\n \"tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\" \r\n\r\n\r\n**Standalone code to reproduce the issue:** \r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Layer\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\n\r\n\r\n##### ------- (1)  check if the eager execution is used -----\r\n\r\nnon_eager_execution = False\r\nif non_eager_execution:\r\n    tf.compat.v1.disable_eager_execution()\r\nelse:\r\n    tf.config.run_functions_eagerly(True)\r\n\r\nprint(\"Eager execution is: \" + str(tf.executing_eagerly()))\r\n\r\n\r\n##### ------- (2)   define the customized RNN layer -----\r\n\r\nclass RNN_simple_layer(Layer):\r\n\r\n    def __init__(self, neurons_numbers, return_sequences=False,**kwargs):\r\n        super(RNN_simple_layer, self).__init__()\r\n        self.neurons_numbers  = neurons_numbers\r\n        self.return_sequences = return_sequences\r\n\r\n    def build(self, input_shape):\r\n        self.kernel_input = self.add_weight('kernel_input', shape=[int(input_shape[-1]), self.neurons_numbers], initializer=\"random_normal\",trainable=True)\r\n        self.kernel_state = self.add_weight('kernel_state', shape=[self.neurons_numbers,self.neurons_numbers], initializer=\"random_normal\",trainable=True)\r\n        self.bias = self.add_weight('bias', shape=[self.neurons_numbers,],initializer=\"zeros\",trainable=True)\r\n\r\n    def call(self, inputs):\r\n\r\n        sequence_length = inputs.shape[-2]\r\n\r\n        # outputs = []\r\n        outputs = tf.TensorArray(tf.float32, size=sequence_length)\r\n        states  = tf.TensorArray(tf.float32, size=sequence_length)\r\n        inital_states  = tf.zeros(shape=[tf.shape(inputs)[0],self.neurons_numbers])\r\n\r\n        states = inital_states\r\n        for i_step in tf.range(sequence_length):\r\n            output_i_step = tf.tanh(tf.matmul(inputs[:,i_step,:],self.kernel_input) + tf.matmul(states, self.kernel_state) + self.bias)\r\n            states        = output_i_step\r\n            outputs = outputs.write(i_step,output_i_step)\r\n\r\n        # output the RNN resuts\r\n        if self.return_sequences == True:\r\n            output = tf.transpose(outputs.stack(), [1,0,2])\r\n        else:\r\n            output = outputs.stack()[-1]\r\n        return output\r\n\r\n    def get_config(self):\r\n        config = super().get_config()\r\n        config.update({\r\n            'neurons_numbers': self.neurons_numbers,\r\n            'return_sequences': self.return_sequences\r\n        })\r\n        return config\r\n\r\n\r\n\r\n##### ------- (3)   Test the customized simple RNN layer-----\r\n\r\nRNN_layer    = RNN_simple_layer(4,return_sequences=True)\r\ninput_array  = tf.ones((32,10,8))\r\noutput_array = RNN_layer(input_array)\r\n\r\nprint('RNN input tensor shape:' + str(input_array.shape))\r\nprint('RNN input tensor has batch size:' + str(input_array.shape[0]))\r\nprint('RNN input tensor has time samples:' + str(input_array.shape[-2]))\r\nprint('RNN input tensor has features of each sample:' + str(input_array.shape[-1]))\r\nprint('RNN output tensor shape:' + str(output_array.shape))\r\n\r\n\r\n##### ------- (4)   build a dummy Tensorflow model-----\r\n\r\ninputs  = tf.keras.Input(shape=(10,20))\r\nx       = tf.keras.layers.Conv1D(10,3)(inputs)\r\nx       = RNN_simple_layer(4,return_sequences=True)(x)\r\noutputs = tf.keras.layers.Dense(30, activation=\"relu\")(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"test_model\")\r\nmodel.summary()\r\n\r\n\r\n\r\n##### ------- (5)   save the customized dummy model-----\r\nmodel.save('Model_simple_RNN')\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nEager execution is: True\r\nRNN input tensor shape:(32, 10, 8)\r\nRNN input tensor has batch size:32\r\nRNN input tensor has time samples:10\r\nRNN input tensor has features of each sample:8\r\nRNN output tensor shape:(32, 10, 4)\r\nModel: \"test_model\"\r\n\r\n#####  _________________________________________________________________\r\n#####  Layer (type)                 Output Shape              Param #   \r\n#####  =================================================================\r\n#####   input_7 (InputLayer)         [(None, 10, 20)]          0         \r\n#####  _________________________________________________________________\r\n#####  conv1d_6 (Conv1D)            (None, 8, 10)             610       \r\n#####  _________________________________________________________________\r\n#####  rnn_simple_layer_13 (RNN_sim (None, 8, 4)              60        \r\n#####  _________________________________________________________________\r\n#####  dense_6 (Dense)              (None, 8, 30)             150       \r\n#####  =================================================================\r\nTotal params: 820\r\nTrainable params: 820\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n\r\n\r\nmodel.save('Model_simple_RNN')\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-9-6d9b393b5457>\", line 1, in <module>\r\n    model.save('Model_simple_RNN')\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\engine\\training.py\", line 2146, in save\r\n    signatures, options, save_traces)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\save.py\", line 150, in save_model\r\n    signatures, options, save_traces)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save.py\", line 91, in save\r\n    model, filepath, signatures, options)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1228, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1399, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 1336, in _build_meta_graph_impl\r\n    checkpoint_graph_view)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_serialization.py\", line 99, in find_function_to_export\r\n    functions = saveable_view.list_functions(saveable_view.root)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\saved_model\\save.py\", line 164, in list_functions\r\n    self._serialization_cache)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\engine\\training.py\", line 2813, in _list_functions_for_serialization\r\n    Model, self)._list_functions_for_serialization(serialization_cache)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 3086, in _list_functions_for_serialization\r\n    .list_functions_for_serialization(serialization_cache))\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\base_serialization.py\", line 93, in list_functions_for_serialization\r\n    fns = self.functions_to_serialize(serialization_cache)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 74, in functions_to_serialize\r\n    serialization_cache).functions_to_serialize)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 90, in _get_serialized_attributes\r\n    serialization_cache)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\model_serialization.py\", line 57, in _get_serialized_attributes_internal\r\n    serialization_cache))\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py\", line 99, in _get_serialized_attributes_internal\r\n    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 197, in wrap_layer_functions\r\n    fn.get_concrete_function()\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\contextlib.py\", line 119, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 359, in tracing_scope\r\n    fn.get_concrete_function(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 1233, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 1213, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 760, in _initialize\r\n    *args, **kwds))\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3066, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3463, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3308, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 1007, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 668, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 572, in wrapper\r\n    ret = method(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\utils.py\", line 166, in wrap_with_training_arg\r\n    lambda: replace_training_and_call(False))\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 106, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\", line 58, in smart_cond\r\n    return false_fn()\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\utils.py\", line 166, in <lambda>\r\n    lambda: replace_training_and_call(False))\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\utils.py\", line 162, in replace_training_and_call\r\n    return wrapped_call(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 651, in call\r\n    return call_and_return_conditional_losses(inputs, *args, **kwargs)[0]\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 609, in __call__\r\n    return self.wrapped_call(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 862, in __call__\r\n    return self._python_function(*args, **kwds)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 572, in wrapper\r\n    ret = method(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\utils.py\", line 166, in wrap_with_training_arg\r\n    lambda: replace_training_and_call(False))\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 106, in smart_cond\r\n    pred, true_fn=true_fn, false_fn=false_fn, name=name)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\framework\\smart_cond.py\", line 58, in smart_cond\r\n    return false_fn()\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\utils.py\", line 166, in <lambda>\r\n    lambda: replace_training_and_call(False))\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\utils.py\", line 162, in replace_training_and_call\r\n    return wrapped_call(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 633, in call_and_return_conditional_losses\r\n    call_output = layer_call(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\engine\\functional.py\", line 415, in call\r\n    inputs, training=training, mask=mask)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\engine\\functional.py\", line 550, in _run_internal_graph\r\n    outputs = node.layer(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1037, in __call__\r\n    outputs = call_fn(inputs, *args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\utils.py\", line 68, in return_outputs_and_add_losses\r\n    outputs, losses = fn(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 609, in __call__\r\n    return self.wrapped_call(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 862, in __call__\r\n    return self._python_function(*args, **kwds)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 572, in wrapper\r\n    ret = method(*args, **kwargs)\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\keras\\saving\\saved_model\\save_impl.py\", line 633, in call_and_return_conditional_losses\r\n    call_output = layer_call(*args, **kwargs)\r\n  File \"<ipython-input-8-4f4d66e68e0a>\", line 45, in call\r\n    for i_step in tf.range(sequence_length):\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 520, in __iter__\r\n    self._disallow_iteration()\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 513, in _disallow_iteration\r\n    self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\r\n  File \"C:\\Users\\zhaoh\\anaconda3\\envs\\tensorflowV26\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 491, in _disallow_when_autograph_enabled\r\n    \" indicate you are trying to use an unsupported feature.\".format(task))\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\r\n\r\n\r\n", "comments": ["Hi @haozh7109 !Could you look at this[ thread ](https://stackoverflow.com/a/58799021/11530462) with similar Error stacktrace for answer ?", "Hi @mohantym, thanks for your response. I have added the get_config() function in the customized layer but still got the same error. I have updated the modified code accordingly. ", "Hi @sanatmpa1 ,\r\nCould you please look at this issue!Issue is replicating is [2.5](https://colab.research.google.com/gist/mohantym/c255b8fa54c273b44e57a52be619825b/github_52121.ipynb#scrollTo=8bPRtXAlfRh6),[2.6](https://colab.research.google.com/gist/mohantym/c255b8fa54c273b44e57a52be619825b/github_52121.ipynb#scrollTo=8bPRtXAlfRh6) and [2.7.0dev(nightly)](https://colab.research.google.com/gist/mohantym/761cb638942802433e3e488ca3b16640/github_52121.ipynb#scrollTo=8bPRtXAlfRh6)", "I am able to reproduce `OperatorNotAllowedInGraphError` error and here's the [gist](https://colab.research.google.com/gist/sanatmpa1/40b4c7ddae7a8559bfc7a09e60cab6c7/52121.ipynb). Found a similar issue #44430", "I managed to save the model by decorating the call( ) function with @tf.function in the customized class and replace the 'tf.range()' to 'range()' for the loop (This is quite confusing in the graph execution). however, I still got errors in the TensorFlow-lite conversion which might relate to the customized model setting. please find the enclosed [Revised_gist](https://colab.research.google.com/gist/haozh7109/90c20c6401c6b38e501a533a8fb2735e/github_52121_2-6_revision.ipynb). "]}, {"number": 52108, "title": "Error using xla_sharding split on XLA_GPU", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Unknown\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.6.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.2/8.2\r\n- GPU model and memory: 8x NVIDIA A100 (40 GB)\r\n\r\n**Describe the current behavior**\r\nI get an error when trying to use xla_sharding on an XLA_GPU device. The error is:\r\n\r\nInvalidArgumentError: Trying to access resource _AnonymousVar584 (defined @ <ipython-input-27-90261c699d9c>:18) located in device /job:localhost/replica:0/task:0/device:XLA_GPU:0 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_test_xla_sharding_split_3074]\r\n\r\n**Describe the expected behavior**\r\nA tensor split into 8 parts across the 0 dimension.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport os\r\nos.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/usr/local/cuda\"\r\nos.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices\"\r\n\r\nfrom tensorflow.compiler.xla.experimental.xla_sharding import xla_sharding\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.config.optimizer.set_jit(True)\r\nprint(\"Tensorflow version \" + tf.__version__)\r\n\r\ndim = 0\r\nnum_parts = 8\r\n\r\nmirrored_strategy = tf.distribute.MirroredStrategy(\r\n    devices=[\"device:XLA_GPU:0\", \"device:XLA_GPU:1\", \"device:XLA_GPU:2\", \"device:XLA_GPU:3\", \r\n             \"device:XLA_GPU:4\", \"device:XLA_GPU:5\", \"device:XLA_GPU:6\", \"device:XLA_GPU:7\"\r\n])\r\n\r\nwith mirrored_strategy.scope():\r\n    var0 = tf.Variable([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0])\r\n    var0_split = tf.Variable([10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0])\r\n\r\n@tf.function(jit_compile=True)\r\ndef test_xla_sharding_split(x, y):\r\n    with mirrored_strategy.scope():\r\n        y.assign(xla_sharding.split(x, dim, num_parts, use_sharding_op=True))\r\n\r\ntest_xla_sharding_split(var0, var0_split)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\nattached tf_env.txt file\r\n", "comments": []}]