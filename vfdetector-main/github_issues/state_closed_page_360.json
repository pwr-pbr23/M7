[{"number": 43262, "title": "Unexpected behaviour for model.evaluate inside keras Callback", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6.9\r\n\r\n**Describe the current behavior**\r\nInside a `keras.callbacks.Callback`, `self.model.evaluate` returns results for the `validation_data` regardless of what is passed in. The issue seems to not be present if `validation_data` is None.\r\n\r\n**Describe the expected behavior**\r\n`self.model.evaluate` should evaluate what is passed in.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab notebook\r\n](https://colab.research.google.com/drive/1RHrGba3sQ3iw9Ju0_m3WRFC5dog4WG6k?usp=sharing) which illustrates the issue.\r\n", "comments": ["I think that problem is that with `def on_train_end` you are still in training mode.", "@adriang133 \r\nPlease update as per above comment", "I'm using `on_epoch_end` in my example. I'm not too familiar with the innards of Keras, but why would `on_epoch_end` have the model in training mode still ?\r\n\r\nIf this is intended, is there a way I can evaluate the model on a separate validation dataset at the end of each epoch ?", "You can see what happens when you are calling `fit`: https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/engine/training.py#L1106", "I see. The model state during training changes if you specify `validation_data` vs if you don't which is unintuitive if you ask me, but if this is intended then feel free to close this issue.\r\n\r\nTLDR for anyone else running into this, a solution is to move the evaluation of both validation datasets to the callback and call `fit(..., validation_data=None)`.\r\n\r\n\r\n", "I don't know what you want to do but generally when you have something in `validation_data` you have also `on_train_begin` `on_train_end` callback available that you could customize.", "I want to evaluate the model on 2 (or more) validation sets at the end of each epoch. `on_train_end` is only called at the end of the training, not per epoch, so it wouldn't do. See #38803 for example. A google search yields a couple of similar solutions. They no longer work when `validation_data` is passed, maybe behaviour has changed ?  \r\n\r\nAs mentioned, this solution seems to work:\r\n\r\n```python\r\nclass EvalCallback(keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        for x, y in validation_datasets:\r\n            print(self.model.evaluate(x, y, return_dict=True, verbose=0))\r\n\r\nmodel.fit(..., validation_data=None, callbacks=[EvalCallback()])\r\n```\r\n\r\nMaybe Keras could provide a callback which evaluates one or more validation sets instead of the `validation_data` parameter?", "If you want direct support for multiple validations we are going in the https://github.com/tensorflow/tensorflow/issues/38803 perimeter.\r\nI think that you can close this and upvote that issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43262\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43262\">No</a>\n"]}, {"number": 43261, "title": "Self-attention on word embeddings using half-precision with mask_zero set to True crashes training", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v1.12.1-41557-gae0a324182 2.4.0-dev20200915\r\n- Python version: 3.6.9\r\n- CUDA/cuDNN version: Bug reproducible without GPU (but the same bug occurs on GPU)\r\n- GPU model and memory: Bug reproducible without GPU (but the same bug occurs on GPU)\r\n\r\n**Describe the current behavior**\r\n\r\nWhen applying self-attention to word embeddings with half-precision, if mask_zero is set to True, then training crashes. If it is set to False, then training completes without crashing.\r\n\r\n**Describe the expected behavior**\r\n\r\nTrain with mask_zero set to True without crashing.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nReproduced bug in tf-nightly here:\r\nhttps://colab.research.google.com/drive/1lzG0SonIWiqrnCLxzxQO-s2e-fYNoG1c?usp=sharing\r\n\r\n**Other info / logs**\r\n\r\nWhen mask_zero is set to True, the error is:\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n   1135 \r\n-> 1136   def binary_op_wrapper(x, y):\r\n   1137     with ops.name_scope(None, op_name, [x, y]) as name:\r\n\r\n13 frames\r\nValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: <tf.Tensor 'attention_19/MatMul:0' shape=(None, None, None) dtype=float16>\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)\r\n    504 \r\n    505             # If we did not match an allowed dtype, try again with the default\r\n--> 506             # dtype. This could be because we have an empty tensor and thus we\r\n    507             # picked the wrong type.\r\n    508             if inferred is not None and inferred.dtype in allowed_list:\r\n\r\nTypeError: Input 'y' of 'Sub' Op has type float32 that does not match type float16 of argument 'x'.\r\n", "comments": ["I have tried in colab with TF nightly version and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/121ceaf5b03c83fe457988833d7bead5/untitled366.ipynb). Thanks!", "I think this issue will be fixed with PR #46321", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43261\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43261\">No</a>\n"]}, {"number": 43260, "title": "TypeError on line 88 should have a condition", "body": "while exporting the model in Tensorflow Object detection API we get *TypeError('Expected Operation, Variable, or Tensor, got ' + str(x))*\r\nPlease refer this https://github.com/tensorflow/models/issues/8881#issuecomment-665486156", "comments": ["> Thanks for the PR. What is the goal here? No behavior appears to change. Perhaps you could just use string formatting to print the error message?\r\n\r\nSorry! I didn't realize that there is no behavior change with this. I made the PR because many people(including me) were using this for TF OD API to work.", "@aniketmaurya  Can you please check @fchollet's comments and keep us posted ? Thanks!", "> @aniketmaurya  Can you please check @fchollet's comments and keep us posted ? Thanks!\n\nSure", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "@aniketmaurya Any update on this PR? Please. Thanks!", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 43259, "title": "Importing Tensorflow after using pyinstaller (ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.)", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution: Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 1.14\r\n- Python version: 3.7.8\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: None (CPU)\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\nI have installed tensorflow using conda and it works fine in the conda environment. I converted my app into binary file using pyinstaller. While running .exe file, it throws ImportError\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nUsed pyinstaller to convert my flask app to .exe file\r\nFirst issue encountered: No module named tensorflow\r\nCopied the tensorflow folder from the conda environment and pasted it in directory where app.exe is located.\r\nNow facing the Import Error\r\nIs there something else, I need to copy to the working directory or a path to set?\r\n\r\n**Any other info / logs**\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"R:\\armorfinal\\cheque_locate_ws\\dist\\app\\tensorflow\\__init__.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"R:\\armorfinal\\cheque_locate_ws\\dist\\app\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"R:\\armorfinal\\cheque_locate_ws\\dist\\app\\tensorflow\\python\\pywrap_tensorflow.py\", line 59, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"R:\\armorfinal\\cheque_locate_ws\\dist\\app\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 35, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"R:\\armorfinal\\cheque_locate_ws\\dist\\app\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\r\n  File \"C:\\Users\\Apurva\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"C:\\Users\\Apurva\\AppData\\Local\\Programs\\Python\\Python38\\lib\\imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n", "comments": ["From the template it looks like you are installing **TensorFlow** (TF) prebuilt binaries:\n   * For TF-GPU - See point 1\n   * For TF-CPU - See point 2\n-----------------------------------------------------------------------------------------------\n**1. Installing **TensorFlow-GPU** (TF) prebuilt binaries**\n\nMake sure you are using compatible TF and CUDA versions. Please refer following TF version and CUDA version compatibility table.\n| TF  | CUDA |\n| :-------------: | :-------------: |\n| 2.1.0 - 2.2.0  | 10.1 |\n| 1.13.1 - 2.0  | 10.0  |\n| 1.5.0 - 1.12.0 | 9.0 |\n\n  * If you have above configuration and using _**Windows**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the %PATH% environment variable.\n    * Refer [windows setup guide](https://www.tensorflow.org/install/gpu#windows_setup).\n  * If you have above configuration and using _**Ubuntu/Linux**_ platform -\n    * Try adding the CUDA, CUPTI, and cuDNN installation directories to the $LD_LIBRARY_PATH environment variable.\n    * Refer [linux setup guide](https://www.tensorflow.org/install/gpu#linux_setup).\n  * If error still persists then, apparently your CPU model does not support AVX instruction sets.\n    * Refer [hardware requirements](https://www.tensorflow.org/install/pip#hardware-requirements).\n\n-----------------------------------------------------------------------------------------------\n**2. Installing **TensorFlow** (TF) CPU prebuilt binaries**\n\n*TensorFlow release binaries version 1.6 and higher are prebuilt with AVX instruction sets.*\n\nTherefore on any CPU that does not have these instruction sets, either CPU or GPU version of TF will fail to load.\nApparently, your CPU model does not support AVX instruction sets. You can still use TensorFlow with the alternatives given below:\n\n   * Try Google Colab to use TensorFlow.\n      * The easiest way to use TF will be to switch to [google colab](https://colab.sandbox.google.com/notebooks/welcome.ipynb#recent=true). You get pre-installed latest stable TF version. Also you can use ```pip install```  to install any other preferred TF version.\n      * It has an added advantage since you can you easily switch to different hardware accelerators (cpu, gpu, tpu) as per the task.\n      * All you need is a good internet connection and you are all set.\n   * Try to build TF from sources by changing CPU optimization flags.\n\n*Please let us know if this helps.*\n", "It was using pyinstaller installed in the base environment which did not have tensorflow installed.\r\nThis solved the issue\r\n\r\nconda activate your_env_name\r\nconda install -c conda-forge pyinstaller ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43259\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43259\">No</a>\n"]}, {"number": 43258, "title": " ValueError: No gradients provided for any variable: ['embedding_2/embeddings:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/kernel:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/recurrent_kernel:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/bias:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/kernel:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/recurrent_kernel:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0'].", "body": "I am training model in colab. The model I'm trying to build is siamese network for text similarity. \r\nthe loss function which I used finds internally the negative sample.\r\n**Explaination of loss function::\r\nkindly, watch this two small videos where the lecturer explains about the loss function I used::**\r\nhttps://www.coursera.org/lecture/sequence-models-in-nlp/computing-the-cost-i-T4Ylj\r\nhttps://www.coursera.org/lecture/sequence-models-in-nlp/computing-the-cost-ii-qXOjN\r\n**the dataset can be found here::**\r\n[https://www.kaggle.com/c/quora-question-pairs/data](url)\r\n**my colab notebook::**\r\n[https://colab.research.google.com/drive/1NCUxSS9fiuLpPd2hOSxKv5QKH2IUuhX2?usp=sharing](url)\r\n\r\n```\r\nsub_model=tf.keras.models.Sequential([Embedding(vocab_size,300,input_length=79), \r\nBidirectional(LSTM(79,return_sequences=True)),\r\nBidirectional(LSTM(79,return_sequences=True)),\r\ntf.keras.layers.GlobalAveragePooling1D(),\r\ntf.keras.layers.Dense(units=158)])\r\nins1=Input((79,),name='input1')\r\nins2=Input((79,),name='input2')\r\nx1=sub_model(ins1)\r\nx2=sub_model(ins2)\r\nnorm1=tf.keras.layers.Layer(lambda x: tf.math.l2_normalize(x,axis=1))(x1)\r\nnorm2=tf.keras.layers.Layer(lambda x: tf.math.l2_normalize(x,axis=1))(x2)\r\n\r\nmodel=Model([ins1,ins2],[norm1,norm2])\r\n```\r\nthe loss function used is \r\n```\r\n\r\ndef get_tripletloss(y_pred1,y_pred2):\r\n    y_pred=tf.matmul(y_pred1,y_pred2,transpose_b=True) ##getting y_pred of (batch,batch)\r\n    batch=y_pred.get_shape().as_list()[0] ##getting batch_size\r\n    alpha_matrix=tf.cast(tf.reshape(tf.repeat(0.2,repeats=batch),shape=(batch,1)),dtype=tf.float32) #making alpha matrix of 0.2's\r\n    diag_part=tf.cast(tf.reshape(tf.linalg.diag_part(y_pred),shape=(batch,1)),dtype=tf.float32) ##taking diag_part\r\n    diagonal_matrix=tf.cast(tf.linalg.diag(tf.linalg.diag_part(y_pred)),dtype=tf.float32) ## making as diagonal_matrix\r\n    sim_an=tf.reshape(tf.reduce_mean(tf.cast(y_pred,dtype=tf.float32)-diagonal_matrix,axis=1),shape=(batch,1)) ## getting only off-diagonal\r\n    sim_an=tf.cast(sim_an,dtype=tf.float32)-diag_part+alpha_matrix ## getting sim_an-sim_ap+alpha\r\n    sim_an=tf.maximum(sim_an,tf.cast(tf.zeros((batch,1)),dtype=tf.float32)) # getting max(loss,0)\r\n    loss1=tf.keras.backend.mean(sim_an) ##final_loss1\r\n    ##########\r\n    y_pred=tf.where(y_pred<diag_part,y_pred,tf.cast(0.000001,dtype=tf.float32))##made to small number where off-diagonal elements are getter than diagonal and also diagonal elements.\r\n    sim_an2=tf.reshape(tf.reduce_max(y_pred,axis=1),shape=(batch,1)) ##getting max value(closet_negative) \r\n    loss2=tf.cast(sim_an2,dtype=tf.float32)-diag_part+alpha_matrix ##getting sim_an-sim_ap+alpha\r\n    loss2=tf.maximum(loss2,tf.cast(tf.zeros((batch,1)),dtype=tf.float32)) ## max(loss2,0)\r\n    loss2=tf.keras.backend.mean(loss2)\r\n    loss=loss1+loss2\r\n    return loss\r\n```\r\nand the training loop is \r\n```\r\ndataset=tf.data.Dataset.from_tensor_slices((seq1,seq2))\r\ndataset=dataset.shuffle(149263)\r\ndataset=dataset.batch(29,drop_remainder=True)\r\ndataset=dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n\r\n@tf.function\r\ndef get_grads(v1,v2):\r\n    with tf.GradientTape() as tape:\r\n        loss=get_tripletloss(v1,v2)\r\n        grads=tape.gradient(loss,model.trainable_variables)\r\n    return loss,grads\r\nloss_per_batch=[]\r\nloss_per_epoch=[]\r\noptimizer=tf.keras.optimizers.Adam()\r\nfor j in range(2):\r\n    training_batches=seq1.shape[0]//29\r\n    for i in range(training_batches):\r\n        data=next(iter(dataset))\r\n        v1,v2=model(data)\r\n        loss,grads=get_grads(v1,v2)\r\n        loss_per_batch.append(loss)\r\n        optimizer.apply_gradients(zip(grads,model.trainable_variables))\r\n    loss_per_epoch.append(np.mean(loss_per_batch))\r\n    loss_per_batch=[]\r\n    print(f\"finsihed {j+1} epoch got loss of {loss_per_epoch[j]}\")\r\n```\r\nthe error is ::\r\n```\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-98-24aaeac0e75b> in <module>()\r\n      9         loss,grads=get_grads(v1,v2)\r\n     10         loss_per_batch.append(loss)\r\n---> 11         optimizer.apply_gradients(zip(grads,model.trainable_variables))\r\n     12     loss_per_epoch.append(np.mean(loss_per_batch))\r\n     13     loss_per_batch=[]\r\n\r\n1 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)\r\n    511       ValueError: If none of the variables have gradients.\r\n    512     \"\"\"\r\n--> 513     grads_and_vars = _filter_grads(grads_and_vars)\r\n    514     var_list = [v for (_, v) in grads_and_vars]\r\n    515 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)\r\n   1269   if not filtered:\r\n   1270     raise ValueError(\"No gradients provided for any variable: %s.\" %\r\n-> 1271                      ([v.name for _, v in grads_and_vars],))\r\n   1272   if vars_with_empty_grads:\r\n   1273     logging.warning(\r\n\r\nValueError: No gradients provided for any variable: ['embedding_2/embeddings:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/kernel:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/recurrent_kernel:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/bias:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/kernel:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/recurrent_kernel:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0'].\r\n```", "comments": ["@RavitejaBadugu \r\nThe colab note book shared, is empty, please update with complete code and error.\r\nPlease refer to these issues with same error: #42038 [link](https://stackoverflow.com/questions/61570051/valueerror-no-gradients-provided-for-any-variable-conv2d-kernel0-conv2d), ", "@Saduf2019  \r\nthat's not a gist. just copy the link and paste it in a new tab. It's not empty.", "How can we create a loss function which doesn't take y_true. But, My loss function which I showed in the issue calculates the loss without y_true. (for information check the links)In documentation of tf, I found that when we are writing a custom loss function, then \r\nwe need to define as \r\ndef loss(y_true,t_pred, any smoothing_parameter):\r\n##\r\nSo, the function is expecting us to give y_true, the logic in my loss function is correct. My doubt is how to define without y_true. May be it is the one causing the problem. I think! but don't know. Please! clarify this doubt @gowthamkpr ", "I got the solution. I did a small mistake in my code.", "@RavitejaBadugu \r\n> I got the solution. I did a small mistake in my code.\r\n\r\nCan you tell me that mistake because i'm also getting same mistake\r\n\r\n", "@RavitejaBadugu \r\nAlso have same mistake.. i would appreciate if you tell how you solved the mistake", "In custom training loop within tape we need to mention model(data) if we mention it before I got the error.\r\n```loss_per_batch=[]\r\nloss_per_epoch=[]\r\noptimizer=tf.keras.optimizers.Adam()\r\nfor j in range(2):\r\n    training_batches=x_train.shape[0]//12\r\n    for i in tqdm(range(training_batches)):\r\n        data=next(iter(dataset))\r\n        outputs=model(data)## it is mentioned outside the tape\r\n        with tf.GradientTape() as tape:\r\n          losses=loss(outputs.get('out1'),outputs.get('out2'),0.3,-2.0)\r\n        grads=tape.gradient(losses,model.trainable_variables)\r\n        loss_per_batch.append(loss)\r\n        optimizer.apply_gradients(zip(grads,model.trainable_variables))\r\n    loss_per_epoch.append(np.mean(loss_per_batch))\r\n    loss_per_batch=[]\r\n    print(f\"finsihed {j+1} epoch got loss of {loss_per_epoch[j]}\")\r\n```\r\ngetting error as \r\n```\r\nValueError: No gradients provided for any variable: ['tf_roberta_model/roberta/encoder/layer_._0/attention/self/query/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._0/attention/self/query/bias:0', 'tf_roberta_model/roberta/encoder/layer_._0/attention/self/key/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._0/attention/self/key/bias:0', 'tf_roberta_model/roberta/encoder/layer_._0/attention/self/value/kernel:0', 'tf_roberta_model/roberta/encoder/layer_._0/attention/self/value/bias:0', 'tf_roberta_model/roberta/encoder/layer_._0/attention/o....\r\n```\r\nbut when I mention the model within the tape the error is gone\r\n```loss_per_batch=[]\r\nloss_per_epoch=[]\r\noptimizer=tf.keras.optimizers.Adam()\r\nfor j in range(2):\r\n    training_batches=x_train.shape[0]//12\r\n    for i in tqdm(range(training_batches)):\r\n        data=next(iter(dataset))\r\n        with tf.GradientTape() as tape:\r\n          outputs=model(data)###################mentioned here\r\n          losses=loss(outputs.get('out1'),outputs.get('out2'),0.3,-2.0)\r\n        grads=tape.gradient(losses,model.trainable_variables)\r\n        loss_per_batch.append(loss)\r\n        optimizer.apply_gradients(zip(grads,model.trainable_variables))\r\n    loss_per_epoch.append(np.mean(loss_per_batch))\r\n    loss_per_batch=[]\r\n    print(f\"finsihed {j+1} epoch got loss of {loss_per_epoch[j]}\")\r\n```\r\n"]}, {"number": 43257, "title": "MobileNetV3 Small/Large not fully quantized after full integer post-training quantization runs with no errors", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A (used Google Colaboratory)\r\n- TensorFlow version (using the command in the template): v1.12.1-39569-gcb34190201 2.4.0-dev20200818\r\n\r\n**Describe the current behavior**\r\nThe model is not fully quantized after full integer quantization even though no error occurs during the quantization. \r\n\r\n**Describe the expected behavior**\r\nI tried using the quantized version in the example Android app but it crashed every time so I wanted to see if the quantized version worked on Coral devices, but then it failed to compile with the error \"Model is not fully quantized.\".\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1B-VbTOCpmGhgv5adOpnGn68VARo30bF9?usp=sharing\r\n", "comments": ["I could reproduce the issue in colab with TF nightly version(`2.4.0-dev20200915`) .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/d303b7aebb1a4c8fef8743902cb0153e/untitled361.ipynb). Thanks!", "Hi..Im trying to convert Mobilenetv3 onnx model to tensorflow.\r\nI need tensorflow==1.12.0 for my inferencing.\r\nI searched all over to find the onnx and onnx-tf versions corresponding to tensorflow==1.12.0\r\n\r\nCould anyone suggest me the compatible versions of onnx,onnx-tf and torch for my required tensorflow version?\r\n@ravikyram @ymodak @drajsel \r\nThanks in advance!", "@drajsel @ravikyram You need to use the old TOCO with `converter.experimental_new_converter = False` for a working coral compilation. \r\nThis is cause:\r\nhttps://github.com/google-coral/edgetpu/issues/168#issuecomment-679233152\r\n/cc @Namburger", "@ravikyram @bhack The compilation is successful now, but it doesn't seem to matter now because of this:\r\n```\r\nNumber of operations that will run on Edge TPU: 1\r\nNumber of operations that will run on CPU: 122\r\n```\r\nThank you all anyway! \r\n\r\nIt worked for the downloaded model, but in the case where I have two outputs it doesn't work (`Unsupported explicit zero output index: serving_default_input_2:0`). Do you have any suggestions, what I should look into? I know it's outside the scope of this bug, but I've tried many approaches in TF 2, TF 1.15, using the toco command line converter like seen in quantization for object detection models in TF 1.x etc. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43257\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43257\">No</a>\n", "@drajsel I don't know If there Is something fresh from the unstable apt channel instead of the stable in echo `\"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list`\nSo that you don't need to use TOCO (converter.experimental_new_converter = False)"]}, {"number": 43256, "title": "mAP drops crazily when replacing Conv with Depthwise_Conv", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\r\n- TensorFlow installed from (source or binary): package manager\r\n- TensorFlow version (or github SHA if from source): 2.3.0\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nIf possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model_yolo)\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.representative_dataset = representative_data_gen\r\n    \r\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n    converter.inference_input_type = tf.int8\r\n    converter.inference_output_type = tf.int8\r\n    \r\n    tflite_model_quant = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n# Nothing. Successfully converted to tflite model.\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\n# Model Definition file see attachment\r\n[mobilenet_v2_yolo_v3.zip](https://github.com/tensorflow/tensorflow/files/5229629/mobilenet_v2_yolo_v3.zip)\r\n\r\n```\r\n\r\n**Failure details**\r\nIf the conversion is successful, but the generated model is wrong,\r\nstate what is wrong:\r\n- Producing huge decrease in accuracy\r\n\r\n\r\n**Any other info / logs**\r\nI am doing object detection using MoblieNet Yolo v3.\r\nFor backbone I use mobilenet v2. I almost copy and paste from official keras implementation. The problem seems happens in YOLO head (I called it \"_make_last_layers\" in my code).\r\nFor the first time, I use  \r\n`_make_last_layers_1()`\r\nas my YOLO head.  After converted to tflite model, mAP only drops around 1~4% compared to original model.\r\n\r\nFor the second time, I use\r\n`_make_last_layers_2()`\r\nas my YOLO head.  After converted to tflite model, mAP drops around **15~19%** compared to original model.\r\n\r\nSo the difference between _make_last_layers_1() and  _make_last_layers_2() is I try to use the combination of Depthwise_Conv+Conv to replace pure Conv so that I can reduce tons of parameters(23M -> 6.8M)\r\n\r\nSo...any idea why this structure causes such great drops in accuracy?", "comments": ["@neesetifa,\r\nOn running the code I am facing an error stating `NameError: name 'representative_data_gen' is not defined`.\r\n\r\n\r\n\r\n\r\n> as my YOLO head. After converted to tflite model, mAP drops around **15~19%** compared to original model.\r\n\r\nCould you please provide the complete code and the dataset to reproduce reported here. Thanks!", "@amahendrakar   Thank you for the reply. I put all the necessary code and the model in a zip file and upload it to google drive. You can find it here:\r\nhttps://drive.google.com/file/d/13lsXf79RaWy11SQ5yuERyp0pT2Lc68Nr/view?usp=drivesdk\r\nAnd I also put a ReadMe file for all the details of the code in the zip file.\r\n", "@neesetifa,\r\nThank you for the update. The code you've provided is fairly complex hence it would be difficult for us to pinpoint the issue. Can you remove the dependencies and get the example down to the simplest possible repro? That will allow us to debug the issue easily. Thanks!", "Hi, @amahendrakar Sorry for the mess of my repo. \r\nFollowing are the files that I provided:\r\nModel -->\r\n1  Original keras model (.h5)\r\n2  TFlite model  (.tflite)\r\nCode -->\r\n1  Complete definition of my model \r\n2  Script for converting original keras model to tflite model\r\n3  Script for testing original keras model on test dataset\r\n4  Script for testing tflite model on the same test dataset  \r\n5  Script for calculating mAP based on the test dataset result\r\n\r\nWith above files, I can reproduce\r\n\r\n> After converted to tflite model, mAP drops around 15~19% compared to original model.\r\n\r\nI don't have much experience for debugging this kind of issue. Could you let me know which part(s) you need for debugging this, so that I can remove all the unnecessary parts and update the repo.   \r\n\r\nThank you", "@neesetifa,\r\nOn running the given code, I am facing an error stating `AttributeError: 'NoneType' object has no attribute 'shape'`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/4f2a37f3d94d9adbe47d5432a7e0db4a/43256.ipynb). \r\n\r\n> Could you let me know which part(s) you need for debugging this, so that I can remove all the unnecessary parts and update the repo.\r\n\r\nPlease provide a simple standalone code snippet which we can run at out end and reproduce the issue. Thanks!", "@amahendrakar  It seems that you might put the image files in the wrong directory. In representative_data_gen(), cv2.imread() didn't get any image so it returns 'None'.  \r\nI understand that the codes need to be as simple as possible.  In order to reproduce the issue, I think at least the conversion code and mAP code are needed.  Let me see if I can upload all the codes with training/test data in correct directory and put a the process in a single script.", "> Let me see if I can upload all the codes with training/test data in correct directory and put a the process in a single script.\r\n\r\n@neesetifa,\r\nAny updates regarding this? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 43255, "title": "Failed to load the native TensorFlow runtime.", "body": "Hi,\r\n\r\nI am using GeForce GTX 1660 Ti 6GB GPU on windows and executed following command.\r\nfrom tensorflow.python.client import device_lib\r\n\r\nError:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Sudhakar\\anaconda3\\envs\\keras_tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Sudhakar\\anaconda3\\envs\\keras_tf\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\Sudhakar\\anaconda3\\envs\\keras_tf\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 39, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"C:\\Users\\Sudhakar\\anaconda3\\envs\\keras_tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\Sudhakar\\anaconda3\\envs\\keras_tf\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.", "comments": ["@UniqueSud\r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See[ hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website.](https://www.tensorflow.org/install/source_windows)\r\n\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167 and see if it helps you.Please, refer similar issues #36167 #36151 #36138 #36054 #36045 #36020 #36003 #35988 #35903 #35880 #35865 #35805 #35789 #35773 #35772 #35767 #35766 #35749 #35721 #35618 #35204\r\nThanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43255\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43255\">No</a>\n"]}, {"number": 43254, "title": "TextVectorization not working on TPU with tf-nightly.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- TensorFlow version (use command below):tf-nightly:2.4.0-dev20200915\r\n- Using TPU: Yes\r\n\r\nI am trying to run TextVectorization on TPU\r\n\r\n```\r\ndef get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=['[MASK]']):\r\n\r\n  vectorize_layer = preprocessing.TextVectorization(\r\n    max_tokens=vocab_size,\r\n    output_mode='int',\r\n    standardize = None,\r\n    output_sequence_length=max_seq)\r\n  \r\n  vectorize_layer.adapt(texts)\r\n\r\n  vocab = vectorize_layer.get_vocabulary()\r\n  vocab = vocab[2:vocab_size-len(special_tokens)] + ['[MASK]']\r\n  vectorize_layer.set_vocabulary(vocab)\r\n  return vectorize_layer\r\n\r\n\r\nvectorize_layer = get_vectorize_layer(data.text.values.tolist(), 20000, 196, special_tokens=['[MASK]'])\r\n```\r\n Error:\r\n\r\n> NotFoundError: 'OptimizeDatasetV2' is neither a type of a primitive operation nor a name of a function registered in binary running on n-86c78cbc-w-0. Make sure the operation or function is registered in the binary running in this process.\r\n\r\nTensorflow: 2.4.0-dev20200915\r\nReference Notebook: https://colab.research.google.com/drive/1QiQmA2M2WkwsjiHHxBhPM0Jw3_LBJnGV?usp=sharing", "comments": ["@Ankur3107 \r\n\r\nI have tried in colab with TF version 2.3 and i am not seeing any issue. Please, find the gist [here.](https://colab.research.google.com/gist/ravikyram/94a3caa305a8d3476f03cfd2afbdc332/untitled358.ipynb).\r\nHowever i am able to reproduce the issue in TF nightly version (`2.4.0-dev20200915`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/8ad11414ca33973fcbf402b68cc74bf6/untitled357.ipynb).Thanks!", "@ravikyram True, it's working with TF version 2.3. I need to use MultiHeadAttention that why i need tf-nightly. Thanks", "This seems to be due to a mismatch between TPU and host code. TPU doesn't have the same Graphdef API as the host.\r\n\r\nNightly build exposes OptimizeV2 as the [default op in Python code](https://github.com/tensorflow/tensorflow/blob/5c42efeedaa92b82fd9c6fe831a307c9fa5d4b6d/tensorflow/python/data/ops/dataset_ops.py#L4497), and the C++ interface on TPU is still expecting OptimizeV1 (OptimizeV2 is not a valid op in the GraphDef). 2.3 [still uses OptimizeV1](https://github.com/tensorflow/tensorflow/blob/ee598066c4cb31ec5ed3106e61ba99ef004a4bae/tensorflow/python/data/ops/dataset_ops.py#L386), which exists and can be deserialized just fine. A [compatibility check was in place](https://github.com/tensorflow/tensorflow/commit/3d0da5f86318483dcf9bd5649830380a647bd3fc) at one point.\r\n\r\nYou can therefore perhaps just edit the python code to use OptimizeV1 as a quick fix.\r\n\r\nAlso, I don't think this is a keras-only issue. The code is on the tf.data path, so any users of the dataloader will likely experience this issue.", "@jsimsa ", "TensorFlow provide backwards compatibility (i.e. older client works with newer server) between minor versions but not forward compatibility (i.e. newer client works with older server). For more details see https://www.tensorflow.org/guide/versions.\r\n\r\nThe issue you have encountered is because you are using a newer client with older server, which is not guaranteed to work. In other words, there is nothing to fix here.", "thanks, @jsimsa @mkuchnik  @jvishnuvardhan @ravikyram. I got my answer,  there is nothing to fix here.!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43254\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43254\">No</a>\n"]}, {"number": 43253, "title": "setting verbosity to 0 does not suppress keras debug messages, no way to turn them off", "body": "Using latest version(s) from Colab\r\n\r\nImpossible to suppress the \"Found X images belonging to N classes.\" that results from Keras flow_from_directory method", "comments": ["@prismspecs,\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and the dataset you are using. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43252, "title": "tf.saved_model.save very slow with second-order tf.autodiff.ForwardAccumulator", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b\r\n- GPU model and memory: No gpu\r\n4xIntel(R) Core(TM) i7-8650U CPU @ 1.90GHz\r\n32 GB RAM\r\n\r\n**Describe the current behavior**\r\n\r\nSaving a tf.module involving second-order tf.autodiff.ForwardAccumulator takes too much time; 1 hour for the example below\r\n\r\n**Describe the expected behavior**\r\n\r\nSaving the graph in the example should take few seconds\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nimport time\r\n\r\nclass Issue_fwd(tf.Module):\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float64)] * 3 +\r\n                                 [tf.TensorSpec([None, 3], tf.float64)] +\r\n                                 [tf.TensorSpec([1, None], tf.float64)] * 4)\r\n    def f(self, x1, x2, x3, c, v1, v2, v3, v4):\r\n\r\n        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \\\r\n                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:\r\n\r\n            with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1, \\\r\n                 tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2, \\\r\n                 tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:\r\n\r\n                p = tf.concat([x1, x2, x3], axis=1)\r\n                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])\r\n                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])\r\n                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)\r\n                G = tf.exp(-r / 2)\r\n\r\n                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)\r\n                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)\r\n                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)\r\n                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)\r\n\r\n            dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz,  d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,\r\n\r\n\r\nf = Issue_fwd()\r\nsaving_path = 'save_path'\r\nos.makedirs(saving_path, exist_ok=True)\r\n\r\nstart_time = time.clock()\r\ntf.saved_model.save(f, saving_path)\r\ndelta_time = time.clock() - start_time\r\nprint('saving took {:f} seconds'.format(delta_time))\r\nprint('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))\r\nprint('tf.version.VERSION={}'.format(tf.version.VERSION))\r\n\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\nsaving took 3942.697280 seconds\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\n```\r\n\r\n", "comments": ["Could you try:\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport time\r\n\r\nclass Issue_fwd(tf.Module):\r\n\r\n    input_signature=([tf.TensorSpec([None, 1], tf.float64)] * 3 +\r\n                                 [tf.TensorSpec([None, 3], tf.float64)] +\r\n                                 [tf.TensorSpec([1, None], tf.float64)] * 4)\r\n    @tf.function\r\n    def f(self, x1, x2, x3, c, v1, v2, v3, v4):\r\n\r\n        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \\\r\n                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:\r\n\r\n            with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1, \\\r\n                 tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2, \\\r\n                 tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:\r\n\r\n                p = tf.concat([x1, x2, x3], axis=1)\r\n                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])\r\n                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])\r\n                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)\r\n                G = tf.exp(-r / 2)\r\n\r\n                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)\r\n                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)\r\n                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)\r\n                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)\r\n\r\n            dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz,  d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,\r\n\r\n\r\nf = Issue_fwd()\r\nsaving_path = 'save_path'\r\nos.makedirs(saving_path, exist_ok=True)\r\n\r\nstart_time = time.clock()\r\ntf.saved_model.save(f, saving_path)\r\ndelta_time = time.clock() - start_time\r\nprint('saving took {:f} seconds'.format(delta_time))\r\nprint('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))\r\nprint('tf.version.VERSION={}'.format(tf.version.VERSION))\r\n```", "Can you post a sample input for `f()`?", "Hi @bhack .  This is the output of [your suggestion](https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-693027496):\r\n\r\n```\r\nsaving took 0.061019 seconds\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\n```\r\n\r\nI included the `input_signature` argument following the recommendation in [this guide](https://www.tensorflow.org/guide/function#controlling_retracing) .  Now I'm confused, who should I prevent retracing but still manage to avoid this excessive time to save the graph?\r\n", "@andrescodas I've tried to simplify your example to perimeter a little bit the main problem.\r\nI've tried to remove the nested `tf.autodiff.ForwardAccumulator` on the same variable (x1,x2).\r\nCan you try to run it?\r\n\r\n```\r\nimport os\r\nimport tensorflow as tf\r\nimport time\r\n\r\nclass Issue_fwd(tf.Module):\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float64)] * 3 +\r\n                                 [tf.TensorSpec([None, 3], tf.float64)] +\r\n                                 [tf.TensorSpec([1, None], tf.float64)] * 4)\r\n    def f(self, x1, x2, x3, c, v1, v2, v3, v4):\r\n\r\n        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \\\r\n                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:\r\n\r\n            with tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:\r\n\r\n                p = tf.concat([x1, x2, x3], axis=1)\r\n                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])\r\n                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])\r\n                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)\r\n                G = tf.exp(-r / 2)\r\n\r\n                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)\r\n                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)\r\n                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)\r\n                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)\r\n             \r\n            \"\"\"dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\"\"\"\r\n\r\n            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        #d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        #d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        #d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        #d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        #d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        #d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        return dudt, dwdt, dbdt\r\nf = Issue_fwd()\r\nsaving_path = 'save_path'\r\nos.makedirs(saving_path, exist_ok=True)\r\n\r\nstart_time = time.clock()\r\ntf.saved_model.save(f, saving_path)\r\ndelta_time = time.clock() - start_time\r\nprint('saving took {:f} seconds'.format(delta_time))\r\nprint('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))\r\nprint('tf.version.VERSION={}'.format(tf.version.VERSION))\r\n```", "the output for [the example above](https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-693407426) is:\r\n\r\n`\r\nsaving took 15.386245 seconds\r\n\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\n\r\ntf.version.VERSION=2.3.0\r\n`", "Can you give a sample input args for `def f(self, x1, x2, x3, c, v1, v2, v3, v4)`?", "> Can you post a sample input for `f()`?\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\nclass Issue_fwd(tf.Module):\r\n\r\n    input_signature=([tf.TensorSpec([None, 1], tf.float64)] * 3 +\r\n                                 [tf.TensorSpec([None, 3], tf.float64)] +\r\n                                 [tf.TensorSpec([1, None], tf.float64)] * 4)\r\n    @tf.function\r\n    def f(self, x1, x2, x3, c, v1, v2, v3, v4):\r\n\r\n        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \\\r\n                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:\r\n\r\n            with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1, \\\r\n                 tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2, \\\r\n                 tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:\r\n\r\n                p = tf.concat([x1, x2, x3], axis=1)\r\n                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])\r\n                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])\r\n                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)\r\n                G = tf.exp(-r / 2)\r\n\r\n                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)\r\n                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)\r\n                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)\r\n                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)\r\n\r\n            dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n        d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz,  d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,\r\n\r\n\r\nissue_fwd = Issue_fwd()\r\n\r\nn = 10\r\nx1 = tf.random.uniform((n, 1), dtype=tf.float64)\r\nx2 = tf.random.uniform((n, 1), dtype=tf.float64)\r\nx3 = tf.random.uniform((n, 1), dtype=tf.float64)\r\n\r\nc = tf.random.uniform((n,3), dtype=tf.float64)\r\n\r\nv1 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv2 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv3 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv4 = tf.random.uniform((1, n), dtype=tf.float64)\r\n\r\n\r\nstart_time = time.clock()\r\ndudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz, d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z = issue_fwd.f(x1, x2, x3, c, v1, v2, v3, v4)\r\ndelta_time = time.clock() - start_time\r\n\r\nprint('running took {:f} seconds'.format(delta_time))\r\nprint('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))\r\nprint('tf.version.VERSION={}'.format(tf.version.VERSION))\r\n```\r\n\r\nI`m trying to run this concrete case and it is taking many minutes already", "Yes in your last example I removed:\r\n```\r\n\r\n    input_signature=([tf.TensorSpec([None, 1], tf.float64)] * 3 +\r\n                                 [tf.TensorSpec([None, 3], tf.float64)] +\r\n                                 [tf.TensorSpec([1, None], tf.float64)] * 4)\r\n    @tf.function\r\n```\r\nand as you can see with a run it is not the (`tf.function`) tracing with the `input_signature` but it is `f`.", "the output of [this concrete case](https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-693412767)  is:\r\n\r\n```\r\nrunning took 738.183305 seconds\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\n```", "Yes but I suppose tracing a `738.183305` second function it will be quite slow. Just if you execute/trace the function 5 times you will something similar to the original `3942.697280`.\r\nSo I want to understand here if you think/expect that `f` needs to run faster (on CPU on GPU?) or something else.", "> So I want to understand here if you think/expect that f needs to run faster (on CPU on GPU?) or something else.\r\n\r\nSure! ... That is why I raised the issue.  I'm wondering how to best use the computing resources to run these computations.  This is an excerpt of a larger code I have, and I'm failing to scale it up, for the reasons being discussed here.", "@allenlavoie Can you give any feedback about the `tf.autodiff.ForwardAccumulator` use in this function?", "I think the outer ForwardAccumulators will end up watching the inner accumulator's output; that's how you get higher-order forward autodiff, which in my experience blows up the graph size if iterated more than 4ish times (even for something simple like tf.cos).\r\n\r\nMaybe the docstring needs an example, but you can pass a list of primals and a list of tangents: https://github.com/tensorflow/tensorflow/blob/3da9cc88d244c415a6ac1c0b81fb0983809829b0/tensorflow/python/eager/forwardprop.py#L344-L347\r\n\r\nIf you do that and only have two levels of nesting it shouldn't create huge graphs. (Note: I've only skimmed the thread, so let me know if the 5-level nesting was intended and I can take another look.)", "> If you do that and only have two levels of nesting it shouldn't create huge graphs. (Note: I've only skimmed the thread, so let me know if the 5-level nesting was intended and I can take another look.)\r\n\r\nthanks @allenlavoie .   5-level nesting was ***NOT*** intended.  `x1`, `x2`, `x3` are independent root-variables of the graph, and I want to compute the partial derivatives an Laplacians of `p`, `b` ,`u`, `w` with respect to `x1`, `x2`, `x3`.  For that, second-order nesting should be sufficient.", "I didn't pass a list of primals to the ForwardAccumulators because I couldn't figure out how to provide multiple right-hand-sides (or tangents -- the 'p' of jvp).  Then I created multiple ForwardAccumulators that I didn't know where nested.", "Yeah that should be documented. This test has an example: https://github.com/tensorflow/tensorflow/blob/3da9cc88d244c415a6ac1c0b81fb0983809829b0/tensorflow/python/eager/forwardprop_test.py#L639-L640\r\n\r\nThe lists are zipped together; the first element of the primals list has its tangent recorded as the first element of the tangents list and so on.", "I don't know if I was clear. When I said multiple right-hand-sides I meant several instances of `tangents=[tangent1, tangent2]`, e.g.:\r\n`tangents_1=[tf.ones_like(m1), tf.zeros_like(m2)]`, and;\r\n`tangents_2=[tf.zeros_like(m1), tf.ones_like(m2)]`.\r\n\r\nCould you point me to an example that would be efficient for this case?\r\n", "Ah, batching multiple tangents associated with one primal? A GSoC student started on built-in `tf.vectorized_map` integration, but unfortunately it's not complete (e.g. won't work for higher-order yet because its tf.function integration has issues).\r\n\r\nYou can use it with tf.vectorized_map yourself for sure: https://github.com/tensorflow/tensorflow/blob/3da9cc88d244c415a6ac1c0b81fb0983809829b0/tensorflow/python/eager/forwardprop_test.py#L93-L97\r\n\r\nOne issue is that `tf.vectorized_map` traces the function at the moment rather than executing it eagerly.\r\n\r\nYou can also run forwardprop in a Python for loop if there aren't many tangents. That'd happen eagerly.", "@allenlavoie Do we want to maintain this issue to track/update some development activity for the subscribers or do you think that we can close it?", "If someone wants to re-purpose it as a request for the built-in vectorization support to be completed I think that's fine. Please re-open (or comment) if so.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43252\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43252\">No</a>\n", "Hi again @bhack and @allenlavoie , sorry for taking so long to answer back.  I tried the solution you suggested:\r\n\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\nimport time\r\n\r\n\r\ndef _jvp(f, primals, tangents):\r\n    with tf.autodiff.ForwardAccumulator(primals, tangents) as acc:\r\n        primals_out = f(*primals)\r\n    return primals_out, acc.jvp(\r\n        primals_out, unconnected_gradients=tf.UnconnectedGradients.ZERO)\r\n\r\n\r\ninput_signature = [tf.TensorSpec([None, 1], tf.float64)] * 3 + \\\r\n                     [tf.TensorSpec([None, 3], tf.float64)] + \\\r\n                     [tf.TensorSpec([1, None], tf.float64)] * 4\r\n\r\n@tf.function(input_signature=input_signature)\r\ndef ff(x, z, t, c, v1, v2, v3, v4):\r\n\r\n    p = tf.concat([x, z, t], axis=1)\r\n    pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])\r\n    ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])\r\n    d = ce - pe\r\n    r = tf.reduce_sum(input_tensor=tf.square(d), axis=2)\r\n    G = tf.exp(-r / 2)\r\n\r\n    p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)\r\n    b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)\r\n    u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)\r\n    w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)\r\n\r\n    return p, b, u, w\r\n\r\nclass Issue_fwd(tf.Module):\r\n    @tf.function(input_signature=input_signature)\r\n    def f(self, x, z, t, c, v1, v2, v3, v4):\r\n\r\n        fi = lambda xi, zi, ti: ff(xi, zi, ti, c, v1, v2, v3, v4)\r\n        primals = [x, z, t]\r\n        tangent_mask = [tf.zeros_like(primal) for primal in primals]\r\n\r\n        with tf.autodiff.ForwardAccumulator(primals=[x], tangents=[tf.ones_like(x)]) as fwd_outer:\r\n            i = 0\r\n            primals = [x, z, t]\r\n            [dpdx, dbdx, dudx, dwdx] = _jvp(fi, primals, tangent_mask[:i] + [tf.ones_like(primals[i])] + tangent_mask[i + 1:])[1]\r\n        [d2bd2x, d2ud2x, d2wd2x] = fwd_outer.jvp([dbdx, dudx, dwdx], tf.UnconnectedGradients.ZERO)\r\n\r\n        with tf.autodiff.ForwardAccumulator(primals=[z], tangents=[tf.ones_like(z)]) as fwd_outer:\r\n            i = 1\r\n            primals = [x, z, t]\r\n            [dpdz, dbdz, dudz, dwdz] = _jvp(fi, primals, tangent_mask[:i] + [tf.ones_like(primals[i])] + tangent_mask[i + 1:])[1]\r\n        [d2bd2z, d2ud2z, d2wd2z] = fwd_outer.jvp([dbdz, dudz, dwdz], tf.UnconnectedGradients.ZERO)\r\n\r\n        i = 2\r\n        [p, b, u, w], [dpdt, dbdt, dudt, dwdt] = _jvp(fi, primals, tangent_mask[:i] + [tf.ones_like(primals[i])] + tangent_mask[i + 1:])\r\n\r\n        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz, d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,\r\n\r\n\r\nissue_fwd = Issue_fwd()\r\nsaving_path = 'save_path'\r\nos.makedirs(saving_path, exist_ok=True)\r\n\r\nstart_time = time.clock()\r\ntf.saved_model.save(issue_fwd, saving_path)\r\ndelta_time = time.clock() - start_time\r\nprint('saving took {:f} seconds'.format(delta_time))\r\nprint('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))\r\nprint('tf.version.VERSION={}'.format(tf.version.VERSION))\r\n\r\n\r\nn = 5000\r\nx = tf.random.uniform((n, 1), dtype=tf.float64)\r\nz = tf.random.uniform((n, 1), dtype=tf.float64)\r\nt = tf.random.uniform((n, 1), dtype=tf.float64)\r\n\r\nc = tf.random.uniform((n,3), dtype=tf.float64)\r\n\r\nv1 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv2 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv3 = tf.random.uniform((1, n), dtype=tf.float64)\r\nv4 = tf.random.uniform((1, n), dtype=tf.float64)\r\n\r\n\r\nstart_time = time.clock()\r\np, b, u, w = ff(x, z, t, c, v1, v2, v3, v4)\r\ndelta_time = time.clock() - start_time\r\nprint('running ff took {:f} seconds'.format(delta_time))\r\n\r\n\r\n\r\nstart_time = time.clock()\r\ndudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz, d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z = issue_fwd.f(x, z, t, c, v1, v2, v3, v4)\r\ndelta_time = time.clock() - start_time\r\nprint('running Issue_fwd.f took {:f} seconds'.format(delta_time))\r\n\r\n```\r\n\r\nthe output of the script is:\r\n\r\n```\r\nsaving took 5.672405 seconds\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\nrunning ff took 3.102973 seconds\r\nrunning Issue_fwd.f took 262.284672 seconds\r\n```\r\n\r\nPlease let me if I'm wrong.  I was expecting `Issue_fwd.f` to take not much more than 3x `ff`.  Further it seems that `Issue_fwd.f` takes all of the 32GB RAM memory of machine and steps a bit into the swap. The largest variables are `d`, `r` and `G` in `ff` which should take at most 1GB of memory if they are all kept simultaneously in memory.\r\n\r\n", "There's a decent amount going on inside that tf.function which could be parallelized by the executor.\r\n\r\nI'd check if eager execution also uses that much peak memory. It probably uses less, I'd just check whether it matches your expectation. If yes you can get that by adding tf.control_dependencies; tf.function tries to execute everything at once if data dependencies don't prevent it.\r\n\r\nYou could also turn on tf.function(experimental_compile=True). XLA executes closer to sequentially (and may apply other memory-saving optimiztions).", "Commenting the decorator  `@tf.function(input_signature=input_signature)` (I'm assuming that is equivalent to running eagerly):\r\n\r\n```\r\nsaving took 0.066272 seconds\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\nrunning ff took 3.957520 seconds\r\nrunning Issue_fwd.f took 100.743039 seconds\r\n```\r\n\r\nIt didn't blow the memory too.\r\n\r\n\r\nusing  `@tf.function(experimental_compile=True)` I get this error:\r\n\r\n`LookupError: No gradient defined for operation 'gradients/gradients/concat_grad/Slice_grad/XlaDynamicUpdateSlice' (op type: XlaDynamicUpdateSlice)`\r\n\r\n------------\r\nI will check how to use `tf.control_dependencies` thanks for the suggestion.\r\n\r\nI'll appreciate if you have other guidelines on how to get this code optimized, I thought that running it with `@tf.function` would help.\r\n\r\nAfter [this comment](https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-693527200) I'm reluctant to dig into `tf.vectorized_map`", "You're changing how `ff` is decorated? Unfortunate that experimental_compile doesn't support higher-order gradients (I'm guessing), but in your case I'd suggest changing the tf.function around `f` instead. A bit awkward, but if you want an apples-to-apples comparison you could have `f` call an un-decorated version of `ff` and then have a decorated version of `ff` to benchmark standalone.\r\n\r\nCompiling all of `f` is where you'd get the most benefit from XLA.", "> You're changing how ff is decorated?\r\n\r\nYes on [these](https://github.com/tensorflow/tensorflow/issues/43252#issuecomment-696991585) experiments I changed both ff and Issue_fwd.f decorators accordingly, i.e, the first experiment commenting out both instances, and in the second experiment including ` @tf.function(experimental_compile=True)` to both functions\r\n\r\n>in your case I'd suggest changing the tf.function around f instead. A bit awkward, but if you want an apples-to-apples comparison you could have f call an un-decorated version of ff and then have a decorated version of ff to benchmark standalone. \r\n>\r\n>Compiling all of f is where you'd get the most benefit from XLA.\r\n\r\nSorry, I got confused with your explanation, then I tried all the alternatives:\r\n\r\n- ` @tf.function(experimental_compile=True)`  raises the error even if I use it alone in `ff` or in  `Issue_fwd.f`.\r\n- I got the best performance so far decorating  `Issue_fwd.f` with `@tf.function(input_signature=input_signature)` and ff with no decorator:\r\n\r\n```\r\nsaving took 2.310552 seconds\r\ntf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087\r\ntf.version.VERSION=2.3.0\r\nrunning ff took 3.793245 seconds\r\nrunning Issue_fwd.f took 69.214752 seconds\r\n```\r\n\r\nIt would be nice to understand why because I tend to use @tf.fuction with input_signature whenever possible following [this guide](https://www.tensorflow.org/guide/function)", "I think the short answer is that `tf.function` produces function call operations, and the gradients/jvps of call operations are more complicated than the equivalent eager operations, especially if you're nesting to take higher-order gradients. Usually graph optimizations re-simplify, but apparently not in this case. There's a plan to unify these and always do the thing we do in eager right now, but it won't happen very quickly.\r\n\r\nI see why you get that error from experimental_compile with `Issue_fwd.f` but not `ff` decorated. It looks like it was added in https://github.com/tensorflow/tensorflow/commit/6a6261c0a0e803891af95f5e754180739df1897d\r\n\r\n@yunxing do we need to register a gradient for xla_dynamic_update_slice? Is there a bug for it?"]}, {"number": 43251, "title": "Sagemaker resolver", "body": "This is a Cluster Resolver for AWS SageMaker. \r\n\r\nSageMaker currently only supports Homogeneous clusters, so this cluster resolver only resolves to all nodes in the cluster to be workers. ", "comments": ["@frankchn Thank you for the quick response. I have added those changes. ", "Thanks for the update! We will have to wait for some internal tests to pass before merging, but this should get in within the next few days.", "@sboshin Can you please address Ubuntu Sanity errors? Thanks!"]}, {"number": 43250, "title": "get_output_shapes does not return the correct dimensions for a Dataset", "body": "**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Using docker container with base image `tensorflow/tensorflow:2.3.0-jupyter`\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI am using AutoKeras to train to train a neural net for image regression, but it is crashing because `tensorflow.compat.v1.data.get_output_shapes(dataset)` function used within AutoKeras is returning shapes that have `None` for each of the dimension lengths rather than an integer. According to [this TensorFlow issue](https://github.com/tensorflow/tensorflow/issues/30774), this function has been replaced by `tf.data.experimental.get_structure`, but both of these return None in their output. I have confirmed that all the tensors in my dataset are the same shape and I have run my Dataset against these functions both with my original Dataset and with the Dataset AutoKeras creates from it and None is returned in both cases as shown in my code snippet below.\r\n\r\n**Describe the expected behavior**\r\n\r\nEither `tensorflow.compat.v1.data.get_output_shapes(dataset)` or `tf.data.experimental.get_structure` would return the correct shape of elements in a Dataset where all elements have the same shape.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nThis snippet contrasts the output of `tensorflow.compat.v1.data.get_output_shapes(dataset)`, `tf.data.experimental.get_structure`, and a custom function that returns the shape of the first element in a Dataset.\r\n```python\r\ndef get_first_element_shape(dataset):\r\n    for thing in dataset:\r\n        if isinstance(thing, tuple):\r\n            return tuple(element.shape for element in thing)\r\n        else:\r\n            return thing.shape\r\n\r\nprint(tf.compat.v1.data.get_output_shapes(train_dataset))\r\nprint(tf.data.experimental.get_structure(train_dataset))\r\nprint(get_first_element_shape(train_dataset))\r\n\r\nx = train_dataset.map(lambda a, b: a)\r\nprint('\\nX dataset (Created by AutoKeras by separating the input values from the output values in the training dataset)')\r\nprint(tf.compat.v1.data.get_output_shapes(x))\r\nprint(tf.data.experimental.get_structure(x))\r\nprint(get_first_element_shape(x))\r\n```\r\n\r\nThis outputs:\r\n```\r\n(TensorShape([None, None, None]), TensorShape([1]))\r\n(TensorSpec(shape=(None, None, None), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))\r\n(TensorShape([600, 600, 3]), TensorShape([1]))\r\n\r\nX dataset (Created by AutoKeras by separating the input values from the output values in the training dataset)\r\n(None, None, None)\r\nTensorSpec(shape=(None, None, None), dtype=tf.float32, name=None)\r\n(600, 600, 3)\r\n```\r\n", "comments": ["Could you share a very minimal but complete example or Colab to reproduce this?", "This code snipped will generate some images, save them to a TFRecord file, and then load the file into a dataset and print out the same output as above.\r\n```python\r\nimport numpy as np\r\nimport os\r\nimport tempfile\r\nimport tensorflow as tf\r\nfrom io import BytesIO\r\nfrom PIL import Image\r\nfrom random import random\r\n\r\n# This function converts an image (as a binary string) and the output value for it to an Example to save to a TFRecord\r\n# This function based on this tutorial: https://www.tensorflow.org/tutorials/load_data/tfrecord#walkthrough_reading_and_writing_image_data\r\ndef image_and_output_to_example(image_string, output_value):\r\n    # Create some helper functions\r\n    def _bytes_feature(value):\r\n        \"\"\"Returns a bytes_list from a string / byte.\"\"\"\r\n        if isinstance(value, type(tf.constant(0))):\r\n            value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\r\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n    def _float_feature(value):\r\n        \"\"\"Returns a float_list from a float / double.\"\"\"\r\n        return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\r\n    def _int64_feature(value):\r\n        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\r\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n\r\n    image_shape = tf.io.decode_png(image_string).shape\r\n    feature = {\r\n        'height': _int64_feature(image_shape[0]),\r\n        'width': _int64_feature(image_shape[1]),\r\n        'depth': _int64_feature(image_shape[2]),\r\n        'label': _float_feature(output_value),\r\n        'image_raw': _bytes_feature(image_string),\r\n    }\r\n    \r\n    return tf.train.Example(features=tf.train.Features(feature=feature))\r\n\r\n# This function converts the Examples in the TFRecord into a tuple of (image_tensor, [output_value])\r\n# Also based on the above tutorial\r\ndef parse_tf_example(example_proto):\r\n    image_feature_description = {\r\n        'height': tf.io.FixedLenFeature([], tf.int64),\r\n        'width': tf.io.FixedLenFeature([], tf.int64),\r\n        'depth': tf.io.FixedLenFeature([], tf.int64),\r\n        'label': tf.io.FixedLenFeature([], tf.float32),\r\n        'image_raw': tf.io.FixedLenFeature([], tf.string),\r\n    }\r\n    \r\n    normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1.0/255)\r\n\r\n    parsed_example = tf.io.parse_single_example(example_proto, image_feature_description)\r\n    image_as_tensor = tf.io.decode_png(parsed_example['image_raw'])\r\n    return (normalization_layer(image_as_tensor), [parsed_example['label']])\r\n\r\n# Generate some images and save them to a TFRecord file\r\nrecord_file = os.path.join(tempfile.gettempdir(), 'images.tfrecords')\r\nwith tf.io.TFRecordWriter(record_file) as writer:\r\n    for image_num in range(50):\r\n        random_pixels = np.random.randint(low=0, high=255, size=(600,600,3), dtype=np.uint8)\r\n        img = Image.fromarray(random_pixels)\r\n        # Get the string of the image\r\n        image_bytes = BytesIO()\r\n        img.save(image_bytes, format=\"PNG\")\r\n        # Convert image and a random output value for the image to a tf.train.Example\r\n        tf_example = image_and_output_to_example(image_bytes.getvalue(), random())\r\n        writer.write(tf_example.SerializeToString())\r\n\r\n# Load the TFRecord file as a dataset and use the map function to convert the Examples into training data\r\nraw_image_dataset = tf.data.TFRecordDataset(record_file)\r\ntrain_dataset = raw_image_dataset.map(parse_tf_example)\r\n\r\n# Returns the shape of the first element in a Dataset\r\ndef get_first_element_shape(dataset):\r\n    for thing in dataset:\r\n        if isinstance(thing, tuple):\r\n            return tuple(element.shape for element in thing)\r\n        else:\r\n            return thing.shape\r\n        \r\nprint('Original Dataset')\r\nprint(tf.compat.v1.data.get_output_shapes(train_dataset)) # in the code\r\nprint(tf.data.experimental.get_structure(train_dataset)) # function that replaces get_ouput_shapes (See TF issue #30774)\r\nprint(get_first_element_shape(train_dataset)) # My function to work around the None sized dimensions\r\n\r\nx = train_dataset.map(lambda a, b: a)\r\nprint('\\nX dataset (split out of input dataset by _process_xy in autokeras.automodel)')\r\nprint(tf.compat.v1.data.get_output_shapes(x)) # in the code\r\nprint(tf.data.experimental.get_structure(x)) # function that replaces get_ouput_shapes (See TF issue #30774)\r\nprint(get_first_element_shape(x)) # My function to work around the None sized dimensions\r\n```", "I have tried in colab with TF version 2.3, nightly version(`2.4.0-dev20200916`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c6e16d465e79b35a2cf19ffcfb9f24c3/untitled367.ipynb). Thanks!", "@BryceSchmitt Sorry for the late reply.\r\n\r\nThe issue arises because the shape of the tensors produced by parse_tf_example is unknown until the actual data is parsed. Shape inference happens before then (parsing the entire dataset to make sure every example is the same size would be too expensive). If you know the shape of your examples, you can use [tf.ensure_shape](https://www.tensorflow.org/api_docs/python/tf/ensure_shape) at the end of `parse_tf_example` so that the shapes will be known. That should resolve the error you're getting from AutoKeras.", "@aaudiber Thanks for the detailed response, that does fix the issue and allows AutoKeras to run.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43250\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43250\">No</a>\n"]}, {"number": 43249, "title": "Fix sanity build", "body": "", "comments": []}, {"number": 43248, "title": "Class weights issue with sparse data from tf.data.dataset where y.shape.rank is None", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: yes\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**: 2.2\r\n-   **Python version**: 3.6.7\r\n-   **CUDA/cuDNN version**: 10.2\r\n-   **GPU model and memory**: RTX 2070, 32\r\n-   **Exact command to reproduce**:\r\n\r\nSorry, this is my first time submitting an issue, this is a small issue with a simple fix but I'm not sure if what I added is a long-term solution. Unfortunately, I can't divulge the full code, but I can explain the steps I took to reproduce the error. I have a simple 3-layer deep model and a tf.dataset generate like so:\r\n\r\n```\r\n#model instantiated above\r\nmodel.compile(\r\n            loss = 'sparse_categorical_crossentropy',\r\n            metrics=['sparse_categorical_accuracy'],\r\n            optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)\r\n            )\r\n\r\nclass generator:\r\n    def __call__(self, file, batch_size): #file is a parquet file\r\n        actual_file = file.decode(\"utf-8\") \r\n        df = pd.read_parquet(actual_file)\r\n        df = df.astype(\"float16\")\r\n        df[df.columns[df.shape[1]-1]] = df[df.columns[df.shape[1]-1]].astype(int)\r\n        filefinish = False\r\n        fileIndex = 0\r\n        fileEnd = df.shape[0]\r\n        fileX = df.shape[1]\r\n        while not filefinish:\r\n            if fileIndex + batch_size >= fileEnd:\r\n                yield df.iloc[fileIndex:fileEnd, 0:fileX-1].values, df.iloc[fileIndex:fileEnd, fileX-1:fileX].values.reshape(fileEnd-fileIndex)\r\n                filefinish = True\r\n            else:\r\n                yield df.iloc[fileIndex:fileIndex+batch_size, 0:fileX-1].values, df.iloc[fileIndex:fileIndex+batch_size, fileX-1:fileX].values.reshape(batch_size)\r\n                fileIndex += batch_size\r\n\r\ntraining_files_dir = [LIST OF TRAINING FILES]\r\nBATCH_SIZE, EPOCHS = 4096, 10\r\n\r\ntraining_generator = tf.data.Dataset.from_tensor_slices(training_files_dir)\r\ntraining_generator = training_generator.interleave(lambda filename: tf.data.Dataset.from_generator(\r\n        generator(), \r\n        output_types=(tf.float16, tf.int8),\r\n        args=(filename, BATCH_SIZE,)), num_parallel_calls=tf.data.experimental.AUTOTUNE, cycle_length=2)\r\ntraining_generator = training_generator.repeat(EPOCHS)\r\n\r\nmodel.fit(x=training_generator, epochs=EPOCHS, steps_per_epoch=EPOCH_STEPS, validation_steps=VALIDATION_STEPS, validation_data=validation_generator, class_weight=class_weights, callbacks=all_callbacks, verbose=1)\r\n```\r\n\r\n### Describe the problem\r\nUsing class weights on fit function and sparse data from tf.data produces the error below as y.shape.rank is None. I added an if statement to data_adapter.py to fix it, but I'm not sure if this is a sustainable fix to leave in for the future.\r\n\r\n```\r\nif y.shape.rank != None:\r\n        if y.shape.rank > 2:\r\n          raise ValueError(\"`class_weight` not supported for \"\r\n                                                  \"3+ dimensional targets.\")\r\n```\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"c:/Users/Harry Wang/Desktop/fnma-deep-mortage-risk/fnma_dnn/main.py\", line 448, in <module>\r\n    model._model.fit(x=training_generator, epochs=EPOCHS, steps_per_epoch=EPOCH_STEPS, validation_steps=VALIDATION_STEPS, validation_data=validation_generator, class_weight=class_weights, callbacks=all_callbacks, verbose=1)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 66, in _method_wrapper \r\n    return method(self, *args, **kwargs)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 815, in fit\r\n    model=self)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\", line 1117, in __init__  \r\n    dataset = dataset.map(_make_class_weight_map_fn(class_weight))\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1621, in map\r\n    return MapDataset(self, map_func, preserve_cardinality=True)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3981, in __init__\r\n    use_legacy_function=use_legacy_function)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3221, in __init__       \r\n    self._function = wrapper_fn.get_concrete_function()\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2532, in get_concrete_function\r\n    *args, **kwargs)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2496, in _get_concrete_function_garbage_collected\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2777, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2667, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 981, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3214, in wrapper_fn\r\n    ret = _wrapper_helper(*args)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 3156, in _wrapper_helper\r\n    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 262, in wrapper\r\n    return converted_call(f, args, kwargs, options=options)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 492, in converted_call\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 346, in _call_unconverted\r\n    return f(*args, **kwargs)\r\n  File \"C:\\Users\\Harry Wang\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\", line 1248, in _class_weights_map_fn\r\n    if y.shape.rank > 2:\r\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\r\n```", "comments": ["Check https://github.com/keras-team/keras/issues/3653#issuecomment-243939748", "My apologies, thanks for the link! Out of curiosity, would there be any way to improve how sample weights could be implemented? Adding weights with each input seems to be a little repetitive with large datasets with large numbers of classes.", "> Out of curiosity, would there be any way to improve how sample weights could be implemented? Adding weights with each input seems to be a little repetitive with large datasets with large numbers of classes.\r\n\r\nThis type of support requests are for our Stackoverflow: https://stackoverflow.com/questions/tagged/tensorflow", "I encountered the same issue. \r\nI think the root cause comes from not specifying the output_shapes during from_generator() so that during class weighting, y.shape.rank returns None."]}, {"number": 43247, "title": "Feature request: MLIR-based TFLite converter support for 16bit conv2d", "body": "### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1\r\n-   **TensorFlow installed from (source or binary)**: source \r\n-   **TensorFlow version (use command below)**: 2.4.0, with git hash 4a896124e344adcf94e30ed335b59900b578e53e\r\n-   **Python version**: 3.6.9\r\n-   **Bazel version (if compiling from source)**: 3.1.0\r\n\r\n### Describe the problem\r\nI have tf.quantization.fake_quant_with_min_max_args+ tf.conv2d, and use tf.lite.TFLiteConverter to convert it into tflite quantized operator. This works for me if I set both input and weight to 8 bits. But the same process doesn't work for 16 bit activation.\r\n\r\nI realize I could optionally use the old and soon to be deprecated TOCO converter (by setting converter.experimental_new_converter=False and converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]). This does generate tflite but that's not well legalized into one tfl.conv2d with native tfl.qint16 input and output tensors.\r\n", "comments": ["I think MLIR converter is able to support the following op set: EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8.\r\n\r\nCould you provide simple reproducible steps that show the above option is not working?", "Hi,\r\n\r\nHere's how you reproduce it:\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass TensorScale:\r\n    def __init__(self, _min, _max, _num_bits, _narrow_range):\r\n        self.min = _min\r\n        self.max = _max\r\n        self.num_bits = _num_bits\r\n        self.narrow_range = _narrow_range\r\n\r\ndef build_fakequant_conv2d(input, weight, input_scale, weight_scale, output_scale, strides, padding, dilations):\r\n    ifm_quant = tf.quantization.fake_quant_with_min_max_args(input, min=input_scale.min, max=input_scale.max, num_bits=input_scale.num_bits, narrow_range=input_scale.narrow_range, name='fake_quant_input')\r\n    weight_quant = tf.quantization.fake_quant_with_min_max_args(weight, min=weight_scale.min, max=weight_scale.max, num_bits=weight_scale.num_bits, narrow_range=weight_scale.narrow_range, name='fake_quant_weight')\r\n\r\n    conv = tf.nn.conv2d(ifm_quant, weight_quant, strides, padding, data_format='NHWC', dilations=dilations, name='conv')\r\n    return tf.quantization.fake_quant_with_min_max_args(conv, min=output_scale.min, max=output_scale.max, num_bits=output_scale.num_bits, narrow_range=output_scale.narrow_range, name='fake_quant_output')\r\n\r\ndef main():\r\n\r\n    # randomly pick a rng\r\n    rng = np.random.default_rng(42)\r\n\r\n    tf.keras.backend.clear_session()\r\n    sess = tf.compat.v1.Session()\r\n    with sess.graph.as_default():\r\n        tf.compat.v1.keras.backend.set_session(sess)\r\n\r\n        # 1. create unit test model\r\n        # input [1, 4, 4, 2] x weight [3, 3, 2, 4] -> output [1, 4, 4, 4]\r\n        # padding='SAME', strides=[1, 1, 1, 1], dilations=[1, 1, 1, 1]\r\n        input_shape = [1, 4, 4, 2]\r\n        weight_shape = [3, 3, 2, 4]\r\n        padding = 'SAME'\r\n        strides = [1, 1, 1, 1]\r\n        dilations = [1, 1, 1, 1]\r\n\r\n        # create input placeholder\r\n        input = tf.compat.v1.placeholder(dtype=tf.float32, shape=input_shape, name='input')\r\n\r\n        # create weight const, with range [-2.0 to 2.0]\r\n        weight_val = np.float32((rng.random(size=weight_shape) - 0.5) * 4.0)\r\n        weight = tf.compat.v1.constant(weight_val, shape=weight_shape, dtype=tf.float32, name='weight')\r\n\r\n\r\n        input_scale = TensorScale(0.0, 6.0, 16, False)\r\n        weight_scale = TensorScale(-2.0, 2.0, 8, False)\r\n        output_scale = TensorScale(-10.0, 10.0, 16, False)\r\n\r\n        fakequant_conv2d_op = build_fakequant_conv2d(input, weight, input_scale, weight_scale, output_scale, strides, padding, dilations)\r\n\r\n        # 2. validate this graph works in tensorflow\r\n        input_sample = np.float32(rng.random(size=input_shape) * 6.0)\r\n        result = sess.run(fakequant_conv2d_op, feed_dict={input.name : input_sample})\r\n        print(result.shape) # (1, 4, 4, 4)\r\n\r\n        # 3. convert to tflite\r\n        converter = tf.compat.v1.lite.TFLiteConverter.from_session(sess, [input], [fakequant_conv2d_op])\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n        converter.experimental_new_converter = True\r\n        converter.inference_input_type = tf.int16\r\n        converter.inference_output_type = tf.int16\r\n\r\n        in_scale = (input_scale.max - input_scale.min) / float(np.iinfo(np.int16).max - np.iinfo(np.int16).min)\r\n        in_zp = int(round((-input_scale.min) / in_scale)) + np.iinfo(np.int16).min\r\n\r\n        converter.input_stats_list = {input.name.split(':')[0]: (in_scale, in_zp)}\r\n\r\n        tflite_model = converter.convert()\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\nThis will die at converter.convert() with message like:\r\n\r\n> tensorflow.lite.python.convert.ConverterError: /home/kevche01/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:777:0: error: Failed to convert element type '!quant.uniform<u16:f32, 9.1554131380178531E-5>': 'isSigned' can only be set for 8-bits integer type\r\n\r\nIf I set converter.experimental_new_converter to False to translate old TOCO converter, converter.convert() won't die (takes way longer than MLIR-based converter), but it's not generating tfl.conv2d() with qi16 input and output as expected.\r\n\r\nI confess even though I'm building v2.4 source (since there's other features in compiler I would like to have) but I'm still using v1 way to generate the graph. I haven't give v2 converter a try yet since v1 converter works smoothly with input.dtype=tf.int8 or tf.uint8. But if that's indeed the reason, I'll try to take them in.", "I am able to replicate the issue please find the [gist here](https://colab.research.google.com/gist/Saduf2019/d64953dbd497cac1579749934a305691/untitled419.ipynb).", "Code is throwing an error even with `tf-nightly`. [Here](https://colab.research.google.com/gist/jvishnuvardhan/f12236e7a74e6d0aa336790c771a5929/untitled419.ipynb) is a gist for our reference. Thanks!\r\n\r\n```\r\n(1, 4, 4, 4)\r\n---------------------------------------------------------------------------\r\nException                                 Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198                                                  debug_info_str,\r\n--> 199                                                  enable_mlir_converter)\r\n    200       return model_str\r\n\r\n6 frames\r\nException: /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:750:0: error: Failed to convert element type '!quant.uniform<u16:f32, 9.1554131380178531E-5>': 'isSigned' can only be set for 8-bits integer type\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:2615:0: note: called from\r\n```", "Hi, I recently revisit this issue, and found this issue doesn't exist if I rewrite everything with v2 Python API, instead of using tf.session and tf.compat.v1.lite.TFLiteConverter.\r\n\r\nHere's how I reproduce it:\r\n\r\n```\r\n#!/usr/bin/env python3\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass TensorScale:\r\n    def __init__(self, _min, _max, _num_bits, _narrow_range):\r\n        self.min = _min\r\n        self.max = _max\r\n        self.num_bits = _num_bits\r\n        self.narrow_range = _narrow_range\r\n\r\nclass tf_v2_conv2d(tf.Module):\r\n    def __init__(self, _input_scale, _weight_scale, _output_scale, _weight, _strides, _padding, _data_format, _dilations):\r\n        self.weight = _weight\r\n        self.strides = _strides\r\n        self.padding = _padding\r\n        self.data_format = _data_format\r\n        self.dilations = _dilations\r\n        self.input_scale = _input_scale\r\n        self.weight_scale = _weight_scale\r\n        self.output_scale = _output_scale\r\n\r\n    @tf.function(input_signature=(tf.TensorSpec(shape=[1, 4, 4, 2], dtype=tf.float32),))\r\n    def __call__(self, x):\r\n        return tf.nn.conv2d(x, self.weight, self.strides, self.padding, data_format=self.data_format, dilations=self.dilations, name='conv')\r\n\r\ndef test_tflite_converter_v2_conv2d_16x8():\r\n    # randomly pick a rng\r\n    rng = np.random.default_rng(42)\r\n\r\n    # 1. create unit test model\r\n    # input [1, 4, 4, 2] x weight [3, 3, 2, 4] -> output [1, 4, 4, 4]\r\n    # padding='SAME', strides=[1, 1, 1, 1], dilations=[1, 1, 1, 1]\r\n    input_shape = [1, 4, 4, 2]\r\n    weight_shape = [3, 3, 2, 4]\r\n    padding = 'SAME'\r\n    strides = [1, 1, 1, 1]\r\n    dilations = [1, 1, 1, 1]\r\n\r\n    # create weight const, with range [-2.0 to 2.0]\r\n    weight_val = np.float32((rng.random(size=weight_shape) - 0.5) * 4.0)\r\n    weight = tf.constant(weight_val, shape=weight_shape, dtype=tf.float32, name='weight')\r\n\r\n    input_scale = TensorScale(0.0, 6.0, 16, False)\r\n    weight_scale = TensorScale(-2.0, 2.0, 8, False)\r\n    output_scale = TensorScale(-10.0, 10.0, 16, False)\r\n\r\n    model = tf_v2_conv2d(input_scale, weight_scale, output_scale, weight, strides, padding, \"NHWC\", dilations)\r\n    # (ro run your model) result = Squared(5.0) # This prints \"25.0\"\r\n    # (to generate a SavedModel) tf.saved_model.save(model, \"saved_model_tf_dir\")\r\n    concrete_func = model.__call__.get_concrete_function()\r\n\r\n    # Convert the model\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.experimental_new_converter = True\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8\r\n    ]\r\n    def input_stats():\r\n        for i in range(0, 3):\r\n            yield [np.float32((rng.random(size=input_shape) - 0.5) * 4.0)]\r\n    converter.representative_dataset = input_stats\r\n    converter.inference_input_type = tf.int16\r\n    converter.inference_output_type = tf.int16\r\n\r\n    tflite_model = converter.convert()\r\n\r\n    # Save the model.\r\n    with open('model.tflite', 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\nif __name__ == \"__main__\":\r\n    test_tflite_converter_v2_conv2d_16x8()\r\n```\r\n\r\nThis does generate 16x8 conv2d with the mlir (translated with flatbuffer_translate --tflite-flatbuffer-to-mlir) as:\r\n```\r\nmodule attributes {tfl.description = \"MLIR Converted.\", tfl.schema_version = 3 : i32}  {\r\n  func @main(%arg0: tensor<1x4x4x2x!quant.uniform<i16:f32, 5.8398745750309899E-5>>) -> tensor<1x4x4x4x!quant.uniform<i16:f32, 3.92310437746346E-4>> attributes {tf.entry_function = {inputs = \"x_int16\", outputs = \"Identity_int16\"}} {\r\n    %0 = \"tfl.pseudo_qconst\"() {qtype = tensor<4x3x3x2x!quant.uniform<i8<-127:127>:f32:0, {0.014777391217648983,0.014980231411755085,0.014825134538114071,0.015516148880124092}>>, value = dense<[[[[74, -110], [-101, 39], [15, 70]], [[75, -94], [-47, -100], [-17, 90]], [[49, 78], [-11, 46], [36, -127]]], [[[-16, 127], [-13, 86], [-116, -39]], [[-82, 49], [-35, -6], [89, 81]], [[-96, 44], [18, -8], [14, -17]]], [[[97, 70], [-35, -15], [88, 127]], [[-9, 66], [-8, -74], [54, -30]], [[-81, 55], [-97, 18], [16, -77]]], [[[51, 74], [110, -70], [34, 101]], [[-118, 121], [-80, 44], [-48, -55]], [[-127, 72], [-99, 68], [-51, -24]]]]> : tensor<4x3x3x2xi8>} : () -> tensor<4x3x3x2x!quant.uniform<i8<-127:127>:f32:0, {0.014777391217648983,0.014980231411755085,0.014825134538114071,0.015516148880124092}>>\r\n    %1 = \"tfl.pseudo_qconst\"() {qtype = tensor<4x!quant.uniform<i64:f32:0, {8.629810963611817E-7,8.7482675326100434E-7,8.6576926605630433E-7,9.0612365966080687E-7}>>, value = dense<0> : tensor<4xi64>} : () -> tensor<4x!quant.uniform<i64:f32:0, {8.629810963611817E-7,8.7482675326100434E-7,8.6576926605630433E-7,9.0612365966080687E-7}>>\r\n    %2 = \"tfl.conv_2d\"(%arg0, %0, %1) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = \"NONE\", padding = \"SAME\", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x4x4x2x!quant.uniform<i16:f32, 5.8398745750309899E-5>>, tensor<4x3x3x2x!quant.uniform<i8<-127:127>:f32:0, {0.014777391217648983,0.014980231411755085,0.014825134538114071,0.015516148880124092}>>, tensor<4x!quant.uniform<i64:f32:0, {8.629810963611817E-7,8.7482675326100434E-7,8.6576926605630433E-7,9.0612365966080687E-7}>>) -> tensor<1x4x4x4x!quant.uniform<i16:f32, 3.92310437746346E-4>>\r\n    return %2 : tensor<1x4x4x4x!quant.uniform<i16:f32, 3.92310437746346E-4>>\r\n  }\r\n}\r\n```\r\n\r\nSo I guess the bug is in v1 Python API wrapper. Seems like https://github.com/tensorflow/tensorflow/blob/dac302eeed30cc234fc1d1ea887626ad88902175/tensorflow/lite/python/lite.py#L1236 only handle tf.int8/tf.uint8, as the original problem in this ticket indicates.\r\n\r\nWill close the issue since this issue doesn't exist in v2 Python API.\r\nThanks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43247\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43247\">No</a>\n"]}, {"number": 43246, "title": "Disable `control_flow_ops_test` test", "body": "Since `absl_py` update to 0.10.0 on Aug 19th, this test would break. We disable the test instead of upper bounding the dependency since the failure is only in test code", "comments": []}, {"number": 43245, "title": "when i am running the sample tutorial code of basic text classification i just cannot download a file named imdb_word_index.json  ", "body": "when Downloading the url \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\" the program run into the method of \"self._sslobj.do_handshake()\" in file of ssl.py and raise an exception of \"TimeoutError: [WinError 10060]\" . yet i dont know how to solve this problem ,so please help me to out ,thanks ", "comments": ["Can you try:\r\n```\r\nimport tensorflow as tf\r\nindex = tf.keras.datasets.imdb.get_word_index(\r\n    path='imdb_word_index.json'\r\n)\r\nprint(index)\r\n```", "@bhack I tried ,but it still failed to download that file,here is the error msg below:\r\nException: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json: None -- [WinError 10060] \r\n", "What Is your TF version?", "@bhack version 2.3.0", "Do you have a proxy?", "@bhack  i dont use a proxy ,i can connect to the internet normally ,when i use my chrome web browser it seems okay to get that file,but in python code it cannot ,i dont know what went wrong, ", "Can you check if you have the same errore with a code snippet without TF.\n\n```\nimport urllib.request\nurl = '<your_url_here>' \nresponse = urllib.request.urlopen(url)\ndata = response.read()\n```", "@bhack yes, i meet the same error, what am i supposed do next ", "Using this code, do you have problems putting another URL as well?", "@bhack no ,i v changed another url but it runs well i can get the response text but use that url address i posted above not", "What Is your python version? ", "@bhack  python version is Python 3.6.6", "If you can I suggest you to give a try with an updated python version (64bit).", "@bhack i will try it, thanks anyway", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43245\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43245\">No</a>\n"]}, {"number": 43244, "title": "I am not able to install tensorflow", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): command prompt\r\n- TensorFlow version: 2.0\r\n- Python version:3.7.3\r\n- Installed using virtualenv? pip? conda?: pip\r\n\r\n**Describe the problem**\r\nI am unable to install tensorflow from my command prompt using pip\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\npip install tensorflow==2.2.0rc4\r\npip install tensorflow\r\n\r\n **ERROR**\r\n`ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0rc4 (from versions: none)\r\nERROR: No matching distribution found for tensorflow==2.2.0rc4`", "comments": ["Is your Python on 32 bits? What is the output of `pip debug verbose`?", "My python is on 64 bits and output for that command is \r\n```\r\npip version: pip 20.2.3 from c:\\users\\naman\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pip (python 3.7)\r\nsys.version: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 21:26:53) [MSC v.1916 32 bit (Intel)]\r\nsys.executable: c:\\users\\naman\\appdata\\local\\programs\\python\\python37-32\\python.exe\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: cp1252\r\nsys.platform: win32\r\nsys.implementation:\r\n  name: cpython\r\n'cert' config value: Not specified\r\nREQUESTS_CA_BUNDLE: None\r\nCURL_CA_BUNDLE: None\r\npip._vendor.certifi.where(): c:\\users\\naman\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pip\\_vendor\\certifi\\cacert.pem\r\npip._vendor.DEBUNDLED: False\r\nvendored library versions:\r\n  appdirs==1.4.4\r\n  CacheControl==0.12.6\r\n  colorama==0.4.3\r\n  contextlib2==0.6.0.post1 (Unable to locate actual module version, using vendor.txt specified version)\r\n  distlib==0.3.1\r\n  distro==1.5.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  html5lib==1.1\r\n  ipaddress==1.0.23\r\n  msgpack==1.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  packaging==20.4\r\n  pep517==0.8.2\r\n  progress==1.5\r\n  pyparsing==2.4.7\r\n  requests==2.24.0\r\n  certifi==2020.06.20\r\n  chardet==3.0.4\r\n  idna==2.10\r\n  urllib3==1.25.9\r\n  resolvelib==0.4.0\r\n  retrying==1.3.3 (Unable to locate actual module version, using vendor.txt specified version)\r\n  setuptools==44.0.0 (Unable to locate actual module version, using vendor.txt specified version)\r\n  six==1.15.0\r\n  toml==0.10.1\r\n  webencodings==0.5.1 (Unable to locate actual module version, using vendor.txt specified version)\r\nCompatible tags: 27\r\n  cp37-cp37m-win32\r\n  cp37-abi3-win32\r\n  cp37-none-win32\r\n  cp36-abi3-win32\r\n  cp35-abi3-win32\r\n  cp34-abi3-win32\r\n  cp33-abi3-win32\r\n  cp32-abi3-win32\r\n  py37-none-win32\r\n  py3-none-win32\r\n  py36-none-win32\r\n  py35-none-win32\r\n  py34-none-win32\r\n  py33-none-win32\r\n  py32-none-win32\r\n  py31-none-win32\r\n  py30-none-win32\r\n  cp37-none-any\r\n  py37-none-any\r\n  py3-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n```", "`sys.platform: win32` and all the `-win32` tags mean you Python is on 32 bits", "Thanks for the help I have upgraded my python to 64 bit\r\nand now I am able to install tensorflow", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43244\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43244\">No</a>\n"]}, {"number": 43243, "title": "Broadcasting not working for divide and divide_no_nan", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary):pip binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: No GPU\r\n- GPU model and memory: No GPU\r\n\r\n\r\n**Describe the current behavior**\r\nBroadcasting not working for divide and divide_no_nan ops. I have 2 tensors, of shape [5,140,280,3,2] and [5,140,280,3]. Attempting to divide with tf.math.divide_no_nan(A, B) yields the following error: \r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [5,140,280,3,2] vs. [5,140,280,3] [Op:DivNoNan]`\r\nI have also tested the regular tf.math.divide() op and it yields the same error.\r\n**Describe the expected behavior**\r\nWith broadcast behavior I would expect to be able to easily divide A by B, with B being broadcast to fit the innermost dimension of A.\r\n**Standalone code to reproduce the issue**\r\na = tf.ones([5,140,280,3,2])\r\nb = tf.ones([5,140,280,3])\r\nc = tf.math.divide_no_nan(a, b)\r\n**Other info / logs** None", "comments": ["Apologies, I missed that broadcasting requires matching inner dimensions. Using expand_dims did the tricks", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43243\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43243\">No</a>\n"]}, {"number": 43242, "title": "Model Input & Output does not match. ", "body": "**System information**\r\nHello everyone,\r\nI am working on a project to implement a prediction algorithm, which is to be implemented for a microcontroller. \r\nTherefore, I have not installed the complete library, instead I have downloaded the most recent repository. Then I built the c++ project on \"S32 Design Studio\" and run it on the laptop with Windows 10(haven't started to compile on microcontroller yet)\r\nI have used the tf-nightly : \r\n```\r\n!pip install tf-nightly\r\nimport tensorflow.compat.v2 as tf\r\ntf.enable_v2_behavior()\r\n```\r\n**the tf keras model which was as below: **\r\n```\r\nn_epoch = 1\r\nn_batch = 1\r\nn_neurons= 25\r\ntime_ev = 6\r\n\r\n# design network\r\nmodel = Sequential()\r\nmodel.add(tf.keras.layers.Input(batch_input_shape=(n_batch,time_ev, 3), name='input')) # nbatch, num_feature, time\r\nmodel.add(tf.keras.layers.Flatten())\r\nmodel.add(Dense(1, activation=tf.keras.activations.relu))\r\nmodel.compile(loss='mean_squared_error', optimizer='adam')\r\n```\r\nand I have converted it as below:\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\n```\r\n```\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2289: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  warnings.warn('`Model.state_updates` will be removed in a future version. '\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1376: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  warnings.warn('`layer.updates` will be removed in a future version. '\r\nINFO:tensorflow:Assets written to: /tmp/tmpztyddjcu/assets\r\nINFO:tensorflow:Assets written to: /tmp/tmpztyddjcu/assets\r\n```\r\nThen I have downloaded the model as a .cc file because I could not use the function to read .tflite file: \r\n\r\n```\r\n# Define paths to model files\r\nimport os\r\n\r\nMODELS_DIR = 'models/'\r\nif not os.path.exists(MODELS_DIR):\r\n    os.mkdir(MODELS_DIR)\r\nMODEL_TF = MODELS_DIR + 'model.pb'\r\nMODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'\r\nMODEL_TFLITE = MODELS_DIR + 'model.tflite'\r\nMODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'\r\n\r\n# # Save the model to disk\r\nopen(MODEL_NO_QUANT_TFLITE, \"wb\").write(tflite_model)\r\n\r\n# Install xxd if it is not available\r\n!apt-get update && apt-get -qq install xxd\r\n# Convert to a C source file\r\n!xxd -i {MODEL_NO_QUANT_TFLITE} > {MODEL_TFLITE_MICRO}\r\n# Update variable names\r\nREPLACE_TEXT = MODEL_NO_QUANT_TFLITE.replace('/', '_').replace('.', '_')\r\n!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}\r\n\r\nfiles.download(\"models/model.cc\") \r\n```\r\n\r\nFinally I have this small script just to import the model and build the interpreter:\r\n\r\n```\r\n\r\n#include <string.h>\r\n\r\n#include \"model.h\"\r\n\r\n#include \"../../tensorflow-lite/tensorflow/lite/micro/all_ops_resolver.h\"\r\n#include \"../../tensorflow-lite/tensorflow/lite/micro/kernels/micro_ops.h\"\r\n#include \"../../tensorflow-lite/tensorflow/lite/micro/micro_error_reporter.h\"\r\n#include \"../../tensorflow-lite/tensorflow/lite/micro/micro_interpreter.h\"\r\n#include \"../../tensorflow-lite/tensorflow/lite/micro/micro_mutable_op_resolver.h\"\r\n#include \"../../tensorflow-lite/tensorflow/lite/micro/micro_optional_debug_tools.h\"\r\n#include \"../../tensorflow-lite/tensorflow/lite/version.h\"\r\n\r\n// Globals, used for compatibility with Arduino-style sketches.\r\nnamespace {\r\ntflite::ErrorReporter* error_reporter;\r\n//const tflite::Model* model ;\r\ntflite::MicroInterpreter* interpreter = nullptr;\r\n\r\n// Create an area of memory to use for input, output, and intermediate arrays.\r\n// Minimum arena size, at the time of writing. After allocating tensors\r\n// you can retrieve this value by invoking interpreter.arena_used_bytes().\r\nconstexpr size_t allocator_buffer_size = 2096 /* optimal arena size at the time of writting. */\r\n\t\t\t+ 16 /* alignment */ + 100 /* some headroom */;\r\n\r\nuint8_t allocator_buffer[allocator_buffer_size];\r\n} // namespace\r\n\r\nint main() {\r\n\r\n\tconst tflite::Model* model = ::tflite::GetModel(g_model);\r\n\r\n\tif (model->version() == TFLITE_SCHEMA_VERSION) {\r\n\t\tputs(\"Model provided is supported.\\n\");\r\n\t\t//fprintf(stderr, \"Model provided is supported.\\n\");\r\n\r\n\t}\r\n\r\n\ttflite::MicroErrorReporter micro_error_reporter;\r\n\terror_reporter = &micro_error_reporter;\r\n\tstatic tflite::AllOpsResolver resolver;\r\n        static tflite::MicroInterpreter static_interpreter(model, resolver, allocator_buffer, allocator_buffer_size, error_reporter);\r\n        interpreter = &static_interpreter;\r\n\tif (interpreter != nullptr) {\r\n\t\tusing namespace std;\r\n\t\tputs(\"interpreter is not empty\\n\");\r\n\t}\r\n\r\n    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);\r\n    printf(\"\\n\\n=== Post-invoke Interpreter State ===\\n\");\r\n    tflite::PrintInterpreterState(interpreter);\r\n\r\n\treturn 0;\r\n\r\n}\r\n\r\n```\r\n\r\nWhen I run the code, I obtain this in the console : \r\n\r\n```\r\nModel provided is supported.\r\n\r\ninterpreter is not empty\r\n\r\n\r\n=== Post-invoke Interpreter State ===\r\nInterpreter has 0 tensors and 2 nodes\r\nInputs: 0\r\nOutputs: 5\r\n\r\n\r\nNode   0 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 0 2\r\n  Outputs: 4\r\nNode   1 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 4 3 1\r\n  Outputs: 5\r\n```\r\n\r\nIsn't this wrong? Input dimension should be (1,6,3) and output should be (1)\r\nI have tested it on python and the tflite_model works as expected,however, when I convert it to a matrix and import with \"model.cc\" is it deteriorated ? I could not set an input matrix with dimension (1,6,3) . \r\n\r\nThank you in advance.", "comments": ["@bertankursun I haven't tried running this, but I think the numbers you see here:\r\nNode   0 Operator Builtin Code  22 RESHAPE\r\n  Inputs: 0 2\r\n  Outputs: 4\r\nNode   1 Operator Builtin Code   9 FULLY_CONNECTED\r\n  Inputs: 4 3 1\r\n  Outputs: 5\r\n\r\nAren't dimensions, they are tensor indices - for node 0 the inputs are tensors 0 and 2 and the output is 4 and for node 1 the inputs are tensors 4,3 and 1 and the output goes to tensor 5.\r\n\r\n", "Closing the issue since its resolved. Feel free to reopen if necessary. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43242\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43242\">No</a>\n"]}, {"number": 43241, "title": "updated documentation for tensorflow lite micro-speech example on NXP\u2026", "body": "\u2026-k66f board. Documentation now explains which versions of Python to use with mbed-cli (step 5). It omits an erroneous step that would cause the compilation stage to fail (previously step 6) by changing compiler flags from c++14 to c++11 for Mbed OS code which resulted in issues that prevented the example from running as code uses things like std::enable_if_t which is a c++14 feature used by Mbed OS. Also, added an extra step explaining DPALink if not already enabled on board. Fixes #43001", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43241) for more info**.\n\n<!-- need_sender_cla -->", "\r\n\r\n@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43241) for more info**.\n\n<!-- ok -->"]}, {"number": 43240, "title": "TFLu: Fix Ethos-U build errors", "body": "-Fix variable length array error in conv_test.\r\n-As for now, suppress -Wdouble-promotion in Ethos-U operator until it is\r\n resolved in Flatbuffers.", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "This is the flexbuffer error:\r\n18:25:16  tensorflow/lite/micro/tools/make/downloads/flatbuffers/include/flatbuffers/flexbuffers.h:1420:25: error: implicit conversion increases floating-point precision: 'float' to 'double' [-Werror,-Wdouble-promotion]\r\n18:25:16      Value(float f) : f_(f), type_(FBT_FLOAT), min_bit_width_(BIT_WIDTH_32) {}\r\n18:25:16                         ~^\r\n18:25:16  1 error generated.", "The flexbuffers error has been fixed by this PR: https://github.com/google/flatbuffers/commit/dca12522a9f9e37f126ab925fd385c807ab4f84e So I will undo the change in tensorflow/lite/micro/kernels/ethos-u/ethosu.cc", "The existing code doesn't really follow the style guide and is worth fixing, IMO.\r\n\r\nMy hope is that the following two PRs will fix your issue:\r\n * I have created #43858 to get the code to conform to the style guide and hopefully also fix the issue with VLA.\r\n * And @njeffrie has separated out the flatbuffer upgrade to its own PR #43856 \r\n", "OK, good. Let's close this PR then."]}, {"number": 43239, "title": "Model conversion fails with cryptic error when feeding sample during post-training quantization", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (or github SHA if from source): Nightly 2.4.0-dev20200914\r\n\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport cv2\r\n\r\ndef representative_dataset_gen(input_image_shape, num_samples_to_generate=100): # dummy generator\r\n  h, w, *_ = input_image_shape\r\n  for _ in range(num_samples_to_generate):\r\n    im = np.random.random([w,h,3])\r\n    yield [im.astype(np.float32)]\r\n\r\n\r\n# Convert the model\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('saved_model') # from the attached zip file\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\r\n                                       tf.lite.OpsSet.SELECT_TF_OPS]\r\nconverter.inference_input_type = tf.int8\r\n#converter.inference_output_type = tf.int8\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = lambda : representative_dataset_gen([256, 1024, 3], 5)\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```\r\n2020-09-15 15:51:43.315246: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.\r\n2020-09-15 15:51:43.315283: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.\r\n2020-09-15 15:51:43.315515: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: models/runs/mobilenet_v2_kitti_1024x256/tflite/saved_model\r\n2020-09-15 15:51:43.373628: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }\r\n2020-09-15 15:51:43.373665: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: models/runs/mobilenet_v2_kitti_1024x256/tflite/saved_model\r\n2020-09-15 15:51:43.373707: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2020-09-15 15:51:43.560665: I tensorflow/cc/saved_model/loader.cc:190] Restoring SavedModel bundle.\r\n2020-09-15 15:51:44.001062: I tensorflow/cc/saved_model/loader.cc:174] Running initialization op on SavedModel bundle at path: models/runs/mobilenet_v2_kitti_1024x256/tflite/saved_model\r\n2020-09-15 15:51:44.187305: I tensorflow/cc/saved_model/loader.cc:261] SavedModel load for tags { serve }; Status: success: OK. Took 871790 microseconds.\r\nsee current operation: %126 = \"tfl.concatenation\"(%125#0, %125#1, %125#2, %125#3) {axis = 1 : i32, fused_activation_function = \"NONE\"} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>\r\n2020-09-15 15:51:46.541754: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %126 = \"tfl.concatenation\"(%125#0, %125#1, %125#2, %125#3) {axis = 1 : i32, fused_activation_function = \"NONE\"} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>\r\n\r\nsee current operation: %129 = \"tfl.concatenation\"(%128#0, %128#1, %128#2, %128#3) {axis = 1 : i32, fused_activation_function = \"NONE\"} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>\r\n2020-09-15 15:51:46.541786: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %129 = \"tfl.concatenation\"(%128#0, %128#1, %128#2, %128#3) {axis = 1 : i32, fused_activation_function = \"NONE\"} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>\r\n\r\nsee current operation: %140 = \"tfl.concatenation\"(%133, %137, %135, %139) {axis = 1 : i32, fused_activation_function = \"NONE\"} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>\r\n2020-09-15 15:51:46.541823: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %140 = \"tfl.concatenation\"(%133, %137, %135, %139) {axis = 1 : i32, fused_activation_function = \"NONE\"} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>\r\n\r\n2020-09-15 15:51:46.618251: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\nINFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 143 nodes with 1 partitions.\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slr/anaconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 724, in convert\r\n    return super(TFLiteSavedModelConverterV2,\r\n  File \"/home/slr/anaconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 648, in convert\r\n    result = self._calibrate_quantize_model(result, **flags)\r\n  File \"/home/slr/anaconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\", line 474, in _calibrate_quantize_model\r\n    return calibrate_quantize.calibrate_and_quantize(\r\n  File \"/home/slr/anaconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 94, in calibrate_and_quantize\r\n    self._calibrator.FeedTensor(sample)\r\nIndexError: _Map_base::at\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n[saved_model.zip](https://github.com/tensorflow/tensorflow/files/5225443/saved_model.zip)\r\n\r\n**Failure details**\r\nWhen converting the model in FP32 mode, the model converts and works correctly.\r\nWhen attempting post-training-quantization, a cryptic error message is raised with no information regarding what exactly went wrong and making it impossible to debug any further.\r\n\r\n", "comments": ["@GPhilo,\r\nThe `saved_model.zip` file you have provided seems to be broken. I have tried extracting the file in multiple machines and it doesn't work. \r\n\r\nCould you please create another zip archive or try uploading it as a tar file? Thanks!", "@amahendrakar Of course! Here's a tar.xz file. GitHub \"doesn't support\" the file type, so I appended a \".zip\" extension. **Please remove the extra .zip extension after downloading.**\r\n[saved_model.tar.xz.zip](https://github.com/tensorflow/tensorflow/files/5231469/saved_model.tar.xz.zip)\r\n\r\nI hope this works, I just tested downloading ad opening it and on my machine it works, but then again, so did the previous one.", "Was able to reproduce the issue.\r\n\r\nOn running the code with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/07b7a59cfa02165a94fba2bda826ebcd/43239.ipynb), Colab notebook crashes. Whereas, running the code with [TF-nightly](https://colab.research.google.com/gist/amahendrakar/c913169ac053463ea790317940d24f70/43239-tf-nightly.ipynb#scrollTo=5Dj-ZTZDmzPr) throws an error stating `IndexError: _Map_base::at`. Please find the attached gist. Thanks!", "@GPhilo Looks like there may be some issue with `representative_dataset_gen` in the code.  When I tried to convert to simple tflite float model, it is converting successfully. Only when `representative_dataset_gen` enables, it is throwing an error. \r\n\r\nCan you please share model building code. If proprietary, then create a simple model with public data to demonstrate the issue. Thanks!", "@jvishnuvardhan Indeed, that was also the behaviour I reported in the issue description above.\r\nI prepared a [colab](https://colab.research.google.com/drive/17KeMdx43KrhLuHV2aTdxRDJE9LbcKr6j?usp=sharing) that builds an example model starting from a pretrained checkpoint ad runs every step I run, including the conversion to FP32 and INT8 at the end. The only difference to my model is the training, the rest is identical.", "Could you try again with tf-nightly? Recently, we have resolved some bugs related to the calibrator component.", "@abattery I ran the colab I linked above and indeed it ran through without raising the error anymore. I'll close this issue as it seems to be resolved, thanks!"]}, {"number": 43238, "title": "Interpreter AllocateTensors() does not finish or report error", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows\r\n- TensorFlow installed from (source or binary): Source - Arduino TensorFlowLite version 2.1.0-ALPHA(not precompiled)\r\n- Tensorflow version (commit SHA if source):\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): SparkFun Edge\r\n\r\n**Describe the problem**\r\n\r\nAllocating a model does not finish either with a error or success. Before I added the SOFTMAX and RESHAPE ops I received at least messages through the report serial at 9600. Now `Serial.print(\"Allocating Tensors ...\\n\");` is printing and nothing happens as if it `TfLiteStatus allocate_status = interpreter->AllocateTensors();` is stuck or an error message is not arriving. I use the biggest arena size I got via decreasing until the SRAM overflow error disappeared. The used model is included at the end. It was created from the [quantized version of mobilenet v1](http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_128_quant.tgz) via the notebook [here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb#scrollTo=HPSFmDL7pv2L).\r\n\r\nI am new to microcontrollers and ML.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n```\r\nnamespace {\r\n\ttflite::ErrorReporter* error_reporter = nullptr;\r\n\tconst tflite::Model* model = nullptr;\r\n\ttflite::MicroInterpreter* interpreter = nullptr;\r\n\tTfLiteTensor* input = nullptr;\r\n\r\n\t// An area of memory to use for input, output, and intermediate arrays.\r\n\tconstexpr int kTensorArenaSize = 270 * 1024;\r\n\tstatic uint8_t tensor_arena[kTensorArenaSize];\r\n} \r\n\r\nvoid setup() {\r\n\r\n\tSerial.begin(460800);\r\n\tdo {\r\n\t\tdelay(500);\r\n\t} while (!Serial);\r\n\r\n\tstatic tflite::MicroErrorReporter micro_error_reporter;\r\n\terror_reporter = &micro_error_reporter;\r\n\r\n\tSerial.print(\"Tensorflow setup...\\n\");\r\n\r\n\tSerial.print(\"Loading model ...\\n\");\r\n\r\n\tmodel = tflite::GetModel(g_object_detect_model_data);\r\n\tif (model->version() != TFLITE_SCHEMA_VERSION) {\r\n\t\tSerial.printf(\"Model provided is schema version %d not equal to supported version %d.\",\r\n\t\t\tmodel->version(), TFLITE_SCHEMA_VERSION);\r\n\t\treturn;\r\n\t}\r\n\r\n\tSerial.print(\"Model loaded ...\\n\");\r\n\r\n\tSerial.print(\"Building interpreter ...\\n\");\r\n\r\n\tstatic tflite::MicroMutableOpResolver<5> micro_op_resolver;\r\n\r\n\tmicro_op_resolver.AddBuiltin(\r\n\t\ttflite::BuiltinOperator_DEPTHWISE_CONV_2D,\r\n\t\ttflite::ops::micro::Register_DEPTHWISE_CONV_2D());\r\n\r\n\tmicro_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,\r\n\t\ttflite::ops::micro::Register_CONV_2D());\r\n\r\n\tmicro_op_resolver.AddBuiltin(tflite::BuiltinOperator_AVERAGE_POOL_2D,\r\n\t\ttflite::ops::micro::Register_AVERAGE_POOL_2D());\r\n\r\n\tmicro_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,\r\n\t\ttflite::ops::micro::Register_SOFTMAX());\r\n\r\n\tmicro_op_resolver.AddBuiltin(\r\n\t\ttflite::BuiltinOperator_RESHAPE,\r\n\t\ttflite::ops::micro::Register_RESHAPE());\r\n\r\n\tstatic tflite::MicroInterpreter static_interpreter(\r\n\t\tmodel, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);\r\n\tinterpreter = &static_interpreter;\r\n\r\n\tSerial.print(\"Interpreter built ...\\n\");\r\n\r\n\tSerial.print(\"Allocating Tensors ...\\n\");\r\n\r\n\tTfLiteStatus allocate_status = interpreter->AllocateTensors();\r\n\r\n\tif (allocate_status != kTfLiteOk) {\r\n\t\tSerial.print(\"AllocateTensors() failed\\n\");\r\n\t\treturn;\r\n\t}\r\n\r\n\tSerial.print(\"Tensors allocated ...\\n\");\r\n\r\n\tinput = interpreter->input(0);\r\n\r\n\tSerial.print(\"Tensorflow setup finished.\\n\");\r\n\r\n}\r\n```\r\n\r\n[model.zip](https://github.com/tensorflow/tensorflow/files/5224464/model.zip)\r\n", "comments": ["What is you Tensorflow version?", "Added it to the description: Arduino TensorFlowLite 2.1.0-ALPHA(not precompiled) is the library I have installed via the arduino library manager.", "Can you test with an updated version?", "![Unbenannt](https://user-images.githubusercontent.com/8292508/93205531-3fee8380-f758-11ea-804a-bb5d703acc9e.png)\r\nThese are the options given by the manager. But I can guess I can install them manually?", "I suppose a nightly build is available in the Manager but you can still generate it yourself:\r\nhttps://www.tensorflow.org/lite/microcontrollers/library#generate_the_arduino_library \r\n/cc @petewarden", "Following the guide you mentioned results in a tflite.zip which contains the same version as far as i can see. At least the library.properties tells so...although f.e. adding Ops to the resolver changed. But still I dont see any output after calling AllocateTensors().\r\n\r\nUPDATE: I have added the namespace including the arena size I used.", "Any updates on this issue?", "@PMatthaei  It looks like you are using an older Version of Tensorflow. Many bugs have been fixed in the latest version. Could you please execute your code using Latest  stable Version of TF 2.5 and let us know if the issue still persists? Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43238\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43238\">No</a>\n"]}, {"number": 43237, "title": "Integrate cuBLASLt API into backend", "body": "The cuBLASLt API provides more features and better control over the execution of GEMMs than the regular cuBLAS API.\r\n\r\nThis PR adds the following:\r\n\r\n- Build integration and stream executor wrappers for the cublasLt API.\r\n- Integration (with autotuning) into the batch_matmul_op_impl class (used by the BatchMatMul and Einsum ops).\r\n- Support for bias + ReLU fusion, which is expected to be useful for XLA in the future.\r\n\r\n(I note that this is a large PR and it may be easier to review the commits separately. Also the diff is a bit strange in places due to some minor refactoring I had to do to simplify the code).\r\n\r\ncc @reedwm @nluehr ", "comments": ["I am investigating the build issues.", "I think I've fixed all the build issues now.", "@benbarsdell  Can you please check @timshen91's comments and keep us posted ? Thanks!", "I finished reviewing stream_executor/*, and will take a look later on tensorflow/core.", "Hi @benbarsdell , as I try to land the PR, I found a correctness regression on the following test: `//tensorflow/python/kernel_tests/distributions:dirichlet_test_gpu`\r\n\r\nthe error message looks like this:\r\n```\r\n======================================================================\r\nFAIL: testCovarianceFromSampling (__main__.DirichletTest)\r\ntestCovarianceFromSampling (__main__.DirichletTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/dirichlet_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1196, in decorated\r\n    f(self, *args, **kwargs)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/dirichlet_test_gpu.runfiles/org_tensorflow/tensorflow/python/kernel_tests/distributions/dirichlet_test.py\", line 201, in testCovarianceFromSampling\r\n    self.assertAllClose(sample_stddev_, analytic_stddev, atol=0.02, rtol=0.)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/dirichlet_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 1236, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/dirichlet_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2698, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/dirichlet_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2658, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/distributions/dirichlet_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 2593, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/usr/local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 1528, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python3.6/site-packages/numpy/testing/_private/utils.py\", line 840, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0, atol=0.02\r\nMismatched value: a is different from b. \r\nnot close where = (array([0, 0, 0, 1, 1]), array([0, 1, 2, 0, 1]))\r\nnot close lhs = [0.05104237 0.06437208 0.06834181 0.06432743 0.0643561 ]\r\nnot close rhs = [0.14085905 0.17817417 0.18898225 0.17747666 0.17760938]\r\nnot close dif = [0.08981667 0.11380209 0.12064043 0.11314923 0.11325329]\r\nnot close tol = [0.02 0.02 0.02 0.02 0.02]\r\ndtype = float32, shape = (2, 3)\r\nMismatched elements: 5 / 6 (83.3%)\r\nMax absolute difference: 0.12064043\r\nMax relative difference: 0.6587895\r\n x: array([[0.051042, 0.064372, 0.068342],\r\n       [0.064327, 0.064356, 0.004876]], dtype=float32)\r\n y: array([[0.140859, 0.178174, 0.188982],\r\n       [0.177477, 0.177609, 0.014291]], dtype=float32)\r\n\r\n```\r\n\r\nThere are other failures that look like precision differences, which only needs to increase the test tolerance. But for this failure, the values are `0.05104237` vs `0.14085905`, which is too large to be a precision issue.\r\n\r\nThis was run on a P100. I can not reproduce with a Volta GPU.", "I have reproduced what looks like the same issue, on a Volta card. I'm not sure why the CI didn't fail.\r\n\r\nIt appears to be due to an issue in cublasLt and I am working on a workaround.", "Hi @benbarsdell , can you rebase this PR? I'm trying to run it with our internal benchmarks."]}, {"number": 43236, "title": "Tensorflow 2.2 and 2.3 not detecting GPU with CUDA 10.1", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.3 and 2.2\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: venv and pip\r\n- GCC/Compiler version (if compiling from source): 7.5\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: K80\r\n\r\n\r\n\r\n**Describe the problem**\r\nAfter installing tensorflow, GPU is not detected and getting error: 'Cannot open dynamic library **libcublas.so.10**'.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. All the steps are followed from the official tensorflow page as it is: https://www.tensorflow.org/install/gpu and https://www.tensorflow.org/install/pip.\r\n2. Also, i have to install cuda-toolkit separately.\r\n3. Finally added CUDA-10.1 path in bashrc file.\r\n\r\n**How I fix the problem**:\r\n\r\nI started with a clean VM on Azure with nothing installed. Then followed the tensorflow guides (above) to install NVIDIA-Driver, CUDA 10.1, cuDNN, cuda-toolkit and tensorflow.\r\n\r\nAfter all these steps, my local folder had two cuda folders (don't know why):\r\n/usr/local/cuda-10.1/lib64/\r\n/usr/localo/cuda-10.2/lib64/\r\n\r\n**The error which I was getting was for dynamic library 'libcublas.so.10'. And this file was not present in folder 'cuda-10.1', but instead it was present in 'cuda-10.2' (note, that i have installed everything in venv)**\r\n\r\n**I have to manually copy all the files (including files inside the 'stubs' folder). And then it works.**\r\n\r\nThis site also mention this issue, where they say that with CUDA 10.1, some of the libraries are installed differently - https://forums.developer.nvidia.com/t/cublas-for-10-1-is-missing/71015/4 (the steps here are when you install libraries at system level and not venv).\r\n\r\nExpected Behaviour:\r\nEither tensorflow should automatically refer to the missing dynamic libraries or mention how to fix this in Install Set up. \r\n\r\nNote: The errors are similar when you install CUDA 10.2, it's just the dynamic library version are different.", "comments": ["@javedsha \r\n\r\nTensorFlow v2.3 is compatible with CUDA 10.1 and cuDNN 7.6. For more information regarding this please take a look at the [tested build configurations](https://www.tensorflow.org/install/source#gpu).\r\n\r\nAnd the CUDA version mismatch query has been explained in this [StackOverflow comment](https://stackoverflow.com/questions/53422407/different-cuda-versions-shown-by-nvcc-and-nvidia-smi/53504578#53504578). \r\n\r\nCan you paste the output of nvida-smi?\r\nThanks!", "@ravikyram \r\n\r\nOutput of NVIDIA-SMI:\r\n\r\nDriver Version: 450.1.06\r\nCUDA Version: 11.0\r\n\r\nnvcc --version\r\n10.1\r\n\r\nI followed all the steps mentioned in the tensorflow gpu guide, the only thing extra I did was install cuda-toolkit 'sudo apt-get install cuda-toolkit'\r\n\r\nWhat am i doing wrong?", "@javedsha The following is a procedure I use for Ubuntu 18.04, confirmed to work with the Ubuntu-shipped python 3.6. Hope it helps to pinpoint your issue.\r\n\r\nIn your case, the trouble possibly started with the `sudo apt-get install cuda-toolkit`, as it's not fixed to 10.1. Having 10.1 parallel to 10.2 and 11.0 is not advisable, nor practically feasible due to the env vars.\r\n\r\nBtw, CUDA version that is reported by the nvidia-smi is not necessarily the CUDA version that Tensorflow picks up (longer story), but with my installation procedure it should report 10.1.\r\n\r\n```\r\n# To start fresh, clean up all the nivida-related packages. Be careful when using the same system as a desktop!\r\nsudo apt-get --purge remove 'cuda*'\r\nsudo apt-get --purge remove 'nvidia*'\r\nsudo apt-get --purge remove 'libnvidia*'\r\n\r\n# Check if all clean\r\nsudo find /usr/local/cuda/ -name '*blas*'\r\nsudo find /usr/lib/ -name '*blas*'\r\n\r\n# CUDA 10.1 instructions for creating a locally available repo and installing from it\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\r\nsudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\r\nwget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\r\nsudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub\r\nsudo apt-get update\r\n\r\n# Make sure the driver number matches the GPU. Also -440 would most likely work.\r\nsudo apt install nvidia-driver-418\r\nsudo apt install cuda-10.1\r\n\r\n# Make sure the libs are now in place\r\nsudo find /usr/local/cuda/ -name '*blas*'\r\nsudo find /usr/lib/ -name '*blas*'\r\n\r\n# Run nvidia-smi for sanity check\r\nnvidia-smi\r\n\r\npython3 -m venv ~/.venv-tf2.3-sanity\r\n. ~/.venv-tf2.3-sanity/bin/activate\r\npip install -U pip\r\npip install tensorflow==2.3\r\npython -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([10000, 10000])))\"\r\n```", "@ahtik is the local thing same as the stable version? I will give this a try today and will post here. Thank you.", "Yes, it's the latest 10.1 cuda, just makes the deployment a bit easier for \nmy use case.\n\nOn September 16, 2020 3:57:53 PM Javed Shaikh <notifications@github.com> wrote:\n>\n> @ahtik is the local thing same as the stable version? I will give this a \n> try today and will post here. Thank you.\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or unsubscribe.\n\n", "I have the same problem with libcublas.so.10. Same OS, same python version, tf 2.3 etc. The only difference was that I didn't use venv and have different GPU.\r\nI followed Ubutnu 18.04 instructions from official guide: https://www.tensorflow.org/install/gpu\r\nI also found cuda 10.2 folder near cuda 10.1 folder with former having libcublas.so.10 and latter having all other libs.\r\n\r\nMy solution was to install cuda 10.2 even if it contradicts the guide. GPU is working in tensorflow now. I have taken cuda-10.2 from nvidia website as a deb package.\r\n\r\nAlso the guide itself (https://www.tensorflow.org/install/gpu) seems to be not perfectly written. It tells you to install CUPTI when there is no way to install it separately. It read as: \"Install CUPTI which ships with the CUDA\u00ae Toolkit. Append its installation directory to the $LD_LIBRARY_PATH environmental variable:\" when it should be IMHO \"Install CUDA Toolkit. You will have CUPTI library installed. Append its installation directory to the $LD_LIBRARY_PATH environmental variable:\" And I still don't get how section with CUPTI goes before section with cuda installation on Ubuntu. I hope my feedback will be useful.", "@Zapunidi Indeed, the official guide for Ubuntu doesn't seem to work for me either (all other libs load fine, getting one Warning):\r\n```\r\n2020-09-17 12:50:29.307000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-09-17 12:50:29.307313: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\r\n2020-09-17 12:50:29.334711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-09-17 12:50:29.340930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-09-17 12:50:29.391160: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-09-17 12:50:29.400149: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\r\n2020-09-17 12:50:29.507706: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\r\n\r\n```\r\nAlmost like something in the NVIDIA machine-learning repo still manages to force an upgrade from 10.1..\r\n\r\nI do not have this warning when using the local installation method I posted previously. For cudnn and tensorrt/libnvinfer I have a separate tensorrt-cuda10.1 setup.", "So the issue is for everyone. It will make sense to upgrade the tensorflow documentation as it is not working.\r\n@Zapunidi did you follow the same steps as mentioned in https://www.tensorflow.org/install/gpu, except that you installed Cuda 10.2 from official package. Did you also installed CUDA toolkit? Could you please post all the steps (detailed) as it can help everyone.", "> @Zapunidi did you follow the same steps as mentioned in https://www.tensorflow.org/install/gpu, except that you installed Cuda 10.2 from official package. Did you also installed CUDA toolkit? Could you please post all the steps (detailed) as it can help everyone.\r\n\r\nI can't see the difference between CUDA and CUDA Toolkit. Even https://www.tensorflow.org/install/gpu joggle these two terms like \"The following NVIDIA\u00ae software must be installed on your system: ... CUDA\u00ae Toolkit \u2014TensorFlow supports CUDA\u00ae 10.1 (TensorFlow >= 2.1.0)...\" Then for Linux setup the manual just mentions \"CUDA\" without \"Toolkit\".\r\nI didn't written down the exact steps, so my report is not reliable, sorry. I remember that I\r\n1) Installed CUDA Toolkit 10.1 from the link from manual: https://developer.nvidia.com/cuda-toolkit-archive\r\n2) Installed cuDNN SDK 7.6.5 from nvidia website.\r\n3) Rebooted\r\n4) Executed every command from Ubuntu 18.04 console commands block https://www.tensorflow.org/install/gpu I didn't reboot in the middle of the block as it tells me to do because I already had required drivers and kernel module. A violation, yes.\r\n\r\nSo it was not a clean install. I do not have a spare machine with supported GPU to make clean test for you guys. I also don't think that GPU virtualization is mature to use virtual machine on my primary PC.", "@Zapunidi Yes, CUDA and CUDA Toolkit is 100% the same. This reboot in the middle does not matter, as long as you still reboot after the last step.\r\n\r\nOne thing that might work is to run on top of everything still the \"local\" installation method like this and see what happens after reboot (taken from my comment above):\r\n```\r\n# CUDA 10.1 instructions for creating a locally available repo and installing from it\r\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\r\nsudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\r\nwget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\r\nsudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\r\nsudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub\r\nsudo apt-get update\r\n\r\nsudo apt install cuda-10.1\r\n\r\n# Check which libs are now where, just for your own sanity; env vars should be set after reboot by themselves.\r\nsudo find /usr/local/cuda/ -name '*blas*'\r\nsudo find /usr/lib/ -name '*blas*'\r\n```\r\n\r\nIF this fails and still curious, you can try with my instructions in https://github.com/tensorflow/tensorflow/issues/43236#issuecomment-693244342 using the \"local\" repo installation method and this way the CUDA version remains 10.1&TF works fine. Just make sure not to reboot before the end and ensure most recent nvidia-driver-418 suitable for your GPU is used (the same that you currently have). This does involve some risk when using on a primary PC, I just don't know a better way to quickly clean up *everything* cuda-related without removing the nvidia driver at the same time.", "The problem is that libcublas seems to be missing when installing cuda-10.1 via apt\r\nYou'll not be able to find libcublas.so.10 under /usr/local/cuda-10.1/lib64/ (default path of installation)\r\n\r\nA work around seems to be installing cuda-10.1 via runfile. I encountered another error during the installation process but maybe it works for you.\r\n\r\nCheck this thread for more details: https://forums.developer.nvidia.com/t/cublas-for-10-1-is-missing/71015/18\r\n\r\n", "@bnsblue If using the local installer method that I detailed in my comment above then `libcublas.so.10` is being installed into `/usr/lib/x86_64-linux-gnu/libcublas.so.10` and everything works fine without additional tweaks [1]. This works both for Ubuntu 18.04 and 20.04. Indeed, the TensorFlow official GPU installation method does not work for me as well. Btw, for Ubuntu 20.04 one should still use the 1804 repo in order to get access to cuda-10.1 (2004 apt only seems to have cuda-11).\r\n\r\n[1] It has involved a bit for our use and does not include the libcudnn7 and tensorrt bits, but this should still work as well. For nvidia drivers using v455. If you're interested, I can provide the full instruction that I'm using.", "@ravikyram why this is waiting for author response? The steps in the documentation doesn't work.", "@ahtik Thanks for the response! It would be awesome if you could share the full instructions :)\r\n", "@ahtik I also tried your \"local install\" and `libcublas.so.10` does not get installed into that location. Any ideas?\r\n\r\n", "Must have gotten something wrong the first time, the libraries show up now. But `nvidia-smi` fails now.\r\n\r\n```\r\nsudo find /usr/lib/ -name '*blas*'\r\n/usr/lib/x86_64-linux-gnu/libnvblas.so.10.2.1.243\r\n/usr/lib/x86_64-linux-gnu/libcublas_static.a\r\n/usr/lib/x86_64-linux-gnu/libcublasLt_static.a\r\n/usr/lib/x86_64-linux-gnu/libcublas.so.10\r\n/usr/lib/x86_64-linux-gnu/libnvblas.so.10\r\n/usr/lib/x86_64-linux-gnu/libcublasLt.so.10.2.1.243\r\n/usr/lib/x86_64-linux-gnu/libcublas.so\r\n/usr/lib/x86_64-linux-gnu/libnvblas.so\r\n/usr/lib/x86_64-linux-gnu/stubs/libcublas.so\r\n/usr/lib/x86_64-linux-gnu/stubs/libcublasLt.so\r\n/usr/lib/x86_64-linux-gnu/libcublasLt.so\r\n/usr/lib/x86_64-linux-gnu/libcublasLt.so.10\r\n/usr/lib/x86_64-linux-gnu/libcublas.so.10.2.1.243\r\n/usr/lib/pkgconfig/cublas-10.pc\r\n```\r\n\r\nThen `nvidia-smi` yields: \"NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\"\r\n\r\nI will continue to try and get this working.", "@johntmyers Did you make sure to restart the machine after all the driver and CUDA installation steps? This error is usually from not restarting.", "@ahtik Yes, same issue. FWIW I'm using 18.04 on Google Compute Engine, so I'm not sure if something there is not working properly.", "@javedsha \r\n\r\nAny updates on the issue please. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@ahtik thanks for your setup above, I would also be grateful if you shared your TensorRT instructions!", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43236\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43236\">No</a>\n", "@ahtik \r\n\r\n**System information**\r\n\r\n    OS Platform and Distribution (e.g., Linux Ubuntu 18.04): Ubuntu 18.04\r\n    TensorFlow installed from (source or binary): binary\r\n    TensorFlow version: 2.3\r\n    Python version: 3.6\r\n    Installed using virtualenv? pip? conda?: pip3\r\n    GCC/Compiler version (if compiling from source): 7.5\r\n    CUDA/cuDNN version: 10.1\r\n    GPU model and memory: 1080 TI\r\n\r\nI have exactly followed the instruction dated Sep1-16. I am encountering the following error:\r\n\r\nioz@ioz-B250M-DS3H:~$ python3.6 -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([10000, 10000])))\"\r\n2020-12-25 21:48:47.947244: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 45, in <module>\r\n    from tensorflow.python import data\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.data import experimental\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 125, in <module>\r\n    from tensorflow.python.data.experimental.ops.parsing_ops import parse_example_dataset\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/parsing_ops.py\", line 26, in <module>\r\n    from tensorflow.python.ops import parsing_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parsing_ops.py\", line 27, in <module>\r\n    from tensorflow.python.ops import parsing_config\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parsing_config.py\", line 31, in <module>\r\n    from tensorflow.python.ops import sparse_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py\", line 42, in <module>\r\n    from tensorflow.python.ops import special_math_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 30, in <module>\r\n    import opt_einsum\r\nModuleNotFoundError: No module named 'opt_einsum'\r\nioz@ioz-B250M-DS3H:~$ python3.6 -c \"import tensorflow as tf;\r\n> sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n> bash: unexpected EOF while looking for matching `\"'\r\nbash: syntax error: unexpected end of file\r\nioz@ioz-B250M-DS3H:~$ python3.6\r\nPython 3.6.9 (default, Oct  8 2020, 12:12:24) \r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as               tf\r\n2020-12-25 21:50:17.792384: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 45, in <module>\r\n    from tensorflow.python import data\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.data import experimental\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 125, in <module>\r\n    from tensorflow.python.data.experimental.ops.parsing_ops import parse_example_dataset\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/parsing_ops.py\", line 26, in <module>\r\n    from tensorflow.python.ops import parsing_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parsing_ops.py\", line 27, in <module>\r\n    from tensorflow.python.ops import parsing_config\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parsing_config.py\", line 31, in <module>\r\n    from tensorflow.python.ops import sparse_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py\", line 42, in <module>\r\n    from tensorflow.python.ops import special_math_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 30, in <module>\r\n    import opt_einsum\r\nModuleNotFoundError: No module named 'opt_einsum'\r\n>>> import tensorflow as tf\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 45, in <module>\r\n    from tensorflow.python import data\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.data import experimental\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 125, in <module>\r\n    from tensorflow.python.data.experimental.ops.parsing_ops import parse_example_dataset\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/parsing_ops.py\", line 26, in <module>\r\n    from tensorflow.python.ops import parsing_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parsing_ops.py\", line 27, in <module>\r\n    from tensorflow.python.ops import parsing_config\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parsing_config.py\", line 31, in <module>\r\n    from tensorflow.python.ops import sparse_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py\", line 42, in <module>\r\n    from tensorflow.python.ops import special_math_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 30, in <module>\r\n    import opt_einsum\r\nModuleNotFoundError: No module named 'opt_einsum'\r\n>>> \r\nioz@ioz-B250M-DS3H:~$ python3.6\r\nPython 3.6.9 (default, Oct  8 2020, 12:12:24) \r\n[GCC 8.4.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n2020-12-25 21:50:37.806079: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 45, in <module>\r\n    from tensorflow.python import data\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/__init__.py\", line 25, in <module>\r\n    from tensorflow.python.data import experimental\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 125, in <module>\r\n    from tensorflow.python.data.experimental.ops.parsing_ops import parse_example_dataset\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/parsing_ops.py\", line 26, in <module>\r\n    from tensorflow.python.ops import parsing_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parsing_ops.py\", line 27, in <module>\r\n    from tensorflow.python.ops import parsing_config\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/parsing_config.py\", line 31, in <module>\r\n    from tensorflow.python.ops import sparse_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py\", line 42, in <module>\r\n    from tensorflow.python.ops import special_math_ops\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/special_math_ops.py\", line 30, in <module>\r\n    import opt_einsum\r\nModuleNotFoundError: No module named 'opt_einsum'\r\n\r\n\r\nCan you please help me with your thoughts.\r\n\r\nThanks\r\nGuru"]}, {"number": 43235, "title": "libtensorflow_inference.so: protobuf failed to link __android_log_write and dl_iterate_phdr", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 (1809)\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: Master\r\n- Python version: 3.7.6\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): 4.9.x\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n\r\n**Describe the [problem**]\r\nI'm trying to build the libtensorflow_inference.so stack for android arm64-v8a (Samsung A10) in order to use a pre trained model and an interial sensor for human activity recognition but for some reason it fails to link __android_log_write and dl_iterate_phdr. \r\n\r\nI found a couple of similar issue within tensorflow (e.g. #29658 ) and within the protobuf library (e.g. [#2719](https://github.com/protocolbuffers/protobuf/issues/2719)) but so far i had no luck with fixing this problem\r\n\r\nI tried the lite stack bevor and its build worked perfectly fine but since lite appears to be focused on speech/image recognition it's a bit limited for our purpose because some of the models (boosted trees in particular) are not supported (correct me if im wrong :)). \r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\nbazel build //tensorflow/tools/android/inference_interface:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=arm64-v8a --verbose_failures -s\r\n```\r\n**Any other info / logs**\r\nI had to make a couple of changes in order to get it that far using the clang compiler. Here is my current config:\r\n[bazelrc.txt](https://github.com/tensorflow/tensorflow/files/5222985/bazelrc.txt)\r\n\r\nThe build log from my last attempt: \r\n```\r\nSUBCOMMAND: # @com_google_protobuf//:protobuf [action 'Compiling external/com_google_protobuf/src/google/protobuf/message.cc', configuration: 84e089be1534a4a8f56cc6899fb88395476bdcb929d979ef8f6aa36da34d3e83, execution platform: @local_execution_config_platform//:platform]\r\ncd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/usr/Anaconda3/envs/tensorflow/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/usr/Anaconda3/envs/tensorflow/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android -D__ANDROID_API__=21 -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig -Werror=return-type -Werror=int-to-pointer-cast -Werror=pointer-to-int-cast -Werror=implicit-function-declaration -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf/message.pic.d -frandom-seed=bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf/message.pic.o -fPIC -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/arm64-v8a-opt/bin/external/zlib -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/arm64-v8a-opt/bin/external/zlib -w -D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI -experimental=preprocessor -std=c++14 -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64 -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/com_google_protobuf/src/google/protobuf/message.cc -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf/message.pic.o\r\nERROR: C:/users/usr/_bazel_usr/hv3mexld/external/com_google_protobuf/BUILD:412:10: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1): clang failed: error executing command\r\n  cd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/usr/Anaconda3/envs/tensorflow/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/usr/Anaconda3/envs/tensorflow/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/protoc bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protoc/main.o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotoc_lib.a -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo -Wl,-no-whole-archive bazel-out/arm64-v8a-opt/bin/external/zlib/libzlib.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -DEBUG -OPT=REF -OPT=ICF -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a -no-canonical-prefixes -Wl,-z,relro -Wl,--gc-sections --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo(common.o): In function `google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char> > const&)':\r\nC:\\users\\usr\\_bazel_usr\\hv3mexld\\execroot\\org_tensorflow/external/com_google_protobuf/src/google/protobuf/stubs/common.cc:149: undefined reference to `__android_log_write'\r\nC:\\users\\usr\\_bazel_usr\\hv3mexld\\execroot\\org_tensorflow/external/com_google_protobuf/src/google/protobuf/stubs/common.cc:157: undefined reference to `__android_log_write'\r\nexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x\\libgcc.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':\r\n/usr/local/google/buildbot/src/android/gcc/toolchain/build/../gcc/gcc-4.9/libgcc/unwind-dw2-fde-dip.c:485: undefined reference to `dl_iterate_phdr'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/android/inference_interface:libtensorflow_inference.so failed to build\r\nINFO: Elapsed time: 116.019s, Critical Path: 27.02s\r\nINFO: 234 processes: 234 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\nThank you in advance", "comments": ["We don't have direct support for conda.\r\nPlease use the supported [build solutions](https://www.tensorflow.org/install/source).\r\n\r\nIf you still need conda you need to request support on conda repo or on stackoverflow.", "Thank you for your quick respond and excuse my slow answer\r\nThe last days i ...\r\n- checked out r2.3 since i was originally trying to build the current master\r\n- installed different Python versions using the official installer instead of conda\r\n- did extensive googling\r\n- reinstalled bazel\r\n- tried different ndks (20.1.5948944, 21.3.6528147)\r\n- banged my head against the table\r\n\r\nSadly all of this measures had no real effect and i'm still getting the same linking errors.\r\nSomehow i got the feeling i'm missing something obvious, but i can't put the finger on it...\r\nAnyway... Thank you already for your time and i hope to hear more tips :)\r\n\r\n\r\n**Updated System information:**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 (1809)\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r2.3\r\n- Python version:  3.6.8, 3.8.5\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC/Compiler version (if compiling from source): 4.9.x\r\n- CUDA/cuDNN version: /\r\n- GPU model and memory: /\r\n- Android SDK: 30.0.2\r\n- Android NDK: 20.1.5948944, 21.3.6528147\r\n\r\n**Build command:**\r\n```\r\nbazel build //tensorflow/tools/android/inference_interface:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=arm64-v8a --verbose_failures -s\r\n```\r\n\r\n**Updated error log (using python 3.6.8):**\r\n```\r\nSUBCOMMAND: # @com_google_absl//absl/synchronization:synchronization [action 'Compiling external/com_google_absl/absl/synchronization/internal/per_thread_sem.cc', configuration: 3b23ed678cc8d3202a2e80175357fd89aadea01105139d3ee6929598f48706d8, execution platform: @local_execution_config_platform//:platform]\r\ncd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android -D__ANDROID_API__=21 -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig -Werror=return-type -Werror=int-to-pointer-cast -Werror=pointer-to-int-cast -Werror=implicit-function-declaration -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/per_thread_sem.pic.d -frandom-seed=bazel-out/arm64-v8a-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/per_thread_sem.pic.o -fPIC -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote external/com_google_absl -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_absl -w -D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI -std=c++14 -Wall -Wextra -Wcast-qual -Wconversion-null -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -Wno-missing-field-initializers -Wno-sign-compare --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64 -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/com_google_absl/absl/synchronization/internal/per_thread_sem.cc -o bazel-out/arm64-v8a-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/per_thread_sem.pic.o\r\nSUBCOMMAND: # @com_google_protobuf//:protobuf_lite [action 'Compiling external/com_google_protobuf/src/google/protobuf/arena.cc', configuration: 3b23ed678cc8d3202a2e80175357fd89aadea01105139d3ee6929598f48706d8, execution platform: @local_execution_config_platform//:platform]\r\ncd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android -D__ANDROID_API__=21 -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig -Werror=return-type -Werror=int-to-pointer-cast -Werror=pointer-to-int-cast -Werror=implicit-function-declaration -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/arena.pic.d -frandom-seed=bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/arena.pic.o -fPIC -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -w -D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI -std=c++14 -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64 -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/com_google_protobuf/src/google/protobuf/arena.cc -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/arena.pic.o\r\nERROR: C:/users/usr/_bazel_usr/hv3mexld/external/com_google_protobuf/BUILD:412:10: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1): clang failed: error executing command\r\n  cd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/protoc bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protoc/main.o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotoc_lib.a -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo -Wl,-no-whole-archive bazel-out/arm64-v8a-opt/bin/external/zlib/libzlib.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -DEBUG -OPT=REF -OPT=ICF -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a -no-canonical-prefixes -Wl,-z,relro -Wl,--gc-sections --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo(common.o): In function `google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char> > const&)':\r\nC:\\users\\usr\\_bazel_usr\\hv3mexld\\execroot\\org_tensorflow/external/com_google_protobuf/src/google/protobuf/stubs/common.cc:149: undefined reference to `__android_log_write'\r\nC:\\users\\usr\\_bazel_usr\\hv3mexld\\execroot\\org_tensorflow/external/com_google_protobuf/src/google/protobuf/stubs/common.cc:157: undefined reference to `__android_log_write'\r\nexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x\\libgcc.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':\r\n/usr/local/google/buildbot/src/android/gcc/toolchain/build/../gcc/gcc-4.9/libgcc/unwind-dw2-fde-dip.c:485: undefined reference to `dl_iterate_phdr'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/android/inference_interface:libtensorflow_inference.so failed to build\r\nERROR: C:/users/usr/repositories/tensorflow/tensorflow/core/BUILD:1372:11 Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1): clang failed: error executing command\r\n  cd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/protoc bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protoc/main.o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotoc_lib.a -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo -Wl,-no-whole-archive bazel-out/arm64-v8a-opt/bin/external/zlib/libzlib.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -DEBUG -OPT=REF -OPT=ICF -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a -no-canonical-prefixes -Wl,-z,relro -Wl,--gc-sections --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 172.208s, Critical Path: 23.80s\r\nINFO: 343 processes: 343 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n\r\nPS: Yes i'm basically a newbie in c++ and bazel.\r\nHopefully it's not a rookie error....\r\n", ">     SET PYTHON_BIN_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/python.exe\r\n>     SET PYTHON_LIB_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python36-32/lib/site-packages\r\n\r\n@VirtualNonsense,\r\nLooks like you are using the 32-bit version of Python. Could you please check if you are facing the same issue with Python 64 bit? Thanks!  ", "@amahendrakar \r\nThank you for your answer but the error persists:\r\n\r\n```\r\nC:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944ERROR: C:/users/usr/_bazel_usr/hv3mexld/external/com_google_protobuf/BUILD:412:10: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1): clang failed: error executing command\r\n  cd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/protoc bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protoc/main.o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotoc_lib.a -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo -Wl,-no-whole-archive bazel-out/arm64-v8a-opt/bin/external/zlib/libzlib.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -v -DEBUG -OPT=REF -OPT=ICF -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a -no-canonical-prefixes -Wl,-z,relro -Wl,--gc-sections --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64\r\nExecution platform: @local_execution_config_platform//:platform\r\nAndroid (5220042 based on r346389c) clang version 8.0.7 (https://android.googlesource.com/toolchain/clang b55f2d4ebfd35bf643d27dbca1bb228957008617) (https://android.googlesource.com/toolchain/llvm 3c393fe7a7e13b0fba4ac75a01aa683d7a5b11cd) (based on LLVM 8.0.7svn)\r\nTarget: aarch64-none-linux-android\r\nThread model: posix\r\nInstalledDir: C:\\users\\usr\\_bazel_usr\\hv3mexld\\execroot\\org_tensorflow\\external\\androidndk\\ndk\\toolchains\\llvm\\prebuilt\\windows-x86_64\\bin\r\nFound candidate GCC installation: external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android\\4.9.x\r\nSelected GCC installation: external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x\r\nCandidate multilib: .;@m64\r\nSelected multilib: .;@m64\r\n \"external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/bin\\\\ld\" --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64 -pie -EL --fix-cortex-a53-843419 -z now -z relro --hash-style=both --enable-new-dtags --eh-frame-hdr -m aarch64linux -dynamic-linker /system/bin/linker64 -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/protoc \"external/androidndk/ndk/platforms/android-21/arch-arm64/usr/lib\\\\crtbegin_dynamic.o\" -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a \"-LC:\\\\users\\\\usr\\\\_bazel_usr\\\\hv3mexld\\\\execroot\\\\org_tensorflow\\\\external\\\\androidndk\\\\ndk\\\\toolchains\\\\llvm\\\\prebuilt\\\\windows-x86_64\\\\lib64\\\\clang\\\\8.0.7\\\\lib\\\\linux\\\\aarch64\" -Lexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x -Lexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib/../lib64 -Lexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x/../../../../aarch64-linux-android/lib -Lexternal/androidndk/ndk/platforms/android-21/arch-arm64/usr/lib bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protoc/main.o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotoc_lib.a -whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf.lo -no-whole-archive -whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo -no-whole-archive bazel-out/arm64-v8a-opt/bin/external/zlib/libzlib.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -z relro --gc-sections -lgcc -lc -lgcc \"external/androidndk/ndk/platforms/android-21/arch-arm64/usr/lib\\\\crtend_android.o\"\r\nbazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo(common.o): In function `google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char> > const&)':\r\nC:\\users\\usr\\_bazel_usr\\hv3mexld\\execroot\\org_tensorflow/external/com_google_protobuf/src/google/protobuf/stubs/common.cc:149: undefined reference to `__android_log_write'\r\nC:\\users\\usr\\_bazel_usr\\hv3mexld\\execroot\\org_tensorflow/external/com_google_protobuf/src/google/protobuf/stubs/common.cc:157: undefined reference to `__android_log_write'\r\nexternal/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x\\libgcc.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':\r\n/usr/local/google/buildbot/src/android/gcc/toolchain/build/../gcc/gcc-4.9/libgcc/unwind-dw2-fde-dip.c:485: undefined reference to `dl_iterate_phdr'\r\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\r\nTarget //tensorflow/tools/android/inference_interface:libtensorflow_inference.so failed to build\r\nERROR: C:/users/usr/repositories/tensorflow/tensorflow/core/BUILD:1372:11 Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1): clang failed: error executing command\r\n  cd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow\r\n  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2\r\n    SET ANDROID_NDK_API_LEVEL=21\r\n    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944\r\n    SET ANDROID_SDK_API_LEVEL=30\r\n    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk\r\n    SET PATH=C:\\msys64\\usr\\bin;C:\\msys64\\bin;C:\\WINDOWS;C:\\WINDOWS\\System32;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\r\n    SET PWD=/proc/self/cwd\r\n    SET PYTHON_BIN_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python38/python.exe\r\n    SET PYTHON_LIB_PATH=C:/Users/usr/AppData/Local/Programs/Python/Python38/lib/site-packages\r\n    SET RUNFILES_MANIFEST_ONLY=1\r\n    SET TF2_BEHAVIOR=1\r\n    SET TF_CONFIGURE_IOS=0\r\n    SET TF_ENABLE_XLA=1\r\n  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/protoc bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protoc/main.o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotoc_lib.a -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo -Wl,-no-whole-archive bazel-out/arm64-v8a-opt/bin/external/zlib/libzlib.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -v -DEBUG -OPT=REF -OPT=ICF -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a -no-canonical-prefixes -Wl,-z,relro -Wl,--gc-sections --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64\r\nExecution platform: @local_execution_config_platform//:platform\r\nINFO: Elapsed time: 192.953s, Critical Path: 24.57s\r\nINFO: 362 processes: 362 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n", "@VirtualNonsense Could you please try on the latest stable TF v2.6.0 and let us know if the issue still persists ? Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43235\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43235\">No</a>\n"]}, {"number": 43234, "title": "tf2.3 c++\u5e93\u7f16\u8bd1\u5931\u8d25", "body": "cenots 8\r\ngcc 8.3.1\r\n\r\nis_not_gcc failed: error executing command \r\n  (cd /home/docker_data/.bazel_cache_2_3/7a9fafc24832c7560e0a6e1a434658e0/execroot/org_tensorflow && \\\r\n  exec env - \\\r\n    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \\\r\n    CUDNN_INSTALL_PATH=/usr/local/cuda-10.2 \\\r\n    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \\\r\n    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib:/usr/local/cuda-10.2/extras/CUPTI/lib64: \\\r\n    NCCL_INSTALL_PATH=/usr \\\r\n    PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/cuda-10.2/bin:/usr/local/cuda-10.2/targets/x86_64-linux/lib:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/usr/bin/python3 \\\r\n    PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages \\\r\n    TENSORRT_INSTALL_PATH=/usr/local/cuda-10.2 \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_CONFIGURE_IOS=0 \\\r\n    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \\\r\n    TF_CUDA_PATHS=/usr/local/cuda-10.2 \\\r\n    TF_CUDA_VERSION=10.2 \\\r\n    TF_CUDNN_VERSION=7 \\\r\n    TF_ENABLE_XLA=1 \\\r\n    TF_NCCL_VERSION=2 \\\r\n    TF_NEED_CUDA=1 \\\r\n    TF_TENSORRT_VERSION=7 \\\r\n  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.3.0-2.params)\r\nExecution platform: @local_execution_config_platform//:platform\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/libkernels.pic.lo(mat_mul_op.pic.o): In function `tensorflow::CSRMatMulGPUOp<float>::Compute(tensorflow::OpKernelContext*)':\r\nmat_mul_op.cc:(.text._ZN10tensorflow14CSRMatMulGPUOpIfE7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow14CSRMatMulGPUOpIfE7ComputeEPNS_15OpKernelContextE]+0x14ba): undefined reference to `tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'\r\nbazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/libkernels.pic.lo(sparse_mat_mul_op.pic.o): In function `tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*)':\r\nsparse_mat_mul_op.cc:(.text._ZN10tensorflow20CSRSparseMatMulGPUOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow20CSRSparseMatMulGPUOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE]+0x8ee): undefined reference to `tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'\r\nsparse_mat_mul_op.cc:(.text._ZN10tensorflow20CSRSparseMatMulGPUOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow20CSRSparseMatMulGPUOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE]+0x97e): undefined reference to `tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'\r\ncollect2: error: ld returned 1 exit status\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 37.074s, Critical Path: 36.43s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully", "comments": ["@passion765 \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error,complete error log]\r\n\r\nYour may verify with [this link](https://github.com/tensorflow/tensorflow/issues/762#issuecomment-173002842), and let us know if it helps. [[link](https://github.com/tensorflow/tensorflow/issues/32775#issuecomment-542337360), [link1](https://stackoverflow.com/questions/35290945/compiling-tensorflow-from-source-undefined-reference-to-tensorflowfunctorfi).]\r\n\r\nyou may also please take a look at the [system requirements](https://www.tensorflow.org/install/pip#system-requirements) and check if you have the correct dependencies installed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43234\">No</a>\n", "> my command is bazel build -c opt --copt=\"-O3\" --copt=\"-fpic\" --cxxopt=\"-fpic\" --cxxopt=\"-O3\" --cxxopt=\"-std=c++14\" --config=cuda --verbose_failures //tensorflow:libtensorflow_cc.so\r\n> \r\n> and you can see much infomation from output above.\r\n> I can build sucessfully tf2.2 , but tf2.3 always be error at \"tensorflow::functor::CSRSparseMatrixTranspose\" , could you help me?\r\n\r\n@Saduf2019 ", "I can build py tf2.3 sucessfully , but libtensorflow_cc.so tf2.3 always failed .  are you serving this question?  @Saduf2019 ", "@passion765 \r\nPlease refer to below links and let us know if it helps.\r\n[link](https://github.com/tensorflow/tensorflow/issues/22240#issuecomment-420782646) [link1](https://stackoverflow.com/questions/62460560/tf2-2-build-libtensorflow-cc-so-for-c-apis), [link2](https://github.com/tensorflow/tensorflow/issues/4279)\r\n#40004 #35623\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43234\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43234\">No</a>\n"]}, {"number": 43233, "title": "Usage of MethodNameUpdater() function", "body": "@florence27 can you please report a new issue along with the complete  code to reproduce the error that is not working and cc me on it as well. \r\nThanks, \r\nGoldie\r\n\r\n_Originally posted by @goldiegadde in https://github.com/tensorflow/tensorflow/issues/34968#issuecomment-692168824_\r\n\r\nAs already mentioned in issue #34968 I'd like to use the `MethodNameUpdater()` function in order to change the method name in the signature from predict to classify. Unfortunately, the following code is not running, due to an `AttributeError`:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(4, input_shape=(FEATURE_SIZE,)))\r\nmodel.add(tf.keras.layers.Dense(1))\r\noptim = tf.keras.optimizers.Adam()\r\nmodel.compile(loss='mean_squared_error', optimizer=optim, metrics=['mae'])\r\n\r\nx, y = load_features(train_files, scaler)\r\nmodel.fit(x, y, epochs=num_epochs, callbacks=callbacks)\r\n\r\njob_dir = '/path/to/job/'\r\nversion = '00000123'\r\nexport_path = os.path.join(job_dir, version)\r\n\r\ntf.keras.models.save_model(\r\n                model,\r\n                export_path,\r\n                overwrite=True,\r\n                include_optimizer=True,\r\n                save_format=None,\r\n                options=None,\r\n            )\r\n\r\nupdater = tf.compat.v1.saved_model.builder.MethodNameUpdater(export_path)\r\nupdater.replace_method_name(signature_key=\"bar\", method_name=\"classify\", tags=\"serve\")\r\nupdater.save(export_path)\r\n```\r\n\r\nThis is the exact error I'm getting: `AttributeError: module 'tensorflow._api.v2.compat.v1.saved_model.builder' has no attribute 'MethodNameUpdater'`. The `load_features()` function takes the training files and a `MinMaxScaler()` as inputs and returns `np.arrays` with input features and corresponding labels. My setup is the following: \r\n\r\nOS Platform: macOS Catalina, Version 10.15.5\r\nTensorflow Version: 2.3.0\r\nPython Version: 3.7.9\r\n\r\nAny help with this would be appreciated!", "comments": ["Can you share a minimal but fully runnable example that we can copy, paste and run to reproduce your error? Also a runnable colab is ok.\r\n\r\nThanks.", "Sure, here you go: \r\n\r\n```\r\nimport tensorflow as tf \r\nimport os \r\n\r\nFEATURE_SIZE = 3\r\nmodel = tf.keras.models.Sequential()\r\nmodel.add(tf.keras.layers.Dense(4, input_shape=(FEATURE_SIZE,)))\r\nmodel.add(tf.keras.layers.Dense(1))\r\noptim = tf.keras.optimizers.Adam()\r\nmodel.compile(loss='mean_squared_error', optimizer=optim, metrics=['mae'])\r\n\r\nx = tf.ones((3, 3))\r\ny = tf.zeros(3)\r\n\r\nnum_epochs = 3\r\nmodel.fit(x, y, epochs=num_epochs)\r\n\r\njob_dir = os.getcwd()\r\nversion = '1'\r\nexport_path = os.path.join(job_dir, version)\r\n\r\ntf.keras.models.save_model(\r\n                model,\r\n                export_path,\r\n                overwrite=True,\r\n                include_optimizer=True,\r\n                save_format=None,\r\n                options=None,\r\n            )\r\n\r\nupdater = tf.compat.v1.saved_model.builder.MethodNameUpdater(export_path)\r\nupdater.replace_method_name(signature_key=\"bar\", method_name=\"classify\", tags=\"serve\")\r\nupdater.save(export_path)\r\n```", "You can use:\r\n`updater = tf.compat.v1.saved_model.signature_def_utils.MethodNameUpdater(export_path)`\r\n\r\nBut your saved model is missing signatures so your example is incomplete.", "@bhack thanks that was the help I needed! I omitted the signatures in this example, but I'm using some in my current project, so that should be fine. Thank you so much!", "@florence27 \r\n\r\nPlease, close this thread if your issue was resolved. Thanks!"]}]