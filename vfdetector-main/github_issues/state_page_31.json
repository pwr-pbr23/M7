[{"number": 48719, "title": "Unclear if Debug build on Windows is supported", "body": "## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\n## Description of issue (what needs changing):\r\nSpecify if building Debug verion of TF on Windows is possible. \r\nFrom looking at bugs here people are having problems, but it is unclear if it is officially not supported, or it is supported, clarifying that in docs would help.\r\n\r\n\r\n", "comments": []}, {"number": 48708, "title": "model.fit() should tell you if batch is not big enough to train with", "body": "Suppose you train a model using model.fit() and your batch size is set to N.  If the input from a generator is < N, tf keras gives the following error which is difficult to interpret: **ValueError: Expect x to be a non-empty array or dataset.**\r\n\r\nReference second answer in : https://stackoverflow.com/questions/63231811/valueerror-expect-x-to-be-a-non-empty-array-or-dataset-tensor-flow-lite-model\r\n\r\nPropose to have tf keras tell user that batch size is larger than data size in place of current error.  The current error implies that the x is empty which isnt this case for all x with size less than N.", "comments": ["@isaacgerg ,\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide the Tensorflow version,complete code and dataset to reproduce the issue reported here.\r\n\r\nThanks!", "Tensorflow 2.4.1", "@isaacgerg ,\r\n\r\nCould you please provide the complete code and dataset or colab link to reproduce the issue reported here.\r\n\r\n", "@tilakrayal  See the stack overflow link in the original post.", "@isaacgerg \r\nYou set a batch size and then enter input less than batch size it would be treated as empty, you cannot have that. \r\nWe cannot have issues to fix user code as this repo is for performance issues and feature request or bugs only.", "@Saduf2019 Please read the original issue carefully and note my proposal.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 48703, "title": "Feature Request: adding class_weights to tfLite Model Maker", "body": "## Description of issue (what needs changing):\r\nIt would be great if somebody could add class_weights to the `model maker` .create() arguments because it's pretty common to work with imbalanced datasets in real life. I'm trying to retrain a model with model maker for image classification and have about 15 labels with very different balances in classes. I'm trying to change the code myself and hoping to do a pull request but I can't my code to work.\r\n\r\n### Clear description\r\n\r\nFor example, why should someone use this method? How is it useful?\r\nMany datasets are imbalanced and having class_weights in the .create() method will help a lot to tackle that issue.\r\n\r\n### Correct links\r\n\r\nhttps://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker/core/task\r\n\r\n### Parameters defined\r\n\r\nDescription which could be added.\r\nclass_weight: : Optional dictionary mapping class indices (integers) to a \r\n        weight (float) value, used for weighting the loss function \r\n        (during training only). This can be useful to tell the model to \r\n        \"pay more attention\" to samples from an under-represented class.\r\n\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\n\r\nI'm trying to fix it myself and would like to do a pull request but my build never works after I changed the code. I've been trying to fix it for a while and finally just figured I should see if somebody else knows how to do it.\r\n", "comments": ["@lintian06 could you review this feature request?", "@lintian06 I changed some code which I think adds the feature. If you're interested, maybe you could check my code? I'm not sure if it works because I can't build from source when I follow the instructions from the model_maker readme (not sure why that is).  This is the PR I made [https://github.com/tensorflow/examples/pull/307](https://github.com/tensorflow/examples/pull/307)"]}, {"number": 48699, "title": "autograph prevents tf.assert protection", "body": "**System information**\r\n\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n    OS Platform and Distribution: Ubuntu 18.04.5 LTS\r\n    TensorFlow installed from: binary\r\n    TensorFlow version: 2.2.0\r\n    Python version: 3.6.9\r\n    CUDA/cuDNN version: 10.1.243 / 7.6.5\r\n    GPU model and memory: NVidia Tesla V100-SXM2-32GB\r\n\r\n**Describe the current behavior**\r\n`tf.assert_xxx()` are not executed due to autograph (correct but unfortunate) error.\r\n\r\nAssume we have a function taking a tensor and a bias to be added, supposedly having the same shape. We want a meaningful error message if the shapes are not the same.\r\nFirst a function that doesn't do the add. Check that the error message works.\r\n\r\n```python\r\n@tf.function(input_signature=[tf.TensorSpec([None, None], tf.float32), tf.TensorSpec([None, None], tf.float32)])\r\ndef my_assert_func_1(x, bias):\r\n    print('tracing...')\r\n    tf.print('tf.shape(bias)[1]', tf.shape(bias)[1])\r\n    tf.assert_equal(tf.shape(x)[1], tf.shape(bias)[1], message='bad shape')\r\n\r\n    # just return x (does nothing useful)\r\n    return x\r\n\r\n# successful case\r\nmy_assert_func_1(tf.ones((2,5)), tf.ones((2,5)))\r\n```\r\n```\r\ntracing...\r\ntf.shape(bias)[1] 5\r\n\r\n<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\r\narray([[1., 1., 1., 1., 1.],\r\n       [1., 1., 1., 1., 1.]], dtype=float32)>\r\n```\r\n```python\r\n# error case : the 'bad shape' message is successfully reported\r\nmy_assert_func_1(tf.ones((2,5)), tf.ones((2,7)))\r\n```\r\n```\r\ntf.shape(bias)[1] 7\r\nInvalidArgumentError:  assertion failed: [bad shape] [Condition x == y did not hold element-wise:] [x (strided_slice_1:0) = ] [5] [y (strided_slice_2:0) = ] [7]\r\n\t [[node assert_equal_1/Assert/Assert (defined at <ipython-input-9-f5089434d4c3>:5) ]] [Op:__inference_my_assert_func_1_234]\r\n```\r\n\r\nNow modify the function to **actually add** the bias\r\n\r\n```python\r\n@tf.function(input_signature=[tf.TensorSpec([None, None], tf.float32), tf.TensorSpec([None, None], tf.float32)])\r\ndef my_assert_func_2(x, bias):\r\n    print('tracing...')\r\n    tf.print('tf.shape(bias)[1]', tf.shape(bias)[1])\r\n    tf.assert_equal(tf.shape(x)[1], tf.shape(bias)[1], message='bad shape')\r\n    # add the bias\r\n    return x + bias\r\n\r\n# successful case: still works\r\nmy_assert_func_2(tf.ones((2,5)), tf.ones((2,5)))\r\n```\r\n```\r\ntracing...\r\ntf.shape(bias)[1] 5\r\n\r\n<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\r\narray([[2., 2., 2., 2., 2.],\r\n       [2., 2., 2., 2., 2.]], dtype=float32)>\r\n```\r\n```python\r\n# error case : the 'bad shape' message is NOT reported.\r\n# Probably due to autograph that catches the error before tf.assert has a chance to execute.\r\n# (full stacktrace in the attached gist)\r\nmy_assert_func_2(tf.ones((3,5)), tf.ones((3,7)))\r\n```\r\n```\r\ntf.shape(bias)[1] 7\r\nInvalidArgumentError:  Incompatible shapes: [3,7] vs. [3,5]\r\n\t [[node add (defined at <ipython-input-12-b2b31a707c39>:7) ]] [Op:__inference_my_assert_func_2_284]\r\n```\r\n\r\n**Describe the expected behavior**\r\n`tf_assert_xxx()` should protect against those errors and report the error message.\r\nNot sure if this is fixable (not sure if bug or feature request, but probably hard to fix).\r\nThe consequence is that we cannot reuse in a `tf.function` libraries of custom keras layers that use `tf.assert_xxx()`.  \r\nThe attached gist shows an example of that.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/gist/thierryherrmann/9d253dfd32a4ab9296fadb99f5ec3651/asserts_in_tf_functions.ipynb\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n\r\n\r\n\r\n", "comments": ["Can you try with `!pip install tf-nightly-cpu` in your colab? As It seems solved on master.", "Ok, I got you meant having `assertion failed: [bad shape]` also in the autograph. So we still have the same behavior on master.", "Can you try with:\r\n```python\r\n@tf.function(input_signature=[tf.TensorSpec([None, None], tf.float32), tf.TensorSpec([None, None], tf.float32)])\r\ndef my_assert_func_2(x, bias):\r\n    print('tracing...')\r\n    tf.control_dependencies((x, bias))\r\n    tf.assert_equal(tf.shape(x)[1], tf.shape(bias)[1], message='bad shape')\r\n    tf.print('tf.shape(bias)[1]', tf.shape(bias)[1])\r\n    out = x + bias\r\n    return out\r\n```\r\n\r\nI know that we have a note in the doc:\r\nhttps://github.com/tensorflow/tensorflow/blob/90525711ff3e5e84b456c91e07323b6694647b40/tensorflow/python/framework/ops.py#L5366-L5369\r\n\r\nBut we also have:\r\nhttps://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/check_ops.py#L646-L652\r\n", "/cc @mdanatg ", "This is a known (and unfortunate) bug in the automatic control dependencies. It is almost always smart enough to wire them properly, but in this case it fails to. The main issue is that the auto control dependencies doesn't properly detect that the assert should happen before `x + bias`. Instead, it lets them run in parallel, and the add seems to win and raise an error first.\r\n\r\nFor now, a workaround is to manually add control deps as @bhack suggested:\r\n\r\n```\r\nwith tf.control_dependencies([tf.assert_equal(tf.shape(x)[1], tf.shape(bias)[1], message='bad shape')]):\r\n    # add the bias\r\n    return x + bias\r\n```\r\n\r\nI think we should increase the priority for fixing this, because it can be unsafe. Technically,we should have a rule that a `tf.assert` should always condition the execution of any computations that depend on any tensors input to the assert itself.\r\ncc @saxenasaurabh ", "@mdanatg I think also my workaround is ok or not?", "@bhack you need to use `control_dependencies` with  a context manager -- writing it on its own has no effect. But I think we meant to represent the same idea?", "Yes", "```python\r\nwith tf.control_dependencies([...]):\r\n   ...\r\n```\r\nis quite a nice and non obtrusive workaround.\r\nThanks guys for the impressively fast response.\r\nUp to you to close this issue or to leave it open to fix the original bug in automatic control dependencies", "@mdanatg As I think that you have collected some well note cases do you think that we could extend the note with the currently known exceptions? \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/90525711ff3e5e84b456c91e07323b6694647b40/tensorflow/python/framework/ops.py#L5366-L5369", "Yes, I believe we should. There are two broad categories that I know of:\r\n * uses of tf.assert (and any of its variants)\r\n * uses of tf.py_function which affects tf.Variables\r\n\r\nAt any rate, the note should say amount to something like \"Most of the time, you shouldn't need this [...]. Only use it to manually control ordering, for example as a workaround to known issues.\". Then, the docstring should include a known issues section that details these two with examples. The docstrings of tf.assert_* and tf.py_function should call that out as well.\r\n\r\nHappy to review a PR if anyone is interested.", "@mdanatg Yes, If you will track these features in other tickets (on github or internally?) we could reserve this to the doc change.\nCan we label this with docs and contribution welcome? \n\n@thierryherrmann are you interested to open this small Doc PR? \nIf yes check https://www.tensorflow.org/community/contribute/docs", "I'm not actively working on this - if anyone from the community is interested in contributing, please add a comment here.", "Hey @mdanatg, I am new to open source but want to start contributing to the Tensorflow repo. I am unsure how to start and work on it, Can you please provide me with some resources to start from? I will be highly obliged. Thanks in advance.", "Hi @Nikhil-Mudgal , https://www.tensorflow.org/community/contribute is a good place to start.", "Thanks @mdanatg. I checked it out but the main problem is finding a good issue for me to start on. I searched some of the issues tagged contributions welcome and good first issue; but they were either too difficult for me as a beginner in Tensorflow or I didn't understand the issue at all. Kindly help me find some issues as a beginner to try. I really want to contribute to tensorflow. \r\nThanks for the help"]}, {"number": 48675, "title": "TFLite bug: RuntimeError: tensorflow/lite/kernels/conv.cc:329 input->dims->size != 4 (3 != 4)Node number 0 (CONV_2D) failed to prepare.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: Python 3.6.9 :: Anaconda, Inc.\r\n\r\n**Describe the current behavior**\r\nI am trying to convert some model to TFLite but the error `RuntimeError: tensorflow/lite/kernels/conv.cc:329 input->dims->size != 4 (3 != 4)Node number 0 (CONV_2D) failed to prepare.` is raised when I try to convert MobileNetV3Large.\r\n\r\n**Describe the expected behavior**\r\nI expect to be able to convert any basic model from `TensorFlow.Keras` to `TFLite`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n\"\"\"\r\nFile to show the runtime error when trying to convert MobileNetV3 to TFLite.\r\n\"\"\"\r\n\r\nimport tensorflow as tf\r\n\r\n\r\nmobile_net = tf.keras.applications.MobileNetV3Large(\r\n    input_tensor=tf.keras.Input(shape=(224, 224, 3), batch_size=1, name=\"input\", dtype=tf.float32)\r\n)\r\nmodel = tf.keras.Model(inputs=mobile_net.inputs, outputs=mobile_net(mobile_net.inputs))\r\nmodel.compile()\r\nmodel.summary(line_length=200)\r\n\r\n\r\ndef representative_dataset_generator():\r\n    \"\"\"Dataset generator that generates random tensor with the same shape as the input\"\"\"\r\n    for _ in range(100):\r\n        yield tf.random.uniform(shape=(1, 224, 224, 3), dtype=tf.float32)\r\n\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n# Set the representative dataset in order to quantize the activations\r\nconverter.representative_dataset = representative_dataset_generator\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.target_spec.supported_types = [tf.int8]\r\n\r\n# Set the input and output tensors to uint8\r\nconverter.inference_input_type = tf.uint8\r\nconverter.inference_output_type = tf.uint8\r\n\r\n# Experimental environment\r\nconverter.experimental_new_converter = True\r\nconverter.experimental_new_quantizer = True\r\n\r\ntflite_model = converter.convert()\r\n\r\n```\r\n**Other infos**\r\nThe problem since to be coming from `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]`. If I comment the `converter.inference_*_type = tf.uint8`, `converter.target_spec.supported_types = [tf.int8]` and also the `converter.optimizations`line, the error is still raised but go away if I change to `converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]`.\r\nIt comes back if I uncomment the line related to the converter.optimizations.\r\nAnyway, I need to do an integer only quantization like in the [TF tutorial here](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only). I am quite lost. Would you have any idea?\r\n", "comments": ["Hi Yann, can you help me and upload the converted model when only `[tf.lite.OpsSet.TFLITE_BUILTINS]` is used?", "Of course, [here](https://github.com/tensorflow/tensorflow/files/6353281/model.zip) it is!", "Could you also share your inference code? I think the input dimension for the image tensor should have four dimensions.", "Hi, there is no inference code. I am just running the conversion code I put in the original issue message above. The input_shape of the model is `(1, 224, 224, 3)` (you can verify with [netron](https://netron.app/)) and the representative dataset is yielding tensors of shape `(1, 224, 224, 3)` ie the exact same."]}, {"number": 48673, "title": "load_model fails for certain keras.layers.Add configurations", "body": "**System information**\r\nPython version: 3.7\r\nTF 2.3 version: v2.3.0-54-gfcc4b966f1 2.3.1\r\nTF 2.4 version: v2.4.0-49-g85c8b2a817f 2.4.1\r\nTF-nightly version: v1.12.1-55314-g6a872f41130 2.6.0-dev20210421\r\n\r\n**Describe the current behavior**\r\nIt's possible to create and infer model with Add layer (or OpLambdaLayer-Add) that adds the input to a constant, either with the constant as first input or the last. I can save it, but can't load it again\r\n\r\nThe following code works in TF 2.3, but fails both in TF 2.4 and the nightly versions\r\n\r\n**Describe the expected behavior**\r\nSeems like it should be able to load the saved model\r\n\r\n**Standalone code to reproduce the issue**\r\n`\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp_const = np.ones((1, 6))\r\nouts = []\r\n_in = tf.keras.layers.Input((6,))\r\nouts.append(tf.keras.layers.Add()((_in, _in)))\r\nouts.append(_in + np_const)\r\nouts.append(tf.keras.layers.Add()((np_const, _in)))  # <=== problematic layer\r\nouts.append(np_const + _in)  # <=== problematic layer\r\n\r\nmodel = tf.keras.Model(inputs=_in, outputs=outs)\r\n\r\nprint('TF version:', tf.__version__)\r\nprint('model output:', model(np.arange(18).reshape(-1, 6)))\r\n\r\nmodel_file = '/tmp/add_model.h5'\r\nmodel.save(model_file)\r\n\r\nloaded_model = tf.keras.models.load_model(model_file, compile=False)\r\nprint('loaded model output:', loaded_model(np.arange(18).reshape(-1, 6)))\r\n`\r\n\r\n", "comments": ["@jvishnuvardhan ,\r\n\r\nI was able to reproduce the issue. Code got executed without error on tf 2.3, whereas with  tf 2.4, nightly it is failing with error.  Please find the [gist](https://colab.research.google.com/gist/tilakrayal/017b8a7566093e4bb0d7644827bdb47d/48673.ipynb) here", "As I wrote in the issue description, the code works fine in TF 2.3\r\nit only started failing after the move to 2.4", "I was able to reproduce the issue on colab using `TF2.6` and `TF-nightly(2.8.0-dev20211003)`. Please find the gist [here](https://colab.research.google.com/gist/chunduriv/2b3a7c25f78f2a65a440638051a65eb0/48673.ipynb) for reference .Thanks!"]}, {"number": 48671, "title": "TFLite produces bogus results on GPU when setPrecisionLossAllowed(true)", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Android 10\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.0\r\n\r\n### 2. Code\r\n\r\nI am using a FC-DenseNet model trained with PyTorch, then converted to ONNX, Tensorflow .pb and finally TFLite.\r\nOriginal torch code is here: https://github.com/petko-nikolov/pysemseg/blob/master/pysemseg/models/densenet.py\r\n\r\nAdditionally, I added a `permute` operation (equivalent to Numpy transpose), `Argmax` and conversion to `np.uint8` in the end of the model. \r\n\r\n### 3. Failure after conversion\r\nModel converts **OK**.\r\nPython tests on Desktop are **OK** both with Tensorflow and TFLite.\r\nModel is **OK** on Android using CPU\r\nModel is **NOT OK** on Android using GPU Delegate\r\nModel is **OK** on Android using GPU Delegate and `setPrecisionLossAllowed(false)`\r\n\r\n", "comments": ["@impjdi could you take a look?", "The title of the bug and \r\n\r\n> Model is OK on Android using GPU Delegate and `setPrecisionLossAllowed(false)`\r\n\r\nare telling contradicting story.  Which one is it?\r\n\r\nAlso, could you attach the model?", "@impjdi the title was wrong and was fixed.\r\nI'm not sure I can attach the model. Can I give you any other relevant info without providing the model? Can I provide the TFLite model file with random weights? This is because it is copyrighted.", "[quantized-fp16.tflite.zip](https://github.com/tensorflow/tensorflow/files/6358625/quantized-fp16.tflite.zip)\r\n\r\nI'm attaching the same model, quantised to FP16, but with different weights. Hope this helps? \r\nAnyway let me know what more information you would need.", "> [quantized-fp16.tflite.zip](https://github.com/tensorflow/tensorflow/files/6358625/quantized-fp16.tflite.zip)\r\n> \r\n> I'm attaching the same model, quantised to FP16, but with different weights. Hope this helps? Anyway let me know what more information you would need.\r\n\r\nI have a same problem with setPrecisionLossAllowed option. @bmmo Have you found the solution?"]}, {"number": 48664, "title": "Eigen update needed to fix tensorflow build failure on ppc64le", "body": "Request to update Eigen in TF to https://gitlab.com/libeigen/eigen/-/commit/06c2760bd1139711eeffa30266ead43423891698\r\nto fix compilation issue on ppc64le.\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source master branch\r\n- TensorFlow version:\r\n- Python version: 3.9\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC/Compiler version (if compiling from source): gcc 9, gcc8, gcc 7\r\n- CUDA/cuDNN version: NO CUDA (CPU only)\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nWhile building tensorflow on ibmcom/tensorflow-ppc64le:devel-manylinux2014 docker image for ppc64le and using gcc 9 (tried gcc 8 and gcc 7 too), we are seeing a compilation error related to Eigen. The fix for the same has been pushed to Eigen repository yesterday at https://gitlab.com/libeigen/eigen/-/commit/06c2760bd1139711eeffa30266ead43423891698\r\nIf TF updates the eigen version to the above mentioned commit, we will get the issue fixed without any patching.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n```\r\nBAZEL_LINKLIBS=-l%:libstdc++.a bazel build -c opt --config=v2 --local_cpu_resources 4 --local_ram_resources 4096 \\\r\n    //tensorflow/tools/pip_package:build_pip_package\r\nbazel-bin/tensorflow/tools/pip_package/build_pip_package --nightly_flag --cpu $WORKSPACE/tensorflow_pkg```\r\n\r\n```\r\n\r\nError:\r\n\r\n```\r\ntensorflow/compiler/xla/service/cpu/runtime_single_threaded_matmul.cc:121:76:   required from here\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:498:20: error: taking address of rvalue [-fpermissive]\r\n  498 |             lhs0 = &lhs(j + 0, i);\r\n      |                    ^~~~~~~~~~~~~~\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:499:20: error: taking address of rvalue [-fpermissive]\r\n  499 |             lhs1 = &lhs(j + 1, i);\r\n      |                    ^~~~~~~~~~~~~~\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:501:20: error: taking address of rvalue [-fpermissive]\r\n  501 |             lhs0 = &lhs(j + 2, i);\r\n      |                    ^~~~~~~~~~~~~~\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:502:20: error: taking address of rvalue [-fpermissive]\r\n  502 |             lhs1 = &lhs(j + 3, i);\r\n      |                    ^~~~~~~~~~~~~~\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:505:20: error: taking address of rvalue [-fpermissive]\r\n  505 |             lhs0 = &lhs(i, j + 0);\r\n      |                    ^~~~~~~~~~~~~~\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:506:20: error: taking address of rvalue [-fpermissive]\r\n  506 |             lhs1 = &lhs(i, j + 1);\r\n      |                    ^~~~~~~~~~~~~~\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:508:20: error: taking address of rvalue [-fpermissive]\r\n  508 |             lhs0 = &lhs(i, j + 2);\r\n      |                    ^~~~~~~~~~~~~~\r\nexternal/eigen_archive/Eigen/src/Core/arch/AltiVec/MatrixProduct.h:509:20: error: taking address of rvalue [-fpermissive]\r\n  509 |             lhs1 = &lhs(i, j + 3);\r\n      |                    ^~~~~~~~~~~~~~\r\n```", "comments": ["I think this should be resolved now?", "@npanpaliya,\r\nLooks like issue is resolved, Take a look at similar thread [#45746](https://github.com/tensorflow/tensorflow/issues/45746#issuecomment-997969395). Thanks!"]}, {"number": 48662, "title": "Gradient penalty with mixed precision training", "body": "**System information**\r\n- TensorFlow version (you are using): 2.5.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nI haven't found a way to implement a stable gradient penalty with mixed precision. I think it comes from the use of second order gradient (the penalty is computed inside the overall loss GradientTape context). Whether I scale the gradient penalty or not (avoiding underflow), the global loss scale quickly decreases to 1 during training. I don't know if this feature necessarily requires a new TensorFlow feature, but at least a tip not mentionned in [the mixed precision guide](https://www.tensorflow.org/guide/mixed_precision).\r\n\r\n**Will this change the current api? How?**\r\nNo\r\n\r\n**Who will benefit with this feature?**\r\nAt least every programmer who would like to implement a Wassertein GAN with gradient penalty, which is now the most validated setup for GAN training actually.", "comments": ["Is this the same as https://github.com/tensorflow/tensorflow/issues/47156 ?", "I am unfamiliar with gradient penalty. @nluehr @benbarsdell do you know how to use gradient penalty with loss scaling?\r\n\r\n@bhack, as you mentioned, #47156 might be related. Given gradient penalty involves second order gradients, we should determine how to apply loss scaling when using gradient penalty.", "@bhack Like @reedwm saied, #47156 is related. Indeed, it seems that I don't have problems when I put all instance normalization layers in float32 precision (I should have said this in the first post). But the problem is also linked to the use of gradient penalty, because with other models, I manage to keep some instance normalization layers in mixed precision without having the loss scale decreasing to 1. Therefore, I think the issue is relevant and as @reedwm saied, it would be good to determine how to manage second order gradients in mixed precision training.", "If I see the example [WGAN_WP in keras](https://keras.io/examples/generative/wgan_gp/) they use a custom `train_step`.\r\nAre the functions in mixed precision [custom_training_loop](https://www.tensorflow.org/guide/mixed_precision?hl=en#training_the_model_with_a_custom_training_loop) section sufficent? \r\n\r\n", "Yes I saw both sites but the keras one doesn't use mixed precision and the tensorflow guide doesn't talk about scaling with second order gradient.", "So are you looking to something like https://pytorch.org/docs/stable/notes/amp_examples.html#gradient-penalty?\r\nThey are unscaling the grad_params to add the penalty to the loss. \r\n\r\n", "That's what I do now. But I don't manage to transpose well the Pytorch example you give in Tensorflow because it seems there isn't a GradientTape equivalent in the code. In my case, I have a GradientTape context which is used to compute the gradients for the gradient penalty inside an other GradientTape which computes the global loss as it is in [the Keras example you linked](https://keras.io/examples/generative/wgan_gp/). So, I don't see how I could adapt this Pytorch example to my case.", "So this is your code example in March right? https://stackoverflow.com/questions/66771093/tensorflow-mixed-precision-second-order-gradient-scaling", "Yes it is.", "In general, loss scaling can be applied as by scaling the loss and unscaling the gradients around every call to `tape.gradient` (the methods `Model.fit`, `Optimizer.minimize` and `Optimizer.get_gradients` do this for you automatically but for the examples ahead, I will use `tape.gradient`). For example, suppose gradient penalty is implemented as follows:\r\n\r\n```python\r\nwith tf.GradientTape() as outer_tape:\r\n  with tf.GradientTape() as inner_tape:\r\n    output = model(x)\r\n    loss = loss_fn(output)\r\n  grads = inner_tape.gradient(loss, weights)\r\n  grad_norm = tf.add_n([tf.reduce_sum(g ** 2) for g in grads])\r\n  grad_norm = tf.sqrt(grad_norm)\r\n  loss += grad_norm\r\ngrads = outer_tape.gradient(loss, weights)\r\nopt.apply_gradients(list(zip(grads, weights)))\r\n```\r\n\r\nTo apply loss scaling, before each call to `tape.gradient`, the loss must be scaled and after each call, the gradients must be unscaled. In the example below, each added line has a commented added to help indicate what has changed:\r\n\r\n```python\r\nopt = tf.keras.mixed_precision.LossScaleOptimizer(opt)         # Wrap opt in LSO\r\nwith tf.GradientTape() as outer_tape:\r\n  with tf.GradientTape() as inner_tape:\r\n    output = model(x)\r\n    loss = loss_fn(output)\r\n    scaled_loss = opt.get_scaled_loss(loss)                     # Scale loss                 \r\n  scaled_gradients = inner_tape.gradient(scaled_loss, weights)      \r\n  gradients = opt.get_unscaled_gradients(scaled_gradients)      # Unscale grads\r\n  grad_norm = tf.add_n([tf.reduce_sum(g ** 2) for g in gradients])\r\n  grad_norm = tf.sqrt(grad_norm)\r\n  loss += grad_norm\r\n  scaled_loss = opt.get_scaled_loss(loss)                       # Scale loss\r\nscaled_gradients = outer_tape.gradient(scaled_loss, weights)\r\ngradients = opt.get_unscaled_gradients(scaled_gradients)        # Unscale grads\r\nopt.apply_gradients(list(zip(gradients, weights)))\r\n```\r\n\r\nThis is the same as when computing single-order gradients, except that `tape.gradient` is called twice instead of once so the loss and gradients need to be scaled twice instead of once. I am still not familiar on gradient penalty and why it is done, but following the example above will ensure every tensor in an intermediate gradient is scaled exactly once (as what must always be done with any model when using mixed precision). This also matches what the PyTorch example does that was linked by @bhack above.", "@reedwm That's what I do currently and this implementation gives me problems with instance normalization layers in mixed-precision mode whereas I don't have these problems with same architectures but without gradient penalty. But maybe I have to admit I can't use instance normalization layers in mixed precision with gradient penalty, hoping I will not have problems with other mixed precision layers.", "In general, computing variance in float16 can cause issues since it may overflow or underflow, so it should always be computed in float32. Instance normalization computes variance, so either the variance within instance normalization must be done in float32 or the entire instance normalization itself must be float32.\r\n\r\nI'm not sure why you only run into issues with mixed precision instance normalization when using gradient penalty. Perhaps gradient penalty causes the variance to be more likely to overflow/underflow in float16.", "I've not tried It but probably you can take a look how mixed precision and gp was handled in https://github.com/bryanlimy/tf2-cyclegan", "@reedwn Yes I know the overflow issue in normalization layers due to mean and variance computing. But make all normalization calculations in float32 is very expansive in terms of memory and time. So I tried to keep a maximum of computation in float16. To be more precise, I observed which normalization layers in my models were likely to overflow by first running in full precision and checking values (checking if sum is in the float16 range) when computing a mean. Here is my layer code : \r\n\r\n```python\r\nclass InstanceNormalization(Layer):\r\n    \r\n    def __init__(self, epsilon=1e-3, mean_f32=False, mean_abs_dev_f32=False, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.epsilon=epsilon\r\n        self.mean_f32 = mean_f32\r\n        self.mean_abs_dev_f32 = mean_abs_dev_f32\r\n        \r\n    def build(self, batch_input_shape):\r\n        self.scale = self.add_weight(\r\n            name='scale',\r\n            shape=batch_input_shape[-1:],\r\n            initializer=random_normal_initializer(1,0.02),\r\n            trainable=True\r\n        )\r\n        self.offset = self.add_weight(\r\n            name='offset',\r\n            shape=batch_input_shape[-1:],\r\n            initializer=\"zeros\",\r\n            trainable=True\r\n        )\r\n        self.axis = range(1, len(batch_input_shape)-1)\r\n        super().build(batch_input_shape)\r\n        \r\n    def call(self, x):\r\n        if self.mean_f32:\r\n            mean = tf_cast(k_mean(tf_cast(x,\"float32\"), axis=self.axis, keepdims=True), \"float16\")\r\n        else:\r\n            mean = k_mean(x, axis=self.axis, keepdims=True)\r\n        dev = x-mean\r\n        if self.mean_abs_dev_f32:\r\n            mean_abs_dev = tf_cast(k_mean(tf_cast(k_abs(dev), \"float32\"), axis=self.axis, keepdims=True),\"float16\")\r\n        else:\r\n            mean_abs_dev = k_mean(k_abs(dev), axis=self.axis, keepdims=True)\r\n        normalized = dev / (mean_abs_dev+self.epsilon)\r\n        return self.scale*normalized + self.offset\r\n```\r\nAs you see in the `call` method, the two if/else control if mean computations are done in float32 or float16. Also note I use average absolute deviation instead of standard deviation to further prevent overflow as it's done [here](https://arxiv.org/abs/1802.09769).\r\n\r\n\r\n@bhack Yes I already saw this code and it seems that they don't use scaling/unscaling for the inner gradient tape. In addition, they use tensorflow normalization layers and I think computations are all done in float32 even if we set the global policy to mixed precision.\r\n", "By using average absolute deviation instead of standard deviation, you remove the risk of overflow, so I think float16 is OK.\r\n\r\nIf you are concerned about performance of running variance calculations in float32, consider [using XLA](https://www.tensorflow.org/xla). XLA can potentially remove the performance cost of doing the variance in float32, since it can fuse the cast-to-float16, the variance calculations, and the cast-to-float32. Also, XLA is faster in general and so will help performance in most cases.", "@reedwm Yes I already use XLA and indeed it saves a little bit of memory and computation time.", "As I feared about, it seems that problem occur even without float16 instance normalization (IN) layers. I tried running WGAN-GP with only float32 IN layers and the training was not good. I quickly got inf values and like before, the loss scale parameter quickly goes down to 1. Thus, I think the problem of mixed precision with second order gradient is a quite general problem."]}, {"number": 48661, "title": "tf.data.Dataset from_generator w/ keras.utils.Sequence doesn't trigger on_epoch_end", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: 11.0 / 8.0.4\r\n- GPU model and memory: RTX 3070\r\n\r\n**Describe the current behavior**\r\nI created a class from `tf.keras.util.Sequence` with `on_epoch_end()` function with a print inside it. Then I created a `tf.data.Dataset from_generator` using that class. Finally used that dataset as input for my `model.fit()`.\r\n\r\n**Describe the expected behavior**\r\nI expect the print inside `on_epoch_end()` appear on console, but it doesn't so I suppose it never trigger that function. \r\n\r\n**Standalone code to reproduce the issue**\r\nGenerator class:\r\n```\r\nclass HDF5Generator(tf.keras.utils.Sequence):\r\n    def __init__(self, hdf5_file):\r\n        print(\"GENERATED\")\r\n        self.hdf5 = h5py.File(hdf5_file, 'r')\r\n        self.indices = list(range(0, len(self.hdf5[\"samples\"])))\r\n        random.Random().shuffle(self.indices)\r\n\r\n    def __len__(self):\r\n        return len(self.hdf5[\"samples\"])\r\n\r\n    def __getitem__(self, idx):\r\n        return self.hdf5[\"samples\"][self.indices[idx]], self.hdf5[\"labels\"][self.indices[idx]]\r\n\r\n    def on_epoch_end(self):\r\n        print(\"SHUFFLE\")\r\n        random.Random().shuffle(self.indices)\r\n```\r\n\r\nUsed class in fit\r\n```\r\nd = tf.data.Dataset.from_generator(HDF5Generator, args=[dataset], output_signature=(...))\r\nd = d.batch(batch_size).prefetch(tf.data.AUTOTUNE).cache()\r\nmodel.fit(d, epochs=epochs)\r\n```\r\nThis is the output:\r\n```\r\nEpoch 1/40\r\nGENERATED\r\n1969/1969 [==============================] - 63s 29ms/step - loss: 0.4823 - accuracy: 0.8020 - f1_score: 0.7989\r\nEpoch 2/40\r\n1969/1969 [==============================] - 22s 11ms/step - loss: 0.1111 - accuracy: 0.9562 - f1_score: 0.9567\r\nEpoch 3/40\r\n1969/1969 [==============================] - 23s 12ms/step - loss: 0.0947 - accuracy: 0.9610 - f1_score: 0.9614\r\nEpoch 4/40\r\n```\r\nAs you can see no SHUFFLE string appear in output", "comments": ["@TheEnigmist ,\r\n\r\nI ran the code shared and face a different error, please find the [gist](https://colab.research.google.com/gist/tilakrayal/6e57541cc1214c25ace3f7de6eeb7cb0/48661.ipynb) here and share all dependencies to replicate the issue or share a colab gist with the reported error.\r\n\r\nThanks!", "> @TheEnigmist ,\r\n> \r\n> I ran the code shared and face a different error, please find the [gist](https://colab.research.google.com/gist/tilakrayal/6e57541cc1214c25ace3f7de6eeb7cb0/48661.ipynb) here and share all dependencies to replicate the issue or share a colab gist with the reported error.\r\n> \r\n> Thanks!\r\n\r\nSorry, I copied only part of my code. This is a [colab code](https://colab.research.google.com/drive/1biTnCIStzEF4doATjCASrNYjDhVsFb9J?usp=sharing) so now you can run it. \r\nAs you can see there is no output at the end of the epoch. I can put a raise Exception too and will never trigger!", "@TheEnigmist ,\r\n\r\nI do not have access to the link you have provided. Could you please provide the required permissions to view the files.\r\n\r\nThanks!", "> @TheEnigmist ,\r\n> \r\n> I do not have access to the link you have provided. Could you please provide the required permissions to view the files.\r\n> \r\n> Thanks!\r\n\r\nAdded view permission for the code. For those who don't want to access colab, the sample code is:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport random\r\nfrom tensorflow.keras import models\r\nfrom tensorflow.python.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\r\nfrom tensorflow import keras\r\n\r\nclass HDF5Generator(tf.keras.utils.Sequence):\r\n    def __init__(self):\r\n        print(\"GENERATED\")\r\n        self.samples = np.random.rand(10000,5,20)\r\n        self.labels = np.random.randint(2, size=(10000, 1))\r\n        self.indices = list(range(0, 10000))\r\n        random.Random().shuffle(self.indices)\r\n\r\n    def __len__(self):\r\n        return 10000\r\n\r\n    def __getitem__(self, idx):\r\n        return self.samples[self.indices[idx]], self.labels[self.indices[idx]]\r\n\r\n    def on_epoch_end(self):\r\n        print(\"SHUFFLE\")\r\n        raise Exception(\"AAA\")\r\n        random.Random().shuffle(self.indices)\r\n\r\nd = tf.data.Dataset.from_generator(HDF5Generator, output_signature=(tf.TensorSpec(shape=(5,20)), tf.TensorSpec(shape=(1,))))\r\nd = d.batch(32).prefetch(tf.data.AUTOTUNE).cache()\r\n\r\nmodel = models.Sequential()\r\nmodel.add(Conv1D(32, kernel_size=1, strides=1, activation=\"relu\", padding=\"same\", input_shape=(5,20)))\r\nmodel.add(MaxPooling1D(pool_size=3, padding=\"same\"))\r\nmodel.add(Conv1D(64, kernel_size=1, strides=1, activation=\"relu\", padding=\"same\", input_shape=(5,20)))\r\nmodel.add(MaxPooling1D(pool_size=3, padding=\"same\"))\r\nmodel.add(Flatten())\r\nmodel.add(Dense(1024, activation=\"relu\"))\r\nmodel.add(Dense(1, activation=\"softmax\"))\r\nmodel.compile(\r\n        optimizer=keras.optimizers.Adam(0.003),\r\n        loss=\"categorical_crossentropy\",\r\n        metrics=[\"accuracy\"],\r\n    )\r\nmodel.fit(d, epochs=5)\r\n```\r\n\r\nThe output is:\r\n```\r\nEpoch 1/5\r\nGENERATED\r\n313/313 [==============================] - 3s 7ms/step - loss: 0.0000e+00 - accuracy: 0.4886\r\nEpoch 2/5\r\n313/313 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.4886\r\nEpoch 3/5\r\n313/313 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.4886\r\nEpoch 4/5\r\n313/313 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.4886\r\nEpoch 5/5\r\n313/313 [==============================] - 1s 3ms/step - loss: 0.0000e+00 - accuracy: 0.4886\r\n```", "@jvishnuvardhan ,\r\n\r\nI was able to reproduce the issue in TF v2.4,2.5.0rc1 and nightly.Please find the [gist](https://colab.research.google.com/gist/tilakrayal/b2f26e0229ab391028177599c7a9965c/48661.ipynb) here.", "@TheEnigmist Based on a good [tutorial](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly) and [SO issue](https://stackoverflow.com/questions/62346965/input-pipeline-w-keras-utils-sequence-object-or-tf-data-dataset), i updated your code with one line to init `on_epoch_end` in the __init__ method.\r\n\r\nWith that modification, now the code throws Exception. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/b2e37b7cfb380af4cbfd73a6b315c4e3/48661.ipynb). Hope it helps. Thanks!", "Well that trigger `on_epoch_end()` only once at init time, but will not trigger it again.\r\n\r\nIf you remove the raise you will see that \"SHUFFLE\" print occurs only before first epoch but never at the end of each epoch, like should be.", "I found a workoround to trigger generator `on_epoch_end()`. I created a custom `Callback` class and on its `on_epoch_end()` calls generator's function.\r\n\r\n```\r\nclass CallbackOnEpochEnd(Callback):\r\n    def __init__(self, generator):\r\n        super(CallbackOnEpochEnd, self).__init__()\r\n        self.generator = generator\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        self.generator.on_epoch_end()\r\n\r\n[...]\r\n\r\ngenerator = HDF5Generator()\r\nd = tf.data.Dataset.from_generator(lambda: generator, output_signature=(tf.TensorSpec(shape=(5,20)), tf.TensorSpec(shape=(1,))))\r\n\r\n[...]\r\n\r\non_epoch_end_callback = CallbackOnEpochEnd(generator)\r\n\r\n[...]\r\n\r\nmodel.fit(d, epochs=5, callbacks=[on_epoch_end_callback])\r\n```", "@TheEnigmist, According to [this](https://github.com/tensorflow/tensorflow/issues/48661#issuecomment-838069103), your issue was resolved.Please feel free to close the issue if it is resolved? Thanks!", "@chunduriv Yes, but this is a workaround, still it doesn't work like explained in the docs. \r\nThe expected behaviour is that `on_epoch_end()` of the class `Sequence` is triggered without forcing it from a callback.\r\nIf you think this is like it works then I close this issue."]}, {"number": 48635, "title": "How to call tf.compat.v1 from the C++ API?", "body": "I have a frozen graph that was frozen in TensorFlow v1 from the python API, and I want to load it using TensorFlow v2.4 from the C++ API using something like tf.compat.v1 in python. I can do it in python with this script:\r\n```python\r\ndef wrap_frozen_graph(graph_def, inputs, outputs):\r\n  def _imports_graph_def():\r\n    tf.compat.v1.import_graph_def(graph_def, name=\"\")\r\n  wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, [])\r\n  import_graph = wrapped_import.graph\r\n  return wrapped_import.prune(\r\n      tf.nest.map_structure(import_graph.as_graph_element, inputs),\r\n      tf.nest.map_structure(import_graph.as_graph_element, outputs))\r\n\r\n\r\ngraph_def = tf.compat.v1.GraphDef()\r\nloaded = graph_def.ParseFromString(open(path,'rb').read())\r\n\r\n\r\nfrozen_func = wrap_frozen_graph(graph_def=graph_def,\r\n                                    inputs=[\"x:0\"],\r\n                                    outputs=[\"Identity:0\", \"Identity_1:0\"])\r\n```\r\nBut I didn't find any related document to do the same from the C++ API.\r\n", "comments": []}, {"number": 48629, "title": "Tesla V100 is slower than GTX 1660 super!?", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.8\r\n- CUDA/cuDNN version: CUDA 11.0, cuDNN 8.0.4\r\n- GPU model and memory: Tesla V100S-16Q / GTX 1660 super 8G\r\n\r\nRecently I'm working on a detection project. The network I'm using is efficientdet-d0. When I'm training my network on company's server, it is slower than training it on my desktop, by 40%~50%. \r\n\r\nFor example, with same code (same model, same data, same batch-size, same everything), my desktop (GTX 1660 super 8G) spends average 250ms each step, while on server (virtual machine, Tesla V100S-16Q), it needs average 360ms per step. And eventually, training time becomes hours more after many epochs.\r\n\r\nBoth task manage and nvidia-smi show that CUDA is working so I'm using GPU training. And my colleagues who use Darknet and Pytorch don't have such issue and they all speed up the training when using Tesla GPU.\r\n\r\nDoes the tf code need adjustment when using Tesla GPU?\r\n\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nThe efficientdet code is from [bubbliiiing](https://github.com/bubbliiiing/efficientdet-tf2), except that I'm using tf.keras.applications.EfficientNetB0 as backbone.\r\n", "comments": ["This is unusual.  Can you grab a profile using the [TensorBoard profiler](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras) to see if anything jumps out?"]}, {"number": 48624, "title": "tf.ragged.stack doesn't work for multidimensional tensors", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS BigSur\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): installed by pip\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\na =tf.constant([[[3,4,5],[3,4,5]],[[3,4,5],[3,4,5]],[[3,4,5],[3,4,5]],[[3,4,5],[3,4,5]]])\r\nb = tf.constant([[[3,3,4,5],[3,3,4,5],[3,3,4,5]],[[3,3,4,5],[3,3,4,5],[3,3,4,5]],[[3,3,4,5],[3,3,4,5],[3,3,4,5]]])\r\nprint(a.shape,b.shape)\r\nc = tf.ragged.stack([a,b],axis=0)\r\n```\r\nWhen I attempt to stack two tensors of same rank. If the rank is $$\\ge 2$$, it produces an error saying:\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [8,3] vs. shape[1] = [9,4] [Op:ConcatV2] name: concat`\r\n\r\nI can workaround this by making a, b both ragged tensors\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should stack them without problem\r\n\r\nI traced it into the definition of the tf.ragged.stack function; It appears that the code runs line 180. It seems to assume that I am trying to use normal tf.stack when I have all elements as tensors.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```\r\n  File \"/Users/michaelhyh/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/Users/michaelhyh/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/ragged/ragged_concat_ops.py\", line 120, in stack\r\n    return _ragged_stack_concat_helper(values, axis, stack_values=True)\r\n  File \"/Users/michaelhyh/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/ragged/ragged_concat_ops.py\", line 197, in _ragged_stack_concat_helper\r\n    return _ragged_stack_concat_axis_0(rt_inputs, stack_values)\r\n  File \"/Users/michaelhyh/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/ragged/ragged_concat_ops.py\", line 222, in _ragged_stack_concat_axis_0\r\n    concatenated_flat_values = array_ops.concat(flat_values, axis=0)\r\n  File \"/Users/michaelhyh/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py\", line 201, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"/Users/michaelhyh/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1677, in concat\r\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n  File \"/Users/michaelhyh/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1193, in concat_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/Users/michaelhyh/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [8,3] vs. shape[1] = [9,4] [Op:ConcatV2] name: concat\r\n```", "comments": ["@michaelyhuang23  I tried to reproduce the issue with 2 dimensional matrix and it is working fine  but  when i tried with 3  dimensional matrix, getting the same error as mentioned.  \r\n\r\nWas able to reproduce the issue with TF 2.4.1 and tf-nightly version as well. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/6d3ffd2c27100a04bc3d0e01436f9561/untitled63.ipynb).Thanks!", "I was able to reproduce the issue on colab using `TF2.6` and `TF-nightly(2.8.0-dev20211003)`. Please find the gist [here](https://colab.research.google.com/gist/chunduriv/da6e69298bb7e6f277e3636a0d7c4c0d/48624.ipynb) for reference.Thanks!"]}, {"number": 48614, "title": "Connectionist Temporal Classification with Maximum Entropy Regularization (EnCTC)", "body": "**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nThere is an extension to CTC (EnCTC) that introduces regularization in order to prevent \"peaky\" and overly confident distributions typical to CTC.\r\n\r\nhttps://papers.nips.cc/paper/2018/file/e44fea3bec53bcea3b7513ccef5857ac-Paper.pdf\r\nConnectionist Temporal Classification with Maximum Entropy Regularization\r\nHu Liu, Sheng Jin, Changshui Zhang\r\n\r\nThis approach has been shown to *\"consistently improve over the CTC baseline without the need to adjust training settings\"* on scene text recognition tasks.\r\n\r\n**Will this change the current api? How?**\r\n\r\nIt would likely extend `tf.nn.ctc_loss` to have an optional regularization argument.\r\n\r\n**Who will benefit with this feature?**\r\n\r\nAnyone who uses CTC in Recurrent Neural Networks, e.g. in OCR, handwriting recognition or speech recognition\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 48612, "title": "@tf.function triggers \"AttributeError: 'Tensor' object has no attribute 'ndim'\"", "body": "TensorFlow version: `v2.4.0-49-g85c8b2a817f 2.4.1`\r\n\r\n**Minimal Example**\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef f(a):\r\n    return a.ndim\r\n\r\nx = tf.zeros((3, 4))\r\nf(x)\r\n```\r\n\r\n**Describe the current behavior**\r\n```python\r\nAttributeError: 'Tensor' object has no attribute 'ndim'\r\n```\r\n\r\n**Describe the expected behavior**\r\n```python\r\n<tf.Tensor: shape=(), dtype=int32, numpy=2>\r\n```\r\n\r\n**This actually works**\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef f(a):\r\n    return len(a.shape)\r\n\r\nx = tf.zeros((3, 4))\r\nf(x)\r\n```\r\n\r\nThis problem occured in https://github.com/jonasrauber/eagerpy/issues/36\r\n", "comments": ["You are in graph mode I think you need something like:\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef f(a):\r\n    tf.print(tf.rank(a))\r\n    return tf.rank(a)\r\n\r\nx = tf.zeros((3, 4))\r\nf(x)\r\n```", "@bhack I am not sure I follow. Of course I can use other ways to achieve the same (see also my shape example), but the issue that I report is that the `ndim` attribute doesn't work in functions decorated with `@tf.function` and it should. Are you saying this is intended behavior? If so, I don't see why.", "Cause `ndim` is in the `EagerTensor` base class not in the `Tensor` class:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/8ce4ae914b6d2e2b7a324ebca3e9a1e5dfba074a/tensorflow/python/framework/ops.py#L1217", "Yes, that's what I think should be changed ;-)\r\nOr is there any reason this has to stay like it is?", "It was introduced 3 years ago https://github.com/tensorflow/tensorflow/commit/7da6a83b74f467929032dce95794ef0197d46b20. Probably just for numpy compatibility cause rank it is not implemented for `EagerTensor`:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/b7121255625688775e5d582d5bcead68b5650629/tensorflow/python/framework/ops.py#L1131-L1141", "In the end it boils down to this: the API should be consistent, so that the same code works with and without `@tf.function`.", "But in Graph execution `_rank` could be `None`. Do you want to handle `ndim` `None` in the graph case?\r\nhttps://github.com/tensorflow/tensorflow/blob/906c8d5c5b9dbd930b0c2d41c0fe8650b746b1cf/tensorflow/python/framework/ops.py#L548-L554\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/906c8d5c5b9dbd930b0c2d41c0fe8650b746b1cf/tensorflow/python/framework/ops.py#L308-L316", "Yes. If you want, you could still raise an `AttributeError`, but only selectively if `ndim` is actually unknown.\r\n\r\nIt doesn't make sense to delegate this to downstream code / libraries. Imho, they should have to care as little as possible about whether the code is eventually wrapped by `@tf.function` or not.\r\n", "@tensorflow/api-owners What do you think about this request?", "@jonasrauber \r\n\r\nCould you Please move this issue to closed status if is is resolved.Thank you\r\n\r\n\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@UsharaniPagadala Has there been an update that fixes this? ", "This is an inconsistency in the Tensor class. I see `ndim` was added to `EagerTensor` (forwarding to `.shape.ndims`), but not to the base `Tensor` class). That should be a straightforward fix.\r\n\r\nThe recommended method right now is `.shape.rank` and that should work consistently, but all these do need to be cleaned up.", "@jonasrauber As the API change is welcome do you want to submit a small PR?"]}, {"number": 48605, "title": "Make GreedyMemoryPlanner::PrintMemoryPlan accessible ", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- All distributions\r\n- Source\r\n- All version to date\r\n- Any platform with UART output\r\n\r\n**Describe the problem**\r\n\r\n[GreedyMemoryPlanner::PrintMemoryPlan()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/memory_planner/greedy_memory_planner.h#L84) prints information about head memory allocation to the console. It has proven quite useful for comparing memory usage by different model archtectures and understanding how to reduce memory use.\r\n\r\nWhile asking questions about memory use, I was advised to add a call to GreedyMemoryPlanner() to [MicroAllocator::CommitPlan()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/micro_allocator.cc#L345) this worked, but required casting the `MemoryPlanner *` to a `GreedyMemoryPlanner *`. We can avoid this cast by moving the call to PrintMemoryPlanner() up the call stack to [MicroAllocator::CommitStaticPlan](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/micro_allocator.cc#L1080). \r\n\r\nSince not all users will benefit from seeing the memory plan, and since it is reasonable to use the user to make this choice at compile time, I suggest adding a new preprocessor define that will, when present, cause the memory plan to be printed.\r\n\r\nThere is a sample PR at #48596 \r\n\r\nMy immediate motivation for this issue is to be allow the memory plan to be printed (when requested) for users of the https://github.com/google/CFU-Playground, and I am working on a non-open source project that would also benefit.\r\n\r\n**Please provide the exact sequence of commands/steps when you ran into the problem**\r\n\r\nnil.\r\n\r\n", "comments": []}, {"number": 48604, "title": "Support for OV7675 camera module", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution: Windows 10\r\n- TensorFlow installed from (source or binary): not applicable (using the Arduino_TensorFlowLite installed from the Arduino IDE)\r\n- Tensorflow version (commit SHA if source): Arduino_TensorFlowLite. 2.4.0-ALPHA\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano 33 BLE Sense\r\n\r\nThe image provider source file for the person detection example under TFLite Micro, extends support only for the ArduCam camera module OV2640. \r\n\r\nCan the support be extended to camera module OV7675, since the TinyML starter kit contains this camera module?\r\n\r\nMeanwhile, I made the following updates to hopefully get it up and running, but without any success:\r\n\r\nFile: Arduino\\libraries\\ArduCAM\\memorysaver.h\r\nUpdates: \r\n1. Uncomment #define ARDUCAM_SHIELD_V2\r\n2. Uncomment #define OV7675_CAM\r\n\r\nFile: Arduino\\libraries\\Arduino_TensorFlowLite\\examples\\person_detection\\arduino_image_provider.cpp\r\nUpdates:\r\n1. LOC 54: Replace OV2640_MINI_2MP_PLUS with OV7675_CAM\r\n2. LOC 65: Replace OV2640 with OV7675\r\n\r\nUpon uploading the compiled software to the microcontroller, the camera initialization fails with the following error:\r\nCan't communicate with Arducam\r\n\r\nI assume the registers are different for the OV7675 as compared to OV2640 and the arduino_image_provider.cpp needs to be updated with the correct register information.\r\n\r\nCould you please provide me a solution to this issue?\r\n\r\nThank you.\r\n\r\n", "comments": []}, {"number": 48601, "title": "Add elastic training for workers in PSv2", "body": "We are glad to have PSv2 last year. The single-client distributed training model is much simpler to use and more intuitive to reason able. We especially love the fault tolerance capability of the new strategy, as most industrial scenarios of the asynchronous training with parameter server usually involve dozens of workers and it's crucial to make sure the job keeps running.\r\n\r\nIn this PR, we will push the fault tolerance capability a step further -- we are adding elastic training for workers in PSv2, which means we could change the number of the workers during training. This would allow us to adjust the worker size by need and make better usage of the computational resources.\r\n\r\nFor this purpose, this PR mainly add 2 methods to the `CoordinatorCluster`:\r\n\r\n- `remove_worker(self, address)`: Remove a `Worker` from `coordinator._cluster` by its address.\r\n- `add_worker(self, address)`: Add a new `Worker` to `coordinator._cluster` by its address.\r\n\r\nNotice that we are leaving the start and stop of the worker servers to the plaform, for instance, KubeFlow.\r\n\r\nBefore discussing about the underlying mechanism, we'd like to split all `Worker`s in the `coordinator._cluster` to 2 states: running and stopped.\r\n\r\n- A running `Worker` is one that keeps on grabbing `Closure`s from `_closure_queue` to process. In other words, a `Worker` is running if its processing thread is running. A typical running `Worker` is one that is just initialized.\r\n\r\n- A stopped `Worker` is one that stops getting `Closure`s and whose processing thread has stopped (or `join`ed).\r\n\r\nWe could call `worker.stop()` to turn a running `Worker` to stopped. And to make a stopped `Worker` back to running, we added a `restart` method to `Worker` class. In `restart` method, the `Worker` will recreate a processing thread if it is stopped.\r\n\r\n```\r\n                     ----- stop ----->\r\n__init__ ->  running                   stopped\r\n                     <--- restart ---- \r\n```\r\n\r\nLet's come back to the mechanism of elastic training. In fact, we are just using the `connect_to_cluster` function. This great function could recreate the topology of the server in runtime. But because every `connect_to_cluster` will clean all server caches, which introduce overhead, instead of calling `connect_to_cluster` in every `remove_worker` and `add_worker`,\r\n\r\n- in `remove_worker`, we will only stop the `Worker` (by running `worker.stop()`) and add the stopped worker to a dict in `coordinator` called `_stopped_workers`, which is a map from the address of the stopped worker to its index;\r\n\r\n- in `add_worker`, if the address is in the `_stopped_workers`, restart the worker to running state (by running `worker.restart()`). Otherwise, we need to use `connect_to_cluster` to recreate the cluster. In this way, we will first remove the stopped workers from the cluster spec and clean the `_stopped_workers`. Then, add the new worker to the cluster spec and call `connect_to_cluster`. Finally, create a new `Worker` and append it to `coordinator._cluster`.\r\n\r\nWe add some auxiliary functions as well, for example, `add_task` and `remove_task` in `ClusterSpec`. And to support sparse worker_index (worker indices like [0, 2, 5]), we changed the `PerWorkerValues` to hold a dict.\r\n\r\nAs for the dataset part, we choose to create new iterator after `add_task`. This may be oversimplied. But the dataset part seems to be relatively isolated with the coordinator and can be updated in maybe another PR.\r\n\r\nA typical use case of the new apis is:\r\n\r\n```python\r\n...\r\nper_worker_train_ds = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\r\nper_worker_train_iter = iter(per_worker_train_ds)\r\n\r\nfor epoch in range(EPOCHS):\r\n    ...\r\n    if epoch == 0:\r\n        coordinator.remove_worker(\"localhost:2101\")\r\n\r\n    if epoch == 1:\r\n        coordinator.add_worker(\"localhost:2102\")\r\n        per_worker_train_ds = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\r\n        per_worker_train_iter = iter(per_worker_train_ds)\r\n\r\n    for i in range(step):\r\n        coordinator.schedule(step_fn,\r\n                              args=(per_worker_train_iter,))\r\n    coordinator.join()\r\n    ...\r\n```\r\n\r\nAny discussion on the design or any details of this PR is welcomed :). It will be great if you can give us some suggestion on the limitation of this design.\r\n\r\nAs for the test, we have tested this PR in multiple elastic scenarios. And we hope to discuss with you on how to integrate test for the elastic training as well.\r\n\r\nThank you for your time on reviewing this PR!\r\n\r\nGently ping @yuefengz @rchao.\r\n\r\ncc @zw0610\r\n", "comments": ["@rchao @yuefengz Could you take a look at this PR? Thank you.", "Hello zhuzilin@, sorry for the delayed response. Thank you for your PR!\r\n\r\nI think having the flexibility of including a worker as one machine becomes available and excluding one when it becomes unavailable is providing value, but I'd like to understand better how to use the APIs; specifically, who would be the caller of them. For example, I'm not sure how I would use those APIs in my training code if I have no information about the availability of resources when the code is being executed at a later time. In the typical use case described above, I'm not fully following why one would remove a worker at epoch 0 and add it back at epoch 1, for example.", "@rchao Thank you for your reply:) The sample code above is just a naive example of how these APIs can be called. In practice, we can create a custom resolver with kubeflow, which will watch the amount of available resources and update the cluster every n iteration.\r\n\r\nAs a starting point of the elastic ps training, I think we could settle the platform independent APIs first and then gradually add adapters to kubeflow, GKE, etc. This PR is intended to be small for better understanding of the mechanism. I wonder if you agree with using `connect_to_cluster` as the underlying mechanism for elastic training. Or if there are better alternatives. We would love to gradually add the missing pieces afterward.\r\n\r\nIf a cluster example is needed, I believe we can provide one. Maybe in a separate repo?\r\n\r\ncc. @zw0610", "Thanks @zhuzilin for the information! As for using `connect_to_cluster` for elastic training, I would defer to @haoyuz and @blueginko who can provide more insight here.", "@haoyuz, @blueGinko Can you please take a look on this PR ? Thanks!", "Hi,\r\n\r\nThere are some issues with \u201cconnect_to_cluster\u201d. It uses \u201cupdate_server_def\u201d which will hang there in some cases. For example, we may have such case:\r\n1. Run PS training in multiple server.\r\n2. Call \u2018update_server_def\u2019 to shrink the cluster.\r\n3. Kill unused. Server.\r\n4. Call \u2018update_server_def\u2019 to extend the cluster.\r\n\r\nAnother thing I want to mention, current cluster metadata appears in many place. It might be inconsistent if only one place is updated. This is something we need to fix.\r\n\r\nThanks,\r\nKai\r\n\r\nOn May 11, 2021, at 7:00 AM, gbaned ***@***.******@***.***>> wrote:\r\n\r\nCAUTION: The Sender of this email is not from within Dalhousie.\r\n\r\n\r\n@haoyuz<https://github.com/haoyuz>, @blueGinko<https://github.com/blueGinko> Can you please take a look on this PR ? Thanks!\r\n\r\n\u2014\r\nYou are receiving this because you were mentioned.\r\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/pull/48601#issuecomment-838526213>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ACS3TEEOLSXATZ6RNIZOCSLTNE2BBANCNFSM43EI6VVQ>.\r\n\r\n", "@blueGinko \r\nThank you for your information.\r\n\r\n> 1. Run PS training in multiple server.\r\n\r\nWe will only do elastic training for worker at the moment, just like the current fault tolerance mechanism. For PS, we would remain raising an error.\r\n\r\n> 2. Call \u2018update_server_def\u2019 to shrink the cluster.\r\n> 3. Kill unused. Server.\r\n> 4. Call \u2018update_server_def\u2019 to extend the cluster.\r\n\r\nActually we have successfully tried to shrink or extend the cluster with `update_server_def`. I think the underlying ClusterSpec Propagation could help with changing the size of the kernel. And the `update_server_def` will not be responsible for killing servers, those will be handled by the cluster orchestrator like kubeflow.\r\n\r\n> Another thing I want to mention, current cluster metadata appears in many place. It might be inconsistent if only one place is updated. This is something we need to fix.\r\n\r\nCould you share more information about the cluster metadata? Maybe we can contribute a fix :).", "Thanks Zilin for the PR!\r\n\r\nAt a high level, I like the idea of being able to dynamically adjust the number of workers in training.  From reading the code, my understanding is that the current implementation is basically that `remote_worker` temporarily suspends the thread that schedules closures to the specified worker, and `add_worker` resumes that thread.  I wonder if the current support has too many hidden assumptions that restrict the use cases.  For example, could you please explain how it works for the following cases:\r\n\r\n(1) At time T, start training with 2 workers. At T+1, expand the worker pool to include a third new worker (its IP:port is only known at T+1).\r\n(2) At time T, training with 2 workers (worker/0 with ip0:port0, worker/1 with ip1:port1). At T+1, the machine (ip0:port0) for worker/0 fails. At T+2, we replace it with a different machine (i.e., now worker/0 runs on ip2:port2).\r\n\r\nIt would be great if the proposal works for these scenarios. If possible, could you add unit tests to verify the solution?  Thank you!", "@zhuzilin Can you please resolve conflicts? Thanks!", "> Thanks Zilin for the PR!\r\n> \r\n> At a high level, I like the idea of being able to dynamically adjust the number of workers in training. From reading the code, my understanding is that the current implementation is basically that `remote_worker` temporarily suspends the thread that schedules closures to the specified worker, and `add_worker` resumes that thread. I wonder if the current support has too many hidden assumptions that restrict the use cases. For example, could you please explain how it works for the following cases:\r\n> \r\n> (1) At time T, start training with 2 workers. At T+1, expand the worker pool to include a third new worker (its IP:port is only known at T+1).\r\n> (2) At time T, training with 2 workers (worker/0 with ip0:port0, worker/1 with ip1:port1). At T+1, the machine (ip0:port0) for worker/0 fails. At T+2, we replace it with a different machine (i.e., now worker/0 runs on ip2:port2).\r\n> \r\n> It would be great if the proposal works for these scenarios. If possible, could you add unit tests to verify the solution? Thank you!\r\n\r\nFor the first case, I've tested with a demo [here](https://github.com/zw0610/ps-worker-elastic/tree/master/mnist-elastic-v2) and found the modification works for this case (except: 1. new workers have to join after next epoch starts; 2. some robustness issue).\r\n\r\nFor the second case, as we work with tf-operator, such 'worker renaming' issue never happens. But as you can consider, as long as the first case is supported, the second case is expected to be supported, too.\r\n\r\nLet me add some unit tests this week (or maybe next week).", "@zw0610, @zhuzilin  Any update on this PR? Please. Thanks!\r\n", "@zw0610, @zhuzilin Any update on this PR? Please. Thanks!", "All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F48601) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "@gbaned @haoyuz  Hi, I've patched this pr with an elastic test after rebasing the master branch. However, ~~I'm not sure why some CI failed since the modified files seems not related to the error messages~~ let's fix the CI issues later.\r\n\r\nFor users who wish to experiment with elastic training, I'll suggest use [this script](https://github.com/zw0610/ps-worker-elastic/blob/master/mnist-elastic-v2/worker.py) for the worker (and parameter server since they are identical when launching the instances in single-client mode). Meanwhile, the script for coordinator need a bit more customization given various training environment. We tested [this coordinator script](https://github.com/zw0610/ps-worker-elastic/blob/master/mnist-elastic-v2/coordinator.py) in a customized (kubeflow/tf-operator)[https://github.com/kubeflow/tf-operator] environment (1. accept tuning replcias; 2. grant kubernetes apiserver permission to coordinator).", "@gbaned @haoyuz Could you take another look at this pr? Thank you!", "@haoyuz  Can you please review this PR ? Thanks!", "@haoyuz Can you please review this PR ? Thanks!", "@haoyuz Can you please review this PR ? Thanks!", "@haoyuz Can you please review this PR ? Thanks!", "Apologies for the delayed response! The use of `remote.connect_to_cluster` looks good to me. For the rest of the changes in PS strategy, please ask @yuefengz and @rchao to take a look. Thank you!", "Thank you @haoyuz. \r\n @yuefengz and @rchao  Can you please take a look on this PR? Thanks!", "@yuefengz, @rchao Can you please take a look on this PR? Thanks!", "@yuefengz, @rchao Can you please take a look on this PR? Thanks!"]}, {"number": 48561, "title": "What does \u201dSets are not currently considered sequences, but this may change in the future, so consider avoiding using them\u201c mean?", "body": "This template is for miscellaneous issues not covered by the other issue categories.\r\n\r\nFor questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\n\r\nIf you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).\r\n\r\nFor high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.\r\n\r\nTensorflow-2.3.0\r\nEvery time when I saved model as :\r\n```\r\nmodel_train.save(FLAG.save_path,include_optimizer=False)\r\n```\r\nThe warming log showed that :\r\n```\r\nW tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n```\r\nIt actually from tensorflow/python/util/util.cc:\r\n```\r\nint IsSequenceHelper(PyObject* o) {\r\n  // We treat dicts and other mappings as special cases of sequences.\r\n  if (IsMappingHelper(o)) return true;\r\n  if (IsMappingViewHelper(o)) return true;\r\n  if (IsAttrsHelper(o)) return true;\r\n  if (PySet_Check(o) && !WarnedThatSetIsNotSequence) {\r\n    LOG(WARNING) << \"Sets are not currently considered sequences, \"\r\n                    \"but this may change in the future, \"\r\n                    \"so consider avoiding using them.\";\r\n    WarnedThatSetIsNotSequence = true;\r\n  }\r\n  static auto* const check_cache = new CachedTypeCheck([](PyObject* to_check) {\r\n    int is_instance = IsInstanceOfRegisteredType(to_check, \"Sequence\");\r\n\r\n    // Don't cache a failed is_instance check.\r\n    if (is_instance == -1) return -1;\r\n\r\n    return static_cast<int>(is_instance != 0 && !IsString(to_check));\r\n  });\r\n  return check_cache->CachedLookup(o);\r\n}\r\n```\r\nAlthough it does not influence the process of traing, I really want to know what does this warming mean?", "comments": ["@Zhuxinpei \r\n\r\nIn order to expedite the trouble-shooting process, please provide a code snippet to reproduce for the issue reported here. Thanks!\r\n\r\n\r\n", "```\r\nclass ST_block(tf.keras.Model):\r\n    def __init__(self,filters,name):\r\n        super().__init__(name=name)\r\n        self.conv = tf.keras.layers.Conv2D(filters[0],[3,3],[2,2],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.af = tf.keras.layers.ReLU(6)\r\n        self.bn = tf.keras.layers.BatchNormalization()\r\n        self.dropout = tf.keras.layers.Dropout(FLAG.drop_rate)\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        x = self.conv(inputs)\r\n        x = self.bn(x,training=training)\r\n        x = self.af(x)\r\n        x = self.dropout(x,training=training)\r\n        return x\r\n\r\nclass ID_block(tf.keras.Model):\r\n    def __init__(self,filters,name):\r\n        super().__init__(name=name)\r\n        self.conv1 = tf.keras.layers.Conv2D(filters[0],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv2 = tf.keras.layers.DepthwiseConv2D([3,3],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv3 = tf.keras.layers.Conv2D(filters[1],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.bn1 = tf.keras.layers.BatchNormalization()\r\n        self.bn2 = tf.keras.layers.BatchNormalization()\r\n        self.bn3 = tf.keras.layers.BatchNormalization()\r\n        self.af1 = tf.keras.layers.ReLU(6)\r\n        self.af2 = tf.keras.layers.ReLU(6)\r\n        self.dropout = tf.keras.layers.Dropout(FLAG.drop_rate)\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        x = self.conv1(inputs)\r\n        x = self.bn1(x,training=training)\r\n        x = self.af1(x)\r\n        x = self.conv2(x)\r\n        x = self.bn2(x,training=training)\r\n        x = self.af2(x)\r\n        x = self.conv3(x)\r\n        x = self.bn3(x,training=training)\r\n        x = self.dropout(x,training=training)\r\n        x += inputs\r\n        return x\r\n\r\nclass IDS_block(tf.keras.Model):\r\n    def __init__(self,filters,name):\r\n        super().__init__(name=name)\r\n        self.conv1 = tf.keras.layers.Conv2D(filters[0],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv2 = tf.keras.layers.DepthwiseConv2D([3,3],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv3 = tf.keras.layers.Conv2D(filters[1],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.bn1 = tf.keras.layers.BatchNormalization()\r\n        self.bn2 = tf.keras.layers.BatchNormalization()\r\n        self.bn3 = tf.keras.layers.BatchNormalization()\r\n        self.af1 = tf.keras.layers.ReLU(6)\r\n        self.af2 = tf.keras.layers.ReLU(6)\r\n        self.dropout = tf.keras.layers.Dropout(FLAG.drop_rate)\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        x = self.conv1(inputs)\r\n        x = self.bn1(x,training=training)\r\n        x = self.af1(x)\r\n        x = self.conv2(x)\r\n        x = self.bn2(x,training=training)\r\n        x = self.af2(x)\r\n        x = self.conv3(x)\r\n        x = self.bn3(x,training=training)\r\n        x = self.dropout(x,training=training)\r\n        return x\r\n\r\nclass DS_block(tf.keras.Model):\r\n    def __init__(self,filters,name):\r\n        super().__init__(name=name)\r\n        self.conv1 = tf.keras.layers.Conv2D(filters[0],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv2 = tf.keras.layers.DepthwiseConv2D([3,3],[2,2],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv3 = tf.keras.layers.Conv2D(filters[1],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.bn1 = tf.keras.layers.BatchNormalization()\r\n        self.bn2 = tf.keras.layers.BatchNormalization()\r\n        self.bn3 = tf.keras.layers.BatchNormalization()\r\n        self.af1 = tf.keras.layers.ReLU(6)\r\n        self.af2 = tf.keras.layers.ReLU(6)\r\n        self.dropout = tf.keras.layers.Dropout(FLAG.drop_rate)\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        x = self.conv1(inputs)\r\n        x = self.bn1(x,training=training)\r\n        x = self.af1(x)\r\n        x = self.conv2(x)\r\n        x = self.bn2(x,training=training)\r\n        x = self.af2(x)\r\n        x = self.conv3(x)\r\n        x = self.bn3(x,training=training)\r\n        x = self.dropout(x,training=training)\r\n        return x\r\n\r\nclass SPP_block(tf.keras.Model):\r\n    def __init__(self,name):\r\n        super().__init__(name=name)\r\n        self.pool1 = tf.keras.layers.MaxPool2D([6,6],[1,1],'same')\r\n        self.pool2 = tf.keras.layers.MaxPool2D([4,4],[1,1],'same')\r\n        self.pool3 = tf.keras.layers.MaxPool2D([2,2],[1,1],'same')\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        x1 = self.pool1(inputs)\r\n        x2 = self.pool2(inputs)\r\n        x3 = self.pool3(inputs)\r\n        x = tf.keras.layers.concatenate([x1,x2,x3,inputs],axis=-1)\r\n        return x\r\n\r\nclass RB_block(tf.keras.Model):\r\n    def __init__(self,name):\r\n        super().__init__(name=name)\r\n        self.upsample = tf.keras.layers.UpSampling2D([2,2],interpolation='bilinear')\r\n        self.bn = tf.keras.layers.BatchNormalization()\r\n        self.af = tf.keras.layers.ReLU(6)\r\n        self.dropout = tf.keras.layers.Dropout(FLAG.drop_rate)\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        x = self.upsample(inputs)\r\n        x = self.bn(x,training=training)\r\n        x = self.af(x)\r\n        x = self.dropout(x,training=training)\r\n        return x\r\n\r\nclass NECK_block(tf.keras.Model):\r\n    def __init__(self,filters,name):\r\n        super().__init__(name=name)\r\n        self.conv = tf.keras.layers.Conv2D(filters[0],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.bn = tf.keras.layers.BatchNormalization()\r\n        self.af = tf.keras.layers.ReLU(6)\r\n        self.dropout = tf.keras.layers.Dropout(FLAG.drop_rate)\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        x = self.conv(inputs)\r\n        x = self.bn(x,training=training)\r\n        x = self.af(x)\r\n        x = self.dropout(x,training=training)\r\n        return x\r\n\r\nclass HEAD_block(tf.keras.Model):\r\n    def __init__(self,filters,name):\r\n        super().__init__(name=name)\r\n        self.conv1 = tf.keras.layers.Conv2D(filters[0],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv2 = tf.keras.layers.DepthwiseConv2D([5,5],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv3 = tf.keras.layers.Conv2D(filters[1],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv4 = tf.keras.layers.DepthwiseConv2D([5,5],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv5 = tf.keras.layers.Conv2D(filters[2],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.conv6 = tf.keras.layers.Conv2D(filters[3],[1,1],[1,1],'same',use_bias=False,kernel_regularizer=tf.keras.regularizers.L2(FLAG.L2_rate))\r\n        self.bn1 = tf.keras.layers.BatchNormalization()\r\n        self.bn2 = tf.keras.layers.BatchNormalization()\r\n        self.bn3 = tf.keras.layers.BatchNormalization()\r\n        self.bn4 = tf.keras.layers.BatchNormalization()\r\n        self.bn5 = tf.keras.layers.BatchNormalization()\r\n        self.af1 = tf.keras.layers.ReLU(6)\r\n        self.af2 = tf.keras.layers.ReLU(6)\r\n        self.af3 = tf.keras.layers.ReLU(6)\r\n        self.af4 = tf.keras.layers.ReLU(6)\r\n        self.af5 = tf.keras.layers.ReLU(6)\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        x = self.conv1(inputs)\r\n        x = self.bn1(x,training=training)\r\n        x = self.af1(x)\r\n        x = self.conv2(x)\r\n        x = self.bn2(x,training=training)\r\n        x = self.af2(x)\r\n        x = self.conv3(x)\r\n        x = self.bn3(x,training=training)\r\n        x = self.af3(x)\r\n        x = self.conv4(x)\r\n        x = self.bn4(x,training=training)\r\n        x = self.af4(x)\r\n        x = self.conv5(x)\r\n        x = self.bn5(x,training=training)\r\n        x = self.af5(x)\r\n        x = self.conv6(x)\r\n        return x\r\n\r\nclass Net(tf.keras.Model):\r\n    def __init__(self):\r\n        super().__init__()\r\n        # [batch,288,512,3]\r\n        self.ST1 = ST_block([16],'ST1') # [3,3,3,16] | [batch,144,256,16]\r\n        self.ID2 = ID_block([96,16],'ID2') # [1,1,16,96],[3,3,96,1],[1,1,96,16] | [batch,144,256,16]\r\n        self.ID3 = ID_block([96,16],'ID3') # [1,1,16,96],[3,3,96,1],[1,1,96,16] | [batch,144,256,16]\r\n        self.DS4 = DS_block([96,24],'DS4') # [1,1,16,96],[3,3,96,1],[1,1,96,24] | [batch,72,128,24]\r\n        self.ID5 = ID_block([144,24],'ID5') # [1,1,24,144],[3,3,144,1],[1,1,144,24] | [batch,72,128,24]\r\n        self.ID6 = ID_block([144,24],'ID6') # [1,1,24,144],[3,3,144,1],[1,1,144,24] | [batch,72,128,24]\r\n        self.DS7 = DS_block([144,32],'DS7') # [1,1,24,144],[3,3,144,1],[1,1,144,32] | [batch,36,64,32]\r\n        self.ID8 = ID_block([192,32],'ID8') # [1,1,32,192],[3,3,192,1],[1,1,192,32] | [batch,36,64,32]\r\n        self.ID9 = ID_block([192,32],'ID9') # [1,1,32,192],[3,3,192,1],[1,1,192,32] | [batch,36,64,32]\r\n        self.ID10 = ID_block([192,32],'ID10') # [1,1,32,192],[3,3,192,1],[1,1,192,32] | [batch,36,64,32]\r\n        self.DS11 = DS_block([192,64],'DS11') # [1,1,32,192],[3,3,192,1],[1,1,192,64] | [batch,18,32,64]\r\n        self.ID12 = ID_block([384,64],'ID12') # [1,1,64,384],[3,3,384,1],[1,1,384,64] | [batch,18,32,64]\r\n        self.ID13 = ID_block([384,64],'ID13') # [1,1,64,384],[3,3,384,1],[1,1,384,64] | [batch,18,32,64]\r\n        self.IDS14 = IDS_block([384,96],'IDS14') # [1,1,64,384],[3,3,384,1],[1,1,384,96] | [batch,18,32,96]\r\n        self.ID15 = ID_block([576,96],'ID15') # [1,1,96,576],[3,3,576,1],[1,1,576,96] | [batch,18,32,96]\r\n        self.ID16 = ID_block([576,96],'ID16') # [1,1,96,576],[3,3,576,1],[1,1,576,96] | [batch,18,32,96]\r\n        self.DS17 = DS_block([576,160],'DS17') # [1,1,96,576],[3,3,576,1],[1,1,576,160] | [batch,9,16,160]\r\n        self.SPP18 = SPP_block('SPP18') # [batch,9,16,640]\r\n        self.IDS19 = IDS_block([576,160],'IDS19') # [1,1,640,576],[3,3,576,1],[1,1,576,160] | [batch,9,16,160]\r\n        self.HEAD20 = HEAD_block([576,384,192,51],'HEAD20') # [1,1,160,576],[5,5,576,1],[1,1,576,384],[5,5,384,1],[1,1,384,192],[1,1,192,51] | [batch,9,16,51]\r\n        self.RB21 = RB_block('RB21') # [batch,18,32,160]\r\n        self.NECK22 = NECK_block([160],'NECK22') # [1,1,96,160] | [batch,18,32,160]\r\n        # CONCATE [batch,18,32,320]\r\n        self.HEAD24 = HEAD_block([384,192,144,51],'HEAD24') # [1,1,320,384],[5,5,384,1],[1,1,384,192],[5,5,192,1],[1,1,192,144],[1,1,144,51] | [batch,18,32,51]\r\n        self.RB25 = RB_block('RB25') # [batch,36,64,320]\r\n        self.NECK26 = NECK_block([80],'NECK26') # [1,1,32,80] | [batch,36,64,80]\r\n        # CONCATE [batch,36,64,400]\r\n        self.HEAD28 = HEAD_block([192,144,96,51],'HEAD28') # [1,1,400,192],[5,5,192,1],[1,1,192,144],[5,5,144,1],[1,1,144,96],[1,1,96,51] | [batch,36,64,51]\r\n\r\n    def call(self,inputs,training=False,mask=None):\r\n        # [batch,288,512,3]\r\n        ST1 = self.ST1(inputs,training) # [3,3,3,16] | [batch,144,256,16]\r\n        ID2 = self.ID2(ST1,training) # [1,1,16,96],[3,3,96,1],[1,1,96,16] | [batch,144,256,16]\r\n        ID3 = self.ID3(ID2,training) # [1,1,16,96],[3,3,96,1],[1,1,96,16] | [batch,144,256,16]\r\n        DS4 = self.DS4(ID3,training) # [1,1,16,96],[3,3,96,1],[1,1,96,24] | [batch,72,128,24]\r\n        ID5 = self.ID5(DS4,training) # [1,1,24,144],[3,3,144,1],[1,1,144,24] | [batch,72,128,24]\r\n        ID6 = self.ID6(ID5,training) # [1,1,24,144],[3,3,144,1],[1,1,144,24] | [batch,72,128,24]\r\n        DS7 = self.DS7(ID6,training) # [1,1,24,144],[3,3,144,1],[1,1,144,32] | [batch,36,64,32]\r\n        ID8 = self.ID8(DS7,training) # [1,1,32,192],[3,3,192,1],[1,1,192,32] | [batch,36,64,32]\r\n        ID9 = self.ID9(ID8,training) # [1,1,32,192],[3,3,192,1],[1,1,192,32] | [batch,36,64,32]\r\n        ID10 = self.ID10(ID9,training) # [1,1,32,192],[3,3,192,1],[1,1,192,32] | [batch,36,64,32]\r\n        DS11 = self.DS11(ID10,training) # [1,1,32,192],[3,3,192,1],[1,1,192,64] | [batch,18,32,64]\r\n        ID12 = self.ID12(DS11,training) # [1,1,64,384],[3,3,384,1],[1,1,384,64] | [batch,18,32,64]\r\n        ID13 = self.ID13(ID12,training) # [1,1,64,384],[3,3,384,1],[1,1,384,64] | [batch,18,32,64]\r\n        IDS14 = self.IDS14(ID13,training) # [1,1,64,384],[3,3,384,1],[1,1,384,96] | [batch,18,32,96]\r\n        ID15 = self.ID15(IDS14,training) # [1,1,96,576],[3,3,576,1],[1,1,576,96] | [batch,18,32,96]\r\n        ID16 = self.ID16(ID15,training) # [1,1,96,576],[3,3,576,1],[1,1,576,96] | [batch,18,32,96]\r\n        DS17 = self.DS17(ID16,training) # [1,1,96,576],[3,3,576,1],[1,1,576,160] | [batch,9,16,160]\r\n        SPP18 = self.SPP18(DS17,training) # [batch,9,16,640]\r\n        IDS19 = self.IDS19(SPP18,training) # [1,1,640,960],[3,3,960,1],[1,1,960,160] | [batch,9,16,160]\r\n        HEAD20 = self.HEAD20(IDS19,training) # [1,1,160,576],[5,5,576,1],[1,1,576,144],[5,5,144,1],[1,1,144,96],[1,1,96,51] | [batch,9,16,51]\r\n        RB21 = self.RB21(IDS19,training) # [batch,18,32,160]\r\n        NECK22 = self.NECK22(ID16,training) # [1,1,96,160] | [batch,18,32,160]\r\n        CONCATE23 = tf.keras.layers.concatenate([RB21,NECK22],axis=-1) # [batch,18,32,320]\r\n        HEAD24 = self.HEAD24(CONCATE23,training) # [1,1,320,384],[5,5,384,1],[1,1,384,192],[5,5,192,1],[1,1,192,144],[1,1,144,51] | [batch,18,32,51]\r\n        RB25 = self.RB25(CONCATE23,training) # [batch,36,64,320]\r\n        NECK26 = self.NECK26(ID10,training) # [1,1,32,80] | [batch,36,64,80]\r\n        CONCATE27 = tf.keras.layers.concatenate([RB25,NECK26],axis=-1) # [batch,36,64,400]\r\n        HEAD28 = self.HEAD28(CONCATE27,training) # [1,1,400,192],[5,5,192,1],[1,1,192,144],[5,5,144,1],[1,1,144,96],[1,1,96,51] | [batch,36,64,51]\r\n        return HEAD20,HEAD24,HEAD28\r\n\r\ndef lossing(label_bbox,pred_bbox,feature_index):\r\n    with tf.name_scope('Loss'):\r\n        pred_bbox_temp1 = tf.reshape(pred_bbox,[-1,FLAG.feature_list[feature_index][1],FLAG.feature_list[feature_index][0],FLAG.num_anchor,5+len(FLAG.class_list)])\r\n        label_bbox_temp1 = tf.reshape(label_bbox,[-1,FLAG.feature_list[feature_index][1],FLAG.feature_list[feature_index][0],FLAG.num_anchor,5+len(FLAG.class_list)])\r\n\r\n        pred_xy,pred_wh,pred_conf,pred_cls = tf.split(pred_bbox_temp1,[2,2,1,len(FLAG.class_list)],axis=-1)\r\n        label_xy,label_wh,label_conf,label_cls = tf.split(label_bbox_temp1,[2,2,1,len(FLAG.class_list)],axis=-1)\r\n\r\n        obj_mask_1 = label_conf\r\n        obj_mask_2 = tf.squeeze(label_conf,axis=-1)\r\n\r\n        xy_loss = tf.reduce_sum(tf.square(label_xy-pred_xy)*obj_mask_1)/FLAG.batch_size\r\n        wh_loss = tf.reduce_sum(tf.square(label_wh-pred_wh)*obj_mask_1)/FLAG.batch_size\r\n\r\n        conf_loss_pos = tf.nn.sigmoid_cross_entropy_with_logits(labels=label_conf,logits=pred_conf)*obj_mask_1\r\n        conf_loss_neg = tf.nn.sigmoid_cross_entropy_with_logits(labels=label_conf,logits=pred_conf)*(1-obj_mask_1)\r\n        conf_loss = tf.reduce_sum(conf_loss_pos+conf_loss_neg)/FLAG.batch_size\r\n\r\n        cls_loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=label_cls,logits=pred_cls)*obj_mask_2)/FLAG.batch_size\r\n    return xy_loss + wh_loss + conf_loss + cls_loss\r\n\r\ndef session():\r\n    model_train = Net()\r\n\r\n    global_step = FLAG.num_data//FLAG.batch_size*FLAG.epoch\r\n    learning_rate = tf.keras.experimental.CosineDecayRestarts(FLAG.learning_rate,global_step//15,t_mul=2.0,m_mul=0.4,alpha=0.0)\r\n\r\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\r\n\r\n    dataExtractTrain = Data_Extract(FLAG.tfrecord_path_train,FLAG.batch_size)\r\n    data_pack_train = dataExtractTrain.TFRecord_unpack()\r\n\r\n    os.makedirs(FLAG.save_path)\r\n    for epoch_iter in range(FLAG.epoch):\r\n\r\n        lossTrain = []\r\n\r\n        for batch_iter,label_train in enumerate(data_pack_train.take(60)):\r\n            cur_step = epoch_iter * (FLAG.num_data//FLAG.batch_size) + (batch_iter)\r\n            image_train = tf.cast(label_train[0],tf.float32)\r\n            with tf.GradientTape() as tape:\r\n                pred_bbox_16_9_train,pred_bbox_32_18_train,pred_bbox_64_36_train = model_train(z_score(image_train),training=True)\r\n                loss_16_9_train = lossing(label_train[1],pred_bbox_16_9_train,0)\r\n                loss_32_18_train = lossing(label_train[2],pred_bbox_32_18_train,1)\r\n                loss_64_36_train = lossing(label_train[3],pred_bbox_64_36_train,2)\r\n                l2_loss = tf.reduce_sum(model_train.losses)\r\n                loss_train = loss_16_9_train + loss_32_18_train + loss_64_36_train + l2_loss\r\n\r\n            grads = tape.gradient(loss_train,model_train.trainable_variables)\r\n            optimizer.apply_gradients(grads_and_vars=zip(grads,model_train.trainable_variables))\r\n            lossTrain.append(loss_train.numpy())\r\n\r\n            print('Step:',cur_step,'lossBacth:',loss_train.numpy())\r\n\r\n        model_train.save(FLAG.save_path,include_optimizer=False)\r\n```", "@Zhuxinpei \r\nI ran the code shared but didn't face any error/warning, please find the [gist](https://colab.research.google.com/gist/UsharaniPagadala/d090cd530b3df7b5bd37c95d541722cd/48561.ipynb#scrollTo=j1FJHUXHjHTs) here.Could you please let us know if it helps.Thanks .\r\n\r\n", "This warning is happening on both Tensorflow 2.6 and 2.7, and it seems to happen on just about any model save, with or without training. The code below reproduces the error when run. I'm using python 3.8 on a mac.\r\nThe warning is worrying because it sounds like it's setting up to cause an error at some stage after a TF upgrade.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nif __name__==\"__main__\":\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.InputLayer(input_shape=(52,)),\r\n        tf.keras.layers.Dense(100, activation='relu'),\r\n        tf.keras.layers.Dense(2, activation='softmax')])\r\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n    print(f\"Tensorflow version {tf.__version__}\")\r\n    model.save(\"Testsave\")\r\n```\r\nResults in:\r\nTensorflow version 2.7.0\r\n2022-01-15 11:44:39.637419: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\n", "I was hoping that message would reopen this issue - can someone (@Zhuxinpei ?) reopen this issue?", "I had exactly the same error in TensorFlow version 2.7.0", "I downgraded to 2.5.2, have the same error: \r\n`W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.`", "Thanks for reopening this. Here's another two four-line pieces of code to repro the warning in TF 2.8.0-rc1:\r\n```python\r\nimport tensorflow as tf\r\nmodel = tf.keras.applications.mobilenet.MobileNet(weights=None, classes=1)\r\nmodel.compile(optimizer=\"sgd\", loss=\"mse\")\r\nmodel.save(\"tmp.tf\")\r\n```\r\n\r\n```python\r\nimport tensorflow as tf\r\nmodel = tf.keras.models.Sequential([tf.keras.layers.InputLayer(input_shape=(1,))])\r\nmodel.compile(optimizer=\"sgd\", loss=\"mse\")\r\nmodel.save(\"tmp.tf\")\r\n```", "I also got the same warning: \r\n```\r\nEpoch 1/2\r\n18/18 [==============================] - 19s 333ms/step - loss: 1.5840 - f1_score: 0.2127 - val_loss: 0.7382 - val_f1_score: 0.3385\r\nEpoch 00001: val_f1_score improved from -inf to 0.33850, saving model to ./Saved_model/hospital/best_model_checkpoint\r\n2022-02-01 13:16:13.039510: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\r\nEpoch 2/2\r\n18/18 [==============================] - 4s 208ms/step - loss: 0.9765 - f1_score: 0.2757 - val_loss: 0.5347 - val_f1_score: 0.3729\r\nEpoch 00002: val_f1_score improved from 0.33850 to 0.37292, saving model to ./Saved_model/hospital/best_model_checkpoint\r\n```\r\nAny solution?", "This looks like an internal bug. The original author didn't explain, but it appears the warning cautions users not to pass `sets` to `tf.nest`, as they will not be flattened, as one may (or may not) expect. But since you don't seem to be using sets anywhere in your code, this must be an internal issue. Might also be worth double-checking that none of your features contain sets.", "> This looks like an internal bug. The original author didn't explain, but it appears the warning cautions users not to pass `sets` to `tf.nest`, as they will not be flattened, as one may (or may not) expect. But since you don't seem to be using sets anywhere in your code, this must be an internal issue. Might also be worth double-checking that none of your features contain sets.\r\n\r\nThank you for the response. I did not get your point completely. May I have some example on `sets`  `tf.nest`. Thank you\r\n", "Compare `tf.nest.flatten([[1, 2], [3]])` with `tf.nest.flatten([{1, 2}, {3}])`. This flattening happens in various places of TensorFlow, such as arguments of `tf.function`. ", "Indeed. I think the warning was added because there were concerns that users may confuse about the different treatment of list/tuple and set. The distinction is sort of handy -- we don't otherwise have a good representation of a collection that is not a nested structure.\r\n\r\nMy take on this is that conceptually, only when the collection has a 'stable' order it can be proper modeled as a nested structure. `set` is not one of those. Unfortunately `tf.nest` make an important exception to this rule by allowing unordered `dict` objects as nested structures (we found a a way to impose an ad-hoc, 'stable' ordering to dict). \r\n\r\nI've quoted 'stable' as I am using the term in a very loose way.\r\n\r\nI will file a fix to remove this warning message, and also update [tf.nest's documentation](https://www.tensorflow.org/api_docs/python/tf/nest) to mention this contrast `{}` vs `[]`. \r\n\r\nFYI: the warning is from a `tf.Module` attempting the flattening its members. With the second reproducer in https://github.com/tensorflow/tensorflow/issues/48561#issuecomment-1021089823, the triggering object is a `set([])`. \r\n"]}, {"number": 48559, "title": "Unexpected behavior of soft device placement", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\nSoft device placement by default is set to True. Despite this running the code below causes it to fail with error:\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Could not satisfy device specification '/job:localhost/replica:0/task:0/device:dummy:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:CPU:0]. [Op:Fill]\r\n\r\nEven though tf.config.get_soft_device_placement() returns True in the error we can see \"enable_soft_placement=0\"\r\nWA for this issue is to explicitly call  tf.config.set_soft_device_placement(True)\r\n\r\n**Describe the expected behavior**\r\nSoft device placement should work as indicated by tf.config.get_soft_device_placement()\r\nAlso tf.device inside tf.function should not have such side effects\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n    # tf.config.set_soft_device_placement(True) # Uncomment this for WA\r\n    @tf.function\r\n    def func():\r\n        with tf.device(\"/device:CPU:0\"): # this tf.device within tf.function breaks soft placement in eager mode later\r\n            return tf.constant([1])    \r\n    func()\r\n\r\n    print(\"tf.config.get_soft_device_placement() = \", tf.config.get_soft_device_placement()) # Always prints True\r\n\r\n    # Commenting code above this line makes code below execute correctly\r\n    with tf.device(\"/device:dummy:0\"):\r\n        tf.fill([1, 1], 1) # Error\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF 2.4 and Nightly versions. Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/3669db70f912c088edc063ba30b32cb7/-48559.ipynb). Thanks!", "I was able to reproduce the issue on colab using `TF2.6` and `TF-nightly(2.8.0-dev20211003)`.Please find the gist [here](https://colab.research.google.com/gist/chunduriv/4aa228014b98f491e10e9eea12d846b7/48559.ipynb) for reference.Thanks!"]}, {"number": 48555, "title": "InvalidArgumentError when using class_weight in Model.fit with labels having extra dimension for time step", "body": "We used the `class_weight` parameter to assign different weights for samples of different classes in `keras.Model.fit()`.\r\nOur data (input) and labels (output) have an extra time-step dimension besides the batch dimension.\r\nThe labels are indices of classes (i.e. not one-hot encoded), used along with the **sparse** CE loss function.\r\nWe've written a minimal reproduction script (as presented below) to simplify the situation.\r\n\r\nIt looks like that `class_weight` is only designed for outputs of shape (batch_dim, n_classes).\r\n\r\nPossible workarounds:\r\n 1. convert `class_weight` to `sample_weight`\r\n 2. collapse time-step axis into batch axis using `tf.reshape()` in a `Lambda` layer, which would mess the code up\r\n\r\nI've noticed there are relevant issues but they are left there and closed as the authors did not reply in time.\r\n\r\n**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 16.04\r\n- TensorFlow installed from (source or binary): Anaconda, binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.5\r\n- CUDA/cuDNN version: cudnn 7.6.5 cuda10.1_0.conda (irrelevant)\r\n- GPU model and memory: Tesla K40c, 11441MiB (irrelevant)\r\n\r\n\r\n**Describe the current behavior**\r\nCrashed on `model.fit` with the following exception (traceback appended at the end as it is too long):\r\n\r\n> InvalidArgumentError: indices[0] = 3 is not in [0, 2)\r\n>\t [[{{node GatherV2}}]] [Op:IteratorGetNext]\r\n\r\nThe training progress bar did not appear.\r\n\r\n**Describe the expected behavior**\r\nComplete the training, though the minimal reproduction script has no actual trainable parameter.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ndef test_class_weight_error():\r\n    # the model simply return the input time_step*n_class=20x2 data as-is\r\n    model = keras.Sequential([keras.layers.Reshape((20, 2), input_shape=(20, 2))])\r\n    # run_eagerly improves the readability of the traceback a bit\r\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', run_eagerly=True)\r\n    model.summary()\r\n    # X (inputs, as well as y_pred): samples*time_step*n_classes=15x20x2\r\n    xs = tf.reshape(tf.one_hot(tf.ones(300, dtype=tf.int32), 2), [-1, 20, 2]).numpy()\r\n    # Y (labels i.e. y_true): samples*time_step=15x20, class labels of 0 or 1\r\n    ys = np.ones([15, 20], dtype=np.int32)\r\n    # without the line below (a.k.a. all labels being 1) there's no exception\r\n    ys[:,:3] = 0\r\n    # here's the crash\r\n    model.fit(xs, ys, batch_size=3, class_weight={0:1.,1:1.})\r\n    \r\ntest_class_weight_error()\r\n```\r\n\r\n**Other info / logs** \r\n\r\nTraceback:\r\n[traceback.txt](https://github.com/tensorflow/tensorflow/files/6323917/traceback.txt)\r\n\r\nModel Structure (in minimal repro script):\r\n > Model: \"sequential_64\"\r\n > Layer \\(type\\)                 Output Shape              Param\r\n > reshape_64 \\(Reshape\\)         \\(None, 20, 2\\)             0         \r\n > Total params: 0\r\n > Trainable params: 0\r\n > Non-trainable params: 0\r\n > _________________________________________________________________", "comments": ["I am able to replicate this issue on tf 2.2, tf 2.4 and nightly, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/aa6c13002327ac0f5b19bef00f93fee8/untitled593.ipynb)", "I was also able to reproduce this on TF 2.4. From the reference page for fit it also seems that the class_weight is argument is specifically created to allow the user to weight by class: \r\n\r\nclass_weight | Optional named list mapping indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\r\n-- | --\r\n\r\n\r\n", "I was able to reproduce the issue on colab using `TF2.6` and `TF-nightly(2.8.0-dev20211003)`.Please find the gist [here](https://colab.research.google.com/gist/chunduriv/12e65022d084187b3384893f60a49ef0/48555.ipynb) for reference.Thanks!"]}, {"number": 48552, "title": "How to create a representative dataset using TFrecords for tensorflow lite post training quantize conversion?", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 \r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly 2.6 dev\r\n\r\n\r\nI am trying to create a representative data set for conversion to tflite and perform a int8 quantize operations. I was wondering, is it possible to use TFrecords as a input data to be used to create a `representative_dataset` instead of looping through each images to generate a `representative_dataset`? This is due to no examples of using TFrecords in the API but the API just says you can load TFrecord and create a dataset for manipulation. ", "comments": ["@teijeong could you triage this issue?", "Can you try the following code?\r\n```\r\ndef representative_dataset_generator():\r\n    \"\"\"Dataset generator that generates random tensor with the same shape as the input\"\"\"\r\n    filenames = [filename]\r\n    raw_dataset = tf.data.TFRecordDataset(filenames) \r\n    for raw_record in raw_dataset.take(10):\r\n       # manipulate the raw_record to get the right shape.\r\n      yield raw_record    # raw_record is just a tensor\r\n```\r\n", "> Can you try the following code?\r\n\r\nJust tried it. Threw a error `TypeError: 'generator' object is not callable` in tensorflow/tensorflow/lite/python/optimize/calibrator.py at:\r\n``` \r\n    for sample in dataset_gen():\r\n      if not initialized:\r\n        initialized = True\r\n        self._calibrator.Prepare([list(s.shape) for s in sample])\r\n      self._calibrator.FeedTensor(sample)\r\n    return self._calibrator.Calibrate()\r\n```\r\n\r\nI will try debug and see what went wrong with the conversion process."]}, {"number": 48545, "title": "Introduce ability to clear GPU memory in Tensorflow 2", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.3.1, 2.4.1\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently there is no way to completely free the (once) allocated GPU RAM. \r\nFor example, i want to use tensorflow in the context of 3d visualization which is made next to impossible by this behavior. Standard solutions like `tf.config.experimental.set_memory_growth(gpus[0], True)` are unfortunately not sufficient, because the once allocated RAM cannot be released again.\r\n\r\nIn #36465 (https://github.com/tensorflow/tensorflow/issues/36465#issuecomment-818742876), it is mentioned that by using `GPUProcessState::TestOnlyReset` and `ProcessState::TestOnlyReset` the option to release GPU memory exists, but is just not exposed or for testing purposes only.\r\n\r\nIt would be very nice for applications using tensorflow to have proper access to gpu ram release functions.\r\n\r\n**Will this change the current api? How?**\r\nIntroduce a new (experimental) function to reset the current session/graph/device/... - state and thus free the GPU RAM completely.\r\n\r\n**Who will benefit with this feature?**\r\nPeople who use Tensorflow in their application in conjunction with other GPU-RAM critical operations such as 3D rendering.", "comments": ["@GatGit12,\r\nSince similar issue is already being tracking in issue [#36465](https://github.com/tensorflow/tensorflow/issues/36465), to avoid duplicates can you please close this and subscribe/follow that issue? Thanks!", "> @GatGit12,\r\n> Since similar issue is already being tracking in issue [#36465](https://github.com/tensorflow/tensorflow/issues/36465), to avoid duplicates can you please close this and subscribe/follow that issue? Thanks!\r\n\r\nBut this issue (#36465) is marked as a bug, with no attention and no explicit feature request, which is why i formally opened this post here as a feature request.\r\n\r\nAlso there are several issues with the gpu ram clearning which are simply ignored...\r\n\r\nHere is a selection (without guarantee of completeness): #39535, #19571, #15880, #20387", "Hi,\r\n\r\nWe expect this to be a non-issue once we're using the CUDA malloc async allocator by default.  Can you give it a try?  You can enable it by adding `TF_GPU_ALLOCATOR=cuda_malloc_async` to the environment.", "Hi,\r\n\r\ncould you please clarify with which TF version and CUDA version i can/should try this option?\r\n\r\nA quick search in this repo revealed that at least CUDA 11.2 is required:\r\nhttps://github.com/tensorflow/tensorflow/blob/926817c9ee822fe0f382248ea95f8f54c882d4bd/tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.h#L44\r\nBut TF 2.4.0/2.4.1 is only built for CUDA 11.0 (https://www.tensorflow.org/install/source_windows#gpu). \r\nSo I tried it with the current TensorFlow 2.5.0-rc1 which supports CUDA 11.2.\r\n\r\n> TensorFlow pip packages are now built with CUDA11.2 and cuDNN 8.1.0\r\n\r\nHowever, the error `tensorflow.python.framework.errors_impl.InternalError: No allocator statistics` already occurs during a simple operation. \r\nComplete Output:\r\n```\r\n>>> import tensorflow as tf\r\n2021-04-20 15:39:08.410710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\nINFO:tensorflow:Enabling eager execution\r\nINFO:tensorflow:Enabling v2 tensorshape\r\nINFO:tensorflow:Enabling resource variables\r\nINFO:tensorflow:Enabling tensor equality\r\nINFO:tensorflow:Enabling control flow v2\r\n>>> tf.__version__\r\n'2.5.0-rc1'\r\n>>> a = tf.constant(1.0)\r\n2021-04-20 15:39:24.562570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\r\n2021-04-20 15:39:24.625868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 48.00GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-04-20 15:39:24.633782: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\r\n2021-04-20 15:39:24.646338: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\r\n2021-04-20 15:39:24.649997: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\r\n2021-04-20 15:39:24.657917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\r\n2021-04-20 15:39:24.663828: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\r\n2021-04-20 15:39:24.676594: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_11.dll\r\n2021-04-20 15:39:24.683808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\r\n2021-04-20 15:39:24.688977: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\r\n2021-04-20 15:39:24.693099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-04-20 15:39:24.696781: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-20 15:39:24.710986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:\r\npciBusID: 0000:01:00.0 name: Quadro RTX 8000 computeCapability: 7.5\r\ncoreClock: 1.77GHz coreCount: 72 deviceMemorySize: 48.00GiB deviceMemoryBandwidth: 625.94GiB/s\r\n2021-04-20 15:39:24.719253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\r\n2021-04-20 15:39:25.394786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2021-04-20 15:39:25.399375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0\r\n2021-04-20 15:39:25.401865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N\r\n2021-04-20 15:39:25.404486: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:210] Using CUDA malloc Async allocator for GPU.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\gat\\.conda\\envs\\TF2.5_TEST\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 264, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n  File \"C:\\Users\\gat\\.conda\\envs\\TF2.5_TEST\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 276, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"C:\\Users\\gat\\.conda\\envs\\TF2.5_TEST\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 301, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"C:\\Users\\gat\\.conda\\envs\\TF2.5_TEST\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 97, in convert_to_eager_tensor\r\n    ctx.ensure_initialized()\r\n  File \"C:\\Users\\gat\\.conda\\envs\\TF2.5_TEST\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 525, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: No allocator statistics\r\n\r\n```\r\nSome general questions regarding `TF_GPU_ALLOCATOR=cuda_malloc_async`\r\nCould you also explain how the new allocator works? For example: When will the gpu ram be released when using the new allocator option? Must and or can this be done manually? Will it also work in the C-API? With which TF version will it be introduced?\r\n\r\nThank you! ", "I'm seeing the same issue with `cuda_malloc_async`. I'm on Linux with kernel 5.11.16, Nvidia driver 465.27-2, Cuda 11.2.2-2, Cudnn 8.2.0.53-1, and python 3.8.8. Tensorflow is tf-nightly-2.6.0.dev20210501.", "I believe this is fixed by https://github.com/tensorflow/tensorflow/pull/49173 (CC @nouiz )", "@sanjoy @GatGit12 \r\n\r\nPR  #49173 that is approved, but not yet merged should fix the `tensorflow.python.framework.errors_impl.InternalError: No allocator statistics` error.\r\n\r\nWhen it is merged, the next day you can probably use TF nightly build to test it again.\r\n\r\nNote, this new allocator can solve the issue, but this ask that all used library use cudaMallocAsync (EDIT: or cudaMalloc)\r\nDo you know how memory allocation is handled by your other library?", "I must correct myself. In fact, it should work if the other lib use cudaMallocAsync or cudaMalloc. But it won't if the other lib keep its unused memory instead of freeing it, like PyTorch pool allocator or TF GPU suballocator, unless that unused memory is \"freed\".\r\n\r\nMemory in cudaMallocAsync pool can also be released implicitly by the CUDA driver in order to allow an unrelated memory allocation request in the same process to succeed. For example, a call to cudaMalloc() or cuMemCreate() could cause CUDA to free unused memory from any memory pool associated with the device in the same process in order to serve the request.", "Is cuda_malloc_async supported in TF 2.5.0? I got InternalError: No allocator statistics with it.", "It was bugged in TF2.5. It should work in TF2.6. You can try TF2.6.0-rc1 that was released recently:\r\nhttps://github.com/tensorflow/tensorflow/releases", "Note, this issue should be closed as the bug is merged. I do not have the right to close it.", "@GatGit12 could you please let us know if this issue is fixed for you as per[ this comment](https://github.com/tensorflow/tensorflow/issues/48545#issuecomment-880707176),thanks!", "> It was bugged in TF2.5. It should work in TF2.6. You can try TF2.6.0-rc1 that was released recently:\r\n> https://github.com/tensorflow/tensorflow/releases\r\n\r\nTested with 2.6.0-rc1 and got the following error with TF_GPU_ALLOCATOR=cuda_malloc_async:\r\n```\r\nI tensorflow/core/common_runtime/gpu/gpu_process_state.cc:215] Using CUDA malloc Async allocator for GPU: 0\r\nProcess finished with exit code -1073740940 (0xC0000374)\r\n```\r\n\r\nSame error with TF_GPU_ALLOCATOR=cuda_malloc - \r\ntensorflow/core/common_runtime/gpu/gpu_process_state.cc:205] Using CUDA malloc allocator for GPU.\r\nProcess finished with exit code -1073740940 (0xC0000374)", "Is it easy for you to share a reproduction?", "Also, which driver version do you use and which OS?", "CUDA 11.2.0 and stock driver came with CUDA SDK.\r\nOS: Windows 10 build 19042.1110.\r\n", "Reproduction:\r\n```\r\nPython 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)] on win32\r\n>>> import os\r\n>>> os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\r\n>>> import tensorflow as tf\r\n>>> a = tf.zeros([], tf.float32)\r\n2021-07-16 22:05:42.163186: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-16 22:05:42.707769: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:215] Using CUDA malloc Async allocator for GPU: 0\r\nProcess finished with exit code -1073740940 (0xC0000374)\r\n```", "I didn't test this on Windows as I do not have a Windows computer setup.\r\nCan you try this small scripts. It enable extra logging, So maybe it will help understand where it crashes:\r\n\r\n```\r\nimport os\r\nos.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\r\nos.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\r\nimport tensorflow as tf\r\na = tf.zeros([], tf.float32)\r\n```", "Note, I have a PR that update the VLOG. It isn't yet merged:\r\nhttps://github.com/tensorflow/tensorflow/pull/50651\r\n\r\nDo you know if there is a nightly build for windows? If so, testing it a day after it is merged could give better error.", "> I didn't test this on Windows as I do not have a Windows computer setup.\r\n> Can you try this small scripts. It enable extra logging, So maybe it will help understand where it crashes:\r\n> \r\n> ```\r\n> import os\r\n> os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\r\n> os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\r\n> import tensorflow as tf\r\n> a = tf.zeros([], tf.float32)\r\n> ```\r\n\r\nSame outcome. But this time I ran with CUDA 11.2.2:\r\n```\r\nPython 3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)] on win32\r\n>>> import os\r\n>>> os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\r\n>>> os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\r\n>>> import tensorflow as tf\r\n>>> a = tf.zeros([], tf.float32)\r\n2021-07-16 23:36:54.937204: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-07-16 23:36:55.475806: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:215] Using CUDA malloc Async allocator for GPU: 0\r\nProcess finished with exit code -1073740940 (0xC0000374)\r\n```", "Can you try with CUDA 11.3 or CUDA 11.4? I do not have the setup to test on window. I do not know when someone can check it.\r\nAlso, how do you use CUDA on Windows? There is a few different way to do this.\r\n", "> Can you try with CUDA 11.3 or CUDA 11.4? I do not have the setup to test on window. I do not know when someone can check it.\r\n> Also, how do you use CUDA on Windows? There is a few different way to do this.\r\n\r\n11.4 gave the same error, even with cuda_malloc.\r\nOn the other hand, I was able to use cuda_malloc with TF2.5.0.", "I'm getting the same with 11.2 and 11.4 with latest drivers on windows. 26.0-rc1 crashes with the debug dump reporting python heap corruption with cuda_malloc or cuda_malloc_async. I can provide the dump file if that would help.\r\n\r\nCUDA was installed with the default windows 10 installer from Nvidia and path envvar was updated to the relevant directories. Cudnn was installed by copying the files from the Nvidia zip file to the appropriate locations.\r\n\r\nI am running the developer preview which is windows 11 so it's possible that's the problem. ", "> I'm getting the same with 11.2 and 11.4 with latest drivers on windows. 26.0-rc1 crashes with the debug dump reporting python heap corruption with cuda_malloc or cuda_malloc_async. I can provide the dump file if that would help.\r\n> \r\n> CUDA was installed with the default windows 10 installer from Nvidia and path envvar was updated to the relevant directories. Cudnn was installed by copying the files from the Nvidia zip file to the appropriate locations.\r\n> \r\n> I am running the developer preview which is windows 11 so it's possible that's the problem.\r\n\r\nNot likely Windows 11's issue. I'm running Windows 10 without insider preview.", "> Can you try with CUDA 11.3 or CUDA 11.4? I do not have the setup to test on window. I do not know when someone can check it.\r\n> Also, how do you use CUDA on Windows? There is a few different way to do this.\r\n\r\nNot using container. Just install CUDA SDK (but without NVTX and PhyS), cudnn, and stock driver that came with SDK.", "Do you have 1 or multiple GPUs? Which GPU(s) do you have?\r\n", "I have a 2080 TI as the only connected GPU and an AMD 3950X processor.", "I have single GTX1060 and intel i7-8750H", "I also fail to run the mentioned code.\r\n\r\nMachine setup is: cuda 11.4, Tensorflow nightly build - tf_nightly-2.7.0.dev20210727, python 3.7.5, 4xRTX2080 Ti, nvidia driver 470.57.02\r\n\r\nRunning this code:\r\n```\r\nimport os\r\nos.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\r\nos.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\r\nimport tensorflow as tf\r\na = tf.zeros([], tf.float32)\r\n```\r\n\r\nSegfaults: \r\n\r\n```bash\r\n2021-07-27 13:13:25.349562: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:215] Using CUDA malloc Async allocator for GPU: 0\r\nSegmentation fault (core dumped)\r\n```\r\n\r\nThe callstack top is:\r\n```bash\r\n(gdb) bt\r\n#0  0x000000000000002c in ?? ()\r\n#1  0x00007ffdab0f8760 in tensorflow::GPUProcessState::GetGPUAllocator(tensorflow::GPUOptions const&, tensorflow::gtl::IntType<tensorflow::TfDeviceId_tag_, int>, unsigned long, std::vector<tensorflow::gtl::IntType<tensorflow::TfDeviceId_tag_, int>, std::allocator<tensorflow::gtl::IntType<tensorflow::TfDeviceId_tag_, int> > > const&) ()\r\n   from REDACTED/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.2\r\n```\r\n\r\nThe same happens for cuda_malloc allocator. When no allocator is set - everything works.\r\n\r\nWhen memory_guard is set, I get:\r\n```\r\n2021-07-27 13:32:07.880641: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:200] Using memory guard allocator for GPU.\r\n2021-07-27 13:32:07.880692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1504] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9672 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:60:00.0, compute capability: 7.5\r\n2021-07-27 13:32:08.077423: F tensorflow/core/framework/tensor.cc:682] Check failed: IsAligned() Aligned and single element\r\nAborted (core dumped)\r\n```\r\n\r\nThe machine is multi-gpu machine, but I set `CUDA_VISIBLE_DEVICES=0`\r\n\r\nI will gladly provide any other information needed - I would really like this feature to land", "Quick update. It was also crashing on Linux. I made this PR to fix the linux crashed:\r\nhttps://github.com/tensorflow/tensorflow/pull/50961\r\n\r\nDo you know if there is nightly build for windows for you to test it?", "The fix for cudaMallocAsync was merged a few hours ago.\r\nCan you wait 24h and try TF nightly build to be sure that it also works for you?\r\nIf you have any comments on that new features, please share with us.\r\n\r\nTo enable it, use the environment variable TF_GPU_ALLOCATOR=cuda_malloc_async.\r\n\r\nPlease share your result on this new feature.\r\n\r\nNote, with this new allocator, the cuda driver will release automatically reserved, but not used memory when other library in the same process make a cudaMalloc calls that miss memory. So you do not need to trigger a trim command yourself.", "> The fix for cudaMallocAsync was merged a few hours ago.\r\n> Can you wait 24h and try TF nightly build to be sure that it also works for you?\r\n> If you have any comments on that new features, please share with us.\r\n> \r\n> To enable it, use the environment variable TF_GPU_ALLOCATOR=cuda_malloc_async.\r\n> \r\n> Please share your result on this new feature.\r\n> \r\n> Note, with this new allocator, the cuda driver will release automatically reserved, but not used memory when other library in the same process make a cudaMalloc calls that miss memory. So you do not need to trigger a trim command yourself.\r\n\r\nGot the same error as the other replies in this thread, and reinstalling 11.4 changed nothing. After downloading the nightly build it started working again. Good work. ", "@meslane thanks for the confirmation. Can you tell me which OS you are using?", "@nouiz I am on Windows 10. GPU is an RTX2060 and CPU is an i7-4790k.", "> I also fail to run the mentioned code.\r\n> \r\n> Machine setup is: cuda 11.4, Tensorflow nightly build - tf_nightly-2.7.0.dev20210727, python 3.7.5, 4xRTX2080 Ti, nvidia driver 470.57.02\r\n> \r\n> Running this code:\r\n> \r\n> ```\r\n> import os\r\n> os.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\r\n> os.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\r\n> import tensorflow as tf\r\n> a = tf.zeros([], tf.float32)\r\n> ```\r\n> \r\n\r\n\r\nWorks on nightly, on the same system. \r\n\r\nThanks!\r\n", "@gilfree Thanks for the confirmation.\r\n@GatGit12 if you have time that this resolve your original issue, it would be great.", "Hi, all!\r\nContinue this topic.\r\n \r\nIn my system configuration:\r\nTensorflow version:  2.7.0-dev20210917 from pip\r\nPython version:  3.8.12 from source clang-12\r\nLocal host OS :  Linux Debian 10 Linux-4.19.0-17-amd64-x86_64-\r\nIPython local host version:  7.27.0\r\nCPU: AMD FX-8350\r\nGPU: GeForce GT 1050 2GB (sm_61)\r\nDriver Version: 470.57.02    CUDA Version: 11.4 CUDNN 8.2.2\r\nthis magic code\r\n```\r\nimport os\r\nos.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\r\nos.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\r\nimport tensorflow as tf\r\na = tf.zeros([], tf.float32)\r\n```\r\ndidn't turn  into in  the `silver bullet` \r\n\r\nWhen I tried compile this simple code which created, deleted and re-created a simple model in the Jupyter Notebook\r\n\r\n```\r\n# Create model\r\nmodel = keras.Sequential([\r\n    keras.layers.Dense(16, activation='relu'),\r\n    keras.layers.Dense(16, activation='relu'),\r\n    keras.layers.Dense(1, activation='sigmoid')])\r\n\r\n\r\n    # Compile model\r\nmodel.compile(optimizer='rmsprop',\r\n              loss='binary_crossentropy',\r\n              metrics=['acc'])\r\n              \r\n\r\n              # Train and validation dataset\r\nx_val = x_train[:10000]\r\npartial_x_train = x_train[10000:]\r\ny_val = y_train[:10000]\r\npartial_y_train = y_train[10000:]\r\n\r\n\r\n# Train and validation model\r\nhistory = model.fit(partial_x_train,\r\n                    partial_y_train,\r\n                    epochs=20,\r\n                    batch_size=512,\r\nvalidation_data=(x_val, y_val))\r\n\r\n\r\n# Del model above and recreate with different numbers of epoches\r\n\r\n# Delete model above\r\ndel model\r\n\r\n# Recreate new model for final evaluation\r\nmodel = keras.Sequential([\r\n    keras.layers.Dense(16, activation='relu'),\r\n    keras.layers.Dense(16, activation='relu'),\r\n    keras.layers.Dense(1, activation='sigmoid')])\r\n\r\nmodel.compile(optimizer='rmsprop',\r\n              loss='mse',\r\n              metrics=['acc'])\r\n\r\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\r\nresults = model.evaluate(x_test, y_test, return_dict=True)\r\nmp_test = model.predict(x_test)\r\n```\r\nI get this result for Tensorflow 2.7.0-dev (short version).\r\n\r\n```\r\n2021-09-17 21:07:21.444997: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1000000000 exceeds 10% of free system memory.\r\n2021-09-17 21:07:32.198909: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 953.67MiB (rounded to 1000000000)requested by op _EagerConst\r\nIf the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \r\nCurrent allocation summary follows.\r\nCurrent allocation summary follows.\r\n.......................\r\n2021-09-17 21:07:32.202662: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 1467809792 memory_limit_: 1467809792 available bytes: 0 curr_region_allocation_bytes_: 2935619584\r\n2021-09-17 21:07:32.202700: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: \r\nLimit:                      1467809792\r\nInUse:                       601411072\r\nMaxInUse:                   1024997120\r\nNumAllocs:                       47638\r\nMaxAllocSize:                600000000\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2021-09-17 21:07:32.202734: W tensorflow/core/common_runtime/bfc_allocator.cc:468]\r\n......................\r\nInternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\r\n\r\n```\r\n\r\nThis means that  the direct command to delete the model object didn't  flush the GPU memory in the TF2.7.0-dev as in the TF2.6.0\r\n\r\nFor some reason, the same code in 2,5,1 above (without allocators) showed warnings in the last lines, but it was executed without problems and errors.\r\n\r\nIn the my case  the miracle did not happen and the elephant did not fly. I end up daring creative experiments that consumption a lot of time  and downgrade  to 2.5.1. \r\n\r\nIf the output listings for this simple code for TF 2.5.1 and 2.7.0-dev are interesting - see the attachment as Jupyter Notebooks.\r\n\r\nBest Regards,\r\nVadim Maklakov.\r\n\r\n[TF251-270-dev_CUDA_11.4.ipynb.tar.gz](https://github.com/tensorflow/tensorflow/files/7188239/TF251-270-dev_CUDA_11.4.ipynb.tar.gz)\r\n\r\n\r\n", "Thanks for this long description.\r\nI suppose that you tried cuda_malloc_async due to OOM with TF 2.7.\r\ncuda_malloc_async won't help if the model isn't deleted.\r\n\r\nNote, in your example, you use the first time partial_x_train and partial_y_train. But the second time you use x_train and y_train that are bigger. So the next time, it is normal that it request more memory. And so maybe the OOM is a real OOM.", "> I suppose that you tried cuda_malloc_async due to OOM with TF 2.7.\r\n\r\nFirst cell code in the my Jupyter Notebook TF2.7.0-dev  was silver bullet:\r\n```\r\nimport os\r\nos.environ[\"TF_GPU_ALLOCATOR\"]=\"cuda_malloc_async\"\r\nos.environ[\"TF_CPP_VMODULE\"]=\"gpu_process_state=10,gpu_cudamallocasync_allocator=10\"\r\nimport tensorflow as tf\r\na = tf.zeros([], tf.float32)\r\n```\r\nbut elephant doesn't take off...\r\nIn the TF 2.5.1 I can and not delete this simple model  - I get only warning message. \r\n", "Your issue is different then this one and I do not think cuda_malloc_async can help you. You should make another issue for your problem.", "Formally GT1050 (sm_61) support CUDA 11.4 toolkit - hence `silver bullet` code must work correctly with TF2.7.0-dev and clear GPU memory when delete old object.", "There can be many different causes to OOM errors. So there can't be a silver bullet solution.\r\nThe problem you hit is that TF doesn't call the free on some GPU memory.\r\n\r\nThis issue is a different one. It is about how TF memory allocator works. cuda_malloc_async add the feature requested in this issue.\r\nYour problem is something totally different then how allocator itself behave, it is how TF behave.\r\nReusing an issue not related to your problem won't get the attention your issue need. For that, you must find an existing issue that is the same as your or create a new issue.\r\nJust create a new issue so that the right people can have a chance of hearing about your issue. I'm not the right person."]}, {"number": 48544, "title": "Tf lite interpreter.allocate_tensors() Runtime error", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):\r\n- TensorFlow installed from (binary):\r\n- TensorFlow version (2.4.1):\r\n- Also tried tf-nightly\r\n\r\n\r\nIve used the following code to convert keras model to tflite. Conversion went successfull without any errors. Note that i used tf-nightly for the conversion as stable version of tf 2.4.1 was unable to convert\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import load_model\r\n\r\nmodelPath = \"u2net_keras.h5\"\r\n\r\ntflite_model = load_model(modelPath)\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(tflite_model)\r\n\r\n# converter = tf.lite.TFLiteConverter.from_keras_model_file( 'u2netp_keras.h5')\r\ntfmodel = converter.convert()\r\nopen (\"model.tflite\" , \"wb\") .write(tfmodel)\r\nprint(\"Sucess!\")\r\n```\r\nNow I am using the following code to get inference from the converted model but code seems to get stuck on \r\n`'interpreter.allocate_tensors()'` line\r\n\r\n```\r\nimport tensorflow as tf\r\nimport PIL.Image as Image\r\nimport numpy as np\r\n\r\nimport cv2\r\n\r\n\r\ndef preprocess(img_path, dim):\r\n    img = Image.open(img_path)\r\n    img = img.resize(dim, Image.BILINEAR)\r\n    img_data = np.array(img)\r\n    img_data = np.transpose(img_data, [2, 0, 1])\r\n    img_data = np.expand_dims(img_data, 0)\r\n    mean_vec = np.array([0.485, 0.456, 0.406])\r\n    stddev_vec = np.array([0.229, 0.224, 0.225])\r\n    norm_img_data = np.zeros(img_data.shape).astype('float32')\r\n    for i in range(img_data.shape[1]):\r\n        norm_img_data[:,i,:,:] = (img_data[:,i,:,:]/255 - mean_vec[i]) / stddev_vec[i]\r\n    return norm_img_data\r\n\r\n\r\nw = 320\r\nh = 320\r\ndim = (w,h)\r\nmodel = \"U2net-tflite-models/u2net.tflite\"\r\nimg_path = 'boat.jpg'\r\nimg = preprocess(img_path, dim)\r\n\r\n\r\n# Load TFLite model and allocate tensors.\r\ninterpreter = tf.lite.Interpreter(model)\r\n\r\ninterpreter.resize_tensor_input(0, [1,3, 320, 320], strict=False)\r\n\r\ninterpreter.allocate_tensors()\r\n\r\n\r\n\r\n#get input and output tensors\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\n\r\n# Read the image and decode to a tensor\r\n\r\n#Preprocess the image to required size and cast\r\ninput_shape = input_details[0]['shape']\r\nprint(\"input shape = \",np.expand_dims(img,0))\r\ninput_tensor= np.array(np.expand_dims(img,0))\r\n\r\n#set the tensor to point to the input data to be inferred\r\ninput_index = interpreter.get_input_details()[0][\"index\"]\r\ninterpreter.set_tensor(input_index, input_tensor)\r\n#Run the inference\r\ninterpreter.invoke()\r\noutput_details = interpreter.get_output_details()\r\n\r\n\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nresults = np.squeeze(output_data)\r\ntop_k = results.argsort()\r\nfor label, idx in train_data_gen.class_indices.items():\r\n    if top_k[idx]==1:\r\n        print(\"Prediction: \" ,label)\r\n```\r\n\r\nERROR log\r\n\r\n```\r\n2021-04-15 18:34:01.240901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\r\n2021-04-15 18:34:01.240937: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nTraceback (most recent call last):\r\n  File \"/home/faraz/Desktop/Esper_Solutions/upwrok/tfLite/U-2-Net-Keras/tflite_inference.py\", line 34, in <module>\r\n    interpreter.allocate_tensors()\r\n  File \"/home/faraz/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py\", line 420, in allocate_tensors\r\n    return self._interpreter.AllocateTensors()\r\nRuntimeError: tensorflow/lite/kernels/space_to_batch_nd.cc:67 op_context->block_shape->dims->data[0] != spatial_dims_num (3 != 2)Node number 49 (SPACE_TO_BATCH_ND) failed to prepare.\r\n\r\n\r\nProcess finished with exit code 1\r\n```\r\n\r\n", "comments": ["inorder to reproduce above error, here is the link of converted keras to tflite [model](https://drive.google.com/file/d/1y3LQSuxZcggilzMo82rPdGW9PMt-Lqam/view)", "@farazBhatti,\r\nI am facing errors while convert the model to `.tflite`, please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/f0b201b5ba066efd0473be8f43dacd0d/48544.ipynb).\r\n\r\nIn order to expedite the trouble-shooting process, could you please provide a minimal reproducible code? Alternatively, you can run the code on [Google Colab](https://colab.research.google.com/) and share the notebook with us. Thanks!\r\n", "@amahendrakar , here is the link to keras model converted to .tflite [model](https://drive.google.com/drive/folders/1GR_CJ9JapoLwzzr7gkEwQZA4OewdWc60). please note I was only able to convert using tf-nightly and not stable version of tensorflow2.4.1. ill also share google colab with you in few minutes. thanks\r\n", "here is the google colab [notebook](https://colab.research.google.com/drive/12sAdJbiXS3w5vy8P75bfFbF6yTdUqK3c?usp=sharing)", "@farazBhatti,\r\nThank you for the gist.\r\n\r\n@rmothukuru,\r\nI was able to reproduce the issue with TF v2.4.1, TF v2.5.0rc1 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/dbe8dcf8ab125e42ae86d891f7cea103/48544.ipynb). Thanks!", "Hi \r\nCan you share the original model before conversion \r\n\r\nThanks", "@farazBhatti, Can you please share original model before conversion as per @karimnosseir [comment](https://github.com/tensorflow/tensorflow/issues/48544#issuecomment-868893248).Thanks!", "@karimnosseir  @chunduriv  You can find it [here](https://drive.google.com/file/d/1y3LQSuxZcggilzMo82rPdGW9PMt-Lqam/view)", "Hey, I am facing a similar error while using tf2onnx for conversion. I am using [DeepLabv3+ model](https://keras.io/examples/vision/deeplabv3_plus/).\r\n\r\n1. I convert TF model to TFlite using\r\n```converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.target_spec.supported_types = [tf.float16]\r\n\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\r\n  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\r\n]\r\n\r\ntflite_fp16_model = converter.convert()\r\n```\r\nThis produces this warning message `WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\r\n8433276`\r\n\r\n2. I converted  Tflite to onnx using tf2onnx. \r\n`\r\n!python -m tf2onnx.convert --opset 13 --tflite model_f16_new.tflite --output model.onnx\r\n`\r\n```\r\nWARNING - Error loading model into tflite interpreter: tensorflow/lite/kernels/space_to_batch_nd.cc:67 op_context->block_shape->dims->data[0] != spatial_dims_num (3 != 2)Node number 99 (SPACE_TO_BATCH_ND) failed to prepare.\r\n```\r\n\r\nIf someone resolves it please tag me."]}, {"number": 48523, "title": "[TensorflowLite GPUDelegate] Outputs NaN Tensors", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Android 10 mobile / Windows 10 Desktop\r\n- Mobile device: OnePlus 7T\r\n- Python version: 3.6.3\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.4.1\r\n- CUDA / cuDNN: CUDA 11.0 / cuDNN 8.0\r\n- TensorFlow Lite version: 2.4.0\r\n\r\n\r\n**Describe the current behavior**\r\nA tensorflow lite model outsputs NaN tensors if GPUDelegate is used. Only on GPUDelegate, not on CPU, not on NnApiDelegate, not on XNNPack. Specific the tensors demo_class01, demo_class02, demo_class03, demo_class04. If I round the input of the tf.keras.layers.Multiply() layers to 4 digits, NaN output is gone, but performance is then the same on GPU as on CPU. Please look at the standalone code to see the tensor naming. The interessting thing is, that I have to use 4 different output classifications (demo_class01, demo_class02, demo_class03, demo_class04) to produce the NaN tensor output in Android Studio console, if less, constant tensors get printed. It seems like GPUDelegate + Multiply() is numerical instabil if the inputs have a too hight precision. Because if I round the inputs, the NaNs are gone (bit this seems more like a workaround).\r\n\r\nEdit:\r\nAnother model with 210x210x3 input outputs NaN tensors even with the rounding.\r\n\r\nThe output printed in Android Studio is the following:\r\n```\r\ndemo_class01: [NaN, NaN, NaN, NaN]\r\ndemo_class02: [NaN, NaN, NaN, NaN]\r\ndemo_class03: [NaN, NaN, NaN, NaN]\r\ndemo_class04: [NaN, NaN, NaN, NaN]\r\n```\r\n\r\nFor rounding I used the following in the \"working\" model:\r\n```\r\n    def tf_round(x, decimals=0):\r\n        multiplier = tf.constant(10**decimals, dtype=x.dtype)\r\n        return tf.round(x * multiplier) / multiplier\r\n    \r\n    out_relu  = tf_round(out_relu, 4)\r\n    demo_slice01 = tf_round(demo_slice01, 4)\r\n    demo_slice02 = tf_round(demo_slice02, 4)\r\n    demo_slice03 = tf_round(demo_slice03, 4)\r\n    demo_slice04 = tf_round(demo_slice04, 4)\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nNon NaN tensors without extra steps.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nFollowing demo model code:\r\n```\r\ndef create_demo():\r\n\r\n    demo_ins = tf.keras.layers.Input(shape=(300, 300, 3), name=\"demo_ins\", batch_size=1)\r\n    \r\n    model = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_shape=(300, 300, 3), \r\n                                              pooling=None, input_tensor=demo_ins)\r\n    out_relu = model.get_layer(name=\"out_relu\").output\r\n    \r\n    demo_conv01 = tf.keras.layers.Conv2D(4, (3, 3), padding=\"same\", strides=(1, 1), \r\n                                         kernel_initializer=\"he_uniform\", name=\"demo_conv01\")(out_relu)\r\n    demo_bn01   = tf.keras.layers.BatchNormalization(name=\"demo_bn01\")(demo_conv01)\r\n    demo_softmax01 = tf.keras.layers.Softmax(axis=2, name=\"a_demo_softmax01\")(demo_bn01)\r\n    \r\n    \r\n    demo_slice01 = tf.keras.layers.Lambda(lambda x: x[:, :, :, 0], name=\"demo_slice01\")(demo_softmax01)\r\n    demo_slice02 = tf.keras.layers.Lambda(lambda x: x[:, :, :, 1], name=\"demo_slice02\")(demo_softmax01)\r\n    demo_slice03 = tf.keras.layers.Lambda(lambda x: x[:, :, :, 2], name=\"demo_slice03\")(demo_softmax01)\r\n    demo_slice04 = tf.keras.layers.Lambda(lambda x: x[:, :, :, 3], name=\"demo_slice04\")(demo_softmax01)\r\n    \r\n    demo_slice01_reshape = tf.keras.layers.Reshape([10, 10, 1], name=\"demo_slice01_reshape\")(demo_slice01)\r\n    demo_slice02_reshape = tf.keras.layers.Reshape([10, 10, 1], name=\"demo_slice02_reshape\")(demo_slice02)\r\n    demo_slice03_reshape = tf.keras.layers.Reshape([10, 10, 1], name=\"demo_slice03_reshape\")(demo_slice03)\r\n    demo_slice04_reshape = tf.keras.layers.Reshape([10, 10, 1], name=\"demo_slice04_reshape\")(demo_slice04)\r\n    \r\n    demo_mul01 = tf.keras.layers.Multiply(name=\"w_demo_mul01\")([out_relu, demo_slice01_reshape])\r\n    demo_mul02 = tf.keras.layers.Multiply(name=\"x_demo_mul02\")([out_relu, demo_slice02_reshape])\r\n    demo_mul03 = tf.keras.layers.Multiply(name=\"y_demo_mul03\")([out_relu, demo_slice03_reshape])\r\n    demo_mul04 = tf.keras.layers.Multiply(name=\"z_demo_mul04\")([out_relu, demo_slice04_reshape])\r\n    \r\n    demo_block_conv01 = tf.keras.layers.Conv2D(3, (3, 3), padding=\"same\", strides=(1, 1), \r\n                                                   kernel_initializer=\"he_uniform\", name=\"demo_block_conv01\")\r\n    demo_block_bn01   = tf.keras.layers.BatchNormalization(name=\"demo_block_bn01\")\r\n    demo_block_relu01 = tf.keras.layers.ReLU(name=\"demo_block_relu01\")\r\n    demo_block_flatten = tf.keras.layers.Flatten(name=\"demo_block_flatten\")\r\n    demo_block_softmax01 = tf.keras.layers.Dense(4, activation=\"softmax\", name=\"demo_block_softmax01\")\r\n    \r\n    def block(inputs):\r\n        \r\n        conv01  = demo_block_conv01(inputs)\r\n        bn01    = demo_block_bn01(conv01)\r\n        relu01  = demo_block_relu01(bn01)\r\n        flatten = demo_block_flatten(relu01)\r\n        softmax01 = demo_block_softmax01(flatten)\r\n        \r\n        return softmax01\r\n        \r\n    demo_class01 = tf.keras.layers.Layer(name=\"b_demo_class01\")(block(demo_mul01))\r\n    demo_class02 = tf.keras.layers.Layer(name=\"c_demo_class02\")(block(demo_mul02))\r\n    demo_class03 = tf.keras.layers.Layer(name=\"d_demo_class03\")(block(demo_mul03))\r\n    demo_class04 = tf.keras.layers.Layer(name=\"e_demo_class04\")(block(demo_mul04))\r\n    \r\n    model = tf.keras.models.Model(\r\n        inputs=[model.input], \r\n        outputs=[demo_softmax01, \r\n                 demo_class01, demo_class02, demo_class03, demo_class04, \r\n                 demo_mul01, demo_mul02, demo_mul03, demo_mul04]\r\n    )\r\n    \r\n    return model \r\n```\r\n\r\nFollowing tfLite convert code:\r\n```\r\nmodel = create_demo()\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\ntflite_model = converter.convert()\r\n\r\nwith open(\"demo.tflite\", 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\nFollowing tfLite Interpreter code:\r\n```\r\nInterpreter.Options opt = new Interpreter.Options();\r\nCompatibilityList compatList = new CompatibilityList();\r\n\r\nGpuDelegate.Options gpuOptions = compatList.getBestOptionsForThisDevice();\r\nGpuDelegate delegate = new GpuDelegate(gpuOptions);\r\n\r\nopt.addDelegate(delegate);\r\ninterpreter = new Interpreter(modelByteBuffer, opt);\r\n```\r\n\r\nEdit02:\r\n\r\nMaybe it helps, the tensorflow lite output map should be the following:\r\n```\r\nMap<Integer, Object> outputMap = new HashMap<>();\r\n\r\nfloat[][][][] aMap = new float[1][10][10][4];\r\n\r\nfloat[][][][] demo_mul01 = new float[1][10][10][1280];\r\nfloat[][][][] demo_mul02 = new float[1][10][10][1280];\r\nfloat[][][][] demo_mul03 = new float[1][10][10][1280];\r\nfloat[][][][] demo_mul04 = new float[1][10][10][1280];\r\n\r\nfloat[][] demo_class01 = new float[1][4];\r\nfloat[][] demo_class02 = new float[1][4];\r\nfloat[][] demo_class03 = new float[1][4];\r\nfloat[][] demo_class04 = new float[1][4];\r\n\r\noutputMap.put(0, aMap);\r\n\r\noutputMap.put(1, demo_class01);\r\noutputMap.put(2, demo_class02);\r\noutputMap.put(3, demo_class03);\r\noutputMap.put(4, demo_class04);\r\n\r\noutputMap.put(5, demo_mul01);\r\noutputMap.put(6, demo_mul02);\r\noutputMap.put(7, demo_mul03);\r\noutputMap.put(8, demo_mul04);\r\n```\r\n\r\n**Other info / logs** \r\nI tryed differnet Tensorflow versions different Converter options like:\r\n\r\n```\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\r\n```\r\n\r\nWith this options the NaNs are gone but the performance impact is so strong, I could just use the old converter and use the mobile CPU.\r\n\r\nI attached a converted TensorflowLite NaN model created with the code above.\r\n[demo_issue.zip](https://github.com/tensorflow/tensorflow/files/6311102/demo_issue.zip)", "comments": ["@impjdi could you take a look?", "Are you running this in FP16 mode?", "If it is the default, yes. How do you set the running mode?\r\n\r\nEdit:\r\n\r\nIf you mean to set:\r\n\r\nconverter.target_spec.supported_types = [tf.float16]\r\n\r\nYes, I already did this. With the demo model I get for \"demo_classXX\" [0.25, 0.25, 0.25, 0.25] always constant tensors and for \"demo_mulXX\" [0.0 ... 0.0] all zero tensors. But only on GPU on CPU normal looking tensors. I also already tried converter.target_spec.supported_types = [tf.float32], same thing.", "Nah, that's just the stage when you create the model.  There's another mode for the actual runtime.  Given that you use:\r\n\r\n```\r\nGpuDelegate.Options gpuOptions = compatList.getBestOptionsForThisDevice();\r\n```\r\n\r\nwhich is implemented as\r\n\r\n```\r\n    return new GpuDelegate.Options();\r\n```\r\n\r\nwhich takes the default values:\r\n\r\n```\r\n    boolean precisionLossAllowed = true;\r\n    boolean quantizedModelsAllowed = true;\r\n    int inferencePreference = INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER;\r\n```\r\n\r\nI think it's running at FP16 mode.  If you set `precisionLossAllowed` to `false` and the problem is gone, that means that your model is not numerically stable enough to be run at FP16.", "I already tried with:\r\n\r\n```\r\nInterpreter.Options opt = new Interpreter.Options();\r\nCompatibilityList compatList = new CompatibilityList();\r\n\r\nGpuDelegate.Options gpuOptions = compatList.getBestOptionsForThisDevice();\r\ngpuOptions.setPrecisionLossAllowed(false);\r\n\r\nGpuDelegate delegate = new GpuDelegate(gpuOptions);\r\nopt.addDelegate(delegate);\r\n\r\nInterpreter interpreter= new Interpreter(modelByteBuffer, opt);\r\n```\r\n\r\nalso tried to play around with:\r\n\r\n```\r\ngpuOptions.setQuantizedModelsAllowed(...)\r\ngpuOptions.setInferencePreference(...)\r\n```\r\n\r\nNothing changed I still get demo_classXX: [NaN, NaN, NaN, NaN]\r\n\r\nEdit:\r\n\r\nIf I use the XNNPack with:\r\n\r\n```\r\nInterpreter.Options opt = new Interpreter.Options();\r\n\r\nopt.setUseXNNPACK(true);\r\nopt.setNumThreads(4);\r\n\r\ninterpreter = new Interpreter(modelByteBuffer, opt);\r\n```\r\n\r\nand Coverter:\r\n\r\n```\r\nmodel = create_demo()\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\nwith open(\"demo.tflite\", 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\neverything runs just fine but it is not fast enough.\r\n\r\nEdit2:\r\n\r\nIf I multiply the feature extractor output \"out_relu\" with 1.0 before the Multiply()s the NaN is also gone on GPU.\r\n\r\n```\r\none = tf.constant([[[[1.0]]]])\r\nout_relu  = tf.keras.layers.Multiply()([out_relu, one])\r\n\r\ndemo_mul01 = tf.keras.layers.Multiply(name=\"w_demo_mul01\")([out_relu, demo_slice01_reshape])\r\ndemo_mul02 = tf.keras.layers.Multiply(name=\"x_demo_mul02\")([out_relu, demo_slice02_reshape])\r\ndemo_mul03 = tf.keras.layers.Multiply(name=\"y_demo_mul03\")([out_relu, demo_slice03_reshape])\r\ndemo_mul04 = tf.keras.layers.Multiply(name=\"z_demo_mul04\")([out_relu, demo_slice04_reshape])\r\n```"]}, {"number": 48520, "title": "Customize continuous string tensor type for feature hashing on GPU.", "body": "**System information**\r\n- TensorFlow version (you are using): 1.15 & 2.4\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n**Background**\r\nIn a recommender system, there are large number of features are expressed in the form of text, like the name of a product, or behavior of user, in advertisement.\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, Tensorflow use discontinuous host memory to store the text. When hashing the text features, it will lead to lack of performance, by some reasons:\r\n1. Extract string tensor from TFRecord will introduce lots of memcpy for moving string elements one by one.\r\n2. When using GPU to hash the text features, is hard to do memcpy from discontinuous memory to continuous memory, both [H2D] or [H2H + single H2D], since we cannot use discontinuous data on GPU.\r\n\r\n**Who will benefit with this feature?**\r\nI implemented a Murmur hashing with CUDA, wrapped by Tensorflow OP. I found that string copying one by one consumes 90% of the time. So it will make a huge improvement if a continuous string Tensor is enabled.\r\n\r\n\r\nI'm new to Tensorflow framework type system. How could I create a new tensor type? And what kinds of problems should I consider?\r\n\r\n", "comments": ["I'm not a TFRecord expert, but I can't easily see how this will work reading the TFRecord [spec](https://www.tensorflow.org/tutorials/load_data/tfrecord#tfrecords_format_details).\r\n\r\nCan you more explicitly state what you're proposing?  Are you suggesting to create a new format that is like TFRecord, but is formatted differently?", "> I'm not a TFRecord expert, but I can't easily see how this will work reading the TFRecord [spec](https://www.tensorflow.org/tutorials/load_data/tfrecord#tfrecords_format_details).\r\n> \r\n> Can you more explicitly state what you're proposing? Are you suggesting to create a new format that is like TFRecord, but is formatted differently?\r\n\r\nHi. I take quite a while to figure out this problem. And sorry for the delay.\r\n\r\nHere is the background: In advertisements, news, videos features, extracted by humans or algorithms, are described as strings or numbers, mostly.\r\nIn common case, for string features, we can use `tf.strings.to_hash_bucket`, `tf.strings. to_hash_bucket_fast`, `tf.strings. to_hash_bucket_strong` to encode the features and get embeddings to them.\r\nAs to the number feature, we could convert it to string at first. And then use the string hash methods.\r\nThe numbers features are widely used in productions mentioned above.\r\n\r\nBut the such methods are using `string` as input to hash function, which is inconvenient for GPU to process. And currently, there are CPU OPs only.\r\n\r\nI was trying to use a new tensor type to reserve the feature values, in continuous address. When parsing the examples in TFRecord, string features could be parsed directly into the continuous address. And GPU can run on them without bunch of Fragmented copies.\r\n\r\nAs for number features, I was tending to make the `AsString` output to the new type of string tensor. But it seems necessary.\r\nI have implemented a set of GPU kernels directly hashing the number features, following using string as intermediate state. About 20 times performance improvement has been achieved, compared to the CPU version.\r\n\r\nI'm wondering would it be possible for TensorFlow to accept a GPU implementation instead of `tf.as_string + tf.strings. to_hash_bucket `? or just a GPU impl for the `AsString` OP?", "> I'm wondering would it be possible for TensorFlow to accept a GPU implementation instead of tf.as_string + tf.strings. to_hash_bucket ? or just a GPU impl for the AsString OP?\r\n\r\n@Lifann I think this is already done, see https://github.com/tensorflow/tensorflow/pull/47936."]}, {"number": 48500, "title": "How to use StatsAggregator (esp. to collect prefetch size)", "body": "## URL(s) with the issue:\r\n\r\nhttps://www.tensorflow.org/api_docs/python/tf/data/experimental/StatsAggregator\r\n\r\n## Description of issue (what needs changing):\r\n\r\nThe documentation does not fully describe how to use `StatsAggregator`, i.e. it is missing what the output should be, where it is and what to do with it.\r\n\r\n### Clear description\r\n\r\nBackstory: I had a setup, where  speeding up the training step did not yield a speedup of the training which turns out to be dataset input pipeline performance and could be solved by `prefetch(large_number)`\r\nHowever using the autotune parameter did not help here and I wanted to know why.\r\nIn the source code I've seen, that there is a possibility to collect the current prefetch buffer size over training steps so I wanted to look what the chosen values are and how they change, i.e. check if autotune is working as expected\r\nI seems it is not increasing the buffer size fast enough or at all.\r\n\r\nHowever I was not able to figure out how to get this information. And when using `StatsAggregator` it shows a deprecation warning and refers to TF Profiler but the information I need is not there at all. \r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nNo\r\n", "comments": ["Regarding prefetch buffer size autotune debugging, first figure out whether or not you are using legacy autotune. This will show up in the TF Profiler trace of the Prefetch op. If you are using legacy autotune, the behavior is described [here](https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/core/kernels/data/prefetch_autotuner.h#L24-L39). Otherwise, the buffer size is tuned in [model.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/model.cc#L116).\r\n\r\nI'm not familiar with StatsAggregator. Given that it's deprecated, I don't think we will invest effort into improving its documentation. TF Profiler gives much more detailed information for performance debugging.", "Thanks for the reply. I've seen the term \"legacy autotune\" in the code but I haven't been able to find anything of it in the TF profiler. Where would I have to look for that and how can I influence which autotune is used?\r\n\r\nIn the \"trace viewer\" tool I see this when selecting the Prefetch op:\r\n\r\n```\r\nArgs | \u00a0\r\n-- | --\r\nbuffer_limit | \"3\"\r\nid | \"1886018925671364970\"\r\nlong_name | \"Iterator::Model::MaxIntraOpParallelism::Prefetch\"\r\nparent_id | \"3180932205155061847\"\r\n```\r\n\r\nWhen running with `TF_CPP_MIN_LOG_LEVEL=0 TF_CPP_MIN_VLOG_LEVEL=2` I don't see any \"tunable\" output related to prefetch, so I'd guess it's the legacy one?\r\n\r\nI think it would be very useful to have a graph printing the chosen values of the buffer sizes over the step number and (as I've seen some code related to the StatsAggregator) the latency of dataset operations (i.e. time from call to return) over the step number but I wasn't able to find anything like that in the Profile tab.\r\n\r\nThe closest seems to be the trace viewer, but even for only 3 local GPUs there are about 2 dozens of `tf_data_*_iterator_resource` timelines which I find very hard to interpret.\r\n\r\nAnd the `tf_data_bottleneck_analysis` shows my 3 devices + 1 host with 32 ranks each with something which could be useful if it wasn't split in (3+1)*32=128 pipeline views which I couldn't find an explanation for how they map to my defined pipeline or to interpret those \"ranks\".\r\n\r\nSo bottom line: Where do I find that information in TF Profile which seemingly is/was available only through StatsAggregator?", "In TF 2.4 the trace viewer will show information about autotuning as well as the current buffer size. Is it possible to update to a newer version?", "I am using TF 2.4.1. Where do I have to look for that information?\r\nThe only place I've seen is when selecting an op/event in the trace viewer but that is unsuitable to see how the buffer size (e.g.) changes, as selecting dozens or hundreds of such events and manually check for changes is unfeasible."]}, {"number": 48499, "title": "[Performance Boost Request] SLIDE algorithm", "body": "I have recently read two new article discussing about accelerating training algorithm using hashtables. And since the library implemetation using TensorFlow, do TF devs would use it for application in TF-CPU to boost up training performance, or even by TF-GPU if possible?\r\nLink: https://arxiv.org/abs/1903.03129 \r\nLink: https://arxiv.org/abs/2103.10891", "comments": []}, {"number": 48497, "title": "Activation layer fails to be merged in TFLite conversion after quantization-aware training", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Windows 10.0.19042 Build 19042\r\n- TensorFlow installation: pip package \r\n- TensorFlow library: 2.6.0-dev20210412\r\n\r\n### 2. Code\r\n\r\n**Model training (with QA training)**: https://colab.research.google.com/gist/Mjonir/c2f281c417b3846887a242c19caa2c06/tensorflow-datasets.ipynb\r\n**Trained model (with QA training)**: https://1drv.ms/u/s!AiIykeHl1htbwsMqPggjpHJ4azvwgg?e=l4II0F\r\n**Model training (no QA training)**: https://colab.research.google.com/gist/Mjonir/f83a50157a722e7b93d99ef6c5a3de9e/tensorflow-datasets.ipynb\r\n**Trained model (no QA training)**: https://1drv.ms/u/s!AiIykeHl1htbwsMtDEYyVWq-dGOGsA?e=HUU30i\r\n**TFLite conversion**: https://colab.research.google.com/gist/Mjonir/956546ac2c73277ab71d7b6a4f536646/tensorflow-lite-debugger-colab.ipynb\r\n**Converted model (with QA training)**: https://1drv.ms/u/s!AiIykeHl1htbwsMpmYF1Y17JNt2Wlw?e=cHk0YD\r\n**Converted model (no QA training)**: https://1drv.ms/u/s!AiIykeHl1htbwsMsaOLa1RKE03beQQ?e=zRHFvN\r\n\r\n### 3. Failure after conversion\r\n\r\nWhen converting a QA-trained model, the Relu activation fails to be merged with the preceding Conv2D/Dense layer. When porting to target hardware, in my case to microcontroller, this results in the activation layers being processed separately on the target instead of during the evaluation of the Conv2D kernel, inducing a 10% performance loss in my overall network.\r\n\r\nFor reference, an identical non-QA trained model has its activation indeed merged with the preceding Conv2D/Dense layer.\r\n\r\n### 5. (optional) Any other info / logs\r\n\r\nNetron visualisations:\r\n**With QA training:**\r\n![with_qa](https://user-images.githubusercontent.com/5678238/114494711-1b3e8980-9c1d-11eb-88ec-a0248d9fc6ef.png)\r\n**No QA training:**\r\n![no_qa](https://user-images.githubusercontent.com/5678238/114494704-18dc2f80-9c1d-11eb-8f54-c0ef7a30c35e.png)\r\n\r\n", "comments": ["@teijeong could you triage this issue?", "David, there are fake quant ops are inserted between the conv and relu. looks like this is a bug of the QAT api."]}, {"number": 48487, "title": "[RNN] TFLite integer 8 quantization yielding worse accuracy than CoreML integer 8", "body": "### 1. System information\r\n\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow installation: pip package\r\n- TensorFlow library: tf-nightly==2.6.0.dev20210407\r\n\r\n### 2. Issue and code\r\n\r\nI'm implementing a Keras GRU model (tensorflow.keras.layers.GRU) on mobile devices for character level language modelling. In order to have a smaller on-device checkpoint I'm experimenting both with TFLite and CoreML integer quantization at 8 bits. The problem is that the TFLite compressed model yields up to 3% worse accuracy results on the test set when compared to the CoreML compressed model. The test set is big enough and the CoreML model results are very close to the full 32 bits model (or the 16 bit model).\r\n\r\nThis is how I'm executing the conversion for TFLite (where model is of type `tf.keras.Model`):\r\n\r\n```\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\nconverter.experimental_new_converter = True\r\nconverter.experimental_new_quantizer = True\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\nconverted = converter.convert()\r\n```\r\n\r\nI've tried with various versions of TensorFlow, including the 2.4.1, 2.5.0 and the nightly.\r\n\r\nThis is how I'm executing the conversion for CoreML (where model is of type `tf.keras.Model`):\r\n\r\n```\r\nimport coremltools as ct\r\n\r\ncoreml_model = ct.convert(model, use_float_arraytype=True)\r\ncoreml_model = ct.models.neural_network.quantization_utils.quantize_weights(coreml_model, 8, \r\n                                                                     quantization_mode=\"linear\")\r\n```\r\nTensorFlow Lite doesn't allow you to specify a quantization mode, while CoreML lets you choose among: linear (default), linear_lut, kmeans_lut, custom_lut, linear_symmetric. I tried all of them but custom_lut. The ones I've tried all yield results very similar to the un-quantized model, except for linear_lut.\r\n\r\nNow the question is: why is the model coming out of the TFLite quantization process so much worse than almost all the approaches coming from CoreML? is there a way to reproduce the CoreML results in TFLite?\r\n\r\nThanks a lot!\r\n", "comments": ["@Xhark could you triage this?", "@teijeong could you triage this issue?", "Hi lucacampanella@, seems you're using dynamic range quantization for TFLite. From my limited Core ML experience, Core ML provides weight quantization, but when inferencing the weights are dequantized back to FP16 to do calculations. CoreML's linear quantization should result in quite same weights as TFLite's dynamic range quantization, but TFLite quantizes activation and do calculations in int8, so it might be the reason for accuracy loss.\r\n\r\nI could provide more debugging tips base on your model, if you could provide one. \r\n\r\n@jianlijianli for more context", "Hi @teijeong and thanks a lot for the feedback, it's much appreciated.\r\n\r\nHere's the code that builds the model:\r\n```\r\ndef _build_test_graph(self, with_tf_lite_ready_setup=False):\r\n        # Inputs\r\n        if with_tf_lite_ready_setup:\r\n            # Set the batch size of 1 when using TFLite, or it will result in a much bigger checkpoint that still only works with batch size 1\r\n            batch_size = 1\r\n            inputs = Input(shape=(1), batch_size=batch_size)\r\n        else:\r\n            batch_size = None\r\n            inputs = Input(shape=(None,))\r\n\r\n        # Input state\r\n        h_0 = [Input(shape=[self.state_dim], batch_size=batch_size)]\r\n\r\n        self.rnn_layer = tensorflow.keras.layers.GRU(self.state_dim, return_state=True)\r\n        rnn_inputs = tf.one_hot(tf.cast(inputs, tf.int32), depth=self.vocab_size)\r\n\r\n        embedder = Embedding(self.vocab_size, self.emb_dim, mask_zero=True)\r\n        m = embedder.compute_mask(inputs) # the token 0 is the padding token\r\n        # Apply RNN\r\n        out = self.rnn_layer(rnn_inputs, initial_state=h_0, mask=m)\r\n\r\n        rnn_out = out[0]\r\n        h_out = out[1:]\r\n\r\n        dense = Dense(self.vocab_size, activation=\"softmax\")\r\n        output = dense(rnn_out)\r\n\r\n        return keras.Model([inputs] + h_0, [output] + h_out)\r\n```\r\n\r\nSo if I understood you correctly, you're saying that TFLite keeps the integer quantization also during inference and thus applies it also to the activations. On the other hand CoreML does inference in float16, which allows the activations to have a broader range of values and yield better accuracy?\r\nWhat is also interesting to notice is that when I zip the checkpoints of TFLite and CoreML the TFLite checkpoint is smaller. TFLite is 905 kB uncompressed, 505 kB compressed; CoreML (with linear quantization, the default) is 914 kB uncompressed and 840 kB compressed. Do you think this suggests, from a very rough point of view, that there is less information in the TFLite model?\r\n\r\nThanks a lot for the help, any tip or suggestion is welcome!", "@lucacampanella \r\n\r\nCould you please confirm if the issue is resolved? if yes, please feel free to move this issue to closed status.\r\n\r\n\r\n\r\n", "Hi @UsharaniPagadala ,\r\nI don't really know why this issue was referenced in number #48546 , they seem totally unrelated to me.\r\n\r\nThe current **issue is not resolved**, I was just waiting for some feedback from @teijeong , who was kind enough to reply to my first message and asked for more code :)\r\n", "Hi @teijeong , did you have a chance to look at the code and at my clarification questions? It would help us a great bunch!\r\nThanks! :)"]}]