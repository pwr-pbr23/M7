[{"number": 45341, "title": ".ckpt to .pb", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nNo OpKernel was registered to support Op 'TPUReplicatedInput' used by {{node input0}} with these attrs: [N=16, is_packed=false, T=DT_INT32, index=0, is_mirrored_variable=false]\r\nRegistered devices: [CPU, GPU, XLA_CPU, XLA_GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[input0]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py in restore(self, sess, save_path)\r\n   1333       # We add a more reasonable error message here to help users (b/110263146)\r\n   1334       raise _wrap_restore_error_with_msg(\r\n-> 1335           err, \"a mismatch between the current graph and the graph\")\r\n   1336 \r\n   1337   @staticmethod\r\n\r\n**Describe the expected behavior**\r\nthe actual model contains .meta,.data,.index files\r\nthe model should be converted from .ckpt to .pb\r\n\r\n**Standalone code to reproduce the issue**\r\nimport os\r\nimport tensorflow as tf\r\n\r\ntrained_checkpoint_prefix = 'models/model.ckpt-49491'\r\nexport_dir = os.path.join('export_dir', '0')\r\n\r\ngraph = tf.Graph()\r\nwith tf.compat.v1.Session(graph=graph) as sess:\r\n    # Restore from checkpoint\r\n    loader = tf.compat.v1.train.import_meta_graph(trained_checkpoint_prefix + '.meta')\r\n    loader.restore(sess, trained_checkpoint_prefix)\r\n\r\n    # Export checkpoint to SavedModel\r\n    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_dir)\r\n    builder.add_meta_graph_and_variables(sess,\r\n                                         [tf.saved_model.TRAINING, tf.saved_model.SERVING],\r\n                                         strip_default_attrs=True)\r\n    builder.save()                \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@gowthamvenkatsairam,\r\nOn running the code, I am facing an error stating `OSError: File models/model.ckpt-49491.meta does not exist`. \r\n\r\nIn order to reproduce the issue reported here, could you please provide the TensorFlow version, the complete code and all the required supporting files. Thanks!", "Also, please go through [this](https://stackoverflow.com/a/56793601) similar StackOverflow query and let us know if it helps. Thanks!", "https://drive.google.com/file/d/0B3APw5BZJ67ETHNPaU9xUkVoV0U/view\r\nmodel is available here", "i also gone through the stackoverflow link that you have shared,but that does'nt solve the problem", "@gowthamvenkatsairam,\r\nI was able to run the code with TF v2.3 without any issues. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/6803c28df6e8ef8e1f8500948b5753ff/45341.ipynb). \r\n\r\nCould you please try running the code in a new virtual environment and check if you are facing the same issue. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45341\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45341\">No</a>\n"]}, {"number": 45340, "title": "tf.ragged.boolean_mask not working in symbolic mode", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0\r\n- Python version: 3.6.8\r\n\r\n**Describe the current behavior**\r\n```python\r\nx = tf.keras.Input(shape=(101,), dtype=tf.float32)\r\nmask = tf.where(tf.math.greater(x, 1.), True, False)\r\nragged_x = tf.ragged.boolean_mask(x, mask)\r\n```\r\nthrows\r\n```\r\nValueError: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor 'Cumsum_1:0' shape=(None,) dtype=int64>\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe above should work. \r\nIt works if x is an eager tensor.\r\n\r\n**Standalone code to reproduce the issue**\r\nSee above.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\n---------------------------------------------------------------------------\r\n\r\n> ValueError                                Traceback (most recent call last)\r\n> <ipython-input-2-9d996c714f77> in <module>()\r\n>       1 x = tf.keras.Input(shape=(101,), dtype=tf.float32)\r\n>       2 mask = tf.where(tf.math.greater(x, 1.), True, False)\r\n> ----> 3 ragged_x = tf.ragged.boolean_mask(x, mask)\r\n> \r\n> 9 frames\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n>     199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n>     200     try:\r\n> --> 201       return target(*args, **kwargs)\r\n>     202     except (TypeError, ValueError):\r\n>     203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/ragged/ragged_array_ops.py in boolean_mask(data, mask, name)\r\n>     191         flattened_masked_lengths = array_ops.reshape(masked_lengths, [-1])\r\n>     192         masked_values = ragged_tensor.RaggedTensor.from_row_lengths(\r\n> --> 193             masked_values, flattened_masked_lengths, validate=False)\r\n>     194 \r\n>     195         # Wrap remaining ragged dimensions.\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/ragged/ragged_tensor.py in from_row_lengths(cls, values, row_lengths, name, validate)\r\n>     473           row_lengths=row_lengths,\r\n>     474           validate=validate,\r\n> --> 475           preferred_dtype=_get_optional_partition_dtype(values))\r\n>     476       return cls._from_row_partition(values, row_partition, validate=validate)\r\n>     477 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/ragged/row_partition.py in from_row_lengths(cls, row_lengths, validate, preferred_dtype)\r\n>     392 \r\n>     393       row_limits = math_ops.cumsum(row_lengths)\r\n> --> 394       row_splits = array_ops.concat([[0], row_limits], axis=0)\r\n>     395       return cls(\r\n>     396           row_splits=row_splits,\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n>     199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n>     200     try:\r\n> --> 201       return target(*args, **kwargs)\r\n>     202     except (TypeError, ValueError):\r\n>     203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)\r\n>    1652           dtype=dtypes.int32).get_shape().assert_has_rank(0)\r\n>    1653       return identity(values[0], name=name)\r\n> -> 1654   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n>    1655 \r\n>    1656 \r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2(values, axis, name)\r\n>    1210     try:\r\n>    1211       return concat_v2_eager_fallback(\r\n> -> 1212           values, axis, name=name, ctx=_ctx)\r\n>    1213     except _core._SymbolicException:\r\n>    1214       pass  # Add nodes to the TensorFlow graph.\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2_eager_fallback(values, axis, name, ctx)\r\n>    1240         \"'concat_v2' Op, not %r.\" % values)\r\n>    1241   _attr_N = len(values)\r\n> -> 1242   _attr_T, values = _execute.args_to_matching_eager(list(values), ctx)\r\n>    1243   _attr_Tidx, (axis,) = _execute.args_to_matching_eager([axis], ctx, _dtypes.int32)\r\n>    1244   _inputs_flat = list(values) + [axis]\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in args_to_matching_eager(l, ctx, default_dtype)\r\n>     261       ret.append(\r\n>     262           ops.convert_to_tensor(\r\n> --> 263               t, dtype, preferred_dtype=default_dtype, ctx=ctx))\r\n>     264       if dtype is None:\r\n>     265         dtype = ret[-1].dtype\r\n> \r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\r\n>    1473       raise ValueError(\r\n>    1474           \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\" %\r\n> -> 1475           (dtype.name, value.dtype.name, value))\r\n>    1476     return value\r\n>    1477 \r\n> \r\n> ValueError: Tensor conversion requested dtype int32 for Tensor with dtype int64: <tf.Tensor 'Cumsum:0' shape=(None,) dtype=int64>", "comments": ["@adriang133,\r\nI was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/c61bb05dd71a50b2085cbf0e5463aa50/45340.ipynb).\r\n\r\nHowever, the issue seems to be fixed in the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/13e81ba348f9d1ea9bd4f911854bac3c/45340-tf-nightly.ipynb). I was able to run the code without any errors. Please check the linked gist for reference. Thanks!", "Thanks @amahendrakar!\r\n\r\nThis issue can be closed then.", "@adriang133,\r\nThank you for the udpate. Closing the issue as it is fixed in the latest TF-nightly. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45340\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45340\">No</a>\n"]}, {"number": 45339, "title": "Fix Const op tensor_content on s390x during save/load", "body": "This PR addresses swapping of  tensor_content data when Tensorflow models are saved across LE/BE systems.\r\n\r\nFew [testcases](https://github.com/tensorflow/serving/blob/42666a18c050c47753449726f65cccc0937b75e0/tensorflow_serving/model_servers/tensorflow_model_server_test.py#L768) in Tensorflow Serving fail because they try to serve TF models that were saved on x86.  In particular, `tensor_content` field of `Const op` tensor has data stored in the native endiness format.  However, `tensor_content` field does not have a way to indicate endiness of the raw bytes that go in the `tensor_content` field.\r\n\r\nLoad and store mechanism of `saved_model` code has been modified to account for endiness when these operations are performed on LE/BE machines.  This is achieved by traversing through `meta_graph_def` proto and modifying the `tensor_content` of `Const op` tensor.\r\n\r\nSpecifically:\r\n\r\n- In `tensorflow/python/saved_model/save.py` a function to swap `tensor_content`  is called on big-endian machine to store the bytes in little-endian format\r\n- In `tensorflow/python/saved_model/load.py`, a reverse operation is performed to get `tensor_content` in big-endian format\r\n- Similar changes are implemented in `tensorflow/cc/saved_model/reader.cc` to byte swap `tensor_content` \r\n\r\nThis implementation is broadly based on the ideas from [this](https://github.com/tensorflow/tensorflow/pull/28490) PR.\r\n\r\n", "comments": ["@ccrusius - please let me know if further modifications are needed. Thx!", "@ccrusius  Any update on this PR? Please. Thanks!", "@rposts can you please check sanity build errors ?", "@rthadur - seems like this one has slipped through the cracks. Is there anything I need to do at my end? Tx.", "@rposts thanks for your patience , this PR is stuck internally , @ccrusius is working on it."]}, {"number": 45338, "title": "Update _inferred_steps when recreating iterator.", "body": "closes https://github.com/tensorflow/tensorflow/issues/41019\r\n\r\nIn case your data generator returns different number of batches per epoch, we need to update the `inferred_steps` after recreating the iterator over the dataset. This allows users to create data generators that modify the datasets and update the batch size every epoch, for example, using an increasing batch size.", "comments": ["I have some trouble setting up the tests locally to see the actual failures. Do you have any guide for that? Or can you point me to the actual errors why tests are failing?", "@tabergma Can you please check @fchollet's comments and keep us posted ? Thanks!", "@gbaned @fchollet Thanks for the review! Not sure when I have time to add the unit test as I have a newborn. I'll keep you posted.", "@tabergma  Any update on this PR? Please. Thanks!", "I did not find time to add a test yet, try to do it in the next week.", "I started looking into it, but had some problems running the tests locally. Not sure when I'll have time to finish it. ", "@tabergma Any update on this PR? Please. Thanks!", "Unfortunately, no update. I guess I'm not able to find time any time soon. Will let you know when I start working on it again. Sorry for the delay.", "It has been 19 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!", "I would like to follow up with this PR. I work in @tabergma 's team. \r\n@gbaned @fchollet Would it be ok to re-open this PR? Would you still accept this contribution?", "Hi @markus-hinsche  It looks like this PR relates to the Keras component. Please submit it to the github.com/keras-team/keras repository instead. Thankyou.\r\n@fchollet, @qlzh727"]}, {"number": 45337, "title": "tf.while_loop how to create a nested loop", "body": "I need to write a loop like this one:\r\n`for i in range(N):\r\n    for j in range(M):\r\n        data[i, j] = 2i+3j`\r\nI've read the documentation and found a way ro write a simple loop:\r\n`def condI(i, _):\r\n    return i < 10`\r\nThe next:\r\n`def bodyI(i, out):\r\n    j = tf.constant(0)\r\n    i = i + 1\r\n    return [i, out]`\r\nAnd the main loop:\r\n`with tf.compat.v1.Session(config=config) as session:\r\n    hl = tf.constant('Hello')\r\n    out = tf.Variable([])\r\n    i = tf.constant(0)\r\n    _, out = tf.while_loop(condI, bodyI, [i, out], shape_invariants=[i.get_shape(), tf.TensorShape([None])])\r\n    session.run(tf.compat.v1.global_variables_initializer())\r\n    result = session.run([_, out])#result = session.run(loss_function(8))\r\n    print(\"TF APPEND GPU: \")\r\n    print(result[1])\r\n   session.close()`\r\nThe problem is to write a nested cycle. ", "comments": ["Here is the full code:\r\n```python\r\nimport tensorflow as tf\r\nimport sys\r\nimport matplotlib as mpl\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport pandas as pd\r\nimport tensorflow.compat.v1 as tfcf\r\nimport time\r\n\r\ntf.compat.v1.enable_eager_execution()\r\ntf.compat.v1.disable_v2_behavior()\r\n\r\ntf.random.set_seed(13)\r\n\r\nconfig=tf.compat.v1.ConfigProto(log_device_placement=True)\r\nconfig.gpu_options.visible_device_list='0'\r\nconfig.gpu_options.allow_growth = True\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.9\r\ntf.compat.v1.reset_default_graph()\r\n\r\nef condI(i, _):\r\n    return i < 10\r\n\r\ndef bodyI(i, out):\r\n     i = i + 1\r\n    return [i, out]\r\n\r\nwith tf.compat.v1.Session(config=config) as session:\r\n  hl = tf.constant('Hello')\r\n  out = tf.Variable([])\r\n  i = tf.constant(0)\r\n  _, out = tf.while_loop(condI, bodyI, [i, out], shape_invariants=[i.get_shape(), tf.TensorShape([None])])\r\n  session.run(tf.compat.v1.global_variables_initializer())\r\n  result = session.run([_, out])\r\n  print(\"TF APPEND GPU: \")\r\n  print(result[1])\r\n  session.close()\r\n```", "@vasilevskykv \r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "In my experience, using @tf.function and for loops works better than using tf.while_loop. Try it once (both in eager and graph modes).", "1. AttributeError: module 'tensorflow' has no attribute 'fucntion'\r\n2. In this case I have to create tensors from my numpy arrays\r\n3. for loops run slower than tf.while_loop on GPU", "I tried everything with tf.while_loop. I asked on StackOverflow but I got no answers. I think the problem is with **shape_invariants**.\r\nIn my opinion, **it's a bad feature that a developer should think about the initial shape of the loop variable**.  I think that **to allow the shape to vary across iterations must be by deafult**. So one more parameter should be. For exmaple, **shape_variation**, that can be **True** or **False**. If **shape_variation = False** - we need to define **shape_invariant**. If **shape_variation = True** - shape has been allowed to vary across iterations, so **shape_invariant** doesn't need to be defined.\r\n\r\nSo in my opinion, it is necessary to correct this feature in the way I suggested", "@vasilevskykv \r\n\r\nPlease, \r\n\r\n1. Share colab link or standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster.\r\n2. Share the `Tensorflow Version` you are using\r\n3. Try to reproduce the issue in `Tensorflow Version 2.x` in case you are using `Version 1.x`.\r\n\r\nThanks!", "I use tf-nightly 2.5.0", "https://stackoverflow.com/q/65122134/13934846\r\n", "Key-words:\r\npython-3.x\r\nnumpy\r\ntensorflow\r\ntensorflow2.0\r\nnested-loops", "@vasilevskykv,\r\nIt's bit confusing. Please help me understand the problem clearly.\r\n\r\n1.  What is the problem with your code specified in [this comment](https://github.com/tensorflow/tensorflow/issues/45337#issuecomment-737175584)?\r\n2. I think you want to implement **`Nested For Loops`** using **`tf.while`** but **`Nested For Loops`** could neither be found in [this Code](https://github.com/tensorflow/tensorflow/issues/45337#issuecomment-737175584) nor in this [Stack Overflow Question](https://stackoverflow.com/questions/65122134/python-tensorflow-rewrite-classic-for-loop-into-tf-while-loop). \r\n\r\nIf this is your problem statement, please share the code with **`Nested For Loops`** and we will help you to implement it using **`tf.while`**. \r\n\r\nPlease correct me if I'm missing something. Thanks!", "I confirm that the problem is to implement the following code in **tf.while_loop**\r\n\r\n```python\r\ndef multivariate_data(dataset, target, start_index, end_index, history_size,\r\n                      target_size, step, single_step=False):\r\n  data = []\r\n  labels = []\r\n  start_index = start_index + history_size\r\n  if end_index is None:\r\n    end_index = len(dataset) - target_size\r\n  #print(history_size)\r\n  for i in range(start_index, end_index):\r\n    indices = range(i-history_size, i, step)\r\n    data.append(dataset[indices])\r\n    if single_step:\r\n      labels.append(target[i+target_size])\r\n    else:\r\n      labels.append(target[i:i+target_size])\r\n  return np.array(data), np.array(labels)\r\n```\r\n\r\nThis is the code in my **Stack Overflow Question**", "@vasilevskykv,\r\nPlease find the below code which uses `Python While Loop`, which is written by **Francois Chollet, Father of Keras**.  You can find the complete code in his [Github Page](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.3-advanced-usage-of-recurrent-neural-networks.ipynb).\r\n\r\n```python\r\ndef generator(data, lookback, delay, min_index, max_index, shuffle = False, batch_size = 128, step = 6):\r\n    if max_index == None:\r\n        max_index = len(data) - 1 - delay\r\n    \r\n    # This variable is used if Shuffle = False \r\n    i = min_index + lookback\r\n    \r\n    while 1:\r\n        # Shuffle is significant for Training and doesn't make much difference for Validation and Test Data\r\n        if shuffle == True:        \r\n            # Shuffle the Indices such that it returns 128 (batch_size) Random Values between Lookback and max_index\r\n            rows = np.random.randint(min_index + lookback, max_index, size = batch_size)\r\n        else:\r\n            if i + batch_size >= max_index:\r\n                i = min_index + batch_size\r\n            rows = np.arange(min_index + lookback, min(i + lookback, max_index))\r\n            i += len(rows)\r\n            \r\n        # Create a Numpy Zeros Array for Features (Samples) and Labels (Targets)\r\n        samples = np.zeros(shape = (len(rows), lookback//step, data.shape[-1]))\r\n        targets = np.zeros(shape = (len(rows),))\r\n        for j,row in enumerate(rows):\r\n            indices = range(row - lookback, row, step)\r\n            samples[j] = data[indices]\r\n            targets[j] = data[row + delay][1]\r\n        yield samples, targets\r\n```", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45337\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45337\">No</a>\n", "[Linking to the section](https://www.tensorflow.org/guide/function#loops) in the @tf.function guide that explains how using autograph makes it easier to write control flow (so you don't have to use `tf.while_loop`)."]}, {"number": 45336, "title": "built wheel is not complete", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows Server 2019\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.7.7.1\r\n- Installed using virtualenv? pip? conda?: NA\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: Gforce MX940\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI Build Tensorflow 2.3.1 from source, because my graphic card (Compute Capabiliy 5) is not anymore supported by the binary releases. The Build process as such works ok, but when building the wheel and I install it to another system I miss some components.\r\n\r\ne.g all the models under keras.applications are just not part of the wheel.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nBasically I follow the instructions from here:\r\n\r\nhttps://www.tensorflow.org/install/source_windows\r\n\r\nbazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --define=no_tensorflow_py_deps=true\r\n\r\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package ..\\tensorflow_pkg\r\n\r\nThe wheel is created and it is functional ... just parts of the python sources is missing\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\nPlease let me know which logs you need, and I will provide.", "comments": ["Are you missing some APIs or just some source files that exist in the repo don't get exposed in the wheel?", "> \r\n> \r\n> Are you missing some APIs or just some source files that exist in the repo don't get exposed in the wheel?\r\n\r\nAs indicated above I think the API is complete but I am missing python sources e.g. whatever is under:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/r2.3/tensorflow/python/keras/applications\r\n", "I don't think they are included in the pip package, by design.", "@mihaimaruseac\r\n\r\nThe  [wheel](https://files.pythonhosted.org/packages/5d/6a/9669836f813b73fe5abf5e9f118ccc9b7fb060f02789d385825b0943f9c8/tensorflow-2.3.1-cp37-cp37m-win_amd64.whl) distributed through pypi has these files ...\r\n\r\n![grafik](https://user-images.githubusercontent.com/8713992/101737893-c26fb080-3ac5-11eb-9591-0f90263dbd56.png)\r\n\r\n\r\n\r\n\r\n", "That's the macos wheel, the bug is about a windows wheel.\r\n\r\nWindows wheels are smaller because of maximum size limits under windows (we cannot create a zip file larger than 2GB at one point during the build).", "@mihaimaruseac\r\n> That's the macos wheel, the bug is about a windows wheel.\r\n\r\nTrue, Sorry for the confusion. However also the windows wheels have the files (see updated screenshot above)\r\n\r\n", "What commit are you building from?", "Tag 2.3.1", "This is weird. We are using the exact same command.\r\n\r\nCan you change\r\n\r\n```\r\nbazel-bin\\tensorflow\\tools\\pip_package\\build_pip_package ..\\tensorflow_pkg\r\n```\r\n\r\nto have a path argument that does not share prefix with the path to the TF sources?\r\n\r\nAlternatively, can you also try on the 2.4 branch?", "actually I now had a closer look into the created wheel ... and the files are actually there ...\r\ntrying to figure out why they haven't been installed in the first place ... could'nt reproduce ...\r\n\r\nI have to appologize for creating the noise here ... not sure what happended", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45336\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45336\">No</a>\n"]}, {"number": 45335, "title": "Tensorboard ignores --prefix_path", "body": "**Describe the current behavior**\r\n\r\nSome tensorboards installed with tensorflow couldn't access the right URLs.\r\n\r\nReproducible steps\r\n\r\n1. install a tf 2.3.1\r\n2. start tensorboard with prefix\r\n\r\n```\r\ntensorboard --logdir logs --port 5001 --bind_all  --path_prefix /abc/def\r\n```\r\n\r\nWe got `failed to fetch runs`\r\n\r\n![image](https://user-images.githubusercontent.com/193223/100859743-93a47980-34ca-11eb-9fe8-74ac99ca12f6.png)\r\n\r\nBecause some requests did not add `abc/def` prefix to the base URL:\r\n\r\n![image](https://user-images.githubusercontent.com/193223/100860177-22b19180-34cb-11eb-9150-97855bf88f5e.png)\r\n\r\n \r\n**Describe the expected behavior**\r\n\r\nAll tensorboard requests should add prefix to the base URL if `--path_prefix` has given.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nI created a regression tests project to investigate the problem\r\n\r\nhttps://github.com/qrtt1/tensorboard-regression/runs/1487646153\r\n\r\nIt got worse after tf 2.3.0 (they use tensorboard `2.4.0`):\r\n\r\n![image](https://user-images.githubusercontent.com/193223/100905996-a1c2bc00-3503-11eb-9f60-27407e10d947.png)\r\n\r\n\r\n\r\n", "comments": ["move to https://github.com/tensorflow/tensorboard/issues/4421", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45335\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45335\">No</a>\n"]}, {"number": 45334, "title": "Fix esp32 ci build", "body": "`readable_run export ` do not export variable for the environment\r\n\r\nUsed direct export instead.\r\n\r\naddresses: https://github.com/tensorflow/tensorflow/issues/44346#issuecomment-737107160", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n", "cc @advaitjain\r\n\r\nCan we please have this ESP32 run enabled somehow?", "> cc @advaitjain\r\n> \r\n> Can we please have this ESP32 run enabled somehow?\r\n\r\nI'm happy to give some pointers on how to add a community supported build for ESP similar to what we have [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro#community-supported-builds). Let's continue this conversation on the [SIG-micro](https://groups.google.com/a/tensorflow.org/g/micro) mailing list.\r\n"]}, {"number": 45333, "title": "MLIR", "body": "**System information**\r\n- Windows 10 Pro\r\n- TensorFlow source, 2.3.1\r\n- targeting Cortex-M4, Cortex-M7, Cortex-M33, ARM CortexM flavors\r\n\r\n**Describe the problem**\r\nI'd like to know if there any connection between TensorFlow-MLIR and TensorFlow Lite for Microcontrollers?\r\nCan TensorFlow-MLIR support TensorFlow Lite for Microcontrollers for ARM Cortex M flavors?\r\n\r\nThank you for your insight.\r\n\r\n\r\n", "comments": ["This is already tracked at https://llvm.discourse.group/t/mlir-for-tf-mikro-arm-cortex-m/2331/3", "/cc @jpienaar ", "Since the issue is being addressed at LLVM Discussion Forum as tagged above we may continue the discussion on that thread. Closing this issue for now. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45333\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45333\">No</a>\n"]}, {"number": 45332, "title": "Didn't find op for builtin opcode 'SOFTMAX' version '1' error on Cortex-M7", "body": "@tensorflow/micro\r\n\r\n**System information**\r\n- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): i.MX8MP's Cortex-M7 core\r\n\r\n**Describe the problem**\r\nI am running tflite micro on i.MX8MP's Cortex-M7 core with mobilenet_v1_1.0_224_quant.tflite. I also enable cmsis-nn in the building. According to the following log, the operators CONV_2D, DEPTHWISE_CONV_2D, SOFTMAX.. are all registered. But SOFTMAX isn't found. It is the last operator for the loop in PrepareNodeAndRegistrationDataFromFlatbuffer().\r\n\r\nMy test code is very simple,\r\n\r\nconst tflite::Model *model = ::tflite::GetModel(job.networkModel.data);\r\ntflite::AllOpsResolver micro_op_resolver;\r\n \r\ntflite::MicroInterpreter interpreter(model, micro_op_resolver, inferenceProcessTensorArena, TENSOR_ARENA_SIZE, reporter);\r\nTfLiteStatus allocate_status = interpreter.AllocateTensors();\r\n...\r\n\r\nLog:\r\n..\r\nRegister cmsis-nn CONV_2D\r\nRegister cmsis-nn DEPTHWISE_CONV_2D\r\nRegister Kernel common RESHAPE\r\nRegister cmsis-nn SOFTMAX\r\n..\r\nmicro_interpreter.cc AllocateTensors\r\nmicro_allocator.cc: subgraph->operators()->size() 31\r\n..\r\nmicro_allocator.cc: PrepareNodeAndRegistrationDataFromFlatbuffer i: 30\r\nmicro_allocator.cc: To run GetRegistrationFromOpCode for op code SOFTMAX\r\nop_solver.cc: GetRegistrationFromOpCode\r\nDidn't find op for builtin opcode 'SOFTMAX' version '1'\r\nFailed to get registration from op code SOFTMAX\r\n \r\nAllocateTensors failed for inference job: job, status 1\r\n", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45332\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45332\">No</a>\n", "Found the root cause and close it."]}, {"number": 45331, "title": "ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version:2.3.1\r\n- Python version:3.8.5\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):?\r\n- GCC/Compiler version (if compiling from source):?\r\n- CUDA/cuDNN version:?\r\n- GPU model and memory: Intel(R) UHD Graphics 600\r\n\r\n\r\nError when trying to import TensorFlow\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have just installed TensorFlow via cmd command: pip install tensorflow\r\nWhen I try to use it in Pycharm and check if everything is working:\r\n\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\nIt gives this error:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\El\u00e8ve\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:/Users/El\u00e8ve/PycharmProjects/pythonProject1/main.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\El\u00e8ve\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\__init__.py\", line 41, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"C:\\Users\\El\u00e8ve\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 40, in <module>\r\n    from tensorflow.python.eager import context\r\n  File \"C:\\Users\\El\u00e8ve\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 35, in <module>\r\n    from tensorflow.python import pywrap_tfe\r\n  File \"C:\\Users\\El\u00e8ve\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\", line 28, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\El\u00e8ve\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 83, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Users\\El\u00e8ve\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\r\n    from tensorflow.python._pywrap_tensorflow_internal import *\r\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/errors\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\nProcess finished with exit code 1", "comments": ["@Skylark055 \r\n\r\nWhat is make/model of your cpu?\r\nI suspect your cpu model does not support AVX instructions sets.See [hardware requirements](https://www.tensorflow.org/install/pip?lang=python3#hardware-requirements)\r\nMake sure to download the [latest microsoft visual c++ redistributable from here](https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads).\r\n.Also, please follow the instructions from to install from [Tensorflow website](https://www.tensorflow.org/install/source_windows).\r\nPlease, check Your CPU/Python is on 32 bits?Please, refer #36167\r\n\r\nThanks!", "I think I've found the issue...\r\nMy cpu is an intel core Celeron(R) which is listed on the wiki as incompatible\r\nSorry I didn't notice beforehand\r\n![image](https://user-images.githubusercontent.com/75360414/100857961-a2b90680-348d-11eb-93e0-8d7f96d3da0f.png)\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45331\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45331\">No</a>\n"]}, {"number": 45329, "title": "variable_scope inherits attributes", "body": "```python\r\nimport tensorflow\r\ndef _custom_getter(getter, *args, **kwargs):\r\n  return getter(*args, **kwargs) \r\nwith tf.compat.v1.variable_scope('', custom_getter=_custom_getter):\r\n  scope = tf.compat.v1.get_variable_scope()\r\n  scope2 = tf.compat.v1.variable_scope(scope)\r\n  print(scope2._custom_getter) # return None\r\n```\r\nNew ```variable_scope``` doesn't inherit last ```variable_scope```'s attributes. Is it expected behavior?", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/021798093fbbf45e93770d191af2a52a/45329.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/ee4b1f520eb8a7eb9723939f6a7f3595/45329-tf-nightly.ipynb). Please check the linked gist for reference. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45329\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45329\">No</a>\n"]}, {"number": 45327, "title": "Training in Colab always gives an error of \"FailedPreconditionError\"", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Google Colab\r\n\r\nStarted training in Colab but unfortunately, I am always having this error:\r\n`\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.moving_mean\r\nW1202 07:14:44.360409 139666145298304 util.py:150] Unresolved object in checkpoint: (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.moving_mean\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.moving_variance\r\nW1202 07:14:44.360842 139666145298304 util.py:150] Unresolved object in checkpoint: (root).model._box_predictor._base_tower_layers_for_heads.class_predictions_with_background.4.10.moving_variance\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\nW1202 07:14:44.361112 139666145298304 util.py:158] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\nWARNING:tensorflow:FailedPreconditionError: models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint; Is a directory\r\nW1202 07:14:44.364649 139666145298304 checkpoint_management.py:295] FailedPreconditionError: models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint; Is a directory\r\nWARNING:tensorflow:models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint: Checkpoint ignored\r\nW1202 07:14:44.364827 139666145298304 checkpoint_management.py:296] models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint: Checkpoint ignored\r\nWARNING:tensorflow:FailedPreconditionError: models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint; Is a directory\r\nW1202 07:14:44.365584 139666145298304 checkpoint_management.py:295] FailedPreconditionError: models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint; Is a directory\r\nWARNING:tensorflow:models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint: Checkpoint ignored\r\nW1202 07:14:44.365750 139666145298304 checkpoint_management.py:296] models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint: Checkpoint ignored\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 300, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"model_main_tf2.py\", line 110, in main\r\n    record_summaries=FLAGS.record_summaries)\r\n  File \"/usr/local/lib/python3.6/dist-packages/object_detection-0.1-py3.6.egg/object_detection/model_lib_v2.py\", line 627, in train_loop\r\n    manager.save()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_management.py\", line 819, in save\r\n    self._record_state()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_management.py\", line 728, in _record_state\r\n    save_relative_paths=True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_management.py\", line 248, in update_checkpoint_state_internal\r\n    text_format.MessageToString(ckpt))\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 570, in atomic_write_string_to_file\r\n    rename(temp_pathname, filename, overwrite)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 529, in rename\r\n    rename_v2(oldname, newname, overwrite)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 546, in rename_v2\r\n    compat.as_bytes(src), compat.as_bytes(dst), overwrite)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint.tmp1e96a55bc7b14447b94ca71713ebdea9; Is a directory\r\n`\r\n\r\nI don't have any idea why it keeps saying the checkpoint.tmp is a directory?\r\n\r\nAppreciate your help.\r\n\r\nThis is my [pipeline.config](https://gist.github.com/androuino/200aa56039b1cfeb02ed6bb301ffa51a)", "comments": ["@androuino \r\n\r\nRequest you to flll [issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\nPlease, share colab link or simple standalone code with supporting files to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "Hi @ravikyram, thank you for your response. This is the colab [link](https://colab.research.google.com/drive/1rrXAduY_KqezWmfID7Q6c5h92W9nWxw5?usp=sharing).\r\n\r\nPlease use this [link](https://www.kaggle.com/jipingsun/object-detection-obama/download) for a simple set of data for training.\r\n\r\nAnd this is the [link](https://medium.com/swlh/tensorflow-2-object-detection-api-with-google-colab-b2af171e81cc) I am following for the tutorial.\r\n\r\nThank you and have a nice day.", "@androuino \r\n\r\nPlease, grant me the access for the colab link. Thanks!", "Hi @ravikyram, thanks for your fast reply. Please kindly check if you already have access to my colab notebook.\n\nThank you.\n\nHave a great day.", "@androuino \r\n\r\nThis issue is more suitable for TensorFlow Models repo. Please post it on Tensorflow Models repo from [here](https://github.com/tensorflow/models/issues/new/choose). Thanks!", "@ravikyram, oh ok. Thanks for routing me to models repo.", "@androuino \r\n\r\nPlease, close this thread here and we can track the issue in Tensorflow Models repo. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45327\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45327\">No</a>\n"]}, {"number": 45325, "title": "tf.keras.preprocessing.image.random_shift consistently failing in graph mode", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 7.6.5\r\n- GPU model and memory: 32G V100, 4G GTX 1650 with Max-Q Design\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n```\r\ntf.compat.v1.disable_eager_execution()\r\nimage = tf.random.uniform([3,32,32])\r\nimg = tf.keras.preprocessing.image.random_shift(image, hrg=0.3, wrg=0.3)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/dz/.conda/envs/py37cu101/lib/python3.7/site-packages/keras_preprocessing/image/affine_transformations.py\", line 89, in random_shift\r\n    order=interpolation_order)\r\n  File \"/home/dz/.conda/envs/py37cu101/lib/python3.7/site-packages/keras_preprocessing/image/affine_transformations.py\", line 323, in apply_affine_transform\r\n    x = np.rollaxis(x, channel_axis, 0)\r\n  File \"<__array_function__ internals>\", line 6, in rollaxis\r\n  File \"/home/dz/.conda/envs/py37cu101/lib/python3.7/site-packages/numpy/core/numeric.py\", line 1259, in rollaxis\r\n    n = a.ndim\r\nAttributeError: 'Tensor' object has no attribute 'ndim'\r\n```\r\n**Describe the expected behavior**\r\nRandom shift the image with no error\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\nimage = tf.random.uniform([3,32,32])\r\nimg = tf.keras.preprocessing.image.random_shift(image, hrg=0.3, wrg=0.3)\r\n```\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue with [TF v2.3](https://colab.research.google.com/gist/amahendrakar/1db118e00f20bed9fac0f4f380d61605/45325.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/1a15fd8ae4052b09952d5c616e6cda57/45325-tf-nightly.ipynb). Please find the attached gist. Thanks!", "Greetings! Any update on this?", "@donglinz Sorry for the late response. In the graph mode, as the error shows 'Tensor' object has no attribute 'ndim'. However running it in eager mode will work without any issue.  [Here](https://colab.research.google.com/gist/jvishnuvardhan/9a7ec379c7ea3cacc3a77173eb35bdf9/45325-tf-nightly.ipynb) is a gist for our reference. \r\n\r\nWhy do you want to disable eager execution? Is there any specific reason for your use-case? Thanks!", "Hi @jvishnuvardhan\r\nThanks for the response! I was using tf.data.Dataset.map for data augmentation which has to be running in the graph mode. I know I can use tf.py_function to wrap some python logic to do the same thing in graph mode as tf.keras.preprocessing.image.random_shift did in eager mode. But generally speaking, tf.py_function have much lower performance compared to built-in tensorflow operations. Does tensorflow have any built-in alternatives that support shift image randomly in graph model?", "Hello,\r\n\r\n`tf.keras.preprocessing.image.random_shift` explicity only works with NumPy arrays (or eager tensors), as specified in the documentation. It isn't meant to operate on graph tensors.\r\n\r\nAlso note that it is outdated functionality. If you need equivalent functionality that works on both arrays and tensors and in graph mode, you can use the preprocessing layer `RandomTranslation`: https://keras.io/api/layers/preprocessing_layers/image_preprocessing/random_translation/\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45325\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45325\">No</a>\n"]}, {"number": 45324, "title": "When tf.image.central_crop taking a tensor as `central_fraction` argument, it raises OperatorNotAllowedInGraphError.", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0-rc3\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source): 3.1\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 11.1 / 8\r\n- GPU model and memory: rtx 3080 10GB\r\n\r\n\r\nColab notebook : https://colab.research.google.com/drive/1owELtE9_mTh1HCx20Yt3fJ3ynXLvds_x#scrollTo=rrehHBjoLUpT\r\n\r\n**Is it prohibited to pass a tensor to `central_fraction` argument of tf.image.central_crop function?**\r\nIt raises OperatorNotAllowedInGraphError.\r\n\r\nWhen used with map of tfdataset, the function raises the error. But using it with a eager tensor doesn't.\r\n\r\n\r\n\r\n\r\n", "comments": ["I have tried in colab with TF version 2.4, nightly version(`2.5.0-dev20201202`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/8d62aff3a12336dd515b4a18313efe27/untitled560.ipynb).Thanks!", "Added a PR #45613 for the fix.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45324\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45324\">No</a>\n"]}, {"number": 45323, "title": "illegal hardware instruction  python while import tensorflow2.2.0", "body": "**My System information**\r\n- OS Platform and Distribution (e.g.MacOS Bigsur 11.0 M1 Chip):\r\n- TensorFlow installed from (pip3 install tensorflow==2.2.0):\r\n- TensorFlow version (tf2.2.0):\r\n\r\nI use pip3 install tensorflow==2.2.0,but when i import tensorflow as tf, it gave me an error \"illegal hardware instruction  python3\", my MacOS is BigSur 11.0,M1 chip. is this problem from BigSur or M1 chip? how to solve it ?", "comments": ["@moseshu,\r\nThe Apple TF on M1 chips is a private fork of TF owned by Apple. We don't have access to fixing code issues there.\r\n\r\nFor more information, please go through issue [#44751](https://github.com/tensorflow/tensorflow/issues/44751#issuecomment-729896963). Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45323\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45323\">No</a>\n"]}, {"number": 45321, "title": "TensorFlow Lite conversion fails due to tf.linalg.diag_part", "body": "**System information**\r\n- OS Platform and Distribution: google colab (same issue locally on Debian GNU/Linux 10)\r\n- TensorFlow installed from: PyPI (`pip install`)\r\n- TensorFlow version: 2.3.0 (google colab), and 2.3.1 (locally)\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\nI am trying to convert a model that uses [tf.linalg.diag_part](https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part) to TensorFlow Lite. As far as I understand, `tf.linalg.diag_part` is not directly supported, but it should be possible to convert the model by setting `converter.target_spec.supported_ops` (as described [here](https://www.tensorflow.org/lite/guide/ops_select#convert_a_model). `tf.linalg.diag_part` seems to rely on `MatrixDiagPartV3`, which is included in the [list](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/flex/allowlisted_flex_ops.cc) of operations that can be enabled. Here is a minimal example ([colab notebook](https://colab.research.google.com/drive/1-sYbh6In8Dubki0UtcnzjmEZuMiMa-Gd?usp=sharing)):\r\n```python\r\nimport tensorflow as tf\r\n\r\nmodel_input = tf.keras.Input(shape=(3, 3), batch_size=1)\r\n\r\ndiagonal = tf.linalg.diag_part(model_input)\r\n\r\nmodel = tf.keras.models.Model(inputs=model_input, outputs=diagonal)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n\r\n# Enable TensorFlow ops that are not directly supported by tf lite\r\n# https://www.tensorflow.org/lite/guide/ops_select#convert_a_model\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n\r\ntflite_model = converter.convert()\r\n```\r\n\r\n**The output from the converter invocation**\r\n\r\n```bash\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\r\nINFO:tensorflow:Assets written to: /tmp/tmpkg1mi0e5/assets\r\n\r\n---------------------------------------------------------------------------\r\n\r\nException                                 Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    198                                                  debug_info_str,\r\n--> 199                                                  enable_mlir_converter)\r\n    200       return model_str\r\n\r\n5 frames\r\n\r\nException: /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:1073:0: error: 'tf.MatrixDiagPartV3' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:1167:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py:804:0: note: called from\r\n<ipython-input-1-9932e84c0ae3>:23:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:1073:0: note: see current operation: %0 = \"tf.MatrixDiagPartV3\"(%arg0, %cst, %cst_0) {T = f32, _cloned = true, align = \"RIGHT_LEFT\", device = \"\"} : (tensor<1x3x3xf32>, tensor<i32>, tensor<f32>) -> tensor<1x3xf32>\r\n<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.MatrixDiagPartV3 {T = f32, _cloned = true, align = \"RIGHT_LEFT\", device = \"\"}\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<1x3x3xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_0 = \"std.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n  %0 = \"tf.MatrixDiagPartV3\"(%arg0, %cst, %cst_0) {T = f32, _cloned = true, align = \"RIGHT_LEFT\", device = \"\"} : (tensor<1x3x3xf32>, tensor<i32>, tensor<f32>) -> tensor<1x3xf32>\r\n  \"std.return\"(%0) : (tensor<1x3xf32>) -> ()\r\n}) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"input_1\", outputs = \"Identity\"}, type = (tensor<1x3x3xf32>) -> tensor<1x3xf32>} : () -> ()\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nConverterError                            Traceback (most recent call last)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\r\n    200       return model_str\r\n    201     except Exception as e:\r\n--> 202       raise ConverterError(str(e))\r\n    203 \r\n    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:\r\n\r\nConverterError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:1073:0: error: 'tf.MatrixDiagPartV3' op is neither a custom op nor a flex op\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:1167:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py:804:0: note: called from\r\n<ipython-input-1-9932e84c0ae3>:23:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2882:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2822:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py:537:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py:208:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py:399:0: note: called from\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:1073:0: note: see current operation: %0 = \"tf.MatrixDiagPartV3\"(%arg0, %cst, %cst_0) {T = f32, _cloned = true, align = \"RIGHT_LEFT\", device = \"\"} : (tensor<1x3x3xf32>, tensor<i32>, tensor<f32>) -> tensor<1x3xf32>\r\n<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):\r\n\ttf.MatrixDiagPartV3 {T = f32, _cloned = true, align = \"RIGHT_LEFT\", device = \"\"}\r\n<unknown>:0: note: see current operation: \"func\"() ( {\r\n^bb0(%arg0: tensor<1x3x3xf32>):  // no predecessors\r\n  %cst = \"std.constant\"() {value = dense<0> : tensor<i32>} : () -> tensor<i32>\r\n  %cst_0 = \"std.constant\"() {value = dense<0.000000e+00> : tensor<f32>} : () -> tensor<f32>\r\n  %0 = \"tf.MatrixDiagPartV3\"(%arg0, %cst, %cst_0) {T = f32, _cloned = true, align = \"RIGHT_LEFT\", device = \"\"} : (tensor<1x3x3xf32>, tensor<i32>, tensor<f32>) -> tensor<1x3xf32>\r\n  \"std.return\"(%0) : (tensor<1x3xf32>) -> ()\r\n}) {sym_name = \"main\", tf.entry_function = {control_outputs = \"\", inputs = \"input_1\", outputs = \"Identity\"}, type = (tensor<1x3x3xf32>) -> tensor<1x3xf32>} : () -> ()\r\n```\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\nThe minimal example does not include a saved model, but see here for the [colab notebook](https://colab.research.google.com/drive/1-sYbh6In8Dubki0UtcnzjmEZuMiMa-Gd?usp=sharing).\r\n\r\n**Failure details**\r\nn/a\r\n\r\n**RNN conversion support**\r\nn/a\r\n\r\n**Any other info / logs**\r\n\r\nI encountered this conversion problem both in TensorFlow 2.3.0 (google colab), and in 2.3.1 (locally, installed via `pip`). Taking the diagonal of matrix is an operation that can be relevant when trying to build lightweight models, e.g. for use in resource constrained environments. In my use case, I am trying to take the diagonal of a Gram matrix in an image style application.", "comments": ["@ingo-m \r\n\r\nAfter adding `converter.allow_custom_ops=True` I am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/cecb0727a4ee880c07db84cb1fdcdd14/untitled555.ipynb).Thanks!", "@ravikyram Thanks for your quick reply. Nice, I wasn't aware of that. Perhaps the missing line should be added to the [guide](https://www.tensorflow.org/lite/guide/ops_select#convert_a_model), so that others don't run into the same problem? I submitted a pull request: https://github.com/tensorflow/tensorflow/pull/45356", "@ingo-m \r\n\r\nPlease,close this thread if your issue was resolved. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45321\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45321\">No</a>\n", "Unfortunately I have to reopen this issue - I had overlooked that when enabling custom operations (`converter.allow_custom_ops = True`), the model can be compiled and saved, but can't perform inference. I updated the [colab notebook](https://colab.research.google.com/drive/1-sYbh6In8Dubki0UtcnzjmEZuMiMa-Gd?usp=sharing) with the complete example, tested in TensorFlow 2.3.1.\r\n\r\nSpecifically, after loading the TensorFlow lite model from disk, `interpreter.allocate_tensors()` fails with the following traceback:\r\n```bash\r\n---------------------------------------------------------------------------\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\n\r\n<ipython-input-4-e8f284b41149> in <module>()\r\n----> 1 interpreter.allocate_tensors()\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in allocate_tensors(self)\r\n    241   def allocate_tensors(self):\r\n    242     self._ensure_safe()\r\n--> 243     return self._interpreter.AllocateTensors()\r\n    244 \r\n    245   def _safe_to_run(self):\r\n\r\nRuntimeError: Encountered unresolved custom op: MatrixDiagPartV3.Node number 0 (MatrixDiagPartV3) failed to prepare.\r\n\r\n```", "PS: The user [guide](https://www.tensorflow.org/lite/guide/ops_select#python) mentions: \"TensorFlow Lite with select TensorFlow ops are available in the TensorFlow pip package version since 2.3 for Linux\", so the [colab example](https://colab.research.google.com/drive/1-sYbh6In8Dubki0UtcnzjmEZuMiMa-Gd?usp=sharing) should work, right?", "Apologies for the delay in response. This is fixed latest TF 2.4.1 release.\r\nSee [gist](https://colab.research.google.com/gist/ymodak/c11224c3f457417dd72b3c246c9fd928/tflite_diag_part_min_example.ipynb). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45321\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45321\">No</a>\n"]}, {"number": 45320, "title": "Why do you want to ruin go's library?", "body": "The only I want to say is \"for_core_protos_go_proto\"!\r\nI am very, very disappointed", "comments": ["@jacobxy,\r\nCould you please elaborate and let us know the issue you are facing, so that we can look into it. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as not a relevant issue, likely spam"]}, {"number": 45319, "title": "Support for CUDA 11.1 and RTX 3090", "body": "**System information**\r\n- TensorFlow version: tf-nightly-gpu-2.5.0.dev20201201\r\n- System: Ubuntu 18.04\r\n- CUDA: 11.1\r\n- NVIDIA Driver: 455.23.05\r\n\r\nI just installed `tf-nightly-gpu-2.5.0.dev20201201` on my machine, and trying to run a test script, and errors shows below:\r\n\r\n![image](https://user-images.githubusercontent.com/13287220/100816986-3dabe380-3482-11eb-9ffc-e2691251940c.png)\r\n\r\n", "comments": ["@jediyoda36,\r\nPlease take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/43947#issuecomment-715295153) from issue #43947 with a similar error and let us know if it helps. Thanks!\r\n\r\n", "@amahendrakar,\r\nI'm not sure whether I can do this. I'm also using PyTorch in other project. If I change this soft link, will it affect PyTorch?\r\n\r\nAnd I tried it, didn't work, same error.", "I've resolved this problem with the method mentioned in [this issue]( https://github.com/tensorflow/tensorflow/issues/44682#issuecomment-732283634)\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45319\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45319\">No</a>\n"]}, {"number": 45318, "title": "Update version numbers for TensorFlow 2.4.0-rc4", "body": "Before merging this PR, please double check that it has correctly updated\n`core/public/version.h`, `tools/pip_package/setup.py`, and\n`tensorflow/tensorflow.bzl`. Also review the execution notes below:\n\n```\nMajor: 2 -> 2\nMinor: 4 -> 4\nPatch: 0 -> 0\n\nNo lingering old version strings \"2.4.0-rc3\" found in source directory \n\"tensorflow/\". Good.\nNo lingering old version strings \"2.4.0rc3\" found in source directory \n\"tensorflow/\". Good.\n```", "comments": []}, {"number": 45317, "title": "tf.shape is broken when drop_remainder=True on TPUs", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.0\r\n- Python version: 3.7.7\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): n/a\r\n- CUDA/cuDNN version: n/a\r\n- GPU model and memory: n/a\r\n\r\n**Describe the current behavior**\r\ntf.shape gives incorrect tensor shapes when used with `drop_remainder=False` while batching a Dataset on TPUs.\r\n\r\n**Describe the expected behavior**\r\ntf.shape should always return the correct tensor shape regardless of the value of `drop_remainder`.\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1EcQMGADKDdVTo5yETzYr3IUfqHjDMzX0?usp=sharing\r\n", "comments": ["I have tried in colab with TF 2.3 and was able to reproduce the issue. However in TF nightly i am seeing the error (`NotFoundError: 'OptimizeDatasetV2' is neither a type of a primitive operation nor a name of a function registered in binary running on n-6e00c1af-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:__inference_run_576]`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1e772687b9d16778dfcdc51d2a9ff803/untitled556.ipynb). Thanks!", "Please remove `with tpu_strategy.scope():` from your code, as you are already running through `strategy.run`.\r\n\r\nThis should run correctly:\r\n```\r\n@tf.function\r\ndef run(ds, strategy):\r\n  def step(features):\r\n    x = tf.concat([features['a'], features['b']], axis=0)\r\n    return x, tf.shape(x)[0]\r\n  return strategy.run(step, args=(next(ds),))\r\n\r\nprint(run(ds, tpu_strategy))\r\n```\r\n\r\nAlso, when running on a TPU, please choose your batch size to be a multiple of the number of cores (8 in this case).\r\n\r\nIf you need to run tf-nightly in colab, besides installing Tensorflow `tf-nightly` version in colab, you will also need to choose a corresponding Tensorflow version on TPU, please see [here](https://stackoverflow.com/questions/63180202/invalidargumenterror-while-initializing-ttpu/63271669#63271669) for the commands.\r\n", "> Please remove `with tpu_strategy.scope():` from your code, as you are already running through `strategy.run`.\r\n> \r\n> This should run correctly:\r\n> \r\n> ```\r\n> @tf.function\r\n> def run(ds, strategy):\r\n>   def step(features):\r\n>     x = tf.concat([features['a'], features['b']], axis=0)\r\n>     return x, tf.shape(x)[0]\r\n>   return strategy.run(step, args=(next(ds),))\r\n> \r\n> print(run(ds, tpu_strategy))\r\n> ```\r\n> \r\n> Also, when running on a TPU, please choose your batch size to be a multiple of the number of cores (8 in this case).\r\n> \r\n> If you need to run tf-nightly in colab, besides installing Tensorflow `tf-nightly` version in colab, you will also need to choose a corresponding Tensorflow version on TPU, please see [here](https://stackoverflow.com/questions/63180202/invalidargumenterror-while-initializing-ttpu/63271669#63271669) for the commands.\r\n\r\nHi, thank you for the tips! However, removing the scope() and changing the batch size doesn't seem to work unfortunately, as you can see here: https://colab.research.google.com/drive/1EcQMGADKDdVTo5yETzYr3IUfqHjDMzX0?usp=sharing", "> Hi, thank you for the tips! However, removing the scope() and changing the batch size doesn't seem to work unfortunately, as you can see here: https://colab.research.google.com/drive/1EcQMGADKDdVTo5yETzYr3IUfqHjDMzX0?usp=sharing\r\n\r\nLooks like this issue was fixed in tf-nightly.\r\n\r\nTo test on tf-nightly, you can use the following code in colab:\r\n\r\n```\r\n!pip install tf-nightly\r\n```\r\n\r\nRestart the runtime and run:\r\n```\r\n!pip install cloud-tpu-client\r\n\r\nimport tensorflow as tf\r\nfrom cloud_tpu_client import Client\r\nprint(tf.__version__)\r\n\r\nClient().configure_tpu_version(tf.__version__, restart_type='ifNeeded')\r\n```\r\n\r\nAfter which the shape should work correctly.\r\n\r\nAnother thing that you can try for TF 2.3 is disabling dynamic batch size in strategy.run:\r\n```\r\nstrategy.run(step, args=(next(ds),), options=tf.distribute.RunOptions(experimental_enable_dynamic_batch_size=False))\r\n```\r\n\r\n", "@gagika Yeah that works! Thanks for the help :+1: ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45317\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45317\">No</a>\n"]}, {"number": 45316, "title": "Fix the release build for xtensa.", "body": "Tested the following command now passes:\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release\r\n```\r\n\r\nFixes #45315\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45315, "title": "BUILD_TYPE=release broken for xtensa hifimini", "body": "@tensorflow/micro\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG keyword_benchmark BUILD_TYPE=release\r\n```\r\n\r\nfails with:\r\n```\r\nIn file included from tensorflow/lite/micro/micro_profiler.cc:16:\r\n./tensorflow/lite/micro/micro_profiler.h:63:26: error: private field 'reporter_' is not used [-Werror,-Wunused-private-field]\r\n  tflite::ErrorReporter* reporter_;\r\n                         ^\r\n1 error generated.\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:488: tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/obj/tensorflow/lite/micro/micro_profiler.o] Error 1\r\nmake: *** Waiting for unfinished jobs....\r\ntensorflow/lite/micro/micro_allocator.cc:187:18: error: private field 'reporter_' is not used [-Werror,-Wunused-private-field]\r\n  ErrorReporter* reporter_ = nullptr;\r\n                 ^\r\n1 error generated.\r\nmake: *** [tensorflow/lite/micro/tools/make/Makefile:487: tensorflow/lite/micro/tools/make/gen/xtensa_hifimini/obj/tensorflow/lite/micro/micro_allocator.o] Error 1\r\nIn file included from tensorflow/lite/micro/micro_interpreter.cc:15:\r\n./tensorflow/lite/micro/micro_interpreter.h:65:18: error: private field 'error_reporter_' is not used [-Werror,-Wunused-private-field]\r\n  ErrorReporter* error_reporter_ = nullptr;\r\n\r\n```\r\n", "comments": []}, {"number": 45314, "title": "ifdef out known failing test cases for Quantize for hifimini.", "body": "Manually confirmed that the following command passes:\r\n\r\n```\r\nmake -f tensorflow/lite/micro/tools/make/Makefile -j8 TARGET=xtensa OPTIMIZED_KERNEL_DIR=xtensa TARGET_ARCH=hifimini XTENSA_CORE=mini1m1m_RG test_kernel_quantize_test\r\n```\r\n\r\nRelated bugs: http://b/174603495, http://b/170297449\r\n", "comments": ["Thanks for contributing to TensorFlow Lite Micro.\n\nTo keep this process moving along, we'd like to make sure that you have completed the items on this list:\n   * Read the [contributing guidelines for TensorFlow Lite Micro](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md)\n   * Created a [TF Lite Micro Github issue](https://github.com/tensorflow/tensorflow/issues/new?labels=comp%3Amicro&template=70-tflite-micro-issue.md)\n   * Linked to the issue from the PR description\n   \n\nWe would like to have a discussion on the Github issue first to determine the best path forward, and then proceed to the PR review.\n"]}, {"number": 45313, "title": "TFLite: Did not get operators, tensors, or buffers in subgraph 0. Error running on Android", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 4a 5g\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: Not using GPU\r\n\r\n**Describe the current behavior**\r\nI am able to convert a concrete function to a TensorFlow Lite model without errors.  I do get errors when trying to run the model on Android.  The error occurs when running this line on Android: tflite = new Interpreter(buffer, opt);\r\n\r\n\r\nI don't believe this has to do with the Android code because I am able to run a TFLite model that I converted using a Keras model with this Android code.\r\n\r\nI've been trying to convert a larger model with more operations and tensors but have gotten stuck on this error, I simplified the code to this but error remains.\r\n\r\n\r\n**Android Error**\r\n\r\n```\r\njava.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Did not get operators, tensors, or buffers in subgraph 0.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:72)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:266)\r\n        at arl.testnmt.MainActivity.loadModel(MainActivity.java:127)\r\n        at arl.testnmt.MainActivity$2.run(MainActivity.java:108)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:223)\r\n        at android.os.HandlerThread.run(HandlerThread.java:67)\r\n```\r\n\r\n**Standalone code to reproduce the issue (Python TFLite conversion)**\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\n\r\nclass SimpleModel:\r\n    def infer_tflite(self, features):\r\n        return tf.zeros(shape=(250, 1), dtype=tf.dtypes.int32)\r\n\r\n\r\ndef tf_lite_convert():\r\n    print('TensorFlow version')\r\n    print(tf.__version__)\r\n    model = SimpleModel()\r\n\r\n    single_elem = tf.zeros(shape=[1, 20], dtype=tf.dtypes.int64)\r\n    print('Running predictions using tflite inference')\r\n    preds = model.infer_tflite(single_elem)\r\n    print('TFlite inference results')\r\n    print(preds)\r\n    print('Turning to Concrete function')\r\n    new_infer_fn = tf.function(model.infer_tflite, input_signature=[tf.TensorSpec((1, None), dtype=tf.int64)])\r\n    infer_concrete = new_infer_fn.get_concrete_function()\r\n\r\n    print('Running inference with concrete function')\r\n    preds = infer_concrete(single_elem)\r\n    print('Concrete Output prediction')\r\n    print(preds)\r\n\r\n    print('Saving to Tflite')\r\n    converter = tf.lite.TFLiteConverter.from_concrete_functions([new_infer_fn.get_concrete_function()])\r\n\r\n    converter.target_spec.supported_ops = [\r\n      tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\r\n      tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\r\n    ]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    converter.target_spec.supported_types = [tf.float16]\r\n    tflite_model_path = os.path.join('', 'lite_model.tflite')\r\n\r\n    tflite_model = converter.convert()\r\n    with tf.io.gfile.GFile(tflite_model_path, 'wb') as f:\r\n        f.write(tflite_model)\r\n\r\n\r\nif __name__ == '__main__':\r\n    tf_lite_convert()\r\n```\r\n\r\n**Standalone code to reproduce the issue (Android)**\r\n```\r\n@WorkerThread\r\n    public synchronized void loadModel(String modelPath) {\r\n        AssetManager assetManager = this.context.getResources().getAssets();\r\n        ByteBuffer buffer = loadModelFile(assetManager, modelPath);\r\n        if (buffer == null){\r\n            return;\r\n        }\r\n        Interpreter.Options opt = new Interpreter.Options();\r\n        opt.setNumThreads(NUM_LITE_THREADS);\r\n        tflite = new Interpreter(buffer, opt);\r\n    }\r\n```\r\n\r\n**Gradle Dependencies**\r\n```\r\ndependencies {\r\n    implementation fileTree(dir: 'libs', include: ['*.jar'])\r\n    androidTestImplementation('androidx.test.espresso:espresso-core:3.1.0', {\r\n        exclude group: 'com.android.support', module: 'support-annotations'\r\n    })\r\n    implementation 'androidx.appcompat:appcompat:1.2.0'\r\n    implementation 'org.tensorflow:tensorflow-lite:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:2.3.0'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.1.0-rc1'\r\n    implementation 'androidx.legacy:legacy-support-v4:1.0.0'\r\n    implementation 'com.google.android.material:material:1.2.0'\r\n    implementation 'androidx.constraintlayout:constraintlayout:1.1.3'\r\n    implementation 'org.apache.commons:commons-io:1.3.2'\r\n}\r\n```\r\n\r\nAny help would be greatly appreciated!\r\n", "comments": ["I get the same error using these Android nightly builds\r\n```\r\n    implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'\r\n    implementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n```\r\n\r\nThe same error occurs when running **2.4-rc3**\r\n\r\n```\r\n    java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: Did not get operators or tensors in subgraph 0.\r\n    \r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:72)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:282)\r\n        at arl.testnmt.MainActivity.loadModel(MainActivity.java:128)\r\n        at arl.testnmt.MainActivity$2.run(MainActivity.java:109)\r\n        at android.os.Handler.handleCallback(Handler.java:938)\r\n        at android.os.Handler.dispatchMessage(Handler.java:99)\r\n        at android.os.Looper.loop(Looper.java:223)\r\n        at android.os.HandlerThread.run(HandlerThread.java:67)\r\n```\r\n\r\nUsing the latest nightly version: **2.5-dev20201201** I also get the same error.\r\n\r\n\r\n", "I think the generated graph from your example is not correct. You can try uploading your `tflite` file in this [netron app](https://netron.app/) and visualize.", "I looked at the output through Netron and the model looks good, I believe.\r\n\r\nIt seems I can get it running if I comment out the line:\r\n`converter.optimizations = [tf.lite.Optimize.DEFAULT]`\r\n\r\nThis seems like a TensorFlow bug to me, I'm not sure why that would cause an issue. (I tested this with 2.4-rc3)", "Interesting. @gcervantes8 Could you provide your tflite to us and the snapshot of the Netron app?", "Hello @abattery I attached a zip with the models.\r\nTwo models; one with the optimizer (doesn't work), the other without the optimizer (works):\r\n[lite_models.zip](https://github.com/tensorflow/tensorflow/files/5654245/lite_models.zip)\r\n\r\nI am using version 4.6.5 of Netron\r\n", "This bug is related to the optimization toolkit. @daverim @teijeong could you take a look?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I'm still getting the bug, so I don't think the issue should be marked as stale or close.", "I can reproduce this issue. Will take a look shortly.", "@gcervantes8 The generated graph does not have any operators since the tf.zero output is fixed regardless of the feature input. TFLite currently requires at least one operator and one tensor in a graph. Is this graph meaningful anyway to your case even though it just provides a constant tensor? I guess you can just use zero values in the target language. This kind of graph is very unusual.", "@abattery This is not a meaningful graph.  I was debugging a model to find why I was getting an error, so I started simplifying the model.  Eventually I got this error and I thought this was the issue. but it seems when removing components of the model, I formed a graph that was invalid.\r\nThank you for helping me with this.  ", "@gcervantes8 Many bugs have been fixed in the latest version of Tensorflow. Can you please execute your code using Latest stable Version of Tensorflow 2.5 and let us know if the issue still persists?   Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45313\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45313\">No</a>\n"]}, {"number": 45312, "title": "TFlite gets the incorrect value dividing by zero or computing tf.log(x)", "body": "**System information**\r\n- OS Platform and Distribution: MacOS Catalina 10.15.6\r\n- TensorFlow installed: from binary\r\n- TensorFlow version: The issue could be reproduced by TF1.x (TF 1.15.2) and TF2.x (TF 2.3.1)\r\n- Python version: 3.6.5\r\n\r\n**Describe the current behavior**\r\nI have the following code that simply creates a TF graph whose output tensor is an input placeholder divided by a float32 constant 0. I would expect the evaluation value of the output tensor to be always \"Inf\" no matter which value is fed into the input placeholder. However, what I got from the following example is, the result from Tensorflow is expected, while the one from TFlite is the max limit of float32.\r\n\r\n```\r\nimport os\r\nimport re\r\nimport tempfile\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nis_tf_2 = bool(re.match(\"2\\.[0-9]+\\.[0-9]+\", tf.version.VERSION))\r\nif is_tf_2:\r\n    print(\"using TF2.x\")\r\n    import tensorflow.compat.v1 as tf\r\n\r\n    tf.compat.v1.disable_eager_execution()\r\n\r\n\r\ndef run_tf_ops():\r\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=[None])\r\n    b = tf.constant(0.0, dtype=tf.float32)\r\n    output_tensor = tf.divide(input_tensor, b)\r\n\r\n    tf_session = tf.Session()\r\n\r\n    with tempfile.TemporaryDirectory(\"\") as tempdir:\r\n        converter = tf.lite.TFLiteConverter.from_session(\r\n            sess=tf_session,\r\n            input_tensors=[input_tensor],\r\n            output_tensors=[output_tensor],\r\n        )\r\n        tflite_model = converter.convert()\r\n        tflite_saved_model_path = os.path.join(tempdir, \"saved_model.tflite\")\r\n        with open(tflite_saved_model_path, \"wb\") as f:\r\n            f.write(tflite_model)\r\n\r\n        # Test TFLite load\r\n        # Load the TFLite model and allocate tensors.\r\n        interpreter = tf.lite.Interpreter(model_path=tflite_saved_model_path)\r\n        interpreter.allocate_tensors()\r\n        # Get input and output tensors.\r\n        input_details = interpreter.get_input_details()\r\n        output_details = interpreter.get_output_details()\r\n\r\n        # Test the model on random input data.\r\n        # get output from tflite model\r\n        interpreter.set_tensor(input_details[0][\"index\"], [np.float32(1.0)])\r\n        interpreter.invoke()\r\n        output_data_from_tflite = interpreter.get_tensor(output_details[0][\"index\"])\r\n\r\n    print(\r\n        f\"Tensorflow result: {tf_session.run(output_tensor, feed_dict={input_tensor: [np.float32(1.0)]})}\"\r\n    )\r\n    print(f\"TFLite result: {output_data_from_tflite}\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    run_tf_ops()\r\n```\r\nThe interesting behavior is if I replace the input placeholder `input_tensor` with a constant float32 tensor like `input_tensor = tf.constant(value=[1.0], dtype=tf.float32)`(also remove the code on feeding data), both Tensorflow and TFlite get the correct result `Inf` in TF 1.15.2, but would have the same issue in TF 2.3.1.\r\n\r\n**NOTE** The same behavior happens for other operations such as \"tf.log(x)\" when we feed x with run time data 0.\r\n\r\n**Standalone code to reproduce the issue**\r\nThe issue could be 100% reproduced by running the above code with the system info.\r\n\r\n", "comments": ["I have tried in colab with TF version 2.3, nightly version(`2.5.0-dev20201201`) and was able to reproduce the issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/bf3062f1ab3fee71e5b3951ef70e5d4a/untitled557.ipynb).Thanks!", "@TJ can you take a look?", "@RuofanKong @WindQAQ Is this inconsistency in behaviors blocking/affecting your model?\r\nDividing to zeros or inf values are not so common I guess.\r\n\r\nWe had a discussion on the PR internally, The PR itself looks OK but output inf results may affect subsequent ops. We need to investigate how all the ops are handing inf values first to ensure not to break the compatibility. That might be a long investigation.", "From my side, we have a check by using `isinf` to detect `inf` and reset them to zero. It's ok for me not to push the change. We have handled it by checking extreme large/small value instead. Thank you!", "FYI, we have also observed that XNNPACK delegate uses infinity as upper/lower bound, which results in different behavior across TFLite", "Thanks for your update. It is still a good point to close the gap between TF and TFLite inf-handling behaviors. We'll put it as a longer-term goal.", "Thanks for the effort. Here are some other info about inf that you might be interested.\r\n\r\n- init value of maxpool and reduce min/max.\r\n- Some calls to other kernels that needs to set activation min/max, such as BroadcastPow4D and LstmCell.", "@thaink Thanks for helping it! On our end, this is indeed blocker issues for our model, when any invalid computation ops are involved, we have no way to tell the output is valid vs invalid values per clipping results, but TF is good. So it would be good to fix it.", "@RuofanKong I think there should be some way to avoid inf value being produced, like checking if dividing to 0 or use isinf for input.", "@thaink The problem is lots of this particular checking on the invalid values (e.g. \"tf.math.is_nan\") are not supported in TFlite..", "the tf.is_nan is supported in TFLite by lowering to equal op.\r\nCould you try to convert with nightly?", "@thaink Okay as I was on the older TF 2.x version, and it's great to see more ops are supported in TFlite now!", "Was able to reproduce your issue in Tf Nightly 2.6, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/347ccd9fd9aa347676bf92caa810baeb/45312.ipynb). Thanks!", "Was able to replicate issue in tf-nightly(2.8.0-dev20211009), please find the gist [here](https://colab.research.google.com/gist/chunduriv/26b98f3f5c4a39fc6761b0e523fc739c/45312.ipynb).Thanks!\r\n\r\n\r\n", "@RuofanKong I tried to replicate the issue on colab using TF v2.8.0  and  didn't face the issue reported here.Could you please have a look at the [gist](https://colab.research.google.com/gist/sushreebarsa/24fa85d95ffee715f2d055b56869c699/gist45321.ipynb) and  confirm the same?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45312\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45312\">No</a>\n"]}, {"number": 45311, "title": "Extract reference for operator FILL to standalone header", "body": "Move the reference implementation to its own header so that micro\r\ncan use it without the unrelated depedencies of reference_ops.h.\r\n\r\nThis PR is part of the work to port operator FILL from lite to micro,\r\nas tracked in #45306.", "comments": []}, {"number": 45310, "title": "Segmentation fault during training simple decision tree classifier", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Catalina 10.15.7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.6\r\n- CUDA/cuDNN version: NA?\r\n- GPU model and memory: using CPU\r\n\r\n**Describe the current behavior**\r\n\r\nI was trying to train a boosted tree, got a seg fault, and ultimately replicated it with this canned example from https://www.tensorflow.org/tutorials/estimator/boosted_trees .\r\n\r\nWhat I get is a segmentation fault detailed below.\r\n\r\n**Describe the expected behavior**\r\n\r\nI would expect this to run and print the result, rather than giving a segmentation fault.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n# Load dataset.\r\ndftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\r\ndfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\r\ny_train = dftrain.pop('survived')\r\ny_eval = dfeval.pop('survived')\r\n\r\nimport tensorflow as tf\r\ntf.random.set_seed(123)\r\n\r\nCATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\r\n                       'embark_town', 'alone']\r\nNUMERIC_COLUMNS = ['age', 'fare']\r\n\r\ndef one_hot_cat_column(feature_name, vocab):\r\n  return tf.feature_column.indicator_column(\r\n      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\r\n                                                 vocab))\r\nfeature_columns = []\r\nfor feature_name in CATEGORICAL_COLUMNS:\r\n  # Need to one-hot encode categorical features.\r\n  vocabulary = dftrain[feature_name].unique()\r\n  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\r\n\r\nfor feature_name in NUMERIC_COLUMNS:\r\n  feature_columns.append(tf.feature_column.numeric_column(feature_name,\r\n                                           dtype=tf.float32))\r\n\r\n# Use entire batch since this is such a small dataset.\r\nNUM_EXAMPLES = len(y_train)\r\n\r\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\r\n  def input_fn():\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\r\n    if shuffle:\r\n      dataset = dataset.shuffle(NUM_EXAMPLES)\r\n    # For training, cycle thru dataset as many times as need (n_epochs=None).\r\n    dataset = dataset.repeat(n_epochs)\r\n    # In memory training doesn't use batching.\r\n    dataset = dataset.batch(NUM_EXAMPLES)\r\n    return dataset\r\n  return input_fn\r\n\r\n# Training and evaluation input functions.\r\ntrain_input_fn = make_input_fn(dftrain, y_train)\r\neval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\r\n\r\n# Since data fits into memory, use entire dataset per layer. It will be faster.\r\n# Above one batch is defined as the entire dataset.\r\nn_batches = 1\r\nest = tf.estimator.BoostedTreesClassifier(feature_columns,\r\n                                          n_batches_per_layer=n_batches)\r\n\r\n# The model will stop training once the specified number of trees is built, not\r\n# based on the number of steps.\r\nest.train(train_input_fn, max_steps=100)\r\n\r\n# Eval.\r\nresult = est.evaluate(eval_input_fn)\r\nprint(pd.Series(result))\r\n```\r\n\r\n**Other info / logs**\r\n```\r\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/97/twhbc2yn6l77_rh7vn94hf7c0000gp/T/tmptaagswd_\r\nWARNING:tensorflow:From /Users/martinl/PyVenvs/tf/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/boosted_trees.py:398: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\r\nWARNING:tensorflow:From /Users/martinl/PyVenvs/tf/lib/python3.8/site-packages/tensorflow/python/training/training_util.py:235: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\n2020-12-01 16:40:43.597377: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-12-01 16:40:43.614731: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8d8e78c990 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-12-01 16:40:43.614747: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\r\nWARNING:tensorflow:Issue encountered when serializing resources.\r\nType is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\r\n'_Resource' object has no attribute 'name'\r\nWARNING:tensorflow:From /Users/martinl/PyVenvs/tf/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/head.py:637: auc (from tensorflow.python.ops.metrics_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe value of AUC returned by this may race with the update so this is deprecated. Please use tf.keras.metrics.AUC instead.\r\nWARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\r\nWARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\r\nzsh: segmentation fault  python temp2.py\r\n```", "comments": ["@mwlon,\r\nI was able to run both the [tutorial](https://colab.research.google.com/gist/amahendrakar/3c85ec6b94a7bbe4ec6f80a3efd3be9d/boosted_trees.ipynb) and the [code snippet](https://colab.research.google.com/gist/amahendrakar/bfe1717eb35ef5fe61cec94122769ec5/45310.ipynb) you have provided, without any issues. Please check the linked gist for reference. \r\n\r\nCould you please try run the code in a new virtual environment and check if you are facing the same issue? Thanks!", "@amahendrakar Yes, that's actually exactly what I did. I created a new venv, installed ONLY tensorflow and pandas (via `pip install`), and then got this issue. Did you run with Catalina, the same package versions, etc?\r\n\r\n```\r\n(tf) % python --version \r\nPython 3.8.6\r\n(tf) % pip list\r\nPackage                Version\r\n---------------------- ---------\r\nabsl-py                0.11.0\r\nastunparse             1.6.3\r\ncachetools             4.1.1\r\ncertifi                2020.11.8\r\nchardet                3.0.4\r\ngast                   0.3.3\r\ngoogle-auth            1.23.0\r\ngoogle-auth-oauthlib   0.4.2\r\ngoogle-pasta           0.2.0\r\ngrpcio                 1.33.2\r\nh5py                   2.10.0\r\nidna                   2.10\r\nKeras-Preprocessing    1.1.2\r\nMarkdown               3.3.3\r\nnumpy                  1.18.5\r\noauthlib               3.1.0\r\nopt-einsum             3.3.0\r\npandas                 1.1.4\r\npip                    20.2.1\r\nprotobuf               3.14.0\r\npyasn1                 0.4.8\r\npyasn1-modules         0.2.8\r\npython-dateutil        2.8.1\r\npytz                   2020.4\r\nrequests               2.25.0\r\nrequests-oauthlib      1.3.0\r\nrsa                    4.6\r\nsetuptools             49.2.1\r\nsix                    1.15.0\r\ntensorboard            2.4.0\r\ntensorboard-plugin-wit 1.7.0\r\ntensorflow             2.3.1\r\ntensorflow-estimator   2.3.0\r\ntermcolor              1.1.0\r\nurllib3                1.26.2\r\nWerkzeug               1.0.1\r\nwheel                  0.36.0\r\nwrapt                  1.12.1\r\n```", "Here's the most relevant segment from the Mac \"Python unexpectedly quit\" popup log. Let me know if you want more detail from this:\r\n```\r\nApplication Specific Information:\r\nabort() called\r\nPython(20849,0x10e2d1dc0) malloc: *** error for object 0x3f8000003f800000: pointer being freed was not allocated\r\n \r\n\r\nThread 0 Crashed:: Dispatch queue: com.apple.main-thread\r\n0   libsystem_kernel.dylib        \t0x00007fff711a633a __pthread_kill + 10\r\n1   libsystem_pthread.dylib       \t0x00007fff71262e60 pthread_kill + 430\r\n2   libsystem_c.dylib             \t0x00007fff7112d808 abort + 120\r\n3   libsystem_malloc.dylib        \t0x00007fff7122350b malloc_vreport + 548\r\n4   libsystem_malloc.dylib        \t0x00007fff7122640f malloc_report + 151\r\n5   libtensorflow_framework.2.dylib\t0x000000013d6281ad tensorflow::shape_inference::InferenceContext::~InferenceContext() + 861\r\n6   libtensorflow_framework.2.dylib\t0x000000013d73b570 std::__1::unique_ptr<tensorflow::ExtendedInferenceContext, std::__1::default_delete<tensorflow::ExtendedInferenceContext> >::~unique_ptr() + 96\r\n7   libtensorflow_framework.2.dylib\t0x000000013d739d56 tensorflow::ShapeRefiner::~ShapeRefiner() + 406\r\n8   _pywrap_tensorflow_internal.so\t0x0000000124b559aa TF_DeleteGraph + 186\r\n9   _pywrap_tf_session.so         \t0x000000013ec728d7 void pybind11::cpp_function::initialize<void (*&)(TF_Graph*), void, TF_Graph*, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::call_guard<pybind11::gil_scoped_release> >(void (*&)(TF_Graph*), void (*)(TF_Graph*), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::call_guard<pybind11::gil_scoped_release> const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 103\r\n10  _pywrap_tf_session.so         \t0x000000013ec5770f pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 2943\r\n11  org.python.python             \t0x000000010c3b3f8e cfunction_call_varargs + 171\r\n12  org.python.python             \t0x000000010c3b3a7d _PyObject_MakeTpCall + 274\r\n13  org.python.python             \t0x000000010c454847 call_function + 804\r\n14  org.python.python             \t0x000000010c451257 _PyEval_EvalFrameDefault + 29861\r\n15  org.python.python             \t0x000000010c3b42c2 function_code_fastcall + 106\r\n16  org.python.python             \t0x000000010c3f9a73 call_unbound_noarg + 76\r\n17  org.python.python             \t0x000000010c3f7500 slot_tp_finalize + 72\r\n18  org.python.python             \t0x000000010c49957f collect + 1918\r\n19  org.python.python             \t0x000000010c498d88 collect_with_callback + 58\r\n20  org.python.python             \t0x000000010c498d1f PyGC_Collect + 93\r\n21  org.python.python             \t0x000000010c47d045 Py_FinalizeEx + 135\r\n22  org.python.python             \t0x000000010c49816f Py_RunMain + 1414\r\n23  org.python.python             \t0x000000010c49867e pymain_main + 306\r\n24  org.python.python             \t0x000000010c4986cc Py_BytesMain + 42\r\n25  libdyld.dylib                 \t0x00007fff7105ecc9 start + 1\r\n```", "@amahendrakar I've replicated the issue on _another_ Catalina (10.15.7) machine, creating a venv with python 3.8.2 this time, installing tensorflow and pandas, and running the exact same script.", "@mwlon \r\nyou can try to run this cmd before run the model\r\n`ulimit -s 102400`", "@zhenql This is what I get:\r\n```\r\n(tf) % ulimit -s 102400\r\nulimit: value exceeds hard limit\r\n\r\n(tf) % ulimit -a       \r\n-t: cpu time (seconds)              unlimited\r\n-f: file size (blocks)              unlimited\r\n-d: data seg size (kbytes)          unlimited\r\n-s: stack size (kbytes)             8192\r\n-c: core file size (blocks)         0\r\n-v: address space (kbytes)          unlimited\r\n-l: locked-in-memory size (kbytes)  unlimited\r\n-u: processes                       2784\r\n-n: file descriptors                256\r\n```", "> Did you run with Catalina, the same package versions, etc?\r\n\r\n@mwlon,\r\nThank you for the updates. I had run the code on Ubuntu 18.04 with TensorFlow 2.3.0 and Pandas 1.1.4. ", "@mwlon \r\n`ulimit -s unlimited` or other size more than 8192. \r\nI'm not sure if the method works though my tf program always require more stack size. \r\n", "@mwlon Did you check whether you have the same issue on the local (without `venv`). May be there is something limiting the memory.\r\n\r\nAlso, can you please check with other python version with `venv`. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45310\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45310\">No</a>\n"]}, {"number": 45309, "title": "[Cherrypick:r2.4] Save Keras metadata in a separate proto and raise deprecation warnings when loading a SavedModel with tf.saved_model.save().", "body": "PiperOrigin-RevId: 339760831\nChange-Id: I8980807eb4f2f0f1a8c4420b7e4c386842f5ebf9", "comments": []}, {"number": 45308, "title": "Docs for install cuda 11.0 and cudnn 8", "body": "Now that tensorflow 2.4.0 is almost out with cuda 11.0 and cudnn 8 support, it would be nice to have the instructions updated for GPU installation.", "comments": ["Hi, I'm willing to work on this issue. However, there are many issues at the moment with RTX cards running on CUDA 11.0. Some suggest that 11.1 works while 11.0 doesn't. Personally, I was facing issues even while using the TF docker container until I was facing issues even while using the TF docker container. I stumbled across an article by [lambda stack](https://lambdalabs.com/blog/install-tensorflow-and-pytorch-on-rtx-30-series/) which allowed me to use TF 2.3 on my new RTX 3080. \r\n\r\nSo I suggest waiting on this issue for a little while longer until the issues have been sorted out. In the meantime, I'll start up a pull request since I think we should refactor the [GPU support](https://www.tensorflow.org/install/gpu) page.", "> Some suggest that 11.1 works while 11.0 doesn't.\r\n\r\n@nluehr Is this true?  This could be a reason to move to 11.1 or 11.2 sooner rather than later.\r\n\r\nCC @pkanwar23 ", "FYI. GPU instructions have been updated. There might be minor omissions, which I'll fix soon.", "Is this still an issue.", "this has been fixed, please refer [to link](https://www.tensorflow.org/install/gpu), moving this to closed status."]}]