[{"number": 8694, "title": "Refactor common_env.sh for Windows Bazel build", "body": "1. Removed a duplicated script\r\n2. Add --host_cpu=-w to suppress more warning messages\r\n@gunan ", "comments": []}, {"number": 8693, "title": "CUDA 8.0 with Xcode 8.0", "body": "I am not able to install CUDA 8.0 with Xcode 8.0 version. Everywhere i found the solution to downgrade the version of Xcode. Is there any other solution without downgrading the version of Xcode.", "comments": ["Xcode 8.2.1 (8C1002) works with CUDA ToolKit V8.0.61, at least for me. ", "As far as I can tell, you are having problems installing CUDA.\r\nYou will need to reach out to NVIDIA, use stackoverflow, or browse nvidia forums for this question.\r\nPlease only use our github repositories issues for bugs in TensorFlow itself."]}, {"number": 8692, "title": "CUDA 8.0 installation with Xcode 8", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n\r\nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\r\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\r\nout of scope for GitHub Issues and point people to StackOverflow.\r\n\r\nFor bugs or installation issues, please provide the following information.\r\nThe more information you provide, the more easily we will be able to offer\r\nhelp and advice.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\n### Environment info\r\nOperating System:\r\n\r\nInstalled version of CUDA and cuDNN: \r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\nIf installed from source, provide \r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\n2. The output of `bazel version`\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n\r\n### What other attempted solutions have you tried?\r\n\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n", "comments": ["Closing as duplicate of #8693."]}, {"number": 8691, "title": "Problems to visualize Tensorboard ", "body": "Hi guys\r\n\r\nI have problems to visualize tensorboard on my browser. ( Google chrome )\r\n\r\nAt the moment I did 3 python files.  ( 2 models and input_data )\r\n\r\nEverything works fine, and I don't have any errors with the terminal ( Docker --> IPython ---> Terminal )\r\n\r\nI already install pandas, urllib3, sklearn, etc etc\r\n\r\nThe terminal say me, you can sulf on Tensorboad at 0.0.0.0:6006 ( example )\r\n\r\nI try to launch the ip on my browser but I don't see anything.\r\n\r\n---- Step by step ---\r\n\r\nStep 1\r\n[My file](http://imgur.com/h39C8nd)\r\n\r\nStep 2\r\n\r\n[Terminal](http://imgur.com/LeT1muD)\r\n\r\nStep 3\r\n\r\n[Browser](http://imgur.com/6ZQSm2T) \r\n\r\nI tried with different browers, disable the firewall but nothing :(\r\n\r\nP.s I'm noob and don't judge me for this question.\r\n\r\nBest regards,\r\n\r\n\r\n", "comments": ["The problem is with the IP address. \r\n\r\nThe thing is, 0.0.0.0 is used to bind to the localcomputer. \r\n\r\nWith docker running, 0.0.0.0 is no more linked to the container, until you forward the port. \r\n\r\nSo you have 2 solutions here. \r\n\r\nFirstly, change the ip address(0.0.0.0) to the ip address of the docker container, which is usually 172.17.0.x, where 0.x usually varies. Check for the ip address using the command  docker inspect <container name> or by using a gui like kitematic. \r\n\r\nThe second solution is to forward a port to the local pc. \r\n\r\nYou can do it by using the -p command while starting the  container.  The syntax is something like docker run -it -p localport:containerport tensorflow/tensorflow. \r\n\r\nThen access in the browser using the url \r\n0.0.0.0:localport\r\n\r\nHope it answers your question.\r\n", "That should do it. Thanks @rgautam98 !\r\n\r\nYou can also use `--net=host` to use the host networking on docker."]}, {"number": 8690, "title": "Fix doc for CTC_Greedy", "body": " The output is the minus the sum of the logits and the logprobs", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "@Garvys feel free to address the comments.", "Jenkins, test this please.", "Ignoring flaky `queue_runner_test`."]}, {"number": 8689, "title": "Clarify and correct Android Sections in WORKSPACE", "body": "Build Tools version updated to latest version shipping with Android Studio.\r\nAdded some comments to clarify both this and NDK Level details that have been causing confusion.\r\nBoth of these have caused problems within Android studio so these edits should avoid the build tools error.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Added commit for build.gradle to match the build tools version change.", "Andrew, can you take a look at this? Thanks!", "@andrewharp Those changes have been made however I am wondering whether it will break people's production environments? As a new starter with a fresh copy of android studio, I have one version of build_tools but others may still be on a previous version and this change may cause issues? This works both ways though as otherwise it breaks fresh builds on fresh android studio version. Although, curiously, you see a different build_tools_version to me and I've tried to update it! Thoughts?", "@tensorflow-jenkins test this please", "Do I need to do anything to address the failed builds? I'm not sure they are failing and, considering how basic the changes were i'm not sure if there's anything I could do!\r\nThanks", "@jubjamie thanks for asking. The failure looks like a flake. Running @tensorflow-jenkins test this please.", "@yifeif Hmmm ok i'll let Jenkins do his thing! Thanks", "@tensorflow-jenkins test this please."]}, {"number": 8688, "title": " could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available", "body": "When I used the tesla K40c train a VGG16, there was a problem leading the kernel died, restarting. Can some one help me resolve it?\r\nThe retails are \r\n\r\n- I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Tesla K40c\r\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.745\r\npciBusID 0000:01:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.10GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3461a00\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: GeForce GTX 750 Ti\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.1105\r\npciBusID 0000:03:00.0\r\nTotal memory: 978.50MiB\r\nFree memory: 678.75MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: GeForce GTX 750 Ti, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:962] Ignoring gpu device (device: 1, name: GeForce GTX 750 Ti, pci bus id: 0000:03:00.0) with Cuda multiprocessor count: 5. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.\r\nE tensorflow/stream_executor/cuda/cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1147] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_ADDRESS\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\r\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\r\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\r\nF tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:198] Unexpected Event status: 1\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1177] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\r\n\r\n\r\n", "comments": ["- #!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thu Mar 23 16:19:39 2017\r\n\r\n@author: wgb\r\n\"\"\"\r\nimport os\r\nimport sys\r\nimport numpy as np \r\nimport types\r\nfrom scipy.misc import imread, imresize\r\nimport matplotlib.pyplot as plt\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\n#import build_txt\r\n#import build_txt\r\n#\r\n#k = build_txt\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nfile_name = os.getcwd()\r\n\r\ntraining_file  = file_name + '/training_webcam.txt'\r\ntest_file = file_name + '/test_dslr.txt'    \r\n\r\n\r\n\r\nimage_size = 224\r\ntraining_batch_size = 16\r\n\r\nsess = tf.InteractiveSession()\r\n\r\n\"\"\"read training and test data    ################################################################################################\"\"\"\r\n\r\ndef read_txt(file_name):\r\n    \r\n    f  = open(file_name, 'r')\r\n    file_lines = f.readlines()\r\n    \r\n    image_path_total = []\r\n    labels_total = []\r\n    image_id_total = []    \r\n    for i in file_lines:\r\n        divid_data = i.split(' ')\r\n        image_path = divid_data[0]  \r\n        label = divid_data[1]\r\n        one_hot = [0] * 10\r\n        one_hot[int(label)-1] = 1\r\n        labels = one_hot\r\n        image_id = divid_data[2]\r\n        \r\n        image_path = ops.convert_to_tensor(image_path, tf.string)\r\n        labels = ops.convert_to_tensor(labels, tf.float32)\r\n        image_id = ops.convert_to_tensor(int(image_id), tf.float32)\r\n            \r\n        image_path_total.append(image_path)\r\n        labels_total.append(labels)\r\n        image_id_total.append(image_id)\r\n        \r\n    return image_path_total,labels_total, image_id_total\r\n\r\n\r\ndef read_image(single_image_queue,size):\r\n    \"\"\"the image_path is the path of single image\"\"\"\r\n    file_contents = tf.read_file(single_image_queue[0])\r\n    image_data = tf.image.decode_jpeg(file_contents, channels=3)\r\n    image_data = tf.cast(image_data, tf.float32)\r\n    image_data = tf.image.resize_images(image_data, [size, size])\r\n    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 3])\r\n    image_data = image_data - mean\r\n    \r\n    image_path = single_image_queue[0]\r\n    label = single_image_queue[1]\r\n    image_id_wgb = single_image_queue[2]\r\n    \r\n    return image_data, label, image_id_wgb,image_path\r\n\r\ntraining_image_total= read_txt(file_name = training_file)\r\ntraining_input_queue = tf.train.slice_input_producer(training_image_total, shuffle=True)\r\ntraining_image_data = read_image(training_input_queue,image_size)\r\ntraining_image_batch = tf.train.shuffle_batch(training_image_data,training_batch_size,num_threads = 2, min_after_dequeue =101,capacity = 1000)\r\n\r\ntest_image_total= read_txt(file_name = test_file)\r\ntest_input_queue = tf.train.slice_input_producer(test_image_total, shuffle=True)\r\ntest_image_data = read_image(test_input_queue,image_size)\r\ntest_image_batch = tf.train.shuffle_batch(test_image_data,batch_size=64,num_threads = 2, min_after_dequeue =101,capacity = 1000)\r\n\r\n\"\"\"define networks    ################################################################################################\"\"\"\r\n\r\n\r\n\r\nx = tf.placeholder(tf.float32, [None, 224,224,3])\r\nx = tf.reshape(x,[-1,224,224,3])\r\n\r\nparameters = []\r\n    \r\nwith tf.device('/cpu:0'):    \r\n    # conv1_1\r\n    with tf.name_scope('conv1_1') as scope:\r\n        kernel1 = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights', trainable = False)\r\n        conv = tf.nn.conv2d(x, kernel1, [1, 1, 1, 1], padding='SAME')\r\n        biases1 = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases1)\r\n        conv1_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel1, biases1]\r\n    \r\n    # conv1_2\r\n    with tf.name_scope('conv1_2') as scope:\r\n        kernel2 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv1_1, kernel2, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv1_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel2, biases]\r\n    \r\n    # pool1\r\n    pool1 = tf.nn.max_pool(conv1_2,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool1')\r\n    \r\n    # conv2_1\r\n    with tf.name_scope('conv2_1') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv2_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv2_2\r\n    with tf.name_scope('conv2_2') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv2_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # pool2\r\n    pool2 = tf.nn.max_pool(conv2_2,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool2')\r\n    \r\n    # conv3_1\r\n    with tf.name_scope('conv3_1') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv3_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv3_2\r\n    with tf.name_scope('conv3_2') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv3_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv3_3\r\n    with tf.name_scope('conv3_3') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv3_3 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # pool3\r\n    pool3 = tf.nn.max_pool(conv3_3,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool3')\r\n    \r\n    # conv4_1\r\n    with tf.name_scope('conv4_1') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv4_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv4_2\r\n    with tf.name_scope('conv4_2') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv4_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv4_3\r\n    with tf.name_scope('conv4_3') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv4_3 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # pool4\r\n    pool4 = tf.nn.max_pool(conv4_3,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool4')\r\n    \r\n    # conv5_1\r\n    with tf.name_scope('conv5_1') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv5_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv5_2\r\n    with tf.name_scope('conv5_2') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv5_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv5_3\r\n    with tf.name_scope('conv5_3') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv5_3 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # pool5\r\n    pool5 = tf.nn.max_pool(conv5_3,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool4')\r\n    \r\n    #fc1\r\n    with tf.name_scope('fc1') as scope:\r\n        shape = int(np.prod(pool5.get_shape()[1:]))\r\n        fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\r\n                                                     dtype=tf.float32,\r\n                                                     stddev=1e-1), name='weights')\r\n        fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        pool5_flat = tf.reshape(pool5, [-1, shape])\r\n        fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\r\n        fc1 = tf.nn.relu(fc1l)\r\n        parameters += [fc1w, fc1b]\r\n    \r\n    # fc2\r\n    with tf.name_scope('fc2') as scope:\r\n        fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\r\n                                                     dtype=tf.float32,\r\n                                                     stddev=1e-1), name='weights')\r\n        fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        fc2l = tf.nn.bias_add(tf.matmul(fc1, fc2w), fc2b)\r\n        fc2 = tf.nn.relu(fc2l)\r\n        parameters += [fc2w, fc2b]\r\n    \r\n    # fc3\r\n    with tf.name_scope('fc3') as scope:\r\n        fc3w = tf.Variable(tf.truncated_normal([4096, 10],  ##wgb\r\n                                                     dtype=tf.float32,\r\n                                                     stddev=1e-1), name='weights')\r\n        fc3b = tf.Variable(tf.constant(1.0, shape=[10], dtype=tf.float32),  ###wgb\r\n                             trainable=True, name='biases')\r\n        fc3l = tf.nn.bias_add(tf.matmul(fc2, fc3w), fc3b)\r\n        parameters += [fc3w, fc3b]\r\n    \r\n    #probs = tf.nn.softmax(self.fc3l)\r\n    \r\n    \r\n    probs = tf.nn.softmax(fc3l)\r\n    \r\n    y_ = tf.placeholder(tf.float32, [None,10])\r\n                              \r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(probs),reduction_indices = [1]))                             \r\n    correct_prediction = tf.equal(tf.argmax(probs,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n    train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)   \r\n                \r\n                \r\nsess = tf.Session()\r\n\r\ntf.global_variables_initializer().run()\r\n\r\nweight_file = 'vgg16_weights.npz'\r\nweights = np.load(weight_file)\r\nkeys = sorted(weights.keys())\r\nfor i, k in enumerate(keys):\r\n    if i  < 30:\r\n        print (i, k, np.shape(weights[k]))\r\n        sess.run(parameters[i].assign(weights[k]))\r\n\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\ntraining_batch = sess.run(training_image_batch)\r\n\r\nkernel_wgb11 = []\r\nkernel_wgb22 = []\r\nfor i in range(10000):\r\n    training_batch = sess.run(training_image_batch)\r\n    print(i)\r\n    kernel_wgb1 = sess.run(biases1,feed_dict = {x: training_batch[0], y_: training_batch[1]})\r\n    \r\n    if i ==0:\r\n        train_accuracy = accuracy.eval(feed_dict = {x: training_batch[0], y_: training_batch[1]})\r\n        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\r\n\r\n        kernel_wgb11 = kernel_wgb1\r\n    \r\n\r\n    print('i  : \\n', kernel_wgb11-kernel_wgb1)\r\n        \r\n        \r\n        \r\n        test_batch = sess.run(test_image_batch)\r\n        t_a = accuracy.eval(feed_dict = {x: test_batch[0], y_: test_batch[1]})\r\n        print(\"test accuracy %g\"%t_a)\r\n#        print(\"test accuracy333 %g\"%accuracy.eval(feed_dict = {x: test_batch[0], y_: test_batch[1]}))\r\n        test_accuracy.append(t_a)\r\n        num_i.append(i)\r\n        plt.plot(num_i,test_accuracy)\r\n    train_step.run(feed_dict = {x: training_batch[0], y_: training_batch[1]})\r\n    \r\n                                      \r\n                                      \r\npreds = (np.argsort(prob)[::-1])[0:5]\r\nfor p in preds:\r\n    print (class_names[p], prob[p])\r\n\r\n\r\ncoord.request_stop()\r\ncoord.join(threads)\r\n\r\nplt.plot(num_i,test_accuracy)"]}, {"number": 8687, "title": "[HDFS]OutOfRangeError cause java runtime error", "body": "### Environment info\r\nTensorFlow: v1.0.0 and v1.0.1\r\nJDK: 1.8.0_111\r\nHadoop: 2.6.0\r\nGCC: 4.8.2\r\n\r\n### OutOfRangeError\r\nI think the `tf.errors.OutOfRangeError` is a bug, at least, if you want to read data(tfrecords, csv, etc) from hdfs with readers and queues. \r\n\r\n### How to reproduce \r\nI followed the guide of [reading data](https://www.tensorflow.org/programmers_guide/reading_data).\r\nI used the recommended mnist example [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data).  \r\n\r\nFirstly, convert MNIST dataset by this script: [convert_to_records.py](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py)\r\n\r\nSecondly, put the tfrecords of MNIST dataset into your hdfs. For example, assume the url is `hdfs://host:port/tfrecords/mnist-data/`.\r\n\r\nThen, train the MNIST network by this script: [fully_connected_reader.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py). And feed the `train_dir` argument with `hdfs://host:port/tfrecords/mnist-data/`.\r\n\r\nFinally, you can get the train log and error log as below:\r\n### Train log\r\n```shell\r\nStep 0: loss = 2.29 (1.795 sec)\r\nStep 100: loss = 1.99 (0.045 sec)\r\nStep 200: loss = 1.62 (0.045 sec)\r\nStep 300: loss = 1.39 (0.043 sec)\r\nStep 400: loss = 1.01 (0.048 sec)\r\nStep 500: loss = 0.77 (0.043 sec)\r\nStep 600: loss = 0.65 (0.045 sec)\r\nStep 700: loss = 0.60 (0.043 sec)\r\nStep 800: loss = 0.65 (0.043 sec)\r\nStep 900: loss = 0.46 (0.043 sec)\r\n```\r\n\r\n### Error log\r\n```shell\r\n#\r\n# A fatal error has been detected by the Java Runtime Environment:\r\n#\r\n#  SIGSEGV (0xb) at pc=0x00007f16a2a39660, pid=43645, tid=0x00007f16a2eff740\r\n#\r\n# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)\r\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)\r\n# Problematic frame:\r\n# C  [libpython2.7.so.1.0+0x134660]  visit_decref+0x0\r\n#\r\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\r\n#\r\n# An error report file with more information is saved as:\r\n# /home/mind/projects/tf_hdfs/hs_err_pid43645.log\r\n#\r\n# If you would like to submit a bug report, please visit:\r\n#   http://bugreport.java.com/bugreport/crash.jsp\r\n# The crash happened outside the Java Virtual Machine in native code.\r\n# See problematic frame for where to report the bug.\r\n#\r\nAborted (core dumped)\r\n```\r\n\r\n### Ask for a better way\r\nI know it's hard to avoid the `OutOfRangeError` because the multithreads of readers. But can we have a better way to read? Especially when you want to read data from hdfs, it's inconvenient and difficult to catch the exception. ", "comments": ["@jhseu, could you take a look?", "Can you run gdb and get a stack trace? I'm not sure how the error message relates to `OutOfRangeError`, but it definitely shouldn't be segfaulting. Perhaps some of the error log is missing?\r\n\r\nNote that `OutOfRangeError` is the standard error code for indicating EOF, and a lot of the internal TF calls catch that error underneath.", "Actually, I changed the `while loop` into `for steps in xrange(max_train_steps)`. And I can get the error info by one of the cases.\r\n**Case 1**: Dequeue all examples until the example queue becomes empty. In this case, it cause the `tf.errors.OutOfRangeError` and print the training log as below:\r\n```shell\r\nStep 0: loss = 2.30 (1.800 sec)\r\nStep 100: loss = 2.10 (0.043 sec)\r\nStep 200: loss = 1.87 (0.045 sec)\r\nStep 300: loss = 1.61 (0.043 sec)\r\nStep 400: loss = 1.27 (0.043 sec)\r\nStep 500: loss = 0.97 (0.043 sec)\r\nDone training for 1 epochs, 550 steps.\r\n```\r\n**Case 2**: Finish the `for loop`, that is, reach the max_train_steps even though the example queue has stuff.\r\nIn this case, it cause some other error and caught by Java. Set `max_train_steps=501` and `num_epochs=10`, and get the training log as below:\r\n```shell\r\nStep 0: loss = 2.32 (1.799 sec)\r\nStep 100: loss = 2.13 (0.043 sec)\r\nStep 200: loss = 1.80 (0.043 sec)\r\nStep 300: loss = 1.49 (0.044 sec)\r\nStep 400: loss = 1.28 (0.045 sec)\r\nStep 500: loss = 0.82 (0.045 sec)\r\n```\r\nBoth case 1 and case 2 print the java error log that I attached above. I guess it relates to `OutOfRangeError`, but I'm not sure. \r\n\r\n@jhseu Do I need to build TensorFlow from source to run gdb and get a stack trace? But, I think the exception would still caught by Java.", "I haven't been able to reproduce the issue. If you modified the code, mind posting it to gist.github.com?\r\n\r\nIt doesn't crash on OutOfRange for me. I'm using Cloudera quickstart for HDFS:\r\n`docker run --hostname=quickstart.cloudera --privileged=true -t -i -p 8020:8020 -p 8022:8022 -p 50070:50070 cloudera/quickstart /usr/bin/docker-quickstart` and running against `hdfs://localhost:8020/`", "@jhseu Please check my code [here](https://gist.github.com/DjangoPeng/07d8bfb8f0c412318e4fb6473ec2f56b)", "@jhseu any updates?", "I haven't been able to reproduce the error with the provided script. It just raises OutOfRangeError in Python, but otherwise works fine. If you feel that this should be reopened, we'll need more info, like a proposal for where the issue is coming from, or a script that reproduces the issue in TF 1.2."]}, {"number": 8686, "title": "R1.1", "body": "", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->"]}, {"number": 8685, "title": "Unable to restore inception v3 retrained model", "body": "I'm retraining the inception v3 model on my image dataset. I use tf.Saver() to store the model at the time of best validation accuracy. But when I'm using the stored model to do the validation again, I don't get the same accuracy. \r\n\r\nPlease let me know if I'm missing anything specifically with inception v3 model. I want store the entire inception v3 graph along with newly added layers, so that I can reuse it normally.\r\n\r\ncode snippet : \r\nsaver.save(sess=sess, save_path=save_path + \"best_valid\", global_step=i)\r\n\r\nThanks,\r\nRohit\r\n", "comments": ["with gfile.FastGFile(FLAGS.output_graph, 'wb') as f:\r\n    f.write(output_graph_def.SerializeToString())\r\n\r\nI'm getting  following exception while saving the protobuf file :\r\n\r\nraise_exception_on_not_ok_status\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: ./output_graph.pb", "Perhaps try using the full path rather than a relative path? Try that and let us know if that works. Thanks!", "I apologize but I am having a hard time understanding what the problem is, where the problem is, and what version it affects. Please resubmit and pay attention to the issue template (https://github.com/tensorflow/tensorflow/issues/new) . Please provide all the information it asks. Thank you."]}, {"number": 8683, "title": "Update linalg_ops.py", "body": "Just add a space to doc of svd...", "comments": ["Can one of the admins verify this patch?"]}, {"number": 8682, "title": "Retrained inception v3 model from retrain.py takes 8-10 seconds to classify an image vs 1-2 seconds taken by sample TFClassify application", "body": "Hi,\r\n\r\nI am using tensorflow image classification in my project. \r\n\r\nInitially i built and installed Camera Demo from GitHub link : https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android\r\n\r\nCase 1: On running this example application (**TFClassify**) it is taking 1 or 2 seconds to do classification and shows result.\r\n\r\n_**(On the other hand)**_\r\n\r\nCase 2: If I use my retrained inception V3 model (which I have retrained using retrain.py) in same example application by placing the retrained model and labels fie in assets package it is taking 8- 10 seconds to classify per image.\r\n\r\nExample application is using inception 5h model i guess and retrain.py is using inception V3 - I am not sure that the difference in classification time is because of this difference in models used, if so I could not find a way to use inception 5h model in retrain.py, it is giving an error :\r\n\r\n_ValueError: Requested return_element 'pool_3/_reshape:0' not found in graph_def._\r\n \r\n\r\nIs there a way to reduce the image classification time for the retrained inception V3 model? \r\n\r\nThanks   \r\n ", "comments": ["Hi,\r\nInception v3 is bigger than 5h and I think the difference in speed that you see is normal.\r\nI run the demo(5h model) at 170ms-450ms on each prediction on a s7 edge exynos and retrained v3 at 800ms-1.5 sec\r\nUsing the graph tool here [eight-bit-calculations](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#eight-bit-calculations) I managed to drop at 450ms-800ms on a v3 retrained model but had weird results that i'm waiting to be resolved soon.(check [here](https://github.com/tensorflow/tensorflow/issues/7150) for issues if you try the transform tool).\r\nDepending on your phone you should see different speeds.\r\n\r\nEdit: just saw [this](https://github.com/tensorflow/tensorflow/issues/7523) so you know what i'm talking about.", "@left13 analysis matches my intuition, so closing for now. I.e. it is a bigger model. TFClassify has also been quantized and reduced. This information is clear on the documentation https://www.tensorflow.org/mobile/\r\n Thanks!", "Hi,\r\n@aselle can you educate me more about inception5h. I would like to know why there is a time difference between inceptionV3 and inception5h models(Like why is it not inceptionV5, why is it not listed in the models list [here](https://github.com/tensorflow/models/tree/master/slim), is inceptionV5 retrained on some other models like inceptionV3?). \r\n\r\nAs @left13 mentioned, even I am able to run the demo(5h model) at 170ms-450ms on each prediction on OnePlus3T device and my [retrained](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py) &[optimized](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py) v3 at 1500ms-2100ms\r\n\r\n@left13 I tried [eight-bit calculations](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#eight-bit-calculations). The inference time has slightly improved to 1200ms~1500ms, but the classification part dosent work, for any image you pass, it shows a constant output(Tested on OnePlus3T device).\r\n\r\nI understand from my experiments that after quantization or [eight-bit calculations](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md#eight-bit-calculations), the size of APK/model has decreased from 82.1 MB to 19.6MB but the result is same:\"Image classification dosent work\"\r\n\r\nIs it possible to retrain inception5h model on my own data?\r\n\r\n\r\n", "I've tried the following approaches:\r\n\r\n```\r\nbazel-bin/tensorflow/tools/quantization/quantize_graph \\\r\n  --input=/path/to/input/graph.pb \\\r\n  --output_node_names=\"final_result\" \\\r\n  --output=/path/to/output/graph.pb \\\r\n  --mode=eightbit\r\n\r\n```\r\nFor this approach, it always return a single wrong result with 100% certainty.\r\n\r\n-------------------\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n  --in_graph=/path/to/input/graph.pb \\\r\n  --out_graph=/path/to/output/graph.pb \\\r\n  --inputs=Mul \\\r\n  --outputs=final_result \\\r\n  --transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes'\r\n```\r\nand\r\n\r\n```\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=/path/to/input/graph.pb \\\r\n--out_graph=/path/to/output/graph.pb \\\r\n--inputs='Mul' \\\r\n--outputs='final_result' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\nFor both of these, the results are similar. There is some variance to the results returned (sometimes different values with around 20% or 50% variance), but a specific result is most of the times returned with 100% certainty as well, even though it's wrong.\r\n\r\n@SubhashKsr, did you manage to fix this?\r\n\r\nAny help is much appreciated."]}, {"number": 8681, "title": "Branch 151087060", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 8680, "title": "Any roadmap for supporting windows 10 GPU for Python 3.6.1?", "body": "Any plan or expected date?\r\n\r\nfor windows 10 x64 + python 3.6.1 x64 + CUDA", "comments": ["Nothing exact yet. We will keep evaluating it with each release."]}, {"number": 8679, "title": "Added checkpoint_path for Estimator.predict()", "body": "To be consistent with `Estimator.evaluate()`. ", "comments": ["Hooray!"]}, {"number": 8678, "title": "wget issue with links in workspace.bzl", "body": "Hello,\r\n\r\nI'm trying to build from the most recent master branch as of today and am getting this error during the ./configure step:\r\n\r\n```ERROR: Evaluation of query \"deps(((//tensorflow/... - //tensorflow/contrib/nccl/...) - //tensorflow/examples/android/...))\" failed: errors were encountered while computing transitive closure.```\r\n\r\n```every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:swig',```\r\n```every rule of type _py_wrap_cc implicitly depends upon the target '@swig//:templates'```\r\n\r\nThe links from a preliminary glance take much longer than expected albeit being relatively small.\r\n\r\nI've already looked at these but the links are outdated.\r\nhttps://github.com/tensorflow/tensorflow/issues/5432\r\nhttps://github.com/tensorflow/tensorflow/issues/8619\r\nhttps://github.com/tensorflow/tensorflow/issues/4312\r\n...\r\nAnd a lot more that don't seem effective\r\n\r\n### Environment info\r\nOperating System:\r\nMacOS Sierra 10.12.3\r\nCUDA Version 8.0.61\r\nCUDNN version 5.1.10\r\nBazel Version: Build label: 0.4.5\r\n\r\n\r\nls -l /usr/local/cuda/lib/libcud* **output**\r\n\r\n-rwxr-xr-x    1 root     wheel  13504 Jan 24 14:58 /usr/local/cuda/lib/libcuda.dylib\r\nlrwxr-xr-x@  1 root     wheel  45 Jan 11 20:33 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudadevrt.a\r\nlrwxr-xr-x@  1 root     wheel  50 Jan 11 20:33 /usr/local/cuda/lib/libcudart.8.0.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib\r\nlrwxr-xr-x@  1 root     wheel  46 Jan 11 20:33 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart.dylib\r\nlrwxr-xr-x@  1 root     wheel  49  Jan 11 20:33 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-8.0/lib/libcudart_static.a\r\n-rwxr-xr-x@ 1 Kevin   staff    82210088 Mar 23 16:18 /usr/local/cuda/lib/libcudnn.5.dylib\r\n-rwxr-xr-x@ 1 root     wheel  82210088 Mar 23 16:18 /usr/local/cuda/lib/libcudnn.dylib\r\n-rw-r--r--@  1 Kevin  staff    66197088 Mar 23 16:18 /usr/local/cuda/lib/libcudnn_static.a\r\n\r\nTensorflow git version c7b80d5  \r\n\r\n##Attempted Solution\r\nLots of changing the links to older versions. Also tried building Bazel by source for 0.4.3 and 0.4.5.\r\nBeen stuck on this for a couple of hourst. Following any previous solution seems to not work as other issues arise (Such as using Bazel 0.4.3). \r\n\r\nMy very strong suspicion is that the links are down. \r\n```\r\nwget http://bazel-mirror.storage.googleapis.com/ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz\r\n```\r\nwill take an obscenely long amount of time and as a result timeout. My internet is fine (university) and all else is fine. My GPU is a NVIDIA 1080 GTX connected to my mac via thunderbolt cable. ", "comments": ["Resolved for some unknown reason..."]}, {"number": 8677, "title": "Added Windows JNI download to README", "body": "The Java README is missing the download link for the Windows JNI download", "comments": ["Can one of the admins verify this patch?", "@Rabrg : Thanks very much for the PR. \r\n\r\nHowever, if you don't mind, I'd like to abandon this PR as I have another change that is updating the same file with both Windows and Maven instructions. There are a few issues with Maven and RC0 on Windows that I've discovered that I'd like to fix.\r\n\r\nFWIW though, that URL is correct for now so feel free to use it."]}, {"number": 8676, "title": "Tensorflow causes whole python to crash with error Python frozen importlib._bootstrap", "body": "If I try to run a computer vision script python crashes with no error message at all.\r\nIf I run with `python -m trace` python console goes in loop with a message repeated continuously that is unreadable because lines are printed endless too fastly. I can see some line related to `sre_compile.py` `text_format.py` `inspect.py` and `charmap codec can't encode \\u2713` in `cp1252.py` and `from tensorflow.contrib.keras`\r\n\r\nIf I try to redirect the output to a file, the file content is\r\n \r\n```\r\n<frozen importlib._bootstrap_external>(376): <frozen importlib._bootstrap_external>(1258): <frozen importlib._bootstrap_external>(1255): <frozen importlib._bootstrap_external>(1256):  --- modulename: _bootstrap_external, funcname: _path_join\r\n<frozen importlib._bootstrap_external>(59): <frozen importlib._bootstrap_external>(60):  --- modulename: _bootstrap_external, funcname: <listcomp>\r\n...again and again\r\n```\r\n\r\nThis isn't the first time that a tensorflow issue cause the whole python to go crazy.\r\nI have tried both today nightly build, past days nightly build, stable version, no way to run and since causes python crash It's very hard to debug. \r\n\r\nTensorflow stable and nightly 1.0.1 on windows 10 x64.\r\n\r\n\r\nTo reproduce this issue run `evaluate.py` scipt of TensorBox project", "comments": ["Could you try running the same script in an Ubuntu VM and see if you experience the same problem? I wonder if this is a Windows-specific issue, given that it looks like TensorBox works for most users.", "@skye happens only with Windows version of Tensorflow that is bugged as the hell.", "@mrry, please take a look.\r\n", "I'm a little unclear on what the problem is here. If you run with `python -m trace` **and** redirect the output to a file, does the program crash or hang (for at least, say, 10 minutes) without appending to the file? It is expected that `python -m trace` will produce a lot of output, so it is not surprising that it fills your console.\r\n\r\n(Judging by your previous issue, #8627, it seems like there might be a problem with your Python installation, as at least one problem was caused by interactions between third-party libraries.)", "@mrry maybe the console crashes because It's filled with too many lines but something doesn't work correctly and broke whole Python environment judging to the content of -m trace where seems cannot log correctly what is going wrong.  My python installation seems working correctly, I don't know why if something goes wrong with certain Tensorflow libs python keep crashing preventing me to see exactly what is the error log (maybe could be related to the fact the error is with some compiled c lib??). ", "We have seen instances of bad interactions between compiled Python extensions and TensorFlow, so that's certainly possible. However we're still going to need more information to help you.\r\n\r\nCan you post the entire contents of the output of `python -m trace evaluate.py` (redirected to a file, to avoid crashing your console)?\r\n\r\nAlternatively, can you confirm which library in that script causes Python to crash? A simple way to do this would be to add `print()` statements after importing each library, at the top of `evaluate.py`.", "@mrry would be useless unless you want to read some billion of \r\n ```\r\n<frozen importlib._bootstrap_external>(376): <frozen importlib._bootstrap_external>(1258): <frozen importlib._bootstrap_external>(1255): <frozen importlib._bootstrap_external>(1256):  --- modulename: _bootstrap_external, funcname: _path_join\r\n<frozen importlib._bootstrap_external>(59): <frozen importlib._bootstrap_external>(60)\r\n```\r\nthere is nothing else in the output. This time is unclear what is the specific lib involved, if I find something new that can be useful I'll update the report", "The entire log would still be useful... I imagine the lines immediately preceding those repeated `importlib` tracing would provide information about the last library successfully imported. ", "@mrry I'm sorry but there is only this error message nothing else. However now I'm trying a different configuration of Tensorflow that doesn't use python distribution of conda environment and cannot reproduce the issue anymore.", "Thanks for confirming that switching to conda fixes things. I suspect this is an environment problem caused by one of the libraries on which TensorFlow depends, but I don't think we'll be able to reproduce it. Feel free to reopen this if you can get any additional information about the crash."]}, {"number": 8675, "title": "Branch 151086011", "body": "", "comments": []}, {"number": 8674, "title": "wrong libjpeg version for 1.1.0-rc0 gcr docker image ", "body": "While running the Mandelbrot Set on the latest docker image on gcr.io, there is the following error:\r\n\r\nIOError: encoder error -2 when writing image file\r\n\r\nIn the console: there is real error message:\r\nWrong JPEG library version: library is 90, caller expects 80\r\n\r\nIt shall be the latest image as of Mar 23.\r\n16715e3f77e1 \r\ntagged: 1.1.0-rc0  latest\r\n\r\nFull command is:\r\n$ docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow\r\n\r\nThe same code works fine on the Docker Hub image.\r\n$ docker run -it -p 8888:8888 tensorflow/tensorflow", "comments": ["Hmm, I tried the MandelBrot Set example in tensorflow/tensorflow:1.1.0-rc0  and tensorflow/tensorflow:1.1.0-rc0-py3. It seems to work fine in both docker images.", "@deuteron Could you provide a full bash script to reproduce the problem.", "Thank you guys. The issue of gcr image is gone. I am not aware of anything I did differently this time.", "Thanks for checking again. I will then close this issue."]}, {"number": 8673, "title": "BNNS Based Implementations for MaxPool and Conv2d", "body": " Fixes https://github.com/tensorflow/tensorflow/issues/3001\r\n\r\nI wanted to get this up for feedback, comments, etc. I know the files are not in the right places.\r\nI called it `Conv2DBNNS` but perhaps `BNNSConv2D` is a better name?\r\n\r\nI also have another PR on top of this that adds an attr `filter_layout` to allow pre-transposing the weights.\r\n", "comments": ["Can one of the admins verify this patch?", "Sorry for not responding sooner.\r\n@petewarden would you or somebody in your team have time to look at this? Thanks.", "Thanks for the review and comments. A few thoughts:\r\n\r\nAs far as the (new) fused `Conv2D` Op that I am using, I used the [MKL Conv2D Op](https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/core/ops/nn_ops.cc#L2632-L2645) as a predicate. In that case Intel has defined a new Op that includes both the convolution Op and the BiasAdd Op. My Conv Op actually isn't pulling in just the activation function, rather, my Op and the MKL both have the bias add has been fused as well.\r\n\r\nI understand that the primitive `Conv2D` op in TF does not currently handle activations or bias addition, but this op may be \"too primitive\". In fact cuDNN is now moving in the direction of fusing these three into one kernel (see: https://github.com/tensorflow/tensorflow/issues/8828). Perhaps, a different FusedConv2D Op should be added that can be used by not just BNNS but also cuDNN v6 and any other underlying platform implementations that can take advantage of doing all three ops together. This would be  along the lines of batch norm, which now offers a `fused` version of the Op to reduce the number of primitive Ops used.\r\n\r\nThe secondary benefit of this Op is it allows for BNNS specific optimization flags, for example the level of precision (float16 vs float32) and control over the `filter_layout` (I already have code that can avoid having to transpose the weights on each call).\r\n\r\nAs far as testing, I have tested locally with some models, the one thing to keep in mind is that these two new Ops work with NCHW so the models do have to work with that channel layout.\r\n\r\nAddendum\r\nThere are even more implementations that can benefit from fusing Conv-Bias-Activation. Others include:\r\n* Metal Performance Shaders (https://github.com/tensorflow/tensorflow/issues/7958). Here it might be extra extra important because of 1. avoiding extra copies back and forth from GPU memory (ie using the tuned `MPSTemporary\u200bImage` and 2. because MPS uses a very weird memory layout and ideally transposing to this only needs to be done once.\r\n* MKL (as linked above, perhaps the MKL specific Op (`MklConv2DWithBias`) can be unified with this new Op.", "@petewarden what do you think?", "@petewarden ping?", "Sorry for the slow response on this one, that's my fault. We've been having some internal debate about the best way to support these kind of fused ops, since this is coming up more often. I like your proposal to create a general fused conv op that can have multiple implementations. I ended up doing something similar for fusing ResizeBilinear and MirrorPad into Conv2D:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/ops/nn_ops.cc#L732\r\n\r\nIdeally I'd love to see a version where the channel order was an attribute, even if the standard order was unimplemented for these cases. It would also be great if we could add a transform to automatically spot this pattern and replace it in graphs, the way I did here for the resize fusion:\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#fuse_convolutions\r\n\r\nAs a start though, would you be willing to put together a FusedConv2DWithBiasAndActivation op proposal? Apologies again for taking so long to get back on this.", "What do you mean by channel order, the order of the input / output tensors dimensions or the order of the filter dimensions? Or something else? ", "@cancan101 I believe he means the \"data_format\" (e.g., NCHW).\r\n\r\nI believe the concrete suggestion is:\r\n\r\n- Add an op that has data_format as an optional attribute, whose default is \"NHWC\". \r\n- In the op, check the data_format and do an OP_REQUIRES if the format isn't NCHW.  All tests would explicitly pass in NCHW as the data_format.\r\n", "(Any updates @cancan101 ?)", "Can one of the admins verify this patch?", "@cancan101 any luck with this?", "I haven't had much time to look at generalizing the approach. ", "@cancan101 marking this as stalled for now. Please ping this thread when you have a chance to get back to it.", "@cancan101 please re-open when you get cycles on this. Thanks!"]}, {"number": 8672, "title": "Update roadmap.md", "body": "", "comments": ["Jenkins, test this please."]}, {"number": 8671, "title": "regarding best approach to update tensorflow without affecting other installed packages", "body": "We have maintained a virtual environment to hold all the deep learning-related development projects. In this environment, we have installed `tensorflow, keras, theano` and quite a lot other` python` libraries for project purposes.\r\n\r\nCurrently, I would like to update `tensorflow `from `0.12.0-rc0` to `1.0`. To keep the current development environment, which is shared by other users, I don\u2019t want to change the current virtual environment at all.\r\n\r\nI am thinking of the following options,\r\n\r\n1) Create another virtual environment, have `tensorflow `with the latest version of `1.0`\r\n\r\n2) Are there any ways to just copy or link all the other python packages in the old environment to the new virtual environment? I just do not want to re-install everything.\r\n\r\nThank you for the advice!", "comments": ["I think you want `pip install --no-deps {tensorflow or tensorflow-gpu}` (assuming you're using pip). This upgrades tensorflow without touching the dependencies.", "This is not a TensorFlow issue. It is merely a request for help with pip/python package management. Such questions are better suited for StackOverflow. Please use github for reaching out to TensorFlow development team about issues in TF itself.\r\n\r\nClosing the issue."]}, {"number": 8670, "title": "Outputs of a model are different depending on the batch size (tested for inception models)", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nhttps://github.com/tensorflow/tensorflow/issues/1299\r\n\r\n### Environment info\r\nOperating System: ArchLinux\r\nCUDA: 8.0.61\r\ncuDNN: 5.1.10\r\nTensorFlow: 1.0.1 (both with and without GPU support for python 2.7)\r\n\r\nI  have also tested it with the latest nightly build:\r\nhttp://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.0.1-cp27-none-linux_x86_64.whl\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nMinimun script for testing: https://gist.github.com/jorgemf/e1f1a97ad39c02ace3576d1248327d2a\r\n\r\nI have tested it with the inception_v1 and inception_v4 models (commented in the code)\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nNone\r\n\r\n### Logs or other output that would be helpful.\r\n\r\nLogs for inception_v1 for the GPU and CPU based on my previous script.  You should expect the same output regardless of the batch size. Output must be the same for all the items of the batch as the input is the same image (which doesn't happen for one of the cases of this logs, I can upload the image and the checkpoint to reproduce that exactly case if required)\r\n\r\nPlease note that in this examples the errors in the output or the values of the layers are really small. You could think it is due to any weird conversion python does, but I have trained models based on inception_v1 where the outputs varies from 0.4 to 1.04.\r\n\r\nThe logs shows the output of the model (num_classes=1) and whether the layers outputs are exactly the same as with previous batch sizes or are different. Note that for some layers the outputs are the same regardless of the batch size, you start seeing errors with deeper layers. I only show few layers as example.\r\n\r\nGPU\r\n```\r\nbatch size: 1\r\n        outputs: [  5.03433178e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        OK Mixed_4f\r\n        OK Mixed_5c\r\nbatch size: 2\r\n        outputs: [  5.03433233e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        ERROR Mixed_4f  Different values than batch_sizes 1.\r\n        ERROR Mixed_5c  Different values than batch_sizes 1.\r\nbatch size: 10\r\n        outputs: [  5.03433233e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        ERROR Mixed_4f  Different values than batch_sizes 1. Same values as batch_sizes 2.\r\n        ERROR Mixed_5c  Different values than batch_sizes 1, 2.\r\nbatch size: 16\r\n        outputs: [  5.03433233e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        ERROR Mixed_4f  Different values than batch_sizes 1, 2, 10.\r\n        ERROR Mixed_5c  Different values than batch_sizes 1, 2, 10.\r\nbatch size: 20\r\n        outputs: [  5.03433287e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        ERROR Mixed_4f  Different values than batch_sizes 1, 16. Same values as batch_sizes 2, 10.\r\n        ERROR Mixed_5c  Different values than batch_sizes 1, 2, 16. Same values as batch_sizes 10.\r\n```\r\n\r\nCPU\r\n```\r\nbatch size: 1\r\n        outputs: [  5.03432962e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        OK Mixed_4f\r\n        OK Mixed_5c\r\nbatch size: 2\r\n        outputs: [  5.03433016e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        OK Mixed_4f\r\n        ERROR Mixed_5c  Different values than batch_sizes 1.\r\nbatch size: 10\r\n        outputs (ERROR: outputs must have the same value): [  5.03433178e-13   5.03433178e-13   5.03433178e-13   5.03433178e-13\r\n   5.03433178e-13   5.03433178e-13   5.03433178e-13   5.03433178e-13\r\n   5.03433016e-13   5.03433016e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        OK Mixed_4f\r\n        ERROR Mixed_5c  Different values than batch_sizes 1. Same values as batch_sizes 2.\r\nbatch size: 16\r\n        outputs: [  5.03433178e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        OK Mixed_4f\r\n        ERROR Mixed_5c  Different values than batch_sizes 1. Same values as batch_sizes 2, 10.\r\nbatch size: 20\r\n        outputs: [  5.03433178e-13]\r\n        OK inputs\r\n        OK Conv2d_1a_7x7\r\n        OK MaxPool_3a_3x3\r\n        OK Mixed_3b\r\n        OK Mixed_4b\r\n        OK Mixed_4f\r\n        ERROR Mixed_5c  Different values than batch_sizes 1. Same values as batch_sizes 2, 10, 16.\r\n```\r\n", "comments": ["I am not sure but I think this could be related with the full connected layers with a lot of inputs/outputs. Somehow the operations are not performed in the same order and the round error is different per batch size, this leads to an accumulation of the error different per batch size. When the layers are big (1024 neurons) the error adds and the output is completely different.", "Could you try this experiment with doubles instead of floats? I wonder if the extra precision will fix the stability issues. I'm not sure if this behavior is expected, maybe @shlens can comment further.", "@skye, I will test it but there will be still some error. What bothers me is that I would expect same error regardless the batch size.\r\n\r\nI have tested more and I am seeing the first errors in `slim.conv2d(branch_2, 32, [3, 3], scope='Conv2d_0b_3x3')`. In most executions the initial error is there but sometimes there is no error there. I am testing always with the same mode loading the same checkpoint and with the same input.", "To be clear\r\n```\r\nbatch size: 1\r\n        outputs: [  5.03433178e-13]\r\nbatch size: 2\r\n        outputs: [  5.03433233e-13]\r\nbatch size: 10\r\n        outputs: [  5.03433233e-13]\r\nbatch size: 16\r\n        outputs: [  5.03433233e-13]\r\nbatch size: 20\r\n        outputs: [  5.03433287e-13]\r\n```\r\nYou are worried that these are not the same error? between batch size 1 and 2. Those numbers are practically the same near zero result.  The floating point operations used between different batches may in fact be different between different tensor sizes due to performance auto tuning. Do you have any concrete reason to worry about this? It looks like not a bug to me.\r\n", "@aselle It is almost near 0 in that example, but I have a trained model that the error accumulates and it is close to 0.4 which it is a lot when the expected output is between 0 and 1, and what lead me to investigate further what was going on. My model is inception_v1 plus a couple of full connected layers.\r\n\r\nI don't mind the floating precision point error. But I expect that error to be consistent. Sometimes I get error in the convolution layer, other times there is no error (with different batch sizes). Regardless any optimization the output of an operation must be the same with the same input, which it is not the case. For the same input I am getting different results in the operations.\r\n\r\nIn some cases I am also getting different outputs with a batch size, like the batch size is 20, I use the same input 20 times and get 3 or 4 different results in the outputs. It is not consistent.\r\n\r\nAnyway, I did tweak my model to decrease that error and now I don't have the huge problem I had before. But I still think TensorFlow must return the same output for the same input, regardless of any optimization.", "@jorgemf, I understand your desire for consistency, but almost every parallel system is non-deterministic or it accepts a significant speed compromise to ensure exact determinism. Your problem might be posed in a way that is on the edge of numeric stability. As such, a perturbation in order of operations caused by gpu or cpu parallelism scheduling could easily cause inconsistent results. The design tradeoff has been chosen to be as fast as possible even if numeric predictability is less clear.\r\n\r\nIf you want us to look at it more closely, please provide a reproducible test case that has a significant error than than the near zero errors of this test case. Thanks!", "Closing the issue as I lost the model with the problem. I will open it again when I can replicate it.", "@aselle Hi  Aselle, I encounter a similar problem. I user inception_v2 as a base network. During training, the batchsize=128. During testing, if the batchsize=128, everything is ok. However, if the batchsize is smaller than 128 the results are different. And the precision declines as the batchsize drops. If the batchsize=1, the network will failed. I also used inception_v3 and inception_v1, the same problems appered. However, if the base network is replaced with Alex network (tensorflow), everything goes well. I think I have not used slim or inception_v2 properly. Could you please give me some advices\uff1f", "@aselle Today, I replace the inception_v2 with vgg (slim), and everything goes well. So I think the bug is associated with inception_v1~v3. Do you has any reminder?", "@JackieLeeTHU11 This is not related with the issue I opened. Please open your own issue and provide code examples of your problem.", "@jorgemf OK, I will. Thanks!"]}, {"number": 8669, "title": "Input ... was not previously added to ShapeRefiner", "body": "When calling `TF_GraphImportGraphDef`, some graphs result in the error \"Input (...) was not previously added to ShapeRefiner\". \r\n\r\nI'm still working on a MWE of a graph that causes this, but just wanted to double-check first that this does indeed indicate a bug. So far the graphs that trigger this involve back edges, although I'm not sure yet if that's necessary or sufficient. ", "comments": ["A MWE would be useful. I think even a not-so-minimal example would be sufficient; it shouldn't be too hard to track down where/how this is happening even in a large graph. This does look like a bug to me (although there might be some weird usage case I'm missing... either way the example would help).", "I don't have an end-to-end working C example yet since I'm working with a binding instead of the C API directly,\r\nbut \r\n\r\n1) if you populate a graph with the content of the serialized metagraph https://storage.googleapis.com/malmaud-stuff/graph.pb and then \r\n2) use `TF_GraphImportGraphDefWithReturnOutputs` on the graph from the metagraph https://storage.googleapis.com/malmaud-stuff/graph_import.pb , remapping \"x:0\"  to the tensor from the first graph named \"out:0\"\r\n\r\nthat triggers the error. If instead if you load those metagraphs into Python and use Python's `TensorFlow.import_graph_def` with the same remapping, it works. So this is unique to the C++ implementation of graph importing. ", "Can you provide a script in whatever language binding you're working with? How are you loading the metagraph in the non-Python case?", "Or provide an example failing graph. Note that graphs with back-edges in them are typically invalid (unless the back-edge was specifically created for the while loop - for which there is special handling in the TF runtime).", "As a very non-minimal working example:\r\n\r\n- Install [julia v0.5](https://julialang.org/downloads/),\r\n- then running `Pkg.add(\"TensorFlow.jl\") in the julia REPL\r\n- replace `~/.julia/v0.5/TensorFlow.jl/deps/usr/bin/libtensorflow.so`, with a `libtensorflow` that was compiled without using @malmaud's [patch](https://github.com/malmaud/TensorFlow.jl/blob/master/deps/build_libtensorflow/cpu/upstream_patch)\r\n- then run `Pkg.test(\"TensorFlow.jl\")` \r\n\r\nIIRC, 2 or 3 tests in the Packages testsets will fail because of this issue.\r\nAt some point, I can re-run the above set of steps and post the core for those tests here.\r\nI do believe they *are* related to the while loop.\r\nand it was the dynamic_rnn tests: https://github.com/malmaud/TensorFlow.jl/blob/f3cfa3342e92ce15564b33298bcd67d8827f0c00/test/nn.jl#L38-L96\r\n\r\n", "@skye \r\nA MWE in julia:\r\n\r\n```julia\r\nusing TensorFlow\r\n\r\nsess = Session(Graph())\r\ni = constant(1)\r\nend_i\u00a0=\u00a0TensorFlow.while_loop((i->i\u00a0<=\u00a010),\u00a0i->i+1, [i])\r\nrun(sess, end_i)\r\n```\r\n\r\nUsing @malmaud 's patched version of libtensorflow.so v1.2.0 ([patch here](https://github.com/malmaud/TensorFlow.jl/blob/master/deps/build_libtensorflow/gpu/upstream_patch))\r\nthe out put is as expected `[11]`\r\n\r\nUsing a freshly built libtensorflow.so v1.3.0 (but I can confirm this happened much earlier very similarly)\r\nthe gives an error on the line starting `end_i = TensorFlow.while_loop(...`\r\n\r\n```\r\nTensorflow error: Status: Input 0 ('Const') for 'while/Enter' was not previously added to ShapeRefiner.\r\n```\r\n\r\nI will now attempt to reduce this example down into something in a more supported language.\r\n\r\n\r\nThe metagraph I can export is below (may be being mangled by the encoding).\r\nBut I do not think it is useful, as the crash occurs before the graph is fully defined.\r\nThat metagraph I can reimport just fine.\r\n\r\n```\r\n\\00\u0012i\r\n[\r\n\u0005Const\u0012\u0005Const\"\\00*\u000b\r\n\u0005dtype\u0012\u00020\t*\u0014\r\n\u0006_class\u0012\r\n\r\n\b\u001a\\00\"\\00*\\002\\00*&\r\n\u0005value\u0012\u001dB\u001b\b\t\u0012\u0002\u0018\\00\u0018\\00\"\\00j\\00*\\002\\00:\\00J\\00R\u0001\u0001Z\\00b\\00\"\u0006\b\u0018\u0010\\00\u001a\\00\u0018\\00\u0012\\00\"\u0018\r\n\u0012TrainableVariables\u0012\u0002\u0012\\00\"\u0012\r\n\fQueueRunners\u0012\u0002\u0012\\00\"$\r\n\r\nwhile_context\u0012\u0013\u0012\u0011\r\n\u000f\r\n\u0005while\u0010\r\n\u0018\u0001 \\00J\\00\"\u000f\r\n\tSummaries\u0012\u0002\u0012\\00\"\u000f\r\n\tVariables\u0012\u0002\u0012\\00\r\n```\r\n\r\n\r\n", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 8668, "title": "get_attr returns dtype objects instead of raw ints", "body": "Fixes #447.\r\n\r\nget_attr is documented to return Python types.  It was returning\r\nraw integers (enumeration values) for 'type' attributes.  Now, those\r\nare converted to dtype objects.", "comments": ["Can one of the admins verify this patch?", "@vrv there may be API to consider, but it may be ok given conversion logic.", "Adding josh, who has thought more about types in TF in general.", "I think this is a good idea in principle (looks like it was an oversight that we didn't do that earlier when we introduced the dtype classes) as long as we have some confidence that the automatic conversion logic for dtypes means this won't break users. My expectation is that not many users are using this API (main exception would be a few gradient functions, but those are being moved to C++ anyway), so there is likely very little breakage introduced.", "Noobie at github. I made the requested changes to RELEASE.md , but a merge conflict was noted on that file in this pull request. I pulled the current tensorflow master branch and addressed the conflict, and pushed the change to my forked branch.  I'm not certain that I've done that right, though. Any feedback would be appreciated.", "So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->", "I resolved the conflict. Thank you for the contribution!", "Jenkins, test this please.", "Jenkins, test this please.", "Jenkins, test this please.", "So, those test failures are a bad omen. Clearly we cannot assign this return type to an int, specifically, not an int which is a field of a proto. I think this sinks this PR, since that's too common a thing to currently do to be able to break this behavior.\r\n\r\nMaybe it's possible to repair by improving automatic conversion. @Mycosynth do you want to give it a try?", "I think this PR could be saved if we added the following to DType:\r\n```\r\ndef __int__(self):\r\n    return self.as_datatype_enum\r\n```\r\nSo that implicit conversion to int would happen.\r\n\r\nThis seems safe to do for a few reasons:\r\n\r\n- Callers invoking implicit conversion are naturally expecting to understand the 'enum' of the DType.\r\n- We validate the correctness of the DType enum value during construction, so the return value of the int is always a true DType.\r\n\r\nThe only potential ambiguity is whether we want to ever return ref types, or their base enum, but since we're moving away from ref types towards resource types, I think it doesn't matter.\r\n\r\n@Mycosynth can you try making the change to add an __int__ method to DType?  If you don't want to, let us know and we can take a look first.", "@Mycosynth do you think you will be able to do this soon?  (Trying to close old PRs that haven't had any progress in a while)", "No. I will not be able to get to it soon. Feel free to take a look at it.\nApologies on the late response.\n\nOn May 4, 2017 2:47 PM, \"Vijay Vasudevan\" <notifications@github.com> wrote:\n\n> @Mycosynth <https://github.com/Mycosynth> do you think you will be able\n> to do this soon? (Trying to close old PRs that haven't had any progress in\n> a while)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/8668#issuecomment-299318190>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AWa4CV6UZOPotB0pnu9RKmZaLkcVx8w_ks5r2kdsgaJpZM4MnHRM>\n> .\n>\n", "Okay, I'll close this then, and maybe I'll try the __int__ approach (and give you credit on this if we end up submitting it)."]}, {"number": 8667, "title": "tf.nn.softmax -- the \"name\" argument is passed to the internal flattened tensor", "body": "Hi, if I use tf.nn.softmax with the `name` argument, I expect the output tensor to be given this name.\r\nHowever, the output tensor is always a Reshape op named `Reshape_...`, and the passed name refers to an internal (flattened) tensor.\r\n\r\nSteps to reproduce (TF v1.0.1):\r\n```\r\nIn [1]: import tensorflow as tf\r\nIn [2]: a = tf.nn.softmax(tf.ones([2, 8, 20]), name=\"softmax_custom_name\")\r\nIn [3]: print a\r\nTensor(\"Reshape_1:0\", shape=(2, 8, 20), dtype=float32)\r\nIn [4]: print tf.get_default_graph().get_tensor_by_name(\"softmax_custom_name:0\")\r\nTensor(\"softmax_custom_name:0\", shape=(16, 20), dtype=float32)\r\n```\r\n\r\nHere is the relevant code from the master branch:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1501\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L1519\r\n\r\nIs this the expected behaviour? In this case it would probably make sense to mention it in the documentation?\r\n\r\nOtherwise I'd submit a fix.", "comments": ["It does look like the reshaped output tensor should have \"name\", rather than its input. Feel free to submit a PR, thanks!"]}, {"number": 8666, "title": "`dynamic_rnn`'s docstring refers to a non-existing `rnn` function", "body": "Here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L392\r\n\r\nThe docstring of the `dynamic_rnn` function refers to a non-existing `rnn` function.\r\nI don't know if it got deleted or moved somewhere else.", "comments": ["@fchollet could you look into updating the documentation? Feel free to redirect as necessary.", "good catch: waiting for the change to propagate through"]}, {"number": 8665, "title": "freeze_graph not initializing tables", "body": "I am not sure if this is an actual bug or if its expected but undocumented behavior.\r\n\r\nI have a model that uses multiple lookup tables created via string_to_index. I freeze the model like so:\r\n`bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/tmp/tf/graph.pbtxt \r\n--input_checkpoint=/tmp/tf/model.ckpt-0 --output_graph=/tmp/ticker_classifier.pb \r\n--output_node_names=sigmoid --initializer_nodes=init_all_tables`\r\n\r\nHowever when the model is reloaded and I attempt to run it I get an error \"Table not initialized.\" I get exactly the same resulting file whether I specify initializer_nodes or not. The behavior I was expecting was for the model to contain the lookup tables in a ready to use state for inference but I don't know if that is an unreasonable expectation.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nI have not seen any issues related to this. I previously posted about this here http://stackoverflow.com/questions/42916383/how-to-properly-freeze-a-tensorflow-graph-containing-a-lookuptable\r\n\r\n### Environment info\r\nOperating System: MacOS and Linux (CentOS 7)\r\n\r\nInstalled version of CUDA and cuDNN: None\r\n\r\nIf installed from source, provide \r\n1. The commit hash (`git rev-parse HEAD`) 07bb8ea2379bd459832b23951fb20ec47f3fdbd4\r\n2. Build label: 0.4.5\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Thu Mar 16 12:19:38 2017 (1489666778)\r\nBuild timestamp: 1489666778\r\nBuild timestamp as int: 1489666778\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nI have been unable to make a small example but I can spend more time on it if needed.\r\n\r\n### What other attempted solutions have you tried?\r\n\r\nThe workaround is to add init_all_tables to the output_nodes and then run init_all_tables before feeding the session examples for inference. This does have the side effect of needing to distribute the source files for the tables to the same path on all nodes that was originally used for training.\r\n", "comments": ["@petewarden, do you have any insight into this?", "\r\n[Freezing_problem.zip](https://github.com/tensorflow/tensorflow/files/906064/Freezing_problem.zip)\r\nWe have a similar problem related to saving the operations as constants while freezing using the algorithm in the attached files.\r\nThe problem can be easily replicated by trying to freeze the graphs generated by the toy example in textsum https://github.com/tensorflow/models/tree/master/textsum . \r\n\r\nModels are trained with the following command:\r\nbazel-bin/textsum/seq2seq_attention     --mode=train     --article_key=article     --abstract_key=abstract     --data_path=textsum/data/data     --vocab_path=textsum/data/vocab     --log_root=textsum/log_root     --train_dir=textsum/log_root/train\r\n\r\nThen freeze_2_textsum.py is called with the following syntax:\r\npython freeze_2_textsum.py <model-folder> <outputnodename-string>\r\nCommand in our case was:\r\npython freeze_2_textsum.py --model_folder=./log_root/  --outputnodes=global_step\r\n\r\nIn this case, we are able to find the saved constants in the frozen_model.pb file.\r\nBut when we try the same syntax for the trained graph in our project, we could not find the constants in the frozen model.pb file, while the freeze_2_textsum.py script prints the log message that \"13 ops were converted to constants\"\r\nThis problem leads to the following error while running the session in our test script:\r\n\"Attempting to use uninitialized value model/generate_embedding_RNN_output/BiRNN/BW/BasicLSTMCell/Linear/Bias\"\r\ncmd line for test script:\r\n\r\npython test_tf_frozen_txtsum.py <path to frozen model.pb>\r\n", "@petewarden is this still an issue?", "One solution to this is to look at calling explicit initialization in the freeze_graph.py script, by specifying the `--initializer_nodes` argument:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py#L242\r\n\r\nYou'll need to know the name of the node that initializes your tables or other data structures though.", "I believe I tried that, but its always possible I did it wrong. Ultimately I have moved away from lookup tables since the java bindings still don't have support for passing arrays of strings. If that is added I will be interested in the issue again.", "seeing the same problems here. it doesn't seem like the `--initializer_nodes` is doing anything. @petewarden ", "I have a node: \r\n`init_table = tf.tables_initializer(name='init_all_tables')`\r\nand when I freeze the graph, I specify `--initializer_nodes=init_all_tables` . is this what I need to do?", "It's possible there's a bug in the way that `--initializer_nodes` is being called, you could add a logging line here to verify it is happening in your case:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py#L107\r\n", "yea I verified that the line of code is being called. but inside `convert_variables_to_constants` method call, the only place uses the `sess` is https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/graph_util_impl.py#L220 ,\r\n```\r\n  if variable_names:\r\n    returned_variables = sess.run(variable_names)\r\n```\r\nIt doesn't seems like it actually use anything from the `sess` about the initialized table, and I would imagine that the initialized table is not being used at all during freezing?", "Any update for this issue?", "@petewarden ^^", "ping. is there anyone can help to look into this bug?", "any updates?", "I have the same problem, I get this error:\r\n```\r\nFailedPreconditionError (see above for traceback): Table not initialized.\r\n```\r\n\r\nI am trying to store my graph as a protobuf and then load it, when I run the graph I get the error above.\r\n\r\nHowever when I run the graph directly (w\\o storing it to .pb first) it works fine.\r\n\r\n```python\r\nwith tf.gfile.GFile(frozen_file_name, \"wb\") as f:\r\n    f.write(frozen_graph_def.SerializeToString())\r\ng = load_graph(frozen_file_name)\r\nwith tf.Session(graph=g, config=utils.get_config_proto()) as sess1:\r\n    prefix = 'prefix/'\r\n    sess1.run(\r\n        prefix + infer_model.iterator.initializer.name,\r\n        feed_dict={\r\n            prefix + infer_model.src_placeholder.name: infer_data,\r\n            prefix + infer_model.batch_size_placeholder.name: hparams.infer_batch_size\r\n            })\r\n```", "Here is a minimal example of this issue:\r\n\r\n```python\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\r\nfrom tensorflow.python.ops.lookup_ops import HashTable, KeyValueTensorInitializer\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nOUTPUT_FOLDER = '/tmp'\r\nOUTPUT_NAME = 'hash_table.pb'\r\nOUTPUT_NAMES = ['output']\r\n\r\n\r\ndef build_graph():\r\n    d = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\r\n    init = KeyValueTensorInitializer(d.keys(), d.values())\r\n    hash_table = HashTable(init, default_value=-1)\r\n    data = tf.placeholder(tf.string, (None,), name='data')\r\n    values = hash_table.lookup(data)\r\n    output = tf.identity(values * 2, 'output')\r\n\r\n\r\ndef freeze_graph():\r\n    with tf.Graph().as_default() as graph:\r\n        build_graph()\r\n\r\n        with tf.Session(graph=graph) as sess:\r\n            sess.run(tf.tables_initializer())\r\n            print sess.run('output:0', feed_dict={'data:0': ['a', 'b', 'c', 'd', 'e']})\r\n\r\n            frozen_graph = convert_variables_to_constants(sess, sess.graph_def, OUTPUT_NAMES)\r\n            tf.train.write_graph(frozen_graph, OUTPUT_FOLDER, OUTPUT_NAME, as_text=False)\r\n\r\n\r\ndef load_frozen_graph():\r\n    with open(os.path.join(OUTPUT_FOLDER, OUTPUT_NAME), 'rb') as f:\r\n        output_graph_def = tf.GraphDef()\r\n        output_graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(output_graph_def, name='')\r\n        with tf.Session(graph=graph) as sess:\r\n            print sess.run('output:0', feed_dict={'data:0': ['a', 'b', 'c', 'd', 'e']})\r\n\r\n\r\nif __name__ == '__main__':\r\n    freeze_graph()\r\n    load_frozen_graph()\r\n```\r\nOutput:\r\n```\r\n[ 2  4  6  8 -2]\r\nConverted 0 variables to const ops.\r\nTraceback (most recent call last):\r\n  File \"/home/kiske/hashmap_test.py\", line 48, in <module>\r\n    load_frozen_graph()\r\n  File \"/home/kiske/hashmap_test.py\", line 43, in load_frozen_graph\r\n    print sess.run('output:0', feed_dict={'data:0': ['a', 'b', 'c', 'd', 'e']})\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1124, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1321, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1340, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Table not initialized.\r\n\t [[Node: hash_table_Lookup = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](hash_table, _arg_data_0_0, hash_table/Const)]]\r\n\r\nCaused by op u'hash_table_Lookup', defined at:\r\n  File \"/home/kiske/hashmap_test.py\", line 48, in <module>\r\n    load_frozen_graph()\r\n  File \"/home/kiske/hashmap_test.py\", line 41, in load_frozen_graph\r\n    tf.import_graph_def(output_graph_def, name='')\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py\", line 313, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nFailedPreconditionError (see above for traceback): Table not initialized.\r\n\t [[Node: hash_table_Lookup = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](hash_table, _arg_data_0_0, hash_table/Const)]]\r\n```\r\nIt seems like `convert_variables_to_constants` is stripping out `init_all_tables`, `key_value_init`, `key_value_init/keys`, and `key_value_init/values` nodes.\r\n\r\nAny help would be appreciated.", "Adding `init_all_tables` to the list of names to export fixes this issue. \r\n\r\n```python\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\r\nfrom tensorflow.python.ops.lookup_ops import HashTable, KeyValueTensorInitializer\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nOUTPUT_FOLDER = '/tmp'\r\nOUTPUT_NAME = 'hash_table.pb'\r\nOUTPUT_NAMES = ['graph/output', 'init_all_tables']\r\n\r\n\r\ndef build_graph():\r\n    d = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\r\n    init = KeyValueTensorInitializer(d.keys(), d.values())\r\n    hash_table = HashTable(init, default_value=-1)\r\n    data = tf.placeholder(tf.string, (None,), name='data')\r\n    values = hash_table.lookup(data)\r\n    output = tf.identity(values * 2, 'output')\r\n\r\n\r\ndef freeze_graph():\r\n    with tf.Graph().as_default() as graph:\r\n        with tf.name_scope('graph'):\r\n            build_graph()\r\n\r\n        with tf.Session(graph=graph) as sess:\r\n            sess.run(tf.tables_initializer())\r\n            print sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']})\r\n            frozen_graph = convert_variables_to_constants(sess, sess.graph_def, OUTPUT_NAMES)\r\n            tf.train.write_graph(frozen_graph, OUTPUT_FOLDER, OUTPUT_NAME, as_text=False)\r\n\r\n\r\ndef load_frozen_graph():\r\n    with open(os.path.join(OUTPUT_FOLDER, OUTPUT_NAME), 'rb') as f:\r\n        output_graph_def = tf.GraphDef()\r\n        output_graph_def.ParseFromString(f.read())\r\n\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(output_graph_def, name='')\r\n        with tf.Session(graph=graph) as sess:\r\n            try:\r\n                sess.run(graph.get_operation_by_name('init_all_tables'))\r\n            except KeyError:\r\n                pass\r\n            print sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']})\r\n\r\n\r\nif __name__ == '__main__':\r\n    freeze_graph()\r\n    load_frozen_graph()\r\n```\r\nThe call to `extract_sub_graph` inside `convert_variables_to_constants` prunes out this op and its descendants (keys, values) if you don't include `init_all_tables` in `output_node_names`. I don't like the idea of running an initializer op during inference and having a try/except seems hacky to me. Is there another way to do this?", "Thanks @jkiske your workaround works for me", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.", "Another freeze graph issue, related to #3628 and #7162. Keeping open, since there are multiple related problems, but I haven't been able to work on them. Adding in Suharsh, since he has been working in this area.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "PING! Any progress?", "@jkiske 's workaround worked for me!\r\n\r\nWe also built on top of it to make it work with `tf.tables_initializer()`, but it requires two other changes:\r\n- OUTPUT_NAMES needs to include the table initialization ops, which can be obtained with `tf.get_collection(tf.GraphKeys.TABLE_INITIALIZERS)`.\r\n- The MetaGraph, instead of the Graph, needs to be what's exported/imported. This is because `tf.tables_initializer()` references the `tf.GraphKeys.TABLE_INITIALIZERS` collection. The Graph does not contain a collection_list, but the MetaGraph does.\r\n\r\nSo here's a solution that works for us:\r\n\r\n```python\r\nimport os\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.graph_util import convert_variables_to_constants\r\nfrom tensorflow.python.ops.lookup_ops import HashTable, KeyValueTensorInitializer\r\n\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n\r\nOUTPUT_FOLDER = '/tmp'\r\nOUTPUT_NAME = 'hash_table.pb'\r\nOUTPUT_NAMES = ['graph/output']\r\n\r\n\r\ndef build_graph():\r\n    d = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\r\n    init = KeyValueTensorInitializer(d.keys(), d.values())\r\n    hash_table = HashTable(init, default_value=-1)\r\n    data = tf.placeholder(tf.string, (None,), name='data')\r\n    values = hash_table.lookup(data)\r\n    output = tf.identity(values * 2, 'output')\r\n\r\n\r\ndef freeze_graph():\r\n    with tf.Graph().as_default() as graph:\r\n        with tf.name_scope('graph'):\r\n            build_graph()\r\n\r\n        with tf.Session(graph=graph) as sess:\r\n            sess.run(tf.tables_initializer())\r\n            print sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']})\r\n            for table_init_op in tf.get_collection(tf.GraphKeys.TABLE_INITIALIZERS):\r\n                OUTPUT_NAMES.append(table_init_op.name)\r\n            frozen_graph = convert_variables_to_constants(sess, sess.graph_def, OUTPUT_NAMES)\r\n            tf.train.export_meta_graph(\r\n                filename=os.path.join(OUTPUT_FOLDER, OUTPUT_NAME),\r\n                graph_def=frozen_graph,\r\n                collection_list=[tf.GraphKeys.TABLE_INITIALIZERS])\r\n\r\n\r\ndef load_frozen_graph():\r\n    with tf.Graph().as_default() as graph:\r\n        tf.train.import_meta_graph(os.path.join(OUTPUT_FOLDER, OUTPUT_NAME))\r\n        with tf.Session(graph=graph) as sess:\r\n            sess.run(tf.tables_initializer())\r\n            print sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']})\r\n\r\n\r\nif __name__ == '__main__':\r\n    freeze_graph()\r\n    load_frozen_graph()\r\n```", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 64 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 79 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 94 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Nagging Assignees @petewarden, @suharshs: It has been 109 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I think the appropriate workarounds are documented here. Additionally using SavedModel with Estimators handles this correctly. I am going to close this issue for now.", "> I think the appropriate workarounds are documented here. Additionally using SavedModel with Estimators handles this correctly. I am going to close this issue for now.\r\n\r\nI met the issue when using https://github.com/tensorflow/models/tree/master/official/wide_deep to do freeze graph and predict. \r\nI think it is using estimator. \r\nI need add init_all_tables to the list of names to export fixes this issue, as jkiske mentioned above.\r\n\r\n```\r\nINPUT_SAVED_MODEL_DIR=./1564303973\r\nINPUT_CHECKPOINT=./model\r\nOUTPUT_NODES=\"head/predictions/probabilities,head/Tile,init_all_tables\"\r\npython -m tensorflow.python.tools.freeze_graph  \\\r\n--input_saved_model_dir ${INPUT_SAVED_MODEL_DIR} \\\r\n--input_checkpoint ${INPUT_CHECKPOINT} \\\r\n--input_binary=false \\\r\n--output_graph=/tmp/frozen_test.pb \\\r\n--output_node_names ${OUTPUT_NODES}\r\n```\r\n\r\n", "> Adding `init_all_tables` to the list of names to export fixes this issue.\r\n> \r\n> ```python\r\n> import os\r\n> \r\n> import tensorflow as tf\r\n> from tensorflow.python.framework.graph_util import convert_variables_to_constants\r\n> from tensorflow.python.ops.lookup_ops import HashTable, KeyValueTensorInitializer\r\n> \r\n> os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\n> \r\n> OUTPUT_FOLDER = '/tmp'\r\n> OUTPUT_NAME = 'hash_table.pb'\r\n> OUTPUT_NAMES = ['graph/output', 'init_all_tables']\r\n> \r\n> \r\n> def build_graph():\r\n>     d = {'a': 1, 'b': 2, 'c': 3, 'd': 4}\r\n>     init = KeyValueTensorInitializer(d.keys(), d.values())\r\n>     hash_table = HashTable(init, default_value=-1)\r\n>     data = tf.placeholder(tf.string, (None,), name='data')\r\n>     values = hash_table.lookup(data)\r\n>     output = tf.identity(values * 2, 'output')\r\n> \r\n> \r\n> def freeze_graph():\r\n>     with tf.Graph().as_default() as graph:\r\n>         with tf.name_scope('graph'):\r\n>             build_graph()\r\n> \r\n>         with tf.Session(graph=graph) as sess:\r\n>             sess.run(tf.tables_initializer())\r\n>             print sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']})\r\n>             frozen_graph = convert_variables_to_constants(sess, sess.graph_def, OUTPUT_NAMES)\r\n>             tf.train.write_graph(frozen_graph, OUTPUT_FOLDER, OUTPUT_NAME, as_text=False)\r\n> \r\n> \r\n> def load_frozen_graph():\r\n>     with open(os.path.join(OUTPUT_FOLDER, OUTPUT_NAME), 'rb') as f:\r\n>         output_graph_def = tf.GraphDef()\r\n>         output_graph_def.ParseFromString(f.read())\r\n> \r\n>     with tf.Graph().as_default() as graph:\r\n>         tf.import_graph_def(output_graph_def, name='')\r\n>         with tf.Session(graph=graph) as sess:\r\n>             try:\r\n>                 sess.run(graph.get_operation_by_name('init_all_tables'))\r\n>             except KeyError:\r\n>                 pass\r\n>             print sess.run('graph/output:0', feed_dict={'graph/data:0': ['a', 'b', 'c', 'd', 'e']})\r\n> \r\n> \r\n> if __name__ == '__main__':\r\n>     freeze_graph()\r\n>     load_frozen_graph()\r\n> ```\r\n> \r\n> The call to `extract_sub_graph` inside `convert_variables_to_constants` prunes out this op and its descendants (keys, values) if you don't include `init_all_tables` in `output_node_names`. I don't like the idea of running an initializer op during inference and having a try/except seems hacky to me. Is there another way to do this?\r\n\r\nuse op.run instead of session.run(tensor), throws no error \r\ninit_op = graph.get_operation_by_name('init_all_tables')\r\ninit_op.run(session=sess)\r\n"]}, {"number": 8664, "title": "SyntaxError: name '_googletest_temp_dir' is used prior to global declaration", "body": "I am following tutorial https://www.tensorflow.org/tutorials/image_retraining\r\n\r\nThe code successfully complied with Bazel but failed to run with command\r\n\r\n`bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos`\r\n\r\nWith a very strange ERROR:\r\n\r\n> Traceback (most recent call last):\r\n  File \"/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/examples/image_retraining/retrain.py\", line 79, in <module>\r\n    import tensorflow as tf\r\n  File \"/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/__init__.py\", line 104, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/test.py\", line 38, in <module>\r\n    from tensorflow.python.framework import test_util as _test_util\r\n  File \"/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 46, in <module>\r\n    from tensorflow.python.platform import googletest\r\n  File \"/home/tai/Desktop/TaiM2/tensorflow/tensorflow/bazel-bin/tensorflow/examples/image_retraining/retrain.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py\", line 118\r\n    global _googletest_temp_dir\r\nSyntaxError: name '_googletest_temp_dir' is used prior to global declaration\r\n\r\nWhen I run the **retrain.py** directly with python, the error did not show up. It only happens when I use bazel. Can anyone explain why?", "comments": ["This is fixed at head by commit https://github.com/tensorflow/tensorflow/commit/81fcc25ae5c92318ff607a5be3ff06446e232134"]}]