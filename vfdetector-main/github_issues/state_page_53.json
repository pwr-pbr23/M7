[{"number": 41796, "title": "the shared network inside a compiled keras model got modified after resetting the trainable attribute of the shared network", "body": "\r\n**System information**\r\n- TensorFlow version (use command below):  2.3.0\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI would like to do domain-translation with three domains with gan. But I found the shared network got modified although my model has been compiled.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Dense, Flatten\r\nfrom tensorflow.keras.models import Model\r\n\r\ndef dump_model( model, indent=0 ):\r\n    print( '-'*indent, model.name, ('None-Trainable', 'Trainable')[model.trainable] )\r\n    if hasattr(model, 'layers'):\r\n        for layer in model.layers:\r\n            dump_model( layer, indent+2 )\r\n\r\ndef build_critic():\r\n    input = Input( shape=(28, 28, 1 ) )\r\n    output = Dense(1)(Flatten()(input))\r\n    return Model( input, output )\r\n\r\ndef generator():\r\n    input = Input( shape=(28, 28, 1 ) )\r\n    output = Conv2D( 1, (3, 3), padding='same' )(input)\r\n    model = Model( input, output )\r\n    model.compile( loss='mae', optimizer='adam' )\r\n    return model\r\n\r\ndef build_mnist( optimizer = None ):\r\n    input_shape = ( 28, 28, 1 )\r\n    g_m2h = generator()\r\n    g_h2m = generator()\r\n    g_h2r = generator()\r\n    g_r2h = generator()\r\n\r\n    m_inputs = Input( shape=input_shape )\r\n    m_output_1 = g_h2m( g_m2h( m_inputs ) )\r\n    m_output_2 = g_h2m( g_r2h( g_h2r( g_m2h( m_inputs ) ) ) )\r\n\r\n    r_inputs = Input( shape=input_shape )\r\n    r_output_1 = g_h2r( g_r2h( r_inputs ) )\r\n    r_output_2 = g_h2r( g_m2h (g_h2m( g_r2h( r_inputs ) ) ) )\r\n\r\n    mh = g_m2h( m_inputs )\r\n    mmh = g_r2h( g_h2r( mh ) )\r\n\r\n    rh = g_r2h( r_inputs )\r\n    rrh = g_m2h( g_h2m( rh ) )\r\n\r\n    model_cycle = Model( inputs=[m_inputs, r_inputs], outputs=[m_output_1, m_output_2, r_output_1, r_output_2, mmh, rrh] )\r\n    model_cycle.compile( loss='mae', optimizer='adam' )\r\n\r\n    g_m2h.trainable = False\r\n    g_h2m.trainable = False\r\n    g_h2r.trainable = False\r\n    g_r2h.trainable = False\r\n    m_inputs = Input( shape=input_shape )\r\n    g_m2r = g_h2r( g_m2h( m_inputs ) )\r\n    model_m2r = Model( m_inputs, g_m2r, trainable=False )\r\n    model_m2r.compile( loss='mae', optimizer='adam' ) # must compile here?\r\n\r\n    critic_r = build_critic()\r\n    critic = critic_r\r\n    generator_model = model_m2r\r\n    real_image = Input(shape=input_shape)\r\n    valid = critic( real_image )\r\n    noisy_image = Input(shape=input_shape)\r\n    fake_image = generator_model(noisy_image)\r\n    fake = critic( fake_image )\r\n    critic_model = Model(inputs=[real_image, noisy_image], outputs=[valid, fake] )\r\n    critic_model.compile(loss='mae', optimizer='adam' )\r\n    model_wgan_r = critic_model\r\n    model_wgan_r.summary()\r\n    dump_model( model_wgan_r )\r\n\r\n    r_inputs = Input( shape=input_shape )\r\n    g_r2m = g_h2m( g_r2h( r_inputs ) )\r\n    model_r2m = Model( r_inputs, g_r2m, trainable = False )\r\n    model_r2m.compile( loss='mae', optimizer='adam' )\r\n\r\n    critic_m = build_critic()\r\n    critic = critic_m\r\n    generator_model = model_r2m\r\n    generator_model.Trainable = False\r\n    real_image = Input(shape=input_shape)\r\n    valid = critic( real_image )\r\n    noisy_image = Input(shape=input_shape)\r\n    fake_image = generator_model(noisy_image)\r\n    fake = critic( fake_image )\r\n    critic_model = Model(inputs=[real_image, noisy_image], outputs=[valid, fake] )\r\n    critic_model.compile(loss='mae', optimizer='adam' )\r\n    model_wgan_m = critic_model\r\n\r\n    g_m2h.trainable = True\r\n    g_h2m.trainable = True\r\n    g_h2r.trainable = True\r\n    g_r2h.trainable = True\r\n\r\n    critic_r.trainable = False\r\n    m_inputs = Input( shape=input_shape )\r\n    m2r_critic = critic_r( g_h2r( g_m2h( m_inputs ) ) )\r\n    model_critic_m2r = Model( m_inputs, m2r_critic  )\r\n    model_critic_m2r.compile( loss='mae', optimizer='adam' )\r\n\r\n    critic_m.trainable = False\r\n    r_inputs = Input( shape=input_shape )\r\n    r2m_critic = critic_m( g_h2m( g_r2h( r_inputs ) ) )\r\n    model_critic_r2m = Model( r_inputs, r2m_critic )\r\n    model_critic_r2m.compile( loss='mae', optimizer='adam' )\r\n\r\n    print( '\\n', '*'*80, '\\n' )\r\n\r\n    model_wgan_r.summary()\r\n    dump_model( model_wgan_r )\r\n\r\n    return model_cycle, model_wgan_r, model_wgan_m, model_critic_m2r, model_critic_r2m, model_m2r, model_r2m\r\n\r\nif __name__ == \"__main__\":\r\n    model_cycle, model_wgan_r, model_wgan_m, model_critic_m2r, model_critic_r2m, model_m2r, model_r2m = build_mnist()\r\n```\r\n\r\nThis code produces the following outputs:\r\n\r\n```\r\nModel: \"functional_15\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_10 (InputLayer)           [(None, 28, 28, 1)]  0                                            \r\n__________________________________________________________________________________________________\r\ninput_9 (InputLayer)            [(None, 28, 28, 1)]  0                                            \r\n__________________________________________________________________________________________________\r\nfunctional_11 (Functional)      (None, 28, 28, 1)    20          input_10[0][0]                   \r\n__________________________________________________________________________________________________\r\nfunctional_13 (Functional)      (None, 1)            785         input_9[0][0]                    \r\n                                                                 functional_11[0][0]              \r\n==================================================================================================\r\nTotal params: 805\r\nTrainable params: 785\r\nNon-trainable params: 20\r\n__________________________________________________________________________________________________\r\n functional_15 Trainable\r\n-- input_10 Trainable\r\n-- input_9 Trainable\r\n-- functional_11 None-Trainable\r\n---- input_7 Trainable\r\n---- functional_1 None-Trainable\r\n------ input_1 None-Trainable\r\n------ conv2d None-Trainable\r\n---- functional_5 None-Trainable\r\n------ input_3 None-Trainable\r\n------ conv2d_2 None-Trainable\r\n-- functional_13 Trainable\r\n---- input_8 Trainable\r\n---- flatten Trainable\r\n---- dense Trainable\r\n\r\n ******************************************************************************** \r\n\r\nModel: \"functional_15\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_10 (InputLayer)           [(None, 28, 28, 1)]  0                                            \r\n__________________________________________________________________________________________________\r\ninput_9 (InputLayer)            [(None, 28, 28, 1)]  0                                            \r\n__________________________________________________________________________________________________\r\nfunctional_11 (Functional)      (None, 28, 28, 1)    20          input_10[0][0]                   \r\n__________________________________________________________________________________________________\r\nfunctional_13 (Functional)      (None, 1)            785         input_9[0][0]                    \r\n                                                                 functional_11[0][0]              \r\n==================================================================================================\r\nTotal params: 805\r\nTrainable params: 0\r\nNon-trainable params: 805\r\n__________________________________________________________________________________________________\r\n functional_15 Trainable\r\n-- input_10 Trainable\r\n-- input_9 Trainable\r\n-- functional_11 None-Trainable\r\n---- input_7 Trainable\r\n---- functional_1 Trainable\r\n------ input_1 Trainable\r\n------ conv2d Trainable\r\n---- functional_5 Trainable\r\n------ input_3 Trainable\r\n------ conv2d_2 Trainable\r\n-- functional_13 None-Trainable\r\n---- input_8 None-Trainable\r\n---- flatten None-Trainable\r\n---- dense None-Trainable\r\n```\r\n\r\nFrom the printed model summary information, we can see that even for a compiled composed model  `M = A * B`,  we can modify its layers trainable attribute without recompiling it, just by resetting `A` or `B`'s trainable attribute.\r\n\r\n\r\n\r\n", "comments": ["Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/725c2a8d77b355844c00cbe287e2cfde/41796.ipynb). Thanks!", "Was able to replicate the issue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/8aeb071632de1ca123acb3c8c7bd4760/untitled108.ipynb#scrollTo=CaYRTr88AUHC)..Thanks !"]}, {"number": 41794, "title": "Loss follows Learning Rate trend when using tf.keras.optimizers.schedules.LearningRateSchedule", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution : CentOS Linux 7.6.1810 \r\n- TensorFlow installed from : binary (pip)\r\n- TensorFlow version (use command below): 2.2.0-rc3\r\n- Python version: 3.6.4\r\n- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7.6.0\r\n- GPU model and memory: TitanX - 12GB\r\n\r\n\r\n**Describe the current behavior**\r\nThe loss follows the pattern of learning rate when using [tf.keras.optimizers.schedules.LearningRateSchedule](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule). \r\n\r\nExamples showing the issue : \r\n![learning_rate_sch](https://user-images.githubusercontent.com/16310456/88607865-4f9df580-d04e-11ea-9cbf-233fa8a67d86.png)\r\nThese are few learning rate schedulers tested on mnist with different step_size and min-max range.\r\nIn all these plots, the trend in loss is similar to that of learning rate.\r\n\r\n**Describe the expected behavior**\r\nThe loss should be independent of learning rate trend. It should show high variance at higher learning rates and should be relatively stable with low learning rate.\r\n\r\n**Standalone code to reproduce the issue**\r\nFind gist [here](https://colab.research.google.com/gist/suraj-maniyar/e9669de59d0694ea6407f310d9923dc7/lrschedule.ipynb)\r\n", "comments": ["@suraj-maniyar \r\nI ran the code shared please find the [gist here](https://colab.research.google.com/gist/Saduf2019/071eee094783c664e5ece8e827c473f5/untitled297.ipynb) please let us know if it confirms your issue.", "@Saduf2019 \r\nYes. The gist you shared confirms my issue. Thanks", "Was able to replicate the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/59f7c28128ae234c0740a93dd44e40f1/untitled109.ipynb)..Thanks !"]}, {"number": 41792, "title": "sampled_softmax_loss weights and logits don't get gradients", "body": "**System information**\r\n- OS Platform and Distribution: Linux Ubuntu 20.04\r\n- TensorFlow version: 2.2.0\r\n- Python version: 3.8.2\r\n- CUDA/cuDNN version: 10.2 / 7.6.2 \r\n- GPU model and memory: TITAN X\r\n\r\n**Describe the current behavior**\r\n\r\nIn order to use `tf.nn.sampled_softmax_loss` weights and biases need to be provided as inputs. I believe internally rows from those tensors are selected based on the samples and hte computation is performed.\r\nThe problem is that if you create a model with a final Dense layer and provide the weights and biases of that layer as input to `tf.nn.sampled_softmax_loss`, you end up receving a warning that gradients for them are not computed:\r\n\r\n```\r\nWARNING:tensorflow:Gradients do not exist for variables ['my_model/dense_1/kernel:0', 'my_model/dense_1/bias:0'] when minimizing the loss.\r\nWARNING:tensorflow:Gradients do not exist for variables ['my_model/dense_1/kernel:0', 'my_model/dense_1/bias:0'] when minimizing the loss.\r\n```\r\nAs a consequence, they never get updated during training.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nGradients for those tensors should be computed and they should get updated.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import Model\r\nfrom tensorflow.keras.layers import Dense\r\nfrom tensorflow.keras.optimizers import SGD\r\n\r\nnum_classes = 500\r\nnum_epochs = 3\r\nnum_samples = 10000\r\nbatch_size = 10\r\nlearning_rate = 0.001\r\n\r\ny = np.random.randint(0, num_classes, num_samples, dtype=np.int64)\r\nx = np.expand_dims(y.astype(np.float32), -1)\r\n\r\nx_test = x[:10]\r\ny_test = y[:10]\r\n\r\n\r\nclass MyModel(Model):\r\n\r\n    def __init__(self, num_classes, *args, **kwargs):\r\n        super().__init__(*args, **kwargs)\r\n        self.dense1 = Dense(10)\r\n        self.dense2 = Dense(num_classes)\r\n        self.first_step = True\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        hidden = self.dense1(inputs)\r\n        if training and not self.first_step:\r\n            return None, hidden\r\n        else:\r\n            logits = self.dense2(hidden)\r\n            return logits, hidden\r\n\r\n\r\nclass SampledSoftmaxCrossEntropyLoss(tf.keras.losses.Loss):\r\n    def __init__(self, decoder_obj=None, num_classes=0):\r\n        super().__init__()\r\n        self.decoder_obj = decoder_obj\r\n        self.num_classes = num_classes\r\n\r\n    def call(self, labels, hidden):\r\n        labels = tf.cast(tf.expand_dims(labels, -1), tf.int64)\r\n\r\n        weights = tf.transpose(self.decoder_obj.get_weights()[0])\r\n        biases = self.decoder_obj.get_weights()[1]\r\n\r\n        sampled_values = tf.random.uniform_candidate_sampler(\r\n            true_classes=labels,\r\n            num_true=1,\r\n            num_sampled=5,\r\n            range_max=self.num_classes,\r\n            unique=False\r\n        )\r\n\r\n        loss_val = tf.nn.sampled_softmax_loss(\r\n            weights=weights,\r\n            biases=biases,\r\n            labels=labels,\r\n            inputs=hidden,\r\n            num_sampled=5,\r\n            num_classes=self.num_classes,\r\n            sampled_values=sampled_values)\r\n\r\n        return loss_val\r\n\r\n\r\nmy_model = MyModel(num_classes)\r\noptimizer = SGD(learning_rate=learning_rate)\r\nsampled_loss = SampledSoftmaxCrossEntropyLoss(\r\n    decoder_obj=my_model.dense2, num_classes=num_classes)\r\n\r\n\r\ndef train_step(model, loss, optimizer, inputs, targets):\r\n    with tf.GradientTape() as tape:\r\n        logits, hidden = model(inputs, training=True)\r\n        loss_val = loss(targets, hidden)\r\n    grads = tape.gradient(loss_val, model.trainable_weights)\r\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n    return loss_val\r\n\r\n\r\ndef oredict(model, inputs):\r\n    logits, _ = model(inputs, training=True)\r\n    predictions = tf.argmax(logits, -1)\r\n    return predictions\r\n\r\n\r\nx_batches = np.split(x, 100)\r\ny_batches = np.split(y, 100)\r\n\r\nprint(x_test)\r\nprint(oredict(my_model, x_test))\r\n\r\nfirst_batch = True\r\nfor i in range(num_epochs):\r\n    for x_batch, y_batch in zip(x_batches, y_batches):\r\n        if first_batch:\r\n            print(\"Weights and biases after first batch\")\r\n            print(my_model.dense2.get_weights()[0])\r\n            print(my_model.dense2.get_weights()[1])\r\n            first_batch = False\r\n\r\n        loss_val = train_step(my_model, sampled_loss, optimizer, x_batch,\r\n                              y_batch)\r\n        print(loss_val)\r\n\r\nprint(x_test)\r\nprint(oredict(my_model, x_test))\r\n\r\nprint(\"Weights and biases after training\")\r\nprint(my_model.dense2.get_weights()[0])\r\nprint(my_model.dense2.get_weights()[1])\r\n\r\n```\r\n\r\n", "comments": ["@w4nderlust \r\n\r\nI have tried in colab with TF version 2.2,2.3-rc2 .Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e0cb04ee83df6f3f047b3981c0945405/untitled.ipynb).You are also seeing the same behavior?\r\nThanks!", "Yes that's exactly the output I get (just with a different random initialization).\r\nAs you can see the weights and biases at the first epoch and at the end of the training are the same and you get those warnings that they are receiving no gradients.", "Was able to reproduce the issue in TF v2.5,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/bb51ec79378fc40b60e758719aa05890/untitled110.ipynb)..Thanks !", "Hi @w4nderlust ! I was able to resolve the issue in TF 2.6 , Attaching [Gist ](https://colab.research.google.com/gist/mohantym/c542c973ace96823d4b1b6b9ed38168e/untitled110.ipynb#scrollTo=ajw7CVUpID6n)for reference.\r\n[Reference.](https://stackoverflow.com/questions/57144586/tensorflow-gradienttape-gradients-does-not-exist-for-variables-intermittently)", "@mohantym from your gist it looks to me it looks that the weights and biases are the same after the first batch and at the end:\r\n\r\n```\r\nWeights and biases after first batch\r\n[[ 0.05411606  0.02750688 -0.09201522 ...  0.00735597  0.00862968\r\n  -0.06153766]\r\n [ 0.10168394  0.10543302 -0.04004839 ... -0.05372164  0.06464603\r\n   0.03657628]\r\n [ 0.03811692 -0.07817607  0.02010193 ...  0.05285154  0.04165239\r\n  -0.01438953]\r\n ...\r\n [-0.02329516  0.03987963  0.02113827 ... -0.03183416  0.02946573\r\n   0.00674187]\r\n [-0.07360842 -0.10110037 -0.06190708 ...  0.09768424 -0.00933281\r\n   0.0934676 ]\r\n [ 0.07280309  0.10233886  0.04173826 ... -0.09212768  0.08369612\r\n   0.01230942]]\r\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n 0. 0. 0. 0. 0. 0...]\r\n\r\nWeights and biases after training\r\n[[ 0.05411606  0.02750688 -0.09201522 ...  0.00735597  0.00862968\r\n  -0.06153766]\r\n [ 0.10168394  0.10543302 -0.04004839 ... -0.05372164  0.06464603\r\n   0.03657628]\r\n [ 0.03811692 -0.07817607  0.02010193 ...  0.05285154  0.04165239\r\n  -0.01438953]\r\n ...\r\n [-0.02329516  0.03987963  0.02113827 ... -0.03183416  0.02946573\r\n   0.00674187]\r\n [-0.07360842 -0.10110037 -0.06190708 ...  0.09768424 -0.00933281\r\n   0.0934676 ]\r\n [ 0.07280309  0.10233886  0.04173826 ... -0.09212768  0.08369612\r\n   0.01230942]]\r\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n 0. 0. 0. 0. 0. 0. 0. 0. 0. ...]"]}, {"number": 41772, "title": "SavedModel with dictionary/list data member", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf-nightly\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently, the SavedModel format doesn't support exporting a customized model with python dict/list (no?) in the graph, possibly due to some restrictions on tf.function tracing. For instance, all models from the object_detection API of model garden have the following workflow:\r\n\r\n```python\r\n# training\r\n# save label_dict (key:string, val:tensor) to model.label_dict (a python dict)\r\nmodel.provide_groundtruth(label_dict)\r\npredict_dict = model.predict(images)\r\n# compute loss based on model.label_dict\r\nloss = model.loss(predict_dict)\r\n```\r\n\r\nAll involved `dict`s have the same structure, e.g., same key values and value shapes.\r\n\r\nMeanwhile, even the feature is not supported now, the current error message is not intuitive. A similar usecase can be found [here](https://colab.research.google.com/drive/1gkG3M_Q4L0J4nmCXtYCyneqTnTe3M49V?usp=sharing).\r\n\r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 41770, "title": "Build TFLite with Flex Ops without Bazel ", "body": "Tensorflow Version: `2.4.0-dev20200712`\r\nOS: Ubuntu 18.10 \r\n(tensorflow/tensorflow docker)\r\n\r\nHello\r\n\r\nFollowed these instructions (C++ section) in the past to build a shared libtensorflowlite.so library\r\nhttps://www.tensorflow.org/lite/guide/ops_select\r\n\r\n```\r\nAdd the TensorFlow ops delegate library dependency to the build dependencies: tensorflow/lite/delegates/flex:delegate.\r\n```\r\n\r\nProblem is now I'm running into compatibility issues when I upload to the target device..\r\n\r\nFew questions:\r\n(1) How can I build a static library using the bazel pipeline? I used the `--config monolithic` flag but upon linking it still requires a newer glibc than my system supports\r\n```bazel build --config=elinux_aarch64 --cpu=aarch64 --config=monolithic --cxxopt=--std=c++14 --define=with_select_tf_ops=true -c opt //tensorflow/lite:libtensorflowlite.so```\r\n\r\n(2) How can I add the same flex op dependency to the makefile in lite/tools in order to build the lib without Bazel?\r\n\r\n\r\n", "comments": ["What TF version you are using?", "> What TF version you are using?\r\n\r\n2.4.0-dev20200712\r\n\r\nAdding to my original post", "@thaink - would you happen to be able to support with my question (2) above?", "(1) We don't provide a rule to build static library via Bazel now. Doesn't shared library work for you? Could you share why?\r\n(2) We don't have this path either.\r\n\r\nIf you want to build libtensorflowlite.so with Flex support, please apply the following patch.\r\nhttps://github.com/terryheo/tensorflow/commit/cdb8f3045669ea7adcd7ac4b115b01f13f7dad6e", "Hi @terryheo - \r\n\r\nThe problem is that the aarch64 toolchain that is provided uses a glibc version that is incompatible with my target. I was hoping to circumvent this through building the library statically . The shared library works for running on the host.\r\n\r\nDo you have any suggestions? I tried to configure a new toolchain but ran into issues mid-compilation that wasn't so obvious for me to debug.\r\n\r\nThanks for the patch ", "I see.\r\nI'm wondering which target OS platform you're using. (also glibc version of the target) If it's popular, I might need to consider supporting it.\r\n\r\nYou can try new toolchain by referring https://github.com/tensorflow/tensorflow/commit/4961f18733ca3967198393abf419e14476b4a85c#diff-f37bff3c4001d49511871eb70980dc27 change. But right, it's not trivial to do it.", "It's an aarch64 linux target with glibc of 2.26.\r\n\r\nIs it not trivial to add the flex dependency to the Makefiles as it is for a Bazel-based build?", "I see.\r\nTo build Flex delegate, it needs to build TF kernels. So until we have a way to build TF framework with Makefile, it's very difficult to build Flex delegate with Makefile."]}, {"number": 41746, "title": "Autograph fails to convert nested **if-else** in a for loop", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: \r\n- GPU model and memory: P100\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nUnexpected behavior of `tf.function` decorator. The decorator fails to convert `if-else` inside a loop and because of this behavior, it can't be used standalone or inside `tf.keras` `train_step` function\r\n \r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass Sampler:\r\n    def __init__(self, sample_size=10):\r\n        self.sample_size = tf.Variable(sample_size, dtype=tf.int32)\r\n        self.samples = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n     \r\n    @tf.function\r\n    def get_new_samples(self, data):\r\n        size = tf.shape(data)[0]\r\n        new_samples = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n        \r\n        for i in range(size):\r\n            if self.samples.size() < self.sample_size:\r\n                self.samples.write(i, data[i,:])\r\n                new_samples.write(i, data[i, :])\r\n            else:\r\n                if (tf.random.uniform([1]) > 0.5):\r\n                    idx = np.random.randint(0, size)\r\n                    new_sample = self.samples.read(idx)\r\n                    self.samples.write(idx, data[i, :])\r\n                    new_samples.write(i, new_sample)\r\n                else:\r\n                    new_samples.write(i, data[i, :])\r\n        return new_samples.stack()\r\n        \r\n    \r\n    def __call__(self, data):\r\n        return tf.cond(tf.equal(self.sample_size, 0),\r\n                      true_fn=lambda: data,\r\n                      false_fn=self.get_new_samples(data))\r\n\r\ns = Sampler()\r\ns(tf.convert_to_tensor(np.random.rand(5, 3).astype(np.float32)))\r\n```\r\n\r\nhttps://colab.research.google.com/drive/1Tf1Pj-_HzpUC8CMG8gS4U2nGV0EPxVd7?usp=sharing\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n```python\r\nOperatorNotAllowedInGraphError: in user code:\r\n\r\n    <ipython-input-2-b3cdab60c185>:13 get_new_samples  *\r\n        self.samples.write(i, data[i,:])\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:235 wrapped  **\r\n        return _add_should_use_warning(fn(*args, **kwargs),\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:1159 write\r\n        return self._implementation.write(index, value, name=name)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:833 write\r\n        self._write(index, value)\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:796 _write\r\n        if index < 0:\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:778 __bool__\r\n        self._disallow_bool_casting()\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:545 _disallow_bool_casting\r\n        \"using a `tf.Tensor` as a Python `bool`\")\r\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:532 _disallow_when_autograph_enabled\r\n        \" decorating it directly with @tf.function.\".format(task))\r\n\r\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\r\n\r\n\r\n```\r\n", "comments": ["I am able to replicate this issue reported, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/42ac41ed8b81114b7ba13b52e11bbc47/untitled301.ipynb)", "Any updates on this? :)", "Hi @AakashKumarNain, if I run this code with tf-nightly I get the slightly different error message that \r\n`AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.`\r\n\r\nHave you taken a look a the Autograph [limitations guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md) to make sure you're not attempting something that Autograph currently doesn't support?  There are some limitations when you apply Autograph to TF control flow (eg if statement with a tf.Tensor condition, as you have in your example). One thing to check is if the for loop is the culprit, ie make sure the if statements run correctly with Autograph when not in a loop.", "Hi @nikitamaia Thanks for the limitation guide. I looked at it and tried to make my example much simpler to replicate. Here is the updated version:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\npool1 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\npool2 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\nnew_items = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n\r\ndef write_to_array(array, idx, item):\r\n    array = array.write(idx, item)\r\n    return array\r\n\r\n@tf.function\r\ndef sampler(items, array1, array2):\r\n    num_items = tf.shape(items)[0]\r\n    for i in tf.range(num_items):\r\n        if array1.size() < 10:\r\n            array1 = write_to_array(array1, i, items[i, :, :, :])\r\n            array2 = write_to_array(array2, i, items[i, :, :, :])\r\n        else:\r\n            array2 = write_to_pool(array2, i, items[i, :, :, :])\r\n    return array2.stack()\r\n\r\nitems = tf.convert_to_tensor(np.random.rand(2, 2, 2, 3).astype(np.float32))\r\nsampler(items, pool1, new_items)\r\n\r\n```\r\n\r\nThis runs fine in eager mode but fails with `@tf.function`. TBH, this is a pretty basic use case where sampling is involved and if we can't run this, then we cannot override  the `train_step` in `keras` for model training.", "Have you tried creating the `TensorArrays` within your function as the error message suggests?\r\n`You may be attempting to capture a TensorArray inside a tf.function or tf.data map function. Instead, construct a new TensorArray inside the function.`\r\n\r\nIf I create pool1, pool2, and new_items within sampler then it seems to work, but I cannot test your code fully since `write_to_pool` hasn't been provided.\r\n", "Sorry, my bad. Let me try to make it clear as much as I can. \r\n\r\nThere are three arrays in total. Two of these arrays, `pool1` and `pool2` will be populated once until each of them contains `10` samples. After that, the values in these arrays won't change. The third array, `new_items` will contain some elements from the current batch `items` and  some other elements from the arrays `pool1` and `pool2` depending on the `if-else` condition.  I can create `new_items` inside the function as it will keep changing but I can't do that for the other two. Here is a much simpler version:\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\npool1 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\npool2 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n\r\ndef write_to_array(array, idx, item):\r\n    array = array.write(idx, item)\r\n    return array\r\n\r\n@tf.function\r\ndef sampler(items, array1):\r\n    num_items = tf.shape(items)[0]\r\n    new_items = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\r\n    for i in tf.range(num_items):\r\n        if array1.size() < 10:\r\n            array1 = write_to_array(array1, i, items[i, :, :, :])\r\n            new_items = write_to_array(new_items, i, items[i, :, :, :])\r\n        else:\r\n            new_items = write_to_array(new_items, i, items[i, :, :, :])\r\n    return new_items.stack()\r\n\r\nitems = tf.convert_to_tensor(np.random.rand(2, 2, 2, 3).astype(np.float32))\r\nsampler(items, pool1, new_items)\r\n```\r\n", "Ah, okay. Linking to a few related issues below since this has been previously reported. It is a known limitation, and unfortunately for now the only workaround is to either convert the TensorArray to a Tensor before passing it as an argument or returning it, or moving the code that creates and uses the TensorArray inside the @tf.function.\r\n\r\n#38454 #34683 #34683", "This seems to be a long standing issue. Is there any roadmap for this to track?", "There's no roadmap I can share that's tracking this bug. It's a known limitation (hence the error message) and I can certainly update this thread when there is any progress to support passing TensorArrays to a tf.function.", "Sure. That would be great help. Thanks @nikitamaia ", "Found same issue when I try to reproduce code:\r\nhttps://github.com/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\r\n\r\nIs there any solution update on most recent version or alternative solution when compare tensor with value in the graph? \r\n\r\nAnd another thing I notice: when I execute it in [colab.reasearch.google.com](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb) it could work.", "Was able to replicate the issue in TF v2.5 ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/5ad8ef27f224b8486ec8acd533f50ad0/untitled112.ipynb)..Thanks !"]}, {"number": 41737, "title": "OP_REQUIRES failure in c++ LoadSavedModel with conv2d w/ bias into an Add", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, latest\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a\r\n- TensorFlow installed from (source or binary): installed from arch community\r\n- TensorFlow version (use command below): tensorflow-opt-cuda 2.3.0rc2-2\r\n- Python version: 3.8.4\r\n- Bazel version (if compiling from source): n/a\r\n- GCC/Compiler version (if compiling from source): 10.1\r\n- CUDA/cuDNN version: cuda 11.0.2-1 cudnn 8.0.0.180-2\r\n- GPU model and memory: nvidia Titan Xp (12GB)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n2020-07-26 12:25:21.992520: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\nunknown 2.3.0-rc2\r\n\r\n**Describe the current behavior**\r\n\r\nI have successfully trained a little custom keras resnet with skip connections using the python side of things, so (simplified) the portion of network that is erroring during c++ load looks like:\r\n\r\n```\r\na\r\n+------------+\r\n|            |\r\nconv2d       |\r\n|            |\r\nlrelu        |\r\n|            |\r\nconv2d       |\r\n|            |\r\nAdd ---------+\r\n|\r\nout\r\n```\r\n\r\nwhen I save this keras model using model.save( 'foo' ), then attempt to load it in and execute it using the c++ layer, during execute, it is unable to run, reporting: Fusion is not implemented: [BiasAdd,Add]\r\n\t [[{{node foo/test_residadd/add}}]]\"\r\n\r\n**Describe the expected behavior**\r\nI would have expected it to load and optimize correctly so I could use it for inference in a c++ application\r\n\r\n**Standalone code to reproduce the issue**\r\nFiles attached. you can see the steps in recreate.sh, but basically, run the python file, this will save a model, then compile / run the c++ program to load the model.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n2020-07-26 13:21:36.941101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-07-26 13:21:36.960504: I tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /home/kimball/Development/bugreport/foo\r\n2020-07-26 13:21:36.961289: I tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\r\n2020-07-26 13:21:36.961304: I tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: /home/kimball/Development/bugreport/foo\r\n2020-07-26 13:21:36.961385: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-26 13:21:36.985670: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3601000000 Hz\r\n2020-07-26 13:21:36.986383: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de6bc0eaa0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-26 13:21:36.986394: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-26 13:21:36.987511: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-26 13:21:37.057104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-26 13:21:37.057480: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55de6bc0e030 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2020-07-26 13:21:37.057492: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1\r\n2020-07-26 13:21:37.057594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-26 13:21:37.057904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: TITAN Xp computeCapability: 6.1\r\ncoreClock: 1.582GHz coreCount: 30 deviceMemorySize: 11.88GiB deviceMemoryBandwidth: 510.07GiB/s\r\n2020-07-26 13:21:37.057932: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-07-26 13:21:37.059248: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-07-26 13:21:37.059806: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-07-26 13:21:37.059948: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-07-26 13:21:37.061270: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-26 13:21:37.061599: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-07-26 13:21:37.061666: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-07-26 13:21:37.061692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-26 13:21:37.061985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-26 13:21:37.062276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-07-26 13:21:37.062290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-07-26 13:21:37.268129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-26 13:21:37.268149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-07-26 13:21:37.268154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-07-26 13:21:37.268252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-26 13:21:37.268624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2020-07-26 13:21:37.268930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10019 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2020-07-26 13:21:37.269287: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2020-07-26 13:21:37.275552: I tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.\r\n2020-07-26 13:21:37.388514: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /home/kimball/Development/bugreport/foo\r\n2020-07-26 13:21:37.391535: I tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 431036 microseconds.\r\nLoaded model ok...\r\n2020-07-26 13:21:37.422171: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at conv_ops_fused_impl.h:700 : Unimplemented: Fusion is not implemented: [BiasAdd,Add]\r\nERROR: Fusion is not implemented: [BiasAdd,Add]\r\n\t [[{{node foo/testresid_add/add}}]]\r\n\r\n\r\n[bugreport.zip](https://github.com/tensorflow/tensorflow/files/4976996/bugreport.zip)", "comments": ["@kthurston \r\n\r\nWill it be possible for you to share code snippet to reproduce the issue.It helps us in localizing the issue faster.Thanks!", "I did - it's all in bugreport.zip that is attached to the issue there at the end of it", "Also faced the same issue while trying to port [this paper](https://github.com/alex04072000/ObstructionRemoval) to tf 2.x. Basically same system settings as Kimball, but on a 2060 Super.\r\n\r\nIf needed, I can try to build a notebook with my current attempt, although I'm not sure if it would be more useful than Kimball's one since the code I'm trying to reproduce seems to be way too complex.\r\n\r\nEdit:\r\n\r\nActually, my error seems to be a little bit different, here's the relevant traceback:\r\n```\r\nWARNING:tensorflow:From /usr/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\r\nInstructions for updating:\r\nUse `tf.global_variables_initializer` instead.\r\n2020-07-29 05:04:46.720381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \r\npciBusID: 0000:01:00.0 name: GeForce RTX 2060 SUPER computeCapability: 7.5\r\ncoreClock: 1.68GHz coreCount: 34 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.29GiB/s\r\n2020-07-29 05:04:46.720437: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0\r\n2020-07-29 05:04:46.720533: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-07-29 05:04:46.720563: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\r\n2020-07-29 05:04:46.720592: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\r\n2020-07-29 05:04:46.720621: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\r\n2020-07-29 05:04:46.720657: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11\r\n2020-07-29 05:04:46.720691: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8\r\n2020-07-29 05:04:46.721668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\r\n2020-07-29 05:04:46.721704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2020-07-29 05:04:46.721717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \r\n2020-07-29 05:04:46.721731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \r\n2020-07-29 05:04:46.722973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6106 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5)\r\nINFO:tensorflow:Restoring parameters from tfoptflow/tfoptflow/models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000\r\nINFO:tensorflow:Restoring parameters from ckpt_decomposition_reflection/model.ckpt\r\nINFO:tensorflow:Restoring parameters from ckpt_reconstruction_reflection/model.ckpt\r\n2020-07-29 05:11:06.138969: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11\r\n2020-07-29 05:11:07.404937: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at conv_ops_fused_impl.h:700 : Unimplemented: Fusion is not implemented: [BiasAdd,Add]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1365, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1349, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1441, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Fusion is not implemented: [BiasAdd,Add]\r\n\t [[{{node pwcnet/ctxt/refined_flow6}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_reflection_online.py\", line 860, in <module>\r\n    train()\r\n  File \"train_reflection_online.py\", line 841, in train\r\n    _, loss_value = sess.run([update_op, loss])\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 957, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1180, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1358, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py\", line 1384, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Fusion is not implemented: [BiasAdd,Add]\r\n\t [[node pwcnet/ctxt/refined_flow6 (defined at tfoptflow/tfoptflow/model_pwcnet.py:1492) ]]\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node pwcnet/ctxt/refined_flow6:\r\n pwcnet/predict_flow/flow6/BiasAdd (defined at tfoptflow/tfoptflow/model_pwcnet.py:1420)\t\r\n pwcnet/ctxt/dc_conv67/BiasAdd (defined at tfoptflow/tfoptflow/model_pwcnet.py:1490)\r\n\r\nOriginal stack trace for 'pwcnet/ctxt/refined_flow6':\r\n  File \"train_reflection_online.py\", line 860, in <module>\r\n    train()\r\n  File \"train_reflection_online.py\", line 409, in train\r\n    FB40_3, FB41_3, FB42_3, FB43_3 = PWC_full(F0_pred_4_up, F1_pred_4_up, F2_pred_4_up, F3_pred_4_up, F4_pred_4_up,\r\n  File \"train_reflection_online.py\", line 239, in PWC_full\r\n    pred_labels, _ = nn.nn(PWC_input, reuse=tf.compat.v1.AUTO_REUSE)\r\n  File \"tfoptflow/tfoptflow/model_pwcnet.py\", line 1545, in nn\r\n    flow = self.refine_flow(upfeat, flow, lvl)\r\n  File \"tfoptflow/tfoptflow/model_pwcnet.py\", line 1492, in refine_flow\r\n    return tf.add(flow, x, name=op_name)\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 356, in add\r\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\r\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3477, in _create_op_internal\r\n    ret = Operation(\r\n  File \"/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1949, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n```", "fyi: retested with cudnn 8.0.2.39 and official tensorflow 2.3.0 and this is still not working, it still throws an error about not being able to fuse BiasAdd and Add.", "@k-w-w Here is the [gist](https://colab.research.google.com/gist/jvishnuvardhan/c8d68a7c5c5f1b1507192d5a13d2ad5f/untitled11.ipynb) for our reference. model building, saving works but the as mentioned by @kdt3rd error is thrown when loading in c++. Thanks", "This is still broken in tensorflow 2.5. However, if I disable MKL by running with the TF_DISABLE_MKL environment variable set, it works. Further, if I #if 0 out the code in tensorflow/core/grappler/optimizers/remapper.cc around line 1940 (where there is an #ifdef INTEL_MKL and it tests for the two BiasAdd + Add contractions), then this also works around this issue. It seems like MKL knows about that optimization, but other backends do not? How do I add that optimization to the other engines, or properly test if the graph is scheduled to run on the GPU vs letting MKL do something with it?"]}, {"number": 41732, "title": "BoostedTreesClassifier only supports dictionary-based dataset", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): All\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\nWhen using `BoostedTreesClassifier` (and likely related classes), the features part of the dataset must be represented as a dictionary of strings to tensors whereas the docs state that the dataset can consist of \"A tuple (features, labels): Where features is a **tf.Tensor or** a dictionary of string feature name to Tensor...\"\r\n\r\n```python\r\n\r\n    772   outputs = {}\r\n    773   with ops.name_scope(\r\n--> 774       None, default_name='transform_features', values=features.values()):\r\n    775     transformation_cache = FeatureTransformationCache(features)\r\n    776     for column in feature_columns:\r\n\r\nAttributeError: 'Tensor' object has no attribute 'values'\r\n```\r\n\r\n**Describe the expected behavior**\r\nThe features part of the dataset should be able to be represented as a tensor (preferred) or the docs should be updated to reflect this constraint.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nimport tensorflow as tf\r\n\r\nN_OBS = 100\r\nN_DIM = 10\r\nBATCH_SIZE = N_OBS\r\n\r\ndf = pd.DataFrame(np.random.rand(N_OBS, N_DIM), columns=[f'dim_{i}' for i in range(N_DIM)])\r\nlabels = pd.Series(np.random.rand(N_OBS).round())\r\n\r\ndef input_fn(df, labels, batch_size=BATCH_SIZE):\r\n    # dataset = tf.data.Dataset.from_tensor_slices((dict(df), labels.values))  # works\r\n    dataset = tf.data.Dataset.from_tensor_slices((df.values, labels.values))  # does not work \r\n    dataset = dataset.repeat().batch(batch_size)\r\n    return dataset\r\n\r\nfeature_columns = [tf.feature_column.numeric_column(f'dim_{i}') for i in range(N_DIM)]\r\nestimator = tf.estimator.BoostedTreesClassifier(\r\n    feature_columns,\r\n    n_batches_per_layer=N_OBS // BATCH_SIZE\r\n)\r\n\r\nestimator.train(lambda: input_fn(df, labels), max_steps=100)\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nFeature column source where `features.values()` is referenced: https://github.com/tensorflow/tensorflow/blob/090f260aab3dab00bcdf0232962e753bb9fab696/tensorflow/python/feature_column/feature_column_v2.py#L418\r\n", "comments": ["@dwyatte \r\nCould you please refer to [this link](https://stackoverflow.com/questions/47030644/tensorboard-error-tensor-object-has-no-attribute-value) and let us know if it helps.\r\nalso [link1](https://github.com/keras-team/keras/issues/7362#issuecomment-315922830) \r\n", "@Saduf2019 this error appears to be specifically due to the expectation that the features part of the dataset has a `.values()` method, which is true of dictionaries.\r\n\r\nLooking at the other canned estimators, they raise a more useful exception e.g. for `LinearClassifier`, \r\n\r\n```\r\n~/GitHub/tf_boosted_trees/venv/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/linear.py in _linear_model_fn_v2(features, labels, mode, head, feature_columns, optimizer, config, sparse_combiner)\r\n    652   if not isinstance(features, dict):\r\n    653     raise ValueError('features should be a dictionary of `Tensor`s. '\r\n--> 654                      'Given type: {}'.format(type(features)))\r\n    655 \r\n    656   del config\r\n\r\nValueError: features should be a dictionary of `Tensor`s. Given type: <class 'tensorflow.python.framework.ops.Tensor'>\r\n```\r\n\r\nI think it would be helpful to raise this same exception for `BoostedTreesEstimator/BoostedTreesClassifier/BoostedTreesRegressor`. Furthermore, we might consider updating the docs to reflect that **only** dictionaries of `Tensor`s are supported for canned estimators (currently they suggest a user can alternatively pass a tensor without wrapping the columns in a dictionary).", "Was able to reproduce the issue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/1ce715eb3d42316042e3900bf919e721/untitled113.ipynb#scrollTo=QyptKXfpNiB6)..Thanks !"]}, {"number": 41718, "title": "High CPU memory usage when calling GradientTape's gradient() when using multiple threads/cores", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.3\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): Binary (pip install tf-nightly)\r\n- TensorFlow version (use command below): tf-nightly 2.4.0.dev20200724\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): None\r\n- CUDA/cuDNN version: None (CPU only)\r\n- GPU model and memory: None (CPU only)\r\n\r\n**Describe the current behavior**\r\nThe memory usage when using multiple threads is ~7GB on the example code provided below.\r\n\r\n**Describe the expected behavior**\r\nThe memory usage should be around ~850-900MB, which is what you get when you only use 1 thread via the following:\r\n```\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\ntf.config.threading.set_intra_op_parallelism_threads(1)\r\n```\r\n\r\n**Standalone code to reproduce the issue**\r\nhttps://colab.research.google.com/drive/1XEcTR433ePm4-OTi3i22PshKUk2aFFq-?usp=sharing\r\nNote that the issue is not reproducible in colab as it only runs on 1 core.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n\r\nMemory usage when tf.config.threading.set_*_op_parallelism_threads to 1:\r\n```\r\n2020-07-25 11:22:37.430254: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-25 11:23:01.051408: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-25 11:23:01.051508: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: UNKNOWN ERROR (-1)\r\n2020-07-25 11:23:01.051596: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (big000): /proc/driver/nvidia/version does not exist\r\n2020-07-25 11:23:01.052370: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-25 11:23:01.069108: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2094995000 Hz\r\n2020-07-25 11:23:01.069468: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5732470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-25 11:23:01.069529: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-25 11:23:01.638778: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n-------------------------------------------------------------------- -1.08216429\r\nMemory usage:  867950592\r\n-------------------------------------------------------------------- -1.02312386\r\nMemory usage:  898691072\r\n-------------------------------------------------------------------- -1.02274418\r\nMemory usage:  899932160\r\n-------------------------------------------------------------------- -0.958470643\r\nMemory usage:  900022272\r\n-------------------------------------------------------------------- -0.994939446\r\nMemory usage:  900042752\r\n-------------------------------------------------------------------- -1.04647171\r\nMemory usage:  900050944\r\n-------------------------------------------------------------------- -1.00102437\r\nMemory usage:  900235264\r\n-------------------------------------------------------------------- -0.987627268\r\nMemory usage:  900382720\r\n-------------------------------------------------------------------- -1.00234675\r\nMemory usage:  900382720\r\n-------------------------------------------------------------------- -0.957870305\r\nMemory usage:  900390912\r\n-------------------------------------------------------------------- -1.04384947\r\nMemory usage:  900399104\r\n-------------------------------------------------------------------- -1.05602765\r\nMemory usage:  900399104\r\n-------------------------------------------------------------------- -1.03467119\r\nMemory usage:  900411392\r\n-------------------------------------------------------------------- -1.00377405\r\nMemory usage:  900415488\r\n-------------------------------------------------------------------- -0.925668299\r\nMemory usage:  900415488\r\n-------------------------------------------------------------------- -1.08483529\r\nMemory usage:  900423680\r\n-------------------------------------------------------------------- -1.00288522\r\nMemory usage:  900423680\r\n-------------------------------------------------------------------- -0.909320414\r\nMemory usage:  900427776\r\n-------------------------------------------------------------------- -1.03661454\r\nMemory usage:  900431872\r\n-------------------------------------------------------------------- -0.955467224\r\n...\r\nMemory usage:  900431872\r\n```\r\n\r\nMemory usage peaks to around ~850MB.\r\n\r\n\r\nMemory usage when *not* setting tf.config.threading.set_*_op_parallelism_threads to 1 i.e., when multiple threads/cores are being used:\r\n\r\n```\r\n2020-07-25 11:06:52.523261: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n2020-07-25 11:07:17.286000: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2020-07-25 11:07:17.286099: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: UNKNOWN ERROR (-1)\r\n2020-07-25 11:07:17.286161: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (big000): /proc/driver/nvidia/version does not exist\r\n2020-07-25 11:07:17.287431: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2020-07-25 11:07:17.325674: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2094995000 Hz\r\n2020-07-25 11:07:17.344702: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x504e590 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-07-25 11:07:17.344800: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2020-07-25 11:07:18.198284: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:118] None of the MLIR optimization passes are enabled (registered 1)\r\n-------------------------------------------------------------------- -1.08216429\r\nMemory usage:  933933056\r\n-------------------------------------------------------------------- -1.02312386\r\nMemory usage:  1463521280\r\n-------------------------------------------------------------------- -1.02274418\r\nMemory usage:  1981599744\r\n-------------------------------------------------------------------- -0.958470643\r\nMemory usage:  2500599808\r\n-------------------------------------------------------------------- -0.994939446\r\nMemory usage:  3082350592\r\n-------------------------------------------------------------------- -1.04647171\r\nMemory usage:  3530485760\r\n-------------------------------------------------------------------- -1.00102437\r\nMemory usage:  4058849280\r\n-------------------------------------------------------------------- -0.987627268\r\nMemory usage:  4124266496\r\n-------------------------------------------------------------------- -1.00234675\r\nMemory usage:  4574625792\r\n-------------------------------------------------------------------- -0.957870305\r\nMemory usage:  5169975296\r\n-------------------------------------------------------------------- -1.04384947\r\nMemory usage:  5198962688\r\n-------------------------------------------------------------------- -1.05602765\r\nMemory usage:  5294829568\r\n-------------------------------------------------------------------- -1.03467119\r\nMemory usage:  5808898048\r\n-------------------------------------------------------------------- -1.00377405\r\nMemory usage:  6403420160\r\n-------------------------------------------------------------------- -0.925668299\r\nMemory usage:  6428565504\r\n-------------------------------------------------------------------- -1.08483529\r\nMemory usage:  6433001472\r\n-------------------------------------------------------------------- -1.00288522\r\nMemory usage:  6433193984\r\n-------------------------------------------------------------------- -0.909320414\r\nMemory usage:  6462169088\r\n-------------------------------------------------------------------- -1.03661454\r\nMemory usage:  6982217728\r\n-------------------------------------------------------------------- -0.955467224\r\nMemory usage:  6985584640\r\n-------------------------------------------------------------------- -1.02524447\r\nMemory usage:  7088070656\r\n-------------------------------------------------------------------- -1.06649399\r\nMemory usage:  7114588160\r\n-------------------------------------------------------------------- -0.991624892\r\nMemory usage:  7116353536\r\n-------------------------------------------------------------------- -1.01972055\r\nMemory usage:  7143043072\r\n-------------------------------------------------------------------- -0.994572461\r\nMemory usage:  7144923136\r\n-------------------------------------------------------------------- -0.965809107\r\nMemory usage:  7149711360\r\n-------------------------------------------------------------------- -1.02491224\r\nMemory usage:  7181701120\r\n-------------------------------------------------------------------- -1.02480865\r\nMemory usage:  7723294720\r\n-------------------------------------------------------------------- -0.970712662\r\nMemory usage:  7724584960\r\n-------------------------------------------------------------------- -1.02038872\r\nMemory usage:  7726415872\r\n-------------------------------------------------------------------- -1.01687396\r\nMemory usage:  7726661632\r\n-------------------------------------------------------------------- -1.03144\r\nMemory usage:  7726739456\r\n-------------------------------------------------------------------- -0.950338304\r\nMemory usage:  7748882432\r\n-------------------------------------------------------------------- -0.9357301\r\nMemory usage:  7748997120\r\n-------------------------------------------------------------------- -0.932868242\r\nMemory usage:  7781220352\r\n-------------------------------------------------------------------- -0.950198233\r\nMemory usage:  7781298176\r\n-------------------------------------------------------------------- -0.966640532\r\nMemory usage:  7782912000\r\n-------------------------------------------------------------------- -0.97049737\r\nMemory usage:  7783198720\r\n-------------------------------------------------------------------- -1.00543392\r\nMemory usage:  7810494464\r\n-------------------------------------------------------------------- -0.949840665\r\nMemory usage:  7810592768\r\n-------------------------------------------------------------------- -1.0022217\r\nMemory usage:  7810596864\r\n-------------------------------------------------------------------- -1.12232506\r\nMemory usage:  7810686976\r\n-------------------------------------------------------------------- -1.01822805\r\nMemory usage:  7813079040\r\n-------------------------------------------------------------------- -1.04360735\r\nMemory usage:  7813513216\r\n-------------------------------------------------------------------- -0.956581\r\nMemory usage:  7813517312\r\n-------------------------------------------------------------------- -0.981084287\r\nMemory usage:  7813537792\r\n-------------------------------------------------------------------- -0.977599561\r\nMemory usage:  7813615616\r\n-------------------------------------------------------------------- -1.01713431\r\nMemory usage:  8277536768\r\n-------------------------------------------------------------------- -1.03842521\r\nMemory usage:  8276979712\r\n-------------------------------------------------------------------- -0.971421421\r\nMemory usage:  8299802624\r\n-------------------------------------------------------------------- -1.06472087\r\nMemory usage:  8299806720\r\n-------------------------------------------------------------------- -1.00037909\r\nMemory usage:  8299974656\r\n-------------------------------------------------------------------- -1.00314832\r\nMemory usage:  8299995136\r\n-------------------------------------------------------------------- -0.99641335\r\nMemory usage:  8299999232\r\n-------------------------------------------------------------------- -1.07826197\r\nMemory usage:  8300007424\r\n-------------------------------------------------------------------- -0.998241484\r\nMemory usage:  8300498944\r\n-------------------------------------------------------------------- -1.01393604\r\nMemory usage:  8300503040\r\n-------------------------------------------------------------------- -1.04430008\r\nMemory usage:  8301498368\r\n-------------------------------------------------------------------- -0.956604362\r\nMemory usage:  8303042560\r\n-------------------------------------------------------------------- -0.994233251\r\nMemory usage:  8303177728\r\n-------------------------------------------------------------------- -1.0575521\r\nMemory usage:  8303247360\r\n-------------------------------------------------------------------- -0.975697637\r\nMemory usage:  8304357376\r\n-------------------------------------------------------------------- -1.0441494\r\nMemory usage:  8304455680\r\n-------------------------------------------------------------------- -0.949515164\r\nMemory usage:  8304496640\r\n-------------------------------------------------------------------- -0.935756862\r\nMemory usage:  8304496640\r\n-------------------------------------------------------------------- -1.00150812\r\nMemory usage:  8304496640\r\n-------------------------------------------------------------------- -0.983478725\r\nMemory usage:  8304627712\r\n-------------------------------------------------------------------- -0.986330569\r\nMemory usage:  8304635904\r\n-------------------------------------------------------------------- -1.13513744\r\nMemory usage:  8287686656\r\n-------------------------------------------------------------------- -1.03896141\r\nMemory usage:  8287719424\r\n-------------------------------------------------------------------- -0.988004625\r\nMemory usage:  8287723520\r\n-------------------------------------------------------------------- -0.985565066\r\nMemory usage:  8287731712\r\n-------------------------------------------------------------------- -1.0104388\r\nMemory usage:  8304316416\r\n-------------------------------------------------------------------- -1.06218076\r\nMemory usage:  8304353280\r\n-------------------------------------------------------------------- -0.94707495\r\nMemory usage:  8304357376\r\n-------------------------------------------------------------------- -0.998919964\r\nMemory usage:  8304357376\r\n-------------------------------------------------------------------- -0.974167764\r\nMemory usage:  8304361472\r\n-------------------------------------------------------------------- -0.97153908\r\nMemory usage:  8304365568\r\n-------------------------------------------------------------------- -0.964009821\r\nMemory usage:  8304369664\r\n-------------------------------------------------------------------- -0.992211759\r\nMemory usage:  8304373760\r\n-------------------------------------------------------------------- -1.04404831\r\nMemory usage:  8304431104\r\n-------------------------------------------------------------------- -1.02076721\r\nMemory usage:  8304435200\r\n-------------------------------------------------------------------- -0.962560713\r\nMemory usage:  8304439296\r\n-------------------------------------------------------------------- -1.02682006\r\nMemory usage:  8304443392\r\n-------------------------------------------------------------------- -1.03967941\r\nMemory usage:  8304447488\r\n-------------------------------------------------------------------- -1.07996821\r\nMemory usage:  8304480256\r\n-------------------------------------------------------------------- -0.98360759\r\nMemory usage:  8305147904\r\n-------------------------------------------------------------------- -1.0019182\r\nMemory usage:  8305152000\r\n-------------------------------------------------------------------- -0.935099125\r\nMemory usage:  8305205248\r\n-------------------------------------------------------------------- -1.03370762\r\nMemory usage:  8305209344\r\n-------------------------------------------------------------------- -0.98182\r\nMemory usage:  8305213440\r\n-------------------------------------------------------------------- -0.964839816\r\nMemory usage:  8305217536\r\n-------------------------------------------------------------------- -0.926978409\r\nMemory usage:  8305221632\r\n-------------------------------------------------------------------- -0.972101748\r\n...\r\n```\r\n\r\nAs you can see, memory usage peaks to around ~8GB.\r\n\r\nThe issue cannot be reproduced on google colab because colab only runs on 1 CPU anyway, so the behavior is the same regardless of whether you call tf.config.threading.set_*_op_parallelism_threads(1) or not.", "comments": ["I have a feeling that this issue is related to the `model.fit()` high memory usage as well.\r\nhttps://github.com/tensorflow/tensorflow/issues/40523", "I have tried in colab with TF nightly version(`2.4.0-dev20200726`).Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/e42914b14ec8d98204c47178215331c9/untitled183.ipynb).Thanks!", "I could try debugging this myself, but I'm not sure where to start looking. If any TF devs are reading this, it would be great if you could point me to the code that could be causing this issue.\r\n\r\nFor what it's worth, the issue disappears when you comment out the \"tape.gradient\" line.", "Quick triage notes:\r\n* This doesn't use keras (even though the repro includes a keras backend clear_session call)\r\n* The repro example appears to use: tf.function, gradienttape, autograph, tensorarrays, iteration. The memory usage appears to be a problem when using multiple threads.\r\n\r\n@jammm , some quick questions for you:\r\n1. are you observing that the memory usage is proportional to the number of threads? Or does it skyrocket to 8 GB as soon as you increase parallelism at all?\r\n2. if you explicitly call `gc.collect()`, what happens to the reported memory? (so we know whether the issue is an actual leak or a reference cycle that python's gc can eventually handle)\r\n3. Can you reproduce your issue when taking out any of the factors mentioned above? (e.g. tensorarrays, the tf.function, the loop, etc.) or does it only happen when all of them are combined?\r\n", "Was able to fix this by adding the following dels and gc.collect() call within my `with GradientTape() as t:` context:\r\n```\r\nvariables = self.trainable_variables\r\ngradients = t.gradient(loss, variables)\r\nself.optimizer.apply_gradients(zip(gradients, variables))\r\ndel gradients\r\ndel variables\r\ngc.collect()\r\n```", "@jammm,\r\nCan you please respond to  [tomerk's comment](https://github.com/tensorflow/tensorflow/issues/41718#issuecomment-700957911) and [rjdbcm's comment](https://github.com/tensorflow/tensorflow/issues/41718#issuecomment-731600429)? Thanks!", "@rmothukuru I haven't had the chance to get back to this yet. I remember decreasing the scale of the training itself so it would keep leaking memory but at a slow enough rate to finish before using too much RAM.\r\n\r\n\r\nAt the moment, I'm not working on the project that was causing that issue. I'll certainly update here if I do. I don't plan to stop using tensorflow just yet!"]}, {"number": 41699, "title": "Golang: SessionOptions & ConfigProto opaqueness & inaccessibility", "body": "**System information**\r\n- TensorFlow version (you are using): 1.15.0 (will move to stable 2.3.0 when it is available)\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe Go bindings of TensorFlow have a somewhat awkward API when it comes to setting the session options: as per the [docs](https://pkg.go.dev/github.com/tensorflow/tensorflow@v1.15.0/tensorflow/go?tab=doc#SessionOptions), you can configure it, but you have to input a binary-serialized [ConfigProto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto) message to do so. \r\nHowever, you cannot actually create a ConfigProto by calling the Go TF bindings - you have to create it in another way, [as demonstrated in the source code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/session_test.go#L366). \r\nCreating the ConfigProto message by using Python is extremely impractical and unnecessarily opaque. I would urge the authors to reconsider using protocol buffers in the Go bindings to allow creating the ConfigProto message from Go, thus making this flow much simpler and user-transparent.\r\n\r\n**Will this change the current API? How?**\r\nThis will introduce a dependency on protocol buffers in the Go bindings and add the generated protobuf structs to the API.\r\n\r\n**Who will benefit with this feature?**\r\nUsers of the TensorFlow Go implementation with a workload that demands more configuration for effective inference.\r\n\r\n", "comments": ["Continuation: maybe even add support for other config options such as `RunOptions` - enable setting things like the run handler pool?", "So, I'm going to take a stab at what needs to happen for this.  Granted, I have no idea if this is right, but this is what I've gathered from poking around the codebase for an hour or so.\r\n\r\nThe config.proto file is defined [here](https://github.com/tensorflow/tensorflow/blob/c2081e9c26a713d86e5f6b68f9fa101d44e43d91/tensorflow/core/protobuf/config.proto).  If compiled via protoc and the go plugin, it should [generate code](https://github.com/tensorflow/tensorflow/blob/c2081e9c26a713d86e5f6b68f9fa101d44e43d91/tensorflow/core/protobuf/config.proto#L16) that will sit at github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto \r\nBut I don't think that we are invoking the go/protoc plugin.  So now we jump into bazel.\r\nI _think_ the bazel step to compile the config.proto file happens [here](https://github.com/tensorflow/tensorflow/blob/c2081e9c26a713d86e5f6b68f9fa101d44e43d91/tensorflow/core/protobuf/BUILD#L156-L186), since `config.proto` is included in the `COMMON_PROTO_SRCS` above in that file.\r\nNow, that `tf_proto_library` function is defined way over [here](https://github.com/tensorflow/tensorflow/blob/48f0418e39203cf29c3508721f14062e4c30e0a3/tensorflow/core/platform/default/build_config.bzl#L493-L514) in `core/platform/default/build_config.bzl`.\r\nThere is now a parameter on line 509: `create_go_proto = False,`  Looking at the commit history that added that line, it seems like we never had a function implemented to call the protoc with go stuff at all.  The TPU code just had a macro placeholder that `gunan` removed...\r\n\r\nHere's where I start to get into assumptions. \r\n\r\nSo, I think we need to add a new function `tf_proto_library_go` that mimics `tf_proto_library_cc`/`tf_proto_library_py`.  Each one of those seems to drop down into another function of `cc_proto_library`/`py_proto_library`.\r\n\r\nThe py_proto_library seems to reference a different repo with bazel stuff, somewhere in [this .bzl file of gh.com/protocolbuffers/protobuf](https://github.com/protocolbuffers/protobuf/blob/master/protobuf.bzl), so maybe there should be a go function in there?  I'm not sure.  The cc versions might be a better template to follow.  But I don't know how to really even read bazels code, so I can't offer much help there.  :/\r\n\r\n------\r\n\r\nAm I anywhere in the right ballpark, or am I way way off?\r\nTo me, it seems like it would be easier to bypass bazel build stuff and just compile the files as part of the Go setup stuff.  \r\n_Something_ like the following? (it's getting late and I'm getting sleepy)\r\n\r\n```\r\nprotoc \\\r\n  --proto_path=tensorflow/core/protobuf \\\r\n  --go_out=tensorflow/go/core/protobuf/for_core_protos_go_proto\" \\\r\n  tensorflow/core/protobuf/*.proto\r\n```\r\n\r\nI didn't run any of this though, so it's all a stab in the dark.", "Ok, from the folder `github.com/tensorflow/tensorflow/tensorflow` (where the go folder is located), I can run\r\n\r\n`protoc -I stream_executor/ -I core/framework/ -I core/protobuf/ --go_out=../../../ core/protobuf/config.proto`, but this gives errors.  I think that's also part of the problem.  But I'm giving up for now.  :)", "Last update.\r\nI changed all of the `go_package` options in all of the protobuf files \r\n```diff\r\n-option go_package = \"github.com/tensorflow/tensorflow/tensorflow/go/core/framework/allocation_description_go_proto\";\r\n+option go_package = \"tensorflow/go/core/framework\";\r\n```\r\nThen I tried to do some protoc compiling, but I got some errors.  HOWEVER, I was able to generate the config go file via\r\n```sh\r\ncd /Users/jacob/go/src/github.com/tensorflow/tensorflow\r\nmkdir -p ./tensorflow/go/core/protobuf/\r\nprotoc \\\r\n    -I ./tensorflow/stream_executor/ \\\r\n    -I ./tensorflow/core/framework/ \\\r\n    -I ./tensorflow/core/protobuf/ \\\r\n    -I ./ \\\r\n    --go_out=./ \\\r\n    tensorflow/core/protobuf/config.proto\r\n```\r\n\r\nThis did have a `ConfigProto` structure in the generated file (i.e. `tensorflow/go/core/protobuf/config.pb.go`), so I _think_ that would then work.  \r\nPerhaps we would want to make the `go_package` include another final sub-package for each protofile.  i.e. `tensorflow/go/core/framework/config`, but I don't have a clear answer on that one.\r\nAlso, are these generated files going to be committed, or do we have to generate them each time we want to build?  I don't know how that works with `go get` commands."]}, {"number": 41698, "title": "tf.signal.dct very slow as compared to scipy.dct even on GPU", "body": "Platform: Google Colab with GPU runtime\r\nTensorflow: version 2.2\r\n\r\n`from scipy.fftpack import dct\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nxx=np.random.random_sample((8192,)).astype(np.float32)\r\n\r\nstart = time.time()\r\nfor i in range(1000):\r\n   \tx_dct=dct(xx,2,norm='ortho')\r\nprint('scipy:{}s'.format(time.time() - start))\r\n\r\nstart = time.time()\r\nx_tensor = tf.convert_to_tensor(xx)\r\nfor i in range(1000):\r\n   \tx_dct=tf.signal.dct(x_tensor,2, norm='ortho')\r\nprint('tf2:{}s'.format(time.time() - start))`\r\n\r\n`scipy: 0.08372211456298828s\r\ntf2: 2.0871903896331787s`\r\n\r\nWhy TF2 is too slow compared to scipy even on GPU runtime. Is is related to slow backend c++ FFT libraries?\r\n\r\n\r\n\r\n\r\n", "comments": ["I have tried in colab with TF 2.2, 2.3-rc1,nightly version(`2.4.0-dev20200724`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/00859fcd6f9a362b56c39264a301e9e5/untitled180.ipynb).Thanks!", "Could replicate the issue with **`Tensorflow Version 2.5`**. Please find [the Gist](https://colab.research.google.com/gist/rmothukuru/17c89c136c4ed2ed9799b2535ca1cd33/untitled180.ipynb). Thanks!"]}, {"number": 41695, "title": "Keras Metric Multiple Outputs / Inputs", "body": "**System information**\r\n- TensorFlow version (you are using): 2.2.0\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nKeras custom `metric` should optionally get as input all inputs and all outputs of the same batch.\r\nThis will allow creating metrics that are dependent on 2 or more outputs, or dependent on the input, etc.\r\n\r\n**Will this change the current api? How?**\r\nYes, a metric will have a property `use_all_inputs` or something like that\r\n\r\n**Who will benefit with this feature?**\r\nAnyone who is looking into custom metrics.\r\n\r\n**Any Other info.**\r\nIt is possible to do something similar with a callback, but that requires running `predict` twice per sample per epoch, and does not work for `model.evaluate`.\r\n\r\n\r\nRelated issues:\r\n\r\n- [https://github.com/keras-team/keras/issues/4506](https://github.com/keras-team/keras/issues/4506)\r\n- [https://stackoverflow.com/questions/63068206/keras-metric-for-multiple-outputs](https://stackoverflow.com/questions/63068206/keras-metric-for-multiple-outputs)\r\n- [https://stackoverflow.com/questions/63058307/tensorflow-dataset-mask-sequence-for-evaluation](https://stackoverflow.com/questions/63058307/tensorflow-dataset-mask-sequence-for-evaluation)\r\n- [https://datascience.stackexchange.com/questions/54443/keras-custom-metric-function-how-to-feed-2-model-outputs-to-a-single-metric-eval](https://datascience.stackexchange.com/questions/54443/keras-custom-metric-function-how-to-feed-2-model-outputs-to-a-single-metric-eval)", "comments": ["Just wanted to echo this!\r\n\r\nIt is currently not possible to have a metric that aggregates multiple model-outputs, without resorting to callback hacks. A common real-world example is the Labeled Attachment Score (LAS), often used in NLP Dependency Parsing. A model essentially outputs both an index and a label, and the LAS is essentially an accuracy measure for when both matches."]}, {"number": 41684, "title": "Weighted CE and BCE", "body": "\r\n**System information**\r\n- TensorFlow version (you are using): 2.0\r\n- Are you willing to contribute it (Yes/No):\r\nprobably not\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nFor now, loss function such as BCE or CE (Cross Entropy) can not use a specific weights for each class. For instance this feature is possible on pytorch. The way to do it is to use the class_weight option in the .fit method. However, it seems more appropriate to be able to use it through the loss function. \r\n\r\n**Will this change the current api? How?**\r\nI do not know\r\n**Who will benefit with this feature?**\r\nEveryone, especially when using multiple outpu,  it is not possible to use class weight in that specific case.\r\n**Any Other info.**\r\n", "comments": []}, {"number": 41625, "title": "Logging of \"TensorFlow with TPUs\" example in Google Colab is happening Twice", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA, as the issue can be replicated in Google Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Available by Default in Google Colab\r\n- TensorFlow version (use command below): 2.2\r\n\r\n**Describe the current behavior** :\r\n\r\nIn the [TensorFlow with TPUs Tutorial](https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=FpvUOuC3j27n) present in Google Colab, the Logging is happening Twice, as shown below:\r\n\r\n```\r\nINFO:tensorflow:Initializing the TPU system: 10.4.82.210:8470\r\nINFO:tensorflow:Initializing the TPU system: 10.4.82.210:8470\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Clearing out eager caches\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Finished initializing TPU system.\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:Found TPU system:\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Cores: 8\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Workers: 1\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Num TPU Cores Per Worker: 8\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\nINFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\r\n```\r\n\r\n**Describe the expected behavior**:\r\n\r\nLogging should happen only once. This has been discussed in this [Stack Overflow Question](https://stackoverflow.com/questions/33662648/tensorflow-causes-logging-messages-to-double) but is not fixed yet.\r\n\r\n**Standalone code to reproduce the issue**:\r\nColab link is https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=FpvUOuC3j27n. Same is the case in the [Tensorflow TPU Tutorial](https://www.tensorflow.org/guide/tpu#tpu_initialization) as well.", "comments": ["@rakeshmothukuru1,\r\nOn adding the line `logger.propagate = False`, messages are logged only once. Please check this [gist](https://colab.research.google.com/gist/amahendrakar/30ae6aa486a1cb7a660a2e80a7f8d81b/41625-v2.ipynb) for reference.\r\n\r\nAlso, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/8023#issuecomment-284102287) from a similar issue and let us know if it helps. Thanks!", "Four lines in the example above can be replaced by one line:\r\n```\r\ntf.get_logger().propagate = False\r\n```\r\nBut of cource doubling messages is a problem that should be fixed.", "> @rakeshmothukuru1,\r\n> On adding the line `logger.propagate = False`, messages are logged only once. Please check this [gist](https://colab.research.google.com/gist/amahendrakar/30ae6aa486a1cb7a660a2e80a7f8d81b/41625-v2.ipynb) for reference.\r\n> \r\n> Also, please take a look at [this comment](https://github.com/tensorflow/tensorflow/issues/8023#issuecomment-284102287) from a similar issue and let us know if it helps. Thanks!\r\n\r\nThat is just a workaround and not a solution, which I have already quoted in the [Stack Overflow Answer](https://stackoverflow.com/questions/33662648/tensorflow-causes-logging-messages-to-double) in my question. But the duplication of logging should be avoided by default, without any action from us. \r\n\r\nThis [SO Answer](https://stackoverflow.com/questions/33662648/tensorflow-causes-logging-messages-to-double) states \r\n\r\n> This was an unintended side-effect of the way tensorflow was using the logging package. I've changed it at HEAD to scope its internal loggers under the name \"tensorflow\" to avoid this pollution. Should be in the github head within a day or so.\r\n\r\nIt indicates that it is a bug and it should be fixed.", "@gowthamkpr I am probably not the right person for this, since this is a Core TF logging issue rather than anything TPU specific. "]}, {"number": 41594, "title": "tf.function breaks the gradient tape when looping over datasets", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): uname -a:\r\nDarwin Daniels-MacBook-Pro.local 18.7.0 Darwin Kernel Version 18.7.0: Tue Aug 20 16:57:14 PDT 2019; root:xnu-4903.271.2~2/RELEASE_X86_64 x86_64\r\n\r\n- TensorFlow installed from (source or binary): binary\r\n\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n\r\n- Python version: Python 3.6.8 :: Anaconda, Inc.\r\n\r\nSame bug in tf-nightly.\r\n\r\n**Describe the current behavior**\r\n\r\nThe gradient tape is broken in graph mode when a statement is introduced in a loop. \r\n\r\nIt works correctly in Eager mode or when the statement is taken out of the loop. \r\n\r\n**Describe the expected behavior**\r\n\r\nAs detailed in the documentation, a tf.function should either behave as in Eager mode or return an error. Here, none of those happen.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee [Colab notebook](https://colab.research.google.com/drive/1y__myjOuJEMF4izbJuoLbGqgDItDm7jn?usp=sharing).\r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef training_step(data, par):\r\n    loss = tf.constant(0, dtype=tf.double)\r\n    with tf.GradientTape() as tape:\r\n        for d in data:\r\n            loss = par # works if we take this line out of the loop\r\n    return tape.gradient(loss, par)\r\n    \r\npar = tf.Variable(tf.zeros(1, dtype=tf.double))\r\ndata = tf.data.Dataset.from_tensor_slices([1,2,3])\r\n\r\ntf.config.experimental_run_functions_eagerly(False)\r\nprint(f\"graph mode: {training_step(data, par)}\")\r\n\r\ntf.config.experimental_run_functions_eagerly(True)\r\nprint(f\"eager mode: {training_step(data, par)}\")\r\n  \r\n```\r\n\r\nObviously, the same happens if `loss = par` is replaced with `loss += par`, as will typically be the case in a training loop.\r\n\r\n**Other info / logs** \r\nThe output is:\r\n\r\n```\r\ngraph mode: None\r\neager mode: [1.]\r\n```\r\n", "comments": ["Was able to reproduce the issue with TF v2.2, TF v2.3.0-rc2 and TF-nightly. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/033e6d33d0099320d6ac1e56542b7f2f/41594-tf-nightly.ipynb). Thanks!", "This is due to a known limitation: `Dataset.reduce` doesn't support gradients.\r\n\r\nA workaround is to use `for d in iter(data)`, which AutoGraph translates into a `tf.while_loop` instead of a `Dataset.reduce`.\r\n\r\n```\r\n    loss = tf.constant((0,), dtype=tf.double)\r\n    with tf.GradientTape() as tape:\r\n        for d in iter(data):\r\n            loss = par\r\n```\r\n\r\nWe're working to change the default behavior in AutoGraph to always use a `tf.while_loop`.", "Was able to reproduce this issue in TF 2.6.0-dev20210526 ,please find the gist[ here ](https://colab.research.google.com/gist/sushreebarsa/8364f9b32e8dfcb394431015be5a4411/untitled9.ipynb)..Thanks !"]}, {"number": 41589, "title": "When dose Tensorflow add elasticity function in worker\uff1f", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):1.15\r\n- Are you willing to contribute it (Yes/No): with pleasure, but I'm a rookie.\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nWe has lots of jobs and a set number of resource. We hope:\r\nWhen resource is enough, jobs can add some workers to increase training speed.\r\nWhen resource is lacking, jobs will delete some workers to release resource.\r\n\r\n\r\n**Any Other info.**\r\nDoes Tensorflow have any plan on elasticity function, and when does TF support elasticity function in workers?", "comments": ["Hi @zhaozheng09, for multiworker training this is not an out of the box feature we currently have in TF. And I don't know that it's a common use case for distributed training with ML training because of the consistent nature of the workload and because you might not always see a direct speedup simply by increasing the number of machines. But if possible my thought is it would probably work best with an asynchronous training strategy (eg [ParameterServer](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy)) and not a synchronous strategy where the GPUs are in sync at the end of each batch.", "@nikitamaia, We use async training now.\r\nWe has a test that multi worker will speedup training(like y = log2x    x:worker nums, y:training speed), and our uesrs only use less worker, so dynamic add worker will has a good effect.\r\nWe hope TF auto add worker to speedup training when resource is enough.\r\nSo I think Dynamic add/delete workers will be a import function.", "If you will be using our TF 2 parameter server training (in the next release) and willing to contribute, I would be happy to help.", "> If you will be using our TF 2 parameter server training (in the next release) and willing to contribute, I would be happy to help.\r\n\r\nthx", "Hi @zhaozheng09. I have developed adding and removing workers \"while in training\". If you (or somebody else) are still interested in this feature you can contact me. ", "Hi, for dynamic worker removing one change in Worker class is necessary. \r\nThe change is here:\r\n```python\r\nclass Worker:\r\n  [...]\r\n  def _process_queue(self):\r\n    \"\"\"Function running in a worker thread to process closure queues.\"\"\"\r\n    self._maybe_delay()\r\n    while self._should_worker_thread_run:\r\n      closure = self._cluster._closure_queue.get()  # pylint: disable=protected-access\r\n      if not self._should_worker_thread_run or closure is None:\r\n        if closure is not None:\r\n          closure.mark_cancelled()\r\n        return\r\n      self._process_closure(closure)\r\n      # To properly stop the worker and preemption threads, it is important that\r\n      # `ClusterCoordinator` object is not held onto so its `__del__` can be\r\n      # called. By removing the reference to the `closure` that has already been\r\n      # processed, we ensure that the `closure` object is released, while\r\n      # getting the next `closure` at above `self._cluster._closure_queue.get()`\r\n      # call.\r\n      del closure\r\n  [...]\r\n```\r\n\r\nCould someone commit it? Thanks in advance."]}, {"number": 41581, "title": "Camera does not resume when activity is resumed - TFLite Android flower-classification codelab", "body": "<em>This is a bug as per the [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md)</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 8\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Moto E5\r\n\r\n**Describe the current behavior**\r\nCamera is stopped when the activity is paused, but does not resume/reinitialize when the activity is resumed\r\n\r\n**Describe the expected behavior**\r\nCamera should reinitialize when the activity is resumed.\r\n\r\n**How to reproduce the issue**\r\nOpen the app. Go to home through system navigation, i.e, pause the activity. Reopen the app from recents screen, i.e, resume the activity. You'll see that the camera is stuck.\r\n\r\n**Other info / logs**\r\nLink to Tensorflow Lite Android flower-classification Codelab project -\r\nhttps://github.com/tensorflow/examples/tree/master/lite/codelabs/flower_classification/android/start\r\n\r\nI've already identified the error and fixed it in my fork. Would be glad to create a pull request here.\r\n", "comments": []}, {"number": 41580, "title": "tensorflow lite performs linear relation between batch size and inference time", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.2.1511\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below):2.3.0.dev2020062301\r\n- Python version:3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):4.8.5\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n  I converted a tiny bert module to tflite and run the  inference with the tensorflow lite c++ api. When batch size=1, tensorflow lite performs average runtime 0.6ms, while tensorflow performs average runtime 1ms(with default threads num); when batch size=10, tensorflow lite performs average runtime 5ms, while tensorflow performs average runtime 3ms.\r\n  It seems tensorflow lite did nothing on multi thread speed up, as i tried to apply SetNumThreads(4).\r\n  And SetNumThreads(4) and SetNumThreads(1) performs same runtime, though cpu usage change from 100% to 200%\r\n\r\n**Describe the expected behavior**\r\n  I am wondering is this a normal performance for tflite in the X86 desktop?\r\n\r\n**Standalone code to reproduce the issue**\r\nHere is the part of my custom tflite c code\r\n ```\r\n class Session {\r\n public:\r\n    Session() {\r\n      model_ = NULL;\r\n      interpreter_ = NULL;\r\n    }\r\n\r\n  bool Open(const std::string &saved_model) {\r\n    model_ = tflite::FlatBufferModel::BuildFromFile(saved_model.c_str());\r\n    if (!model_) {\r\n      return false;\r\n    }\r\n\r\n    tflite::InterpreterBuilder(*model_.get(), resolver_)(&interpreter_);\r\n\r\n    if (!interpreter_) {\r\n      return false;\r\n    }\r\n    interpreter_->SetNumThreads(4);\r\n    return true;\r\n  }\r\n\r\n  bool Run(std::vector<int> &dims, int32_t *tok_id, int32_t *msk_id, int32_t *seg_id, float *output) const {\r\n     int tok_index = interpreter_->inputs()[2];\r\n     int msk_index = interpreter_->inputs()[1];\r\n     int seg_index = interpreter_->inputs()[0];\r\n     interpreter_->ResizeInputTensor(tok_index, dims);\r\n     interpreter_->ResizeInputTensor(msk_index, dims);\r\n     interpreter_->ResizeInputTensor(seg_index, dims);\r\n\r\n     if(interpreter_->AllocateTensors() != kTfLiteOk) //remove AllocateTensors() did not change the runtime\r\n         return false;\r\n     int32_t bytes = dims[0] * dims[1] * sizeof(int32_t);\r\n     int32_t* tok_tensor = interpreter_->typed_tensor<int32_t>(tok_index);\r\n     memcpy(tok_tensor, tok_id, bytes);\r\n     int32_t* msk_tensor = interpreter_->typed_tensor<int32_t>(msk_index);\r\n     memcpy(msk_tensor, msk_id, bytes);\r\n     int32_t* seg_tensor = interpreter_->typed_tensor<int32_t>(seg_index);\r\n     memcpy(seg_tensor, seg_id, bytes);\r\n     if(interpreter_->Invoke() != kTfLiteOk)\r\n         return false;\r\n     bytes = dims[0] * sizeof(float);\r\n     float* result = interpreter_->typed_output_tensor<float>(0);\r\n     memcpy(output, result, bytes);\r\n     return true;\r\n  }\r\n\r\nprivate:\r\n  std::unique_ptr<tflite::FlatBufferModel> model_;\r\n  std::unique_ptr<tflite::Interpreter> interpreter_;\r\n  tflite::ops::builtin::BuiltinOpResolver resolver_;\r\n};\r\n\r\n```\r\n", "comments": ["T.J Can you please help.\r\n\r\nThanks"]}, {"number": 41525, "title": "Suggestions For Tensorflow v3", "body": "1.  Rebuild from scratch.  At this point there are too many errors, warnings, and old code to do anything about.  It might be an idea to streamline Tensorflow, by creating a new version with only the most important features, and then adding new ones.\r\n\r\n2.  Make updates compatible with previous versions.  Do not change syntax used in previous versions.  For instance, any repository written for Tensorflow 3.2 should be completely compatible with Tensorflow 3.8 .  Currently, if a repository is written in Tensorflow and the version is not specifically mentioned by the repo's author, it is anyone's guess as to which specific version of Tensorflow 1 or 2 will work.  A repo written in Tensorflow 1.4 is completely incompatible with Tensorflow 1.5.  Can you imagine if Microsoft Word worked like this?  For every new version update, all your documents are now incompatible and have to be re-written?\r\n\r\n3.  Fewer warnings.  When starting up Tensorflow, a full page of warnings appear.  I would be nice to not have these.\r\n\r\n4.  Install correct version of Protoc automatically.  This will avoid the common error \"ImportError: DLL load failed\".\r\n\r\n5.  If possible, automatically install the correct versions of Cuda and CUDNN.  The process can be long and fairly difficult.  Even better if this can be done inside a virtual environment so it does not affect the host system's settings.\r\n\r\n6.  If tensorflow version is incorrect for the repository, give a hint as to which tensorflow version to install instead, and which commands to enter for that purpose.\r\n\r\n7.  No more guessing games: more helpful error messages.  Take the most common stackoverflow errors with tensorflow, and add the solutions to the error messages received from Tensorflow.\r\n\r\n8.  Coordinate more closely with Python.org, Nvidia, and other dependencies to ensure compatbility with new versions of Tensorflow.\r\n\r\n9.  Make installation process on computers easier and more automatic.  \"pip install tensorflow\" should not install most recent version of Tensorflow, but should install the specific version of Tensorflow compatible with that particular distribution of Python, and compatible with that repository.  This could be accomplished through point #2, making all subsequent versions of Tensorflow compatible with previous versions.  To illustrate, a repository coded in Python 3.7 using Tensorflow 1.4, will not be able to run on Tensorflow 1.5.  It would be wonderful if any repository made in Tensorflow 3 could run on any version of Tensorflow 3: this could be possible by keeping the syntax and commands the same across all versions of Tensorflow 3, and adding features if necessary as the versions go on.\r\n\r\n10.  I know you all are geniuses.  But the rest of us need a little help.  It might be an idea to simplify things as much as possible, and make running and installing Tensorflow so easy a child could do it.  Pytorch has been gaining traction over Tensorflow because it is just easier to use.  One helpful reddit thread: https://www.reddit.com/r/MachineLearning/comments/bo0nxh/d_what_are_you_using_tensorflow_vs_pytorch/", "comments": ["Thank you for the suggestions. Note though that some are impossible to achieve due to technical reasons (depend on multiple CUDA versions), legal reasons (automatically bundle some dependencies) or process reasons (rewriting from scratch). Furthermore, some of the suggestions don't directly apply to TF (e.g., `pip` recommendation in 9. is likely something that needs to happen at the level of PyPI, assuming it would be possible).\r\n\r\nWe don't have any plans for v3 at the moment but we can extract some of these suggestions and take them into account for that time.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n"]}, {"number": 41519, "title": "Any way to specific op name when I use saved_model", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):2.2\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nThe saved model will override the op name which I specific in graph\r\n**Describe the expected behavior**\r\nSaved model save the op names that I specific\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\\\r\n\r\nhttps://colab.research.google.com/drive/16kZkauTJwAkUZTsM4UoxtyHz2TuV9rXe\r\n\r\nThe input op name should be 'input'\r\nThe output op name should be 'test'\r\n\r\nCurrent implementation:\r\ninput op: serving_default_input\r\noutput op: PartitionedCall:0 PartitionedCall:1\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["I have tried in colab with TF version 2.2, 2.3-rc1 ,nightly versions(`2.4.0-dev20200719`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/0d7823a174667111c362fd04961ec639/untitled.ipynb#scrollTo=JsNyrzHx-rLh).Thanks!", "Was able to reproduce this issue using TF-nightly v2.6.0-dev20210526 ,please check the gist [here](https://colab.research.google.com/gist/sushreebarsa/d1b8ef57039ad2f4e2b3ed7745faa448/untitled140.ipynb?authuser=1)..Thanks !"]}, {"number": 41467, "title": "Implement LSH-based Methods for Enhanced CPU-Only Performance", "body": "I am writing to inquire if the Tensorflow team has any interest or plans for implementing locality sensitive hashing based optimizations for enhanced CPU only performance and reduced overall computational resource consumption as detailed in this paper? \r\n\r\nhttps://www.cs.rice.edu/~as143/Papers/SLIDE_MLSys.pdf\r\n\r\nIt would appear that these techniques have reached a level maturity to consider implementation and that this would be a great benefit to reducing the GPU barrier of entry for developers, reduce complexity and expense for both training and deploying large deep learning models, and also reduce waste in terms of not only hardware expense but also energy usage and ecological impact.\r\n", "comments": []}, {"number": 41451, "title": "LUP factorization (tensorflow.linalg.lu) throws \"Input is not invertible\" error.", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 20\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not tested\r\n- TensorFlow installed from (source or binary): binary. Installed tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl from https://www.wheelodex.org/projects/tensorflow/\r\n- TensorFlow version (use command below):\r\n- Python version: v2.2.0-rc4-8-g2b96f3662b\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: Cuda: 10.1.243 and cuDNN: 7.6.5\r\n- GPU model and memory: GTX 1050 (Notebook) with 4GB\r\n\r\nWhen I am trying to compute the LUP factorization of a particular type of rank deficient square matrix using tensorflow.linalg.lu, I am getting the following error regarding the input being non-invertible:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"LU_bug.py\", line 10, in <module>\r\n    lu, _ = tf.linalg.lu(A_tensor)\r\n  File \"/home/touqir/pyenvs/py3.8/lib/python3.8/site-packages/tensorflow/python/ops/gen_linalg_ops.py\", line 1183, in lu\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/home/touqir/pyenvs/py3.8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Input is not invertible. [Op:Lu]\r\n```\r\n\r\nThe following code reproduces the above error message:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nA = np.random.rand(4,6)\r\nm, n = A.shape\r\nA_tensor = np.zeros(shape=[m+n, m+n], dtype=A.dtype)\r\nA_tensor[:m, :n] = A\r\nA_tensor[-n:, -m:] = A.T\r\nA_tensor = tf.convert_to_tensor(A_tensor)\r\nlu, _ = tf.linalg.lu(A_tensor)\r\n```\r\n\r\nHowever, LUP factorization does not require the input to be a full rank matrix. ie. not invertible. I have tried with other types of rank deficient square matrices for which tensorflow's LUP factorization works fine.\r\n\r\n", "comments": ["I have tried in colab with TF versions 2.2, 2.3-rc1,nightly versions(`2.4.0-dev20200716`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/c5316f4ca70a6918564972b1a8c8830c/untitled142.ipynb).Thanks!", "@touqir14 From the [TF website](https://www.tensorflow.org/api_docs/python/tf/linalg/lu), `The input has to be invertible.` but the input is not invertible in your case. Just printed the `input`. Please let me know what you think. Thanks!\r\n\r\n```\r\ntf.Tensor(\r\n[[0.13732078 0.36229924 0.76132223 0.0201788  0.89308874 0.98938961\r\n  0.         0.         0.         0.        ]\r\n [0.024854   0.54382015 0.27550137 0.40642625 0.76440885 0.9682706\r\n  0.         0.         0.         0.        ]\r\n [0.25347575 0.13755833 0.33073567 0.05386851 0.79757004 0.56537101\r\n  0.         0.         0.         0.        ]\r\n [0.44360238 0.34834656 0.13454647 0.82118215 0.21367512 0.05590622\r\n  0.         0.         0.         0.        ]\r\n [0.         0.         0.         0.         0.         0.\r\n  0.13732078 0.024854   0.25347575 0.44360238]\r\n [0.         0.         0.         0.         0.         0.\r\n  0.36229924 0.54382015 0.13755833 0.34834656]\r\n [0.         0.         0.         0.         0.         0.\r\n  0.76132223 0.27550137 0.33073567 0.13454647]\r\n [0.         0.         0.         0.         0.         0.\r\n  0.0201788  0.40642625 0.05386851 0.82118215]\r\n [0.         0.         0.         0.         0.         0.\r\n  0.89308874 0.76440885 0.79757004 0.21367512]\r\n [0.         0.         0.         0.         0.         0.\r\n  0.98938961 0.9682706  0.56537101 0.05590622]], shape=(10, 10), dtype=float64)\r\n```", "I must have missed that line from the tensorflow website. However, there are non-invertible matrices for which the function works just fine. I have been consistently getting correct results from randomly generated rank-deficient matrices. Consider the following example:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nA = np.array([[ 0.3255562 ,  0.61857172,  3.04467256,  1.34192447],\r\n               [ 0.06105028,  0.32260879,  0.31583175,  2.40428355],\r\n               [ 0.26450592,  0.29596293,  2.72884081, -1.06235908],\r\n               [ 0.33050965,  1.15576452,  1.07663265,  1.40485265]])\r\n\r\nprint(\"Rank of A:\", np.linalg.matrix_rank(A)) # matrix rank is 3 and thus A is not invertible\r\nA_tensor = tf.convert_to_tensor(A)\r\nlu, p = tf.linalg.lu(A_tensor)\r\nl, u, p = np.tril(lu.numpy()), np.triu(lu.numpy()), p.numpy()\r\nnp.fill_diagonal(l, 1)\r\nprint(\"Successful Reconstruction: \", np.allclose(A[p] - l @ u, np.zeros((4, 4))))\r\n```\r\nI have put the above code along with the code for generating the random matrices in a [gist](https://colab.research.google.com/drive/16I20Ylx0R_lBXJRr2SmAxCKxgYLDBNaL?usp=sharing).\r\n\r\nThe reason why I am interested in the TF's LU function is that I wanted to write a GPU accelerated matrix rank computing algorithm that uses LUP factorization which can be faster than the SVD based matrix rank computing function. I was able to do so using PyTorch's LUP factorization that does not require the input matrix to be invertible nor square. Would it be possible to extend TF's LU function to also work for non-invertible matrices and also not require the matrices to be square? ", "Was able to replicate the issue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/46a5a4758e3287007779ce892681eaa2/untitled137.ipynb)..Thanks !"]}, {"number": 41448, "title": "tf.keras cannot weight classes when using multiple outputs", "body": "This post is a mirror of https://github.com/keras-team/keras/issues/11735, showing the need to handle class weight for multiple outputs.\r\n\r\nVersion 2.2.0 used.\r\n\r\n------\r\n \r\nThis is a minimal source code, by @GalAvineri, to reproduce the issue (please comment/uncomment the class weight line):\r\n\r\n````python3\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.data import Dataset\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef preprocess_sample(features, labels):\r\n    label1, label2 = labels\r\n    label1 = tf.one_hot(label1, 2)\r\n    label2 = tf.one_hot(label2, 3)\r\n    return features, (label1, label2)\r\n\r\n\r\nbatch_size = 32\r\n\r\nnum_samples = 1000\r\nnum_features = 10\r\n\r\nfeatures = np.random.rand(num_samples, num_features)\r\nlabels1 = np.random.randint(2, size=num_samples)\r\nlabels2 = np.random.randint(3, size=num_samples)\r\n\r\ntrain = Dataset.from_tensor_slices((features, (labels1, labels2))).map(preprocess_sample).batch(batch_size).repeat()\r\n\r\n# Model\r\ninputs = Input(shape=(num_features, ))\r\noutput1 = Dense(2, activation='softmax', name='output1')(inputs)\r\noutput2 = Dense(3, activation='softmax', name='output2')(inputs)\r\nmodel = Model(inputs, [output1, output2])\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\nclass_weights = {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}\r\nmodel.fit(train, epochs=10, steps_per_epoch=num_samples // batch_size,\r\n         #  class_weight=class_weights\r\n          )\r\n````\r\n\r\nUncommenting yields this error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-38-d137ff6fb3f9> in <module>\r\n     33 class_weights = {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}\r\n     34 model.fit(train, epochs=10, steps_per_epoch=num_samples // batch_size,\r\n---> 35            class_weight=class_weights\r\n     36           )\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    813           workers=workers,\r\n    814           use_multiprocessing=use_multiprocessing,\r\n--> 815           model=self)\r\n    816 \r\n    817       # Container that configures and calls `tf.keras.Callback`s.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\r\n   1115     dataset = self._adapter.get_dataset()\r\n   1116     if class_weight:\r\n-> 1117       dataset = dataset.map(_make_class_weight_map_fn(class_weight))\r\n   1118     self._inferred_steps = self._infer_steps(steps_per_epoch, dataset)\r\n   1119     self._dataset = strategy.experimental_distribute_dataset(dataset)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _make_class_weight_map_fn(class_weight)\r\n   1233         \"Expected `class_weight` to be a dict with keys from 0 to one less \"\r\n   1234         \"than the number of classes, found {}\").format(class_weight)\r\n-> 1235     raise ValueError(error_msg)\r\n   1236 \r\n   1237   class_weight_tensor = ops.convert_to_tensor_v2(\r\n\r\nValueError: Expected `class_weight` to be a dict with keys from 0 to one less than the number of classes, found {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}\r\n````", "comments": ["I am able to reproduce the issue in colab with TF versions 2.2,2.3-rc1,nightly version(`2.4.0-dev20200716`), when we uncomment the class weight line.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/fd621368c2e4ef185093d3dc1a1ced58/untitled.ipynb).Thanks!", "Has anyone found a solution to this issue? ", "Class weights with multiple outputs is important to us.  Any updates?", "+1 this is important! ", "Also stuck with this issue, any updates?", "Any progress? +1", "checking in on this as well, haven't seen any updates in a few months", "Working with TF 2.4.1, same issue. Any resolution?", "Any progress?", "Still interested in an update on this!", "Any updates? Joining the club of those who really would appreciate this being implemented", " Was able to reproduce  this using TF 2.5 version ,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/ff1c4e1eb2a33cbdc05c9601d130ba7d/untitled112.ipynb?authuser=1)..Thanks !", "I ended up solving this with a custom loss function that weights the classes the way I need. Still would appreciate having this built within Keras!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41448\">No</a>\n", "> \r\n> \r\n> Closing this issue as it is fixed in latest version of TensorFlow. Please feel free to reopen the issue if you still have a concern. Thanks!\r\n\r\nIn which version is this fixed? I'm using TF 2.5.0 and the issue is still present there.", "Same problem,  TF 2.5.0\r\n\r\nExpected `class_weight` to be a dict with keys from 0 to one less than the number of classes, found {'output_1': ...}", "Any progress on this issue?", "@sushreebarsa Can we get an update?  I note that you closed this issue stating that \"it is fixed in latest version of TensorFlow\".  However, others have noted no improvement in TF2.5. \r\nWe are doing training with imbalanced classes and have a strong need for this.\r\nCan we get an update?  Thanks!", "@maxpv  @kevinashaw  Sorry for the late response! This issue is not fixed in TF v2.5 .I tried to run the code on tf-nightly 2.7.0-dev20210810 and was able to replicate the issue as reported .Please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/592aba2b176d84464536ae8e3ba313b6/untitled373.ipynb#scrollTo=pd8xz8hOEszk) for reference . Could you please have a look at the [link1](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) , [link2](https://github.com/keras-team/keras/issues/11735) , [link3](https://datascience.stackexchange.com/questions/41698/how-to-apply-class-weight-to-a-multi-output-model) and let us know if it helps? Thank you! ", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41448\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41448\">No</a>\n", "Apparently Keras thinks this issue is fixed: https://keras.io/examples/structured_data/imbalanced_classification/", "@sushreebarsa Was this issue closed on purpose?", "Wish this issue will be resolved soon. Shouldn't it be an easy fix since it worked in version 2.1.0?", "> Apparently Keras thinks this issue is fixed: https://keras.io/examples/structured_data/imbalanced_classification/\r\n\r\nthis example uses a single output. the problem happens when using multiple output models in keras. i believe the issue is still ongoing ", "@omalleyt12 \r\nI wonder if this has to do something with what is mentioned here https://www.tensorflow.org/api_docs/python/tf/keras/Model\r\n\r\n\"If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless loss_weights is specified.\"", "Refer to this [guide](https://www.tensorflow.org/guide/keras/train_and_evaluate#passing_data_to_multi-input_multi-output_models), there are 3 weights: `class_weight`, `sample_weight`, `loss_weight`.\r\n\r\nI can set `loss_weight` in `model.compile` to weight more in **hard task**, but I also want use `class_weight` to weight more in **rare labels** in **both tasks**.\r\n\r\nEspecially when generate data, how to set `sample_weight`? A list or dictionary?\r\n\r\n```python\r\nimport tensorflow.data.Dataset as tfds\r\n\r\ncalss_weight = {\r\n  \"hard_task\": {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\r\n  \"easy_task\": {0: 1, 1: 2, 2: 3}}\r\n\r\ndef gen_date(df):\r\n  def gen():\r\n    for _, row in df.iterrows():\r\n      x = row[\"feature\"]\r\n      y = {\r\n        \"hard_task\": row[\"y1\"],\r\n        \"easy_task\": row[\"y2\"]}\r\n      # list or dict?\r\n      sample_weight1 = [calss_weight[\"hard_task\"][row[\"y1\"]],\r\n                        calss_weight[\"easy_task\"][row[\"y2\"]]]\r\n      sample_weight2 = {\r\n        \"hard_task\": calss_weight[\"hard_task\"][row[\"y1\"]],\r\n        \"easy_task\": calss_weight[\"easy_task\"][row[\"y2\"]]}\r\n      yield x, y, sample_weight\r\n  return gen\r\n\r\nds_train = tfds.from_generator(gen_data(df_train), output_signature).batch(256).prefetch(1)\r\nds_valid = tfds.from_generator(gen_data(df_valid), output_signature).batch(256).prefetch(1)\r\n\r\nloss = {\r\n  \"hard_task\": CategoricalCrossentropy(from_logits=True),\r\n  \"easy_task\": CategoricalCrossentropy(from_logits=True)}\r\nmetric = {\r\n  \"hard_task\": CategoricalAccuracy(\"accuracy\"),\r\n  \"easy_task\": CategoricalAccuracy(\"accuracy\")}\r\n\r\nmodel.compile(\r\n    optimizer=Adam(),\r\n    loss=loss,\r\n    metrics=metric,\r\n    loss_weights={\"hard_task\": 2, \"easy_task\": 1})\r\n\r\nmodel.fit(\r\n    ds_train,\r\n    validation_data=ds_valid,\r\n    calss_weight=calss_weight)\r\n```", "Has anyone got solution to this issue?", "any updates?", "Progress updates?", "We really need this fixed.  Any updates?", "Any updates on this? Thanks!"]}, {"number": 41439, "title": "Tensorflow model benchmarking with keras model file", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): TF2.2\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\nAdd keras model benchmarking in tensorflow benchmarking tool (tensorflow/tools/benchmark)\r\n\r\n**Describe the feature and the current behavior/state.**\r\n\r\nCurrently tensorflow benchmarking tool under tensorflow/tools/benchmark supports only graph files. \r\n\r\n**Will this change the current api? How?**\r\n\r\n**Who will benefit with this feature?**\r\n\r\n**Any Other info.**\r\n", "comments": []}, {"number": 41424, "title": "[MLIR] HLO to LHLO conversion and fusion", "body": "I see that recently there has been lot of work from @bondhugula for HLO to LHLO conversion. I am wondering what the plan here is w.r.t fusion. Is the fusion being done in HLO or LHLO? I remember a previous PR added fusion to XLA-HLO (using OpInterfaces), is that being used? Is the plan to do fusion in LHLO, which operates on memrefs?\r\n\r\nThe reason I ask is that in IREE we have been relying heavily on fusion at tensor level rather than fusion at buffer level. For elementwise operations this has worked fairly well so far. These two passes implement the fusion that covers a large number of elementwise-operations and their fusion\r\n1) The conversion from HLO to Linalg on tensors [here](https://github.com/tensorflow/tensorflow/blob/6e7992cae2f4af31ebc9753d7baf21ede69c39b3/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/legalize_to_linalg.cc#L905)\r\n2) Fusion of linalg operations on tensors [here](https://github.com/llvm/llvm-project/blob/4f763b2172c591ab253c8489fcd53af0c544d5cb/mlir/lib/Dialect/Linalg/Transforms/Fusion.cpp#L1025)\r\n\r\nThe latter covers fusion of generic ops, indexed generic ops and tensor reshape ops with each other. Fusion at tensor level is much easier to do, and the fusion is effectively doing a fixed-point iteration to do maximal fusion. (There might be cases where maximal fusion can be detrimental, but I haven't encountered such a case when element-wise operations are involved). I wanted to see if it would benefit the overall codegen in Tensorflow by using these passes. They aren't complete and might need enhancements to cover additional cases we haven't encountered in IREE, but so far for fusion elementwise operations, these two passes together have worked very well for us. I am trying to see if we can align codegen strategies between IREE and TF so that we can build more shared components. \r\n\r\n@stellaraccident , @sherhut , @joker-eph , @antiagainst , @nicolasvasilache for comments/visiblity.\r\n", "comments": ["Does it have to be either/or or can we have both? Maybe with a different focus at different level/layers of the stack?\r\n(at the moment I believe the only fusion pass in the MLIR-HLO land is operating on tensor)", "We also have an implementation for fusion in Linalg that works at the buffer level (in the [same file](https://github.com/llvm/llvm-project/blob/4f763b2172c591ab253c8489fcd53af0c544d5cb/mlir/lib/Dialect/Linalg/Transforms/Fusion.cpp#L392) that was linked, called `fuseLinalgOpsGreedily`). The approach there is different and I agree with @MaheshRavishankar that fusing at the tensor level often is simpler as one does not need to trace reads and writes to buffers (which is implemented as `linalg::LinalgDependenceGraph`). @pifon2a experimented with using tensor based fusion in the XLA emitter pipeline and that worked well. The main obstacle back then was the lack of bufferization support for Linalg, which has now landed in MLIR core, so we could revisit it. It is not a high priority right now, though.\r\n\r\nOne difference we have in XLA is that we have a [fusion pass](https://github.com/tensorflow/tensorflow/blob/6e7992cae2f4af31ebc9753d7baf21ede69c39b3/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/mhlo_fusion.cc) that pre-computes what operations we want to fuse, so we do not use a fix point during the actual fusion on Linalg itself. I believe that is also what the aforementioned interfaces are used for. This is modeled with a `FusionOp` node and we just fuse everything starting from the result of that node.\r\n\r\nI think we already share the core components (Linalg fusion, which is in core, HLO to Linalg, which is in the TensorFlow) and we should expand on building these out. There are some LinAlg to HLO patterns for example that would be great to move from IREE to TensorFlow. ", "Thanks @joker-eph , @sherhut  for the replies\r\n\r\n> Does it have to be either/or or can we have both? Maybe with a different focus at different level/layers of the stack?\r\n> (at the moment I believe the only fusion pass in the MLIR-HLO land is operating on tensor)\r\n\r\nWe will need some fusion at buffer level for sure. Operations that have reduction semantics cannot be represented as well in linalg on tensors (its an on-going disucssion). So fusing things like matmul/conv/reduce with other ops will probably use linalg on buffers or vector dialect. \r\n\r\n\r\n> The main obstacle back then was the lack of bufferization support for Linalg, which has now landed in MLIR core, so we could revisit it. It is not a high priority right now, though.\r\n\r\nFWIW, the conversion from HLO to Linalg and fusion parts like in TF and MLIR respectively, and just invoking two passes should give you that. Then the missing piece as you mention is the bufferization. But if I understand correctly its similar to the one being used (or being considered) for buffer allocation from HLO -> LHLO. So then using the fusion on tensors pass in Linalg should hopefully not take a lot of work. If there are any issues, we can address them cause we are using these and have an incentive to fix them.\r\n\r\n> One difference we have in XLA is that we have a [fusion pass](https://github.com/tensorflow/tensorflow/blob/6e7992cae2f4af31ebc9753d7baf21ede69c39b3/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/mhlo_fusion.cc) that pre-computes what operations we want to fuse, so we do not use a fix point during the actual fusion on Linalg itself. I believe that is also what the aforementioned interfaces are used for. This is modeled with a `FusionOp` node and we just fuse everything starting from the result of that node.\r\n\r\nNot sure how using a fixed point doesnt achieve what you want. In IREE too, the \"things to be fused\" are decided before hand. Using the fusion on tensors should give you the same thing.\r\n\r\n\r\n\r\n> I think we already share the core components (Linalg fusion, which is in core, HLO to Linalg, which is in the TensorFlow) and we should expand on building these out. There are some LinAlg to HLO patterns for example that would be great to move from IREE to TensorFlow.\r\n\r\nThis has been a source of confusion for a long time. The only ones we have in IREE go from HLO -> Linalg on buffers. The reason they are in IREE is cause AFAICS, the TF codegen strategy is to go from HLO -> LHLO -> Linalg. That diverges a bit. IREE uses HLO as an \"input\" IR for now. All other dialects used in IREE either live in IREE or in MLIR. Is there something specific you are looking at that you think we could port from IREE to TF which is outside of the HLO -> Linalg on buffers conversion.", "On Fri, Jul 17, 2020 at 11:25 AM MaheshRavishankar <notifications@github.com>\nwrote:\n\n> Thanks @joker-eph <https://github.com/joker-eph> , @sherhut\n> <https://github.com/sherhut> for the replies\n>\n> Does it have to be either/or or can we have both? Maybe with a different\n> focus at different level/layers of the stack?\n> (at the moment I believe the only fusion pass in the MLIR-HLO land is\n> operating on tensor)\n>\n> We will need some fusion at buffer level for sure. Operations that have\n> reduction semantics cannot be represented as well in linalg on tensors (its\n> an on-going disucssion). So fusing things like matmul/conv/reduce with\n> other ops will probably use linalg on buffers or vector dialect.\n>\n> The main obstacle back then was the lack of bufferization support for\n> Linalg, which has now landed in MLIR core, so we could revisit it. It is\n> not a high priority right now, though.\n>\n> FWIW, the conversion from HLO to Linalg and fusion parts like in TF and\n> MLIR respectively, and just invoking two passes should give you that. Then\n> the missing piece as you mention is the bufferization. But if I understand\n> correctly its similar to the one being used (or being considered) for\n> buffer allocation from HLO -> LHLO. So then using the fusion on tensors\n> pass in Linalg should hopefully not take a lot of work. If there are any\n> issues, we can address them cause we are using these and have an incentive\n> to fix them.\n>\n> One difference we have in XLA is that we have a fusion pass\n> <https://github.com/tensorflow/tensorflow/blob/6e7992cae2f4af31ebc9753d7baf21ede69c39b3/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/mhlo_fusion.cc>\n> that pre-computes what operations we want to fuse, so we do not use a fix\n> point during the actual fusion on Linalg itself. I believe that is also\n> what the aforementioned interfaces are used for. This is modeled with a\n> FusionOp node and we just fuse everything starting from the result of\n> that node.\n>\n> Not sure how using a fixed point doesnt achieve what you want. In IREE\n> too, the \"things to be fused\" are decided before hand. Using the fusion on\n> tensors should give you the same thing.\n>\n> I think we already share the core components (Linalg fusion, which is in\n> core, HLO to Linalg, which is in the TensorFlow) and we should expand on\n> building these out. There are some LinAlg to HLO patterns for example that\n> would be great to move from IREE to TensorFlow.\n>\n> This has been a source of confusion for a long time. The only ones we have\n> in IREE go from HLO -> Linalg on buffers. The reason they are in IREE is\n> cause AFAICS, the TF codegen strategy is to go from HLO -> LHLO -> Linalg.\n>\nI still don't understand, so the confusion isn't over yet. There is not a\n\"single unique codegen strategy in TF\" and I don't understand\nwhy HLO->Linalg should be outside of TF right now.\n\nRef: https://github.com/google/iree/issues/2011#issuecomment-633283866\n\n-- \nMehdi\n\n\n\n> That diverges a bit. IREE uses HLO as an \"input\" IR for now. All other\n> dialects used in IREE either live in IREE or in MLIR. Is there something\n> specific you are looking at that you think we could port from IREE to TF\n> which is outside of the HLO -> Linalg on buffers conversion.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/41424#issuecomment-660269811>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AAZXKDDA6RQ2RVW6R635OXLR4CJRHANCNFSM4O2WVSKA>\n> .\n>\n", "> > The main obstacle back then was the lack of bufferization support for Linalg, which has now landed in MLIR core, so we could revisit it. It is not a high priority right now, though.\r\n> \r\n> FWIW, the conversion from HLO to Linalg and fusion parts like in TF and MLIR respectively, and just invoking two passes should give you that. Then the missing piece as you mention is the bufferization. But if I understand correctly its similar to the one being used (or being considered) for buffer allocation from HLO -> LHLO. So then using the fusion on tensors pass in Linalg should hopefully not take a lot of work. If there are any issues, we can address them cause we are using these and have an incentive to fix them.\r\n\r\nI meat to say that it **was** an obstacle and the generic buffer allocation support in MLIR could handle it now. It simply was not a priority to get done but that might change once this code gets more use again.\r\n \r\n> > One difference we have in XLA is that we have a [fusion pass](https://github.com/tensorflow/tensorflow/blob/6e7992cae2f4af31ebc9753d7baf21ede69c39b3/tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/transforms/mhlo_fusion.cc) that pre-computes what operations we want to fuse, so we do not use a fix point during the actual fusion on Linalg itself. I believe that is also what the aforementioned interfaces are used for. This is modeled with a `FusionOp` node and we just fuse everything starting from the result of that node.\r\n> \r\n> Not sure how using a fixed point doesnt achieve what you want. In IREE too, the \"things to be fused\" are decided before hand. Using the fusion on tensors should give you the same thing.\r\n\r\nI am not saying that a fix point does not achieve it. I just tried to explain what the fusion pass that landed recently is about and why these interfaces were added. That is independent of whether we fuse on tensors or buffers and certainly not an argument against fusion on tensors. In fact, I am not arguing against it at all, just providing context. \r\n\r\n> > I think we already share the core components (Linalg fusion, which is in core, HLO to Linalg, which is in the TensorFlow) and we should expand on building these out. There are some LinAlg to HLO patterns for example that would be great to move from IREE to TensorFlow.\r\n> \r\n> This has been a source of confusion for a long time. The only ones we have in IREE go from HLO -> Linalg on buffers. The reason they are in IREE is cause AFAICS, the TF codegen strategy is to go from HLO -> LHLO -> Linalg. That diverges a bit. IREE uses HLO as an \"input\" IR for now. All other dialects used in IREE either live in IREE or in MLIR. Is there something specific you are looking at that you think we could port from IREE to TF which is outside of the HLO -> Linalg on buffers conversion.\r\n\r\nI wondered about [these patterns](https://github.com/google/iree/blob/main/iree/compiler/Conversion/HLOToLinalg/HLOToLinalgOnBuffers.cpp). They go from HLO to LinAlg and could hence be placed in TF/HLO repos."]}, {"number": 41418, "title": "Built-in Method to check if a GPU is already Reserved", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): tf-nightly==2.3.0dev20200622 & tf-nightly-gpu==2.3.0dev20200622\r\n- Are you willing to contribute it (Yes/No): No - this is far beyond my current abilities\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nThe idea is to have a TF 2.X compatible function that checks to see which GPUs/Resources that are actually available for computation as opposed to simply addressable. An example of this would be running one python script that assigns only 1 gpu out of 2 available gpus, and then another python script, in a completely separate instance, would call said function to check which of the 2 gpus is currently being used and which is available to still be used.\r\n\r\n**Will this change the current api? How?**\r\nIt would add a new function/method to a module. It shouldn't need to modify currently existing methods.\r\n**Who will benefit with this feature?**\r\nAnyone who wants to run multiple instances of a training script without having to explicitly program which GPU/CPU/TPU to use. There are certainly ways to do this task, but they require the usage of either custom message passing, bash scripted resource management, or an external python script that uses multiprocessing or subprocess packages. Having something that can give a status update on truly available resources would be useful.\r\n\r\n**Additional Info**\r\nIt is possible that this is done behind the scenes in tensorflow already. If it is, it is not clearly reflected in Tensorflow documentation. However, it would appear that it isn't done behind the scenes because I have network and dataset that causes core dumps on a 2 gpu system if I run two instances but succeeds if I run only one instance. As far as I can tell it's because the GPUs are running out of memory.", "comments": ["We can probably base this off of the amount of GPU memory allocated -- if only a small fraction of the GPU memory is left we can (practically) assume that the GPU is \"reserved\" by another TF process."]}, {"number": 41405, "title": "Error: #include nested too deeply (libtensorflow_cc.so)", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0-rc1\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): 7.5.0\r\n\r\n**Describe the problem**\r\nThe problem occurs when using libtensorflow_cc.so. When including <tensorflow/core/public/session.h> in the main file of my program that uses libtensorflow_cc.so, it includes <tensorflow/core/framework/tensor.h>, which in turn includes <unsupported/Eigen/CXX11/Tensor>. This file includes itself and results in the error #include nested too deeply.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n`#include <tensorflow/core/public/session.h>` in the main file of my program that uses libtensorflow_cc.so.\r\n\r\n**Any other info / logs**\r\n\r\n```\r\nIn file included from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:0,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/tensorflow/core/framework/tensor.h:22,\r\n                 from /media/ssd512/build/tensorflow-prefix/src/tensorflow/tensorflow/core/public/session.h:24,\r\n                 from /media/ssd512/src/main.cpp:2:\r\n/media/ssd512/build/tensorflow-prefix/src/tensorflow/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1:42: error: #include nested too deeply\r\n #include \"unsupported/Eigen/CXX11/Tensor\"\r\n```", "comments": ["@Lotte1990 \r\n\r\nProvide the exact sequence of commands / steps that you executed before running into the problem.Thanks!", "A minimal example to reproduce this error would be the following:\r\n\r\nBuild libtensorflow_cc.so\r\n```\r\nsudo apt-get install python3 python3-pip\r\npip3 install numpy\r\n\r\npython3 configure.py\r\n    empty\r\n    empty\r\n    n\r\n    n\r\n    n\r\n    n\r\n    empty\r\n    n\r\n\r\nbazel build -c opt --config=mkl //tensorflow:libtensorflow_cc.so\r\n```\r\n\r\nCreate new project with the following CMakeLists.txt\r\n```\r\ncmake_minimum_required(VERSION 3.11 FATAL_ERROR)\r\n\r\ninclude(ExternalProject)\r\n\r\nset(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"Choose the type of build, options are: None Debug Release RelWithDebInfo MinSizeRel ...\" FORCE)\r\n\r\nproject(tfinference)\r\n\r\nexternalproject_add(protobuf GIT_REPOSITORY https://github.com/protocolbuffers/protobuf.git GIT_TAG v3.9.2 CONFIGURE_COMMAND \"\" BUILD_COMMAND \"\" INSTALL_COMMAND \"\")\r\nexternalproject_get_property(protobuf SOURCE_DIR)\r\nset(srcdir_protobuf ${SOURCE_DIR})\r\ninclude_directories(${srcdir_protobuf}/src)\r\n\r\nexternalproject_add(tensorflow GIT_REPOSITORY https://github.com/tensorflow/tensorflow.git GIT_TAG v2.3.0-rc1 CONFIGURE_COMMAND \"\" BUILD_COMMAND \"\" INSTALL_COMMAND \"\")\r\nexternalproject_get_property(tensorflow SOURCE_DIR)\r\nset(srcdir_tensorflow ${SOURCE_DIR})\r\ninclude_directories(${srcdir_tensorflow})\r\ninclude_directories(${srcdir_tensorflow}/third_party/eigen3)\r\n\r\ninclude_directories(/opt/tensorflow-2.3.0-rc1/bazel-bin)\r\n\r\nadd_executable(tfinference main.cpp)\r\n\r\nadd_dependencies(tfinference protobuf)\r\nadd_dependencies(tfinference tensorflow)\r\n\r\ntarget_link_libraries(tfinference ${CMAKE_SOURCE_DIR}/libtensorflow_cc.so)\r\n```\r\n\r\nAnd main.cpp\r\n```\r\n#include <tensorflow/core/public/session.h>\r\n\r\nint main()\r\n{\r\n    std::cout << \"Test...\" << std::endl;\r\n    return 0;\r\n}\r\n```\r\n\r\nThen build the project\r\n`make -j`", "@rmlarsen @av8ramit ", "Also @ezhulenev ", "@gunan @rmlarsen @av8ramit @ezhulenev Any updates on this?", "Unfortunately, I'm not sure, maybe @rmlarsen may have more insight. ", "I had same problem with TF2!\r\nAnyone can help?", "Sorry, but I personally have not had cycles to look into libtensorflow_cc. I'll see if I can carve time in Q4. ", "I see the same issue when using vim YCM completion as of tensorflow git 108aa90714c8369a2392fba43fa62c2d0cec523a:\r\n```\r\nIn included file: #include nested too deeply /home/uday/tensorflow/third_party/eigen3/Eigen/Core:1:10: note: error occurred here [pp_include_too_deep]\r\n```\r\nIn particular, `third_party/eigen3/Eigen/Core` has:\r\n```\r\n#include \"Eigen/Core\"\r\n```\r\nThis doesn't make sense(?) - it's a recursive include and it's anyway missing a header guard. At the very least, it's missing a comment on the intention. \r\n", "Same issue here using tensorflow 2.7 branch compiled from sources.\r\nThe issue rises when trying to use CMAKE."]}, {"number": 41392, "title": "Debugging predictions step with tf.debugging.experimental.enable_dump_debug_info throws an error.", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I slightly modified a colab notebook from tf ranking package to demonstrate the issue. The modified notebook is  https://colab.research.google.com/drive/1FVkkYCo_ZtN6mu-kmF1IqDYI4C1-jxaw?usp=sharing \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n**Describe the current behavior**\r\nI get error while trying to log debug information with [tf.debugging.experimental.enable_dump_debug_info](https://www.tensorflow.org/api_docs/python/tf/debugging/experimental/enable_dump_debug_info) .\r\nThis is the only change to the notebook is that I added a cell with `tf.debugging.experimental.enable_dump_debug_info(\"/tmp/debug\")` right before calling predictions (in the end of the notebook).\r\nThe error is \r\n```\r\nINFO:tensorflow:vocabulary_size = 30522 in document_tokens is inferred from the number of elements in the vocabulary_file /tmp/vocab.txt.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-33-81eaf5e40876> in <module>()\r\n----> 1 x = next(predictions)\r\n      2 assert(len(x) == _LIST_SIZE)  ## Note that this includes padding.\r\n\r\n13 frames\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/normalization.py in build(self, input_shape)\r\n    374       if axis_to_dim[x] is None:\r\n    375         raise ValueError('Input has undefined `axis` dimension. Input shape: ',\r\n--> 376                          input_shape)\r\n    377     self.input_spec = InputSpec(ndim=ndims, axes=axis_to_dim)\r\n    378 \r\n\r\nValueError: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([None, None]))\r\n```\r\n**Describe the expected behavior**\r\nI expect to get debugging information while calling predictions.\r\n\r\n**Standalone code to reproduce the issue**\r\nTo reproduce run this notebook https://colab.research.google.com/drive/1FVkkYCo_ZtN6mu-kmF1IqDYI4C1-jxaw?usp=sharing \r\n", "comments": ["@mshavlovsky \r\nCan you please share simple stand alone indented code such that we can replicate the issue faced, the code shared is too large to replicate.\r\n\r\nsimilar issues:\r\n[link](https://stackoverflow.com/questions/43480732/none-dimension-raise-valueerror-in-batch-norm-with-tensorflow )[link1](https://github.com/tensorflow/tensorflow/issues/14809) #31751 ", "@Saduf2019 \r\nI simplified the [notebook](https://colab.research.google.com/drive/1FVkkYCo_ZtN6mu-kmF1IqDYI4C1-jxaw?usp=sharing) to have only two cells. The first one doing the necessary job from installing TF to training the model . The second cell is where the error appears. It is where I enable debugging and call `predict`.\r\n```\r\ntf.debugging.experimental.enable_dump_debug_info(\"/tmp/debug\")\r\n\r\npredictions = ranker.predict(input_fn=lambda: predict_input_fn(\"/tmp/test.tfrecords\"))\r\nx = next(predictions)\r\n```\r\n Note that the fist cell is large, but it takes about five to ten minutes to complete. Its content is taken from the [tfranking colab tutorial](https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb).", "@mshavlovsky Can you try the following\r\n\r\n`tf.debugging.experimental.enable_dump_debug_info(\"/tmp/debug\", tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)`\r\n\r\nThe `tensor_debug_mode` specifies a mode that gives more information compared to the default `\"NO_TENSOR\"` mode; it'll also probably bypass the problem you are experiencing. The `circular_buffer_size=-1` lets the debugger dump all tensor info, instead of only the most recent 1000.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/lib/dumping_callback.py#L747 for more info about the two arguments. \r\n\r\nLet me know if this resolves your issue.", "@caisq Thanks for looking into the problem. The following code still returns an error\r\n```\r\ntf.debugging.experimental.enable_dump_debug_info(\"/tmp/debug\", tensor_debug_mode=\"FULL_HEALTH\", circular_buffer_size=-1)\r\n\r\npredictions = ranker.predict(input_fn=lambda: predict_input_fn(\"/tmp/test.tfrecords\"))\r\nx = next(predictions)\r\n```\r\nThe output is:\r\n\r\n\r\n> INFO:tensorflow:Done calling model_fn.\r\n> ---------------------------------------------------------------------------\r\n> TypeError                                 Traceback (most recent call last)\r\n> <ipython-input-2-b971928e3770> in <module>()\r\n>       3 \r\n>       4 predictions = ranker.predict(input_fn=lambda: predict_input_fn(\"/tmp/test.tfrecords\"))\r\n> ----> 5 x = next(predictions)\r\n> \r\n> 14 frames\r\n> /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saving/saveable_object_util.py in op_list_to_dict(op_list, convert_variable_to_tensor)\r\n>     290             var = ops.convert_to_tensor(var, as_ref=True)\r\n>     291           if not _tensor_comes_from_variable(var):\r\n> --> 292             raise TypeError(\"Variable to save is not a Variable: %s\" % var)\r\n>     293         if var.op.type == \"ReadVariableOp\":\r\n>     294           name = var.op.inputs[0].op.name\r\n> \r\n> TypeError: Variable to save is not a Variable: Tensor(\"encoding_layer/document_tokens_embedding/embedding_weights/Read/ReadVariableOp/Identity:0\", shape=(30522, 20), dtype=float32)", "Yep. The new error \"Variable to save is not a Variable\" is related to a known issue that the `enable_dump_debug_info()` API is incompatible with checkpoint saving. Can you disable saving in your program during debugging and try again?", "After disabling checkpoint saving, I still get the \"Variable to save is not a Variable\" error. To disable checkpoint saving, I use the following RunConfig\r\n```\r\n  run_config = tf.estimator.RunConfig(\r\n      save_checkpoints_steps=None,\r\n      save_checkpoints_secs=None,\r\n      save_summary_steps=None,\r\n      keep_checkpoint_every_n_hours=None)\r\n```\r\n\r\nAfter training, the model's directory does not have `ckpt` files. Checking the model's directory\r\n`! ls \"/tmp/ranking_model_dir\"` yields \r\n\r\n> events.out.tfevents.1595773363.3ec7490ced7d\r\n\r\nCan it be that checkpoint saving is not properly disabled ? Or the issue still persists?", "@caisq Any update on the issue? The problem still persists after disabling checkpoint saving. Thank you.", "Was able to replicate the isuue in TF 2.6.0-dev20210530,please find the gist [here](https://colab.research.google.com/gist/sushreebarsa/eb00193a698ade7a8799a99610ae5c9e/untitled136.ipynb)..Thanks !"]}, {"number": 41380, "title": "Windows - tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ", "body": "**System information**\r\n\r\nOS Name:                   Microsoft Windows 10 Enterprise\r\nOS Version:                10.0.17763 N/A Build 17763\r\nTensorFlow installed using 'conda'.\r\ntensorflow v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\nPython 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 15:18:16) [MSC v.1916 64 bit (AMD64)] on win32\r\n\r\n**Describe the current behavior**\r\n\r\nSaving checkpoint files from tensorflow is failing on Windows 10.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Git\\<redacted>\\tests\\integration\\validate_train_model.py\", line 216, in <module>\r\n    main()\r\n  File \"C:\\Git\\<redacted>\\tests\\integration\\validate_train_model.py\", line 176, in main\r\n    fig_save_freq = fig_save_freq)\r\n  File \"c:\\git\\<redacted>\\src\\pointnet\\model.py\", line 640, in fit\r\n    self.save_best_model()\r\n  File \"c:\\git\\<redacted>\\src\\pointnet\\model.py\", line 493, in save_best_model\r\n    check_interval = False)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 823, in save\r\n    self._record_state()\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 728, in _record_state\r\n    save_relative_paths=True)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 248, in update_checkpoint_state_internal\r\n    text_format.MessageToString(ckpt))\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 532, in atomic_write_string_to_file\r\n    rename(temp_pathname, filename, overwrite)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 491, in rename\r\n    rename_v2(oldname, newname, overwrite)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 508, in rename_v2\r\n    compat.as_bytes(src), compat.as_bytes(dst), overwrite)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: tests\\files\\checkpoints\\0000_00_00_00_00_00\\checkpoint.tmpc6ee5d6bc5a445c884bba8c3acadf01f to: tests\\files\\checkpoints\\0000_00_00_00_00_00\\checkpoint : Access is denied.\r\n; Input/output error\r\n```\r\n\r\nProblem traced to:\r\ntensorflow.python.lib.io.file_io, line 532, function atomic_write_string_to_file\r\n\r\nFrom debugging, tensorflow attempts to create, then overwrite a file while saving a checkpoint.  For some reason, the 'overwrite' parameter, although set to True, does nothing.  This causes the rename to fail (since the file seems to get created earlier in the checkpoint save process).\r\n\r\nWe tried deleting the 'checkpoint' file before the 'save', but the checkpoint file that it's trying to overwrite appears to be created as a part of the 'save' call.\r\n\r\nI was able to get checkpoint saving working again by modifying atomic_write_string_to_file as follows.  My change checks for existence of the rename target and deletes it using os.remove if overwrite is True, rather than relying on the tensorflow custom machinery that doesn't seem to be working:\r\n\r\n```python\r\ndef atomic_write_string_to_file(filename, contents, overwrite=True):\r\n  if not has_atomic_move(filename):\r\n    write_string_to_file(filename, contents)\r\n  else:\r\n    temp_pathname = filename + \".tmp\" + uuid.uuid4().hex\r\n    write_string_to_file(temp_pathname, contents)\r\n    try:\r\n      if overwrite and os.path.exists(filename):\r\n        os.remove(filename)\r\n      rename(temp_pathname, filename, overwrite)\r\n    except errors.OpError:\r\n      delete_file(temp_pathname)\r\n      raise\r\n```\r\n\r\nThe stack trace we got suggested that this is the same issue as someone was reporting for tensorflow.models:\r\nhttps://github.com/tensorflow/models/issues/4177\r\n\r\n**Describe the expected behavior**\r\n\r\nWe should be able to successfully save a checkpoint on Windows 10.\r\n", "comments": ["@dtmaidenmueller \r\nI ran the code shared by you and do not face any error,please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e057d5a0f6e9cbcb82385b1f61e57768/untitled278.ipynb). Please share reproducible code or a gist with the errror for us to analyse.\r\n\r\nsimilar issues:\r\n[link](https://stackoverflow.com/questions/41116270/tensorflow-windows-accessing-folders-deniednewrandomaccessfile-failed-to-creat/41867803) [link1](https://stackoverflow.com/questions/53327026/tensorflow-access-denied-in-config-util-py-while-training) ", "The code snippet I shared shows the changes that I made to _fix_ the problem (by adding the call to os.remove), not the non-working version.", "The core of the problem is that there are instances where your custom tensorflow filesystem code is not able to remove / overwrite a file, but the os.remove function is able to remove it.", "Another note: we are seeing this on our Windows 10 systems, but not on Linux VMs.", "Hi! I'm facing the same error described above. In the next days I will test @dtmaidenmueller 's fix and will report whether it worked or not.\r\n\r\nNotice that I am using KerasTuner to apply Hyperband hyperparameter optimization, hence thousands of trials will be performed... So it might take a while. Also, I want to check if TensorBoard has anything to do with the issue:  tensorflow/tensorboard#892", "@dtmaidenmueller Looks like you are using a 32 bit system. Tensorflow is not supported on 32 bit systems. May be this might be the cause of this error.", "It's 64-bit Python:\r\n```batch\r\n>python\r\nPython 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 15:18:16) [MSC v.1916 64 bit (AMD64)] on win32\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import platform\r\n>>> platform.architecture()\r\n('64bit', 'WindowsPE')\r\n>>> import struct\r\n>>> struct.calcsize('P')*8\r\n64\r\n>>>\r\n```", "A quick update: I tried @dtmaidenmueller 's solution, and it worked. KerasTuner wrote over 2.000 models without prompting any error. I have to ensure that the algorithm is not showing unwanted behaviour though; the result was kinda weird, and during other runs prior to changing the file_io.py it took over 6.000 models written for the error to prompt.\r\n\r\nI'll try now deactivating TensorBoard and using original function and perhaps a bigger search so that the number of models created is pushed up. ", "I have confirmed that TensorBoard has nothing to do with this kind of error. I would greatly appreciate if someone else could test the solution proposed. It seems to work fine after a couple of 8 hours runs, but further testing is needed.", "Is there anyone who works on tensorflow.python.io who could comment on why, sometimes, the 'overwrite' flag for 'rename' from that subpackage does nothing?  It would be cleaner to fix the underlying code than to proceed with my workaround.", "Unfortunately @dtmaidenmueller workaround did not work for me. I am also using Windows 10 Enterprise, with **Python 3.8.5**, **Tensorflow 2.3.0** and **Keras-Tuner 1.0.1**. I am also saving the results for visualization on TensorBoard. Tensorflow was installed without conda.\r\nThe error started to appear only when I increased the number of maximum trials for the tuner from 30 to 150 (and above) and changed the python _script_ I was using to call the keras-tuner to a _function_, which is now called by another python _script_. \r\n\r\n```\r\n  File \"...Python38\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\", line 130, in search \r\n    self.run_trial(trial, *fit_args, **fit_kwargs) \r\n  File \"...Python38\\lib\\site-packages\\kerastuner\\engine\\multi_execution_tuner.py\", line 96, in run_trial \r\n    history = model.fit(*fit_args, **copied_fit_kwargs) \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 108, in _method_wrapper \r\n    return method(self, *args, **kwargs) \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 1137, in fit \r\n    callbacks.on_epoch_end(epoch, epoch_logs) \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 412, in on_epoch_end \r\n    callback.on_epoch_end(epoch, logs) \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1249, in on_epoch_end \r\n    self._save_model(epoch=epoch, logs=logs) \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\", line 1298, in _save_model \r\n    self.model.save_weights( \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\", line 2101, in save_weights \r\n    self._trackable_saver.save(filepath, session=session, options=options) \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\", line 1199, in save \r\n    save_path, new_feed_additions = self._save_cached_when_graph_building( \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\", line 1145, in _save_cached_when_graph_building \r\n    save_op = saver.save(file_prefix, options=options) \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\", line 295, in save \r\n    return save_fn() \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\training\\saving\\functional_saver.py\", line 281, in save_fn \r\n    return gen_io_ops.merge_v2_checkpoints( \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 504, in merge_v2_checkpoints \r\n    return merge_v2_checkpoints_eager_fallback( \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 529, in merge_v2_checkpoints_eager_fallback \r\n    _result = _execute.execute(b\"MergeV2Checkpoints\", 0, inputs=_inputs_flat, \r\n  File \"...Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 59, in quick_execute \r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, \r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: optm_python\\bayes\\trial_18985bf723f3fa00697399e0ad7712cc\\checkpoints\\epoch_0\\checkpoint_temp_c52e87337ed14d0dac98560fcadb2092/part-00000-of-00001.data-00000-of-00001 to: optm_python\\bayes\\trial_18985bf723f3fa00697399e0ad7712cc\\checkpoints\\epoch_0\\checkpoint.data-00000-of-00001 : Access is denied  \r\n; Input/output error [Op:MergeV2Checkpoints] \r\n```", "I am getting this same error when trying to train a custom object detection model using TF2:\r\n\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: training\\checkpoint.tmpa7a5285bf7fa4fc1861942fc88f3e099 to: training\\checkpoint : Access is denied.\r\n; Input/output error\r\n\r\n\r\n", "I am getting this error as well. \r\n\r\nStackoverflow thread: https://stackoverflow.com/questions/65461750/tensorflow-python-framework-errors-impl-unknownerror-failed-to-rename-access\r\n\r\n**Code:**\r\n\r\n```\r\nimport tensorflow_datasets as tfds\r\ndatasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\r\n```\r\n**Output**\r\n\r\n```\r\nWriting...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\nShuffling...:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 18/20 [00:01<00:00, 14.15 shard/s]\r\nReading...: 0 examples [00:00, ? examples/s]\r\n                                            \r\nWriting...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\n                                                           \r\nReading...: 0 examples [00:00, ? examples/s]\r\n                                            \r\nWriting...:   0%|          | 0/2500 [00:00<?, ? examples/s]\r\nTraceback (most recent call last):\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-3b586bfe81d7>\", line 3, in <module>\r\n    datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py\", line 300, in load\r\n    dbuilder.download_and_prepare(**download_and_prepare_kwargs)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py\", line 52, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\", line 307, in download_and_prepare\r\n    self.info.write_to_directory(self._data_dir)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\contextlib.py\", line 119, in __exit__\r\n    next(self.gen)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py\", line 200, in incomplete_dir\r\n    tf.io.gfile.rename(tmp_dir, dirname)\r\n  File \"C:\\Anaconda3\\envs\\ml_tf\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 546, in rename_v2\r\n    compat.as_bytes(src), compat.as_bytes(dst), overwrite)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: C:\\Users\\User\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0.incomplete5JQVCL to: C:\\Users\\User\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0 : Access is denied.\r\n; Input/output error\r\n\r\n\r\n```\r\n\r\nI am on Windows 10; TF 2.3; Python 3.7.9; \r\n\r\n**HEre's conda list**\r\n\r\n```\r\nsqlite                    3.33.0               h2a8f88b_0\r\ntensorboard               2.3.0              pyh4dce500_0\r\ntensorboard-plugin-wit    1.6.0                      py_0\r\ntensorflow                2.3.0           mkl_py37h3bad0a6_0\r\ntensorflow-base           2.3.0           eigen_py37h17acbac_0\r\ntensorflow-datasets       1.2.0                    py37_0\r\ntensorflow-estimator      2.3.0              pyheb71bc4_0\r\ntensorflow-metadata       0.14.0             pyhe6710b0_1\r\ntensorflow-mkl            2.3.0                h93d2e19_0\r\n\r\n```\r\n\r\nCan someone please help?", "I am also getting this error on: Windows 10; TF 2.3; Python 3.7\r\n\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: path\\trial_5a095c02600a30dc086a9efe046b1272\\checkpoints\\epoch_0\\checkpoint_temp/part-00000-of-00001.data-00000-of-00001 to: path\\trial_5a095c02600a30dc086a9efe046b1272\\checkpoints\\epoch_0\\checkpoint.data-00000-of-00001 : Access is denied. \r\n; Input/output error [Op:MergeV2Checkpoints]\r\n", "> I am also getting this error on: Windows 10; TF 2.3; Python 3.7\r\n> \r\n> tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: path\\trial_5a095c02600a30dc086a9efe046b1272\\checkpoints\\epoch_0\\checkpoint_temp/part-00000-of-00001.data-00000-of-00001 to: path\\trial_5a095c02600a30dc086a9efe046b1272\\checkpoints\\epoch_0\\checkpoint.data-00000-of-00001 : Access is denied.\r\n> ; Input/output error [Op:MergeV2Checkpoints]\r\n\r\nAre you Running another Process while Learning Like Eval for Example ", "I can confirm problem still exists for both tensorflow 2.3 and 2.4 in Windows. I have tried all recommended solutions, including modifying the atomic_write_string_to_file function as described on top, specifying different folders for checkpoint and save, shutting down antivirus and all cloud back up services etc. But still ran into \"failed to rename error\" repeatedly in normal tensorflow model training. \r\n\r\nI guess it's time for Linux? WSL2 is premature and don't have multiple GPU support yet. I feel that Windows is just not very loved.  ", " Failed to rename: path\\trial_5a095c02600a30dc086a9efe046b1272\\checkpoints\\epoch_0\\checkpoint_temp/part-00000-of-00001.data-00000-of-00001 to: path\\trial_5a095c02600a30dc086a9efe046b1272\\checkpoints\\epoch_0\\checkpoint.data-00000-of-00001\r\n \r\n try to make this path is shorter less than 255 character ", "I have made the switch to **Ubuntu 20.04. Tensorflow 2.4 is running like a dream**. No errors, No Failed to Rename surprises to torture you. No nasty exceptions when you least expect it. As Stevo once said: \"It Just Works!\".\r\nLooks like this is a Windows-related issue. \r\n**AND My dual GPU setup can finally train together - NCCL goodness.** ", "**checkpoint_path = r\"\\training_1\\cp-lowest-loss.ckpt\"\r\ncheckpoint_dir = os.path.dirname(checkpoint_path)**\r\n\r\nMake sure your checkpoint path contains the name of the .ckpt files and the error got resolved for me", "> tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: training\\checkpoint.tmpa7a5285bf7fa4fc1861942fc88f3e099 to: training\\checkpoint : Access is denied.\r\n> ; Input/output error\r\n\r\n\"training\\checkpoint.tmpa7a5285bf7fa4fc1861942fc88f3e099\"\r\n\"training\\checkpoint\"\r\nHad problem with this either. Directory \"training\\checkpoint\" existed so it was impossible to rename any file to this name anymore.", "> > tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: training\\checkpoint.tmpa7a5285bf7fa4fc1861942fc88f3e099 to: training\\checkpoint : Access is denied.\r\n> > ; Input/output error\r\n> \r\n> \"training\\checkpoint.tmpa7a5285bf7fa4fc1861942fc88f3e099\"\r\n> \"training\\checkpoint\"\r\n> Had problem with this either. Directory \"training\\checkpoint\" existed so it was impossible to rename any file to this name anymore.\r\n\r\nWhat did you do then to solve it then? any suggestion", "> \r\n> \r\n> > > tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: training\\checkpoint.tmpa7a5285bf7fa4fc1861942fc88f3e099 to: training\\checkpoint : Access is denied.\r\n> > > ; Input/output error\r\n> > \r\n> > \r\n> > \"training\\checkpoint.tmpa7a5285bf7fa4fc1861942fc88f3e099\"\r\n> > \"training\\checkpoint\"\r\n> > Had problem with this either. Directory \"training\\checkpoint\" existed so it was impossible to rename any file to this name anymore.\r\n> \r\n> What did you do then to solve it then? any suggestion\r\n\r\nJust rename \"checkpoint\" directory to \"cp\" for example (remember to change paths in pipeline.config).", "I am actively experiencing this issue on Windows 10 as well. If you delete the folders from scratch each time you still get the error.\r\n![Capture](https://user-images.githubusercontent.com/34655934/116759747-f477a580-a9c7-11eb-814a-38ed8c48f725.JPG)\r\n", "Actually, I think I found an explanation on stack overflow that led to a solution for me ( https://stackoverflow.com/questions/41365318/access-is-denied-when-renaming-folder ). \r\n\r\nBasically the python script can't rename folders in windows if the target directory is the folder the process is running in, or a sub-folder of that folder. I changed my training folder to a c:/Temp/Training, and now it's not in the script's directory path at all. \r\n\r\nThis solution works for me, no more access denied errors in Windows 10.\r\n\r\n", "> Actually, I think I found an explanation on stack overflow that led to a solution for me ( https://stackoverflow.com/questions/41365318/access-is-denied-when-renaming-folder ).\r\n> \r\n> Basically the python script can't rename folders in windows if the target directory is the folder the process is running in, or a sub-folder of that folder. I changed my training folder to a c:/Temp/Training, and now it's not in the script's directory path at all.\r\n> \r\n> This solution works for me, no more access denied errors in Windows 10.\r\n\r\nI was having the same access denied issue.  I followed this advice and it solved the issue for me with a caveat.  I couldn't use /Temp for some reason (was getting permission denied).  But when I used /ProgramData/PythonTraining/my_checkpoint all errors went away.\r\n\r\nps: note I had long file names enabled in registry as well prior to this and it did not help. ", "I had this issue and moving my code and data outside of Dropbox directory solved the problem. (This didn't used to happen with Dropbox, but now it does).", "I am following the tutorial on https://www.tensorflow.org/tutorials/keras/keras_tuner and am getting this error message. I upgraded TF to 2.5.0, using Python 3.7 in Windows 10 and made the amendment recommended by @dtmaidenmueller, this didn't work for me. However, @borgarpa linked a thread in which I found a solution which worked, which was to close the file explorer. I have seen other posts where people found saving outside the current directory worked as well. This seems a bit clunky, are there any plans to fix this for Windows users? \r\n\r\nEDIT: This worked, and now doesn't. I have no idea what's going on here. \r\n\r\nEDIT: @VicaR's solution in https://github.com/keras-team/keras-tuner/issues/198 seems to work fine so far. \r\n"]}, {"number": 41373, "title": "Not able to use libtensorflow-lite.a static library in Android studio for including headers.", "body": "\r\n\r\n**System information**\r\n- OS Platform and Distribution (windows 10):\r\n- Mobile device (oppo reno2) i am building and running on this device through android studio.\r\n- TensorFlow installed from (source or binary): Ubuntu terminal through pip3\r\n- TensorFlow version:\r\n- Python version: python 3.8.2\r\n- Installed using pip \r\n- Bazel version (if compiling from source): I have created libtensorflow-lite.a using bazle . Now trying to directly use the lib. \r\n- GCC/Compiler version (if compiling from source): gnu++11\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n![image](https://user-images.githubusercontent.com/23177883/87418459-5872de00-c5ef-11ea-8b71-9f1d69a8a161.png)\r\nI am trying to inclue interpreter.h, model.h etc in native c code. But it is showing file not found. If i add tensorflow folder and add the include paths in cmake then build is sucecssfull but include dont work- like I am not able to load model or run interpreter. \r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nI have build the libtensorflow-lite.a for arm64 platform and now using it as static library to include the required dependencies of tflite, to code in native cpp in Android studio using NDK.\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["For using TFLite C API, you'd better check this.\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/c\r\n\r\nFor using prebuilt library with Android Studio, please check this.\r\nhttps://developer.android.com/ndk/guides/cmake#using_prebuilt_libraries", "Thanks terryheo, I am using shared object libtensorflow.so , created using https://www.tensorflow.org/lite/guide/android#use_tflite_c_api\r\n\r\nwhile build and install via android studio, i am getting these error related to interpretor. Please help\r\n\r\n[8/8] Linking CXX shared library C:\\Users\\admin\\Documents\\TeamTalk\\download\\IN006407\\android_tflite-master\\tflite_segmentation\\app\\build\\intermediates\\cmake\\debug\\obj\\arm64-v8a\\libnative-activity.so\r\nFAILED: C:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-activity.so \r\ncmd.exe /C \"cd . && C:\\Users\\admin\\AppData\\Local\\Android\\Sdk\\ndk\\21.2.6472646\\toolchains\\llvm\\prebuilt\\windows-x86_64\\bin\\clang++.exe --target=aarch64-none-linux-android27 --gcc-toolchain=C:/Users/admin/AppData/Local/Android/Sdk/ndk/21.2.6472646/toolchains/llvm/prebuilt/windows-x86_64 --sysroot=C:/Users/admin/AppData/Local/Android/Sdk/ndk/21.2.6472646/toolchains/llvm/prebuilt/windows-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security   -std=gnu++11 -Wall -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libgcc_real.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments  -u ANativeActivity_onCreate -shared -Wl,-soname,libnative-activity.so -o C:\\Users\\admin\\Documents\\TeamTalk\\download\\IN006407\\android_tflite-master\\tflite_segmentation\\app\\build\\intermediates\\cmake\\debug\\obj\\arm64-v8a\\libnative-activity.so CMakeFiles/native-activity.dir/android_main.cpp.o CMakeFiles/native-activity.dir/app_engine.cpp.o CMakeFiles/native-activity.dir/tflite_deeplab.cpp.o CMakeFiles/native-activity.dir/camera_manager.cpp.o CMakeFiles/native-activity.dir/9a7cbc7930929ec4da523570f5b7600c/app/src/include/common/android/camera_utils.cpp.o CMakeFiles/native-activity.dir/9a7cbc7930929ec4da523570f5b7600c/app/src/include/common/android/util_asset.cpp.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_tflite.cpp.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/assertgl.c.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/assertegl.c.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_egl.c.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_shader.c.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_matrix.c.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_texture.c.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_render2d.c.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_debugstr.c.o CMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_pmeter.c.o CMakeFiles/native-activity.dir/9a7cbc7930929ec4da523570f5b7600c/app/src/include/common/winsys/winsys_null.c.o  -landroid libnative_app_glue.a -lm -lcamera2ndk -lmediandk -lEGL -lGLESv2 C:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/src/main/cpp/../jniLibs/arm64-v8a/libtensorflowlite.so -llog -latomic -lm && cd .\"\r\nCMakeFiles/native-activity.dir/tflite_deeplab.cpp.o: In function `invoke_deeplab':\r\nC:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/src/main/cpp/tflite_deeplab.cpp:110: undefined reference to `tflite::impl::Interpreter::Invoke()'\r\nCMakeFiles/native-activity.dir/tflite_deeplab.cpp.o: In function `std::__ndk1::default_delete<tflite::impl::Interpreter>::operator()(tflite::impl::Interpreter*) const':\r\nC:/Users/admin/AppData/Local/Android/Sdk/ndk/21.2.6472646/toolchains/llvm/prebuilt/windows-x86_64/sysroot/usr/include/c++/v1/memory:2338: undefined reference to `tflite::impl::Interpreter::~Interpreter()'\r\nCMakeFiles/native-activity.dir/cca59a257428af77379c25c93fd919d9/tflite_segmentation/app/src/include/common/util_tflite.cpp.o: In function `tflite_create_interpreter(tflite_interpreter_t*, char const*, unsigned long)':\r\nC:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/src/include/common/util_tflite.cpp:97: undefined reference to `tflite::impl::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'\r\nC:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/src/include/common/util_tflite.cpp:97: undefined reference to `tflite::impl::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::impl::Interpreter, std::__ndk1::default_delete<tflite::impl::Interpreter> >*)'\r\nC:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/src/include/common/util_tflite.cpp:97: undefined reference to `tflite::impl::InterpreterBuilder::~InterpreterBuilder()'\r\nC:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/src/include/common/util_tflite.cpp:97: undefined reference to `tflite::impl::InterpreterBuilder::~InterpreterBuilder()'\r\nC:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/src/include/common/util_tflite.cpp:175: undefined reference to `tflite::impl::Interpreter::SetNumThreads(int)'\r\nC:/Users/admin/Documents/TeamTalk/download/IN006407/android_tflite-master/tflite_segmentation/app/src/include/common/util_tflite.cpp:176: undefined reference to `tflite::impl::Interpreter::AllocateTensors()'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)", "Could you try to provide \"-Wl,--whole-archive\" link option to link libtensorflowlite.so ?", "![image](https://user-images.githubusercontent.com/23177883/88025834-89ed2b80-cb52-11ea-83e7-ded96f16fe2e.png)\r\n\r\nDo you mean to change in condition default option and rebuild the shared object?", "No. I meant linker command to your hello-libs JNI target. ", "![image](https://user-images.githubusercontent.com/23177883/88037221-3e427e00-cb62-11ea-9272-cbb734de1406.png)\r\n\r\nI have added this in my cmake file. still problem persist.", "[CMakeLists.txt](https://github.com/tensorflow/tensorflow/files/4952527/CMakeLists.txt)\r\n\r\nI am also attaching cmake file for reference."]}]