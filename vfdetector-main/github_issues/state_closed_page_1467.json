[{"number": 8938, "title": "New issue template.", "body": "### Please complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: \r\n- *TensorFlow installed from (source or binary)?*: \r\n- *TensorFlow version*: \r\n- *Operating system*:\r\n- *Bazel version (if compiling from source)*:  \r\n- *CUDA/cuDNN version*: \r\n- *GPU Model and Memory*: \r\n- *Exact command to reproduce*: \r\n\r\nIf you are unsure how to obtain this information, some of it can be obtained by running our collection script and copying and pasting the appropriate data.\r\n\r\n### Source Code / Logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. Large logs and files should be attached. Include full tracebacks. Please keep your examples as short as possible, because longer code makes the chance of your issue being answered lower.", "comments": ["test"]}, {"number": 8937, "title": "Has complex MVN been implemented?", "body": "I have below error for my code , where muC and SigmaC are complex vector and matrix\r\n```\r\ndist = tf.contrib.distributions.MultivariateNormalFull(muC, SigmaC)\r\n```\r\n```\r\nTypeError: Value passed to parameter 'input' has DataType complex64 not in list of allowed values: float64, float32\r\n```", "comments": ["I tried to implemented the pdf of MVN by myself, but it looks like tf.matrix_inverse also don't support complex tensor.", "What's more generally the intended scope of support for complex operations in tensorflow?", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 8936, "title": "Branch 152048672", "body": "", "comments": []}, {"number": 8935, "title": "Fix broken ./configure on mac because of non-portable sed syntax", "body": "Running `./configure` on mac complains with the following error:\r\n\r\n```shell\r\nsed: can't read /tf_configure/d: No such file or directory\r\n```\r\n\r\nAccording to [this SO post](http://stackoverflow.com/questions/4247068/sed-command-with-i-option-failing-on-mac-but-works-on-linux) the syntax to avoid creating backup files with sed on 10.9+ is without a space between `-i ''`.\r\n\r\nFixes #9553.", "comments": ["Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Jenkins, test this please.", "@gunan what version of OSX are the mac slaves running? If it's pre-10.9 that would explain why it's failing (and worked before). I can try to make the code more portable for this.", "@JVillella our executors should be el capitan and sierra only, (all 10.11+).", "Error seems invalid.\r\n\r\nJenkins, test this please.", "The failure in mac cpu tests is caused by the changes in this PR.\r\n\r\n@JVillella do you have any insight about the failure, or how to fix it?\r\nIf not, I recommend we close this PR", "@gunan looking again now.", "So I seem to be running `gnu-sed 4.2.2` which was likely installed from `coreutils` at some point in time. It has a different syntax. MacOS's built-in `sed` wants the empty string, `gsed` doesn't which is why there is a check (though for platform, assuming if you're on mac you're not using gsed):\r\n\r\n```shell\r\nfunction sed_hyphen_i() {\r\n  if is_macos; then\r\n    sed -i '' \"$@\"\r\n  else\r\n    sed -i \"$@\"\r\n  fi\r\n}\r\n```\r\n\r\nUsing backups seems to be more portable (e.x. `sed -i '.bak' ...`). No platform check is required either. We could do this and just remove the backup. If you guys are ok with this solution I can make the change. `awk` (not `gawk`) doesn't let us do it in place either.\r\n\r\nThis issue only affects someone running `gsed` in place of `sed` on mac. If this makes it not-a-concern we can close the ticket.", "Sorry for leaving this hanging. I think using .bak is a fine solution, especially if you delete them afterwards. I'd like to avoid the if, which seems brittle.", "@JVillella any updates?", "Hey @martinwicke, @vrv. I've updated the PR with a more portable solution. Let me know if this is okay with you guys.", "Looks good to me. \r\n\r\nJenkins, test this please."]}, {"number": 8934, "title": "complex weight for LSTM?", "body": "I am having complex input and output for LSTM, and it looks like I have to initialize LSTM weights as complex tensor. Below is my code:\r\n```\r\nrnn_size = 128\r\ncell = core_rnn_cell.BasicLSTMCell(rnn_size, state_is_tuple=False)\r\nprint 'initiate LSTM cell: ',cell\r\nnum_layers = 1\r\nbatch_size = 1\r\nseq_length = T.shape[0]\r\ninitial_state = cell.zero_state(batch_size=batch_size, dtype=tf.complex64)\r\nlearning_rate = 0.003\r\ninput_data = tf.placeholder(tf.complex64, [seq_length-1, 3],name='input_data')\r\ntarget_data = tf.placeholder(tf.complex64, [seq_length-1, 3],name = 'target_data')\r\nlr = tf.Variable(learning_rate, trainable=False, name=\"learning_rate\")\r\nK = 3\r\noutput_size = K + K*(K+1)/2 \r\nembedding_size = 128\r\nwith tf.variable_scope(\"coordinate_embedding\"):\r\n#     real_w = tf.Variable(tf.truncated_normal([total_arg_size, output_size], stddev=0.1), name = \"complex_weight_real\")\r\n#     imag_w = tf.Variable(tf.truncated_normal([total_arg_size, output_size], stddev=0.1), name = \"complex_weight_imag\")\r\n#     matrix = tf.complex(real_w, imag_w)\r\n    real_embedding_w = tf.get_variable(\"real_embedding_w\", [K, embedding_size])\r\n    imag_embedding_w = tf.get_variable(\"imag_embedding_w\", [K, embedding_size])\r\n    embedding_w = tf.complex(real_embedding_w, imag_embedding_w)\r\n    real_embedding_b = tf.get_variable(\"real_embedding_b\", [embedding_size])\r\n    imag_embedding_b = tf.get_variable(\"imag_embedding_b\", [embedding_size])\r\n    embedding_b = tf.complex(real_embedding_b, imag_embedding_b)\r\n\r\nwith tf.variable_scope(\"rnnlm\"): \r\n    real_output_w = tf.get_variable(\"real_output_w\", [rnn_size, output_size], initializer=tf.truncated_normal_initializer(stddev=0.01), trainable=True)\r\n    imag_output_w = tf.get_variable(\"imag_output_w\", [rnn_size, output_size], initializer=tf.truncated_normal_initializer(stddev=0.01), trainable=True)\r\n    output_w = tf.complex(real_output_w, imag_output_w)\r\n    real_output_b = tf.get_variable(\"real_output_b\", [output_size], initializer=tf.constant_initializer(0.01), trainable=True)\r\n    imag_output_b = tf.get_variable(\"imag_output_b\", [output_size], initializer=tf.constant_initializer(0.01), trainable=True)\r\n    output_b = tf.complex(real_output_b, imag_output_b)\r\ninputs = tf.split(input_data, seq_length-1, 0)\r\nstates = []\r\ninitial_state = cell.zero_state(batch_size=batch_size, dtype=tf.complex64)\r\nstate = initial_state\r\noutputs = []\r\npredict_initial_state = cell.zero_state(batch_size=batch_size, dtype=tf.complex64)\r\npredict_input =  tf.placeholder(tf.complex64, [1,  2])\r\npredict_sequence = 100\r\nindex =0\r\npredict_outputs = []\r\noutput, new_state = cell(inputs[i], state)\r\n```\r\nBut I have the below error, but I checked the documentation and didn't find out how to initialize the weights of LSTM cell as complex tensors.\r\n```\r\nValueError: An initializer for variable basic_lstm_cell/weights of <dtype: 'complex64'> is required\r\n```\r\n\r\nCould anyone give me some hint on this? Thanks in advance!", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "Hi, were you able to resolve the issue? If yes, could you please upload your code?"]}, {"number": 8933, "title": "Prediction based on day / time with tensorFlow", "body": "I did some digging but I couldn't find anything (maybe i just didn't know what do search exactly),\r\nI'm in my final year of study and I am starting my final year project in Software engineering,\r\nMy plain is to (lets take smart house as an example) create a machine that will learn the householders habits and then do that action based on what they did (e.g: at Sunday, 7 AM turn on the boiler for 30 minutes because only one resident need to take a shower).\r\n\r\nCan TensorFlow learn and act like that?\r\n\r\nPlease help.\r\n\r\nThanks.", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n", "Ok, thanks, I did that."]}, {"number": 8932, "title": "Can't register new xla device", "body": "I'm trying to register a fake 'xpu' device to use with xla, but it hasn't been working. I've gotten Tensorflow to build after making the changes found in the attached file ([diffOutput.txt](https://github.com/tensorflow/tensorflow/files/891147/diffOutput.txt), the output from git diff). But, when I run the following sample code:\r\n```\r\nimport argparse\r\nimport sys\r\n\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.python.client import timeline\r\n\r\nFLAGS = None\r\n\r\n\r\ndef main(_):\r\n    config = tf.ConfigProto(log_device_placement=True)\r\n    jit_level = 0\r\n    if FLAGS.xla:\r\n        # Turns on XLA JIT compilation.\r\n        jit_level = tf.OptimizerOptions.ON_1\r\n        print('XLA flag on')\r\n\r\n    config.graph_options.optimizer_options.global_jit_level = jit_level\r\n    run_metadata = tf.RunMetadata()\r\n    # Creates a session with log_device_placement set to True.\r\n    with tf.Session(config=config) as sess:\r\n        # Creates a graph.\r\n        with tf.device('/job:localhost/replica:0/task:0/device:XLA_XPU:0'):\r\n        #with tf.device('/device:CPU:0'):\r\n            a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\r\n                            shape=[2, 3], name='a')\r\n\r\n            b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\r\n                            shape=[3, 2], name='b')\r\n            c = tf.matmul(a, b)\r\n\r\n        # Runs the op.\r\n        print(sess.run(c,\r\n              options=tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE),\r\n              run_metadata=run_metadata))\r\n        trace = timeline.Timeline(step_stats=run_metadata.step_stats)\r\n        with open('timeline.ctf.json', 'w') as trace_file:\r\n            trace_file.write(trace.generate_chrome_trace_format())\r\n\r\nif __name__ == \"__main__\":\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--data_dir', type=str,\r\n            default='/tmp/tensorflow/mnist/input_data',\r\n            help='Directory for storing input data')\r\n    parser.add_argument(\r\n              '--xla', type=bool, default=True, help='Turn xla via JIT on')\r\n    FLAGS, unparsed = parser.parse_known_args()\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\r\n```\r\n\r\nI get the error:\r\n```\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'MatMul': Could not satisfy explicit device specification '/job:localhost/replica:0/task:0/device:XLA_XPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0\r\n\t [[Node: MatMul = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:XLA_XPU:0\"](a, b)]]\r\n```\r\nWhat else needs to change in order for my code to work?", "comments": ["@tatatodd, can you take a quick look please?", "I attached a diff file of all the changes I needed to make in order to make it work.\r\n\r\n@aidan-plenert-macdonald\r\n[add_xpu.txt](https://github.com/tensorflow/tensorflow/files/911655/add_xpu.txt)\r\n", "We found some indications that XPU wasn't being used - the output graph information labeled everything as CPU, and it didn't go into the XPU compile statement. We think it's finally doing what we want, so I'm attaching the diff file: \r\n[diffOutput.txt](https://github.com/tensorflow/tensorflow/files/968621/diffOutput.txt)\r\n"]}, {"number": 8931, "title": "Remove --no-deps flag from pip install", "body": "of tensorflow whl in pip.sh.\r\nFixes ongoing breakages related to the new dependencies of the\r\ntensorflow wheel (e.g., bleach and markdown).", "comments": ["@yifeif I just added the flag back.", "Also tested: GPU Python3.4 and 3.5 pip builds:\r\nhttp://ci.tensorflow.org/view/Experimental/job/experimental-cais-matrix-linux-gpu/2/"]}, {"number": 8930, "title": "Add gradient op for MaxPoolWithArgmax", "body": "This is a reattempt of #4014, which fixes #1793.", "comments": ["Can one of the admins verify this patch?", "Jenkins, test this please.\r\n\r\n@keveman what do you think?", "Will this change also eventually fix this issue [#6035](https://github.com/tensorflow/tensorflow/issues/6035)? I have not been able to get that snippet of code working with just CPU. Thanks guys!", "Hopefully yes, although I've only tested it on the GPU myself. That was IIRC the same error message I had before this fix.", "Awesome to hear! I'm pretty new to this; how would I go about updating to the latest version that's on here? Been looking to use this. Thanks again!", "Until the next release, you may have to install from the master branch. See [Installing TensorFlow from Sources](https://www.tensorflow.org/install/install_sources).", "Still seems not to be working on CPUs.\r\n`tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'MaxPoolWithArgmax' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n  <no registered kernels>`\r\n\r\nThanks guys~", "reopened,  the PR I referenced as verified that it does not work on CPU by @zhenbangchen above.  Thank you for checking.", "Is there a clue on what should be done here to achieve CPU support?"]}, {"number": 8929, "title": "Enabled the generation of big kernels by libxsmm", "body": "", "comments": []}, {"number": 8928, "title": "cudnnFindConvolutionForwardAlgorithmEx vs cudnnGetConvolutionForwardAlgorithm", "body": "Following up on https://github.com/tensorflow/tensorflow/issues/7187#issuecomment-290284053, why does Tensorflow use [`cudnnGetConvolutionForwardAlgorithm`](https://github.com/tensorflow/tensorflow/blob/904edee4456a61d50d5b1ffe9858a7772acc423e/tensorflow/stream_executor/cuda/cuda_dnn.cc#L1844) rather than `cudnnFindConvolutionForwardAlgorithmEx`?  It looks like [Tensorflow tries to do the more complete profiling itself](https://github.com/tensorflow/tensorflow/blob/f8d8172c0086a42dfc1ed732fcf083dbd62c42e4/tensorflow/core/kernels/conv_ops.cc#L664-L689). \r\n\r\nFor reference, `cudnnGetConvolutionForwardAlgorithm` serves as a heuristic for obtaining the best suited algorithm for cudnnConvolutionForward for the given layer specifications. Based on the input preference, this function will either return the fastest algorithm or the fastest algorithm within a given memory limit. For an exhaustive search for the fastest algorithm, please use `cudnnFindConvolutionForwardAlgorithm`.\r\n\r\nWhereas:\r\n`cudnnFindConvolutionForwardAlgorithmEx` function attempts all available cuDNN algorithms for cudnnConvolutionForward, using user-allocated GPU memory, and outputs performance metrics to a user-allocated array of cudnnConvolutionFwdAlgoPerf_t. These metrics are written in sorted fashion where the first element has the lowest compute time.\r\n\r\nLooking at a number of other DNN, they seem to use `cudnnFindConvolutionForwardAlgorithmEx` / `cudnnFindConvolutionForwardAlgorithm`:\r\n* [pytorch](https://github.com/pytorch/pytorch/blob/e50a1f19b3dc735f0710929b97b0af384aafe09b/torch/csrc/cudnn/Conv.cpp#L214) (when benchmark is on): \r\n* [Theano](https://github.com/Theano/Theano/blob/c6ffa460d43b6650aefbe7939f332084caafeaa4/theano/gpuarray/dnn_fwd.c#L125) (if `time_once` or `time_on_shape_change`)\r\n* [cntk](https://github.com/Microsoft/CNTK/blob/5651d574c8f3fecf2533cf01beebaaa6e07453e2/Source/Math/CuDnnConvolutionEngine.cu#L229) (non-static finder)\r\n\r\n/CC @Yangqing @zheng-xq", "comments": ["@zheng-xq @vrv : Might one of you have some historical background on this choice, or general comments?", "cudnnGetConvolutionForwardAlgorithm is the fallback path. By default, TensorFlow does the autotuning by itself before cudnnFindConvolutionForwardAlgorithmEx is available. Also the custom implementation enables us to filter out the noise through multiple run steps. At this point, cudnnFindConvolutionForwardAlgorithmEx doesn't seem to offer more functionalities to justify a change. \r\n\r\nIn the future, the plan is to autotune both Cudnn algorithms and other custom kernels together, so we can also pick the fastest among both worlds.", "@cancan101 : Does that answer your question? (Will wait a while before closing this out as intended behavior)", "Yea, it does make sense. As an aside, might be nice to logout the results of the profiling runs. I think pytorch  / torch7 [has an option to do this](https://github.com/soumith/cudnn.torch#modes).", "Thanks. Closing this out.\r\n\r\nIt might make sense for the selected algorithm to be logged either to the logging system or maybe in the `RunMetadata` protocol buffer. If you'd like to make a contribution towards that, we'll be glad to take a look!\r\n\r\n"]}, {"number": 8927, "title": "tutorials/monitors/iris_monitor.py fixes", "body": "the code was not working due to some deprecations as well as duplicate code. This is more effective. Please commit. Thanks.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 8925, "title": "RuntimeError when executing models/tutorials/image/cifar10/cifar10_train.py", "body": "### Environment info\r\nOperating System:\r\nUbuntu Server 16.04LTS\r\n\r\nInstalled version of CUDA and cuDNN: \r\nCUDA 8.0 cuDNN 5.1\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\nls -l /usr/local/cuda/lib64/libcud*\r\n-rw-r--r-- 1 root root   560184 Mar  7 09:54 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Mar  7 09:54 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Mar  7 09:54 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\r\n-rwxr-xr-x 1 root root   394472 Mar  7 09:54 /usr/local/cuda/lib64/libcudart.so.8.0.27\r\n-rw-r--r-- 1 root root   737516 Mar  7 09:54 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 84163560 Mar  9 03:26 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 84163560 Mar  9 03:26 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 84163560 Mar  9 03:26 /usr/local/cuda/lib64/libcudnn.so.5.1.10\r\n-rw-r--r-- 1 root root 70364814 Mar  9 03:26 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed:\r\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp27-none-linux_x86_64.whl\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\n1.0.1\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nI followed this [tutorial](https://www.tensorflow.org/tutorials/deep_cnn#launching_and_training_the_model) to launch the model:\r\n```\r\npython cifar10_train.py\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n```\r\nTraceback (most recent call last):\r\n  File \"cifar10_train.py\", line 124, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 44, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"cifar10_train.py\", line 120, in main\r\n    train()\r\n  File \"cifar10_train.py\", line 110, in train\r\n    log_device_placement=FLAGS.log_device_placement)) as mon_sess:\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 315, in MonitoredTrainingSession\r\n    return MonitoredSession(session_creator=session_creator, hooks=all_hooks)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 601, in __init__\r\n    session_creator, hooks, should_recover=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 434, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 767, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 772, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 494, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 375, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py\", line 272, in prepare_session\r\n    msg))\r\nRuntimeError: Init operations did not make model ready for local_init.  Init op: group_deps, init fn: None, error: Variables not initialized: global_step, conv1/weights, conv1/biases, conv2/weights, conv2/biases, local3/weights, local3/biases, local4/weights, local4/biases, softmax_linear/weights, softmax_linear/biases, conv1/weight_loss/avg, conv2/weight_loss/avg, local3/weight_loss/avg, local4/weight_loss/avg, softmax_linear/weight_loss/avg, cross_entropy/avg, total_loss/avg, conv1/weights/ExponentialMovingAverage, conv1/biases/ExponentialMovingAverage, conv2/weights/ExponentialMovingAverage, conv2/biases/ExponentialMovingAverage, local3/weights/ExponentialMovingAverage, local3/biases/ExponentialMovingAverage, local4/weights/ExponentialMovingAverage, local4/biases/ExponentialMovingAverage, softmax_linear/weights/ExponentialMovingAverage, softmax_linear/biases/ExponentialMovingAverage\r\n```", "comments": ["It looks like variables have not been initialized. Try adding this line right after you start the session\r\nsess.run(tf.global_variables_initializer())", "@Evan-Gao : Is it possible that you're not using the latest versions of the tutorial files (from https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10)? Or perhaps there are some stale files in `/tmp/cifar10_train` (the [default directory](https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_train.py#L48) where the tutorial places it files).\r\n\r\nI was unable to reproduce the error with release 1.0.1 and the latest contents of those files.\r\nCould you try with a fresh snapshot (and after `rm -rf /tmp/cifar10_train`)?", "@asimshankar I pull the latest version and everything is fine now. thx"]}, {"number": 8924, "title": "Update get_started.md", "body": "Correcting output.", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "\u2714\ufe0f Please visit https://cla.developers.google.com/ to sign.", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please", "@gunan thank you for merging. Can you tell me how the process of building and deploying the documentation is beeing handle by you guys? I see many differences between the source documentation and the one published.\r\n", "The root of the website will always be from the latest stable version, which is currently 1.0.\r\nYour changes in master will be deployed under `https://www.tensorflow.org/versions/master/get_started/get_started`.\r\nYour changes will also be in the next release cut which will be 1.2 (1.1 is already cut, and nearing the final release.)\r\n", "Thanks for the info. \ud83d\udc4d"]}, {"number": 8923, "title": "Hiding the re2 symbols from libtensorflow.so", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nThis recent issue [8320](https://github.com/tensorflow/tensorflow/issues/8320) is related but asked for the opposite (more symbol visibility rather than less symbol visibility).\r\n\r\n### Environment info\r\nOperating System: Linux\r\n\r\nInstalled version of CUDA and cuDNN: n/a\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nRunning the following command line on the [pre-built linux binaries](https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.0.0.tar.gz) returns quite a few symbols.\r\n objdump -T libtensorflow.so  | grep re2\r\n\r\nIt looks like libtensorflow.so is using re2 internally and exposing its symbols. This is annoying when linking with some existing code that uses a different version of re2 as it could easily result in the wrong symbol being called depending on the order of the linker flags. In my case, the re2 calls made by my code ended up using the tensorflow exposed symbols and failing because the version was different.\r\n\r\nMaybe the visibility of the re2 symbols embedded in libtensorflow.so could be set to hidden ? (I don't know much about symbol visibility but happy to help if you think it makes sense)", "comments": ["Sorry about that, yes `libtensorflow.so` should export much fewer symbols than it does today.\r\n\r\nWill take a look.", "Thanks for the quick fix @asimshankar and @rohan100jain , linking with the latest nightly build worked like a charm."]}, {"number": 8922, "title": "Android tiny-yolo model", "body": "I was successfully able to run tiny-yolo-voc.pb on android. However, when I tried to run tiny-yolo.pb (which is for coco dataset) generated from https://github.com/thtrieu/darkflow , the app crashes with java.nio.BufferOverflowException.\r\n\r\nCould you please point out what needs to be changed in the code to be able to run tiny-yolo. \r\n\r\nThanks", "comments": ["This question is probably better posed on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) as the Github issues are meant to be focused on bugs and feature requests.\r\n\r\nThat said, some quick comments:\r\n\r\n- It's hard to suggest the changes without more information (for example, which of the various versions in https://github.com/thtrieu/darkflow/tree/master/cfg is being used). If you post on stackoverflow, you should include this information.\r\n- Model-specific configuration might need to be changed. For example, see the constants in [`DetectorActivity.java`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L71) and [`TensorFlowYoloDetector.java`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java#L41), which will have to be consistent with the configuration of the model you're using.\r\n\r\nHope that helps. Closing this issue out as it doesn't seem to be a bug and more support is best asked for on Stackoverflow.\r\n\r\n(FYI @andrewharp)", "@ramarajan09 hi, could you tell me how to run yolo model ?thanks\uff01\uff01", "Use  darknet and darkflow generate   pb file, and put it into assets , change the  YOLO_MODEL_FILE DetectorMode MODE = DetectorMode.YOLO; in DetectorActivity\r\n\r\nflow --model cfg/yolo.cfg --load bin/yolo.weights --savepb\r\n"]}, {"number": 8921, "title": "Python test failure in TF (KeyError: 'TEST_SRCDIR') on Ubuntu 16.04 ", "body": "### Environment info: \r\nOperating System: Ubuntu 16.04 (ppc64le)\r\n\r\n###Installed version of CUDA and cuDNN:  CUDA support is disabled \r\n\r\n###Build and test INFO\r\nBuilt TF (version 1.0.1) successfully on Ubuntu 16.04 and RHEL 7.3. Now I am trying to run the python tests , but 3 tests are failing with the same error :\r\n         [Test name:\r\n                           1) python ./python/saved_model/saved_model_test.py\r\n\t                   2) python ./contrib/session_bundle/session_bundle_test.py\r\n\t                   3) python ./contrib/session_bundle/bundle_shim_test.py ]\r\n##############################Error log########################################\r\nERROR: testMaybeSavedModelDir (__main__.SavedModelTest)\r\n           ----------------------------------------------------------------------\r\n          Traceback (most recent call last):\r\n          File \"./python/saved_model/saved_model_test.py\", line 112, in testMaybeSavedModelDir\r\n          base_path = test.test_src_dir_path(\"/python/saved_model\")\r\n          File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/test.py\", line 84, in test_src_dir_path\r\n          return _googletest.test_src_dir_path(relative_path)\r\n          File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/googletest.py\", line 112, in test_src_dir_path\r\n          return os.path.join(os.environ['TEST_SRCDIR'],\r\n          File \"/usr/lib64/python2.7/UserDict.py\", line 23, in __getitem__\r\n          raise KeyError(key)\r\n          KeyError: 'TEST_SRCDIR' \r\n\r\n\r\nAny suggestion/solution ?", "comments": ["~~TEST_SRCDIR looks like it refers to a directory. Can you please make sure that all required files for your script are in the correct locations?~~\r\nEDIT: I stand very corrected :)", "TEST_SRCDIR environment variable is actually created by bazel.\r\nIf you are not running with bazel, could you try to set it to the root of your TF source repository before you run your tests? In your case, I think the following should suffice\r\n```\r\nexport TEST_SRCDIR=`pwd`\r\n```", "I have exported the \"TEST_SRCDIR\" env var to the root of TF source repository, and now that error is gone , however I am getting some different kind of error\r\n\r\n======================================================================\r\nFAIL: testMaybeSavedModelDir (__main__.SavedModelTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"./python/saved_model/saved_model_test.py\", line 115, in testMaybeSavedModelDir\r\n    self.assertTrue(loader.maybe_saved_model_directory(base_path))\r\nAssertionError: False is not true\r\n\r\n----------------------------------------------------------------------\r\nRan 18 tests in 3.913s\r\n\r\nFAILED (failures=1)\r\n", "Most of our tests are written to work with bazel.\r\nI highly recommend against running them outside bazel, and we do not really guarantee tests passing when run outside bazel.\r\nHere is why you are seeing issues:\r\nSaved_model test has a data dependency as defined here:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/BUILD#L106\r\n\r\nWhat bazel does is it arranges all the data dependencies under TEST_SRCDIR.\r\nThen when we run this line after building with bazel, the folder is accessible to us:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/saved_model_test.py#L113\r\n\r\nBecause bazel arranges all the dependencies for us, and gives a way for us to access them.\r\n\r\nTherefore, It will be tricky to run this test without bazel. And once you get it running, it will be brittle, because it is written using the concepts used by bazel, like the data dependencies.\r\n\r\nI will close this issue, as it is not a TF bug.\r\nI would recommend reaching out to stackoverflow if you still would like to run TF unit tests outside bazel."]}, {"number": 8920, "title": "problem with exemple in API documentation for tf.contrib.distributions.bijector.ScaleAndShift", "body": "Hello\r\n\r\nI am currently using tensorflow version 1.0.0 (from conda vanilla installation),\r\nand I am running a snippet code fromAPI documentation :\r\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/distributions/TransformedDistribution?authuser=3\r\n\r\nI get an AttributeError which seems an accurate error since I can't seem to find ScaleAndShift on the github repo for tensorflow r1.0 \r\n\r\n\r\n    >>> ds.TransformedDistribution(\r\n    ...   distribution=ds.Normal(mu=0., sigma=1.),\r\n    ...   bijector=ds.bijector.ScaleAndShift(loc=mu, scale=sigma,   event_ndims=0),\r\n    ...   name=\"NormalTransformedDistribution\")\r\n    Traceback (most recent call last):\r\n      File \"<stdin>\", line 3, in <module>\r\n    AttributeError: 'module' object has no attribute 'ScaleAndShift'\r\n\r\n\r\n", "comments": ["Thanks for the report.\r\nThat documentation needs to be updated.\r\n\r\nIIUC, in 1.0.0 this would be replaced by [`ds.bijector.Affine`](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/contrib/distributions/python/ops/bijector.py#L1463)\r\n\r\n@jvdillon @ebrevdo : Something still seems fishy at head and in 1.1.0-rc1 where `tf.contrib.distributions.bijector` doesn't seem to exist. Mind taking a look?", "Hi chetzacoalt, asimshankar. Thanks for bringing this to our attention and apologies for the trouble!\r\n\r\nYes it looks like the docstring was out of date.  What's worse is that it also looks like we still messed up the bijector importing.  I've created and verified a fix; it should make head within 1 to 2 days.", "@jvdillon : Assigned to you - just to remind you to close it out when ready. Thanks!", "thanks a lot, that will be quite helpful ! \r\n", "This appears to be fixed in head.  @jvdillon , can you ask for a cherrypick on this change into the r1.0 and r1.1 branch?", "Automatically closing due to lack of recent activity. Since this issue is old at this point, please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you."]}, {"number": 8919, "title": "TensorFlow Windows Bazel build is failing", "body": "http://ci.tensorflow.org/job/tf-master-win-bzl/709/console\r\n```\r\nERROR: C:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x86_64/tensorflow/tools/pip_package/BUILD:61:1: error loading package 'tensorflow/python': Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf//': Traceback (most recent call last):\r\n\tFile \"C:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x86_64/tensorflow/workspace.bzl\", line 88\r\n\t\t_apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n\tFile \"C:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x86_64/tensorflow/workspace.bzl\", line 79, in _apply_patch\r\n\t\t_execute_and_check_ret_code(repo_ctx, [\"patch\", \"-p1\", \"-d\", r...), <2 more arguments>])\r\n\tFile \"C:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x86_64/tensorflow/workspace.bzl\", line 71, in _execute_and_check_ret_code\r\n\t\tfail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(256) when executing 'patch -p1 -d C:/tmp/_bazel_system/wgutbc48/external/protobuf -i C:/jenkins/workspace/tensorflow/bazel_version/head/platform_name/windows-x86_64/third_party/protobuf/add_noinlines.patch':\r\nStdout: \r\nStderr: java.io.IOException: CreateProcess(): The system cannot find the file specified.\r\n```\r\nCulprit: 8d393ea2fab0ea88ecd11e36d89f186cbc884dbe\r\nReason: patch command is not installed in MSYS\r\nSolution: run `pacman -Syuu --noconfirm patch`\r\n@gunan ", "comments": ["@gunan @av8ramit @yifeif : Seems like we need to install `patch` on Windows. Could one of you take a look?", "Running the installation on all machines.\r\nAlso started a test:\r\nhttp://ci.tensorflow.org/job/tf-master-win-bzl/711/", "The build is past this issue.", "I installed patch using `pacman -Syuu --noconfirm patch` but still facing same issue on windows 7 64 bit", "@DeepakDonde Can you provide more information?\r\nCan you run `patch` in MSYS directly?", "@meteorcloudy \r\n**So, This is what i tried:**\r\n1)Download tensorflow using `git clone --recurse-submodules  https://github.com/tensorflow/tensorflow.git`\r\n2)Edit `WORKSPACE `file to set android SDK and NDK path.\r\n3)Download and install  `bazel` using [chocolatey](https://chocolatey.org/install)\r\n4)Go to msys64 directory, open msys2.exe and execute `pacman -Syuu --noconfirm patch` command\r\n5)Go to tensorflow directory and execute `bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \r\n   --crosstool_top=//external:android/crosstool \r\n   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \r\n   --cpu=armeabi-v7a` command using cmd\r\n\r\nAfter this command I get following issue:\r\n`ERROR: D:/ocr/tensorflow/tensorflow/contrib/android/BUILD:72:1: error loading pa\r\nckage 'tensorflow/core': Encountered error while reading extension file 'protobu\r\nf.bzl': no such package '@protobuf//': Traceback (most recent call last):\r\n        File \"D:/ocr/tensorflow/tensorflow/workspace.bzl\", line 116\r\n                _apply_patch(repo_ctx, repo_ctx.attr.patch_file)\r\n        File \"D:/ocr/tensorflow/tensorflow/workspace.bzl\", line 107, in _apply_p\r\natch\r\n                _execute_and_check_ret_code(repo_ctx, cmd)\r\n        File \"D:/ocr/tensorflow/tensorflow/workspace.bzl\", line 91, in _execute_\r\nand_check_ret_code\r\n                fail(\"Non-zero return code({1}) when ..., <2 more arguments>))\r\nNon-zero return code(127) when executing 'c:/tools/msys64/usr/bin/bash.exe -c pa\r\ntch -p1 -d C:/users/deepakd/appdata/local/temp/_bazel_deepakd/rqaligb5/external/\r\nprotobuf -i D:/ocr/tensorflow/third_party/protobuf/add_noinlines.patch':\r\nStdout:\r\nStderr: /usr/bin/bash: patch: command not found\r\n and referenced by '//tensorflow/contrib/android:libtensorflow_inference.so'.\r\nERROR: Analysis of target '//tensorflow/contrib/android:libtensorflow_inference.\r\nso' failed; build aborted.`\r\n\r\nI tried to run `patch` command in msys2 but it gives no output and the console remains blank.\r\n\r\n**System Configuration:**\r\nWindow 7 - 64 bit\r\nJDK version -1.8.0_60\r\nNDK version -r14b\r\n\r\n**P.S:** I followed [this tutorial](https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc)", "@DeepakDonde Unfortunately, Bazel current doesn't support building Android app on Windows yet, but we are actively working on it.", "@meteorcloudy  Ok. Please let us know if you found solution for this.", "I have a similar issue, installed patch in MSYS (64) in Windows with `pacman -Syuu --noconfirm patch` but fails. I can use `patch --help` from MSYS shell (C:...\\msys64\\msys2_shell.cmd -mingw64) but the command based on the BAZEL_SH env variable (`C:\\msys64\\usr\\bin\\bash.exe -c patch --help`) doesn't work.\r\n\r\nHowever, if in the cmd I run the bash.exe (enters bash) and then bash (actually starts the MINGW64 shell) then I see patch (and all other stuff) available.", "Managed to make it work by installing patch using choco (choco install patch), running a cmd as administrator. (Windows 7)"]}, {"number": 8918, "title": "Make sure we don't compile with SSE for i386 iOS simulator builds", "body": "This prevents a fatal error on launch, see #8914", "comments": []}, {"number": 8917, "title": "how to join the tf.string SparseTensor to 1-D dense tensor", "body": "tensorflow has tf.string_split function that can split dense tensor to SparseTensor, but not provided the opposite function.\r\n\r\nanyone knows how to do it? thanks~\r\n\r\nfor example: SparseTensor:\r\n```\r\n[[\"a\", \"b\", \"c\"]\r\n [\"d\", \"e\"]\r\n [\"f\", \"g\", \"h\", \"i\"]]\r\n```\r\n\r\njoin SparseTensor with separator \" \" to dense tensor:\r\n```\r\n[\"a b c\",\r\n \"d e\",\r\n \"f g h i\"]\r\n```", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 8916, "title": "Cannot assign a device to node 'Variable'", "body": "### Environment info\r\nOperating System: Ubuntu 16.04\r\n\r\nInstalled version of CUDA and cuDNN: \r\n/usr/local/cuda-8.0/lib64/libcudadevrt.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudnn.so.5\r\n/usr/local/cuda-8.0/lib64/libcudart.so\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudnn_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0\r\n/usr/local/cuda-8.0/lib64/libcudnn.so\r\n\r\nTensorflow version: 1.0.1\r\n\r\n### If possible, provide a minimal reproducible example\r\nwith tf.device('/gpu:0'):\r\n    a = tf.Variable([[1, 1]], tf.float32)\r\n\r\nwith tf.device('/cpu:0'):\r\n    b = tf.Variable([[0, 0]], tf.float32)\r\n\r\nIt will get:\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n...\r\n\r\n### What other attempted solutions have you tried?\r\nwith tf.device('/gpu:0'):\r\n    a = tf.Variable([[1.0, 1.0]], tf.float32)\r\n\r\nwith tf.device('/cpu:0'):\r\n    b = tf.Variable([[0.0, 0.0]], tf.float32)\r\nIf I change the contents of the initial value from integer to float, everything will be fine.\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce 940M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.176\r\npciBusID 0000:04:00.0\r\nTotal memory: 1.96GiB\r\nFree memory: 1.60GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce 940M, pci bus id: 0000:04:00.0)\r\nDevice mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce 940M, pci bus id: 0000:04:00.0\r\nI tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:\r\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce 940M, pci bus id: 0000:04:00.0\r\n\r\nVariable_1: (VariableV2): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Variable_1: (VariableV2)/job:localhost/replica:0/task:0/cpu:0\r\nVariable_1/read: (Identity): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Variable_1/read: (Identity)/job:localhost/replica:0/task:0/cpu:0\r\nPrint_1: (Print): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Print_1: (Print)/job:localhost/replica:0/task:0/cpu:0\r\nVariable_1/Assign: (Assign): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] Variable_1/Assign: (Assign)/job:localhost/replica:0/task:0/cpu:0\r\ninit/NoOp: (NoOp): /job:localhost/replica:0/task:0/cpu:0\r\nI tensorflow/core/common_runtime/simple_placer.cc:841] init/NoOp: (NoOp)/job:localhost/replica:0/task:0/cpu:0\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 20, in <module>\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nAssign: CPU \r\nIdentity: CPU \r\nVariableV2: CPU \r\n\t [[Node: Variable = VariableV2[container=\"\", dtype=DT_INT32, shape=[1,2], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n\r\nCaused by op u'Variable', defined at:\r\n  File \"test.py\", line 8, in <module>\r\n    a = tf.Variable([[1, 1]], tf.float32)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 197, in __init__\r\n    expected_shape=expected_shape)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 293, in _init_from_args\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 128, in variable_op_v2\r\n    shared_name=shared_name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 708, in _variable_v2\r\n    name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device to node 'Variable': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\nColocation Debug Info:\r\nColocation group had the following types and devices: \r\nAssign: CPU \r\nIdentity: CPU \r\nVariableV2: CPU \r\n\t [[Node: Variable = VariableV2[container=\"\", dtype=DT_INT32, shape=[1,2], shared_name=\"\", _device=\"/device:GPU:0\"]()]]\r\n\r\n", "comments": ["GPUs currently support only `tf.half` (aka `tf.float16`), `tf.float32` and `tf.float64`. Variables of other types cannot be placed on GPU.\r\n\r\nThis should probably be better documented. However, it is intended at this time.\r\n\r\n(For details, see [`variable_ops.cc`](https://github.com/tensorflow/tensorflow/blob/2c8d0dc/tensorflow/core/kernels/variable_ops.cc#L66) and [`register_types.h`](https://github.com/tensorflow/tensorflow/blob/2c8d0dc/tensorflow/core/framework/register_types.h#L172))\r\n\r\nHope that helps!"]}, {"number": 8915, "title": "Update lstm_ops.cc", "body": "fix o in doc", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "@tensorflow-jenkins test this please"]}, {"number": 8914, "title": "TensorFlow iOS apps crash at launch on iPhone 4s/iPhone 5 simulators", "body": "Running [the simple iOS example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ios_examples/simple) provided by this repository will crash if I target any iPhone 4s/iPhone 5 **simulator**. The app will crash immediately at launch, before entering `didFinishLaunchingWithOptions`. The last call I can identify in the debugger is usually `tensorflow::internal::LogMessageFatal::~LogMessageFatal:`.\r\n\r\nThe crashes happen on iPhone 5 simulators running iOS 10.2, 9.3, and 9.1. And on iPhone 4s simulators running iOS 9.3, 9.1, and 8.4.\r\n\r\nI can reproduce this behavior with other TensorFlow iOS apps.\r\n\r\nThe same apps that crash in an iPhone 4s/iPhone 5 simulator, will build & run fine on other simulators & devices, _e.g._ on iPad Air simulator running 10.2, a physical iPad Air running 10.2, or physical iPhone 6s Plus running 10.2. Tests on a device farm including iPhone 4s & iPhone 5 devices were inconclusive\u2026 I\u2019m seeing a very similar crash happen, [mostly affecting devices running 9.3.5](https://docs.google.com/spreadsheets/d/1a1Erp-KPNA5bZuin5Fokzx0rVoqr2EEFEbKSEjEVpTw/edit), but the devices/iOS combos that fail on physical devices are slightly inconsistent from run to run (in the simulator the same devices crash 100% of the time no matter what OS I use, and from run to run).\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nThis [issue](https://github.com/tensorflow/tensorflow/issues/4640 ) is the only hit I get, but seems unrelated.\r\n\r\n### Environment info\r\n* Operating System: macOS Sierra 10.12.13\r\n* Commit hash: b393fd79fd43242ea9be3ff3141f1809c412fd71 (master as of this writing)\r\n* Bazel Version: \r\n    ```\r\n    Build label: 0.4.3-jdk7\r\n    Build target: bazel-out/local-    fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n    Build time: Thu Dec 22 12:31:38 2016 (1482409898)\r\n    Build timestamp: 1482409898\r\n    Build timestamp as int: 1482409898\r\n    ```\r\n\r\n### Minimal reproducible example\r\n\r\n1. Prepare the simple iOS example provided by Tensorflow\r\n    ```bash\r\n    $ git clone https://github.com/tensorflow/tensorflow.git\r\n    $ cd tensorflow\r\n    $ tensorflow/contrib/makefile/download_dependencies.sh\r\n    $ tensorflow/contrib/makefile/build_all_ios.sh\r\n    $ open tensorflow/contrib/ios_examples/simple/tf_ios_makefile_example.xcodeproj\r\n    ```\r\n2. Download the Inception graph per the [instructions](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile#ios)\r\n3. Target the iPhone 5 simulator on iOS 10.2, and Run.\r\n4. The app will crash before entering `didFinishLaunchingWithOptions`.\r\n5. (If you target the iPad Air simulator on iOS 10.2, it will work.)", "comments": ["I've reproduced this in the iPhone 5 simulator, and the console message is:\r\n```\r\n2017-04-03 08:29:17.629562: F tensorflow/core/platform/cpu_feature_guard.cc:35] The TensorFlow library was compiled to use SSE instructions, but these aren't available on your machine.\r\n```\r\nAs a quick workaround, you can try changing the LOG(FATAL) in cpu_feature_guard.cc to a LOG(WARNING):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/cpu_feature_guard.cc#L30\r\nI believe this should be fixable by updating the build flags for i386, so I'll look into that. I fear this may be a different issue than the one that's causing intermittent crashes though, since this problem would always occur.", "> I fear this may be a different issue than the one that's causing intermittent crashes though, since this problem would always occur.\r\n\r\nYes, and good news on the intermittent crashes / physical devices front: I just tried doing test on a completely different device farm, and I\u2019m not seeing any of these crashes. Sounds like it was an issue with the first device farm I was using\u2026\r\n\r\nMy bad for linking the two issues: they had very similar behavior/symptoms, and I only discovered the simulator crash while pursuing the device crash, but my suspicion was incorrect! Thanks for your quick response @petewarden, sorry for the trouble.\r\n\r\n", "No problem at all, it was a good catch, and I'm very glad to hear your physical device issues are looking less problematic (knock on wood). I have PR #8918 pending for the simulator bug.", "I think this is actually related to #9809. In that bug I said we don't have a supported 32-bit x86 platform, but that's not true for the older iPhone emulators. I will close that one as a duplicate of this, and we'll aim to get this one fixed.", "This should be fixed now that #8918 is checked in."]}, {"number": 8913, "title": "Segmentation fault: 11 (Python3, Conda env, GPU)", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI've tried importing numpy or scipy before importing tensorflow, but it does not help from [issue #2034](https://github.com/tensorflow/tensorflow/issues/2034)\r\n\r\n### Environment info\r\nOperating System: Max OSX 10.12.3\r\n\r\nInstalled version of CUDA and cuDNN: \r\n```\r\n(bigfish) jenniferstark@Jennifers-MacBook-Pro:~/Documents/Repositories/bigfish$ brew cask info cuda\r\ncuda: 8.0.61\r\nhttps://developer.nvidia.com/cuda-zone\r\nNot installed\r\nFrom: https://github.com/caskroom/homebrew-cask/blob/master/Casks/cuda.rb\r\n==> Name\r\nNvidia CUDA\r\n==> Artifacts\r\n(bigfish) jenniferstark@Jennifers-MacBook-Pro:~/Documents/Repositories/bigfish$ \r\n```\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n```\r\n(bigfish) jenniferstark@Jennifers-MacBook-Pro:~/Documents/Repositories/bigfish$ ls -l /Users/jenniferstark/cuda/lib*\r\ntotal 289880\r\nlrwxr-xr-x  1 jenniferstark  staff    13B Mar  4 19:32 libcuda.1.dylib@ -> libcuda.dylib\r\n-rwxr-xr-x@ 1 jenniferstark  staff    78M Nov  7 02:58 libcudnn.5.dylib*\r\nlrwxr-xr-x@ 1 jenniferstark  staff    16B Nov  7 03:19 libcudnn.dylib@ -> libcudnn.5.dylib\r\n-rw-r--r--@ 1 jenniferstark  staff    63M Nov  7 02:58 libcudnn_static.a\r\n```\r\n\r\nIf installed from binary pip package, provide:\r\n\r\n1. A link to the pip package you installed: \r\n* https://www.tensorflow.org/versions/master/install/install_mac#NVIDIARequirements  \r\n* Installing for Anaconda, GPU, Python3  inside a conda environment.\r\nusing command:\r\n```\r\n/bigfish$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl\r\nCollecting tensorflow-gpu==1.0.1 from https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl\r\n  Downloading https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.1-py3-none-any.whl (89.0MB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 89.0MB 9.7kB/s \r\nCollecting protobuf>=3.1.0 (from tensorflow-gpu==1.0.1)\r\n  Using cached protobuf-3.2.0-py2.py3-none-any.whl\r\nCollecting numpy>=1.11.0 (from tensorflow-gpu==1.0.1)\r\n  Using cached numpy-1.12.1-cp35-cp35m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\r\nCollecting six>=1.10.0 (from tensorflow-gpu==1.0.1)\r\n  Using cached six-1.10.0-py2.py3-none-any.whl\r\nCollecting wheel>=0.26 (from tensorflow-gpu==1.0.1)\r\n  Using cached wheel-0.29.0-py2.py3-none-any.whl\r\nCollecting setuptools (from protobuf>=3.1.0->tensorflow-gpu==1.0.1)\r\n  Downloading setuptools-34.3.3-py2.py3-none-any.whl (389kB)\r\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 399kB 1.8MB/s \r\nCollecting appdirs>=1.4.0 (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)\r\n  Using cached appdirs-1.4.3-py2.py3-none-any.whl\r\nCollecting packaging>=16.8 (from setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)\r\n  Using cached packaging-16.8-py2.py3-none-any.whl\r\nCollecting pyparsing (from packaging>=16.8->setuptools->protobuf>=3.1.0->tensorflow-gpu==1.0.1)\r\n  Using cached pyparsing-2.2.0-py2.py3-none-any.whl\r\nInstalling collected packages: appdirs, six, pyparsing, packaging, setuptools, protobuf, numpy, wheel, tensorflow-gpu\r\nSuccessfully installed appdirs-1.4.3 numpy-1.12.1 packaging-16.8 protobuf-3.2.0 pyparsing-2.2.0 setuptools-34.3.3 six-1.10.0 tensorflow-gpu-1.0.1 wheel-0.29.0\r\n```\r\n\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n```\r\n(bigfish) jenniferstark@Jennifers-MacBook-Pro:~/Documents/Repositories/bigfish$ python -c \"import tensorflow; print(tensorflow.__version__)\"\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nSegmentation fault: 11\r\n(bigfish) jenniferstark@Jennifers-MacBook-Pro:~/Documents/Repositories/bigfish$ \r\n```\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nPython 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:52:12) \r\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import numpy\r\n>>> numpy.__version__\r\n'1.12.1'\r\n>>> import tensorflow\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nSegmentation fault: 11\r\n```\r\n\r\n### What other attempted solutions have you tried?\r\n* Installing the equivalent version for CPU Python3 version based on [these](https://www.tensorflow.org/versions/master/install/install_mac#NVIDIARequirements  ) instructions worked fine. e.g.: \r\n```/bigfish$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.0.1-py3-none-any.whl\r\n```\r\nSeems like the solution for issue #2034 was for python2 and solved by using an older version of numpy, so not relevant here.", "comments": ["Additional information: \r\nWhen running from Spyder, I get this Traceback\r\n```\r\nimport keras\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-2-c74e2bd4ca71>\", line 1, in <module>\r\n    import keras\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import activations\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/activations.py\", line 3, in <module>\r\n    from . import backend as K\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/backend/__init__.py\", line 64, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\nMy `.bash_profile` _should_ be right, but I've followed so many different tutorials, that I may have mixed up different ways of setting things up. It's totally possible I've messed up here \ud83d\ude22 \r\n\r\n**Path to library in question:**\r\n` /Developer/NVIDIA/CUDA-8.0/lib/libcudart.8.0.dylib ` \r\n\r\n**.bash_profile**\r\n```\r\n#export PATH=/Developer/NVIDIA/CUDA-8.0/bin:$PATH\r\nexport PATH=/Developer/NVIDIA/CUDA-8.0/bin${PATH:+:${PATH}}   # from http://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/#axzz4aOAsXP00\r\nexport DYLD_LIBRARY_PATH=\"$DYLD_LIBRARY_PATH:$CUDA_HOME/lib:/Developer/NVIDIA/CUDA-8.0/lib\"\r\nexport DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-8.0/lib${DYLD_LIBRARY_PATH:+:${DYLD_LIBRARY_PATH}}  # from http://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/#axzz4aOAsXP00\r\n\r\n# NVIDIA cuDNN extracted to ~/\r\n#export CUDA_ROOT=~/cuda\r\n#export LIBRARY_PATH=$CUDA_ROOT/lib:$CUDA_ROOT/lib64:$LIBRARY_PATH\r\n\r\n\r\n####\r\n# INSTALLING TENSORFLOW-GPU FOR PYTHON 3\r\n# pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-1.0.0-py3-none-any.whl\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/Users/jenniferstark/cuda/lib\"\r\nexport CUDA_HOME=/Users/jenniferstark/local/cuda\r\n```\r\n\r\nThank you in advance!!!", "In my macbookpro, I expanded my `DYLD_LIBRARY_PATH` as follows to get around this issue.\r\ncant verify further as the macbook I tried this on is now a brick, but I hope this helps:\r\n```\r\nexport DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-8.0:/Developer/NVIDIA/CUDA-8.0/lib:/usr/local/cuda:/usr/local/cuda/lib:$DYLD_LIBRARY_PATH\r\n```\r\n\r\nWe seem to be resolving some libraries with the `lib` in the path and some without.\r\nThat seems to be causing the problem.", "@gunan thank you so much! My `.bash_profile` now reads\r\n```\r\nexport DYLD_LIBRARY_PATH=/Developer/NVIDIA/CUDA-8.0:/Developer/NVIDIA/CUDA-8.0/lib:/usr/local/cuda:/usr/local/cuda/lib:$DYLD_LIBRARY_PATH \r\n\r\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/Users/jenniferstark/cuda/lib\"\r\nexport CUDA_HOME=/Users/jenniferstark/cuda\r\n```\r\n**keras** and **tensorflow** now each import successfully. Running \r\n```\r\nimport tensorflow as tf\r\n\r\n# Creates a graph.\r\na = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\r\nb = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\r\nc = tf.matmul(a, b)\r\n\r\n# Creates a session with log_device_placement set to True.\r\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\n\r\n# Runs the op.\r\nprint sess.run(c)\r\n``` \r\nalso _seems_ to do what it should, as far as I can tell. **However**, now when I try to run my keras cnn model, I run into an issue with scipy.linalg, which is called in [Keras' image preprocessing code](https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py):\r\n```\r\n(bigfish) jenniferstark@Jennifers-MacBook-Pro:~/Documents/Repositories/kaggle/bigfish/src/models$ python train_keras_model.py \r\nUsing TensorFlow backend.\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.5.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.8.0.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.1.dylib locally\r\nI tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.8.0.dylib locally\r\nTraceback (most recent call last):\r\n  File \"train_keras_model.py\", line 5, in <module>\r\n    from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/preprocessing/image.py\", line 10, in <module>\r\n    from scipy import linalg\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/scipy/linalg/__init__.py\", line 175, in <module>\r\n    from .misc import *\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/scipy/linalg/misc.py\", line 5, in <module>\r\n    from .blas import get_blas_funcs\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/scipy/linalg/blas.py\", line 155, in <module>\r\n    from scipy.linalg import _fblas\r\nImportError: dlopen(/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/scipy/linalg/_fblas.cpython-35m-darwin.so, 2): Library not loaded: /usr/local/lib/libgcc_s.1.dylib\r\n  Referenced from: /Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/scipy/linalg/_fblas.cpython-35m-darwin.so\r\n  Reason: image not found\r\n```\r\n**While** in Spyder, I still get what appears to be the _original_ error:\r\n```\r\nimport keras\r\nUsing TensorFlow backend.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-4-c74e2bd4ca71>\", line 1, in <module>\r\n    import keras\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/__init__.py\", line 3, in <module>\r\n    from . import activations\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/activations.py\", line 3, in <module>\r\n    from . import backend as K\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/backend/__init__.py\", line 64, in <module>\r\n    from .tensorflow_backend import *\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\n\r\nImportError: Traceback (most recent call last):\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\r\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/imp.py\", line 242, in load_module\r\n    return load_dynamic(name, filename, file)\r\n  File \"/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/imp.py\", line 342, in load_dynamic\r\n    return _load(spec)\r\nImportError: dlopen(/Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so, 10): Library not loaded: @rpath/libcudart.8.0.dylib\r\n  Referenced from: /Users/jenniferstark/anaconda/envs/bigfish/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n  Reason: image not found\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n```\r\n\r\nSo now I am unsure whether this issue is related to TensorFlow, Keras, or Scipy \ud83d\ude1f \r\n\r\nAgain, thank you in advance \ud83e\udd16 ", "```\r\n>>> keras.__version__\r\n'2.0.1'\r\n>>> scipy.__version__\r\n'0.19.0'\r\n>>> tensorflow.__version__\r\n'1.0.1'\r\n```", "Ok, I downgraded scipy based on [this](http://stackoverflow.com/questions/38854513/conda-python-import-error-image-not-found-in-jupyter-notebook-only)\r\n```\r\n conda install scipy=0.17.0\r\n```\r\nwhich seems to have helped (at least in the terminal. Spyder still displays original error).\r\nNow I get memory issues, which I think are unrelated to this original issue, so I'm going to close this issue. \r\n\r\nThanks \ud83d\udc6f\u200d\u2642\ufe0f "]}, {"number": 8912, "title": "Make tensorflow pip install work in dist_test Dockerfile", "body": "Recently added dependencies of the pip package no longer work with the\r\n--no-install-recommended flag of apt-get install python python-pip.", "comments": []}, {"number": 8910, "title": "regarding the ValueError: inputs must be a list of at least one Tensor with the same dtype and shape", "body": "There is a program that defines the loss function as follows:\r\n```\r\nreg_loss_col = tf.GraphKeys.REGULARIZATION_LOSSES\r\nweight_loss = tf.add_n(tf.get_collection(reg_loss_col),name='reg_loss')\r\n\r\n```\r\n\r\nRunning the program raises the following error message\r\n\r\n> File \"/home/ decoder/kitti_multiloss.py\", line 86, in loss\r\n>     name='reg_loss')\r\n>   File \"/devl /tensorflow/tf_0.12/lib/python3.4/site-packages/tensorflow/python/ops/math_ops.py\", line 1827, in add_n\r\n>     raise ValueError(\"inputs must be a list of at least one Tensor with the \"\r\n> ValueError: inputs must be a list of at least one Tensor with the same dtype and shape\r\n> \r\n\r\nI am curious how to print out the tensor information of the first parameter `tf.get_collection(reg_loss_col)` in `tf.add_n`, so that I can figure out why this cause the error.", "comments": ["This seems to be a program you have written, so this question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!\r\n\r\nBut the error basically means that the collection with key `tf.GraphKeys.REGULARIZATION_LOSSES` has no members. If you cannot find the source of this, be sure to post the full source of the program to stackoverflow as it is impossible to help you with some tiny fragment of the program. Thanks!\r\n"]}, {"number": 8909, "title": "Polish weighted_sparse_column doc", "body": "This is necessary otherwise `Returns` and `Raises` sections will be included as part of argument `dtype`'s description. ", "comments": []}, {"number": 8908, "title": "Bijectors in contrib.distributions not available", "body": "In 1.1.0rc0 and beyond, all bijectors are moved into a subdirectory in contrib.distributions. This seems to make it unavailable?\r\n```python\r\nds = tf.contrib.distributions\r\nds.bijectors\r\n## Traceback (most recent call last):\r\n##   File \"<stdin>\", line 1, in <module>\r\n## AttributeError: 'module' object has no attribute 'bijectors'\r\n\r\nds.Bijector\r\n## Traceback (most recent call last):\r\n##   File \"<stdin>\", line 1, in <module>\r\n## AttributeError: 'module' object has no attribute 'Bijector'\r\n```\r\nIn my install location of `/Users/dvt/Envs/tf1.1/lib/python2.7/site-packages/tensorflow/contrib/distributions`, it seems like it would be imported, as the following line exists:\r\n```python\r\nfrom tensorflow.contrib.distributions.python.ops.bijectors import *\r\n```\r\nHowever, inspecting the `ds` submodule, no bijectors are available.\r\n```python\r\nds.__dict__.keys()\r\n## ['Deterministic', '__path__', 'QuantizedDistribution', 'softplus_inverse', 'Mixture', 'ExpRelaxedOneHotCategorical', 'ConditionalTransformedDistribution', 'Exponential', 'ConditionalDistribution', '__file__', 'StudentTWithAbsDfSoftplusScale', 'RelaxedOneHotCategorical', 'StudentT', 'ReparameterizationType', 'Categorical', 'MultivariateNormalTriL', 'VectorDeterministic', 'TransformedDistribution', 'Chi2', 'MultivariateNormalDiagPlusLowRank', '_allowed_symbols', 'Gamma', 'normal_conjugates_known_scale_predictive', '__builtins__', 'WishartFull', '__name__', 'Normal', 'ExponentialWithSoftplusRate', 'InverseGamma', 'WishartCholesky', 'Distribution', 'RelaxedBernoulli', 'Bernoulli', 'Beta', 'Binomial', '__doc__', 'BetaWithSoftplusConcentration', 'MultivariateNormalDiagWithSoftplusScale', 'NOT_REPARAMETERIZED', 'BernoulliWithSigmoidProbs', 'Laplace', 'GammaWithSoftplusConcentrationRate', 'Poisson', 'FULLY_REPARAMETERIZED', 'DirichletMultinomial', 'matrix_diag_transform', 'RegisterKL', 'InverseGammaWithSoftplusConcentrationRate', 'Uniform', 'NegativeBinomial', 'Geometric', 'LaplaceWithSoftplusScale', '__package__', 'Dirichlet', 'MultivariateNormalDiag', 'Logistic', 'Chi2WithAbsDf', 'NormalWithSoftplusScale', 'normal_conjugates_known_scale_posterior', 'kl', 'Multinomial', 'OneHotCategorical']\r\n```\r\n", "comments": ["Please take  look @ebrevdo ", "A workaround:\r\n```\r\nimport sys\r\nsys.path.insert(0, tf.__path__[0] + '/contrib/distributions/python/ops')\r\nimport bijectors as bijector\r\n```", "Does this work?\n\nfrom tensorflow.contrib.distributions.python.ops import bijectors as bijector\n\n\nOn Jun 9, 2017 5:01 AM, \"Frans Zdyb\" <notifications@github.com> wrote:\n\n> A workaround:\n>\n> import sys\n> sys.path.insert(0, tf.__path__[0] + '/contrib/distributions/python/ops')\n> import bijectors as bijector\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8908#issuecomment-307370582>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimyLxAW1y0VDzP1jgldQOl1laQBx0ks5sCTQygaJpZM4Mw2Q1>\n> .\n>\n", "Yes.", "Use that for now.  We're shuffling things around to get part of the code\ninto core and I think some of these issues will be fixed then.\n\nOn Jun 12, 2017 1:17 PM, \"Dustin Tran\" <notifications@github.com> wrote:\n\n> Yes.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/8908#issuecomment-307911394>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim3cEoPPG0CWNdLN4pxV9y_dkAUsMks5sDZzFgaJpZM4Mw2Q1>\n> .\n>\n"]}, {"number": 8907, "title": "corrected which graph_def to validate", "body": "After each transformation comes a check for validity.\r\nmerged_graph_def was already tested a few lines above.\r\noutput_graph_def should be tested directly afterwards.", "comments": ["Can one of the admins verify this patch?", "Pete... this does look okay to me, but just wanted to double check.", "@tensorflow-jenkins test this please", "@tensorflow-jenkins test this please"]}]