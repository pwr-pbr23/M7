[{"number": 44844, "title": "Confusing results from tf.math.mod. An issue with precision?", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MACOS 10.14.5\r\n- TensorFlow installed from (source or binary): installed using pip\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.7\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n```\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n```\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am attempting to use tensorflow to compute the modulus of two numbers and seeing some strange behaviour that seems to be related to the precision being used.\r\n\r\nI have attached an example block of code and two example outputs below.\r\nThe example code simply computes the modulus of the two input numbers (x, y) and as you can see, depending on the precision that the inputs are cast to the output can be quite different.\r\n\r\nAs a cross check the code example also computes the modulus using numpy (and casts to the equivalent? dtypes).\r\nI find that the higher precision numpy results are much closer to the true answer.\r\n\r\nInterestingly, we find that the results for\r\n numpy.float16 and tf.float16 agree\r\n numpy.float32 and tf.float32 agree\r\n\r\nbut\r\nnumpy.float64 and tf.float64 *disagree*\r\n\r\nInstead tf.float32 and tf.float64 actually agree with each other though. Does this point to a bug with the tf.float64?\r\n\r\nThanks in advance!\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef example_for_git(x, y):\r\n    print(\"tensorflow\\n\")\r\n    print(f\"tf fl16: {tf.math.mod(tf.cast(x, tf.float16), tf.cast(y, tf.float16)).numpy()}\")\r\n    print(f\"tf fl32: {tf.math.mod(tf.cast(x, tf.float32), tf.cast(y, tf.float32)).numpy()}\")\r\n    print(f\"tf fl64: {tf.math.mod(tf.cast(x, tf.float64), tf.cast(y, tf.float64)).numpy()}\")\r\n    print(\"\\nnumpy\\n\")\r\n    print(f\"np fl16: {np.mod(np.float16(x), np.float16(y))}\")\r\n    print(f\"np fl32: {np.mod(np.float32(x), np.float32(y))}\")\r\n    print(f\"np fl64: {np.mod(np.float64(x), np.float64(y))}\")\r\n    print(f\"np fl128: {np.mod(np.float128(x), np.float128(y))}\")\r\n\r\nexample_for_git(80002.2, 1)\r\n\"\"\" output: the expected result of mod(80002.2, 1) is 0.2\r\ntensorflow\r\n\r\ntf fl16: nan\r\ntf fl32: 0.203125\r\ntf fl64: 0.203125\r\n\r\nnumpy\r\n\r\nnp fl16: nan\r\nnp fl32: 0.203125\r\nnp fl64: 0.19999999999708962\r\nnp fl128: 0.19999999999708962\r\n\"\"\"\r\n\r\n\r\nexample_for_git(851839.8270638183, 2*np.pi)\r\n\"\"\" output\r\ntensorflow\r\n\r\ntf fl16: nan\r\ntf fl32: 3.2239599227905273\r\ntf fl64: 3.2239599227905273\r\n\r\nnumpy\r\n\r\nnp fl16: nan\r\nnp fl32: 3.2239599227905273\r\nnp fl64: 3.262228253121961\r\nnp fl128: 3.262228253121961\r\n\"\"\"\r\n```\r\n\r\n\r\n", "comments": ["See the difference with `example_for_git(851839.8270638183, 2*np.pi)`\r\n```\r\n    print(tf.cast(x, tf.float64))\r\n    print(tf.convert_to_tensor(x, tf.float64))\r\n    print(np.float64(x))\r\n```\r\n", "Hi @bhack, thanks for replying to me!\r\n\r\nI tested this and using `tf.convert_to_tensor(x, tf.float64)` worked!!\r\n\r\nI guess I will just always use `tf.convert_to_tensor` for everything instead of `tf.cast` then...\r\n\r\nIs this the desired behaviour for tf.cast or is it a bug?\r\n\r\nThanks again!\r\n\r\n", "There was a related thread about convert/cast at https://github.com/tensorflow/tensorflow/issues/35938\nhttps://github.com/GPflow/GPflow/pull/1213", "See the specific @josh11b TODO in `tf.cast` at:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/ops/math_ops.py#L916-L919\r\n\r\n@Cyberface Do you want to try to open a PR for this TODO?", "@bhack I have a first attempt at solving this issue here https://github.com/Cyberface/tensorflow/pull/1\r\n\r\nIs there an easy way to install my fork of tensorflow to test the code?", "See https://stackoverflow.com/questions/34204551/run-tensorflow-unit-tests\n\nIf you don't want to compile probably the fast way is to open a PR so that It will be tested by TF CI", "OK thanks! I've opened a PR and will look into compiling from source", "@Cyberface \r\n\r\nAny update on the issue please. Thanks!", "The PR was at https://github.com/tensorflow/tensorflow/pull/44902", "Sorry, I am unable to help on this anymore!", "Was able to reproduce your issue in Tensorflow 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/8af6098353550dc46b03596625e069ee/44844.ipynb). Thanks!", "As hinted above, this is unrelated to `tf.math.mod`, and is more an issue of converting numpy arrays to tensors.\r\n\r\nEnsuring the input tensors are of the correct type (e.g. `tf.constant(x, tf.float64)`) resolves the issue.", "Could you please move this issue to closed as per the comment above. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44844\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44844\">No</a>\n"]}, {"number": 44843, "title": "ValueError: Checkpoint version should be V2", "body": "GPU: NVIDIA GEFORCE RTX 2060\r\nCPU: 16GB RAM, 6 processor cores\r\nTensorFlow: 2.3.1\r\nPython: 3.8.6\r\nCUDA: 10.1\r\ncuDNN: 7.6\r\n\r\nI am training a Mask R-CNN Inception ResNet V2 1024x1024 algorithm, as downloaded from the [TensorFlow 2 Detection Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) . I am training this algorithm on my custom dataset, which I have labeled using [Label-img](https://github.com/tzutalin/labelImg) . When I train the model using the Anaconda command ```python model_main_tf2.py --model_dir=models/my_faster_rcnn --pipeline_config_path=models/my_faster_rcnn/pipeline.config```, I get the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"model_main_tf2.py\", line 113, in <module>\r\n    tf.compat.v1.app.run()\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\absl\\app.py\", line 303, in run\r\n    _run_main(main, args)\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"model_main_tf2.py\", line 104, in main\r\n    model_lib_v2.train_loop(\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 564, in train_loop\r\n    load_fine_tune_checkpoint(detection_model,\r\n  File \"C:\\user\\anaconda3\\envs\\object_detection_api\\lib\\site-packages\\object_detection\\model_lib_v2.py\", line 348, in load_fine_tune_checkpoint\r\n    raise ValueError('Checkpoint version should be V2')\r\nValueError: Checkpoint version should be V2\r\n```\r\n\r\n**How can I resolve this error?** Below are the scripts referenced in the error:\r\n\r\nmodel_main_tf2.py\r\n```\r\n# Lint as: python3\r\n# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\nr\"\"\"Creates and runs TF2 object detection models.\r\n\r\nFor local training/evaluation run:\r\nPIPELINE_CONFIG_PATH=path/to/pipeline.config\r\nMODEL_DIR=/tmp/model_outputs\r\nNUM_TRAIN_STEPS=10000\r\nSAMPLE_1_OF_N_EVAL_EXAMPLES=1\r\npython model_main_tf2.py -- \\\r\n  --model_dir=$MODEL_DIR --num_train_steps=$NUM_TRAIN_STEPS \\\r\n  --sample_1_of_n_eval_examples=$SAMPLE_1_OF_N_EVAL_EXAMPLES \\\r\n  --pipeline_config_path=$PIPELINE_CONFIG_PATH \\\r\n  --alsologtostderr\r\n\"\"\"\r\nfrom absl import flags\r\nimport tensorflow.compat.v2 as tf\r\nfrom object_detection import model_lib_v2\r\n\r\nflags.DEFINE_string('pipeline_config_path', None, 'Path to pipeline config '\r\n                    'file.')\r\nflags.DEFINE_integer('num_train_steps', None, 'Number of train steps.')\r\nflags.DEFINE_bool('eval_on_train_data', False, 'Enable evaluating on train '\r\n                  'data (only supported in distributed training).')\r\nflags.DEFINE_integer('sample_1_of_n_eval_examples', None, 'Will sample one of '\r\n                     'every n eval input examples, where n is provided.')\r\nflags.DEFINE_integer('sample_1_of_n_eval_on_train_examples', 5, 'Will sample '\r\n                     'one of every n train input examples for evaluation, '\r\n                     'where n is provided. This is only used if '\r\n                     '`eval_training_data` is True.')\r\nflags.DEFINE_string(\r\n    'model_dir', None, 'Path to output model directory '\r\n                       'where event and checkpoint files will be written.')\r\nflags.DEFINE_string(\r\n    'checkpoint_dir', None, 'Path to directory holding a checkpoint.  If '\r\n    '`checkpoint_dir` is provided, this binary operates in eval-only mode, '\r\n    'writing resulting metrics to `model_dir`.')\r\n\r\nflags.DEFINE_integer('eval_timeout', 3600, 'Number of seconds to wait for an'\r\n                     'evaluation checkpoint before exiting.')\r\n\r\nflags.DEFINE_bool('use_tpu', False, 'Whether the job is executing on a TPU.')\r\nflags.DEFINE_string(\r\n    'tpu_name',\r\n    default=None,\r\n    help='Name of the Cloud TPU for Cluster Resolvers.')\r\nflags.DEFINE_integer(\r\n    'num_workers', 1, 'When num_workers > 1, training uses '\r\n    'MultiWorkerMirroredStrategy. When num_workers = 1 it uses '\r\n    'MirroredStrategy.')\r\nflags.DEFINE_integer(\r\n    'checkpoint_every_n', 1000, 'Integer defining how often we checkpoint.')\r\nflags.DEFINE_boolean('record_summaries', True,\r\n                     ('Whether or not to record summaries during'\r\n                      ' training.'))\r\n\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef main(unused_argv):\r\n  flags.mark_flag_as_required('model_dir')\r\n  flags.mark_flag_as_required('pipeline_config_path')\r\n  tf.config.set_soft_device_placement(True)\r\n\r\n  if FLAGS.checkpoint_dir:\r\n    model_lib_v2.eval_continuously(\r\n        pipeline_config_path=FLAGS.pipeline_config_path,\r\n        model_dir=FLAGS.model_dir,\r\n        train_steps=FLAGS.num_train_steps,\r\n        sample_1_of_n_eval_examples=FLAGS.sample_1_of_n_eval_examples,\r\n        sample_1_of_n_eval_on_train_examples=(\r\n            FLAGS.sample_1_of_n_eval_on_train_examples),\r\n        checkpoint_dir=FLAGS.checkpoint_dir,\r\n        wait_interval=300, timeout=FLAGS.eval_timeout)\r\n  else:\r\n    if FLAGS.use_tpu:\r\n      # TPU is automatically inferred if tpu_name is None and\r\n      # we are running under cloud ai-platform.\r\n      resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\r\n          FLAGS.tpu_name)\r\n      tf.config.experimental_connect_to_cluster(resolver)\r\n      tf.tpu.experimental.initialize_tpu_system(resolver)\r\n      strategy = tf.distribute.experimental.TPUStrategy(resolver)\r\n    elif FLAGS.num_workers > 1:\r\n      strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    else:\r\n      strategy = tf.compat.v2.distribute.MirroredStrategy()\r\n\r\n    with strategy.scope():\r\n      model_lib_v2.train_loop(\r\n          pipeline_config_path=FLAGS.pipeline_config_path,\r\n          model_dir=FLAGS.model_dir,\r\n          train_steps=FLAGS.num_train_steps,\r\n          use_tpu=FLAGS.use_tpu,\r\n          checkpoint_every_n=FLAGS.checkpoint_every_n,\r\n          record_summaries=FLAGS.record_summaries)\r\n\r\nif __name__ == '__main__':\r\n  tf.compat.v1.app.run()\r\n```\r\n\r\npipeline.config file:\r\n```\r\n# Mask R-CNN with Inception Resnet v2 (no atrous)\r\n# Sync-trained on COCO (with 8 GPUs) with batch size 16 (1024x1024 resolution)\r\n# Initialized from Imagenet classification checkpoint\r\n# TF2-Compatible, *Not* TPU-Compatible\r\n#\r\n# Achieves XXX mAP on COCO\r\n\r\nmodel {\r\n  faster_rcnn {\r\n    number_of_stages: 3\r\n    num_classes: 1\r\n    image_resizer {\r\n      fixed_shape_resizer {\r\n        height: 1024\r\n        width: 1024\r\n        # pad_to_max_dimension: true\r\n      }\r\n    }\r\n    feature_extractor {\r\n      type: 'faster_rcnn_inception_resnet_v2_keras'\r\n    }\r\n    first_stage_anchor_generator {\r\n      grid_anchor_generator {\r\n        scales: [0.25, 0.5, 1.0, 2.0]\r\n        aspect_ratios: [0.5, 1.0, 2.0]\r\n        height_stride: 16\r\n        width_stride: 16\r\n      }\r\n    }\r\n    first_stage_box_predictor_conv_hyperparams {\r\n      op: CONV\r\n      regularizer {\r\n        l2_regularizer {\r\n          weight: 0.0\r\n        }\r\n      }\r\n      initializer {\r\n        truncated_normal_initializer {\r\n          stddev: 0.01\r\n        }\r\n      }\r\n    }\r\n    first_stage_nms_score_threshold: 0.0\r\n    first_stage_nms_iou_threshold: 0.7\r\n    first_stage_max_proposals: 300\r\n    first_stage_localization_loss_weight: 2.0\r\n    first_stage_objectness_loss_weight: 1.0\r\n    initial_crop_size: 17\r\n    maxpool_kernel_size: 1\r\n    maxpool_stride: 1\r\n    second_stage_box_predictor {\r\n      mask_rcnn_box_predictor {\r\n        use_dropout: false\r\n        dropout_keep_probability: 1.0\r\n        fc_hyperparams {\r\n          op: FC\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            variance_scaling_initializer {\r\n              factor: 1.0\r\n              uniform: true\r\n              mode: FAN_AVG\r\n            }\r\n          }\r\n        }\r\n        mask_height: 33\r\n        mask_width: 33\r\n        mask_prediction_conv_depth: 0\r\n        mask_prediction_num_conv_layers: 4\r\n        conv_hyperparams {\r\n          op: CONV\r\n          regularizer {\r\n            l2_regularizer {\r\n              weight: 0.0\r\n            }\r\n          }\r\n          initializer {\r\n            truncated_normal_initializer {\r\n              stddev: 0.01\r\n            }\r\n          }\r\n        }\r\n        predict_instance_masks: true\r\n      }\r\n    }\r\n    second_stage_post_processing {\r\n      batch_non_max_suppression {\r\n        score_threshold: 0.0\r\n        iou_threshold: 0.6\r\n        max_detections_per_class: 100\r\n        max_total_detections: 100\r\n      }\r\n      score_converter: SOFTMAX\r\n    }\r\n    second_stage_localization_loss_weight: 2.0\r\n    second_stage_classification_loss_weight: 1.0\r\n    second_stage_mask_prediction_loss_weight: 4.0\r\n    resize_masks: false\r\n  }\r\n}\r\n\r\ntrain_config: {\r\n  batch_size: 1\r\n  num_steps: 200000\r\n  optimizer {\r\n    momentum_optimizer: {\r\n      learning_rate: {\r\n        cosine_decay_learning_rate {\r\n          learning_rate_base: 0.008\r\n          total_steps: 200000\r\n          warmup_learning_rate: 0.0\r\n          warmup_steps: 5000\r\n        }\r\n      }\r\n      momentum_optimizer_value: 0.9\r\n    }\r\n    use_moving_average: false\r\n  }\r\n  gradient_clipping_by_norm: 10.0\r\n  fine_tune_checkpoint: \"pre-trained-models/mask_rcnn_inception_resnet_v2_1024x1024_coco17_gpu-8/checkpoint/ckpt-0\"\r\n  data_augmentation_options {\r\n    random_horizontal_flip {\r\n    }\r\n  }\r\n}\r\n\r\ntrain_input_reader: {\r\n  tf_record_input_reader {\r\n    input_path: \"annotations/train.record\"\r\n  }\r\n  load_instance_masks: true\r\n  mask_type: PNG_MASKS\r\n}\r\n\r\neval_config: {\r\n  metrics_set: \"coco_detection_metrics\"\r\n  metrics_set: \"coco_mask_metrics\"\r\n  eval_instance_masks: true\r\n  use_moving_averages: false\r\n  batch_size: 1\r\n  include_metrics_per_category: true\r\n}\r\n\r\neval_input_reader: {\r\n  label_map_path: \"annotations/label_map.pbtxt\"\r\n  shuffle: false\r\n  num_epochs: 1\r\n  tf_record_input_reader {\r\n    input_path: \"annotations/test.record\"\r\n  }\r\n  load_instance_masks: true\r\n  mask_type: PNG_MASKS\r\n}\r\n```\r\n\r\napp.py (first reference):\r\n```\r\n# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\n\r\n\"\"\"Generic entry point script.\"\"\"\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport sys as _sys\r\n\r\nfrom absl.app import run as _run\r\n\r\nfrom tensorflow.python.platform import flags\r\nfrom tensorflow.python.util.tf_export import tf_export\r\n\r\n\r\ndef _parse_flags_tolerate_undef(argv):\r\n  \"\"\"Parse args, returning any unknown flags (ABSL defaults to crashing).\"\"\"\r\n  return flags.FLAGS(_sys.argv if argv is None else argv, known_only=True)\r\n\r\n\r\n@tf_export(v1=['app.run'])\r\ndef run(main=None, argv=None):\r\n  \"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\r\n\r\n  main = main or _sys.modules['__main__'].main\r\n\r\n  _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n```\r\n\r\napp.py (second and third reference):\r\n```\r\n# Copyright 2017 The Abseil Authors.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#      http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n\"\"\"Generic entry point for Abseil Python applications.\r\n\r\nTo use this module, define a 'main' function with a single 'argv' argument and\r\ncall app.run(main). For example:\r\n\r\n    def main(argv):\r\n      if len(argv) > 1:\r\n        raise app.UsageError('Too many command-line arguments.')\r\n\r\n    if __name__ == '__main__':\r\n      app.run(main)\r\n\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport collections\r\nimport errno\r\nimport os\r\nimport pdb\r\nimport sys\r\nimport traceback\r\n\r\nfrom absl import command_name\r\nfrom absl import flags\r\nfrom absl import logging\r\n\r\ntry:\r\n  import faulthandler\r\nexcept ImportError:\r\n  faulthandler = None\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nflags.DEFINE_boolean('run_with_pdb', False, 'Set to true for PDB debug mode')\r\nflags.DEFINE_boolean('pdb_post_mortem', False,\r\n                     'Set to true to handle uncaught exceptions with PDB '\r\n                     'post mortem.')\r\nflags.DEFINE_alias('pdb', 'pdb_post_mortem')\r\nflags.DEFINE_boolean('run_with_profiling', False,\r\n                     'Set to true for profiling the script. '\r\n                     'Execution will be slower, and the output format might '\r\n                     'change over time.')\r\nflags.DEFINE_string('profile_file', None,\r\n                    'Dump profile information to a file (for python -m '\r\n                    'pstats). Implies --run_with_profiling.')\r\nflags.DEFINE_boolean('use_cprofile_for_profiling', True,\r\n                     'Use cProfile instead of the profile module for '\r\n                     'profiling. This has no effect unless '\r\n                     '--run_with_profiling is set.')\r\nflags.DEFINE_boolean('only_check_args', False,\r\n                     'Set to true to validate args and exit.',\r\n                     allow_hide_cpp=True)\r\n\r\n\r\n# If main() exits via an abnormal exception, call into these\r\n# handlers before exiting.\r\nEXCEPTION_HANDLERS = []\r\n\r\n\r\nclass Error(Exception):\r\n  pass\r\n\r\n\r\nclass UsageError(Error):\r\n  \"\"\"Exception raised when the arguments supplied by the user are invalid.\r\n\r\n  Raise this when the arguments supplied are invalid from the point of\r\n  view of the application. For example when two mutually exclusive\r\n  flags have been supplied or when there are not enough non-flag\r\n  arguments. It is distinct from flags.Error which covers the lower\r\n  level of parsing and validating individual flags.\r\n  \"\"\"\r\n\r\n  def __init__(self, message, exitcode=1):\r\n    super(UsageError, self).__init__(message)\r\n    self.exitcode = exitcode\r\n\r\n\r\nclass HelpFlag(flags.BooleanFlag):\r\n  \"\"\"Special boolean flag that displays usage and raises SystemExit.\"\"\"\r\n  NAME = 'help'\r\n  SHORT_NAME = '?'\r\n\r\n  def __init__(self):\r\n    super(HelpFlag, self).__init__(\r\n        self.NAME, False, 'show this help',\r\n        short_name=self.SHORT_NAME, allow_hide_cpp=True)\r\n\r\n  def parse(self, arg):\r\n    if self._parse(arg):\r\n      usage(shorthelp=True, writeto_stdout=True)\r\n      # Advertise --helpfull on stdout, since usage() was on stdout.\r\n      print()\r\n      print('Try --helpfull to get a list of all flags.')\r\n      sys.exit(1)\r\n\r\n\r\nclass HelpshortFlag(HelpFlag):\r\n  \"\"\"--helpshort is an alias for --help.\"\"\"\r\n  NAME = 'helpshort'\r\n  SHORT_NAME = None\r\n\r\n\r\nclass HelpfullFlag(flags.BooleanFlag):\r\n  \"\"\"Display help for flags in the main module and all dependent modules.\"\"\"\r\n\r\n  def __init__(self):\r\n    super(HelpfullFlag, self).__init__(\r\n        'helpfull', False, 'show full help', allow_hide_cpp=True)\r\n\r\n  def parse(self, arg):\r\n    if self._parse(arg):\r\n      usage(writeto_stdout=True)\r\n      sys.exit(1)\r\n\r\n\r\nclass HelpXMLFlag(flags.BooleanFlag):\r\n  \"\"\"Similar to HelpfullFlag, but generates output in XML format.\"\"\"\r\n\r\n  def __init__(self):\r\n    super(HelpXMLFlag, self).__init__(\r\n        'helpxml', False, 'like --helpfull, but generates XML output',\r\n        allow_hide_cpp=True)\r\n\r\n  def parse(self, arg):\r\n    if self._parse(arg):\r\n      flags.FLAGS.write_help_in_xml_format(sys.stdout)\r\n      sys.exit(1)\r\n\r\n\r\ndef parse_flags_with_usage(args):\r\n  \"\"\"Tries to parse the flags, print usage, and exit if unparseable.\r\n\r\n  Args:\r\n    args: [str], a non-empty list of the command line arguments including\r\n        program name.\r\n\r\n  Returns:\r\n    [str], a non-empty list of remaining command line arguments after parsing\r\n    flags, including program name.\r\n  \"\"\"\r\n  try:\r\n    return FLAGS(args)\r\n  except flags.Error as error:\r\n    sys.stderr.write('FATAL Flags parsing error: %s\\n' % error)\r\n    sys.stderr.write('Pass --helpshort or --helpfull to see help on flags.\\n')\r\n    sys.exit(1)\r\n\r\n\r\n_define_help_flags_called = False\r\n\r\n\r\ndef define_help_flags():\r\n  \"\"\"Registers help flags. Idempotent.\"\"\"\r\n  # Use a global to ensure idempotence.\r\n  global _define_help_flags_called\r\n\r\n  if not _define_help_flags_called:\r\n    flags.DEFINE_flag(HelpFlag())\r\n    flags.DEFINE_flag(HelpshortFlag())  # alias for --help\r\n    flags.DEFINE_flag(HelpfullFlag())\r\n    flags.DEFINE_flag(HelpXMLFlag())\r\n    _define_help_flags_called = True\r\n\r\n\r\ndef _register_and_parse_flags_with_usage(\r\n    argv=None,\r\n    flags_parser=parse_flags_with_usage,\r\n):\r\n  \"\"\"Registers help flags, parses arguments and shows usage if appropriate.\r\n\r\n  This also calls sys.exit(0) if flag --only_check_args is True.\r\n\r\n  Args:\r\n    argv: [str], a non-empty list of the command line arguments including\r\n        program name, sys.argv is used if None.\r\n    flags_parser: Callable[[List[Text]], Any], the function used to parse flags.\r\n        The return value of this function is passed to `main` untouched.\r\n        It must guarantee FLAGS is parsed after this function is called.\r\n\r\n  Returns:\r\n    The return value of `flags_parser`. When using the default `flags_parser`,\r\n    it returns the following:\r\n    [str], a non-empty list of remaining command line arguments after parsing\r\n    flags, including program name.\r\n\r\n  Raises:\r\n    Error: Raised when flags_parser is called, but FLAGS is not parsed.\r\n    SystemError: Raised when it's called more than once.\r\n  \"\"\"\r\n  if _register_and_parse_flags_with_usage.done:\r\n    raise SystemError('Flag registration can be done only once.')\r\n\r\n  define_help_flags()\r\n\r\n  original_argv = sys.argv if argv is None else argv\r\n  args_to_main = flags_parser(original_argv)\r\n  if not FLAGS.is_parsed():\r\n    raise Error('FLAGS must be parsed after flags_parser is called.')\r\n\r\n  # Exit when told so.\r\n  if FLAGS.only_check_args:\r\n    sys.exit(0)\r\n  # Immediately after flags are parsed, bump verbosity to INFO if the flag has\r\n  # not been set.\r\n  if FLAGS['verbosity'].using_default_value:\r\n    FLAGS.verbosity = 0\r\n  _register_and_parse_flags_with_usage.done = True\r\n\r\n  return args_to_main\r\n\r\n_register_and_parse_flags_with_usage.done = False\r\n\r\n\r\ndef _run_main(main, argv):\r\n  \"\"\"Calls main, optionally with pdb or profiler.\"\"\"\r\n  if FLAGS.run_with_pdb:\r\n    sys.exit(pdb.runcall(main, argv))\r\n  elif FLAGS.run_with_profiling or FLAGS.profile_file:\r\n    # Avoid import overhead since most apps (including performance-sensitive\r\n    # ones) won't be run with profiling.\r\n    import atexit\r\n    if FLAGS.use_cprofile_for_profiling:\r\n      import cProfile as profile\r\n    else:\r\n      import profile\r\n    profiler = profile.Profile()\r\n    if FLAGS.profile_file:\r\n      atexit.register(profiler.dump_stats, FLAGS.profile_file)\r\n    else:\r\n      atexit.register(profiler.print_stats)\r\n    retval = profiler.runcall(main, argv)\r\n    sys.exit(retval)\r\n  else:\r\n    sys.exit(main(argv))\r\n\r\n\r\ndef _call_exception_handlers(exception):\r\n  \"\"\"Calls any installed exception handlers.\"\"\"\r\n  for handler in EXCEPTION_HANDLERS:\r\n    try:\r\n      if handler.wants(exception):\r\n        handler.handle(exception)\r\n    except:  # pylint: disable=bare-except\r\n      try:\r\n        # We don't want to stop for exceptions in the exception handlers but\r\n        # we shouldn't hide them either.\r\n        logging.error(traceback.format_exc())\r\n      except:  # pylint: disable=bare-except\r\n        # In case even the logging statement fails, ignore.\r\n        pass\r\n\r\n\r\ndef run(\r\n    main,\r\n    argv=None,\r\n    flags_parser=parse_flags_with_usage,\r\n):\r\n  \"\"\"Begins executing the program.\r\n\r\n  Args:\r\n    main: The main function to execute. It takes an single argument \"argv\",\r\n        which is a list of command line arguments with parsed flags removed.\r\n        The return value is passed to `sys.exit`, and so for example\r\n        a return value of 0 or None results in a successful termination, whereas\r\n        a return value of 1 results in abnormal termination.\r\n        For more details, see https://docs.python.org/3/library/sys#sys.exit\r\n    argv: A non-empty list of the command line arguments including program name,\r\n        sys.argv is used if None.\r\n    flags_parser: Callable[[List[Text]], Any], the function used to parse flags.\r\n        The return value of this function is passed to `main` untouched.\r\n        It must guarantee FLAGS is parsed after this function is called.\r\n  - Parses command line flags with the flag module.\r\n  - If there are any errors, prints usage().\r\n  - Calls main() with the remaining arguments.\r\n  - If main() raises a UsageError, prints usage and the error message.\r\n  \"\"\"\r\n  try:\r\n    args = _run_init(\r\n        sys.argv if argv is None else argv,\r\n        flags_parser,\r\n    )\r\n    while _init_callbacks:\r\n      callback = _init_callbacks.popleft()\r\n      callback()\r\n    try:\r\n      _run_main(main, args)\r\n    except UsageError as error:\r\n      usage(shorthelp=True, detailed_error=error, exitcode=error.exitcode)\r\n    except:\r\n      exc = sys.exc_info()[1]\r\n      # Don't try to post-mortem debug successful SystemExits, since those\r\n      # mean there wasn't actually an error. In particular, the test framework\r\n      # raises SystemExit(False) even if all tests passed.\r\n      if isinstance(exc, SystemExit) and not exc.code:\r\n        raise\r\n\r\n      # Check the tty so that we don't hang waiting for input in an\r\n      # non-interactive scenario.\r\n      if FLAGS.pdb_post_mortem and sys.stdout.isatty():\r\n        traceback.print_exc()\r\n        print()\r\n        print(' *** Entering post-mortem debugging ***')\r\n        print()\r\n        pdb.post_mortem()\r\n      raise\r\n  except Exception as e:\r\n    _call_exception_handlers(e)\r\n    raise\r\n\r\n# Callbacks which have been deferred until after _run_init has been called.\r\n_init_callbacks = collections.deque()\r\n\r\n\r\ndef call_after_init(callback):\r\n  \"\"\"Calls the given callback only once ABSL has finished initialization.\r\n\r\n  If ABSL has already finished initialization when `call_after_init` is\r\n  called then the callback is executed immediately, otherwise `callback` is\r\n  stored to be executed after `app.run` has finished initializing (aka. just\r\n  before the main function is called).\r\n\r\n  If called after `app.run`, this is equivalent to calling `callback()` in the\r\n  caller thread. If called before `app.run`, callbacks are run sequentially (in\r\n  an undefined order) in the same thread as `app.run`.\r\n\r\n  Args:\r\n    callback: a callable to be called once ABSL has finished initialization.\r\n      This may be immediate if initialization has already finished. It\r\n      takes no arguments and returns nothing.\r\n  \"\"\"\r\n  if _run_init.done:\r\n    callback()\r\n  else:\r\n    _init_callbacks.append(callback)\r\n\r\n\r\ndef _run_init(\r\n    argv,\r\n    flags_parser,\r\n):\r\n  \"\"\"Does one-time initialization and re-parses flags on rerun.\"\"\"\r\n  if _run_init.done:\r\n    return flags_parser(argv)\r\n  command_name.make_process_name_useful()\r\n  # Set up absl logging handler.\r\n  logging.use_absl_handler()\r\n  args = _register_and_parse_flags_with_usage(\r\n      argv=argv,\r\n      flags_parser=flags_parser,\r\n  )\r\n  if faulthandler:\r\n    try:\r\n      faulthandler.enable()\r\n    except Exception:  # pylint: disable=broad-except\r\n      # Some tests verify stderr output very closely, so don't print anything.\r\n      # Disabled faulthandler is a low-impact error.\r\n      pass\r\n  _run_init.done = True\r\n  return args\r\n\r\n\r\n_run_init.done = False\r\n\r\n\r\ndef usage(shorthelp=False, writeto_stdout=False, detailed_error=None,\r\n          exitcode=None):\r\n  \"\"\"Writes __main__'s docstring to stderr with some help text.\r\n\r\n  Args:\r\n    shorthelp: bool, if True, prints only flags from the main module,\r\n        rather than all flags.\r\n    writeto_stdout: bool, if True, writes help message to stdout,\r\n        rather than to stderr.\r\n    detailed_error: str, additional detail about why usage info was presented.\r\n    exitcode: optional integer, if set, exits with this status code after\r\n        writing help.\r\n  \"\"\"\r\n  if writeto_stdout:\r\n    stdfile = sys.stdout\r\n  else:\r\n    stdfile = sys.stderr\r\n\r\n  doc = sys.modules['__main__'].__doc__\r\n  if not doc:\r\n    doc = '\\nUSAGE: %s [flags]\\n' % sys.argv[0]\r\n    doc = flags.text_wrap(doc, indent='       ', firstline_indent='')\r\n  else:\r\n    # Replace all '%s' with sys.argv[0], and all '%%' with '%'.\r\n    num_specifiers = doc.count('%') - 2 * doc.count('%%')\r\n    try:\r\n      doc %= (sys.argv[0],) * num_specifiers\r\n    except (OverflowError, TypeError, ValueError):\r\n      # Just display the docstring as-is.\r\n      pass\r\n  if shorthelp:\r\n    flag_str = FLAGS.main_module_help()\r\n  else:\r\n    flag_str = FLAGS.get_help()\r\n  try:\r\n    stdfile.write(doc)\r\n    if flag_str:\r\n      stdfile.write('\\nflags:\\n')\r\n      stdfile.write(flag_str)\r\n    stdfile.write('\\n')\r\n    if detailed_error is not None:\r\n      stdfile.write('\\n%s\\n' % detailed_error)\r\n  except IOError as e:\r\n    # We avoid printing a huge backtrace if we get EPIPE, because\r\n    # \"foo.par --help | less\" is a frequent use case.\r\n    if e.errno != errno.EPIPE:\r\n      raise\r\n  if exitcode is not None:\r\n    sys.exit(exitcode)\r\n\r\n\r\nclass ExceptionHandler(object):\r\n  \"\"\"Base exception handler from which other may inherit.\"\"\"\r\n\r\n  def wants(self, exc):\r\n    \"\"\"Returns whether this handler wants to handle the exception or not.\r\n\r\n    This base class returns True for all exceptions by default. Override in\r\n    subclass if it wants to be more selective.\r\n\r\n    Args:\r\n      exc: Exception, the current exception.\r\n    \"\"\"\r\n    del exc  # Unused.\r\n    return True\r\n\r\n  def handle(self, exc):\r\n    \"\"\"Do something with the current exception.\r\n\r\n    Args:\r\n      exc: Exception, the current exception\r\n\r\n    This method must be overridden.\r\n    \"\"\"\r\n    raise NotImplementedError()\r\n\r\n\r\ndef install_exception_handler(handler):\r\n  \"\"\"Installs an exception handler.\r\n\r\n  Args:\r\n    handler: ExceptionHandler, the exception handler to install.\r\n\r\n  Raises:\r\n    TypeError: Raised when the handler was not of the correct type.\r\n\r\n  All installed exception handlers will be called if main() exits via\r\n  an abnormal exception, i.e. not one of SystemExit, KeyboardInterrupt,\r\n  FlagsError or UsageError.\r\n  \"\"\"\r\n  if not isinstance(handler, ExceptionHandler):\r\n    raise TypeError('handler of type %s does not inherit from ExceptionHandler'\r\n                    % type(handler))\r\n  EXCEPTION_HANDLERS.append(handler)\r\n```\r\n\r\nmodel_lib_v2.py:\r\n```\r\n# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n# ==============================================================================\r\nr\"\"\"Constructs model, inputs, and training environment.\"\"\"\r\n\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport copy\r\nimport os\r\nimport time\r\n\r\nimport tensorflow.compat.v1 as tf\r\nimport tensorflow.compat.v2 as tf2\r\n\r\nfrom object_detection import eval_util\r\nfrom object_detection import inputs\r\nfrom object_detection import model_lib\r\nfrom object_detection.builders import optimizer_builder\r\nfrom object_detection.core import standard_fields as fields\r\nfrom object_detection.protos import train_pb2\r\nfrom object_detection.utils import config_util\r\nfrom object_detection.utils import label_map_util\r\nfrom object_detection.utils import ops\r\nfrom object_detection.utils import visualization_utils as vutils\r\n\r\n# pylint: disable=g-import-not-at-top\r\ntry:\r\n  from tensorflow.contrib import tpu as contrib_tpu\r\nexcept ImportError:\r\n  # TF 2.0 doesn't ship with contrib.\r\n  pass\r\n# pylint: enable=g-import-not-at-top\r\n\r\nMODEL_BUILD_UTIL_MAP = model_lib.MODEL_BUILD_UTIL_MAP\r\n\r\n\r\nRESTORE_MAP_ERROR_TEMPLATE = (\r\n    'Since we are restoring a v2 style checkpoint'\r\n    ' restore_map was expected to return a (str -> Model) mapping,'\r\n    ' but we received a ({} -> {}) mapping instead.'\r\n)\r\n\r\n\r\ndef _compute_losses_and_predictions_dicts(\r\n    model, features, labels,\r\n    add_regularization_loss=True):\r\n  \"\"\"Computes the losses dict and predictions dict for a model on inputs.\r\n\r\n  Args:\r\n    model: a DetectionModel (based on Keras).\r\n    features: Dictionary of feature tensors from the input dataset.\r\n      Should be in the format output by `inputs.train_input` and\r\n      `inputs.eval_input`.\r\n        features[fields.InputDataFields.image] is a [batch_size, H, W, C]\r\n          float32 tensor with preprocessed images.\r\n        features[HASH_KEY] is a [batch_size] int32 tensor representing unique\r\n          identifiers for the images.\r\n        features[fields.InputDataFields.true_image_shape] is a [batch_size, 3]\r\n          int32 tensor representing the true image shapes, as preprocessed\r\n          images could be padded.\r\n        features[fields.InputDataFields.original_image] (optional) is a\r\n          [batch_size, H, W, C] float32 tensor with original images.\r\n    labels: A dictionary of groundtruth tensors post-unstacking. The original\r\n      labels are of the form returned by `inputs.train_input` and\r\n      `inputs.eval_input`. The shapes may have been modified by unstacking with\r\n      `model_lib.unstack_batch`. However, the dictionary includes the following\r\n      fields.\r\n        labels[fields.InputDataFields.num_groundtruth_boxes] is a\r\n          int32 tensor indicating the number of valid groundtruth boxes\r\n          per image.\r\n        labels[fields.InputDataFields.groundtruth_boxes] is a float32 tensor\r\n          containing the corners of the groundtruth boxes.\r\n        labels[fields.InputDataFields.groundtruth_classes] is a float32\r\n          one-hot tensor of classes.\r\n        labels[fields.InputDataFields.groundtruth_weights] is a float32 tensor\r\n          containing groundtruth weights for the boxes.\r\n        -- Optional --\r\n        labels[fields.InputDataFields.groundtruth_instance_masks] is a\r\n          float32 tensor containing only binary values, which represent\r\n          instance masks for objects.\r\n        labels[fields.InputDataFields.groundtruth_keypoints] is a\r\n          float32 tensor containing keypoints for each box.\r\n        labels[fields.InputDataFields.groundtruth_dp_num_points] is an int32\r\n          tensor with the number of sampled DensePose points per object.\r\n        labels[fields.InputDataFields.groundtruth_dp_part_ids] is an int32\r\n          tensor with the DensePose part ids (0-indexed) per object.\r\n        labels[fields.InputDataFields.groundtruth_dp_surface_coords] is a\r\n          float32 tensor with the DensePose surface coordinates.\r\n        labels[fields.InputDataFields.groundtruth_group_of] is a tf.bool tensor\r\n          containing group_of annotations.\r\n        labels[fields.InputDataFields.groundtruth_labeled_classes] is a float32\r\n          k-hot tensor of classes.\r\n        labels[fields.InputDataFields.groundtruth_track_ids] is a int32\r\n          tensor of track IDs.\r\n    add_regularization_loss: Whether or not to include the model's\r\n      regularization loss in the losses dictionary.\r\n\r\n  Returns:\r\n    A tuple containing the losses dictionary (with the total loss under\r\n    the key 'Loss/total_loss'), and the predictions dictionary produced by\r\n    `model.predict`.\r\n\r\n  \"\"\"\r\n  model_lib.provide_groundtruth(model, labels)\r\n  preprocessed_images = features[fields.InputDataFields.image]\r\n\r\n  prediction_dict = model.predict(\r\n      preprocessed_images,\r\n      features[fields.InputDataFields.true_image_shape],\r\n      **model.get_side_inputs(features))\r\n  prediction_dict = ops.bfloat16_to_float32_nested(prediction_dict)\r\n\r\n  losses_dict = model.loss(\r\n      prediction_dict, features[fields.InputDataFields.true_image_shape])\r\n  losses = [loss_tensor for loss_tensor in losses_dict.values()]\r\n  if add_regularization_loss:\r\n    # TODO(kaftan): As we figure out mixed precision & bfloat 16, we may\r\n    ## need to convert these regularization losses from bfloat16 to float32\r\n    ## as well.\r\n    regularization_losses = model.regularization_losses()\r\n    if regularization_losses:\r\n      regularization_losses = ops.bfloat16_to_float32_nested(\r\n          regularization_losses)\r\n      regularization_loss = tf.add_n(\r\n          regularization_losses, name='regularization_loss')\r\n      losses.append(regularization_loss)\r\n      losses_dict['Loss/regularization_loss'] = regularization_loss\r\n\r\n  total_loss = tf.add_n(losses, name='total_loss')\r\n  losses_dict['Loss/total_loss'] = total_loss\r\n\r\n  return losses_dict, prediction_dict\r\n\r\n\r\n# TODO(kaftan): Explore removing learning_rate from this method & returning\r\n## The full losses dict instead of just total_loss, then doing all summaries\r\n## saving in a utility method called by the outer training loop.\r\n# TODO(kaftan): Explore adding gradient summaries\r\ndef eager_train_step(detection_model,\r\n                     features,\r\n                     labels,\r\n                     unpad_groundtruth_tensors,\r\n                     optimizer,\r\n                     learning_rate,\r\n                     add_regularization_loss=True,\r\n                     clip_gradients_value=None,\r\n                     global_step=None,\r\n                     num_replicas=1.0):\r\n  \"\"\"Process a single training batch.\r\n\r\n  This method computes the loss for the model on a single training batch,\r\n  while tracking the gradients with a gradient tape. It then updates the\r\n  model variables with the optimizer, clipping the gradients if\r\n  clip_gradients_value is present.\r\n\r\n  This method can run eagerly or inside a tf.function.\r\n\r\n  Args:\r\n    detection_model: A DetectionModel (based on Keras) to train.\r\n    features: Dictionary of feature tensors from the input dataset.\r\n      Should be in the format output by `inputs.train_input.\r\n        features[fields.InputDataFields.image] is a [batch_size, H, W, C]\r\n          float32 tensor with preprocessed images.\r\n        features[HASH_KEY] is a [batch_size] int32 tensor representing unique\r\n          identifiers for the images.\r\n        features[fields.InputDataFields.true_image_shape] is a [batch_size, 3]\r\n          int32 tensor representing the true image shapes, as preprocessed\r\n          images could be padded.\r\n        features[fields.InputDataFields.original_image] (optional, not used\r\n          during training) is a\r\n          [batch_size, H, W, C] float32 tensor with original images.\r\n    labels: A dictionary of groundtruth tensors. This method unstacks\r\n      these labels using model_lib.unstack_batch. The stacked labels are of\r\n      the form returned by `inputs.train_input` and `inputs.eval_input`.\r\n        labels[fields.InputDataFields.num_groundtruth_boxes] is a [batch_size]\r\n          int32 tensor indicating the number of valid groundtruth boxes\r\n          per image.\r\n        labels[fields.InputDataFields.groundtruth_boxes] is a\r\n          [batch_size, num_boxes, 4] float32 tensor containing the corners of\r\n          the groundtruth boxes.\r\n        labels[fields.InputDataFields.groundtruth_classes] is a\r\n          [batch_size, num_boxes, num_classes] float32 one-hot tensor of\r\n          classes. num_classes includes the background class.\r\n        labels[fields.InputDataFields.groundtruth_weights] is a\r\n          [batch_size, num_boxes] float32 tensor containing groundtruth weights\r\n          for the boxes.\r\n        -- Optional --\r\n        labels[fields.InputDataFields.groundtruth_instance_masks] is a\r\n          [batch_size, num_boxes, H, W] float32 tensor containing only binary\r\n          values, which represent instance masks for objects.\r\n        labels[fields.InputDataFields.groundtruth_keypoints] is a\r\n          [batch_size, num_boxes, num_keypoints, 2] float32 tensor containing\r\n          keypoints for each box.\r\n        labels[fields.InputDataFields.groundtruth_dp_num_points] is a\r\n          [batch_size, num_boxes] int32 tensor with the number of DensePose\r\n          sampled points per instance.\r\n        labels[fields.InputDataFields.groundtruth_dp_part_ids] is a\r\n          [batch_size, num_boxes, max_sampled_points] int32 tensor with the\r\n          part ids (0-indexed) for each instance.\r\n        labels[fields.InputDataFields.groundtruth_dp_surface_coords] is a\r\n          [batch_size, num_boxes, max_sampled_points, 4] float32 tensor with the\r\n          surface coordinates for each point. Each surface coordinate is of the\r\n          form (y, x, v, u) where (y, x) are normalized image locations and\r\n          (v, u) are part-relative normalized surface coordinates.\r\n        labels[fields.InputDataFields.groundtruth_labeled_classes] is a float32\r\n          k-hot tensor of classes.\r\n        labels[fields.InputDataFields.groundtruth_track_ids] is a int32\r\n          tensor of track IDs.\r\n    unpad_groundtruth_tensors: A parameter passed to unstack_batch.\r\n    optimizer: The training optimizer that will update the variables.\r\n    learning_rate: The learning rate tensor for the current training step.\r\n      This is used only for TensorBoard logging purposes, it does not affect\r\n       model training.\r\n    add_regularization_loss: Whether or not to include the model's\r\n      regularization loss in the losses dictionary.\r\n    clip_gradients_value: If this is present, clip the gradients global norm\r\n      at this value using `tf.clip_by_global_norm`.\r\n    global_step: The current training step. Used for TensorBoard logging\r\n      purposes. This step is not updated by this function and must be\r\n      incremented separately.\r\n    num_replicas: The number of replicas in the current distribution strategy.\r\n      This is used to scale the total loss so that training in a distribution\r\n      strategy works correctly.\r\n\r\n  Returns:\r\n    The total loss observed at this training step\r\n  \"\"\"\r\n  # \"\"\"Execute a single training step in the TF v2 style loop.\"\"\"\r\n  is_training = True\r\n\r\n  detection_model._is_training = is_training  # pylint: disable=protected-access\r\n  tf.keras.backend.set_learning_phase(is_training)\r\n\r\n  labels = model_lib.unstack_batch(\r\n      labels, unpad_groundtruth_tensors=unpad_groundtruth_tensors)\r\n\r\n  with tf.GradientTape() as tape:\r\n    losses_dict, _ = _compute_losses_and_predictions_dicts(\r\n        detection_model, features, labels, add_regularization_loss)\r\n\r\n    total_loss = losses_dict['Loss/total_loss']\r\n\r\n    # Normalize loss for num replicas\r\n    total_loss = tf.math.divide(total_loss,\r\n                                tf.constant(num_replicas, dtype=tf.float32))\r\n    losses_dict['Loss/normalized_total_loss'] = total_loss\r\n\r\n  for loss_type in losses_dict:\r\n    tf.compat.v2.summary.scalar(\r\n        loss_type, losses_dict[loss_type], step=global_step)\r\n\r\n  trainable_variables = detection_model.trainable_variables\r\n\r\n  gradients = tape.gradient(total_loss, trainable_variables)\r\n\r\n  if clip_gradients_value:\r\n    gradients, _ = tf.clip_by_global_norm(gradients, clip_gradients_value)\r\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\r\n  tf.compat.v2.summary.scalar('learning_rate', learning_rate, step=global_step)\r\n  tf.compat.v2.summary.image(\r\n      name='train_input_images',\r\n      step=global_step,\r\n      data=features[fields.InputDataFields.image],\r\n      max_outputs=3)\r\n  return total_loss\r\n\r\n\r\ndef validate_tf_v2_checkpoint_restore_map(checkpoint_restore_map):\r\n  \"\"\"Ensure that given dict is a valid TF v2 style restore map.\r\n\r\n  Args:\r\n    checkpoint_restore_map: A nested dict mapping strings to\r\n      tf.keras.Model objects.\r\n\r\n  Raises:\r\n    ValueError: If they keys in checkpoint_restore_map are not strings or if\r\n      the values are not keras Model objects.\r\n\r\n  \"\"\"\r\n\r\n  for key, value in checkpoint_restore_map.items():\r\n    if not (isinstance(key, str) and\r\n            (isinstance(value, tf.Module)\r\n             or isinstance(value, tf.train.Checkpoint))):\r\n      if isinstance(key, str) and isinstance(value, dict):\r\n        validate_tf_v2_checkpoint_restore_map(value)\r\n      else:\r\n        raise TypeError(\r\n            RESTORE_MAP_ERROR_TEMPLATE.format(key.__class__.__name__,\r\n                                              value.__class__.__name__))\r\n\r\n\r\ndef is_object_based_checkpoint(checkpoint_path):\r\n  \"\"\"Returns true if `checkpoint_path` points to an object-based checkpoint.\"\"\"\r\n  var_names = [var[0] for var in tf.train.list_variables(checkpoint_path)]\r\n  return '_CHECKPOINTABLE_OBJECT_GRAPH' in var_names\r\n\r\n\r\ndef load_fine_tune_checkpoint(\r\n    model, checkpoint_path, checkpoint_type, checkpoint_version, input_dataset,\r\n    unpad_groundtruth_tensors):\r\n  \"\"\"Load a fine tuning classification or detection checkpoint.\r\n\r\n  To make sure the model variables are all built, this method first executes\r\n  the model by computing a dummy loss. (Models might not have built their\r\n  variables before their first execution)\r\n\r\n  It then loads an object-based classification or detection checkpoint.\r\n\r\n  This method updates the model in-place and does not return a value.\r\n\r\n  Args:\r\n    model: A DetectionModel (based on Keras) to load a fine-tuning\r\n      checkpoint for.\r\n    checkpoint_path: Directory with checkpoints file or path to checkpoint.\r\n    checkpoint_type: Whether to restore from a full detection\r\n      checkpoint (with compatible variable names) or to restore from a\r\n      classification checkpoint for initialization prior to training.\r\n      Valid values: `detection`, `classification`.\r\n    checkpoint_version: train_pb2.CheckpointVersion.V1 or V2 enum indicating\r\n      whether to load checkpoints in V1 style or V2 style.  In this binary\r\n      we only support V2 style (object-based) checkpoints.\r\n    input_dataset: The tf.data Dataset the model is being trained on. Needed\r\n      to get the shapes for the dummy loss computation.\r\n    unpad_groundtruth_tensors: A parameter passed to unstack_batch.\r\n\r\n  Raises:\r\n    IOError: if `checkpoint_path` does not point at a valid object-based\r\n      checkpoint\r\n    ValueError: if `checkpoint_version` is not train_pb2.CheckpointVersion.V2\r\n  \"\"\"\r\n  if not is_object_based_checkpoint(checkpoint_path):\r\n    raise IOError('Checkpoint is expected to be an object-based checkpoint.')\r\n  if checkpoint_version == train_pb2.CheckpointVersion.V1:\r\n    raise ValueError('Checkpoint version should be V2')\r\n\r\n  features, labels = iter(input_dataset).next()\r\n\r\n  @tf.function\r\n  def _dummy_computation_fn(features, labels):\r\n    model._is_training = False  # pylint: disable=protected-access\r\n    tf.keras.backend.set_learning_phase(False)\r\n\r\n    labels = model_lib.unstack_batch(\r\n        labels, unpad_groundtruth_tensors=unpad_groundtruth_tensors)\r\n\r\n    return _compute_losses_and_predictions_dicts(\r\n        model,\r\n        features,\r\n        labels)\r\n\r\n  strategy = tf.compat.v2.distribute.get_strategy()\r\n  if hasattr(tf.distribute.Strategy, 'run'):\r\n    strategy.run(\r\n        _dummy_computation_fn, args=(\r\n            features,\r\n            labels,\r\n        ))\r\n  else:\r\n    strategy.experimental_run_v2(\r\n        _dummy_computation_fn, args=(\r\n            features,\r\n            labels,\r\n        ))\r\n\r\n  restore_from_objects_dict = model.restore_from_objects(\r\n      fine_tune_checkpoint_type=checkpoint_type)\r\n  validate_tf_v2_checkpoint_restore_map(restore_from_objects_dict)\r\n  ckpt = tf.train.Checkpoint(**restore_from_objects_dict)\r\n  ckpt.restore(checkpoint_path).assert_existing_objects_matched()\r\n\r\n\r\ndef get_filepath(strategy, filepath):\r\n  \"\"\"Get appropriate filepath for worker.\r\n\r\n  Args:\r\n    strategy: A tf.distribute.Strategy object.\r\n    filepath: A path to where the Checkpoint object is stored.\r\n\r\n  Returns:\r\n    A temporary filepath for non-chief workers to use or the original filepath\r\n    for the chief.\r\n  \"\"\"\r\n  if strategy.extended.should_checkpoint:\r\n    return filepath\r\n  else:\r\n    # TODO(vighneshb) Replace with the public API when TF exposes it.\r\n    task_id = strategy.extended._task_id  # pylint:disable=protected-access\r\n    return os.path.join(filepath, 'temp_worker_{:03d}'.format(task_id))\r\n\r\n\r\ndef clean_temporary_directories(strategy, filepath):\r\n  \"\"\"Temporary directory clean up for MultiWorker Mirrored Strategy.\r\n\r\n  This is needed for all non-chief workers.\r\n\r\n  Args:\r\n    strategy: A tf.distribute.Strategy object.\r\n    filepath: The filepath for the temporary directory.\r\n  \"\"\"\r\n  if not strategy.extended.should_checkpoint:\r\n    if tf.io.gfile.exists(filepath) and tf.io.gfile.isdir(filepath):\r\n      tf.io.gfile.rmtree(filepath)\r\n\r\n\r\ndef train_loop(\r\n    pipeline_config_path,\r\n    model_dir,\r\n    config_override=None,\r\n    train_steps=None,\r\n    use_tpu=False,\r\n    save_final_config=False,\r\n    checkpoint_every_n=1000,\r\n    checkpoint_max_to_keep=7,\r\n    record_summaries=True,\r\n    **kwargs):\r\n  \"\"\"Trains a model using eager + functions.\r\n\r\n  This method:\r\n    1. Processes the pipeline configs\r\n    2. (Optionally) saves the as-run config\r\n    3. Builds the model & optimizer\r\n    4. Gets the training input data\r\n    5. Loads a fine-tuning detection or classification checkpoint if requested\r\n    6. Loops over the train data, executing distributed training steps inside\r\n       tf.functions.\r\n    7. Checkpoints the model every `checkpoint_every_n` training steps.\r\n    8. Logs the training metrics as TensorBoard summaries.\r\n\r\n  Args:\r\n    pipeline_config_path: A path to a pipeline config file.\r\n    model_dir:\r\n      The directory to save checkpoints and summaries to.\r\n    config_override: A pipeline_pb2.TrainEvalPipelineConfig text proto to\r\n      override the config from `pipeline_config_path`.\r\n    train_steps: Number of training steps. If None, the number of training steps\r\n      is set from the `TrainConfig` proto.\r\n    use_tpu: Boolean, whether training and evaluation should run on TPU.\r\n    save_final_config: Whether to save final config (obtained after applying\r\n      overrides) to `model_dir`.\r\n    checkpoint_every_n:\r\n      Checkpoint every n training steps.\r\n    checkpoint_max_to_keep:\r\n      int, the number of most recent checkpoints to keep in the model directory.\r\n    record_summaries: Boolean, whether or not to record summaries.\r\n    **kwargs: Additional keyword arguments for configuration override.\r\n  \"\"\"\r\n  ## Parse the configs\r\n  get_configs_from_pipeline_file = MODEL_BUILD_UTIL_MAP[\r\n      'get_configs_from_pipeline_file']\r\n  merge_external_params_with_configs = MODEL_BUILD_UTIL_MAP[\r\n      'merge_external_params_with_configs']\r\n  create_pipeline_proto_from_configs = MODEL_BUILD_UTIL_MAP[\r\n      'create_pipeline_proto_from_configs']\r\n\r\n  configs = get_configs_from_pipeline_file(\r\n      pipeline_config_path, config_override=config_override)\r\n  kwargs.update({\r\n      'train_steps': train_steps,\r\n      'use_bfloat16': configs['train_config'].use_bfloat16 and use_tpu\r\n  })\r\n  configs = merge_external_params_with_configs(\r\n      configs, None, kwargs_dict=kwargs)\r\n  model_config = configs['model']\r\n  train_config = configs['train_config']\r\n  train_input_config = configs['train_input_config']\r\n\r\n  unpad_groundtruth_tensors = train_config.unpad_groundtruth_tensors\r\n  add_regularization_loss = train_config.add_regularization_loss\r\n  clip_gradients_value = None\r\n  if train_config.gradient_clipping_by_norm > 0:\r\n    clip_gradients_value = train_config.gradient_clipping_by_norm\r\n\r\n  # update train_steps from config but only when non-zero value is provided\r\n  if train_steps is None and train_config.num_steps != 0:\r\n    train_steps = train_config.num_steps\r\n\r\n  if kwargs['use_bfloat16']:\r\n    tf.compat.v2.keras.mixed_precision.experimental.set_policy('mixed_bfloat16')\r\n\r\n  if train_config.load_all_detection_checkpoint_vars:\r\n    raise ValueError('train_pb2.load_all_detection_checkpoint_vars '\r\n                     'unsupported in TF2')\r\n\r\n  config_util.update_fine_tune_checkpoint_type(train_config)\r\n  fine_tune_checkpoint_type = train_config.fine_tune_checkpoint_type\r\n  fine_tune_checkpoint_version = train_config.fine_tune_checkpoint_version\r\n\r\n  # Write the as-run pipeline config to disk.\r\n  if save_final_config:\r\n    pipeline_config_final = create_pipeline_proto_from_configs(configs)\r\n    config_util.save_pipeline_config(pipeline_config_final, model_dir)\r\n\r\n  # Build the model, optimizer, and training input\r\n  strategy = tf.compat.v2.distribute.get_strategy()\r\n  with strategy.scope():\r\n    detection_model = MODEL_BUILD_UTIL_MAP['detection_model_fn_base'](\r\n        model_config=model_config, is_training=True)\r\n\r\n    def train_dataset_fn(input_context):\r\n      \"\"\"Callable to create train input.\"\"\"\r\n      # Create the inputs.\r\n      train_input = inputs.train_input(\r\n          train_config=train_config,\r\n          train_input_config=train_input_config,\r\n          model_config=model_config,\r\n          model=detection_model,\r\n          input_context=input_context)\r\n      train_input = train_input.repeat()\r\n      return train_input\r\n\r\n    train_input = strategy.experimental_distribute_datasets_from_function(\r\n        train_dataset_fn)\r\n\r\n\r\n    global_step = tf.Variable(\r\n        0, trainable=False, dtype=tf.compat.v2.dtypes.int64, name='global_step',\r\n        aggregation=tf.compat.v2.VariableAggregation.ONLY_FIRST_REPLICA)\r\n    optimizer, (learning_rate,) = optimizer_builder.build(\r\n        train_config.optimizer, global_step=global_step)\r\n\r\n    if callable(learning_rate):\r\n      learning_rate_fn = learning_rate\r\n    else:\r\n      learning_rate_fn = lambda: learning_rate\r\n\r\n  ## Train the model\r\n  # Get the appropriate filepath (temporary or not) based on whether the worker\r\n  # is the chief.\r\n  summary_writer_filepath = get_filepath(strategy,\r\n                                         os.path.join(model_dir, 'train'))\r\n  if record_summaries:\r\n    summary_writer = tf.compat.v2.summary.create_file_writer(\r\n        summary_writer_filepath)\r\n  else:\r\n    summary_writer = tf2.summary.create_noop_writer()\r\n\r\n  if use_tpu:\r\n    num_steps_per_iteration = 100\r\n  else:\r\n    # TODO(b/135933080) Explore setting to 100 when GPU performance issues\r\n    # are fixed.\r\n    num_steps_per_iteration = 1\r\n\r\n  with summary_writer.as_default():\r\n    with strategy.scope():\r\n      with tf.compat.v2.summary.record_if(\r\n          lambda: global_step % num_steps_per_iteration == 0):\r\n        # Load a fine-tuning checkpoint.\r\n        if train_config.fine_tune_checkpoint:\r\n          load_fine_tune_checkpoint(detection_model,\r\n                                    train_config.fine_tune_checkpoint,\r\n                                    fine_tune_checkpoint_type,\r\n                                    fine_tune_checkpoint_version,\r\n                                    train_input,\r\n                                    unpad_groundtruth_tensors)\r\n\r\n        ckpt = tf.compat.v2.train.Checkpoint(\r\n            step=global_step, model=detection_model, optimizer=optimizer)\r\n\r\n        manager_dir = get_filepath(strategy, model_dir)\r\n        if not strategy.extended.should_checkpoint:\r\n          checkpoint_max_to_keep = 1\r\n        manager = tf.compat.v2.train.CheckpointManager(\r\n            ckpt, manager_dir, max_to_keep=checkpoint_max_to_keep)\r\n\r\n        # We use the following instead of manager.latest_checkpoint because\r\n        # manager_dir does not point to the model directory when we are running\r\n        # in a worker.\r\n        latest_checkpoint = tf.train.latest_checkpoint(model_dir)\r\n        ckpt.restore(latest_checkpoint)\r\n\r\n        def train_step_fn(features, labels):\r\n          \"\"\"Single train step.\"\"\"\r\n          loss = eager_train_step(\r\n              detection_model,\r\n              features,\r\n              labels,\r\n              unpad_groundtruth_tensors,\r\n              optimizer,\r\n              learning_rate=learning_rate_fn(),\r\n              add_regularization_loss=add_regularization_loss,\r\n              clip_gradients_value=clip_gradients_value,\r\n              global_step=global_step,\r\n              num_replicas=strategy.num_replicas_in_sync)\r\n          global_step.assign_add(1)\r\n          return loss\r\n\r\n        def _sample_and_train(strategy, train_step_fn, data_iterator):\r\n          features, labels = data_iterator.next()\r\n          if hasattr(tf.distribute.Strategy, 'run'):\r\n            per_replica_losses = strategy.run(\r\n                train_step_fn, args=(features, labels))\r\n          else:\r\n            per_replica_losses = strategy.experimental_run_v2(\r\n                train_step_fn, args=(features, labels))\r\n          # TODO(anjalisridhar): explore if it is safe to remove the\r\n          ## num_replicas scaling of the loss and switch this to a ReduceOp.Mean\r\n          return strategy.reduce(tf.distribute.ReduceOp.SUM,\r\n                                 per_replica_losses, axis=None)\r\n\r\n        @tf.function\r\n        def _dist_train_step(data_iterator):\r\n          \"\"\"A distributed train step.\"\"\"\r\n\r\n          if num_steps_per_iteration > 1:\r\n            for _ in tf.range(num_steps_per_iteration - 1):\r\n              # Following suggestion on yaqs/5402607292645376\r\n              with tf.name_scope(''):\r\n                _sample_and_train(strategy, train_step_fn, data_iterator)\r\n\r\n          return _sample_and_train(strategy, train_step_fn, data_iterator)\r\n\r\n        train_input_iter = iter(train_input)\r\n\r\n        if int(global_step.value()) == 0:\r\n          manager.save()\r\n\r\n        checkpointed_step = int(global_step.value())\r\n        logged_step = global_step.value()\r\n\r\n        last_step_time = time.time()\r\n        for _ in range(global_step.value(), train_steps,\r\n                       num_steps_per_iteration):\r\n\r\n          loss = _dist_train_step(train_input_iter)\r\n\r\n          time_taken = time.time() - last_step_time\r\n          last_step_time = time.time()\r\n\r\n          tf.compat.v2.summary.scalar(\r\n              'steps_per_sec', num_steps_per_iteration * 1.0 / time_taken,\r\n              step=global_step)\r\n\r\n          if global_step.value() - logged_step >= 100:\r\n            tf.logging.info(\r\n                'Step {} per-step time {:.3f}s loss={:.3f}'.format(\r\n                    global_step.value(), time_taken / num_steps_per_iteration,\r\n                    loss))\r\n            logged_step = global_step.value()\r\n\r\n          if ((int(global_step.value()) - checkpointed_step) >=\r\n              checkpoint_every_n):\r\n            manager.save()\r\n            checkpointed_step = int(global_step.value())\r\n\r\n  # Remove the checkpoint directories of the non-chief workers that\r\n  # MultiWorkerMirroredStrategy forces us to save during sync distributed\r\n  # training.\r\n  clean_temporary_directories(strategy, manager_dir)\r\n  clean_temporary_directories(strategy, summary_writer_filepath)\r\n\r\n\r\ndef eager_eval_loop(\r\n    detection_model,\r\n    configs,\r\n    eval_dataset,\r\n    use_tpu=False,\r\n    postprocess_on_cpu=False,\r\n    global_step=None):\r\n  \"\"\"Evaluate the model eagerly on the evaluation dataset.\r\n\r\n  This method will compute the evaluation metrics specified in the configs on\r\n  the entire evaluation dataset, then return the metrics. It will also log\r\n  the metrics to TensorBoard.\r\n\r\n  Args:\r\n    detection_model: A DetectionModel (based on Keras) to evaluate.\r\n    configs: Object detection configs that specify the evaluators that should\r\n      be used, as well as whether regularization loss should be included and\r\n      if bfloat16 should be used on TPUs.\r\n    eval_dataset: Dataset containing evaluation data.\r\n    use_tpu: Whether a TPU is being used to execute the model for evaluation.\r\n    postprocess_on_cpu: Whether model postprocessing should happen on\r\n      the CPU when using a TPU to execute the model.\r\n    global_step: A variable containing the training step this model was trained\r\n      to. Used for logging purposes.\r\n\r\n  Returns:\r\n    A dict of evaluation metrics representing the results of this evaluation.\r\n  \"\"\"\r\n  train_config = configs['train_config']\r\n  eval_input_config = configs['eval_input_config']\r\n  eval_config = configs['eval_config']\r\n  add_regularization_loss = train_config.add_regularization_loss\r\n\r\n  is_training = False\r\n  detection_model._is_training = is_training  # pylint: disable=protected-access\r\n  tf.keras.backend.set_learning_phase(is_training)\r\n\r\n  evaluator_options = eval_util.evaluator_options_from_eval_config(\r\n      eval_config)\r\n  batch_size = eval_config.batch_size\r\n\r\n  class_agnostic_category_index = (\r\n      label_map_util.create_class_agnostic_category_index())\r\n  class_agnostic_evaluators = eval_util.get_evaluators(\r\n      eval_config,\r\n      list(class_agnostic_category_index.values()),\r\n      evaluator_options)\r\n\r\n  class_aware_evaluators = None\r\n  if eval_input_config.label_map_path:\r\n    class_aware_category_index = (\r\n        label_map_util.create_category_index_from_labelmap(\r\n            eval_input_config.label_map_path))\r\n    class_aware_evaluators = eval_util.get_evaluators(\r\n        eval_config,\r\n        list(class_aware_category_index.values()),\r\n        evaluator_options)\r\n\r\n  evaluators = None\r\n  loss_metrics = {}\r\n\r\n  @tf.function\r\n  def compute_eval_dict(features, labels):\r\n    \"\"\"Compute the evaluation result on an image.\"\"\"\r\n    # For evaling on train data, it is necessary to check whether groundtruth\r\n    # must be unpadded.\r\n    boxes_shape = (\r\n        labels[fields.InputDataFields.groundtruth_boxes].get_shape().as_list())\r\n    unpad_groundtruth_tensors = (boxes_shape[1] is not None\r\n                                 and not use_tpu\r\n                                 and batch_size == 1)\r\n    labels = model_lib.unstack_batch(\r\n        labels, unpad_groundtruth_tensors=unpad_groundtruth_tensors)\r\n\r\n    losses_dict, prediction_dict = _compute_losses_and_predictions_dicts(\r\n        detection_model, features, labels, add_regularization_loss)\r\n\r\n    def postprocess_wrapper(args):\r\n      return detection_model.postprocess(args[0], args[1])\r\n\r\n    # TODO(kaftan): Depending on how postprocessing will work for TPUS w/\r\n    ## TPUStrategy, may be good to move wrapping to a utility method\r\n    if use_tpu and postprocess_on_cpu:\r\n      detections = contrib_tpu.outside_compilation(\r\n          postprocess_wrapper,\r\n          (prediction_dict, features[fields.InputDataFields.true_image_shape]))\r\n    else:\r\n      detections = postprocess_wrapper(\r\n          (prediction_dict, features[fields.InputDataFields.true_image_shape]))\r\n\r\n    class_agnostic = (\r\n        fields.DetectionResultFields.detection_classes not in detections)\r\n    # TODO(kaftan) (or anyone): move `_prepare_groundtruth_for_eval to eval_util\r\n    ## and call this from there.\r\n    groundtruth = model_lib._prepare_groundtruth_for_eval(  # pylint: disable=protected-access\r\n        detection_model, class_agnostic, eval_input_config.max_number_of_boxes)\r\n    use_original_images = fields.InputDataFields.original_image in features\r\n    if use_original_images:\r\n      eval_images = features[fields.InputDataFields.original_image]\r\n      true_image_shapes = tf.slice(\r\n          features[fields.InputDataFields.true_image_shape], [0, 0], [-1, 3])\r\n      original_image_spatial_shapes = features[\r\n          fields.InputDataFields.original_image_spatial_shape]\r\n    else:\r\n      eval_images = features[fields.InputDataFields.image]\r\n      true_image_shapes = None\r\n      original_image_spatial_shapes = None\r\n\r\n    keys = features[inputs.HASH_KEY]\r\n    if eval_input_config.include_source_id:\r\n      keys = features[fields.InputDataFields.source_id]\r\n    eval_dict = eval_util.result_dict_for_batched_example(\r\n        eval_images,\r\n        keys,\r\n        detections,\r\n        groundtruth,\r\n        class_agnostic=class_agnostic,\r\n        scale_to_absolute=True,\r\n        original_image_spatial_shapes=original_image_spatial_shapes,\r\n        true_image_shapes=true_image_shapes)\r\n\r\n    return eval_dict, losses_dict, class_agnostic\r\n\r\n  agnostic_categories = label_map_util.create_class_agnostic_category_index()\r\n  per_class_categories = label_map_util.create_category_index_from_labelmap(\r\n      eval_input_config.label_map_path)\r\n  keypoint_edges = [\r\n      (kp.start, kp.end) for kp in eval_config.keypoint_edge]\r\n\r\n  for i, (features, labels) in enumerate(eval_dataset):\r\n    eval_dict, losses_dict, class_agnostic = compute_eval_dict(features, labels)\r\n\r\n    if class_agnostic:\r\n      category_index = agnostic_categories\r\n    else:\r\n      category_index = per_class_categories\r\n\r\n    if i % 100 == 0:\r\n      tf.logging.info('Finished eval step %d', i)\r\n\r\n    use_original_images = fields.InputDataFields.original_image in features\r\n    if (use_original_images and i < eval_config.num_visualizations\r\n        and batch_size == 1):\r\n      sbys_image_list = vutils.draw_side_by_side_evaluation_image(\r\n          eval_dict,\r\n          category_index=category_index,\r\n          max_boxes_to_draw=eval_config.max_num_boxes_to_visualize,\r\n          min_score_thresh=eval_config.min_score_threshold,\r\n          use_normalized_coordinates=False,\r\n          keypoint_edges=keypoint_edges or None)\r\n      sbys_images = tf.concat(sbys_image_list, axis=0)\r\n      tf.compat.v2.summary.image(\r\n          name='eval_side_by_side_' + str(i),\r\n          step=global_step,\r\n          data=sbys_images,\r\n          max_outputs=eval_config.num_visualizations)\r\n      if eval_util.has_densepose(eval_dict):\r\n        dp_image_list = vutils.draw_densepose_visualizations(\r\n            eval_dict)\r\n        dp_images = tf.concat(dp_image_list, axis=0)\r\n        tf.compat.v2.summary.image(\r\n            name='densepose_detections_' + str(i),\r\n            step=global_step,\r\n            data=dp_images,\r\n            max_outputs=eval_config.num_visualizations)\r\n\r\n    if evaluators is None:\r\n      if class_agnostic:\r\n        evaluators = class_agnostic_evaluators\r\n      else:\r\n        evaluators = class_aware_evaluators\r\n\r\n    for evaluator in evaluators:\r\n      evaluator.add_eval_dict(eval_dict)\r\n\r\n    for loss_key, loss_tensor in iter(losses_dict.items()):\r\n      if loss_key not in loss_metrics:\r\n        loss_metrics[loss_key] = tf.keras.metrics.Mean()\r\n      # Skip the loss with value equal or lower than 0.0 when calculating the\r\n      # average loss since they don't usually reflect the normal loss values\r\n      # causing spurious average loss value.\r\n      if loss_tensor <= 0.0:\r\n        continue\r\n      loss_metrics[loss_key].update_state(loss_tensor)\r\n\r\n  eval_metrics = {}\r\n\r\n  for evaluator in evaluators:\r\n    eval_metrics.update(evaluator.evaluate())\r\n  for loss_key in loss_metrics:\r\n    eval_metrics[loss_key] = loss_metrics[loss_key].result()\r\n\r\n  eval_metrics = {str(k): v for k, v in eval_metrics.items()}\r\n  tf.logging.info('Eval metrics at step %d', global_step)\r\n  for k in eval_metrics:\r\n    tf.compat.v2.summary.scalar(k, eval_metrics[k], step=global_step)\r\n    tf.logging.info('\\t+ %s: %f', k, eval_metrics[k])\r\n\r\n  return eval_metrics\r\n\r\n\r\ndef eval_continuously(\r\n    pipeline_config_path,\r\n    config_override=None,\r\n    train_steps=None,\r\n    sample_1_of_n_eval_examples=1,\r\n    sample_1_of_n_eval_on_train_examples=1,\r\n    use_tpu=False,\r\n    override_eval_num_epochs=True,\r\n    postprocess_on_cpu=False,\r\n    model_dir=None,\r\n    checkpoint_dir=None,\r\n    wait_interval=180,\r\n    timeout=3600,\r\n    eval_index=None,\r\n    **kwargs):\r\n  \"\"\"Run continuous evaluation of a detection model eagerly.\r\n\r\n  This method builds the model, and continously restores it from the most\r\n  recent training checkpoint in the checkpoint directory & evaluates it\r\n  on the evaluation data.\r\n\r\n  Args:\r\n    pipeline_config_path: A path to a pipeline config file.\r\n    config_override: A pipeline_pb2.TrainEvalPipelineConfig text proto to\r\n      override the config from `pipeline_config_path`.\r\n    train_steps: Number of training steps. If None, the number of training steps\r\n      is set from the `TrainConfig` proto.\r\n    sample_1_of_n_eval_examples: Integer representing how often an eval example\r\n      should be sampled. If 1, will sample all examples.\r\n    sample_1_of_n_eval_on_train_examples: Similar to\r\n      `sample_1_of_n_eval_examples`, except controls the sampling of training\r\n      data for evaluation.\r\n    use_tpu: Boolean, whether training and evaluation should run on TPU.\r\n    override_eval_num_epochs: Whether to overwrite the number of epochs to 1 for\r\n      eval_input.\r\n    postprocess_on_cpu: When use_tpu and postprocess_on_cpu are true,\r\n      postprocess is scheduled on the host cpu.\r\n    model_dir: Directory to output resulting evaluation summaries to.\r\n    checkpoint_dir: Directory that contains the training checkpoints.\r\n    wait_interval: The mimmum number of seconds to wait before checking for a\r\n      new checkpoint.\r\n    timeout: The maximum number of seconds to wait for a checkpoint. Execution\r\n      will terminate if no new checkpoints are found after these many seconds.\r\n    eval_index: int, optional If give, only evaluate the dataset at the given\r\n      index.\r\n\r\n    **kwargs: Additional keyword arguments for configuration override.\r\n  \"\"\"\r\n  get_configs_from_pipeline_file = MODEL_BUILD_UTIL_MAP[\r\n      'get_configs_from_pipeline_file']\r\n  merge_external_params_with_configs = MODEL_BUILD_UTIL_MAP[\r\n      'merge_external_params_with_configs']\r\n\r\n  configs = get_configs_from_pipeline_file(\r\n      pipeline_config_path, config_override=config_override)\r\n  kwargs.update({\r\n      'sample_1_of_n_eval_examples': sample_1_of_n_eval_examples,\r\n      'use_bfloat16': configs['train_config'].use_bfloat16 and use_tpu\r\n  })\r\n  if train_steps is not None:\r\n    kwargs['train_steps'] = train_steps\r\n  if override_eval_num_epochs:\r\n    kwargs.update({'eval_num_epochs': 1})\r\n    tf.logging.warning(\r\n        'Forced number of epochs for all eval validations to be 1.')\r\n  configs = merge_external_params_with_configs(\r\n      configs, None, kwargs_dict=kwargs)\r\n  model_config = configs['model']\r\n  train_input_config = configs['train_input_config']\r\n  eval_config = configs['eval_config']\r\n  eval_input_configs = configs['eval_input_configs']\r\n  eval_on_train_input_config = copy.deepcopy(train_input_config)\r\n  eval_on_train_input_config.sample_1_of_n_examples = (\r\n      sample_1_of_n_eval_on_train_examples)\r\n  if override_eval_num_epochs and eval_on_train_input_config.num_epochs != 1:\r\n    tf.logging.warning('Expected number of evaluation epochs is 1, but '\r\n                       'instead encountered `eval_on_train_input_config'\r\n                       '.num_epochs` = '\r\n                       '{}. Overwriting `num_epochs` to 1.'.format(\r\n                           eval_on_train_input_config.num_epochs))\r\n    eval_on_train_input_config.num_epochs = 1\r\n\r\n  if kwargs['use_bfloat16']:\r\n    tf.compat.v2.keras.mixed_precision.experimental.set_policy('mixed_bfloat16')\r\n\r\n  detection_model = MODEL_BUILD_UTIL_MAP['detection_model_fn_base'](\r\n      model_config=model_config, is_training=True)\r\n\r\n  # Create the inputs.\r\n  eval_inputs = []\r\n  for eval_input_config in eval_input_configs:\r\n    next_eval_input = inputs.eval_input(\r\n        eval_config=eval_config,\r\n        eval_input_config=eval_input_config,\r\n        model_config=model_config,\r\n        model=detection_model)\r\n    eval_inputs.append((eval_input_config.name, next_eval_input))\r\n\r\n  if eval_index is not None:\r\n    eval_inputs = [eval_inputs[eval_index]]\r\n\r\n  global_step = tf.compat.v2.Variable(\r\n      0, trainable=False, dtype=tf.compat.v2.dtypes.int64)\r\n\r\n  for latest_checkpoint in tf.train.checkpoints_iterator(\r\n      checkpoint_dir, timeout=timeout, min_interval_secs=wait_interval):\r\n    ckpt = tf.compat.v2.train.Checkpoint(\r\n        step=global_step, model=detection_model)\r\n\r\n    ckpt.restore(latest_checkpoint).expect_partial()\r\n\r\n    for eval_name, eval_input in eval_inputs:\r\n      summary_writer = tf.compat.v2.summary.create_file_writer(\r\n          os.path.join(model_dir, 'eval', eval_name))\r\n      with summary_writer.as_default():\r\n        eager_eval_loop(\r\n            detection_model,\r\n            configs,\r\n            eval_input,\r\n            use_tpu=use_tpu,\r\n            postprocess_on_cpu=postprocess_on_cpu,\r\n            global_step=global_step)\r\n```\r\n", "comments": ["https://stackoverflow.com/questions/64824662/tensorflow-valueerror-checkpoint-version-should-be-v2/64826609#64826609"]}, {"number": 44842, "title": "Error while trying to load model's weights for fine-tuning (transfer learning)", "body": "**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\nTensorFlow installed from (source or binary): binary (no errors during installation)\r\nTensorFlow version: 2.3.1 (with GPU support)\r\nPython version: 3.7.5\r\nCUDA/cuDNN version: 10.1\r\nGPU model and memory: RTX 1660 Ti 6.00GB\r\n\r\n**Describe the current behavior**\r\nI reproduce my issue on oversimplified code.\r\nI'm trying to train model for one dataset (say, on MNIST) and then fine-tune it on another (with different number of classes, say 5).\r\nAfter I have created model for '5 classes task' I'm trying to load the weights from original model using \r\n`model.load_weights(path, by_name=True, skip_mismatch=True)`\r\n\r\nBut I got an error:\r\n`ValueError: Shapes (5,) and (10,) are incompatible`\r\n\r\nI hoped that setting the params `by_name=True, skip_mismatch=True `will be enough to handle differences between the models.\r\nHow to correct load weights for slightly different model?\r\n\r\n**Standalone code to reproduce the issue**\r\nOversimplified script to recreate the trouble\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\n\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\ny_train = tf.keras.utils.to_categorical(y_train, 10)\r\ny_test = tf.keras.utils.to_categorical(y_test, 10)\r\nx_train = np.expand_dims(x_train, -1)\r\nx_test = np.expand_dims(x_test, -1)\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Input(shape=(28, 28, 1)),\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\r\n    tf.keras.layers.AveragePooling2D(),\r\n    tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\r\n    tf.keras.layers.AveragePooling2D(),\r\n    tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\r\n    tf.keras.layers.AveragePooling2D(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(10, activation='softmax')])\r\n\r\nmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam())\r\n\r\nbatch_size = 128\r\nepochs = 3\r\nmodel.fit(x_train, y_train, batch_size, epochs, validation_data=(x_test, y_test))\r\n\r\nver = 0\r\npath = os.path.join(os.getcwd(), str(ver))\r\nmodel.save_weights(path)\r\n\r\n\r\ndifferent_model = tf.keras.models.Sequential([\r\n    tf.keras.layers.Input(shape=(28, 28, 1)),\r\n    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same'),\r\n    tf.keras.layers.AveragePooling2D(),\r\n    tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\r\n    tf.keras.layers.AveragePooling2D(),\r\n    tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\r\n    tf.keras.layers.AveragePooling2D(),\r\n    tf.keras.layers.Flatten(),\r\n    tf.keras.layers.Dense(5, activation='softmax')]) # say, we change the number of classes here for further fine-tuning\r\n\r\n# here I got an error!!!!!!! isn't by_name=True, skip_mismatch=True not enough?\r\ndifferent_model.load_weights(path, by_name=True, skip_mismatch=True)\r\n\r\n# fine-tuning/transfer learning is supposed here for some application where we classify 5 cls\r\n\r\nprint('loaded!')\r\n```\r\n\r\nIt's a serious and urgent problem for my project!\r\nAny help is appreciated!!!\r\n", "comments": ["I think this is a project specific support request not a bug. Please close this and post on our [Stackoverflow channel](https://stackoverflow.com/questions/tagged/tensorflow)", "@bhack I wouldn't say it's project specific. The issue described entirely in term of MNIST and simple 'standard' CNN (in fact, you can use just 'one-conv-layer-followed-by-dense-classifier' network to reproduce it)\r\nFound work-around loading weights before adding layers which may cause shape mismatch. More about loading checkpoints in TF's docs here https://www.tensorflow.org/guide/checkpoint#loading_mechanics", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44842\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44842\">No</a>\n", "> (in fact, you can use just 'one-conv-layer-followed-by-dense-classifier' network to reproduce it)\r\n\r\nYes it is what I meant, but in you case was a code snippet, and I also I meant that it is not bug as you have used a bug template for this issue. \r\nIt could be a bug if you are confused by the error message and you want to improve it.\r\nBut seems to me a support request on your code example and it is why I suggest you to use our Stackoverflow channel.", "Also if you use `h5`ext filename format in your save and load path it will run fine as is"]}, {"number": 44841, "title": "No rule to make target 'hello_world_bin'.  Stop", "body": "ake: *** No rule to make target 'third_party_download'.  Stop.\r\nlenovo :: F:\\tensorflow <master> \u00bb make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge hello_world_bin\r\nFIND: \u53c2\u6570\u683c\u5f0f\u4e0d\u6b63\u786e\r\nFIND: \u53c2\u6570\u683c\u5f0f\u4e0d\u6b63\u786e\r\nmake: *** No rule to make target 'hello_world_bin'.  Stop.\r\n\r\nwhy\uff1f\r\nI am a novice and can't solve this problem", "comments": ["@YANxu-666 \r\nPlease refer to [this comment](https://github.com/tensorflow/tensorflow/issues/42049#issuecomment-669376969) and let us know if it helps.\r\n\r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "Adding Advait and Pete, in case you have experienced similar failure on Windows.", "> Adding Advait and Pete, in case you have experienced similar failure on Windows.\r\n\r\nAdvait and Pete? I'm sorry i don't know\r\nCould you make it clearer?", "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world#compile-the-binary\r\nI am compiling a binary file and an error occurs\uff1a\r\n\r\nlenovo :: F:\\tensorflow <master> \u00bb make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge hello_world_bin\r\nFIND: \u53c2\u6570\u683c\u5f0f\u4e0d\u6b63\u786e\r\nFIND: \u53c2\u6570\u683c\u5f0f\u4e0d\u6b63\u786e\r\nmake: *** No rule to make target 'hello_world_bin'.  Stop.\r\nI use Windows terminal", "Any chance that you could try Linux?", "@YANxu-666\r\nPlease update", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "It might be the make command doesn't work well on Window.. I install a virtual machine Obuntu and run the command lines.", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44841\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44841\">No</a>\n"]}, {"number": 44840, "title": "\"An op outside of the function building code is being passed a \"Graph\" tensor\" when using custom loss", "body": "Hi, i made a code with subclass api(https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example) and custom loss for a model.\r\n\r\nThe problem is when using custom loss in addition to main loss, this error is raised:\r\n```\r\nTypeError: An op outside of the function building code is being passed\r\na \"Graph\" tensor. It is possible to have Graph tensors\r\nleak out of the function building context by including a\r\ntf.init_scope in your function building code.\r\nFor example, the following function will fail:\r\n  @tf.function\r\n  def has_init_scope():\r\n    my_constant = tf.constant(1.)\r\n    with tf.init_scope():\r\n      added = my_constant * 2\r\nThe graph tensor has name: add_1:0\r\n```\r\n\r\nIf i comment `self.add_loss(lambda: tf.reduce_mean(VAE_loss))` the error is not raised.\r\n\r\nColab notebook : https://colab.research.google.com/drive/17zKRFQsPCpKjHkmMJqMC4EuDDhOLvuVy?usp=sharing\r\n\r\n", "comments": ["@Crispy13 Can you minimize you colab example to reproduce this?", "> @Crispy13 Can you minimize you colab example to reproduce this?\r\n\r\nI'm sorry but this was the best I can. I already minimized once.\r\nI can't minimize the model part.\r\n", "I solved it.\r\n\r\n**First, I commented the build function of the custom model.**\r\n```\r\n    def build(self, batch_input_shape):\r\n        n_inputs = batch_input_shape[-1]\r\n        \r\n        \r\n        ### super build\r\n        super().build(batch_input_shape)\r\n```\r\n\r\n**And `run_eagerly = True` when compiling** or **using a custom layer calculating the custom loss worked.**\r\n**For example**, write a custom layer code:\r\n```\r\nclass LossVAE(keras.layers.Layer):\r\n    def __init__(self, weight_L2, weight_KL, n, **kwargs):\r\n        super().__init__(**kwargs)\r\n        \r\n        self.weight_L2 = weight_L2\r\n        self.weight_KL = weight_KL\r\n        self.n = n\r\n        \r\n    def call(self, inputs):\r\n        x, out_VAE, z_mean, z_var = inputs\r\n        loss_L2 = tf.reduce_mean(tf.square(x - out_VAE), axis=(1, 2, 3, 4)) # original axis value is (1,2,3,4).\r\n        loss_KL = (1 / self.n) * tf.reduce_sum(\r\n            tf.exp(z_var) + tf.square(z_mean) - 1. - z_var,\r\n            axis=-1\r\n        )\r\n        \r\n        VAE_loss = tf.reduce_mean(tf.add(self.weight_L2 * loss_L2, self.weight_KL * loss_KL, name = \"add_L2_KL\"), name = \"mean_VAELoss\")\r\n        self.add_loss(VAE_loss)\r\n        \r\n        return         \r\n```\r\nand assign an instance of it in `__init__` of the custom model, insert `self.LossVAE([Z, out_VAE, z_mean, z_var])` into call method of the custom model.\r\n\r\nBut i don't understand still why the error was raised when using [the initial code](https://colab.research.google.com/drive/17zKRFQsPCpKjHkmMJqMC4EuDDhOLvuVy?usp=sharing#scrollTo=feircePBr9MX).\r\n\r\n\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44840\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44840\">No</a>\n", "@Crispy13 See https://github.com/tensorflow/tensorflow/issues/44861#issuecomment-727202741"]}, {"number": 44839, "title": "Extracting information from yolov3 tflite model of output shape [1,2535,15] is giving weird results", "body": "I tried using my yolov3-tiny.tflite. I'm not sure why but I'm getting something like this\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/c5Ypq.jpg\r\n\r\n\r\nHere is my code\r\n\r\n\r\n    private  float[][][] getBoxes(float [][][] map){\r\n        float[][][] bboxes = new float[map.length][map[0].length][5];\r\n        for(int i = 0; i < map.length; i++){\r\n            for(int j = 0; j < map[i].length; j++){\r\n                for(int k = 0; k < 5; k++){\r\n                    bboxes[i][j][k] = map[i][j][k];\r\n                }\r\n            }\r\n        }\r\n        return bboxes;\r\n    }\r\n    \r\n    \r\n    private  float[][][] getScore(float [][][] map){\r\n        float[][][] scores = new float[map.length][map[0].length][labels.size()];\r\n        for(int i = 0; i < map.length; i++){\r\n            for(int j = 0; j < map[i].length; j++){\r\n                for(int k = 0; k < labels.size(); k++){\r\n                    scores[i][j][k] = map[i][j][k+5];\r\n                }\r\n            }\r\n        }\r\n        return scores;\r\n    }\r\n    \r\n    private ArrayList<Recognition> getDetectionsForTiny(ByteBuffer byteBuffer, Bitmap bitmap) {\r\n        ArrayList<Recognition> detections = new ArrayList<Recognition>();\r\n        Map<Integer, Object> outputMap = new HashMap<>();\r\n        outputMap.put(0, new float[1][OUTPUT_WIDTH_TINY[0]][5 + labels.size()]);\r\n        //outputMap.put(1, new float[1][OUTPUT_WIDTH_TINY[1]][labels.size()]);\r\n        Object[] inputArray = {byteBuffer};\r\n        tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n    \r\n        int gridWidth = OUTPUT_WIDTH_TINY[0];\r\n        float[][][] bboxes = getBoxes((float[][][])outputMap.get(0));\r\n        float[][][] out_score = getScore((float[][][]) outputMap.get(0));\r\n    \r\n        int detectedClass = -1;\r\n        float maxClass = 0;\r\n    \r\n        for (int i = 0; i < gridWidth;i++){\r\n            final float[] classes = new float[labels.size()];\r\n            for (int c = 0;c< labels.size();c++){\r\n                classes [c] = out_score[0][i][c];\r\n            }\r\n            for (int c = 0;c<labels.size();++c){\r\n                if (classes[c] > maxClass){\r\n                    detectedClass = c;\r\n                    maxClass = classes[c];\r\n                }\r\n            }\r\n            final float score = maxClass;\r\n            if (score > getObjThresh()){\r\n                final float xPos = bboxes[0][i][0];\r\n                final float yPos = bboxes[0][i][1];\r\n                final float w = bboxes[0][i][2];\r\n                final float h = bboxes[0][i][3];\r\n                final RectF rectF = new RectF(\r\n                        Math.max(0, xPos - w / 2),\r\n                        Math.max(0, yPos - h / 2),\r\n                        Math.min(bitmap.getWidth() - 1, xPos + w / 2),\r\n                        Math.min(bitmap.getHeight() - 1, yPos + h / 2));\r\n                detections.add(new Recognition(\"\" + i, labels.get(detectedClass),score,rectF,detectedClass ));\r\n                Log.d(\"Title ****************\",\" \"+labels.get(detectedClass));\r\n    \r\n            }\r\n        }\r\n    \r\n        return detections;\r\n    }\r\n\r\nI don't know where I'm going wrong. Thank you!", "comments": ["@Saduf2019 Hi, Could you help me to extract information from this output tensor? I feel I'm doing everything right? Do you want me to upload the tflite model as well?", "It is a support request. Please ask this in our [Stackoverflow channel](https://stackoverflow.com/questions/tagged/tensorflow)"]}, {"number": 44838, "title": "Strange behavior for tf.keras.backend.function in tf 2.3.1 for intermediate values", "body": "I was training an Xception model with custom output layers. So, clasic Xception model + dropout layer + dense layer alpha + dropout layer alpha + softmax dense layer for output.\r\nAfter training, my goal is to split the model in half: Xception+dropout and dense layer alpha...softmax output.\r\nFor this, i am doing>\r\n```\r\nmodel = load_model(path)\r\nbase_model= Model(inputs=model.input, outputs=model.get_layer('base_drop').output) - Works fine\r\nsecond_half = K.function([model.get_layer('dense_alpha').input, K.learning_phase()],\r\n                            [model.get_layer('dense_out').output]) - Error\r\n```\r\nThe error is:\r\n```\r\nWARNING:tensorflow:Functional inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"functional_4\" was not an Input tensor, it was generated by layer base_drop.\r\nNote that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\r\nThe tensor that caused the issue was: base_drop/cond/Identity:0\r\n\r\nValueError: Input tensors to a Functional must come from `tf.keras.Input`. Received: 0 (missing previous layer metadata).\r\n\r\n```\r\nAlso, if i am doing\r\n```\r\nsecond_half  = Model(inputs=model.get_layer('dense_alpha').input,\r\n                            outputs=model.get_layer('dense_out').output)\r\n```\r\nI get the following error\r\n\r\n`Graph disconnected: cannot obtain value for tensor Tensor(\"input_1:0\", shape=(None, 300, 480, 3), dtype=float32) at layer \"xception\". The following previous layers were accessed without issue: []`\r\n\r\nI didn't got these errors with tf 2.2.0 + i have read that the behavior of tf.keras.backend.function has changed in the 2.3.0\r\nIs this a bug or it's something expected. In the second case, what should i change in order to achieve previous functionality?\r\n\r\nI need this split in order to run base_model only one time with my test dataset and pass the results through the function multiple times with active second dropout\r\n                ", "comments": ["Do you have a very, very minimal standalone code (or Colab) code snippet that we could just copy and run to reproduce this?", "> \r\n> \r\n> Do you have a very, very minimal standalone code (or Colab) code snippet that we could just copy and run to reproduce this?\r\n\r\nIs this ok?>>>\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.applications.xception import Xception\r\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D\r\nfrom tensorflow.keras.optimizers import Adam\r\nfrom tensorflow.keras import backend as K\r\n\r\ninp = Input((300,480,3))\r\nbase_model = Xception(include_top=False,\r\n                              weights='imagenet',\r\n                              input_tensor=inp,\r\n                              pooling='avg')\r\n\r\nfor layer in base_model.layers[:65]:\r\n\tlayer.trainable = False\r\nfor layer in base_model.layers[65:]:\r\n\tlayer.trainable = True\r\n\t\r\n\t\r\nx = base_model(inp)\r\n\r\nx = Dropout(0.5, name='base_drop')(x)\r\ndense_alpha = Dense(1024, activation='relu', name='dense_alpha')(x)\r\ndense_alpha = Dropout(0.5, name='dense_alpha_drop')(dense_alpha)\r\ndense_alpha = Dense(2, activation='softmax', name='dense_out')(dense_alpha)\r\n\r\nlosses = [tf.keras.losses.BinaryCrossentropy()]\r\n\r\nmodel = Model(inputs=inp, outputs=[dense_alpha])\r\nmodel.compile(optimizer=Adam(lr=0.0001),\r\n\t\t\t  loss=losses,\r\n\t\t\t  metrics=['accuracy'])\r\n\r\nmodel_base = Model(inputs=model.input, outputs=model.get_layer('base_drop').output)\r\n                \r\nsecond_part = K.function([model.get_layer('dense_alpha').input, K.learning_phase()],\r\n                            [model.get_layer('dense_out').output])\r\n```", "The example it is enough thanks. It seems similar to https://stackoverflow.com/questions/63238203/how-to-get-intermediate-outputs-in-tf-2-3-eager-with-learning-phase", "> \r\n> \r\n> The example it is enough thanks. It seems similar to https://stackoverflow.com/questions/63238203/how-to-get-intermediate-outputs-in-tf-2-3-eager-with-learning-phase\r\n\r\nYes. The thing is that if i will replace last line with \r\n\r\n```\r\nsecond_part = Model([model.get_layer('dense_alpha').input],\r\n                            [model.get_layer('dense_out').output])\r\n```\r\nI'll get this error:\r\n`\r\nValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"input_1:0\", shape=(None, 300, 480, 3), dtype=float32) at layer \"xception\". The following previous layers were accessed without issue: []`\r\n\r\nSo, the question is: is this a bug, or it is expected behavior. If it is expected, why it worked with previous tf versions?\r\n\r\n~~P.S. from tensorflow.python.keras.backend import eager_learning_phase_scope will solve/hide the error, but still, it looks like a workaround instead of an explanation why it doesn't work now~~\r\n\r\n", "EDIT: from tensorflow.python.keras.backend import eager_learning_phase_scope is not helping, since the error is happening in this part: \r\nsecond_part = K.function([model.get_layer('dense_alpha').input, K.learning_phase()],\r\n                            [model.get_layer('dense_out').output])\r\nIn other words, solution from stackoverflow doesn't work", "When you use\r\n```\r\nsecond_part = Model([model.get_layer('dense_alpha').input],\r\n                            [model.get_layer('dense_out').output])\r\n```\r\nYou are in the same case as\r\nhttps://github.com/tensorflow/tensorflow/issues/43025", "> \r\n> \r\n> When you use\r\n> \r\n> ```\r\n> second_part = Model([model.get_layer('dense_alpha').input],\r\n>                             [model.get_layer('dense_out').output])\r\n> ```\r\n> \r\n> You are in the same case as\r\n> #43025\r\n\r\nWell, as i understand, in current implementation, the method with Model is a bit harder to use if i have branches in the network(which is my case) and at the same time, the Backend.function will not behave as it was in the past, so i will not be able to use it. In other words, in my case the best method would be to downgrade the tf version OR disable the eager execution and use backend.function, since it will go to another branch", "You can explore to reorganize with subclassed as suggested in https://github.com/tensorflow/tensorflow/issues/43025#issuecomment-688957850", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44838\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44838\">No</a>\n"]}, {"number": 44836, "title": "How to accelerate tensor transferring from host to device (GPU) in TF 2.x?", "body": "**Describe the current behavior**\r\nHi, I want to train my DNN model with TF. And now the input is from `tf.data.Dataset.from_tensor_slices`, which will prefetch the numpy value into TF tensors. \r\n\r\nBecause the training is on GPU, therefore tensor transferring from CPU to GPU is triggerd automatically, which results in bad performance in my training process.\r\n\r\nSpecifically, one iteration is about 205 ms, while data transfering from CPU to GPU will spend 136 ms.\r\n![image](https://user-images.githubusercontent.com/69858819/99072211-8cd2c700-25ee-11eb-946d-e017575b586f.png)\r\n\r\n\r\n**Describe the expected behavior**\r\nIs there any way to change the memory used by `tf.data.Dataset` to CPU pinned memory rather than pageable? So that tensor transferring from CPU to GPU will be much faster.\r\n\r\nOr any other methods I can try to accelerate data reading from numpy to TF tensors on GPU? \r\n", "comments": ["Have you already read this to optimize your data pipeline for GPU: https://www.tensorflow.org/guide/data_performance?", "> Have you already read this to optimize your data pipeline for GPU: https://www.tensorflow.org/guide/data_performance?\r\n\r\n@bhack  Yes, I did. \r\nCurrently, my data pipeline is: reading data from numpy values, then prefetch it. No other operations (such as map, interleave).\r\n ", "If you want to control the prefect to GPU device you could try `prefetch_to_device`. Check if it is not currently broken for your use case https://github.com/tensorflow/tensorflow/issues/43905", "> If you want to control the prefect to GPU device you could try `prefetch_to_device`. Check if it is not currently broken for your use case #43905\r\n\r\nThanks for the link.\r\n\r\nI changed the last line of my dataset to `dataset = dataset.apply(tf.data.experimental.prefetch_to_device(device='/gpu:0', buffer_size=16))`, and iteration elapsed time is less than before (`dataset.prefetch(buffer_size=16)`).\r\n\r\nAnd now, the timeline is:\r\n![image](https://user-images.githubusercontent.com/69858819/99096582-6a05da00-2611-11eb-9260-90953b5a7d86.png)\r\n\r\nThere are **still memcyp H2D**, although it is faster than before.\r\n\r\nSeems like `prefetch_to_device` does not really `prefetch` datas?\r\n", "I think that you can comment after https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-720453284", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "I think we can close this as It Is tracked in the other ticket.", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44834, "title": "NotFoundError when using an optimizer on complex variables", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (minimal working example provided)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-3.10.0-1127.19.1.el7.x86_64-x86_64-with-glibc2.10\r\n- TensorFlow installed from (source or binary): Binary (installed using conda)\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: Quadro P1000 96GB\r\n\r\n**Describe the current behavior**\r\nWhen I attempt to optimise a loss function in complex variables, I get a NotFoundError when using the apply_gradients function. The error persists for all optimisers that I have tried (SGD is shown in the example). If I replace the complex variable with a float there are no issues.\r\n\r\n**Describe the expected behavior**\r\nApply_gradients should carry out an SGD step.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.config.list_physical_devices('GPU'))\r\n\r\n# Initialise a complex matrix\r\nmat = tf.random.uniform([1000, 1000], dtype=tf.float64)\r\nmat = tf.complex(mat, mat)\r\n\r\nvar = tf.Variable(mat, trainable=True)\r\n\r\n# Return the squared norm of this matrix as the loss function\r\ndef lossFn():\r\n    return tf.math.abs(tf.linalg.trace(var @ tf.linalg.adjoint(var)))\r\n\r\n# SGD optimizer\r\nopt = tf.keras.optimizers.SGD(learning_rate=0.01)\r\n\r\nnumSteps=0\r\nwhile numSteps < 100:\r\n    with tf.GradientTape() as tape:\r\n        loss = lossFn()\r\n    grads = tape.gradient(loss, [var])\r\n\r\n    # This is the step that fails\r\n    opt.apply_gradients(zip(grads, [var]))\r\n    numSteps += 1\r\n    print(loss.numpy())\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nTraceback (most recent call last):\r\n  File \"gpuTest.py\", line 25, in <module>\r\n    opt.apply_gradients(zip(grads, [var]))\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 504, in apply_gradients\r\n    return distribute_ctx.get_replica_context().merge_call(\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2420, in merge_call\r\n    return self._merge_call(merge_fn, args, kwargs)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2427, in _merge_call\r\n    return merge_fn(self._strategy, *args, **kwargs)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 282, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 591, in _distributed_apply\r\n    update_ops.extend(distribution.extended.update(\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2013, in update\r\n    return self._update(var, fn, args, kwargs, group)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2659, in _update\r\n    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 2665, in _update_non_slot\r\n    result = fn(*args, **kwargs)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 282, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 567, in apply_grad_to_update_var\r\n    update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/gradient_descent.py\", line 143, in _resource_apply_dense\r\n    return training_ops.resource_apply_gradient_descent(\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/gen_training_ops.py\", line 1908, in resource_apply_gradient_descent\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/rds/general/user/dlh16/home/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 6653, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'ResourceApplyGradientDescent' OpKernel for 'GPU' devices compatible with node {{node ResourceApplyGradientDescent}}\r\n         (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_COMPLEX128, use_locking=true\r\n        .  Registered:  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n [Op:ResourceApplyGradientDescent]\r\n", "comments": ["@davidho95 \r\nI ran the code on tf nightly and do not face any error please find the [gist here](https://colab.research.google.com/gist/Saduf2019/1b9f65c07c9a31a02c323503bb51b759/untitled458.ipynb).", "Hi Saduf,\r\n\r\nThank you for your reply.\r\n\r\nWhen I run the notebook you link, the line ```print(tf.config.list_physical_devices('GPU'))``` outputs an empty list, and the calculation is much slower than expected on a GPU. I think the process is running on the CPU rather than the GPU; is there any way to test this?", "@davidho95 \r\nI ran the code on gpu and its much faster but with the fist output as blank, please find the [gist here](https://colab.research.google.com/gist/Saduf2019/e132c8e0ca302b28b2336bbcf08b3c47/untitled458.ipynb).", "When I try to run the notebook the log files show:\r\n\r\n2020-11-13 17:18:05.423840: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\r\n\r\nand other errors indicating no GPU is being used", "@davidho95 \r\nPlease click on \"runtime\">\"change runtime\">\"gpu\" option and re run.", "This still occurs even with the \"runtime\">\"change runtime\">\"gpu\" set", "Apparently this is an [issue](https://github.com/googlecolab/colabtools/issues/1574) that others have experienced with tf-nightly builds on colab", "This issue here is that we don't have good complex128 support on GPU for these kernels https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L838\r\n\r\nThis depends on some support in Eigen - https://gitlab.com/libeigen/eigen/-/issues/1905 which hasn't been prioritized for some time and I'm not sure how quickly we'll be able to get to it. \r\n\r\nIs this blocking your work?", "> This issue here is that we don't have good complex128 support on GPU for these kernels https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/training_ops.cc#L838\r\n> \r\n> This depends on some support in Eigen - https://gitlab.com/libeigen/eigen/-/issues/1905 which hasn't been prioritized for some time and I'm not sure how quickly we'll be able to get to it.\r\n> \r\n> Is this blocking your work?\r\n\r\nHi Rohan,\r\n\r\nThank you for this, it's good to know it is not a problem with my build. Initially I thought this was prohibitive to my work, but actually I have realised the nature of the calculations I am doing don't benefit from GPU optimisation (they involve batch multiplication of large numbers of small matrices). So my current work no longer requires GPU support.\r\n\r\nIn the future calculations with complex numbers on GPUs may be very useful though: I work in theoretical physics where complex numbers are ubiquitous. I think TensorFlow's tools are very well suited to calculations in my field and if this support were available I think there is a significant potential user base. ", "Hey! I am working on complex valued neural networks that require complex valued backpropagation. Is there any chance this will be released anytime soon?", "In case this is useful to anyone, I found a workaround for this issue by definining the real and complex part of each weight separately and joining them as a complex tensor using tf.complex() during forprop.\r\n\r\nHowerver, I'm not entirely sure how this affects the computation of momenta of optimizers such as Adam. It worked just as expected on my experiments tho.", "Was able to run the code without any error in Tensorflow GPU 2.5, please find the gist [here](https://colab.research.google.com/gist/sachinprasadhs/b0a1dd43e284259282c99c0a09f0f3a9/44834.ipynb). \r\nAlso, closing the issue since it is resolved in the latest version, feel free to reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44834\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44834\">No</a>\n"]}, {"number": 44832, "title": "Blas xGEMM launch failed: RTX3080 with CUDA 10.0", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04 LTS, used within a Docker container \r\n- TensorFlow installed from (source or binary): from Docker image tensorflow/tensorflow:1.14.0-gpu-py3\r\n- TensorFlow version (use command below): 1.14.0-gpu\r\n- Python version: 3.7\r\n- CUDA/cuDNN version: 10.0/7.6.0.64\r\n- GPU model and memory: 2xRTX 3080\r\n\r\n**Describe the current behavior**\r\nWhile loading a PointCNN model I got the following error (snippet): \r\n`(0) Internal: Blas xGEMM launch failed : a.shape=[1,20000,3], b.shape=[1,3,20000], m=20000, n=20000, k=3`\r\n\r\nTensorflow is run with the following config options:\r\n`  config.gpu_options.allow_growth = True`\r\n`  config.allow_soft_placement = True`\r\n\r\nI also tried setting them to false.\r\n\r\n**Describe the expected behavior**\r\nThe same model is successfully loaded on another machine with Ubuntu 20.04 LTS within the same Docker container but with Nvidia 970GTX.\r\n\r\nI think this may be related to CUDA 10.0 support on RTX 3080? \r\n\r\nFull error log:\r\n\r\n`Exception has occurred: InternalError`\r\n`2 root error(s) found.`\r\n`  (0) Internal: Blas xGEMM launch failed : a.shape=[1,20000,3], b.shape=[1,3,20000], m=20000, n=20000, k=3`\r\n`\t [[node while/generator_ops/PointCNN/MatMul (defined at src/src/tf_utils/pointfly.py:572) ]]`\r\n`  (1) Internal: Blas xGEMM launch failed : a.shape=[1,20000,3], b.shape=[1,3,20000], m=20000, n=20000, k=3`\r\n`\t [[node while/generator_ops/PointCNN/MatMul (defined at src/src/tf_utils/pointfly.py:572) ]]`\r\n`\t [[while/preprocessing_ops/batch_nonuniform_sample/while/concat_1/_807]]`\r\n`0 successful operations.`\r\n`0 derived errors ignored.`\r\n\r\n`Errors may have originated from an input operation.`\r\n`Input Source operations connected to node while/generator_ops/PointCNN/MatMul:`\r\n` while/preprocessing_ops/batch_nonuniform_sample/Reshape (defined at src/src/tf_utils/pointfly.py:370)`\r\n\r\n`Input Source operations connected to node while/generator_ops/PointCNN/MatMul:`\r\n` while/preprocessing_ops/batch_nonuniform_sample/Reshape (defined at src/src/tf_utils/pointfly.py:370)`\r\n", "comments": ["I think that `Docker image tensorflow/tensorflow:1.14.0-gpu-py3` doesn't support RTX3080. Can you try `tensorflow/tensorflow:2.4.0rc1-gpu`?\r\n", "> I think that `Docker image tensorflow/tensorflow:1.14.0-gpu-py3` doesn't support RTX3080. Can you try `tensorflow/tensorflow:2.4.0rc1-gpu`?\r\n\r\nYes, it seems to be working with 2.4.0rc1. Thank you!", "@AZadora,\r\nThank you for the update. Marking this issue as closed, as it is resolved. Please feel free to re-open if necessary."]}, {"number": 44831, "title": "The TFLite Android app throws Fatal Error: \"Buffer Overflow Exception\"", "body": "Hi there! \r\nI recerntly tried to build an Object Detection Android App using TFLite model. I built my own custom model (a Keras Model in HDF5 format) and converted the model succesfully into a custom TFLite model using the following command:\r\n\r\n`tflite_convert --keras_model_file=detect.h5 --output_file=detect.tflite --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_dev_values=127 --change_concat_input_ranges=false --allow_custom_ops`\r\n\r\nI further added the associated MetaData to this particular model using this code:\r\n\r\n`import tensorflow as tf\r\nfrom tflite_support import metadata as _metadata`\r\n\r\n`populator = _metadata.MetadataPopulator.with_model_file(\"detect.tflite\")\r\npopulator.load_associated_files([\"labelmap.txt\"])\r\npopulator.populate()`\r\n\r\n\r\nI then configured this model in the Android Package [Example](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) by tensorflow and made some tweaks to the _Build.gradle_ file, _DetectorActivity.java_ and _TFLiteObjectDetectionAPIModel.java_, respectively. I also made some UI changes according to what and how I needed it to look like. Additionally, I had to change the _'numBytesPerChannel'_\r\n value for Float model from '4' to '3' since I was getting an error like this:\r\n\r\n`Cannot convert between a TensorFlowLite buffer with XYZ bytes and a ByteBuffer with ABC bytes`\r\n\r\n\r\nThe build is successful yet the debugger throws me a fatal exception of \"BufferOverFlowError\". \r\n\r\n`11/13 14:57:02: Launching 'app' on Physical Device.\r\nInstall successfully finished in 16 s 851 ms.\r\n$ adb shell am start -n \"org.tensorflow.lite.examples.detection/org.tensorflow.lite.examples.detection.DetectorActivity\" -a android.intent.action.MAIN -c android.intent.category.LAUNCHER -D\r\nWaiting for application to come online: org.tensorflow.lite.examples.detection.test | org.tensorflow.lite.examples.detection\r\nWaiting for application to come online: org.tensorflow.lite.examples.detection.test | org.tensorflow.lite.examples.detection\r\nConnected to process 22667 on device 'samsung-sm_m315f-RZ8N50B0M5K'.\r\nWaiting for application to come online: org.tensorflow.lite.examples.detection.test | org.tensorflow.lite.examples.detection\r\nConnecting to org.tensorflow.lite.examples.detection\r\nConnected to the target VM, address: 'localhost:46069', transport: 'socket'\r\nCapturing and displaying logcat messages from application. This behavior can be disabled in the \"Logcat output\" section of the \"Debugger\" settings page.\r\nI/mples.detectio: Late-enabling -Xcheck:jni\r\nE/mples.detectio: Unknown bits set in runtime_flags: 0x8000\r\nD/ActivityThread: setConscryptValidator\r\n    setConscryptValidator - put\r\nW/ActivityThread: Application org.tensorflow.lite.examples.detection is waiting for the debugger on port 8100...\r\nI/System.out: Sending WAIT chunk\r\nI/System.out: Debugger has connected\r\n    waiting for debugger to settle...\r\nI/chatty: uid=10379(org.tensorflow.lite.examples.detection) identical 1 line\r\nI/System.out: waiting for debugger to settle...\r\nI/System.out: waiting for debugger to settle...\r\nI/System.out: waiting for debugger to settle...\r\nI/System.out: waiting for debugger to settle...\r\nI/System.out: waiting for debugger to settle...\r\nI/System.out: waiting for debugger to settle...\r\nI/chatty: uid=10379(org.tensorflow.lite.examples.detection) identical 2 lines\r\nI/System.out: waiting for debugger to settle...\r\nI/System.out: waiting for debugger to settle...\r\nI/System.out: debugger has settled (1478)\r\nI/mples.detectio: Waiting for a blocking GC ClassLinker\r\nI/mples.detectio: WaitForGcToComplete blocked ClassLinker on ClassLinker for 7.502ms\r\nD/tensorflow: CameraActivity: onCreate org.tensorflow.lite.examples.detection.DetectorActivity@4d5b875\r\nD/PhoneWindow: forceLight changed to true [] from com.android.internal.policy.PhoneWindow.updateForceLightNavigationBar:4274 com.android.internal.policy.DecorView.updateColorViews:1547 com.android.internal.policy.PhoneWindow.dispatchWindowAttributesChanged:3252 android.view.Window.setFlags:1153 com.android.internal.policy.PhoneWindow.generateLayout:2474 \r\nI/MultiWindowDecorSupport: [INFO] isPopOver = false\r\nI/MultiWindowDecorSupport: updateCaptionType >> DecorView@59812d[], isFloating: false, isApplication: true, hasWindowDecorCaption: false, hasWindowControllerCallback: true\r\nD/MultiWindowDecorSupport: setCaptionType = 0, DecorView = DecorView@59812d[]\r\nW/mples.detectio: Accessing hidden method Landroid/view/View;->computeFitSystemWindows(Landroid/graphics/Rect;Landroid/graphics/Rect;)Z (greylist, reflection, allowed)\r\nW/mples.detectio: Accessing hidden method Landroid/view/ViewGroup;->makeOptionalFitsSystemWindows()V (greylist, reflection, allowed)\r\nI/CameraManagerGlobal: Connecting to camera service\r\nD/VendorTagDescriptor: addVendorDescriptor: vendor tag id 3854507339 added\r\nI/CameraManagerGlobal: Camera 0 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client com.snapchat.android API Level 1\r\nI/CameraManagerGlobal: Camera 1 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client com.dolby.dolby234 API Level 2\r\nI/CameraManagerGlobal: Camera 2 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client com.whatsapp API Level 1\r\nI/CameraManagerGlobal: Camera 20 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\nI/CameraManagerGlobal: Camera 23 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\nI/CameraManagerGlobal: Camera 3 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client com.sec.android.app.camera API Level 2\r\nI/CameraManagerGlobal: Camera 4 facing CAMERA_FACING_FRONT state now CAMERA_STATE_CLOSED for client vendor.client.pid<4503> API Level 2\r\nI/CameraManagerGlobal: Camera 50 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client com.sec.android.app.camera API Level 2\r\nI/CameraManagerGlobal: Camera 52 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\nI/CameraManagerGlobal: Camera 54 facing CAMERA_FACING_BACK state now CAMERA_STATE_CLOSED for client android.system API Level 2\r\nI/tensorflow: CameraActivity: Camera API lv2?: false\r\nD/tensorflow: CameraActivity: onStart org.tensorflow.lite.examples.detection.DetectorActivity@4d5b875\r\nD/tensorflow: CameraActivity: onResume org.tensorflow.lite.examples.detection.DetectorActivity@4d5b875\r\nI/ViewRootImpl@a101c3c[DetectorActivity]: setView = com.android.internal.policy.DecorView@59812d TM=true MM=false\r\nI/ViewRootImpl@a101c3c[DetectorActivity]: Relayout returned: old=(0,0,1080,2340) new=(0,0,1080,2340) req=(1080,2340)0 dur=31 res=0x7 s={true 532883185664} ch=true\r\nD/OpenGLRenderer: createReliableSurface : 0x7c1211ecc0(0x7c12502000)\r\nD/OpenGLRenderer: makeCurrent EglSurface : 0x0 -> 0x0\r\nI/mali_winsys: new_window_surface() [1080x2340] return: 0x3000\r\nD/OpenGLRenderer: eglCreateWindowSurface : 0x7c120c3600\r\nI/CameraManagerGlobal: Camera 0 facing CAMERA_FACING_BACK state now CAMERA_STATE_OPEN for client org.tensorflow.lite.examples.detection API Level 1\r\nI/tensorflow: CameraConnectionFragment: Desired size: 640x480, min size: 480x480\r\nI/tensorflow: CameraConnectionFragment: Valid preview sizes: [1920x1080, 1440x1080, 1280x720, 1088x1088, 1024x768, 960x720, 720x720, 720x480, 640x480]\r\nI/tensorflow: CameraConnectionFragment: Rejected preview sizes: [800x450, 640x360, 352x288, 320x240, 256x144, 176x144]\r\n    CameraConnectionFragment: Exact size match found.\r\nW/Gralloc3: mapper 3.x is not supported\r\nI/gralloc: Arm Module v1.0\r\nW/Gralloc3: allocator 3.x is not supported\r\nD/OpenGLRenderer: makeCurrent EglSurface : 0x0 -> 0x7c120c3600\r\nI/Choreographer: Skipped 34 frames!  The application may be doing too much work on its main thread.\r\nI/ViewRootImpl@a101c3c[DetectorActivity]: MSG_WINDOW_FOCUS_CHANGED 1 1\r\nD/InputMethodManager: prepareNavigationBarInfo() DecorView@59812d[DetectorActivity]\r\nD/InputMethodManager: getNavigationBarColor() -855310\r\nD/InputMethodManager: prepareNavigationBarInfo() DecorView@59812d[DetectorActivity]\r\nD/InputMethodManager: getNavigationBarColor() -855310\r\nV/InputMethodManager: Starting input: tba=org.tensorflow.lite.examples.detection ic=null mNaviBarColor -855310 mIsGetNaviBarColorSuccess true , NavVisible : true , NavTrans : false\r\nD/InputMethodManager: startInputInner - Id : 0\r\nI/InputMethodManager: startInputInner - mService.startInputOrWindowGainedFocus\r\nI/ViewRootImpl@a101c3c[DetectorActivity]: MSG_RESIZED: frame=(0,0,1080,2340) ci=(0,83,0,39) vi=(0,83,0,39) or=1\r\nD/InputMethodManager: prepareNavigationBarInfo() DecorView@59812d[DetectorActivity]\r\n    getNavigationBarColor() -855310\r\nV/InputMethodManager: Starting input: tba=org.tensorflow.lite.examples.detection ic=null mNaviBarColor -855310 mIsGetNaviBarColorSuccess true , NavVisible : true , NavTrans : false\r\nD/InputMethodManager: startInputInner - Id : 0\r\nI/CameraManagerGlobal: Camera 0 facing CAMERA_FACING_BACK state now CAMERA_STATE_ACTIVE for client org.tensorflow.lite.examples.detection API Level 1\r\nW/TFLiteObjectDetectionAPIModelWithInterpreter: cow1\r\n    cow2\r\n    cow3\r\n    cow4\r\nW/TFLiteObjectDetectionAPIModelWithInterpreter: cow5\r\n    cow6\r\nI/tflite: Initialized TensorFlow Lite runtime.\r\nI/tensorflow: DetectorActivity: Camera orientation relative to screen canvas: 90\r\nI/tensorflow: DetectorActivity: Initializing at size 640x480\r\nI/tensorflow: DetectorActivity: Preparing image 1 for detection in bg thread.\r\nW/System: A resource failed to call close. \r\nI/tensorflow: DetectorActivity: Running detection on image 1`\r\n\r\n**E/AndroidRuntime: FATAL EXCEPTION: inference\r\n    Process: org.tensorflow.lite.examples.detection, PID: 22667\r\n    java.nio.BufferOverflowException\r\n        at java.nio.Buffer.nextPutIndex(Buffer.java:542)\r\n        at java.nio.DirectByteBuffer.putFloat(DirectByteBuffer.java:809)\r\n        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:187)\r\n        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:183)\r\n        at android.os.Handler.handleCallback(Handler.java:883)\r\n        at android.os.Handler.dispatchMessage(Handler.java:100)\r\n        at android.os.Looper.loop(Looper.java:237)\r\n        at android.os.HandlerThread.run(HandlerThread.java:67)\r\nI/Process: Sending signal. PID: 22667 SIG: 9\r\nDisconnected from the target VM, address: 'localhost:46069', transport: 'socket'**\r\n\r\n\r\n\r\nThe error suggests a change in these lines:\r\n1. In TFLiteObjectDetectionAPIModel.java:\r\n  private static final float IMAGE_MEAN = 127.5f;\r\n  private static final float IMAGE_STD = 127.5f;\r\n\r\n  //...\r\n\r\n@Override\r\n  protected void addPixelValue(int pixelValue) {\r\n    **imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);**\r\n    imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n    imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n  }\r\n\r\n2. DetectorActivity.java:\r\n @Override\r\n   public void run() {\r\n   LOGGER.i(\"Running detection on image \" + currTimestamp);\r\n   final long startTime = SystemClock.uptimeMillis();\r\n  \r\n **final List<Detector.Recognition> results = detector.recognizeImage(croppedBitmap);**\r\n\r\n   lastProcessingTimeMs = SystemClock.uptimeMillis() - startTime;\r\n\r\n\r\nPlease let me know if I missed any step or did anything wrong.\r\n\r\nP. S. - I used a dull-trained model before this and the app worked just fine, except for the fact that it showed all the boundary boxes at once with negligible changes in any detections. I am currently using a well trained model which looks like this (via netron):\r\n\r\n![detect tflite](https://user-images.githubusercontent.com/38679773/99057523-e83a9000-25c1-11eb-8fb2-e9515ced1187.png)\r\n", "comments": ["@AbhiAva \r\nPlease share complete indented code for us to replicate the issue or if possible share a colab gist with the error reported.\r\n\r\nPlease refer to this issue with respect to the error reported: [link](https://github.com/tensorflow/tensorflow/issues/32970), [link1](https://stackoverflow.com/questions/20785409/what-is-the-cause-of-bufferoverflowexception)", "> @AbhiAva\r\n> Please share complete indented code for us to replicate the issue or if possible share a colab gist with the error reported.\r\n> \r\n> Please refer to this issue with respect to the error reported: [link](https://github.com/tensorflow/tensorflow/issues/32970), [link1](https://stackoverflow.com/questions/20785409/what-is-the-cause-of-bufferoverflowexception)\r\n\r\nHi. \r\nThank you for the links! I'll follow it and let you know.\r\n\r\nAbout the complete code, \r\n**1. Here is the code for DetectorActivity.java:**\r\n`\r\npackage org.tensorflow.lite.examples.detection;\r\n\r\nimport android.graphics.Bitmap;\r\nimport android.graphics.Bitmap.Config;\r\nimport android.graphics.Canvas;\r\nimport android.graphics.Color;\r\nimport android.graphics.Matrix;\r\nimport android.graphics.Paint;\r\nimport android.graphics.Paint.Style;\r\nimport android.graphics.RectF;\r\nimport android.graphics.Typeface;\r\nimport android.media.ImageReader.OnImageAvailableListener;\r\nimport android.os.SystemClock;\r\nimport android.util.Size;\r\nimport android.util.TypedValue;\r\nimport android.widget.Toast;\r\n\r\nimport org.tensorflow.lite.examples.detection.customview.OverlayView;\r\nimport org.tensorflow.lite.examples.detection.customview.OverlayView.DrawCallback;\r\nimport org.tensorflow.lite.examples.detection.env.BorderedText;\r\nimport org.tensorflow.lite.examples.detection.env.ImageUtils;\r\nimport org.tensorflow.lite.examples.detection.env.Logger;\r\nimport org.tensorflow.lite.examples.detection.tflite.Detector;\r\nimport org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel;\r\nimport org.tensorflow.lite.examples.detection.tracking.MultiBoxTracker;\r\n\r\nimport java.io.IOException;\r\nimport java.util.ArrayList;\r\nimport java.util.List;\r\n\r\n/**\r\n * An activity that uses a TensorFlowMultiBoxDetector and ObjectTracker to detect and then track\r\n * objects.\r\n */\r\npublic class DetectorActivity extends CameraActivity implements OnImageAvailableListener {\r\n  private static final Logger LOGGER = new Logger();\r\n\r\n  // Configuration values for the prepackaged SSD model.\r\n  private static final int TF_OD_API_INPUT_SIZE = 300;\r\n  private static final boolean TF_OD_API_IS_QUANTIZED = false;\r\n  private static final String TF_OD_API_MODEL_FILE = \"detect.tflite\";\r\n  private static final String TF_OD_API_LABELS_FILE = \"labelmap.txt\";\r\n  private static final DetectorMode MODE = DetectorMode.TF_OD_API;\r\n  // Minimum detection confidence to track a detection.\r\n  private static final float MINIMUM_CONFIDENCE_TF_OD_API = 0.2f;\r\n  private static final boolean MAINTAIN_ASPECT = false;\r\n  private static final Size DESIRED_PREVIEW_SIZE = new Size(640, 480);\r\n  private static final boolean SAVE_PREVIEW_BITMAP = false;\r\n  private static final float TEXT_SIZE_DIP = 10;\r\n  OverlayView trackingOverlay;\r\n  private Integer sensorOrientation;\r\n\r\n  private Detector detector;\r\n\r\n  private long lastProcessingTimeMs;\r\n  private Bitmap rgbFrameBitmap = null;\r\n  private Bitmap croppedBitmap = null;\r\n  private Bitmap cropCopyBitmap = null;\r\n\r\n  private boolean computingDetection = false;\r\n\r\n  private long timestamp = 0;\r\n\r\n  private Matrix frameToCropTransform;\r\n  private Matrix cropToFrameTransform;\r\n\r\n  private MultiBoxTracker tracker;\r\n\r\n  private BorderedText borderedText;\r\n\r\n  @Override\r\n  public void onPreviewSizeChosen(final Size size, final int rotation) {\r\n    final float textSizePx =\r\n        TypedValue.applyDimension(\r\n            TypedValue.COMPLEX_UNIT_DIP, TEXT_SIZE_DIP, getResources().getDisplayMetrics());\r\n    borderedText = new BorderedText(textSizePx);\r\n    borderedText.setTypeface(Typeface.MONOSPACE);\r\n\r\n    tracker = new MultiBoxTracker(this);\r\n\r\n    int cropSize = TF_OD_API_INPUT_SIZE;\r\n\r\n    try {\r\n      detector =\r\n          TFLiteObjectDetectionAPIModel.create(\r\n              this,\r\n              TF_OD_API_MODEL_FILE,\r\n              TF_OD_API_LABELS_FILE,\r\n              TF_OD_API_INPUT_SIZE,\r\n              TF_OD_API_IS_QUANTIZED);\r\n      cropSize = TF_OD_API_INPUT_SIZE;\r\n    } catch (final IOException e) {\r\n      e.printStackTrace();\r\n      LOGGER.e(e, \"Exception initializing Detector!\");\r\n      Toast toast =\r\n          Toast.makeText(\r\n              getApplicationContext(), \"Detector could not be initialized\", Toast.LENGTH_SHORT);\r\n      toast.show();\r\n      finish();\r\n    }\r\n\r\n    previewWidth = size.getWidth();\r\n    previewHeight = size.getHeight();\r\n\r\n    sensorOrientation = rotation - getScreenOrientation();\r\n    LOGGER.i(\"Camera orientation relative to screen canvas: %d\", sensorOrientation);\r\n\r\n    LOGGER.i(\"Initializing at size %dx%d\", previewWidth, previewHeight);\r\n    rgbFrameBitmap = Bitmap.createBitmap(previewWidth, previewHeight, Config.ARGB_8888);\r\n    croppedBitmap = Bitmap.createBitmap(cropSize, cropSize, Config.ARGB_8888);\r\n\r\n    frameToCropTransform =\r\n        ImageUtils.getTransformationMatrix(\r\n            previewWidth, previewHeight,\r\n            cropSize, cropSize,\r\n            sensorOrientation, MAINTAIN_ASPECT);\r\n\r\n    cropToFrameTransform = new Matrix();\r\n    frameToCropTransform.invert(cropToFrameTransform);\r\n\r\n    trackingOverlay = (OverlayView) findViewById(R.id.tracking_overlay);\r\n    trackingOverlay.addCallback(\r\n        new DrawCallback() {\r\n          @Override\r\n          public void drawCallback(final Canvas canvas) {\r\n            tracker.draw(canvas);\r\n            if (isDebug()) {\r\n              tracker.drawDebug(canvas);\r\n            }\r\n          }\r\n        });\r\n\r\n    tracker.setFrameConfiguration(previewWidth, previewHeight, sensorOrientation);\r\n  }\r\n\r\n  @Override\r\n  protected void processImage() {\r\n    ++timestamp;\r\n    final long currTimestamp = timestamp;\r\n    trackingOverlay.postInvalidate();\r\n\r\n    // No mutex needed as this method is not reentrant.\r\n    if (computingDetection) {\r\n      readyForNextImage();\r\n      return;\r\n    }\r\n    computingDetection = true;\r\n    LOGGER.i(\"Preparing image \" + currTimestamp + \" for detection in bg thread.\");\r\n\r\n    rgbFrameBitmap.setPixels(getRgbBytes(), 0, previewWidth, 0, 0, previewWidth, previewHeight);\r\n\r\n    readyForNextImage();\r\n\r\n    final Canvas canvas = new Canvas(croppedBitmap);\r\n    canvas.drawBitmap(rgbFrameBitmap, frameToCropTransform, null);\r\n    // For examining the actual TF input.\r\n    if (SAVE_PREVIEW_BITMAP) {\r\n      ImageUtils.saveBitmap(croppedBitmap);\r\n    }\r\n\r\n    runInBackground(\r\n        new Runnable() {\r\n          @Override\r\n          public void run() {\r\n            LOGGER.i(\"Running detection on image \" + currTimestamp);\r\n            final long startTime = SystemClock.uptimeMillis();\r\n            final List<Detector.Recognition> results = detector.recognizeImage(croppedBitmap);\r\n            lastProcessingTimeMs = SystemClock.uptimeMillis() - startTime;\r\n\r\n            cropCopyBitmap = Bitmap.createBitmap(croppedBitmap);\r\n            final Canvas canvas = new Canvas(cropCopyBitmap);\r\n            final Paint paint = new Paint();\r\n            paint.setColor(Color.RED);\r\n            paint.setStyle(Style.STROKE);\r\n            paint.setStrokeWidth(2.0f);\r\n\r\n            float minimumConfidence = MINIMUM_CONFIDENCE_TF_OD_API;\r\n            switch (MODE) {\r\n              case TF_OD_API:\r\n                minimumConfidence = MINIMUM_CONFIDENCE_TF_OD_API;\r\n                break;\r\n            }\r\n\r\n            final List<Detector.Recognition> mappedRecognitions =\r\n                new ArrayList<Detector.Recognition>();\r\n\r\n            for (final Detector.Recognition result : results) {\r\n              final RectF location = result.getLocation();\r\n              if (location != null && result.getConfidence() >= minimumConfidence) {\r\n                canvas.drawRect(location, paint);\r\n\r\n                cropToFrameTransform.mapRect(location);\r\n\r\n                result.setLocation(location);\r\n                mappedRecognitions.add(result);\r\n              }\r\n            }\r\n\r\n            tracker.trackResults(mappedRecognitions, currTimestamp);\r\n            trackingOverlay.postInvalidate();\r\n\r\n            computingDetection = false;\r\n\r\n            runOnUiThread(\r\n                new Runnable() {\r\n                  @Override\r\n                  public void run() {\r\n                    showFrameInfo(previewWidth + \"x\" + previewHeight);\r\n                    showCropInfo(cropCopyBitmap.getWidth() + \"x\" + cropCopyBitmap.getHeight());\r\n                    showInference(lastProcessingTimeMs + \"ms\");\r\n                  }\r\n                });\r\n          }\r\n        });\r\n  }\r\n\r\n  @Override\r\n  protected int getLayoutId() {\r\n    return R.layout.tfe_od_camera_connection_fragment_tracking;\r\n  }\r\n\r\n  @Override\r\n  protected Size getDesiredPreviewFrameSize() {\r\n    return DESIRED_PREVIEW_SIZE;\r\n  }\r\n\r\n  // Which detection model to use: by default uses Tensorflow Object Detection API frozen\r\n  // checkpoints.\r\n  private enum DetectorMode {\r\n    TF_OD_API;\r\n  }\r\n\r\n  @Override\r\n  protected void setUseNNAPI(final boolean isChecked) {\r\n    runInBackground(\r\n        () -> {\r\n          try {\r\n            detector.setUseNNAPI(isChecked);\r\n          } catch (UnsupportedOperationException e) {\r\n            LOGGER.e(e, \"Failed to set \\\"Use NNAPI\\\".\");\r\n            runOnUiThread(\r\n                () -> {\r\n                  Toast.makeText(this, e.getMessage(), Toast.LENGTH_LONG).show();\r\n                });\r\n          }\r\n        });\r\n  }\r\n\r\n  @Override\r\n  protected void setNumThreads(final int numThreads) {\r\n    runInBackground(\r\n        () -> {\r\n          try {\r\n            detector.setNumThreads(numThreads);\r\n          } catch (IllegalArgumentException e) {\r\n            LOGGER.e(e, \"Failed to set multithreads.\");\r\n            runOnUiThread(\r\n                () -> {\r\n                  Toast.makeText(this, e.getMessage(), Toast.LENGTH_LONG).show();\r\n                });\r\n          }\r\n        });\r\n  }\r\n}\r\n\r\n`\r\n\r\n**2. Here is the code for TFLiteObjectDetectionModelAPI.java:**\r\n`\r\npackage org.tensorflow.lite.examples.detection.tflite;\r\n\r\nimport android.content.Context;\r\nimport android.content.res.AssetFileDescriptor;\r\nimport android.content.res.AssetManager;\r\nimport android.graphics.Bitmap;\r\nimport android.graphics.RectF;\r\nimport android.os.Trace;\r\nimport android.util.Log;\r\n\r\nimport org.tensorflow.lite.Interpreter;\r\nimport org.tensorflow.lite.support.metadata.MetadataExtractor;\r\n\r\nimport java.io.BufferedReader;\r\nimport java.io.FileInputStream;\r\nimport java.io.IOException;\r\nimport java.io.InputStreamReader;\r\nimport java.nio.ByteBuffer;\r\nimport java.nio.ByteOrder;\r\nimport java.nio.MappedByteBuffer;\r\nimport java.nio.channels.FileChannel;\r\nimport java.nio.charset.Charset;\r\nimport java.util.ArrayList;\r\nimport java.util.HashMap;\r\nimport java.util.List;\r\nimport java.util.Map;\r\n\r\nimport static java.lang.Math.min;\r\n\r\n\r\npublic class TFLiteObjectDetectionAPIModel implements Detector {\r\n  private static final String TAG = \"TFLiteObjectDetectionAPIModelWithInterpreter\";\r\n\r\n  // Only return this many results.\r\n  private static final int NUM_DETECTIONS = 10;\r\n  // Float model\r\n  private static final float IMAGE_MEAN = 127.5f;\r\n  private static final float IMAGE_STD = 127.5f;\r\n  // Number of threads in the java app\r\n  private static final int NUM_THREADS = 4;\r\n  private boolean isModelQuantized;\r\n  // Config values.\r\n  private int inputSize;\r\n  // Pre-allocated buffers.\r\n  private final List<String> labels = new ArrayList<>();\r\n  private int[] intValues;\r\n  // outputLocations: array of shape [Batchsize, NUM_DETECTIONS,4]\r\n  // contains the location of detected boxes\r\n  private float[][][] outputLocations;\r\n  // outputClasses: array of shape [Batchsize, NUM_DETECTIONS]\r\n  // contains the classes of detected boxes\r\n  private float[][] outputClasses;\r\n  // outputScores: array of shape [Batchsize, NUM_DETECTIONS]\r\n  // contains the scores of detected boxes\r\n  private float[][] outputScores;\r\n  // numDetections: array of shape [Batchsize]\r\n  // contains the number of detected boxes\r\n  private float[] numDetections;\r\n\r\n  private ByteBuffer imgData;\r\n\r\n  private MappedByteBuffer tfLiteModel;\r\n  private Interpreter.Options tfLiteOptions;\r\n  private Interpreter tfLite;\r\n\r\n  private TFLiteObjectDetectionAPIModel() {}\r\n\r\n  /** Memory-map the model file in Assets. */\r\n  private static MappedByteBuffer loadModelFile(AssetManager assets, String modelFilename)\r\n          throws IOException {\r\n    AssetFileDescriptor fileDescriptor = assets.openFd(modelFilename);\r\n    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\r\n    FileChannel fileChannel = inputStream.getChannel();\r\n    long startOffset = fileDescriptor.getStartOffset();\r\n    long declaredLength = fileDescriptor.getDeclaredLength();\r\n    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\r\n  }\r\n\r\n  /**\r\n   * Initializes a native TensorFlow session for classifying images.\r\n   *\r\n   * @param modelFilename The model file path relative to the assets folder\r\n   * @param labelFilename The label file path relative to the assets folder\r\n   * @param inputSize The size of image input\r\n   * @param isQuantized Boolean representing model is quantized or not\r\n   */\r\n  public static Detector create(\r\n          final Context context,\r\n          final String modelFilename,\r\n          final String labelFilename,\r\n          final int inputSize,\r\n          final boolean isQuantized)\r\n          throws IOException {\r\n    final TFLiteObjectDetectionAPIModel d = new TFLiteObjectDetectionAPIModel();\r\n\r\n    MappedByteBuffer modelFile = loadModelFile(context.getAssets(), modelFilename);\r\n    MetadataExtractor metadata = new MetadataExtractor(modelFile);\r\n    try (BufferedReader br =\r\n                 new BufferedReader(\r\n                         new InputStreamReader(\r\n                                 metadata.getAssociatedFile(labelFilename), Charset.defaultCharset()))) {\r\n      String line;\r\n      while ((line = br.readLine()) != null) {\r\n        Log.w(TAG, line);\r\n        d.labels.add(line);\r\n      }\r\n    }\r\n\r\n    d.inputSize = inputSize;\r\n\r\n    try {\r\n      Interpreter.Options options = new Interpreter.Options();\r\n      options.setNumThreads(NUM_THREADS);\r\n      d.tfLite = new Interpreter(modelFile, options);\r\n      d.tfLiteModel = modelFile;\r\n      d.tfLiteOptions = options;\r\n    } catch (Exception e) {\r\n      throw new RuntimeException(e);\r\n    }\r\n\r\n    d.isModelQuantized = isQuantized;\r\n    // Pre-allocate buffers.\r\n    int numBytesPerChannel;\r\n    if (isQuantized) {\r\n      numBytesPerChannel = 1; // Quantized\r\n    } else {\r\n      numBytesPerChannel = 3; // Floating point\r\n    }\r\n    d.imgData = ByteBuffer.allocateDirect(1 * d.inputSize * d.inputSize * 3 * numBytesPerChannel);\r\n    d.imgData.order(ByteOrder.nativeOrder());\r\n    d.intValues = new int[d.inputSize * d.inputSize];\r\n\r\n    d.outputLocations = new float[1][NUM_DETECTIONS][4];\r\n    d.outputClasses = new float[1][NUM_DETECTIONS];\r\n    d.outputScores = new float[1][NUM_DETECTIONS];\r\n    d.numDetections = new float[1];\r\n    return d;\r\n  }\r\n\r\n  @Override\r\n  public List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n\r\n    imgData.rewind();\r\n    for (int i = 0; i < inputSize; ++i) {\r\n      for (int j = 0; j < inputSize; ++j) {\r\n        int pixelValue = intValues[i * inputSize + j];\r\n        if (isModelQuantized) {\r\n          // Quantized model\r\n          imgData.put((byte) ((pixelValue >> 16) & 0xFF));\r\n          imgData.put((byte) ((pixelValue >> 8) & 0xFF));\r\n          imgData.put((byte) (pixelValue & 0xFF));\r\n        } else { // Float model\r\n          imgData.putFloat((((pixelValue >> 16) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat((((pixelValue >> 8) & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n          imgData.putFloat(((pixelValue & 0xFF) - IMAGE_MEAN) / IMAGE_STD);\r\n        }\r\n      }\r\n    }\r\n    Trace.endSection(); // preprocessBitmap\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n    outputLocations = new float[1][NUM_DETECTIONS][4];\r\n    outputClasses = new float[1][NUM_DETECTIONS];\r\n    outputScores = new float[1][NUM_DETECTIONS];\r\n    numDetections = new float[1];\r\n\r\n    Object[] inputArray = {imgData};\r\n    Map<Integer, Object> outputMap = new HashMap<>();\r\n    outputMap.put(0, outputLocations);\r\n    outputMap.put(1, outputClasses);\r\n    outputMap.put(2, outputScores);\r\n    outputMap.put(3, numDetections);\r\n    Trace.endSection();\r\n\r\n    // Run the inference call.\r\n    Trace.beginSection(\"run\");\r\n    tfLite.runForMultipleInputsOutputs(inputArray, outputMap);\r\n    Trace.endSection();\r\n\r\n    // Show the best detections.\r\n    // after scaling them back to the input size.\r\n    // You need to use the number of detections from the output and not the NUM_DETECTONS variable\r\n    // declared on top\r\n    // because on some models, they don't always output the same total number of detections\r\n    // For example, your model's NUM_DETECTIONS = 20, but sometimes it only outputs 16 predictions\r\n    // If you don't use the output's numDetections, you'll get nonsensical data\r\n    int numDetectionsOutput =\r\n            min(\r\n                    NUM_DETECTIONS,\r\n                    (int) numDetections[0]); // cast from float to integer, use min for safety\r\n\r\n    final ArrayList<Recognition> recognitions = new ArrayList<>(numDetectionsOutput);\r\n    for (int i = 0; i < numDetectionsOutput; ++i) {\r\n      final RectF detection =\r\n              new RectF(\r\n                      outputLocations[0][i][1] * inputSize,\r\n                      outputLocations[0][i][0] * inputSize,\r\n                      outputLocations[0][i][3] * inputSize,\r\n                      outputLocations[0][i][2] * inputSize);\r\n\r\n      recognitions.add(\r\n              new Recognition(\r\n                      \"\" + i, labels.get((int) outputClasses[0][i]), outputScores[0][i], detection));\r\n    }\r\n    Trace.endSection(); // \"recognizeImage\"\r\n    return recognitions;\r\n  }\r\n\r\n  @Override\r\n  public void enableStatLogging(final boolean logStats) {}\r\n\r\n  @Override\r\n  public String getStatString() {\r\n    return \"\";\r\n  }\r\n\r\n  @Override\r\n  public void close() {\r\n    if (tfLite != null) {\r\n      tfLite.close();\r\n      tfLite = null;\r\n    }\r\n  }\r\n\r\n  @Override\r\n  public void setNumThreads(int numThreads) {\r\n    if (tfLite != null) {\r\n      tfLiteOptions.setNumThreads(numThreads);\r\n      recreateInterpreter();\r\n    }\r\n  }\r\n\r\n  @Override\r\n  public void setUseNNAPI(boolean isChecked) {\r\n    if (tfLite != null) {\r\n      tfLiteOptions.setUseNNAPI(isChecked);\r\n      recreateInterpreter();\r\n    }\r\n  }\r\n\r\n  private void recreateInterpreter() {\r\n    tfLite.close();\r\n    tfLite = new Interpreter(tfLiteModel, tfLiteOptions);\r\n  }\r\n}\r\n`\r\n", "Hey @Saduf2019 .\r\nAny update?\r\nI am still stuck with the issue even after referring to the links you've provided. ", "Hola!\r\n\r\nI was able to solve the error. So basically, the tflite model that I used had a highly big input size. This happened since I used a custom model (this means, the fine-tuning was random and the model was hence not compatible with the android's TFLiteObjectDetectionAPI. \r\nAnother thing here is, I accidentally used mobilenet-v2 model as a reference model to train my own model whereas the default model that the TFLiteObjectDetectionAPI uses ssd-mobilenet-v1. I don't think this has anything to do with my error but this might throw a compatibility exception and hence lead to some ambiguous errors.\r\n\r\nThus I used this [link](https://github.com/practical-learning/object-detection-on-android) to train my model with custom parameters and had to make some changes in the pipeline.config file. Otherwise, the model that I've trained apparently works just fine and gives me apt results and is 70% accurate which is enough for my purpose as of now.\r\n\r\nThank you @Saduf2019 for your help. I appreciate the structure of Tensorflow's training work-flow. \r\n\r\nI am closing this issue right now as this was a small error when I think of it. \r\nCiao!"]}, {"number": 44830, "title": "The TFLite test reported an error", "body": "The Tflite model failed to test Android on the development board\r\nCompiled tflite library:\r\n![15b3b25151b93f1da414d6f36260ac7](https://user-images.githubusercontent.com/54087172/99056074-bb43a880-25d4-11eb-8226-44a1fb8d058d.jpg)\r\nerror:\r\n![a2e375014e28b2d18f295e065cd6e3d](https://user-images.githubusercontent.com/54087172/99056182-e3cba280-25d4-11eb-9fda-727686fa1f51.jpg)\r\n\r\nwhy not librk_codec.so? what is  librk_codec.so ?\r\n", "comments": ["@huafeihuayu \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]\r\n\r\nPlease paste the error message (using makrdown formatting around it) instead of screenshotting. Screenshots are not searchable so they don't help in looking for the issue and also don't help other people having the same error from finding about the issue.\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44830\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44830\">No</a>\n"]}, {"number": 44829, "title": "Incorrect File Formats mentioned and Raises Section is missing in ImageDataGenerator Documentation", "body": "The TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nPlease provide a link to the documentation entry, for example:\r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\r\n\r\n## Description of issue (what needs changing): \r\n\r\n1. In the description corresponding to `Save Format` argument in [ImageDataGenerator.flow](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow), the statement,\r\n\r\n> one of \"png\", \"jpeg\" (only relevant if\u00a0save_to_dir\u00a0is set). Default: \"png\" \r\n\r\nshould be changed to \r\n\r\n> one of \"png\", \"jpeg\", \"jpg\", \"bmp\", \"pdf\", \"gif\" (only relevant if\u00a0save_to_dir\u00a0is set). Default: \"png\" \r\n\r\nPlease find the [Github Gist](https://colab.research.google.com/gist/rmothukuru/a7b57e5dffaa290dd440ba26b33ea041/file_format_imagedatagenerator.ipynb) that demonstrates acceptance of other formats.\r\n\r\n2. **`Raises`** Error section is missing for `ImageDataGenerator`, `ImageDataGenerator.flow`, `ImageDataGenerator.flow_from_directory`, `ImageDataGenerator.flow_from_dataframe`, etc..\r\n\r\n### Clear description\r\n\r\n**`ImageDataGenerator`** is extremely useful for Computer Vision tasks. So, the more clear and better its documentation is, the more it will help developers.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct? : Yes\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly? : Yes\r\n\r\n### Returns defined\r\n\r\nAre return values defined? : Yes\r\n\r\n### Raises listed and defined : No\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises : No\r\n\r\n### Usage example\r\n\r\nIs there a usage example? : Missing for `ImageDataGenerator.flow_from_dataframe`\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content? : Yes\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": []}, {"number": 44828, "title": "Don't enable get_next_as_optional if the dataset is finite.", "body": "PiperOrigin-RevId: 341945136\nChange-Id: I79fdec366be2119b6a28063f193e6cecb7a5f9e2", "comments": []}, {"number": 44827, "title": "Adding the description corresponding to subset=validation, added raises section and modified save_format", "body": "Addresses the doc issue #44587, #44829", "comments": ["We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44827) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44827) for more info**.\n\n<!-- need_author_cla -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and then comment `@googlebot I fixed it.`. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44827) for more info**.\n\n<!-- need_author_cla -->", "cc @markomernick", "there are still sanity build failures , can you please check.", "> there are still sanity build failures , can you please check.\r\n\r\nThey are resolved now, @rthadur "]}, {"number": 44826, "title": "Always enable get_next_as_optional unless the dataset is finite.", "body": "PiperOrigin-RevId: 341945136\nChange-Id: I79fdec366be2119b6a28063f193e6cecb7a5f9e2", "comments": []}, {"number": 44825, "title": "Add loss raises InaccessibleTensorError", "body": "I made a code for a model reffering to [https://www.tensorflow.org/guide/keras/custom_layers_and_models](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\r\n\r\nBut the code raised this error:\r\n```\r\n    InaccessibleTensorError: The tensor 'Tensor(\"add_1:0\", shape=(2,), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=140714038692944); accessed from: FuncGraph(name=train_function, id=140714048668056).\r\n```\r\nThe code has making custom model part using subclass api.<br>\r\n\r\nCustom model's call method  includes:\r\n```\r\n        ### VAE loss\r\n        loss_L2 = tf.reduce_mean(tf.square(Z - out_VAE), axis=(1, 2, 3, 4)) # original axis value is (1,2,3,4).\r\n        loss_KL = (1 / self.n) * tf.reduce_sum(\r\n            tf.exp(z_var) + tf.square(z_mean) - 1. - z_var,\r\n            axis=-1\r\n        )\r\n\r\n        VAE_loss = self.weight_L2 * loss_L2 + self.weight_KL * loss_KL\r\n        self.add_loss(VAE_loss)\r\n```\r\nWhen i commented this block, the error was not raised.\r\n\r\nFull reproducible code : https://colab.research.google.com/drive/1IpGEGSRP2c6AroCTvualVspqp-4Rnifr#scrollTo=8rFNHMCBpnIo", "comments": ["Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44825\">No</a>\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44825\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44825\">No</a>\n"]}, {"number": 44824, "title": "ERROR:tensorflow:   Failed to close session after error.Other threads may hang.", "body": "I am trying to pretrain my ELECTRA base, I keep getting this output:\r\n\r\nRunning training\r\n================================================================================\r\n2020-11-13 08:00:18.044763: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\r\nModel is built!\r\n2020-11-13 08:00:48.956655: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\r\nERROR:tensorflow:Error recorded from infeed: From /job:worker/replica:0/task:0:\r\n{{function_node __inference_tf_data_experimental_map_and_batch_<lambda>_69}} Key: segment_ids.  Can't parse serialized Example.\r\n\t [[{{node ParseSingleExample/ParseSingleExample}}]]\r\n\t [[input_pipeline_task0/while/IteratorGetNext]]\r\nERROR:tensorflow:Closing session due to error From /job:worker/replica:0/task:0:\r\n{{function_node __inference_tf_data_experimental_map_and_batch_<lambda>_69}} Key: segment_ids.  Can't parse serialized Example.\r\n\t [[{{node ParseSingleExample/ParseSingleExample}}]]\r\n\t [[input_pipeline_task0/while/IteratorGetNext]]\r\n2020-11-13 08:01:08.642776: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1605254468.642525410\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\r\n2020-11-13 08:01:08.642779: W tensorflow/core/distributed_runtime/rpc/grpc_remote_master.cc:157] RPC failed with status = \"Unavailable: Socket closed\" and grpc_error_string = \"{\"created\":\"@1605254468.642549072\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Socket closed\",\"grpc_status\":14}\", maybe retrying the RPC\r\nERROR:tensorflow:Error recorded from outfeed: Step was cancelled by an explicit call to `Session::Close()`.\r\nERROR:tensorflow:\r\n\r\n\r\nFailed to close session after error.Other threads may hang.\r\n\r\n\r\n\r\n2020-11-13 08:01:50.857700: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:370] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\r\nERROR:tensorflow:Error recorded from infeed: From /job:worker/replica:0/task:0:\r\n{{function_node __inference_tf_data_experimental_map_and_batch_<lambda>_69}} Key: segment_ids.  Can't parse serialized Example.\r\n\t [[{{node ParseSingleExample/ParseSingleExample}}]]\r\n\t [[input_pipeline_task0/while/IteratorGetNext]]", "comments": ["@etetteh \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "I am using  TensorFlow version 1.15.4, and a Google cloud TPU of the same version on GCP.\r\nI am also using python version 3.6\r\n\r\n```\r\nimport logging\r\nimport tensorflow as tf\r\n \r\nlog = logging.getLogger('tensorflow')\r\nlog.setLevel(logging.INFO)\r\n\r\nformatter = logging.Formatter('%(asctime)s :  %(message)s')\r\nsh = logging.StreamHandler()\r\nsh.setLevel(logging.INFO)\r\nsh.setFormatter(formatter)\r\nlog.handlers = [sh]\r\n```\r\n\r\nPretty much, what I am doing is running the following line of code:\r\n\r\n```\r\n python run_pretraining.py  \\\r\n                          --data-dir <my_data_dir> \\\r\n                          --model-name <my_model_name> \\\r\n                          --hparams '{\"model_size\": \"base\", \"use_tpu\": True, \"num_tpu_cores\": 8,  tpu_name: <my_ypu_name>,  tpu_zone: us-central1-f,  gcp_project: <my_gcp_id>}'\r\n```", "@etetteh \r\nCan you please upgrade to 2.x as there is no support for 1.x now, and let us know if you face any issues.", "@Saduf2019 The GOOGLE ELECTRA code base is in version 1.15, or you mean upgrade the version of the tpu?", "Closing this issue since its resolved on another [thread](https://github.com/google-research/electra/issues/102). Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44824\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44824\">No</a>\n"]}, {"number": 44823, "title": "[INTEL MKL] DNN 0.x code cleanup - forward conv ops", "body": "DNN 0.x cleanup of forward conv ops:\r\n(1) Remove all DNN 0.x related code;\r\n(2) Replace all DNN 1.x macro usages", "comments": []}, {"number": 44822, "title": "Weirdly cannot import tensorflow today (ImportError: cannot import name 'torch' from partially initialized module 'opt_einsum.backends' (most likely due to a circular import))", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina and Big Sur\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): `pipenv install tensorflow` and `pip install tensorflow`\r\n- TensorFlow version: 2.3.1\r\n- Python version: 3.8.5\r\n- Installed using virtualenv? pip? conda?: pip and pipenv install inside virtual environment\r\n- Bazel version (if compiling from source): NA\r\n- GCC/Compiler version (if compiling from source): NA\r\n- CUDA/cuDNN version: Not using CUDA\r\n- GPU model and memory: NA\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nCannot import tensorflow today. I searched my error, but nothing helps. I also searched \"circular import\", but I don't know how to fix it.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n1. pipenv shell\r\n2. pipenv install tensorflow\r\n3. jupyter lab\r\n4. import tensorflow as tf\r\n5. Error!\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\n<ipython-input-1-64156d691fe5> in <module>\r\n----> 1 import tensorflow as tf\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/__init__.py in <module>\r\n     39 import sys as _sys\r\n     40 \r\n---> 41 from tensorflow.python.tools import module_util as _module_util\r\n     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader\r\n     43 \r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/python/__init__.py in <module>\r\n     43 \r\n     44 # Bring in subpackages.\r\n---> 45 from tensorflow.python import data\r\n     46 from tensorflow.python import distribute\r\n     47 from tensorflow.python import keras\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/python/data/__init__.py in <module>\r\n     23 \r\n     24 # pylint: disable=unused-import\r\n---> 25 from tensorflow.python.data import experimental\r\n     26 from tensorflow.python.data.ops.dataset_ops import Dataset\r\n     27 from tensorflow.python.data.ops.dataset_ops import INFINITE as INFINITE_CARDINALITY\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/__init__.py in <module>\r\n    123 from tensorflow.python.data.experimental.ops.optimization_options import MapVectorizationOptions\r\n    124 from tensorflow.python.data.experimental.ops.optimization_options import OptimizationOptions\r\n--> 125 from tensorflow.python.data.experimental.ops.parsing_ops import parse_example_dataset\r\n    126 from tensorflow.python.data.experimental.ops.prefetching_ops import copy_to_device\r\n    127 from tensorflow.python.data.experimental.ops.prefetching_ops import prefetch_to_device\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/python/data/experimental/ops/parsing_ops.py in <module>\r\n     24 from tensorflow.python.framework import tensor_spec\r\n     25 from tensorflow.python.ops import gen_experimental_dataset_ops\r\n---> 26 from tensorflow.python.ops import parsing_ops\r\n     27 from tensorflow.python.ops.ragged import ragged_tensor\r\n     28 from tensorflow.python.util.tf_export import tf_export\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/python/ops/parsing_ops.py in <module>\r\n     25 from tensorflow.python.ops import gen_parsing_ops\r\n     26 from tensorflow.python.ops import math_ops\r\n---> 27 from tensorflow.python.ops import parsing_config\r\n     28 # go/tf-wildcard-import\r\n     29 # pylint: disable=wildcard-import,undefined-variable\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/python/ops/parsing_config.py in <module>\r\n     29 from tensorflow.python.ops import check_ops\r\n     30 from tensorflow.python.ops import math_ops\r\n---> 31 from tensorflow.python.ops import sparse_ops\r\n     32 from tensorflow.python.ops.ragged import ragged_math_ops\r\n     33 from tensorflow.python.ops.ragged import ragged_tensor\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/python/ops/sparse_ops.py in <module>\r\n     40 from tensorflow.python.ops import gen_sparse_ops\r\n     41 from tensorflow.python.ops import math_ops\r\n---> 42 from tensorflow.python.ops import special_math_ops\r\n     43 # go/tf-wildcard-import\r\n     44 # pylint: disable=wildcard-import\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/tensorflow/python/ops/special_math_ops.py in <module>\r\n     28 \r\n     29 import numpy as np\r\n---> 30 import opt_einsum\r\n     31 import six\r\n     32 \r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/opt_einsum/__init__.py in <module>\r\n      7 from . import paths\r\n      8 from . import path_random\r\n----> 9 from .contract import contract, contract_path, contract_expression\r\n     10 from .parser import get_symbol\r\n     11 from .sharing import shared_intermediates\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/opt_einsum/contract.py in <module>\r\n      8 import numpy as np\r\n      9 \r\n---> 10 from . import backends, blas, helpers, parser, paths, sharing\r\n     11 \r\n     12 __all__ = [\"contract_path\", \"contract\", \"format_const_einsum_str\", \"ContractExpression\", \"shape_only\"]\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/opt_einsum/backends/__init__.py in <module>\r\n      5 # Backends\r\n      6 from .cupy import to_cupy\r\n----> 7 from .dispatch import (get_func, has_einsum, has_tensordot, build_expression, evaluate_constants, has_backend)\r\n      8 from .tensorflow import to_tensorflow\r\n      9 from .theano import to_theano\r\n\r\n~/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/opt_einsum/backends/dispatch.py in <module>\r\n     14 from . import tensorflow as _tensorflow\r\n     15 from . import theano as _theano\r\n---> 16 from . import torch as _torch\r\n     17 \r\n     18 __all__ = [\"get_func\", \"has_einsum\", \"has_tensordot\", \"build_expression\", \"evaluate_constants\", \"has_backend\"]\r\n\r\nImportError: cannot import name 'torch' from partially initialized module 'opt_einsum.backends' (most likely due to a circular import) (/Users/anthony/Documents/Academic/USC/Fall 2020/DSCI 510/DSCI510-Final-Project/.venv/lib/python3.8/site-packages/opt_einsum/backends/__init__.py)\r\n\r\n\r\n```\r\n", "comments": ["My packages in virtualenv:\r\n\r\n```\r\nabsl-py==0.11.0\r\nappnope==0.1.0\r\nargon2-cffi==20.1.0\r\nastunparse==1.6.3\r\nasync-generator==1.10\r\nattrs==20.3.0\r\nbackcall==0.2.0\r\nbeautifulsoup4==4.9.3\r\nbing-image-downloader==1.0.4\r\nbleach==3.2.1\r\nbs4==0.0.1\r\ncachetools==4.1.1\r\ncertifi==2020.11.8\r\ncffi==1.14.3\r\nchardet==3.0.4\r\ncycler==0.10.0\r\ndataclasses==0.6\r\ndecorator==4.4.2\r\ndefusedxml==0.6.0\r\nentrypoints==0.3\r\nfuture==0.18.2\r\ngast==0.3.3\r\ngoogle-auth==1.23.0\r\ngoogle-auth-oauthlib==0.4.2\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.33.2\r\nh5py==2.10.0\r\nidna==2.10\r\nipykernel==5.3.4\r\nipython==7.19.0\r\nipython-genutils==0.2.0\r\nipywidgets==7.5.1\r\njedi==0.17.2\r\nJinja2==2.11.2\r\njoblib==0.17.0\r\njsonschema==3.2.0\r\njupyter-client==6.1.7\r\njupyter-core==4.6.3\r\njupyterlab-pygments==0.1.2\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.3.1\r\nMarkdown==3.3.3\r\nMarkupSafe==1.1.1\r\nmatplotlib==3.3.3\r\nmistune==0.8.4\r\nnbclient==0.5.1\r\nnbconvert==6.0.7\r\nnbformat==5.0.8\r\nnest-asyncio==1.4.3\r\nnotebook==6.1.5\r\nnumpy==1.19.4\r\noauthlib==3.1.0\r\nopencv-python==4.4.0.46\r\nopt-einsum==3.3.0\r\npackaging==20.4\r\npandas==1.1.4\r\npandocfilters==1.4.3\r\nparso==0.7.1\r\npexpect==4.8.0\r\npickleshare==0.7.5\r\nPillow==8.0.1\r\nprometheus-client==0.8.0\r\nprompt-toolkit==3.0.8\r\nprotobuf==3.13.0\r\nptyprocess==0.6.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npycparser==2.20\r\nPygments==2.7.2\r\npyparsing==2.4.7\r\npyrsistent==0.17.3\r\npython-dateutil==2.8.1\r\npytz==2020.4\r\npyzmq==19.0.2\r\nrequests==2.25.0\r\nrequests-oauthlib==1.3.0\r\nrsa==4.6\r\nscikit-learn==0.23.2\r\nscipy==1.5.4\r\nselenium==3.141.0\r\nSend2Trash==1.5.0\r\nsix==1.15.0\r\nsklearn==0.0\r\nsoupsieve==2.0.1\r\ntensorboard==2.4.0\r\ntensorboard-plugin-wit==1.7.0\r\ntensorflow==2.3.1\r\ntensorflow-estimator==2.3.0\r\ntermcolor==1.1.0\r\nterminado==0.9.1\r\ntestpath==0.4.4\r\nthreadpoolctl==2.1.0\r\ntorch==1.7.0\r\ntorchaudio==0.7.0\r\ntorchvision==0.8.1\r\ntornado==6.1\r\ntqdm==4.51.0\r\ntraitlets==5.0.5\r\ntyping-extensions==3.7.4.3\r\nurllib3==1.26.2\r\nwcwidth==0.2.5\r\nwebencodings==0.5.1\r\nWerkzeug==1.0.1\r\nwidgetsnbextension==3.5.1\r\nwrapt==1.12.1\r\n```", "@Anthonyive,\r\nCould you please try importing TensorFlow in a new virtual environment and check if you are facing the same issue? Thanks!", "> @Anthonyive,\r\n> Could you please try importing TensorFlow in a new virtual environment and check if you are facing the same issue? Thanks!\r\n\r\nIt seems like only this environment has this problem.", "I also tried reinstall tensorflow, pytorch, and opt_einsum, but none of them works", "@Anthonyive,\r\nIn order to expedite the trouble-shooting process, could you please provide the Python script/notebook you are running. Thanks!", "> @Anthonyive,\r\n> In order to expedite the trouble-shooting process, could you please provide the Python script/notebook you are running. Thanks!\r\n\r\nI am using jupyter lab.\r\n\r\nThank you for your help! I re-created the virtual environment and worked for some reason. I guess there're some conflicts with packages.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44822\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44822\">No</a>\n", "Hello, \r\n\r\nI have the same issue with a simple project.\r\n\r\nSource code from https://www.tensorflow.org/api_docs/python/tf/keras\r\n\r\nI just copied first strings - \"import\"\r\n\r\n```\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport os\r\nimport PIL\r\nimport tensorflow as tf\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.models import Sequential\r\n```\r\n\r\nOS - Wn 10 64 bit\r\nIDE - VS 2019 16.9.6\r\nPython 3.8.10 - 64 bit. (Downloaded from https://www.python.org/downloads/windows/)\r\n\r\nError \r\n`Cannot import name 'keras' from partially initialized module 'tensorflow' (most likely due to a circular import) (C:\\Users\\ytigiev\\source\\repos\\tensorflow\\tensorflow.py)`\r\n\r\n\r\nrequirements.txt\r\n```\r\nabsl-py==0.12.0\r\nastunparse==1.6.3\r\ncachetools==4.2.2\r\ncertifi==2020.12.5\r\nchardet==4.0.0\r\ncycler==0.10.0\r\nflatbuffers==2.0\r\ngast==0.4.0\r\ngoogle-auth==2.0.0.dev0\r\ngoogle-auth-oauthlib==0.4.4\r\ngoogle-pasta==0.2.0\r\ngrpcio==1.38.0\r\nh5py==3.2.1\r\nidna==3.1\r\nKeras==2.4.3\r\nkeras-nightly==2.5.0.dev2021032900\r\nKeras-Preprocessing==1.1.2\r\nkiwisolver==1.3.1\r\nMarkdown==3.3.4\r\nmatplotlib==3.4.2\r\nnumpy==1.20.3\r\noauthlib==3.1.0\r\nopt-einsum==3.3.0\r\nPillow==8.2.0\r\npip==21.1.2\r\nprotobuf==3.17.0\r\npyasn1==0.4.8\r\npyasn1-modules==0.2.8\r\npyparsing==2.4.7\r\npython-dateutil==2.8.1\r\nPyYAML==5.4.1\r\nrequests==2.25.1\r\nrequests-oauthlib==1.3.0\r\nrsa==4.7.2\r\nscipy==1.6.3\r\nsetuptools==57.0.0\r\nsix==1.16.0\r\ntensorboard==2.5.0\r\ntensorboard-data-server==0.6.1\r\ntensorboard-plugin-wit==1.8.0\r\ntensorflow==2.5.0\r\ntensorflow-estimator==2.5.0\r\ntermcolor==1.1.0\r\ntyping-extensions==3.10.0.0\r\nurllib3==1.26.4\r\nWerkzeug==2.0.1\r\nwheel==0.36.2\r\nwrapt==1.12.1\r\n```\r\n\r\n"]}, {"number": 44821, "title": "postnet  android", "body": "\r\nHow to detect the actions of multiple people in android, there is only one person's behavior detection in the current demo", "comments": ["@carterinchina \r\nPlease share the link of the demo you are referring to.", "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android\r\n\r\ni used this posenet, How to detect the pose of multiple people", "Any thoughts on this Tian? TFJS postnet seems to support multiple people.", "> Any thoughts on this Tian? TFJS postnet seems to support multiple people.\r\n\r\nso how to use it on tflite (Tensor lite)", "Hi, I will start by saying, yes the multiperson pose estimation is supported by tflite, but for it you will have to use another tflite model, not the model proposed by tensorflow posenet, as this model only support one person detection.\r\n\r\ni have used the flutter plugin code for multi person detection in images and videos in android (kotlin/java) with TFlite. The code is in my GitHub repository, as it is a big code, i can't post it here. The code can be found here (https://github.com/stevelaclasse/posenet/blob/main/app/src/main/java/cm/stevru/andropose/inference/DecodePose.java).\r\nThere you will see multiPoseEstimation() function. \r\n\r\nI also experimented another approach for multipose estimation with the one person model. I have used a person detector model to detect all persons on the image, then i crop each detected person and create a new resized image with only this person, that i send to the one person poseEstimation model. It works pretty well on good images, but the cropped image with only a detected person has loose his quality so that the poseEstimation doesn't detect all poses on the image, and at the end just few poses are estimated on persons on the image. That is why i have chosen the multiPerson poseEstimation model.\r\n\r\nYou can also check this blog https://medium.com/flutter-community/posenet-for-ios-android-and-flutter-using-tensorflow-lite-836788a110c7 , to better understand the code. \r\n\r\nThe flutter plugin code for multiperson pose estimation can be found here: https://github.com/shaqian/flutter_tflite/tree/master/android .\r\n\r\nI hope it will help you or someone.\r\n\r\n[i have posted the same answer on stackoverflow: [https://stackoverflow.com/a/70579416/17831367](https://stackoverflow.com/a/70579416/17831367) ]", "For multi-person pose estimation, please use the MoveNet MultiPose model here.\r\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/android"]}, {"number": 44820, "title": "Parallelization of tf.DistributedDataset Prefetch? ", "body": "<em>Please make sure that this is an issue related to performance of TensorFlow.\r\nAs per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:performance_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 - DGX OS 4\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Pip\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory: 4x V100 32GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nDistributedDataset doesnt seem to keep up with input - CPU utilization is low but docs only provide information for tf.Dataset, not tf.DistributedDataset. Curious if there are ways to use AUTOTUNE on a DiatributedDataset to parallelize the prefetching operations. \r\n\r\n**Describe the expected behavior**\r\nPrefetch allows for the data to come off host RAM quick enough to be utilized rapidly.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nThis is part of a package I'm developing, so standalone code is rather difficult. Below is the fit function for the model to be trained. Unfortunately it wont be possible to reproduce it standalone. \r\n\r\nobj is a class that is being passed in. This contains the loss, etc. Really the question lies in the first ~30 lines where the dataset is turned into a tf.distributeddataset and then in the last bit where it is iterated\r\n\r\n```\r\ndef fit_dist(obj, tf_iter, newton_iter, batch_sz = None, newton_eager = True):\r\n\r\n    BUFFER_SIZE = len(obj.x_f)\r\n    EPOCHS = tf_iter\r\n    obj.strategy = tf.distribute.MirroredStrategy(cross_device_ops = tf.distribute.NcclAllReduce())\r\n    print(\"number of devices: {}\".format(obj.strategy.num_replicas_in_sync))\r\n\r\n    if batch_sz is not None:\r\n        obj.batch_sz = batch_sz\r\n    else:\r\n        obj.batch_sz = len(obj.x_f)\r\n\r\n    N_f = len(obj.x_f)\r\n    n_batches =  N_f // obj.batch_sz\r\n\r\n    BATCH_SIZE_PER_REPLICA = obj.batch_sz\r\n    GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * obj.strategy.num_replicas_in_sync\r\n\r\n    train_dataset = tf.data.Dataset.from_tensor_slices((obj.x_f, obj.t_f)).batch(GLOBAL_BATCH_SIZE)\r\n\r\n    #print(GLOBAL_BATCH_SIZE)\r\n    obj.train_dist_dataset = obj.strategy.experimental_distribute_dataset(train_dataset)\r\n\r\n\r\n    start_time = time.time()\r\n\r\n    with obj.strategy.scope():\r\n        obj.u_model = neural_net(obj.layer_sizes)\r\n        obj.tf_optimizer = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.99)\r\n        obj.tf_optimizer_weights = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.99)\r\n        #Can adjust batch size for collocation points, here we set it to N_f\r\n\r\n        if obj.isAdaptive:\r\n            obj.col_weights = tf.Variable(tf.random.uniform([50000, 1]))\r\n            obj.u_weights = tf.Variable(100*tf.random.uniform([200, 1]))\r\n\r\n\r\n    print(\"starting Adam training\")\r\n    STEPS = np.max((n_batches // obj.strategy.num_replicas_in_sync,1))\r\n    \r\n    for epoch in range(tf_iter):\r\n            train_loss = train_epoch(obj, obj.train_dist_dataset, obj.col_weights, STEPS)\r\n            if epoch == 2:\r\n                tf.profiler.experimental.start('../cache/tblogdir1')\r\n            if epoch % 10 == 0:\r\n                elapsed = time.time() - start_time\r\n                print('It: %d, Time: %.2f' % (epoch, elapsed))\r\n                tf.print(f\"total loss: {train_loss}\")\r\n                start_time = time.time()\r\n\r\n    tf.profiler.experimental.stop()\r\n    #l-bfgs-b optimization\r\n    print(\"Starting L-BFGS training\")\r\n    lbfgs_train(obj, newton_iter)\r\n\r\n@tf.function\r\ndef train_epoch(obj, dataset, col_weights, STEPS):\r\n    total_loss = 0.0\r\n    num_batches = 0.0\r\n    #dist_col_weights = iter(col_weights)\r\n    dist_dataset_iterator = iter(dataset)\r\n    for _ in range(STEPS):\r\n        total_loss += distributed_train_step(obj, next(dist_dataset_iterator), col_weights)\r\n        num_batches += 1\r\n    train_loss = total_loss / num_batches\r\n    return train_loss\r\n\r\ndef train_step(obj, inputs, col_weights):\r\n    obj.dist_x_f, obj.dist_t_f = inputs\r\n    obj.dist_col_weights = col_weights\r\n    if obj.isAdaptive:\r\n        obj.variables = obj.u_model.trainable_variables\r\n        obj.variables.extend([obj.u_weights, obj.dist_col_weights])\r\n        loss_value, mse_0, mse_b, mse_f, grads = obj.grad()\r\n        obj.tf_optimizer.apply_gradients(zip(grads[:-2], obj.u_model.trainable_variables))\r\n        obj.tf_optimizer_weights.apply_gradients(zip([-grads[-2], -grads[-1]], [obj.u_weights, obj.dist_col_weights]))\r\n    else:\r\n        obj.variables = obj.u_model.trainable_variables\r\n        loss_value, mse_0, mse_b, mse_f, grads = obj.grad()\r\n        obj.tf_optimizer.apply_gradients(zip(grads, obj.u_model.trainable_variables))\r\n    return loss_value\r\n\r\ndef distributed_train_step(obj, dataset_inputs, col_weights):\r\n    per_replica_losses = obj.strategy.run(train_step, args=(obj, dataset_inputs, col_weights))\r\n    return obj.strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\r\n                         axis=None)\r\n```\r\n\r\n**What I have tried**\r\nI have been through the docs in custom training loops with mirrored strategy, distributed dataset training, distributed training with Keras, as well as optimizing on GPU with the profiler, better performance with tf.data API, and Optimize TensorFlow performance using the Profiler, most of the associated how-tos and self helps.\r\n\r\nReally my question is about more support with tf.DistributedDataset - It prefetches alright (I checked the traces) but it doesn't really utilize the CPU fully (it would appear) and the ops on GPU outrun the data fetching. There is no data preprocessing for my training, all it has to do is pull the points (literally coordinates, a tuple of 2 tf.float32 values) from memory. They're not images or anything large. This is why I think the GPU processing is simply outrunning the data fetching, but I wanted to see if there was a way to better parallelize the tf.DistributedDataset memory fetches on CPU, etc to help get more data in the GPU faster. Like is there an \"autotune\" that I need to enable to do that better in tf.distributedDataset?\r\n\r\nI'm attaching my profile traces. Another question would be why only 9% of my ops live on the GPU. When I run on a single GPU without mirrored strategy all the ops live on GPU, here only 9% do. These two event are likely related, but I cant seem to coerce tf into putting more ops on GPU. \r\n\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n[2020_11_13_04_12_22.zip](https://github.com/tensorflow/tensorflow/files/5534766/2020_11_13_04_12_22.zip)", "comments": ["Hi @levimcclenny I want to clarify your question first. You're asking if there are any best practices for optimizing the input pipeline when using a distributed dataset? It seems you have reviewed the tf.data performance guides, but even after that your input is still a bottleneck, so you're wondering if there's anything specific you should do in the distributed dataset case. Is that correct?\r\n\r\nAlso, have you maximized the batch size to keep the GPUs busy for longer?", "Hi @nikitamaia, Actually I was having the opposite issue - my batch size was too large it would appear. Dropping it helped out pretty tremendously. I'll close the issue. \r\n\r\nThanks!"]}, {"number": 44819, "title": "How to pass the seq length in the LSTMs in tfLite", "body": "Is there an option to pass seq length in LSTM tfLite ?\r\nIs batch size option suffice seq length ?\r\nDoes seq length required during training ?\r\nCan we pass variable seq length during inference ? If we can , How we can do that ?", "comments": ["@pranathibl,\r\nCould you please provide a minimal code sample of the use case you are trying to implement? Thanks!", "I a trying support of variable sequences in tfLite.\r\nThis is the script i created.\r\nGetting issues while i was trying variable sequences.\r\n\r\n\r\n[lstminput_var_sequencemode.txt](https://github.com/tensorflow/tensorflow/files/5557342/lstminput_var_sequencemode.txt)\r\n", "Currently we don't support seq_len, by default lstm will consume all your sequences", "Then why its working in this particular case,\r\nWhen i created model with just lstm layer its working,\r\nCould you explain the difference and why its not working in above case and working when i used lstm layer just ?\r\n\r\n[lstminput_var_sequencemode_layer.txt](https://github.com/tensorflow/tensorflow/files/5561846/lstminput_var_sequencemode_layer.txt)\r\n\r\n", "our current [kernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/unidirectional_sequence_lstm.cc) does not support seq_len yet. (we will process all sequences)\r\n\r\nalso since you're using `return_sequences=True` which means you're consuming all the sequences (which caused dynamic shapes and it's hard for the following dense layer to figure out the dims)\r\n\r\nIf you disable `return_sequences` it should be fine.", "Is there any thing to check for support ? Which will work and which does not work ?\r\nWill you add support in future ?", "We will support this in future, but it's unlikely to happen in the near term.", "@pranathibl \r\nCould you please let us know if this is still an issue in latest stable TF v2.6.0 ?Thank you!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44819\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44819\">No</a>\n"]}, {"number": 44818, "title": "add CheckpointManager example.", "body": "add `CheckpointManager` example.", "comments": ["Please reopen against `master` branch, not the release one. We are very restrictive with what we cherrypick during the release process."]}, {"number": 44817, "title": "[CherryPick:r2.4] Bump libjpeg-turbo from 2.0.4 to 2.0.5", "body": "Note: this is an r2.4 cherry-pick /cc @mihaimaruseac\r\n\r\nThis PR is the cherry-pick of d68b2fe (PR #44781) to r2.4.\r\n\r\nThis PR bumps the version of libjpeg-turbo from 2.0.4 to 2.0.5\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": []}, {"number": 44816, "title": "2.4.0 cherry-pick request: update tensorboard dependency to 2.4.x", "body": "Consequences of not including this cherry-pick in TF 2.4.0:\r\nusers who pip install tensorflow will be left with an outdated\r\ntensorboard version.\r\n\r\nTensorBoard release: https://pypi.org/project/tensorboard/2.4.0/\r\n\r\nPiperOrigin-RevId: 342150986\r\nChange-Id: I6f3945a744a9de482b72342cb1ed770d0fe6bf36", "comments": []}, {"number": 44813, "title": "eager execution and gradients", "body": "I tried to visualize CNN activations but faced with the next error:\r\n\r\n`RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\r\n<img width=\"588\" alt=\"zi80a\" src=\"https://user-images.githubusercontent.com/74381598/98996923-835f3700-24e8-11eb-866e-cfc9e1e5780e.png\">\r\n`\r\n\r\n```\r\nlayer_dict = dict([(layer.name,layer) for layer in baseline_model.layers[1:]])\r\nimg_input = baseline_model.input\r\nneuron_index = 1\r\nlayer_output = layer_dict['co[enter image description here][1]nv2d_171'].output\r\nloss = keras.backend.mean(layer_output[:,neuron_index,:,:])\r\ngradients = keras.backend.gradients(loss,img_input)[0]\r\ngradients = tf.linalg.normalize(gradients)\r\niterate = keras.function([img_input],[loss,gradents])\r\n```", "comments": ["how can I solve it? thank u", "If you want to visualize filters please check https://keras.io/examples/vision/visualizing_what_convnets_learn/", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 44812, "title": "Markdown typo", "body": "showed up in the docs", "comments": []}, {"number": 44811, "title": "XNNPACK Delegate error", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Redmi 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: 2.3.0\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: conda\r\n- Bazel version (if compiling from source): 3.1.0\r\n- GCC/Compiler version (if compiling from source): ndk builder for android-28 API version 20.0.5594570\r\n- CUDA/cuDNN version: 10.1\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nHow do I get XNNPACK as an optional delegate for my application purely in c++? I tried building with \"define xnnpack=true\" but then it won't allow me to modify the graph with other delegates saying ```ERROR:Graph is immutable\". I tried building XNNPACK separately and link it as required by \"evaluations;:utils\" header, to no avail. Help?\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n* I built the tensorflowlite using ```bazel build -c opt --config android_arm64 --define tflite_with_xnnpack=true tensorflow/lite:libtensorflowlite.so ```, I compiled the libraries for Hexagon and GPU and linked them as required by ```evaluation::utils``` header. \r\n* When runing the application it automatically creates XNNPACK Delegate for me. But when I try to modify graph with GPU delegate, I get \r\n```\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Created TensorFlow Lite delegate for GPU.\r\nINFO: Initialized OpenCL-based API.\r\nINFO: Created 1 GPU delegate kernels.\r\nERROR: ModifyGraphWithDelegate is disallowed when graph is immutable.\r\n\r\n```\r\n\r\nHow do I allow changing delegates in XNNPACK support or how do I add optional XNNPACK support? I checked the README.md, following which I got an ```.lo``` file which I'm not sure what to do with. Help\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["To sum up, I have two questions\r\n1) Can I apply more than two delegates? If so, how? Because I've tried applying two delegates in benchmark model and I received the same error ```Graph immutable```. But then, why is there an option to apply multiple delegates in ```label_image``` and ```benchmark_model``` code?\r\n2) If we can't apply more than two delegates, then I'm stuck with XNNPACK delegate as backend, how do I make it as optional?", "> To sum up, I have two questions\r\n> \r\n> 1. Can I apply more than two delegates? If so, how? Because I've tried applying two delegates in benchmark model and I received the same error `Graph immutable`. But then, why is there an option to apply multiple delegates in `label_image` and `benchmark_model` code?\r\n\r\nTechnically, more than two delegates could be applied to the TFLite model graph. But the order of delegate application does matter as two delegates may support the same TFLite op where the first applied delegate will be used to delegate the execution of this op. From this perspective, I think it's better to write your own code to manually specify the order of applying delegates.\r\n\r\nAs for the tools you mentioned here (i.e. label_image, benchmark_model) haven't been fully tried with applying multiple delegates. I'm not sure which option suggests applying multiple delegates in those tools. Could you post a reference here? I think we should clarify this. Thx!\r\n\r\n> 2. If we can't apply more than two delegates, then I'm stuck with XNNPACK delegate as backend, how do I make it as optional?\r\n\r\nWhen you build the TFLite library (2.3.0), could you remove \"--define tflite_with_xnnpack=true\" option? This option is to enable using xnnpack delegate by default. In this way, I think the \"graph-immutable\" error should be gone, and the application of XNNPACK delegate will become optional.\r\n", "> As for the tools you mentioned here (i.e. label_image, benchmark_model) haven't been fully tried with applying multiple delegates. I'm not sure which option suggests applying multiple delegates in those tools. Could you post a reference here? I think we should clarify this. Thx!\r\n\r\nIn ```label_image.cc``` we have \r\n\r\n```\r\n  auto delegates_ = GetDelegates(s);\r\n  for (const auto& delegate : delegates_) {\r\n    if (interpreter->ModifyGraphWithDelegate(delegate.second.get()) !=\r\n        kTfLiteOk) {\r\n      LOG(FATAL) << \"Failed to apply \" << delegate.first << \" delegate.\";\r\n    } else {\r\n      LOG(INFO) << \"Applied \" << delegate.first << \" delegate.\";\r\n    }\r\n  }\r\n  ```\r\nInside ```void RunInference(Settings* s)``` code line 230 (in v.2.3.0). If we investigate the \"GetDelegates\" method, we find that it creates a map storing all the delegates. \r\n\r\n```\r\nTfLiteDelegatePtrMap GetDelegates(Settings* s) {\r\n  TfLiteDelegatePtrMap delegates;\r\n  if (s->gl_backend) {\r\n    auto delegate = CreateGPUDelegate(s);\r\n    if (!delegate) {\r\n      LOG(INFO) << \"GPU acceleration is unsupported on this platform.\";\r\n    } else {\r\n      delegates.emplace(\"GPU\", std::move(delegate));\r\n    }\r\n  }\r\n.......\r\nso on for all delegates\r\n\r\nreturn delegates\r\n```\r\n\r\nThis clearly seems to suggest that more than one delegates is the norm.", "> When you build the TFLite library (2.3.0), could you remove \"--define tflite_with_xnnpack=true\" option? This option is to enable using xnnpack delegate by default. In this way, I think the \"graph-immutable\" error should be gone, and the application of XNNPACK delegate will become optional.\r\n\r\nI'm creating my own framework in which I want my users to be able to choose from various delegates. I've used \r\n```#include \"tensorflow/lite/tools/evaluation/utils.h\"``` to get the delegate interface. Using this header forces me to link my executable with all the delegate libraries.\r\n\r\nFor linking, I've manually compiled each part of a delegate and linked to my library. For instance, for hexagon I've compiled the following\r\n```\r\nlibhexagon_delegate.a  \r\nlibhexagon_delegate_kernel.a  \r\nlibhexagon_implementation.a  l\r\nlibhexagon_utils.a \r\nliibop_builder.a\r\n```\r\nI'm unable to do the same for XNNPACK as it requires some external libraries and its getting confusing (and so I resorted to default backend which lead to the original problem). @multiverse-tf\r\n\r\n\r\n", "> > When you build the TFLite library (2.3.0), could you remove \"--define tflite_with_xnnpack=true\" option? This option is to enable using xnnpack delegate by default. In this way, I think the \"graph-immutable\" error should be gone, and the application of XNNPACK delegate will become optional.\r\n> \r\n> I'm creating my own framework in which I want my users to be able to choose from various delegates. I've used\r\n> `#include \"tensorflow/lite/tools/evaluation/utils.h\"` to get the delegate interface. Using this header forces me to link my executable with all the delegate libraries.\r\n\r\nAcked. If you want your users to be able to choose from various delegates at runtime, then the executable has to be linked with all delegates that could be chosen?\r\n\r\n> \r\n> For linking, I've manually compiled each part of a delegate and linked to my library. For instance, for hexagon I've compiled the following\r\n> \r\n> ```\r\n> libhexagon_delegate.a  \r\n> libhexagon_delegate_kernel.a  \r\n> libhexagon_implementation.a  l\r\n> libhexagon_utils.a \r\n> liibop_builder.a\r\n> ```\r\n> \r\n> I'm unable to do the same for XNNPACK as it requires some external libraries and its getting confusing (and so I resorted to default backend which lead to the original problem). @multiverse-tf\r\n\r\nHow did you compile XNNPACK delegate? If using bazel, \"bazel build ${BUILD FLAGS} //tensorflow/lite/delegates/xnnpack:xnnpack_delegate\" could produce a static xnnpack delegate library. Recently (after 2.3 release), we have added cmake support (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt). Although I don't think we add a particular target for xnnpack delegate, I guess you may tweak the file to build it. Then you may follow this https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/delegates/xnnpack#enable-xnnpack-via-low-level-delegate-api-not-recommended to use the xnnpack delegate.\r\n\r\n", "> > As for the tools you mentioned here (i.e. label_image, benchmark_model) haven't been fully tried with applying multiple delegates. I'm not sure which option suggests applying multiple delegates in those tools. Could you post a reference here? I think we should clarify this. Thx!\r\n> \r\n> In `label_image.cc` we have\r\n> \r\n> ```\r\n>   auto delegates_ = GetDelegates(s);\r\n>   for (const auto& delegate : delegates_) {\r\n>     if (interpreter->ModifyGraphWithDelegate(delegate.second.get()) !=\r\n>         kTfLiteOk) {\r\n>       LOG(FATAL) << \"Failed to apply \" << delegate.first << \" delegate.\";\r\n>     } else {\r\n>       LOG(INFO) << \"Applied \" << delegate.first << \" delegate.\";\r\n>     }\r\n>   }\r\n> ```\r\n> \r\n> Inside `void RunInference(Settings* s)` code line 230 (in v.2.3.0). If we investigate the \"GetDelegates\" method, we find that it creates a map storing all the delegates.\r\n> \r\n> ```\r\n> TfLiteDelegatePtrMap GetDelegates(Settings* s) {\r\n>   TfLiteDelegatePtrMap delegates;\r\n>   if (s->gl_backend) {\r\n>     auto delegate = CreateGPUDelegate(s);\r\n>     if (!delegate) {\r\n>       LOG(INFO) << \"GPU acceleration is unsupported on this platform.\";\r\n>     } else {\r\n>       delegates.emplace(\"GPU\", std::move(delegate));\r\n>     }\r\n>   }\r\n> .......\r\n> so on for all delegates\r\n> \r\n> return delegates\r\n> ```\r\n> \r\n> This clearly seems to suggest that more than one delegates is the norm.\r\n\r\nI see. That we implemented the delegate application in this way is to allow applying multiple delegates to the TFLite interpreter. But in general, when using these tools, we only specify one delegate to be applied via command-line flags, like \"--use_gpu=true\" etc, without specifying another one like \"--use_nnapi=true\" at the same time.", "> Acked. If you want your users to be able to choose from various delegates at runtime, then the executable has to be linked with all delegates that could be chosen?\r\n\r\nAll the delegates used in ```evaluations/utils.h```, yeah.\r\n\r\n> How did you compile XNNPACK delegate? If using bazel, \"bazel build ${BUILD FLAGS} //tensorflow/lite/delegates/xnnpack:xnnpack_delegate\" could produce a static xnnpack delegate library. \r\n \r\nThe libxnnpack_delegate.a  produced has deps which I'm unable to link with. (like I've done with hexagon by compiling libhexagon_implementation.a etc). \r\n\r\nHere's the error\r\n\r\n``` bash\r\n/home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `tflite::evaluation::CreateXNNPACKDelegate()':\r\n/proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:178: undefined reference to `TfLiteXNNPackDelegateOptionsDefault'\r\n/home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `tflite::evaluation::CreateXNNPACKDelegate(TfLiteXNNPackDelegateOptions const*)':\r\n/proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:184: undefined reference to `TfLiteXNNPackDelegateCreate'\r\n/proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:184: undefined reference to `TfLiteXNNPackDelegateCreate'\r\n/home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `tflite::evaluation::CreateXNNPACKDelegate(int)':\r\n/proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:191: undefined reference to `TfLiteXNNPackDelegateOptionsDefault'\r\n/home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `tflite::evaluation::CreateXNNPACKDelegate(TfLiteXNNPackDelegateOptions const*)':\r\n/proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:184: undefined reference to `TfLiteXNNPackDelegateCreate'\r\n/home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `operator()':\r\n/proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:186: undefined reference to `TfLiteXNNPackDelegateDelete'\r\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\nmake[3]: *** [CMakeFiles/gmac_test.dir/build.make:106: gmac_test] Error 1\r\nmake[2]: *** [CMakeFiles/Makefile2:101: CMakeFiles/gmac_test.dir/all] Error 2\r\nmake[1]: *** [CMakeFiles/Makefile2:108: CMakeFiles/gmac_test.dir/rule] Error 2\r\nmake: *** [Makefile:138: gmac_test] Error 2\r\n```\r\n\r\nThis means I need to compile addiitonal dependency and link to it, (where I'm stuck).\r\n\r\n\r\n> Recently (after 2.3 release), we have added cmake support (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt). Although I don't think we add a particular target for xnnpack delegate, I guess you may tweak the file to build it. \r\n\r\nFrom what I understand, this wont allow runtime change.\r\n\r\nI can opt for the low leel method you've said, but recall that the header would still force me to link against xnnpack delegate static library.\r\n\r\n", "> > Acked. If you want your users to be able to choose from various delegates at runtime, then the executable has to be linked with all delegates that could be chosen?\r\n> \r\n> All the delegates used in `evaluations/utils.h`, yeah.\r\n> \r\n> > How did you compile XNNPACK delegate? If using bazel, \"bazel build ${BUILD FLAGS} //tensorflow/lite/delegates/xnnpack:xnnpack_delegate\" could produce a static xnnpack delegate library.\r\n> \r\n> The libxnnpack_delegate.a produced has deps which I'm unable to link with. (like I've done with hexagon by compiling libhexagon_implementation.a etc).\r\n> \r\n> Here's the error\r\n> \r\n> ```shell\r\n> /home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `tflite::evaluation::CreateXNNPACKDelegate()':\r\n> /proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:178: undefined reference to `TfLiteXNNPackDelegateOptionsDefault'\r\n> /home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `tflite::evaluation::CreateXNNPACKDelegate(TfLiteXNNPackDelegateOptions const*)':\r\n> /proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:184: undefined reference to `TfLiteXNNPackDelegateCreate'\r\n> /proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:184: undefined reference to `TfLiteXNNPackDelegateCreate'\r\n> /home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `tflite::evaluation::CreateXNNPACKDelegate(int)':\r\n> /proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:191: undefined reference to `TfLiteXNNPackDelegateOptionsDefault'\r\n> /home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `tflite::evaluation::CreateXNNPACKDelegate(TfLiteXNNPackDelegateOptions const*)':\r\n> /proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:184: undefined reference to `TfLiteXNNPackDelegateCreate'\r\n> /home/psykik/CLionProjects/gmac_sai/benchmark_library/external/libs/delegates/libutils.a(utils.o): In function `operator()':\r\n> /proc/self/cwd/tensorflow/lite/tools/evaluation/utils.cc:186: undefined reference to `TfLiteXNNPackDelegateDelete'\r\n> clang++: error: linker command failed with exit code 1 (use -v to see invocation)\r\n> make[3]: *** [CMakeFiles/gmac_test.dir/build.make:106: gmac_test] Error 1\r\n> make[2]: *** [CMakeFiles/Makefile2:101: CMakeFiles/gmac_test.dir/all] Error 2\r\n> make[1]: *** [CMakeFiles/Makefile2:108: CMakeFiles/gmac_test.dir/rule] Error 2\r\n> make: *** [Makefile:138: gmac_test] Error 2\r\n> ```\r\n> \r\n\r\nBased on the path, it seems that you are using MacOS to do the cross-compilation. I think this compilation error might be related to this issue (https://github.com/bazelbuild/bazel/issues/11552). Could you try static linking (on the target library that links with xnnpack delegate) to see if this error is gone? Or try it on a Linux machine? Anyway, we will find a systematic way to fix this on Mac eventually.\r\n\r\n> This means I need to compile addiitonal dependency and link to it, (where I'm stuck).\r\n> \r\n> > Recently (after 2.3 release), we have added cmake support (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/CMakeLists.txt). Although I don't think we add a particular target for xnnpack delegate, I guess you may tweak the file to build it.\r\n> \r\n> From what I understand, this wont allow runtime change.\r\n\r\nWhat I meant is to build a independent xnnpack delegate library via cmake, like the way you do via bazel. But considering the compilation issue above, I guess we might end up with the same problem.\r\n\r\n> \r\n> I can opt for the low leel method you've said, but recall that the header would still force me to link against xnnpack delegate static library.\r\n", "Actually I'm using **linux**. So I've compiled ```libxnnpack_delegate.a``` using this command ```bazel build -c opt --config android_arm64 tensorflow/lite/delegates/xnnpack:xnnpack_delegate``` and then linked it to my library. I get this [error](https://pastebin.com/a6P1yqVH)\r\n\r\nI figured I'd need build xnnpack and link to it. So I checked the \"external\" folder in bazel-bin and found XNNPACK folder, in that I copied ```libxnnpack_f32.a``` and linked to my library. This solved some of the previous error and brought new errors.\r\nHave a look https://pastebin.com/zXp4N01j\r\n\r\nWhat's especially intruiging is it tells me to \"recompile with fPIC\", which I think I can get from ```libxnnpack_f32.pic.a```? But then I'm unable to load that library in my CMake  :( .", "> Actually I'm using **linux**. So I've compiled `libxnnpack_delegate.a` using this command `bazel build -c opt --config android_arm64 tensorflow/lite/delegates/xnnpack:xnnpack_delegate` and then linked it to my library. I get this [error](https://pastebin.com/a6P1yqVH)\r\n\r\nI think the [cc_library rule](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/BUILD#L22-L37) of xnnpack_delegate implies libxnnpack_f32.a will be automatically built and included as part of the lib.\r\n\r\n> \r\n> I figured I'd need build xnnpack and link to it. So I checked the \"external\" folder in bazel-bin and found XNNPACK folder, in that I copied ```libxnnpack_f32.a`` and linked to my library. This solved some of the previous error and brought new errors.\r\n> Have a look https://pastebin.com/zXp4N01j\r\n> \r\n> What's especially intruiging is it tells me to \"recompile with fPIC\", which I think I can get from `libxnnpack_f32.pic.a`? But then I'm unable to load that library in my CMake :( .\r\n\r\nRegarding this, could you try adding --copt=\"-fpic\" ([details](https://docs.bazel.build/versions/master/user-manual.html#flag--copt)) to your bazel command to recompile the whole xnnpack delegate (including the XNNPACK lib) and see if it works?\r\n\r\n", "> I think the cc_library rule of xnnpack_delegate implies libxnnpack_f32.a will be automatically built and included as part of the lib.\r\n\r\nCorrect me if I'm wrong but those lines doesn't hint that they're getting linked to tensorflowlite.so.\r\n\r\nI'm unable to get ```libxnnpack_f32.a``` , so I compiled it manually using the XNNPACK repository and used the fpic command as you've suggested and it still didn't help. I used this command '\r\n\r\n* for xnnpack_f32 (from XNNPACK repository)\r\n```bazel build --copt=\"-fpic\" -c opt --config android_arm64 xnnpack_f32```\r\n* for xnnpack_delegate\r\n```bazel build --copt=\"-fpic\" -c opt --config android_arm64 tensorflow/lite/delegates/xnnpack:xnnpack_delegate```\r\n\r\nIn my CMake I've used\r\n```\r\nfind_library(xnnpack_delegate REQUIRED libxnnpack_delegate.a)\r\nfind_library(xnnpack_f32 REQUIRED libxnnpack_f32.a)\r\ntarget_link_library(my_lib xnnpack_delegate xnnpack_f32)\r\n```\r\n\r\nI get the following error:\r\nhttps://pastebin.com/3qCBCtkR\r\n\r\n\r\n\r\nAside from that I've noticed tf_with_xnnpack and tf_with_xnnpack_optional(in the lite folder which can be built using bazel) , what are those? will they solve my problem?\r\n\r\n==============================================================================================\r\n\r\n**UPDATE:** (you can ignore the above)\r\n\r\nI've\r\n- Cloned tensorflow library\r\n- Checked out v2.3.0\r\n- ```bazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so```\r\n-  ```bazel build --copt=\"-fpic\" -c opt --config android_arm64 tensorflow/lite/delegates/xnnpack:xnnpack_delegate```\r\n-    Copy all the files from ```bazel-bin/external/xnnpack``` and link my library against it.\r\n-    Pthreadpool error https://pastebin.com/Y8FJNQ82\r\n\r\nI feel we are getting closer and closer to the solution. Please stick around, thanks!\r\n\r\n", "> > I think the cc_library rule of xnnpack_delegate implies libxnnpack_f32.a will be automatically built and included as part of the lib.\r\n> \r\n> Correct me if I'm wrong but those lines doesn't hint that they're getting linked to tensorflowlite.so.\r\n> \r\n> I'm unable to get `libxnnpack_f32.a` , so I compiled it manually using the XNNPACK repository and used the fpic command as you've suggested and it still didn't help. I used this command '\r\n> \r\n> * for xnnpack_f32 (from XNNPACK repository)\r\n>   `bazel build --copt=\"-fpic\" -c opt --config android_arm64 xnnpack_f32`\r\n> * for xnnpack_delegate\r\n>   `bazel build --copt=\"-fpic\" -c opt --config android_arm64 tensorflow/lite/delegates/xnnpack:xnnpack_delegate`\r\n> \r\n> In my CMake I've used\r\n> \r\n> ```\r\n> find_library(xnnpack_delegate REQUIRED libxnnpack_delegate.a)\r\n> find_library(xnnpack_f32 REQUIRED libxnnpack_f32.a)\r\n> target_link_library(my_lib xnnpack_delegate xnnpack_f32)\r\n> ```\r\n> \r\n> I get the following error:\r\n> https://pastebin.com/3qCBCtkR\r\n> \r\n> Aside from that I've noticed tf_with_xnnpack and tf_with_xnnpack_optional(in the lite folder which can be built using bazel) , what are those? will they solve my problem?\r\n> \r\n> ==============================================================================================\r\n> \r\n> **UPDATE:** (you can ignore the above)\r\n> \r\n> I've\r\n> \r\n> * Cloned tensorflow library\r\n> * Checked out v2.3.0\r\n> * `bazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so`\r\n> * `bazel build --copt=\"-fpic\" -c opt --config android_arm64 tensorflow/lite/delegates/xnnpack:xnnpack_delegate`\r\n> * Copy all the files from `bazel-bin/external/xnnpack` and link my library against it.\r\n> * Pthreadpool error https://pastebin.com/Y8FJNQ82\r\n> \r\n> I feel we are getting closer and closer to the solution. Please stick around, thanks!\r\n\r\n@Maratyszcza, could you shed some light on this compiling issue? Many thanks!", "> @Maratyszcza, could you shed some light on this compiling issue? Many thanks!\r\n\r\nThis is fixed, the answer turned out to be quite simple. Just had to change the order in ```target_link_libraries``` that fixed it. This is how it looks now\r\n\r\n```\r\ntarget_link_libraries(my_lib xnnpack_f32 asm_ukernels neonfma_ukernels operator_run\r\n                      psimd_fastmath_ukernels tables neon_ukernels operators indirection\r\n                      memory_planner neonv8_ukernels psimd_accmath_ukernels scalar_ukernels  pthreadpool)\r\n```\r\n\r\n\r\nThe original problem **still** remains. Even after all the trouble I've went through, it still makes the XNNPACK as backend by default! This doesn't allow me to use GPU Backend.\r\n\r\nP.S. Happy new year ya'll :) !\r\n", "Turns out, I forgot to push the libtensorflowlite.so without the \"--define\" Flags. It's working fine now. Thank you @multiverse-tf for your patience!! \n\n\nHappy new year again! \nI'm closing this issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44811\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44811\">No</a>\n", "> > I think the cc_library rule of xnnpack_delegate implies libxnnpack_f32.a will be automatically built and included as part of the lib.\r\n> \r\n> Correct me if I'm wrong but those lines doesn't hint that they're getting linked to tensorflowlite.so.\r\n> \r\n> I'm unable to get `libxnnpack_f32.a` , so I compiled it manually using the XNNPACK repository and used the fpic command as you've suggested and it still didn't help. I used this command '\r\n> \r\n>     * for xnnpack_f32 (from XNNPACK repository)\r\n>       `bazel build --copt=\"-fpic\" -c opt --config android_arm64 xnnpack_f32`\r\n> \r\n>     * for xnnpack_delegate\r\n>       `bazel build --copt=\"-fpic\" -c opt --config android_arm64 tensorflow/lite/delegates/xnnpack:xnnpack_delegate`\r\n> \r\n> \r\n> In my CMake I've used\r\n> \r\n> ```\r\n> find_library(xnnpack_delegate REQUIRED libxnnpack_delegate.a)\r\n> find_library(xnnpack_f32 REQUIRED libxnnpack_f32.a)\r\n> target_link_library(my_lib xnnpack_delegate xnnpack_f32)\r\n> ```\r\n> \r\n> I get the following error: https://pastebin.com/3qCBCtkR\r\n> \r\n> Aside from that I've noticed tf_with_xnnpack and tf_with_xnnpack_optional(in the lite folder which can be built using bazel) , what are those? will they solve my problem?\r\n> \r\n> ==============================================================================================\r\n> \r\n> **UPDATE:** (you can ignore the above)\r\n> \r\n> I've\r\n> \r\n>     * Cloned tensorflow library\r\n> \r\n>     * Checked out v2.3.0\r\n> \r\n>     * `bazel build -c opt --config=android_arm64 //tensorflow/lite:libtensorflowlite.so`\r\n> \r\n>     * `bazel build --copt=\"-fpic\" -c opt --config android_arm64 tensorflow/lite/delegates/xnnpack:xnnpack_delegate`\r\n> \r\n>     * Copy all the files from `bazel-bin/external/xnnpack` and link my library against it.\r\n> \r\n>     * Pthreadpool error https://pastebin.com/Y8FJNQ82\r\n> \r\n> \r\n> I feel we are getting closer and closer to the solution. Please stick around, thanks!\r\n\"Copy all the files from bazel-bin/external/xnnpack and link my library against it.\", but it only '_obj' dictionary in the bazel-bin/external/xnnpack, it means copy this dictionary?\r\n"]}, {"number": 44810, "title": "Use fixed arch-specific device memory reserve", "body": "Replaces current 6% policy for device memory not claimed by BFC allocator\r\nwith fixed reservation size.\r\n\r\nDefault system_memory reserve sizes are:\r\n*  500MB for Compute Capability <= 6.x (Pascal and earlier)\r\n* 1050MB for Compute Capabilities of 7.x (Volta, Turing)\r\n* 1536MB for Compute Capability >= 8.x (Ampere and later)\r\n\r\nAlso adds TF_DEVICE_MIN_SYS_MEMORY_IN_MB env var to directly override the\r\nabove defaults. This is needed in cases where external packages require\r\nsignificant device memory allocations.\r\n\r\nUsing a fixed-sized system memory reserve can reduce the amount of device memory available to TF models on small-memory GPUs, such as Geforce cards. But it should eliminate the majority of out-of-memory cuda library initialization errors such as https://github.com/tensorflow/tensorflow/issues/44072, https://github.com/tensorflow/tensorflow/issues/43764, https://github.com/tensorflow/tensorflow/issues/42163, https://github.com/tensorflow/tensorflow/issues/41377, and https://github.com/tensorflow/tensorflow/issues/41196. \r\n\r\nAttn: @sanjoy ", "comments": ["All (the pull request submitter and all commit authors) CLAs are signed, **but** one or more commits were authored or co-authored by someone other than the pull request submitter.\n\nWe need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that by leaving a comment that contains only `@googlebot I consent.` in this pull request.\n\n*Note to project maintainer:* There may be cases where the author cannot leave a comment, or the comment is not properly detected as consent.  In those cases, you can manually confirm consent of the commit author(s), and set the `cla` label to `yes` (if enabled on your project).\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F44810) for more info**.\n\n<!-- need_author_consent -->", "@googlebot I consent.", "@nluehr The internal CI is failing and the failures look legitimate, can you PTAL?", "It should be working now, @sanjoy.", "Seems auto-merge is not happening but the changes are merged into master now, so we can close this. Thank you for the PR."]}]