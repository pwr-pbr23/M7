[{"number": 1990, "title": "Installation of Tensorflow 0.8 in an Anaconda Python 3.5 Environment Failed", "body": "Dear friends,\n\nI have tried to install tensorflow 0.8 in an anaconda python 3.5 environment but it didn't worked. I got a message saying something like \"not a supported wheel on this platform\".\n### Environment info\n\nOperating System: Ubuntu 14.04.LTS\n### Steps to reproduce\n\n```\ndlm@pc-aero-01:~$ \ndlm@pc-aero-01:~$ conda create -n tensorflow python=3.5\n(...)\ndlm@pc-aero-01:~$ \ndlm@pc-aero-01:~$ source activate tensorflow\ndiscarding /home/dlm/anaconda3/bin from PATH\nprepending /home/dlm/anaconda3/envs/tensorflow/bin to PATH\n(tensorflow)dlm@pc-aero-01:~$ \n(tensorflow)dlm@pc-aero-01:~$ python -V\nPython 3.5.1 :: Continuum Analytics, Inc.\n(tensorflow)dlm@pc-aero-01:~$ pip -V\npip 8.1.1 from /home/dlm/anaconda3/envs/tensorflow/lib/python3.5/site-packages (python 3.5)\n(tensorflow)dlm@pc-aero-01:~$ \n(tensorflow)dlm@pc-aero-01:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl\n**tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl is not a supported wheel on this platform.**\n(tensorflow)dlm@pc-aero-01:~$ \n```\n### What have you tried?\n\nI have tried to use the pip3 for python 3.4 instead of 3.5. It appears to install but didn't worked after all. I think the the problem now is that conda is using the pip of the system environment since this version isn't installed in the tensorflow environment. See below:\n\n```\n(tensorflow)dlm@pc-aero-01:~$ \n(tensorflow)dlm@pc-aero-01:~$ pip3 -V\npip 8.1.1 from /usr/local/lib/python3.4/dist-packages (python 3.4)\n(tensorflow)dlm@pc-aero-01:~$ \n(tensorflow)dlm@pc-aero-01:~$ sudo -H pip3 install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl\n[sudo] password for dlm: \nCollecting tensorflow==0.8.0rc0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl\n  Using cached https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl\nCollecting numpy>=1.8.2 (from tensorflow==0.8.0rc0)\n  Using cached numpy-1.11.0-cp34-cp34m-manylinux1_x86_64.whl\nCollecting wheel>=0.26 (from tensorflow==0.8.0rc0)\n  Using cached wheel-0.29.0-py2.py3-none-any.whl\nCollecting six>=1.10.0 (from tensorflow==0.8.0rc0)\n  Using cached six-1.10.0-py2.py3-none-any.whl\nCollecting protobuf==3.0.0b2 (from tensorflow==0.8.0rc0)\n  Using cached protobuf-3.0.0b2-py2.py3-none-any.whl\nCollecting setuptools (from protobuf==3.0.0b2->tensorflow==0.8.0rc0)\n  Using cached setuptools-20.9.0-py2.py3-none-any.whl\nInstalling collected packages: numpy, wheel, six, setuptools, protobuf, tensorflow\nSuccessfully installed numpy-1.11.0 protobuf-3.0.0b2 setuptools-20.9.0 six-1.10.0 tensorflow-0.8.0rc0 wheel-0.29.0\n(tensorflow)dlm@pc-aero-01:~$ \n(tensorflow)dlm@pc-aero-01:~$ python\nPython 3.5.1 |Continuum Analytics, Inc.| (default, Dec  7 2015, 11:16:01) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import sys\n>>> sys.path\n['', '/home/dlm/anaconda3/envs/tensorflow/lib/python35.zip', '/home/dlm/anaconda3/envs/tensorflow/lib/python3.5', '/home/dlm/anaconda3/envs/tensorflow/lib/python3.5/plat-linux', '/home/dlm/anaconda3/envs/tensorflow/lib/python3.5/lib-dynload', '/home/dlm/anaconda3/envs/tensorflow/lib/python3.5/site-packages', '/home/dlm/anaconda3/envs/tensorflow/lib/python3.5/site-packages/setuptools-20.7.0-py3.5.egg']\n>>> \n>>> import tensorflow\n>>> \n(tensorflow)dlm@pc-aero-01:~$ \n(tensorflow)dlm@pc-aero-01:~$ python -c 'import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/dlm/anaconda3/envs/tensorflow/lib/python3.5/inspect.py\", line 607, in getfile\n    raise TypeError('{!r} is a built-in module'.format(object))\nTypeError: <module 'tensorflow' (namespace)> is a built-in module\n(tensorflow)dlm@pc-aero-01:~$ python3 -c 'import os; import inspect; import tensorflow; print(os.path.dirname(inspect.getfile(tensorflow)))'\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/dlm/anaconda3/envs/tensorflow/lib/python3.5/inspect.py\", line 607, in getfile\n    raise TypeError('{!r} is a built-in module'.format(object))\nTypeError: <module 'tensorflow' (namespace)> is a built-in module\n(tensorflow)dlm@pc-aero-01:~$ \n(tensorflow)dlm@pc-aero-01:~$ python -m tensorflow.models.image.mnist.convolutional\n/home/dlm/anaconda3/envs/tensorflow/bin/python: Error while finding spec for 'tensorflow.models.image.mnist.convolutional' (<class 'ImportError'>: No module named 'tensorflow.models')\n(tensorflow)dlm@pc-aero-01:~$ \n(tensorflow)dlm@pc-aero-01:~$ python -c \"import tensorflow; print(tensorflow.__version__)\"\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\nAttributeError: module 'tensorflow' has no attribute '__version__'\n(tensorflow)dlm@pc-aero-01:~$ \n```\n\nBut, if you try to create a conda environment using python 3.4 it works. Therefore, one possible way to fix it is change the installation procedure in order to create a conda environment using python 3.4 and not python 3.5.\n\n```\ndlm@pc-aero-01:~$ conda create -n tensorflowX python=3.4\nUsing Anaconda Cloud api site https://api.anaconda.org\nFetching package metadata: ....\nSolving package specifications: .........\n(...)\ndlm@pc-aero-01:~$ source activate tensorflowX\ndiscarding /home/dlm/anaconda3/bin from PATH\nprepending /home/dlm/anaconda3/envs/tensorflowX/bin to PATH\n(tensorflowX)dlm@pc-aero-01:~$ \n(tensorflowX)dlm@pc-aero-01:~$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl\nCollecting tensorflow==0.8.0rc0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl\n(...)\nSuccessfully installed numpy-1.11.0 protobuf-3.0.0b2 setuptools-20.7.0 six-1.10.0 tensorflow-0.8.0rc0 wheel-0.29.0\n(tensorflowX)dlm@pc-aero-01:~$ python -m tensorflow.models.image.mnist.convolutional\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980 Ti\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.19\npciBusID 0000:01:00.0\nTotal memory: 6.00GiB\nFree memory: 5.46GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\nInitialized!\nStep 0 (epoch 0.00), 18.8 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 6.0 ms\nMinibatch loss: 3.281, learning rate: 0.010000\nMinibatch error: 6.2%\nValidation error: 6.9%\nStep 200 (epoch 0.23), 5.7 ms\n```\n", "comments": ["The pip wheel contains the python version in its name (cp34-cp34m). If you download the whl file and rename it to say py3-none instead, it should work. Can you try that?\n", "It worked fine.\n\n```\ndlm@pc-aero-01:~$ \ndlm@pc-aero-01:~$ conda create -n tensorflow3 python=3.5\nUsing Anaconda Cloud api site https://api.anaconda.org\nFetching package metadata: ....\nSolving package specifications: .........\nPackage plan for installation in environment /home/dlm/anaconda3/envs/tensorflow3:\n(...)\ndlm@pc-aero-01:~$ \ndlm@pc-aero-01:~$ wget https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl\n(...)\ndlm@pc-aero-01:~$ \ndlm@pc-aero-01:~$ mv tensorflow-0.8.0rc0-cp34-cp34m-linux_x86_64.whl tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl\ndlm@pc-aero-01:~$ \ndlm@pc-aero-01:~$ source activate tensorflow3\ndiscarding /home/dlm/anaconda3/bin from PATH\nprepending /home/dlm/anaconda3/envs/tensorflow3/bin to PATH\n(tensorflow3)dlm@pc-aero-01:~$ \n(tensorflow3)dlm@pc-aero-01:~$ pip install --ignore-installed --upgrade tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl\nProcessing ./tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl\nCollecting numpy>=1.8.2 (from tensorflow==0.8.0rc0)\n(...)\n(tensorflow3)dlm@pc-aero-01:~$ \n(tensorflow3)dlm@pc-aero-01:~$ python -V\nPython 3.5.1 :: Continuum Analytics, Inc.\n(tensorflow3)dlm@pc-aero-01:~$ \n(tensorflow3)dlm@pc-aero-01:~$ python\nPython 3.5.1 |Continuum Analytics, Inc.| (default, Dec  7 2015, 11:16:01) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n>>> import tensorflow\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n>>> \n(tensorflow3)dlm@pc-aero-01:~$ \n(tensorflow3)dlm@pc-aero-01:~$ python -m tensorflow.models.image.mnist.convolutional\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nExtracting data/train-images-idx3-ubyte.gz\nExtracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980 Ti\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.19\npciBusID 0000:01:00.0\nTotal memory: 6.00GiB\nFree memory: 5.63GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)\nInitialized!\nStep 0 (epoch 0.00), 18.1 ms\nMinibatch loss: 12.054, learning rate: 0.010000\nMinibatch error: 90.6%\nValidation error: 84.6%\nStep 100 (epoch 0.12), 5.9 ms\nMinibatch loss: 3.284, learning rate: 0.010000\nMinibatch error: 6.2%\nValidation error: 7.1%\n(...)\n(tensorflow3)dlm@pc-aero-01:~$ \n\n```\n", "We want to add more platforms to our build, so we have a properly named whl for everyone. For now, this is going to be it. I'll close this issue, sorry for the trouble.\n", "I don't know if you understand what I mean. If you leave the way it is now, the installation won't work for anaconda users that choose python 3 support because the installation procedure is asking to create a python 3.5 environment and the file is currently called cp34-cp34m. I suggest you change the installation procedure to build a python 3.4 environment or to rename the file to py3-none.\n", "Ah, thank you. I did indeed not understand properly. Can you take a look at #2032, would that fix the problem (at least temporarily, until we have whl files for 3.5)?\n", "Yes.\nPull request #2032 will do the job.\nThank you,\nDavid\n", "Hi \nI have the same problem with python 3.5.1\n\nwhen i run this:\n$ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n\nI have the platform error.\nPlease help me, i don't like to use python 2.7.\n\nThanks \n", "You can rename the wheel file as pointed out in the installation docs. Sorry for the inconvenience.\n", "Renaming the wheel file worked fine for me!\n", "Hi - does anyone know what the path is to download using anaconda for a windows machine? is it the same one as for linux?\n", "We do not currently support a Windows binary and I am not sure whether that\nwould work. I heard rumors that it is possible to build TensorFlow on\nWindows 10, but I haven't done it myself.\nOn Wed, May 11, 2016 at 20:39 stationedabroad notifications@github.com\nwrote:\n\n> Hi - does anyone know what the path is to download using anaconda for a\n> windows machine? is it the same one as for linux?\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1990#issuecomment-218651487\n", "thanks for reply. I meant I am using anaconda on windows - i followed all advice above, saving the fie locally even - but thought since the set up files have 'linux' in the name they would not work regardless.  So is there no way to install tensorflow on windows through anaconda? the message i get when trying the above when using pip and the copied file as tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl is:\n\n\"tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl is not a supported wheel on this platform.\"\n", "There is currently no way to install TensorFlow from a binary package for\nWindows.\n\nOn Wed, May 11, 2016 at 10:53 PM stationedabroad notifications@github.com\nwrote:\n\n> thanks for reply. I meant I am using anaconda on windows - i followed all\n> advice above, saving the fie locally even - but thought since the set up\n> files have 'linux' in the name they would not work regardless. So is there\n> no way to install tensorflow on windows through anaconda? the message i get\n> when trying the above when using pip and the copied file as\n> tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl is:\n> \n> \"tensorflow-0.8.0rc0-py3-none-linux_x86_64.whl is not a supported wheel on\n> this platform.\"\n> \n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1990#issuecomment-218666518\n", "renaming the file to py3 worked for me\nthanks\n", "With PR #2585, we now have Linux Python 3.5 whl files built and tested nightly. The links to the whl files and build history can be found in the main README.md: \nhttps://github.com/tensorflow/tensorflow/\n", "@martinwicke Thank you, your advice worked for me."]}, {"number": 1989, "title": "Fix docstring of dynamic_rnn", "body": "minor docstring fix: time_major = True avoids extra transposes in dynamic_rnn\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "Thanks for the contribution: We can't look at or accept this until the CLA is signed -- please close if you are unable to sign!\n", "Hi Vijay, I've signed the CLA. (Now I see I should have written it here explicitly).\nThe submitted fix is pretty trivial, not a lot to look at :)\n", "CLAs look good, thanks!\n\n<!-- ok -->\n"]}, {"number": 1988, "title": "tf.Print not printing", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 LST\nTensorflow v0.7.1 (no CUDA)\nPython 2.7.6\n### Steps to reproduce\n\n``` python\nimport tensorflow as tf\nsess = tf.InteractiveSession()\nx = tf.placeholder(tf.float32, shape=[None, 2, 2])\nx = tf.Print(x, [x], message=\"P1\")\ni = tf.reshape(x, [-1, 4])\ni = tf.Print(i, [i], message=\"P2\")\ni.eval(feed_dict={x: [[[1,2], [3,4]], [[5,6], [7,8]]]})\n```\n\nprints\n\n```\nI tensorflow/core/kernels/logging_ops.cc:79] P2[1 2 3...]\narray([[ 1.,  2.,  3.,  4.],\n       [ 5.,  6.,  7.,  8.]], dtype=float32)\n```\n\nWhere is the first print with P1 message?\n", "comments": ["You replaced x with Print(x,...), and then you fed x in your eval, replacing the result of Print and going directly into reshape.\n\nTry:\n\n```\nx = tf.placeholder(...)\nx_print = tf.Print(x, [x], ...)\ni = tf.reshape(x_print, ...)\n```\n\nSo that when you feed 'x', x_print is still run.\n", "![5](https://user-images.githubusercontent.com/17430186/37584863-08164f9a-2b92-11e8-9e6e-8953a36f4398.PNG)\r\n"]}, {"number": 1987, "title": "Stream Executor: perform a cudnn library vs. binary check, add relu6/x support.", "body": "During initialization, we check whether the loaded version\nof the cudnn library matches the version of the library that\nthe binary was compiled against.  When there is a mismatch,\nthrow an error.\n\nExample: I built with cudnnv5 but I am loading V4, and I get:\n\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:298] Loaded cudnn library: 4007 but source was compiled against 5004.  If using a binary install, upgrade your cudnn library to match.  If building from sources, make sure the library loaded matches the version you specified during compile configuration.\n\nI then copy the v5 version of libcudnn.so into /usr/local/cuda/lib64\nand when I re-run the program, the error goes away.\n\nThoughts, @zheng-xq, @leary-google, @zffchen78 ?\n", "comments": ["+1. Should I merge this?\n", "However, aren't cuDNN backwards compatible, at least to some degree? Should we check >=?\n", "Not really. Even when the api is the same it sometimes doesn't work\n\nOn Tue, Apr 19, 2016, 9:15 AM Martin Wicke notifications@github.com wrote:\n\n> However, aren't cuDNN backwards compatible, at least to some degree?\n> Should we check >=?\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1987#issuecomment-212000836\n", "Ideally, we only want to check the major versions, since the minor versions\nand patch levels do not affect backward compatibility. That is true for all\nofficial binaries starting from RC.\n\nHowever, given we have a number of EA binaries floating around, this is not\nnecessarily true any more. So the current check is a bit too strict, but\nfine in practice.\n\nOn Tue, Apr 19, 2016 at 9:19 AM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> Not really. Even when the api is the same it sometimes doesn't work\n> \n> On Tue, Apr 19, 2016, 9:15 AM Martin Wicke notifications@github.com\n> wrote:\n> \n> > However, aren't cuDNN backwards compatible, at least to some degree?\n> > Should we check >=?\n> > \n> > \u2014\n> > You are receiving this because you authored the thread.\n> > Reply to this email directly or view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/pull/1987#issuecomment-212000836>\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1987#issuecomment-212001797\n"]}, {"number": 1986, "title": "Add a comment to install mentioning cuda and cudnn requirements (#1985)", "body": "for PIP installs.\n\n(Also cherry-pick anaconda instructions)\n", "comments": []}, {"number": 1985, "title": "Add a comment to install mentioning cuda and cudnn requirements", "body": "for PIP installs.\n", "comments": []}, {"number": 1984, "title": "Unclear what parallel_iterations does in While/scan (docs issue, possible functional_ops issue)", "body": "It's unclear from both the code and documentation why scan() defaults to 10 parallel iterations. doesn't that cause it to sometimes run the function before the accumulated value is available? I also poked around and read the documentation for While, and that didn't clear things up. Things it looks like it might do:\n- Run steps in parallel if and only if there are no graph dependencies on the last step\n- Run steps in parallel but provide nonsense values for graph dependencies on the last step\n- Ignore it because the parameter doesn't do anything yet?\n- Do magic beyond my ken to skip ahead in serial computations? if so, can I tell it to only run the last one, thereby creating a halting oracle?\n### Steps to reproduce\n1. read the docs in master for scan() and while()\n2. read the code for functional_ops.scan()\n3. compare to the code for functional_ops.map_fn()\n4. be confused that scan() appears to get the right answer when you use it\n", "comments": ["scan is defined in terms of while_loop.  So I will try to answer your question using while_loop. \n\nwhile_loop implements non-strict semantics.  An iteration can start as soon as one of the ops for this iteration is ready (i.e., all its inputs are available.) for execution.  So a while_loop can easily have multiple iterations running in parallel.  For example, for scan, even if the accumulated value is not available in a step, the step can still start and execute any ops that don't depend on the accumulated value. One problem to allow multiple iterations to run in parallel is resource management. parallel_iterations is introduced to give users some control of memory consumption and execution order. \n\nFor _correct_ programs, a while_loop should compute the same value for any value of parallel_iterations >= 1. \n", "@yuanbyu: can you document this somewhere in control_flow_ops.py ?\n", "I will. But let us first see if it clears up the question(s) by lahwran@.\n", "It does, and that's an impressively more fine-grained parallelism implementation than I expected. Cool! \n", "Thanks. Added some more documentations.\n", "@yuanbyu I believe your comment should be directly put into the documentation. Specifically, the following should be added:\r\n\r\n> while_loop implements non-strict semantics. An iteration can start as soon as one of the ops for this iteration is ready (i.e., all its inputs are available.) for execution. So a while_loop can easily have multiple iterations running in parallel.  \r\n\r\nHowever, I think you meant to say \"(i.e., NOT all its inputs are available.)\". \r\n\r\nAlso, it would be useful to know if these semantics depend on the order of the inputs or not.  For instance, suppose we want to run some function `f(x)` 10 times. If the argument order is important for  non-strict semantics, then the first example cannot run in parallel because the iteration counter is holding everything up, while the second version should indeed run in parallel:\r\n\r\n```\r\ni = tf.constant(0) \r\nx = ... \r\ncond1 = lambda i, x: tf.less(i, 10) \r\ndef body1(i, x):\r\n       y = f(x)\r\n       return i+1, y  \r\ntf.while_loop(cond1, body1, [i, x])\r\n\r\ncond2 = lambda x,i: tf.less(i, 10) \r\ndef body2(x,i):\r\n       y = f(x)\r\n       return y, i+1\r\ntf.while_loop(cond2, body2, [x,i])\r\n\r\n```\r\n\r\nIn any event, I feel that such a discussion should definitely be included in the docs since there is no real discussion around the canonical \"for loop\" implementation that is provided:\r\n\r\n> python i = tf.constant(0) c = lambda i: tf.less(i, 10) b = lambda i: tf.add(i, 1) r = tf.while_loop(c, b, [i])\r\n", "@mholzel \"(i.e., all its inputs are available)\" is correct. An iteration can start as soon as one of the ops for this iteration is ready. An op is ready when all of its inputs are available. ", "Right, \"all its inputs\" means \"all the Op's inputs\" not \"all the loop's inputs\". Would you like to submit a PR to incorporate the comment in the doc in a way that you think is going to comprehensible? Sometimes it's hard to tell from inside the codebase :/", "I could do the PR, but I still do not understand this completely myself....\r\n\r\nI think the docs should be updated with a MWE to illustrate exactly what is going on. If you have a good example which demonstrates good vs bad practice, I would appreciate it. Particularly, I think the docs need to have an example that illustrates the \"gotchas\" here, that is, the innocuous things that might be slowing down the ability to parallelize the loop. \r\n\r\nFor instance, a \"while loop\" written to emulate the behavior of a \"for loop\" will have an internal counter. Will that counter limit tensorflow's ability to parallelize the loop? \r\nExample:\r\n\r\n```\r\ni = tf.constant(0) \r\nx = ... \r\ncond_ix = lambda i, x: tf.less(i, 10) \r\n\r\ndef body1(i, x):\r\n       y = f(x)\r\n       return i+1, y  \r\ntf.while_loop(cond_ix, body1, [i, x])\r\n```\r\n\r\nA fundamental question that I cannot tease out of your statements is whether more than one call to `body1(x)` can run in parallel? \r\n", "Let's take the limiting case where parallel_iterations is infinity. Now, if you imagine unrolling the whole loop as a dataflow graph, then the graph will just execute according to its data dependencies.\r\n\r\nSo in your example, there can't be two instances of `y` running in parallel, because `y`'s input in step `i+1` is the output of `y` in step `i`. Likewise, there can't be two instances of `i+1` running in parallel for the same reason.\r\n\r\nHowever, if `f(.)` is slow, all the `i+1` ops will run ahead to completion without waiting for even the `f(.)` in the first iteration to complete, because there's no dependency of `f` that's needed to compute the next `i`.\r\n\r\nIf you had said\r\n```\r\ni = tf.constant(0) \r\nx = ... \r\ncond_ix = lambda i, x: tf.less(i, 10) \r\n\r\ndef body1(i, x):\r\n       y = g(i)\r\n       return i+1, y  \r\ntf.while_loop(cond_ix, body1, [i, x])\r\n```\r\nthen if `g` were slow you could easily end up with `g(0)`, `g(1)`, ..., `g(9)` all running in parallel because there's no dependency on the output of `g(0)` that is needed to start `g(1)`.", "Excellent example and discussion. Precisely along the lines of what should be in the docs. ", "@mholzel Would you be interested in doing PR based on the information in the above reply from @michaelisard? If you do it'd be very helpful!", "Yes. But this week is very busy. I will try to get to it next week. ", "Thank you! We are looking forward to your PR.", "Is there a way to tell while_loop: \"Use as much parallelism as you want\"?\r\nI tried `parallel_iterations=np.iinfo(np.int).max` which worked until TF 1.9, but now breaks.", "@ebrevdo any idea? (Reopening since this is being worked on: we can close when the PR is donw.)", "Please remove the assignee, as this issue is inviting external contributions. Otherwise, remove the `contributions welcome` label. Thank you.", "100 is a safe number\n\nOn Tue, Aug 14, 2018, 7:51 PM Alfred Sorten Wolf <notifications@github.com>\nwrote:\n\n> Please remove the assignee, as this issue is inviting external\n> contributions. Otherwise, remove the contributions welcome label. Thank\n> you.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/1984#issuecomment-413078359>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim80z6qjTZAG6bmGZo0AIxCDJ0RK8ks5uQ4yrgaJpZM4II93M>\n> .\n>\n", "@mholzel Hi, any update on the PR you are working on ? Could you please reference the PR# if you've created.", "I think this is obsolete?", "@michaelisard\r\n\r\nDoes it works on CPU only or also on GPU?"]}, {"number": 1983, "title": "./configure failed on mac with GPU support", "body": "GitHub issues are for bugs / installation problems / feature requests.  \nFor general support from the community, see [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).\nTo make bugs and feature requests more easy to find and organize, we close issues that are deemed\nout of scope for GitHub Issues and point people to StackOverflow.\n\nFor bugs or installation issues, please provide the following information.\nThe more information you provide, the more easily we will be able to offer\nhelp and advice.\n### Environment info\n\nOperating System: mac os 10.11\n\nInstalled version of CUDA and cuDNN: 7.5\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rwxr-xr-x  1 root  wheel  8280 Oct 29 09:50 /usr/local/cuda/lib/libcuda.dylib\nlrwxr-xr-x@ 1 root  wheel    45 Sep 25  2015 /usr/local/cuda/lib/libcudadevrt.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudadevrt.a\nlrwxr-xr-x@ 1 root  wheel    50 Sep 25  2015 /usr/local/cuda/lib/libcudart.7.5.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.7.5.dylib\nlrwxr-xr-x@ 1 root  wheel    46 Sep 25  2015 /usr/local/cuda/lib/libcudart.dylib -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart.dylib\nlrwxr-xr-x@ 1 root  wheel    49 Sep 25  2015 /usr/local/cuda/lib/libcudart_static.a -> /Developer/NVIDIA/CUDA-7.5/lib/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   alembic (0.8.5)\n   appnope (0.1.0)\n   Babel (2.2.0)\n   cffi (1.5.2)\n   click (6.4)\n   colorama (0.3.7)\n   cryptography (1.3.1)\n   cvxopt (1.1.8)\n   Cython (0.23.4)\n   decorator (4.0.9)\n   enum34 (1.1.2)\n   Flask (0.10.1)\n   Flask-AppBuilder (1.6.0)\n   Flask-BabelPkg (0.9.6)\n   Flask-Cache (0.13.1)\n   Flask-Login (0.2.11)\n   Flask-Migrate (1.8.0)\n   Flask-OpenID (1.2.5)\n   Flask-Script (2.0.5)\n   Flask-SQLAlchemy (2.0)\n   Flask-Testing (0.4.2)\n   Flask-WTF (0.12)\n   gnureadline (6.3.3)\n   gunicorn (19.4.5)\n   h5py (2.5.0)\n   humanize (0.5.1)\n   idna (2.1)\n   ipaddress (1.0.16)\n   ipython (4.1.2)\n   ipython-genutils (0.1.0)\n   itsdangerous (0.24)\n   Jinja2 (2.8)\n   Mako (1.0.4)\n   Markdown (2.6.6)\n   MarkupSafe (0.23)\n   numpy (1.11.0)\n   pathlib2 (2.1.0)\n   pexpect (4.0.1)\n   pickleshare (0.7.2)\n   pip (8.1.1)\n   ptyprocess (0.5.1)\n   pyasn1 (0.1.9)\n   pycparser (2.14)\n   python-editor (0.5)\n   python-openid (2.2.5)\n   setuptools (20.9.0)\n   simplegeneric (0.8.1)\n   six (1.10.0)\n   speaklater (1.3)\n   SQLAlchemy (1.0.12)\n   traitlets (4.2.1)\n   vboxapi (1.0)\n   Werkzeug (0.11.5)\n   wheel (0.26.0)\n   WTForms (2.1)\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\nIf installed from sources, provide the commit hash:\n### Steps to reproduce\n\nI tried to build tensorflow from source on macbook pro. It only looks for .so cuda library but on mac the shared libraries end with .dylib. Also it only looks for lib64 sub-directory, which is not present for cuda 7.5 on mac, in which the libraries are in /usr/local/cuda/lib folder. Does the configure script support mac with gpu now?\n\nTianweis-Macbook:tensorflow STW$ ./configure \nPlease specify the location of python. [Default is /usr/local/bin/python]: \nDo you wish to build TensorFlow with GPU support? [y/N] y\nGPU support will be enabled for TensorFlow\nPlease specify which gcc nvcc should use as the host compiler. [Default is /usr/bin/gcc]: \nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \nPlease specify the location where CUDA  toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \nInvalid path to CUDA  toolkit. /usr/local/cuda/lib64/libcudart.so cannot be found\nPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: \n### What have you tried?\n\nI tried to build tensorflow on mac with GPU support. It seems that tensorflow only supports gpu build on linux?\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n", "comments": ["Yes, we don't yet support GPU on Mac.  \n", "Closing as dup of #491\n"]}, {"number": 1982, "title": "add a reference for Caffe in tutorial", "body": "add a reference for Caffe in deepdream tutorial\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1981, "title": "Make sklearn adapter to use LabelBinarizer by default", "body": "TensorFlowDNNClassifier doesn't handle cases, when target labels are not in `{0, 1}`. Usually `scikit-learn` models transform labels internally using `LabelBinarizer` so this is slightly misleading. \n\n``` python\nfrom sklearn import datasets, metrics, cross_validation\nfrom tensorflow.contrib import skflow\n\n## Load dataset and select only first two classes\niris = datasets.load_iris()\nids = np.where((iris.target==0) | (iris.target==1))\niris.data = iris.data[ids]\niris.target = iris.target[ids]\n\n## Change labels from 0/1 to -1/1\n## Comment out the following line to get 1.0 accuracy\niris.target[iris.target==0] = -1 \n\n## Train and predict on iris (copied from example)\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target,\n    test_size=0.2, random_state=42)\n\nclassifier = skflow.TensorFlowDNNClassifier(hidden_units=[10, 20, 10],\n    n_classes=3, steps=200)\n\n# Fit and predict.\nclassifier.fit(X_train, y_train)\nscore = metrics.accuracy_score(y_test, classifier.predict(X_test))\nprint('Accuracy: {0:f}'.format(score)) # Prints 0.4, but prints 1.0 without label change\n```\n\nTF version is `0.8.0rc0`\n\nEDIT:\n\nWhat happens is that `classifier.predict(X_test)` is in [1,2] instead of [-1, 1]\n", "comments": ["@ilblackdragon Any update on this?\n", "We can't do LabelEncoding for you due to support of streaming data - where we don't know what classes you have. Also when data is streaming through the graph estimators don't control the execution flow in anyway.\n\nBecause of that, please use encoding before the model using scikilearn.LabelEncoder or you can do it inside the model function, by using `tf.contrib.lookup.HashTable` lookups.\n", "@ilblackdragon hmm. Some thoughts from outside point of view: standard support for cases like that in sklearn is passing classes as arguments (for instance in `sklearn.metrics`). Why not add classes argument to fit and enforce it in the case of streaming data, and if data is not streaming, do the transformation? It seems slightly confusing for heavy sklearn users (as the example hopefully shows)?\n", "@kudkudak We are actually planning to separate sklearn support into an adapter, where we can implement auto-relabelling magically indeed. I'll add this to the current working proposal. Thanks!\n", "OK, it sounds like the code might have moved around since this issue was last touched. Feel free to open a new github issue if the problem still persists in recent versions, referencing this one."]}, {"number": 1980, "title": "fixed broken links and markdown styling", "body": "fixes #1970\n", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "I signed it\n", "CLAs look good, thanks!\n\n<!-- ok -->\n", "@tensorflow-jenkins test this please.\n(for auto-merge happiness even though it's just a doc change)\n"]}, {"number": 1979, "title": "Fix tb r0.8", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "These are already in master  (#1926)\n"]}, {"number": 1978, "title": "Feature Request: Viewing Only Computation Graph in Tensorboard", "body": "For study and debug purposes it could be very useful to view computation network graph in tensor flow, without having to dump any histogram/learning data. \n\n@dsmilkov , I learnt from @danmane , that such a feature already exists for debug purposes in tensor flow, not yet available for users. What do think about possibility of making it accessible to users.\n", "comments": ["One way we could do this is to host a static graph visualizer on `tensorflow.org/tensorboard/graph-visualizer` (or similar) and by navigating to that url, you could upload a pbtxt file from your computer and visualize it.\n\nI imagine though that you would probably like a cleaner workflow so you don't need to move these pbtxt files around yourself. What should the API look like? Maybe something like:\n\ntensorboard.visualize_graph(graph) <- this will launch a TensorBoard server that visualizes your graph?\n", "I think, tensorboard.visualize_graph(graph) sounds reasonable. But we should also support the same in the usual flow with something like tf.export_graph() , which should export the pbtxt file in the loddir, and then tensor board --logdir=<> works as usual, if there is nothing else to show, it shows only the graph.\nWhat do you think?\n", "Having tensorboard read pbtxts directly from the logdir is possible, although it's a material departure from the way TensorBoard is currently organized (right now it always tries to read from events files). I'm not sure if that's important enough / common enough to change TensorBoard to read the pbtxts - if you had a tensorboard.export_graph(graph) api why would you want to bother with the pbtxt files?\n\nAlthough, if what you want right now is to visualize your pbtxts without needing to turn on TensorBoard, you can already do that: navigate to [https://www.tensorflow.org/tensorboard/index.html#graphs](https://www.tensorflow.org/tensorboard/index.html#graphs) and click on `Upload: Choose File` in the left sidebar.\n", "@ultrons, There's a way to show graph visualization in IPython notebook,  see the function \"show_graph\" in Alexander's deep dream [notebook](http://nbviewer.jupyter.org/github/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb)\n", "Closing due to lack of activity.\n", "Now that tensorboard url is down ( https://www.tensorflow.org/tensorboard/index.html#graphs) . Having this feature becomes more important in usual workflow.", "For anyone who might've stumbled upon this thread:\r\n\r\nUsers who have a *.pbtxt can go to TensorBoard.dev, the service for hosting TB data\r\n- Go to https://tensorboard.dev/experiment/QFRIzZJpTZCNRzi8N7zomA/#graphs\r\n- Click 'Upload file' in the left sidepane to upload your graph from your filesystem\r\n\r\nUsers running a Python notebook can use the TensorBoard notebook magic to launch TB and visualize a graph.  See https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks for details.\r\n\r\nUsers without a notebook interface can write a Python GraphDef object into a *.pbtxt manually using `tf.io.write_graph()`, as described in https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/graph/README.md#creating-a-graph."]}, {"number": 1977, "title": "Revert change to r0.8 branch that points to new URLs", "body": "They haven't been uploaded yet, and so our website is pointing people at the wrong location.\n", "comments": ["Did we ever actually deploy the website with those changes? Better to be safe, for sure. Possibly in the future, we should point the website at a tag again (possibly the latest tag on each branch, we can find those easily).\n", "(We did, so I rolled it back and re-deployed the website).\n", "Thanks. :(\nOn Sun, Apr 17, 2016 at 10:08 Vijay Vasudevan notifications@github.com\nwrote:\n\n> (We did, so I rolled it back and re-deployed the website).\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1977#issuecomment-211060220\n"]}, {"number": 1976, "title": "add a reference for Laplacian pyramid", "body": "add a reference for Laplacian pyramid\n", "comments": ["Can one of the admins verify this patch?\n"]}, {"number": 1975, "title": "Incorrect Download URL", "body": "It appears the location has been changed and not updated in the tutorial.\n\n```\nsudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n\nCollecting tensorflow==0.8.0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n  HTTP error 404 while getting https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n  Could not install requirement tensorflow==0.8.0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl because of error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\nCould not install requirement tensorflow==0.8.0 from https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl because of HTTP error 404 Client Error: Not Found for url: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl for URL https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n```\n", "comments": ["Getting the same error.\n", "Same.\n", "Same!!!\n", "Same!! :(\n", "It's still 0.8.0rc0, we changed the install instructions before we uploaded the new packages.  Sigh.\n\nExample:\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl\n", "This works for mac:\n`sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.8.0rc0-py2-none-any.whl`\n", "Fixed in github, pushed to website.\n", "I'm stil getting the same error \n", "I still get the same error\n", "Following the installation instructions [here](https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#virtualenv-installation) when I run this it works\n\n```\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc1-py3-none-any.whl\n```\n\nBUT on the next step of \n\n```\npip3 install --upgrade $TF_BINARY_URL\n```\n\nI get the error `HTTP error 404 while getting https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc1-py3-none-any.whl`\n\n```\n```\n", "Same here \ud83d\ude2d \n", "I installed tensorflow in conda environment and it worked. Maybe you should give a try. \n\nAfter installing anaconda make sure to set your path. Or else it would raise command error \"conda: command not available\" \n", "Use this URL: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.11.0rc0-cp27-none-linux_x86_64.whl\n\nIt works (replace rc1 for rc0)\n", "it works to install but doesn't work to run because it wants cuda 7.5 and the latest is 8 :P \n", "create a simlink on the cuda 7.5 file to cuda 8.0 file with a same name and it works :P\n", "Maybe you can just \r\n\r\ncpu\r\n`pip install tensorflow`\r\n\r\ngpu\r\n`pip install tensorflow-gpu`"]}, {"number": 1974, "title": "master: Fix TensorBoard README url for demo TensorBoard", "body": "", "comments": []}, {"number": 1973, "title": "r0.8: Fix the TensorBoard README.md url for demo TensorBoard", "body": "", "comments": []}, {"number": 1972, "title": "trivial typo in ci_build README", "body": "bellow -> below\n", "comments": ["Can one of the admins verify this patch?\n", "LGTM\n", "@tensorflow-jenkins test this please.\n(for auto-merge happiness even though it's just a doc change)\n", "Jenkins, this this PLEASE!\n", "Jenkins? test this please?\n", "Man, sometimes he's really stuck up.\n", "It's a Friday - he probably left early to go to Yosemite.  In fairness, the falls are absolutely gorgeous this time of year.\n", "@martinwicke, you mistyped the first \"test this\".  Maybe Jenkins needs some\nNLP autocorrect.  :P\n\nOn Fri, Apr 15, 2016 at 4:11 PM, David Andersen notifications@github.com\nwrote:\n\n> It's a Friday - he probably left early to go to Yosemite. In fairness, the\n> falls are absolutely gorgeous this time of year.\n> \n> \u2014\n> You are receiving this because you authored the thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/1972#issuecomment-210678466\n"]}, {"number": 1971, "title": "Merging changes from internal.", "body": "", "comments": ["Can one of the admins verify this patch?\n", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit https://cla.developers.google.com/ to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If you signed the CLA as a corporation, please let us know the company's name.\n\n<!-- need_sender_cla -->\n", "test this please\n", "@tensorflow-jenkins test this please\n"]}, {"number": 1970, "title": "Scikit Flow Readme file", "body": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/README.md\n\nSeems to me that README file extension should be rst. But instead it is a markdown and now all links are broken etc.\n\nIf this is true then first row should be changed to `|License| |Join the chat at https://gitter.im/tensorflow/skflow|` and everything is alright.\n", "comments": ["@Vlsarro You are right, it was Rst (to put it in pip easily). Though now after move inside tensorflow, to easily display on website it should be Markdown. I'll address links to actually be markdown this weekend.\n", "@ilblackdragon Don't waste your time, I already fixed most of it yesterday. Just wanted to make sure that it should be markdown. Check pull request\n"]}, {"number": 1969, "title": "What version of cuda and cudnn does TF 0.8 support?", "body": "Hi, I have installed Cuda 7.5 and Cudnn v5 for use with TensorFlow 0.8 and am getting the following error when trying to run /tensorflow/models/image/mnist/convolutional.py\n\nF tensorflow/stream_executor/cuda/cuda_dnn.cc:427] could not set cudnn filter descriptor: CUDNN_STATUS_BAD_PARAM\nAborted (core dumped)\n\nShould I be using an older version of Cuda and/or Cudnn? Thank you\n", "comments": ["See https://github.com/tensorflow/tensorflow/issues/1787#issuecomment-210560825\n"]}, {"number": 1968, "title": "Common shape ops for SparseTensor", "body": "It should be easy to extend/modify `tf.rank()`, `tf.shape()`, and `tf.size()` to have them work on `SparseTensor`s.\n\nHappy to review PRs!\n", "comments": ["Hi! \nI'm working on this.\n", "@concretevitamin I'm having some problem with bazel-build. Not able to build from source. Is there any other way I can build from source?\n", "What are the errors?  I trust that the latest 0.8 docs on the website did\nnot help?\n\nOn Sunday, April 17, 2016, Anish Shah notifications@github.com wrote:\n\n> @concretevitamin https://github.com/concretevitamin I'm having some\n> problem with bazel-build. Not able to build from source. Is there any other\n> way I can build from source?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1968#issuecomment-211090564\n", "@concretevitamin Could you provide some hints as to how this might be done? I spent some time looking at some sparse operations to understand how `SparseTensor`s are handled. In most of the cases, `SparseTensor`s are converted to `Tensor`s for the underlying Op. One exception I found is `sparse_matmul`. Are `a_is_sparse`, `b_is_sparse` somehow relevant?\n", "@siddharth-agrawal Essentially, SparseTensor in Python can be thought of as a 3-tuple of dense tensors, `(indices, values, shape)`.  You're right that most underlying Ops take these three dense Tensors separately, but there's no \"conversion\" going on. \n\nAs for `sparse_matmul()`: due to historical reasons it's actually a misnomer.  It does not operate on `SparseTensor`s, but instead uses an optimized algorithm to operate on two dense Tensors depending on the two flags.  So that op is irrelevant. \n", "@concretevitamin If we take the special case of `tf.rank()`, then we need some way making [this statement](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/shape_ops.cc#L132) generic so that it works for a `SparseTensor` as well. Are there any utilities that can differentiate between a `Tensor` and a `SparseTensor`? If not, can you tell me how to approach this (since you said that i should be easy in the description)? I did a lot of digging, but I still have no idea how to solve this. Even a 'where to look' reply would be fine.\n", "@siddharth-agrawal I think this can be implemented entirely in Python.  In [array_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py), you could add a `def rank(...)` function with the correct signature and convention.  Inside that, if `isinstance(t, ops.SparseTensor)`, you can return `size(t.shape)`, because the number of elements in `t.shape` is the rank of `t`. \n\n(More explanation: right now, when `tf.rank()` is called, what really happens is [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L89). So once we override it properly, the Python frontend can handle the dispatch. )\n", "@concretevitamin Is there any example of overriding? I'm not familiar with how it's done in TensorFlow. I saw the binary op overrides in `math_ops.py`, but I'm guessing this will be somewhat different.\n", "@siddharth-agrawal One recent example is the introduction of `tf.sparse_softmax()` [here](https://github.com/tensorflow/tensorflow/commit/7e5261b7f35871282010f655de5d72e2d2edc29f).  The Python wrapper in `sparse_ops.py` -- named `sparse_softmax()` and which accepts one input -- overrides the function `gen_sparse_ops.sparse_softmax()`, which accepts three inputs.  This is what I meant by overriding.  Does this make sense?  \n", "@concretevitamin Yes, this helps. I will try to get something out by Friday.\n", "@concretevitamin Would like your comments on the PR. I retained the comment for `_sparsify()` so that you don't miss it while factoring things out.\n", "@concretevitamin New PR for `tf.shape()`. Would like your comments.\n", "@concretevitamin For `tf.size()`, what should the output be for `SparseTensor`? The size of the tensor or the number of non-zero values?\n", "I am leaning towards making it consistent with dense Tensors, i.e. it\nreturns the size of the tensor.  Consistency here might be the less\nsurprising choice.\n\n(Also, quick heads up I will be out this coming week.  @ebrevdo and\n@rmlarsen might be good persons to look at SparseTensor-related changes!)\n\nOn Sat, Jun 4, 2016 at 11:08 PM, Siddharth Agrawal <notifications@github.com\n\n> wrote:\n> \n> @concretevitamin https://github.com/concretevitamin For tf.size(), what\n> should the output be for SparseTensor? The size of the tensor or the\n> number of non-zero values?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1968#issuecomment-223795234,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/AAkLHld0QxdDB5qSAFGmU2MU_9MeyezPks5qImfqgaJpZM4IIer2\n> .\n", "+1 for having it return reduce_prod(sparse_tensor.shape)\n\nOn Sat, Jun 4, 2016 at 11:16 PM, Zongheng Yang notifications@github.com\nwrote:\n\n> I am leaning towards making it consistent with dense Tensors, i.e. it\n> returns the size of the tensor. Consistency here might be the less\n> surprising choice.\n> \n> (Also, quick heads up I will be out this coming week. @ebrevdo and\n> @rmlarsen might be good persons to look at SparseTensor-related changes!)\n> \n> On Sat, Jun 4, 2016 at 11:08 PM, Siddharth Agrawal <\n> notifications@github.com\n> \n> > wrote:\n> > \n> > @concretevitamin https://github.com/concretevitamin For tf.size(),\n> > what\n> > should the output be for SparseTensor? The size of the tensor or the\n> > number of non-zero values?\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/1968#issuecomment-223795234\n> > ,\n> > or mute the thread\n> > <\n> > https://github.com/notifications/unsubscribe/AAkLHld0QxdDB5qSAFGmU2MU_9MeyezPks5qImfqgaJpZM4IIer2\n> > \n> > .\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1968#issuecomment-223795458,\n> or mute the thread\n> https://github.com/notifications/unsubscribe/ABtim3ryad0V6lkPzSXxdLwEdKN_K_ecks5qImm3gaJpZM4IIer2\n> .\n", "@ebrevdo New PR for tf.size(). Would like your comments on it.\n", "Drive by comment to say thank you @siddharth-agrawal for all the PRs!\n", "@girving My pleasure! It is fun working with the TF team!\n", "@concretevitamin Your comment [here](https://github.com/tensorflow/tensorflow/pull/2681#discussion_r66742242) applies to my earlier PR [here](https://github.com/tensorflow/tensorflow/pull/2659) too. I will update it once the `tf.size()` PR is merged.\n", "@concretevitamin You can close the issue now, all changes done.\n", "@siddharth-agrawal Thanks for your contributions!\n", "@concretevitamin My pleasure!\n"]}, {"number": 1967, "title": "how could tf.nn.max_pool work with dynamic kernel size?", "body": "", "comments": ["Not with the current op interface. You would have to write a new op that takes the kernel size as an input tensor. The kernel implementation can be refactored to use most of the current max_pool implementation. \n", "Closing. Feel free to reopen if this is a feature request worth tracking.\n", "+1\nI would like to request that feature. \n"]}, {"number": 1966, "title": "Fixed bug in `tf.train.ClusterSpec` constructor.", "body": "Creating a `tf.train.ClusterSpec` from another ClusterSpec was broken,\nwhich in turn broke creating a `tf.train.Server` from a ClusterSpec.\n\nFixes #1961.\nChange: 119954117\n", "comments": ["Can one of the admins verify this patch?\n", "@tensorflow-jenkins test this please.\n", "LGTM. All PR tests passed. Will merge and build the docker images for the GRPC TF server and run the tests in dist_tests.\n"]}, {"number": 1965, "title": "AttributeError: NewBase is_abstract, ImportError: libcudart.so.7.5", "body": "Hi,\n\nI have tried to install tensorflow as in https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html\nI have cuda 7.0 installed under /usr/local/cuda\n After I install gpu version using pip it gives the error-  ImportError: libcudart.so.7.5: cannot open shared object file: No such file or directory. \nBut I have  libcudart.so.7.0 file. Why this is referring 7.5 by default ? \n(issues for version 0.7 and 0.8)\n\nThen I tried to install cpu only version and giving me the error\nAttributeError: type object 'NewBase' has no attribute 'is_abstract' (only to version 0.8)\n\nHow can I solve those issues ?\nThanks\n", "comments": ["FYI, saw similar issue when installing 0.8rc0 in the Travis: https://travis-ci.org/tensorflow/skflow/jobs/123599570\n", "cuda issue: our pip installs require libcuda 7.5  I've updated the documentation and the website to clear this up.  If you want to use a lower libcuda version, you need to install from sources.\n", "the NewBase one looks related to benchmark.py, assigning to @ebrevdo \n", "I have the exactly same problem. Could you tell me what you did to solve it?\n", "I had the same problem and solved by upgrading a library named 'Six'\n", "Hi guys,\n\nI have the same problem after the tensorflow installation via pip and updating the Six library didn't help:\n\n> > > import tensorflow as tf I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/local/lib/python2.7/dist-packages/tensorflow/**init**.py\", line 23, in <module> from tensorflow.python import \\* File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/**init**.py\", line 94, in <module> from tensorflow.python.platform import test File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/test.py\", line 62, in <module> from tensorflow.python.framework import test_util File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 41, in <module> from tensorflow.python.platform import googletest File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/googletest.py\", line 32, in <module> from tensorflow.python.platform import benchmark # pylint: disable=unused-import File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py\", line 112, in <module> class Benchmark(six.with_metaclass(_BenchmarkRegistrar, object)): File \"/usr/lib/python2.7/dist-packages/six.py\", line 617, in with_metaclass return meta(\"NewBase\", bases, {}) File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/benchmark.py\", line 107, in __new__ if not newclass.is_abstract(): AttributeError: type object 'NewBase' has no attribute 'is_abstract'\n\nI'm using Nvidia GTX Titan X (comp. capability 5.2) and CUDA 7.5. Does anybody have an idea?\n\nMany thanks in advance for your help!\n\nCheers,\nA\n", "Looking specifically at the NewBase issue: please tell me the exact version of python and six that you are using.\n", "specifically, run:\n\n```\nimport six\nprint(six.__version__)\n```\n", "python 2.7.6\nsix 1.5.2\n\nI got this one by pip upgrade\n", "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L41\n\nWe require > 1.10  for six.  I'm wondering why setup.py didn't require upgrading your 'six' :(  Can you try updating?\n", "I can import six, but when I want to print six.version, it says \" 'module' object has no attribute 'version' \"\n", "Here is the thing: I have installed \"six-v1.10.0\" by pip upgrading, but when I open the python and print the version of six, it still says \"1.5.2\".\n", "I have figured out how to upgrade the package \"six\" properly. \nThe default location of \"pip install\" in my computer is \"/usr/local/lib/python2.7/dist-packages\". So when I upgrade \"six\", it is the file in \"/usr/local/lib/python2.7/dist-packages\" that is upgraded. \nHowever, when we import some package in Python, another location \"/usr/lib/python2.7/dist-packages\" is searched first, while the file \"six.py\" in this location has never been upgraded.\nSo what we need to do is to modify the default location of \"pip install\" from \"/usr/local/lib/python2.7/dist-packages\" to \"/usr/lib/python2.7/dist-packages\"; then we can upgrade the right \"six.py\".\nTo modify the default location, we can make a new file \"/etc/pip.conf\", and type the following into this file:\n\"\n[global]\ntarget=/usr/lib/python2.7/dist-packages\n\".\nNow you can upgrade the package \"six\" properly.\nTo avoid some unknown error, you may want to change the default location back to \"/usr/local/lib/python2.7/dist-packages\" after upgrading \"six\".\n", "I only installed the CPU-version tensorflow. Now everything works well for me. Hope this could be helpful for you.\n", "**xuxy09**'s solution also works for GPU version\n", "Thanks a lot for the help! Updating Six library to 1.10 helped!\n", "Yes Thank you. I have uninstalled apt-get numpy and reinstalled pip numpy v11 and upgraded the six 1.10 package and working fine on CPU. \n", "CUDA 7.5 and cuDNN 5.0. the above mentioned error no longer appears after doing what xuxy09 suggested. I would only like to make it a little easier for you with the next line:\n\nsudo pip install six --upgrade --target=\"/usr/lib/python2.7/dist-packages\"\n", "It's strange that it works well in my PC--which the path is `/usr/local/lib/python2.7/dist-packages`--but occur errors in my workshop computer with the same path. After I set the target path as `/usr/lib/python2.7/dist-packages`, this problem is gone. Now my tensorflow works well, thank you very much! @yselivonchyk \n", "The most popular method of get the version one package is as follows.\r\n`$ python`\r\n`>>>import six`\r\n`>>>print six.__version__`\r\n`...`\r\n`>>>import protobuf`\r\n`>>>print protubuf.__version__`\r\n\r\nRecently, I have learnt a simple method to get the version of the packages, such as numpy, six and protobuf, like this:\r\n`pip show numpy six protobuf`\r\n\r\nWith this method, both the verison and orther detailed information of the package will be presented, so I want to share with you.\r\n", "Aha\nI have another solution:\n$ cd /usr/lib/python2.7/dist-packages\n$ sudo rm six*\n", "That won't work in the long run if your package manager / os manages six in\nthat directory.\nOn May 10, 2016 2:32 AM, \"Zhang Wang\" notifications@github.com wrote:\n\n> Aha\n> I have another solution:\n> $ cd /usr/lib/python2.7/dist-packages\n> $ sudo rm six*\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1965#issuecomment-218106367\n", "I agree. It's quite dangerous to delete some package directly.\n\nxuxiangyu2014@qq.com\n\nFrom: ebrevdo\nDate: 2016-05-10 14:30\nTo: tensorflow/tensorflow\nCC: XiangyuXu; Comment\nSubject: Re: [tensorflow/tensorflow] AttributeError: NewBase is_abstract, ImportError: libcudart.so.7.5 (#1965)\nThat won't work in the long run if your package manager / os manages six in\nthat directory.\nOn May 10, 2016 2:32 AM, \"Zhang Wang\" notifications@github.com wrote:\n\n> Aha\n> I have another solution:\n> $ cd /usr/lib/python2.7/dist-packages\n> $ sudo rm six*\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1965#issuecomment-218106367\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly or view it on GitHub\n", "$home/.pip/pip.conf is better and also worked.\n", "I had a similar error:\n`AttributeError: type object 'NewBase' has no attribute 'is_abstract'`\nThis fixed my issue:\n`sudo pip install six --upgrade --target=\"/usr/lib/python2.7/dist-packages\"`\nShouldn't TF test for a particular six version, or at least help the user on his way.\n", "Seems like the issue is fixed by updating six correctly. Closing.\n", "If you get error `Not uninstalling six at /path/to/six, owned by OS`, run `sudo easy_install six`\n", "for the record as pr this thread a really simple solution was just to install python with brew, which aligns all the paths for you. \n", "mac os : 10.11\nbefore run the command in the following line , we must uninstall the `six` by running `sudo pip uninstall six`\nI met this erro and fix it by run `sudo easy_install six`\n", "My problem is exactly the same as @xuxy09, and I used the solution provided by @yselivonchyk. It solved my issue. Thanks a lot.", "I can confirm either methods ( @xuxy09 / @yselivonchyk ) solve this particular issue on `Ubuntu Trusty`.", "I needed to add a `-U` to the command to get it to work: `sudo easy_install3 -U six`", "For me, I had the CPU version running on ubuntu 64bit. I had the same error \"AttributeError: type object 'NewBase' has no attribute 'is_abstract' \" \r\n\r\nI fixed it simply by running `easy_install3 -U six` while inside the activated virtualenv. Hope that helps"]}, {"number": 1964, "title": "add a wiki reference for partial differential equation", "body": "add a wiki reference for partial differential equation\n", "comments": ["Can one of the admins verify this patch?\n", "Can one of the admins verify this patch?\n", "btw, feel free to batch all of these changes into one commit.\n"]}, {"number": 1963, "title": "add a wiki reference for Mandelbrot set", "body": "add a wiki reference for Mandelbrot set\n", "comments": ["Can one of the admins verify this patch?\n", "Can one of the admins verify this patch?\n"]}, {"number": 1962, "title": "Killed during checkpoint save (v0.8)", "body": "### Environment info\n\nOperating System: Ubuntu 15.10\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n`-rw-r--r-- 1 root root   322936 aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024 mar  6 15:08 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5\n-rwxr-xr-x 1 root root 11172416 feb 29 22:12 /usr/local/cuda/lib64/libcudnn.so.6.5.48\n-rw-r--r-- 1 root root 62025862 mar  6 15:08 /usr/local/cuda/lib64/libcudnn_static.a`\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n\n`0.8.0rc0`\n### Relevant code:\n\nUnfortunately I can't upload everything, but the code is based on your CIFAR-10 example. Here is the structure of my network:\nhttp://pastebin.com/5QEJWqtm\n\nAfter running for some time, I save a checkpoint:\n\n`saver.save(sess, checkpoint_path, global_step=step)`\n\nWhich _sometimes_ allocates all memory on my system and gets killed. I have 8gb RAM and 8gb swap. For the first few checkpoints it seems fine, it allocates 2gb RAM (equal to checkpoint file size), but after some time it locks up my entire system and gets killed automatically.\n\nDidn't have any issues in 0.7.\n", "comments": ["Could you please try calling save with this option:\n\nsaver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n\nSherry\n", "Adding `write_meta_graph=False` fixed the error for me where saver.save would result in `MemoryError` on an Amazon GPU. But now I randomly get `terminate called after throwing an instance of std::bad_alloc what(): std::bad_alloc Aborted (core dumped)`. I assume that the model is just too big and random fluctuations in memory use push it over the edge.\n", "Sorry for the delay @sherrym, finally got to try this out. `write_meta_graph=False` seems to have solved the issue for me. I don't get @jmugan's error either.\n", "This is happening for me while saving a checkpoint with or without `write_meta_graph=False` on tf.0.8.0rc0 on Ubuntu 14.04.  The checkpoint appears to be in a temporary state just before the crash occurs.\n\n```\n-rw-rw-r-- 1 ubuntu ubuntu 590993388 May 11 07:51 gan.ckpt-1400\n-rw-rw-r-- 1 ubuntu ubuntu 67731865 May 11 07:59 gan.ckpt-1500.tempstate5056164176772455283\n```\n", "On further inspection, this occurs once the system has run out of memory.  In my particular application, it seems the process grows from using ~ 1 GB to using the entire system memory ~16 GB (no swap) over the course of 8 hours, and then crashes with the above message.   I can at least continue training by using a swap space, but it seems there is another issue here causing a memory leak.\n", "@jmugan, I agree with you that the model is probably too big for your system.\n\n@pkmital, could you please file a bug if you believe the memory leak is in TensorFlow, and not in the way your model is constructed?\n\nClosing this bug as the fundamental issue is memory, not the saver itself.\n", "Hi @sherrym  : has this issue been resolved?  I am running into the same problem where the program crashes while trying to save the checkpoint file. \n", "I am facing the same error and write_meta_graph=False doesn't seem to work in my case. I have a RAM of about 5.5 GB. The training is aborted at the first step only. Can anyone suggest what could be the issue? ", "I have this error, too.\r\nAnd I have a RAM of 60GB. I get this error after some save operators. It seems that the save operator don't release the memory rightly. My version is '0.12.0'.", "Same error with 1.0", "Same error with 1.0", "same error (with 1.01) has anyone figured a solution out?", "I found out that I do not have a swap partition. Creating one solves this problem. Verify with `sudo free -h`\r\n\r\nTo create, (guide from https://linuxize.com/post/create-a-linux-swap-file/)\r\n`sudo fallocate -l 32G /swapfile`\r\n`sudo dd if=/dev/zero of=/swapfile bs=1024 count=33554432`\r\n`sudo chmod 600 /swapfile`\r\n`sudo mkswap /swapfile`\r\n`sudo swapon /swapfile`"]}, {"number": 1961, "title": "'ClusterSpec' object has no attribute '_cluster_spec'", "body": "### Environment info\n\nOperating System: Ubuntu 14.04\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0rc0-cp27-none-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.8.0rc0\n### Steps to reproduce\n1. https://github.com/dsindex/tensorflow/blob/master/mlp_mnist_dist.py\n2. python mlp_mnist_dist.py --ps_hosts=localhost:2222,localhost:2223 --worker_hosts=localhost:2224,localhost:2225 --job_name=ps --task_index=0\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n\n{'ps': ['localhost:2222', 'localhost:2223'], 'worker': ['localhost:2224', 'localhost:2225']}\nTraceback (most recent call last):\n  File \"mlp_mnist_dist.py\", line 90, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 30, in run\n    sys.exit(main(sys.argv))\n  File \"mlp_mnist_dist.py\", line 35, in main\n    server = tf.train.Server(cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/server_lib.py\", line 135, in **init**\n    job_name, task_index, protocol)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/server_lib.py\", line 65, in _make_server_def\n    cluster_spec = ClusterSpec(server_or_cluster_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/server_lib.py\", line 242, in __init__\n    self._cluster_spec[job_def.name] = [t for t in job_def.tasks.values()]\nAttributeError: 'ClusterSpec' object has no attribute '_cluster_spec'\n", "comments": ["Oops, this is an embarrassing bug - thanks for catching it! I've got a fix in the pipeline. For now, you can make it work by changing line 35 of your `mlp_mnist_dist.py` to:\n\n``` python\nserver = tf.train.Server(cluster.as_cluster_def(), job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n```\n", "@mrry thanks~ it works now :)\n", "@mrry Let's put the fix into 0.8.0 final release.\n", "@caisq: Agreed! Shall I send an individual PR with the cherrypicked commit?\n", "@mrry: Yes, please send the PR to branch r0.8 and we'll merge it back to master.\n"]}]