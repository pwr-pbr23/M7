[{"number": 30395, "title": "ERROR: Invalid command prefix \"invoke_stepper\"", "body": "When I use tfdbg and I input the command line: invoke_stepper. It appears ERROR: Invalid command prefix \"invoke_stepper\"", "comments": ["I just use the example:python -m tensorflow.python.debug.examples.debug_mnist --debug", "Thanks very much if you can help me", "Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I also found this problem in tensorflow 1.50.0 ,both on WIN10 and Ubuntu20.04, "]}, {"number": 30394, "title": "Removed Deprecated API from Pythons ops", "body": "", "comments": ["@amitsrivastava78 Could you please address the build failures? Thanks!", "@gbaned , thanks for intimation tomorrow i will update this PR.\r\n\r\nRegards\r\nAmit", "@alexp, can you please re-approve the PR i have fixed the indentation issues,\r\n\r\nRegards\r\nAmit", "@gbaned , can you pls help to get this PR merged.\r\n\r\nRegards\r\nAmit", "@gbaned , I have checked the failure is not because of my changes, can you please help to get this PR merged.\r\n\r\nRegards\r\nAmit", "@gbaned can you help to get this PR merged.\r\n\r\nRegards\r\nAmit", "Yes, on these files either revert the change or fix the test or manually\nmimic the old-where broadcasting behavior.\n\nOn Fri, Jul 19, 2019 at 4:40 AM Amit <notifications@github.com> wrote:\n\n> *@amitsrivastava78* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/sparsemax/python/ops/sparsemax.py\n> <https://github.com/tensorflow/tensorflow/pull/30394#discussion_r305317016>\n> :\n>\n> > @@ -85,7 +85,7 @@ def sparsemax(logits, name=None):\n>      p = math_ops.maximum(\n>          math_ops.cast(0, logits.dtype), z - tau_z[:, array_ops.newaxis])\n>      # If k_z = 0 or if z = nan, then the input is invalid\n> -    p_safe = array_ops.where(\n> +    p_safe = array_ops.where_v2(\n>\n> @alextp <https://github.com/alextp> thanks for the comments, do you\n> recommend to revert back the change ?\n>\n> Regards\n> Amit\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30394?email_source=notifications&email_token=AAABHRMQ35JJSNF7MHQZDGLQAGR4TA5CNFSM4H5S52UKYY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOB67MAXQ#discussion_r305317016>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAABHRPZIS2XJHHYNVSYNPDQAGR4TANCNFSM4H5S52UA>\n> .\n>\n\n\n-- \n - Alex\n", "@alextp changes are reverted for these files, kindly check.\r\n\r\nRegards\r\nAmit"]}, {"number": 30393, "title": "TFLite on Android (JNI) gives inconsistent results when the input ByteBuffer is reused", "body": "**System information**\r\n- Have I written custom code: yes\r\n- OS Platform and Distribution: Android 7.1 (compileSdkVersion 28)\r\n- Mobile device: Galaxy S7 (custom ROM Android 7.1)\r\n- TensorFlow version: TFLite 1.14.0\r\n\r\n**Describe the current behavior**\r\nI create a TFLite Interpreter and allocate a direct ByteBuffer in Java to process a sequence of images. I then copy image data into the buffer for inference, which runs in an AsyncTask with a serial executor, so the Interpreter and buffer are only used by one forward pass at a time.\r\n\r\nDoing this the predictions sometimes (always?) relate to an earlier (possibly always the first) image in the sequence, not the current image. So things work reliably for the first image, but not for the following images.\r\n\r\nI can circumvent this problem by allocating a new ByteBuffer for every frame. I'd like to avoid these frequent allocations, but more importantly I am having a hard time figuring out why I am getting this behaviour in the first place. It's almost like TFLite doesn't use the updated data in the buffer when it sees that the buffer reference hasn't changed.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect to be able to reuse the input ByteBuffer with updated contents across multiple inference calls.", "comments": ["You should be able to reuse a ByteBuffer, as [that is what we do in all of our samples](https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L84). When re-populating the ByteBuffer each frame, make sure you [rewind it first](https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/android/app/src/main/java/org/tensorflow/lite/examples/classification/tflite/Classifier.java#L230). You'll also need to make sure you provided that ByteBuffer to every Interpreter.run() call. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30393\">No</a>\n", "I update the ByteBuffer in a native library via JNI calls. The C++ code does not update the position or limit values, it simply retrieves the pointer and overwrites all RGB pixel values of the buffer, so rewind isn't necessary. This JNI call happens from the same AsyncTask instance that is also running inference. Here is the relevant code snippet run by AsyncTask.doInBackground. Variable `floatImage` is the ByteBuffer used for inference. The `outputMap` is newly allocated for this inference call.\r\n\r\n      // Convert the image to float in range [-1, 1].\r\n      Trace.beginSection(\"convertToFloat\");\r\n      try {\r\n        Utils.convertImageToFloat(mInput, mInputWidth, mInputHeight, NUM_CHANNELS, 1f / 127.5f,\r\n            -1f, floatImage);\r\n      } catch (Exception e) {\r\n        Log.e(TAG, mData.id, \": \", e.getMessage());\r\n        return;\r\n      } finally {\r\n        Trace.endSection(); // convertToFloat\r\n      }\r\n\r\n      Trace.beginSection(\"inference\");\r\n      try {\r\n        mTFLite.runForMultipleInputsOutputs(new Object[]{floatImage}, outputMap);\r\n      } catch (Exception e) {\r\n        Log.e(TAG, mData.id, \": \", e.getMessage());\r\n        return;\r\n      } finally {\r\n        Trace.endSection(); // inference\r\n      }\r\n", "Just curious, if you're using JNI to convert to a ByteBuffer in native code, why not simply use the native C++ APIs that we provide? Or is it for convenience in using the BinTray/JCenter .aars that we distribute?\r\n\r\nJust confirming that your ByteBuffer is a direct ByteBuffer (allocateDirect())? Are you using the 1.14 .aar that we just published? Or a nightly build?", "Yes I could have used the native APIs. It was simply familiarity with the JNI APIs, as I had used them before. Given that the input data is in a direct ByteBuffer (yes, allocated with `allocateDirect()`), the overhead seemed negligible.\r\n\r\nI used the JCenter `implementation 'org.tensorflow:tensorflow-lite:1.13.1'` dependency previously, but for this model I needed TFLite 1.14.0 and the dependency wasn't available yet when I was working on this a couple of weeks ago. I ended up building libtensorflowlite.jar and libtensorflowlite_jni.so from the source revision tagged 1.14.0 in Git. I'll go back to using the dependency when I next touch this code.\r\n\r\nI haven't spent any more time on this as it works correctly with a new ByteBuffer per frame, but I'm still at a loss as to what's causing the issues I encountered when reusing a single buffer.", "Yeah, this is very odd given that all of our samples reuse a persistent ByteBuffer across frames. The only difference is that we populate the (direct) ByteBuffer from Java, not from native.\r\n\r\nIn your native code, are you calling GetDirectBufferAddress() every single frame? Or are you caching the native pointer? If the latter, I'm not sure there are guarantees that the address will remain persistent for the duration of the ByteBuffer's lifetime.", "Yes I call GetDirectBufferAddress() for every frame. The implementation of convertImageToFloat is very straight forward (with only a couple of checks omitted):\r\n\r\n```\r\n  const size_t inSize = static_cast<size_t>(height) * width * channels;\r\n  const auto* inData = sns::bufferAs<const uint8_t>(env, image);\r\n  auto* outData = sns::bufferAs<float>(env, out);\r\n  if (!inData || !outData) return;\r\n\r\n  for (size_t i = 0; i < inSize; i++) {\r\n    outData[i] = static_cast<float>(inData[i]) * scale + add;\r\n  }\r\n\r\ntemplate<class T>\r\nT* bufferAs(JNIEnv* env, jobject buffer) {\r\n  void* buf = env->GetDirectBufferAddress(buffer);\r\n  if (buf == nullptr) {\r\n    throwException(env, JEX_ILLEGAL_ARGUMENT, \"Cannot access the buffer\");\r\n    return nullptr;\r\n  }\r\n  return reinterpret_cast<T*>(buf);\r\n}\r\n```\r\n\r\nI'll experiment some more when I have time for it."]}, {"number": 30392, "title": "[Intel MKL] Replace redundant attribute function with a generic function.", "body": "In MKL layout pass, use an iterator to get all attributes from original node, and copy them to the node builder instead of specific function. It helps to remove redundant code.\r\nTF will throw an error if the two nodes have different attributes, so it can be easily found when copied wrong attributes.\r\n\r\nmodified:\r\n- tensorflow/core/graph/mkl_layout_pass.cc\r\n\r\nSigned-off-by: Lu Teng teng.lu@intel.com", "comments": ["@Zantares Could you please resolve the conflicts? Thanks!", "@Zantares Could you please help resolve the conflicts? Thank you!", "> @Zantares Could you please help resolve the conflicts? Thank you!\r\n\r\nSorry for the delay, I was on a vacation few days ago. I have merged the master to this branch and fixed the conflict, please take a review again, thanks!", "This PR helps me to find redundant attributes in UT,  I removed them and made a new commit. "]}, {"number": 30391, "title": "Removed the Deprecated API from entire Keras Module", "body": "", "comments": ["Closing the duplicate PR"]}, {"number": 30390, "title": "run bazel failed:FATAL: ExecuteProgram(C:\\Users\\Administrator/_bazel_Administrator/install/9470fb600225157f1e34c45c6d4dc834/_embedded_binaries/embedded_tools/jdk/bin/java.exe) failed: 6", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no\r\n- TensorFlow installed from (source or binary):source\r\n- TensorFlow version: 2.0\r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?:conda\r\n- Bazel version (if compiling from source): 0.27\r\n- GCC/Compiler version (if compiling from source):no\r\n- CUDA/cuDNN version:no\r\n- GPU model and memory:no\r\n\r\n\r\n\r\n**Describe the problem**\r\nAfter I installed bazel 0.27.1, I try to run the command \"bazel\", it show the error\r\n\"FATAL: ExecuteProgram(C:\\Users\\Administrator/_bazel_Administrator/install/9470fb600225157f1e34c45c6d4dc834/_embedded_binaries/embedded_tools/jdk/bin/java.exe) failed: 6\"\r\nbut when move into the path \"C:\\Users\\Administrator/_bazel_Administrator/install/9470fb600225157f1e34c45c6d4dc834/_embedded_binaries/embedded_tools/jdk/bin/\", it is ok to run \"bazel\", do you have idea on it? and how to solve it.\r\nThanks,\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@cj741 Just to verify, did you follow the steps mentioned in official [Tensorflow](https://www.tensorflow.org/install/source_windows) Website and did bazel installed properly? Thanks!", "@gadagashwini  Thanks for your response, I have try to install bazel from Chocolatey and download the Bazel binary (bazel-<version>-windows-x86_64.exe), both of them are still have the same error.", "Could you downgrade Bazel to 0.26.0 as that is the max version allowed. Let us know if this resolves your problem. Thanks!", "I have tried Bazel 0.23.0, since on the stackoverflow, someone say the higher versions are not work well with tensorflow, but still got the same problem.", "From what I can see, this is a bazel bug.\r\nPlease reach out to bazel team.", "ok, I send the problem to the bazel team, thanks~", "@cj741 Can you please let us know if you are happy to close this issue. Thanks!"]}, {"number": 30389, "title": "The GPU is not used after installed from source. ", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nUbuntu 16.04.3 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\nbuild from source\r\n- TensorFlow version:\r\n1.14\r\n- Python version:\r\n2.7\r\n- Installed using virtualenv? pip? conda?:\r\npip\r\n- Bazel version (if compiling from source):\r\n0.24\r\n- GCC/Compiler version (if compiling from source):\r\ngcc 5.4\r\n- CUDA/cuDNN version:\r\nCUDA 9.0 cuDNN 7.0\r\n- GPU model and memory:\r\nTesla K80\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\n`bazel build -c opt --config=cuda --config=mkl --config=nogcp --config=nohdfs --config=nokafka --config=noignite --config=nonccl //tensorflow/tools/pip_package:build_pip_package\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg\r\npip install /tmp/tensorflow_pkg/tensorflow-1.14.0-cp27-cp27mu-linux_x86_64.whl\r\n`\r\n\r\nAfter installing, I am trying to use the GPU to traning, get such error.\r\n\r\n`2019-07-04 03:30:19.903538: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.9.0'; dlerror: /usr/local/cuda/lib64/libcusolver.so.9.0: undefined symbol: GOMP_critical_end; LD_LIBRARY_PATH: /usr/lib64/openmpi/lib/:/usr/local/cuda/lib64:/usr/local/lib:/usr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/mpi/lib:/lib/:`\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "comments": ["find the reason. the tf 1.14 need cuda 10.", "I have the same issue and it looks like the problem is much deeper. The same error appears when I build any TF 2.0 version with CUDA 9.0 support", "So, TensorFlow 1.14 does not support CUDA 9?", "@njzjz I tried building versions `1.13, 1.14.0, 1.14.2, 2.0.0beta0, 2.0.0beta1` on multiple machines but always got the same error `Could not dlopen library 'libcusolver.so.9.0'; dlerror: /usr/local/cuda/lib64/libcusolver.so.9.0: undefined symbol: GOMP_critical_end; ` \r\nI'm not really experienced in building things from source so I was unable to solve this problem", "just compiled TF `1.12.0` - everything works fine", "rollback to 1.12.0 (CUDA 9.0), everything is ok now.", "@jdxyw  but it doesn't solve the original issue. Compiling `TF>=1.13` from source for CUDA 9.0 doesn't work. ", "meet the same issues , how to solve it ?", "@liyi193328 check the issue 30753 above. There is a solution there", "@bonlime Got it, very tricky. Thanks a lot", "It seems if you have pytorch 1.1.0, and if you import torch before tensorflow, this problems does not appear and tf2.0 (build with cuda9) can work. I dont know why, but on my machine it works. Could anyone tell my the reason?"]}, {"number": 30388, "title": "tensorflow2.0 detected 'xla_gpu' , but 'gpu' expected", "body": "**System information**\r\n-  Linux Ubuntu 16.04 :\r\n- TensorFlow installed from binary, tensorflow2.0-gpu-beta1:\r\n- Python version 3.6.4:\r\n- CUDA 9.1, cuDNN 7.0:\r\n- GPU: Titan Xp.\r\n \r\ntensorflow detected 'xla_gpu' , but 'gpu' expected\r\n\r\n`train_model = keras.utils.multi_gpu_model(train_model, gpus=2)\r\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/utils/multi_gpu_utils.py\", line 182, in multi_gpu_model\r\n    available_devices))\r\nValueError: To call `multi_gpu_model` with `gpus=2`, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1']. However this machine only has: ['/cpu:0', '/xla_gpu:0', '/xla_gpu:1', '/xla_cpu:0']. Try reducing `gpus`.\r\n`  ", "comments": ["In tf2.0, using [distribute_strategy](https://www.tensorflow.org/beta/guide/distribute_strategy) to utilize multiple gpus is a better way.", "> In tf2.0, using [distribute_strategy](https://www.tensorflow.org/beta/guide/distribute_strategy) to utilize multiple gpus is a better way.\r\n\r\nThank you , but \r\nif I use \r\n`    mirrored_strategy = tf.distribute.MirroredStrategy(\r\n        devices=[\"/gpu:0\", \"/gpu:1\"])`\r\nthen:\r\n\r\nRuntimeError: Error copying tensor to device: /job:localhost/replica:0/task:0/device:GPU:0. /job:localhost/replica:0/task:0/device:GPU:0 unknown device.\r\n\r\nif I use `    mirrored_strategy = tf.distribute.MirroredStrategy(\r\n        devices=[\"/xla_gpu:0\", \"/xla_gpu:1\"])`\r\nthen:\r\nValueError: Unknown attribute: 'xla_gpu' in '/xla_gpu:0'\r\n", "seems that you are using a XLA compiled tf build. Not sure if it's a bug. Need some confirmation from the tf team members.", "problem solved, after update CUDA to 10.0, CUDNN to 7.4 ", "Does this problem solved for CUDA 10.0 and CUDNN 7.4\r\n\r\nI have installed CUDA 10.1 and CUDNN 7.5, it does not work.\r\n```\r\ntf.distribute.MirroredStrategy( devices=[\"/xla_gpu:0\", \"/xla_gpu:1\"])\r\nValueError: Unknown attribute: 'xla_gpu' in '/xla_gpu:0'\r\n``` \r\n\r\nsame error for Tensorflow Version 2.0", "Exact same issue for me using\r\nCUDA 10.2\r\ncuDNN 7.6\r\nTensorflow 2.0\r\nUbuntu 18.04\r\nIt might be a compatibility issue with the latest CUDA and cuDNN with tf? Not sure.", "Any news concerning this issue ? ", "@mbenguig from the sound of all the reading I've done, the latest version of tf has some bugs in it and doesn't work properly? Honestly, until they can fix some of these issues I've given up on tensorflow roulette and am piping some of my work to pytorch to try my luck there.", "Finally, you can get rid of this issue by uninstalling / reinstalling (tested on Ubuntu 18.04):\r\n\r\n- Tensorflow 2.0\r\n- CUDA 10.0\r\n- cuDNN 7.6.4 (described as dedicated for CUDA 10.0)\r\n\r\nhttps://www.tensorflow.org/install/source#tested_build_configurations. You will get xla devices with corresponding non xla devices.\r\n", "same issue\r\nCUDA 10.2\r\ncuDNN 7.6\r\nTensorflow 1.15\r\nUbuntu 18.04", "Same.\r\nCUDA 10.1\r\nCuDNN 7.6\r\nTensorflow 2.1.0", "Same issue here:\r\nCUDA 10.1\r\nCuDNN 7.6\r\nTensorflow 1.15\r\n\r\n-----------\r\nDowngraded to (which worked):\r\nCUDA 10.0\r\nCuDNN 7.4\r\nTensorflow 1.15", "Same.\r\nCUDA 10.1\r\nCuDNN 7.6\r\nTensorflow 2.1.0", "Same.\r\nCUDA 10.2\r\nCuDNN 7.6.5.32\r\nTensorflow 2.1.0", "Same:\r\n\r\n* CUDA 11.0.1 RC\r\n* cuDNN 8.0.2 RC2\r\n* TensorFlow 2.2.0", " Hi,\r\n\r\nI had the same issue and I think I figured out a way around it. In my case, I am working on an HPC and I intalled keras on my /.local, whereas Tensorflow and CUDA are installed by the IT staff, anyway I encountered the same error above. I am using **Tensorflow==1.15.0** and **Keras==2.3.1**\r\n\r\nI noticed that the message error:\r\n- ValueError: To call `multi_gpu_model` with `gpus=2`, we expect the following devices to be available: ['/cpu:0', '/gpu:0', '/gpu:1'].\r\nHowever this machine only has: ['/cpu:0', '/xla_cpu:0', '/xla_gpu:0', '/xla_gpu:1']. Try reducing `gpus`.\r\n\r\nis located in the following file of keras, line 184:\r\n- /home/.local/lib/python3.7/site-packages/keras/utils/multi_gpu_utils.py\r\n\r\nI solved this, by replacing the **line 175** with the following:\r\n- target_devices = ['/cpu:0'] + ['/gpu:%d' % i for i in target_gpu_ids] **(before)**\r\ntarget_devices = ['/cpu:0'] + ['/xla_gpu:%d' % i for i in target_gpu_ids] **(after)**\r\n\r\nMoreover, I modified the following keras file:\r\n- /home/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\r\n\r\nso I replaced the line 510 with:\r\n- return [x for x in _LOCAL_DEVICES if 'device:gpu' in x.lower()] **(before)**\r\nreturn [x for x in _LOCAL_DEVICES if 'device:XLA_GPU' in x] **(after)**\r\n\r\nLong story short only to say that apparently this is a bug of Keras and not of some environmental setup. \r\n\r\nAfter such modification my network was able to run with the xla_gpus, I hope this is somehow helpful.\r\n\r\n\r\nCheers,\r\nMichele", "@micbia \r\nTried your solution, but for me the files .../multi_gpu_utils.py and .../tensorflow_backend.py don't even exist.\r\nI have tensorflow 2.3.1 though.\r\nIn general, is there any solution to tensorflow/keras being unable to use XLA-GPU that does not involve reinstalling CUDA+tf+cudnn?", "@arb-git \r\nnotice that path for the files I mentioned above are for **keras** module, If you are not using keras then I can suggest you to use the tf.keras. \r\n\r\nThe location of the multi_gpu_utils.py module for TensorFlow2.3.0 (stable) is basically the same [tf.keras.utils.multi_gpu_model](https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model), and the [line 172](https://github.com/tensorflow/tensorflow/blob/fcc4b966f1265f466e82617020af93670141b009/tensorflow/python/keras/utils/multi_gpu_utils.py#L172) is to change as I suggested\r\n\r\nIn the case of tensorflow_backend.py it changes a little bit [tf.keras.backend.backend](https://www.tensorflow.org/api_docs/python/tf/keras/backend/backend) and the [line 783](https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/backend.py#L783) is to change, but by replacing:\r\n-   return [x.name for x in _LOCAL_DEVICES if x.device_type == 'GPU'] (before)\r\n-   return [x.name for x in _LOCAL_DEVICES if x.device_type == 'XLA_GPU'] (after)\r\n\r\nI have not tried personally on TensorFlow2.3.0. It would be interesting if you could share at let us know if it works in your case with this changes.\r\n\r\nIn my case I was using the module environment of an HPC, so I couldn't reinstalling CUDA, tf and cudnn. But online I have found very little information about this problem.\r\n\r\nCheers,\r\nMichele\r\n\r\n\r\n"]}, {"number": 30387, "title": "TensorFlow Lite Metadata samples", "body": "Are there any sample files or documentation how `metadata` and `metadata_buffer` in TensorFlow Lite models is used? @wangtz 494ed4d2100d94b3cca782d7134eac306d79d59b", "comments": ["@lutzroeder \r\n\r\nNope, not really.  We are using it for user-defined info at the moment.  For example, how to normalize (it's often hard to tell if you have a model with float input... should it be 0-255.f or -1f~1f or 0-1f etc.)  or how to interpret output (which of these 400 points corresponds to eyes, nose, lips etc. in a face mesh model).  All of these depends on the model and its author.  We don't have standardized methods to read from or write to it."]}, {"number": 30386, "title": "[INTEL MKL] Enabling MKL support for FusedBatchNormV3", "body": "Adding Intel MKL support for FusedBatchNormV3", "comments": ["Made changes to the PR as suggested!\r\nThanks\r\n\r\n"]}, {"number": 30385, "title": " Extend the cache filename in CachaDatasetOp::FileDataset", "body": "This PR tries to make the cache filename more unique. It will update the input cache filename with the node name and iterator prefix, which will be similar to the resource [name](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/cache_dataset_ops.cc#L699-L700) used in `CacheDatasetOp::MemoryDataset`.  With the updated cache filename, it can avoid the filename conflicts after the graph is rewritten, such as the cases mentioned in #30105. \r\n\r\ncc: @jsimsa ", "comments": ["@feihugis Can you please resolve conflicts? Thanks!", "@gbaned Thanks for the remind! The conflicts have been resolved.\r\n\r\nBTW, I did not request the reviewers. Does GitHub automatically request reviewers?", "@jsimsa Thanks for looking into this PR. It makes sense. Close this PR."]}, {"number": 30384, "title": "Problem with keras saving when using custom loss in compile (problem in custom_objects parameter passing)", "body": "**System information**\r\n\r\nSystem: windows 10, wsl with ubuntu 18 LTS\r\nTensorflow Version: 2.0.0b1 in CPU mode (default, installed from pip)\r\nPython version: 3.6.8\r\n\r\nIt also happens in real linux environments (actually, it's easy to simulate each these errors)\r\n\r\n**Describe the current behavior**\r\n\r\nThe code can't handle custom losses (added with `.compile`) when loading a model\r\n\r\n**Describe the expected behavior**\r\n\r\nWhen I use a custom loss and compile it, tensorflow returns the following error:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/teste.py\", line 27, in <module>\r\n    tf.keras.models.load_model('model.keras.tf', custom_objects={'null_fn': null_fn})\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 141, in load_model\r\n    return saved_model.load_from_saved_model_v2(filepath, compile)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 1225, in load_from_saved_model_v2\r\n    model._training_config))  # pylint: disable=protected-access\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 458, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 300, in compile\r\n    self.loss, self.output_names)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 1240, in prepare_loss_functions\r\n    loss_functions = [get_loss_function(loss) for _ in output_names]\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 1240, in <listcomp>\r\n    loss_functions = [get_loss_function(loss) for _ in output_names]\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 1029, in get_loss_function\r\n    loss_fn = losses.get(loss)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py\", line 1122, in get\r\n    return deserialize(identifier)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py\", line 1113, in deserialize\r\n    printable_module_name='loss function')\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\", line 211, in deserialize_keras_object\r\n    raise ValueError('Unknown ' + printable_module_name + ':' + object_name)\r\nValueError: Unknown loss function:null_fn\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\ninp = tf.keras.Input(batch_size=8, shape=(32, 32, 3))\r\ntensor = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3))(inp)\r\nmodel = tf.keras.Model(inputs=inp, outputs=tensor)\r\ndef null_fn(y_true, y_pred):\r\n    return tf.constant(0.)\r\n\r\nmodel.compile('adam',loss=null_fn)\r\nmodel.save('model.keras.tf', save_format='tf')\r\ntf.keras.models.load_model('model.keras.tf', custom_objects={'null_fn': null_fn})\r\n```\r\n\r\n**Other info / logs**\r\n\r\nIn   File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\", line 1240, function `prepare_loss_functions`, the custom_objects are not passed as parameter (actually the compile does not accept it currently).\r\n\r\nConsisdering the stack, in the two latest files, there are generic keras functions which receive `custom_objects` params. But as they were not passed in the other functions, they are not treated correctly there. So it seems a real bug concerning custom losses.\r\n\r\nThe keras `.compile` uses the same stack of code, but in that situation, when tryining to get the corresponding function we have a callable. When loading, we have dict.\r\n\r\n File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/losses.py\", line 1117\r\n\r\n``` python\r\n@keras_export('keras.losses.get')\r\ndef get(identifier):\r\n  if identifier is None:\r\n    return None\r\n  if isinstance(identifier, six.string_types):\r\n    identifier = str(identifier)\r\n    return deserialize(identifier)\r\n  if isinstance(identifier, dict):\r\n    return deserialize(identifier)\r\n  elif callable(identifier):\r\n    return identifier\r\n  else:\r\n    raise ValueError('Could not interpret '\r\n                     'loss function identifier:', identifier)\r\n```\r\n\r\nSo, there's a need of passing the `custom_objects` param to the `deserialize` method somehow.", "comments": ["Digging a bit into the keras [code](https://github.com/tensorflow/tensorflow/blob/2a705b9d1524a856cd4c36a53629f25de97aba65/tensorflow/python/keras/saving/saved_model/load.py#L63) it seems this is still not implemented: \r\n```\r\ndef load(path, compile=True):  # pylint: disable=redefined-builtin\r\n  \"\"\"Loads Keras objects from a SavedModel.\r\n  Any Keras layer or model saved to the SavedModel will be loaded back\r\n  as Keras objects. Other objects are loaded as regular trackable objects (same\r\n  as `tf.saved_model.load`).\r\n  Currently, Keras saving/loading only retains the Keras object's weights,\r\n  losses, and call function.\r\n  The loaded model can be re-compiled, but the original optimizer, compiled loss\r\n  functions, and metrics are not retained. This is temporary, and `model.save`\r\n  will soon be able to serialize compiled models.\r\n  Args:\r\n    path: Path to SavedModel.\r\n    compile: If true, compile the model after loading it.\r\n  Returns:\r\n    Object loaded from SavedModel.\r\n  \"\"\"\r\n  # TODO(kathywu): Add saving/loading of optimizer, compiled losses and metrics.\r\n  # TODO(kathywu): Add code to load from objects that contain all endpoints\r\n```", "@nguerinjr I can reproduce the issue with `!pip install tf-nightly-2.0-preview`. Please take a look at the [gist here](https://colab.sandbox.google.com/gist/jvishnuvardhan/56079fe0a7ddb24d432d1c8bed89a6e3/tf_30384_tf.ipynb). However, this is not an issue if you use *.h5 format. Please take a look at this [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/bcb082252f19bbb6b71899e1cb35a44c/tf_30384.ipynb). Thanks!", "@nguerinjr When you have any custom_objects, there are couple of ways you can make it work.\r\n1. create_model -->save the model --> compile -->load the model--> compile the loaded model with the custom_objects.\r\n2. create_model --> compile --> save the model -->load the model--> compile the loaded model with the custom_objects.\r\n\r\nCustom functions are not serializable as they are not compatible. You could also get around this by defining  a custom Metric/loss object, and overwriting the get_config() . Check the [link](https://www.tensorflow.org/beta/guide/keras/training_and_evaluation) for more details. \r\n\r\nThere is another [similar issue](https://github.com/tensorflow/tensorflow/issues/32612) that we will use to track the progress in saving model with custom_objects. Thanks!\r\n\r\nI am closing this issue.  Please feel free to open it if I am mistaken. Thanks!\r\n\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30384\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30384\">No</a>\n"]}, {"number": 30383, "title": "intel-tensorflow (MKL) throws \"could not initialize a memory descriptor\" (CPU, GPU work fine)", "body": "\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat Enterprise 7.6\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/a\r\n\r\n**Describe the current behavior**\r\n\r\nTensorflow with MKL-DNN (intel-tensorflow) throws exception \"could not initialize a memory descriptor, in file tensorflow/core/kernels/mkl_concat_op.cc:380\"\r\n\r\n**Describe the expected behavior**\r\n\r\nNo exception thrown.\r\n\r\n**Code to reproduce the issue**\r\n\r\npip install intel-tensorflow\r\ntar xvzf testcase_2367.tar.gz # attached\r\ncd testcase_2367\r\npython testcase_2367.py\r\n\r\n**Other info / logs**\r\n\r\nThis inference network runs fine on tensorflow CPU and tensorflow GPU. Only MKL-DNN tensorflow fails.\r\n\r\nYes this is similar to issue #23145, but it is definitely not fixed in r1.13.1.\r\n\r\nIt is also not fixed in r1.14, which I confirmed by compiling from source (although the line number changes).\r\n\r\n[testcase_2367.tar.gz](https://github.com/tensorflow/tensorflow/files/3357086/testcase_2367.tar.gz)\r\n", "comments": ["It seems mkldnn failed to create the memory description :\r\n```\r\n 462 dst_md = memory::desc(dst_dims_in_nchw, MklDnnType<T>(), mkl_common_format);\r\n```\r\n", "Further debug shows that: the value of one input parameter(**mkl_common_format**) to create mkldnn memory descriptor  is **mkldnn_blocked**\r\nWhich makes the construction of md failed and report the above error message.\r\n\r\n", "The memory format(blocked) of this op(concat) is inherited from the input tensor:\r\n```\r\n      return static_cast<memory::format>(\r\n          input_shapes[0].GetMklLayout().data.format);\r\n```\r\nas for the input_tensor, in this case the input op is _MklSlice\r\ncheck the function **ComputeMklSlice**\r\nit do create the memory in blocked format.\r\nMaybe that's the problem.\r\n\r\n\r\n", "@plopresti \r\nFind a work around to fix the problem, and get the result with your script:\r\noutMax_max=-0.4230261, outMax_min=-0.4230261\r\nIs the result make sense?", "@Leslie-Fang Could you elaborate about your work-around?\r\n\r\n(And do you think this is a bug in MKL-DNN, TensorFlow, or our script/network?)\r\n\r\nThanks!", "Should this be assigned to @Intel-tensorflow ?", "Paging @agramesh1", "@plopresti We will look into this issue and get back to you soon.", "@nhasabni: Any update on this?\r\n\r\nCan/should I open an issue against MKL-DNN?\r\n\r\nLet me know if there is anything else I can do.\r\n\r\nThanks!", "@plopresti  Code fixing the issue is coming up soon. It is not an issue of MKL-DNN. We are fixing it inside TensorFlow. Please stay tuned for the next few days. Thanks for reporting and trying TensorFlow on Intel architectures!", "@plopresti I believe the patch has been merged to the master branch now. Could you have a try?", "this fix not in master as of yet, it's available in the 1.15 branch. Can you please try in this branch?", "@preethivenkatesh @plopresti Master has the merged code https://github.com/tensorflow/tensorflow/commit/15bd4863bd63968ad0396493d2c41bd0e5d8390b   as a result of merging https://github.com/tensorflow/tensorflow/pull/31777", "Confirmed that we can now run inference on this case and others we are using.\r\n\r\nThank you!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30383\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30383\">No</a>\n"]}, {"number": 30382, "title": "[XLA:GPU][ROCm] adding a DISABLED_ON_GPU_ROCM macro ", "body": "adding a DISABLED_ON_GPU_ROCM macro to disable subtests that are not yet supported on ROCm.\r\n\r\nApplying that macro to a few subtests in convolution_test.cc and convolution_variants_test.cc", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30382) for more info**.\n\n<!-- need_author_consent -->", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30382) for more info**.\n\n<!-- ok -->", "@thomasjoerg Thanks for the review. After checking with @deven-amd and @jerryyin I've revised this PR to reflect the latest status on ROCm, where 3D convolution on 5D tensors are actually supported.", "@thomasjoerg a gentle ping."]}, {"number": 30381, "title": "[XLA:GPU][ROCm] Change XLA platform_util to support both CUDA/NVPTX and ROCm/AMDGPU", "body": "Change XLA platform_util to support both CUDA/NVPTX and ROCm/AMDGPU", "comments": ["@thomasjoerg a gentle ping.", "Can one of the admins verify this patch?", "Anything I can help with here?", "@cheshire from my side I'm seeing all the check marks and the failure in \"Android Demo App\" should have nothing to do with this PR. is there anything i can help to get the PR merged please let me know.", "Let's try again, because other CLs do go through.", "Flaky test failed the build this time. Let's retry.", "I'm really not sure what the failures are caused by, seems to be fairly consistent. Do you want to try to rebase?", "Looking at the failure in \"Android Demo App\" target I don't really believe it has anything to do with this PR.", "I think it went through this time, let's see whether it passes internal tests."]}, {"number": 30380, "title": "Improve Flatten to avoid using dynamic shapes when possible", "body": "Flatten currently creates a reshape which introduces a dependency on the size of the batch dimension. In the cases where flatten is commonly used, the batch dimension is usually not known statically but the rest of the dimensions are. \r\nThis means that the constant folding grappler pass cannot resolve the shape ahead of time. This also makes it difficult to convert using TF-TRT.\r\n\r\nThe implementation of Flatten can be rewritten such that there is no dependency on an unknown batch dimension.\r\n\r\nPrevious implementation (simplification):\r\n```\r\ndef flatten(x):\r\n  return tf.reshape(x, [tf.shape(x)[0], -1])\r\n```\r\nThis implementation:\r\n```\r\ndef flatten(x):\r\n  if all(x.shape[1:]):\r\n    return tf.reshape(x, shape=(-1, prod(x.shape[1:]))\r\n  else:\r\n    return tf.reshape(x, shape=(tf.shape(x)[0], -1))\r\n```\r\n\r\nFixes #25402", "comments": ["@ebrevdo gentle ping to review PR..!", "@ebrevdo   Can you take a look or let us know if we should reassign?", "I added a label here just in case no one from keras is assigned to this.", "Actually; I'd first check that shape[0].is_fully_defined() because that's a\nfaster check than shape[1:].is_fully_defined().  swap the if/else\ncomponents.\n\nOn Wed, Jul 17, 2019 at 2:56 PM tanzhenyu <notifications@github.com> wrote:\n\n> @tanzhenyu <https://github.com/tanzhenyu> requested your review on: #30380\n> <https://github.com/tensorflow/tensorflow/pull/30380> Improve Flatten to\n> avoid using dynamic shapes when possible.\n>\n> \u2014\n> You are receiving this because your review was requested.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/30380?email_source=notifications&email_token=AANWFGYQL7426WKJZNM4WUTP76ISNA5CNFSM4H5MUUGKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOSR4VZWY#event-2490981595>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AANWFG2JRNIUPRGRN4IIWOTP76ISNANCNFSM4H5MUUGA>\n> .\n>\n", "> Actually; I'd first check that shape[0].is_fully_defined() because that's a faster check than shape[1:].is_fully_defined(). swap the if/else components.\r\n\r\nBut shape[0] being undefined doesn't mean than shape[1:] will be fully defined. We can only do this optimization when shape[1:] is fully defined.", "@ebrevdo would you please help to take a look at this PR? Thanks.", "I see failures like [this one](https://source.cloud.google.com/results/invocations/1895d420-b447-4d78-b679-0f421227eb03/targets/%2F%2Ftensorflow%2Fpython:layers_core_test/tests).  Probably what's happening is you are passing an np.prod() on an empty array is an empty float32 array.  you need to cast the result to int64, say, for that special case.", "> I see failures like [this one](https://source.cloud.google.com/results/invocations/1895d420-b447-4d78-b679-0f421227eb03/targets/%2F%2Ftensorflow%2Fpython:layers_core_test/tests). Probably what's happening is you are passing an np.prod() on an empty array is an empty float32 array. you need to cast the result to int64, say, for that special case.\r\n\r\nThanks @ebrevdo, I had fixed those tests in [this commit](https://github.com/tensorflow/tensorflow/pull/30380/commits/2f61f75e244891e9ce1d10fa3a34fd4cb419a5d4) by passing new dim value to `int()`. Is this an acceptable solution? Would using the `dtype` param of `np.prod` be better?", "We had to roll this PR back, likely due to an integer overflow in the reshape op:\r\n\r\n```python\r\ninputs = tf.zeros(shape=[8, 21316, 21316, 80])\r\ntf.reshape(inputs,[-1, 36349748480])\r\n`\r\ngives an error:\r\n\"Dimension size must be evenly divisible by 1990010112 but is 290797987840 for 'Reshape_2' (op: 'Reshape') with input shapes: [8,21316,21316,80], [2] and with input tensors computed as partial shapes: input[1] = [?,1990010112].\"\r\n\r\nNote that 1990010112 = np.int32(36349748480).\r\n```\r\n\r\nLooks like somewhere in ReshapeV2 we're not handling the int64 case properly.\r\n\r\nProbably we can ", "Actually the bug looks like it's somewhere in the shape inference code.  Trying to track it down.", "Can you resubmit with int64 casting?  e.g., replace the shape tuple with a casted version:\r\n\r\n```\r\nmath_ops.cast((-1, flattened_dim), dtypes.int64)\r\n```", "(only cast if flattened_dim is > np.iinfo(np.int32).max)", "[this would be a temporary solution, we're working on an internal bugfix since the bug seems deeper than just your use case]\r\n\r\nalso instead of math_ops.cast, use constant_op.constant()", "Thanks @ebrevdo for investigating that bug! Here's a new PR with your requested changes: https://github.com/tensorflow/tensorflow/pull/31450"]}, {"number": 30379, "title": "[XLA:GPU][ROCm] Register ROCm platform on XLA computation placer", "body": "Register ROCm platform on XLA computation placer", "comments": []}, {"number": 30378, "title": "Problems with keras model saving when there's a loss added with add_loss", "body": "**System information**\r\n\r\nSystem: windows 10, wsl with ubuntu 18 LTS\r\nTensorflow Version: 2.0.0b1 in CPU mode (default, installed from pip)\r\nPython version: 3.6.8\r\n\r\nIt also happens in real linux environments (actually, it's easy to simulate each these errors)\r\n\r\n**Describe the current behavior**\r\n\r\nI'm having many problems when saving/loading keras models with custom loss (added with add_loss). I'll describe each one of the scenarios below (I think they're all related)\r\n\r\n**Code to reproduce the issue**\r\n\r\n``` python\r\ninp = tf.keras.Input(batch_size=32, shape=(32, 32, 3))\r\ntensor = tf.keras.layers.Conv2D(filters=16, kernel_size=3)(inp)\r\nmodel = tf.keras.Model(inputs=inp, outputs=tensor)\r\nmodel.add_loss(tf.keras.losses.mean_absolute_error(tensor, tensor + 1))\r\nmodel.compile('adam')\r\ntf.keras.experimental.export_saved_model(model, 'model.tf')\r\n```\r\n\r\n> **When not using a keras layer as loss, it produces a non-valid JSON**:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/teste.py\", line 8, in <module>\r\n    tf.keras.experimental.export_saved_model(model, 'model.tf')\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 169, in export_saved_model\r\n    _export_model_json(model, saved_model_path)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 177, in _export_model_json\r\n    model_json = model.to_json()\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1449, in to_json\r\n    model_config, default=serialization.get_json_type, **kwargs)\r\n  File \"/usr/lib/python3.7/json/__init__.py\", line 238, in dumps\r\n    **kw).encode(obj)\r\n  File \"/usr/lib/python3.7/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.7/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/util/serialization.py\", line 69, in get_json_type\r\n    raise TypeError('Not JSON Serializable:', obj)\r\nTypeError: ('Not JSON Serializable:', b'\\n\\x03add\\x12\\x03Add\\x1a\\x0fconv2d/Identity\\x1a\\x05add/y*\\x07\\n\\x01T\\x12\\x020\\x01')\r\n\r\nNow, a code that uses keras layers\r\n\r\n**Code to reproduce the issue**\r\n\r\n``` python\r\ninp = tf.keras.Input(batch_size=32, shape=(32, 32, 3))\r\ntensor = tf.keras.layers.Conv2D(filters=16, kernel_size=3)(inp)\r\nmodel = tf.keras.Model(inputs=inp, outputs=tensor)\r\nlbd = tf.keras.layers.Lambda(lambda i: tf.keras.losses.mean_absolute_error(i[0], i[1]))\r\nmodel.add_loss(lbd([tensor, tensor + 1]))\r\nmodel.compile('adam')\r\ntf.keras.experimental.export_saved_model(model, 'model.tf')\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/teste.py\", line 16, in <module>\r\n    tf.keras.experimental.export_saved_model(model, 'model.tf')\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 166, in export_saved_model\r\n    input_signature)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 236, in _save_v1_format\r\n    _export_mode(mode_keys.ModeKeys.TRAIN, has_saved_vars, **export_args)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 299, in _export_mode\r\n    compile_clone=compile_clone)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py\", line 538, in clone_and_build_model\r\n    clone = clone_model(model, input_tensors=input_tensors)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py\", line 326, in clone_model\r\n    model, input_tensors=input_tensors, layer_fn=clone_function)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py\", line 202, in _clone_functional_model\r\n    model._insert_layers(ancillary_layers, relevant_nodes=relevant_nodes)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 1633, in _insert_layers\r\n    for node in layer.inbound_nodes\r\nValueError: min() arg is an empty sequence\r\n\r\n> **When using a keras layer, it loses their inbound_nodes (I've debugged it). It puts them apart from other layers, but loses information of the objects (it's not a question of passing or not `custom_objects` as param).**\r\n\r\nNow, trying to use non-experimental saves/loads.\r\n\r\n**Code to reproduce the issue**\r\n\r\n``` python\r\ninp = tf.keras.Input(batch_size=8, shape=(32, 32, 3))\r\ntensor = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3))(inp)\r\nmodel = tf.keras.Model(inputs=inp, outputs=tensor)\r\nlbd = tf.keras.layers.Lambda(lambda i: tf.keras.losses.mean_absolute_error(i[0], i[1]))\r\nmodel.add_loss(lbd([tensor, tensor + 1]))\r\nmodel.compile('adam')\r\nmodel.save('model.keras.tf', save_format='tf')\r\ntf.keras.models.load_model('model.keras.tf', custom_objects={'lambda': lbd})\r\n```\r\n\r\n> **A first annoying thing is that it's based on .ext. Even though it only saves in tf2 (put any extension), in load it verifies these extensions. It not a clear way of working in my opinion. Maybe some additional information on the files saved could make it not use the extensions.**\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/teste.py\", line 26, in <module>\r\n    tf.keras.models.load_model('model.keras.tf', custom_objects={'lambda': lbd})\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\", line 141, in load_model\r\n    return saved_model.load_from_saved_model_v2(filepath, compile)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py\", line 1225, in load_from_saved_model_v2\r\n    model._training_config))  # pylint: disable=protected-access\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 458, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 337, in compile\r\n    self._compile_weights_loss_and_weighted_metrics()\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\", line 458, in _method_wrapper\r\n    result = method(self, *args, **kwargs)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1494, in _compile_weights_loss_and_weighted_metrics\r\n    self.total_loss = self._prepare_total_loss(masks)\r\n  File \"/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1595, in _prepare_total_loss\r\n    raise ValueError('The model cannot be compiled '\r\nValueError: The model cannot be compiled because it has no loss to optimize.\r\n\r\n\r\n> **The second is related to the loading, is that it does not accept no loss to compile. Keras accepts it (you can see in the examples). It accepts because it accounts for the non-default losses added. This is another saving/loading bug.**\r\n\r\nBoth experimental and non-experimental functions seem to have some problems in saving / loading. I think if you simulate some custom scenarios with keras, you'll find a bunch of other errors. So, it's worth to take a whole look at them (specially considering that Keras is the default prototyping tool in tf2).\r\n\r\nIn the current situation, i've not thought of a simple and easy way to save keras models with custom components (those which are not in the list of arguments of compile)", "comments": ["Hi @nguerinjr,\r\n\r\nThanks for creating this issue. I've also struggled with this, but it's possible: https://github.com/tensorflow/tensorflow/commit/1ad6aae48d97937d7caed2627a1ff1f0889b2cc7.\r\n\r\nThe trick is here: https://github.com/tensorflow/tensorflow/blob/1ad6aae48d97937d7caed2627a1ff1f0889b2cc7/tensorflow/python/keras/saving/hdf5_format_test.py#L821\r\n\r\nto connect the loss calculation with the rest of the network. It's not ideal, I know. There really should be an auxiliary outputs type argument in Model, or add_loss should retrace graph, but the workaround isn't terrible in this case.\r\n\r\nHope this helps.\r\n", "@nguerinjr ,\r\nCan you please confirm if @ppham27's workaround is working for you.", "Hi @ppham27, @rmothukuru ,\r\n\r\nYes, that's working in the scenario of the codes I've put here. \r\n\r\nI'll implement this workaround in my real code, where I have a bunch of custom_objects. \r\n\r\nAs you've mentioned, @ppham27, it's not ideal. Since there are those problems in the code, it's a good deal to make things work this way.\r\n\r\nThanks!", "Great for what it's worth, I have a pending internal change that will trace the history when using `add_loss` and `add_metric`, so you don't need that line. Ideally, it will land at the end of next week, but I can't say for certain since I'm not actually on the TensorFlow team.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30378\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30378\">No</a>\n"]}, {"number": 30377, "title": "[XLA:GPU][ROCm] Emit llvm::Intrinsic::nvvm_atomic_load_add_f32 only on NVPTX", "body": "Emit llvm::Intrinsic::nvvm_atomic_load_add_f32 only on NVPTX", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored or co-authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of all the commit author(s), set the `cla` label to `yes` (if enabled on your project), and then merge this pull request when appropriate.*\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30377) for more info**.\n\n<!-- need_author_consent -->", ":frowning_face: Sorry, but only Googlers may change the label `cla: yes`.", "@thomasjoerg I've address your comments in this PR. Could you help re-review again? Thanks. Also I may need your help put `cla: yes` label back on. The bot somehow removed it.", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30377) for more info**.\n\n<!-- cla_yes -->"]}, {"number": 30376, "title": "Tflite for microcontrollers : micro speech example with more keywords (yes, no, up, down ..) fails", "body": "**System information**\r\n- Have I written custom code : No\r\n- OS Platform and Distribution : Linux Ubuntu 16.04 host machine with TensorFlow docker image\r\n- Mobile device : SparkFun Edge Development Board\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version : 1.14.0\r\n- Python version: Python 2.7.15+\r\n- Bazel version : 0.27 (apt repository)\r\n- GCC/Compiler version : 7.4.0\r\n- GPU model and memory: Nvidia GTX 1070\r\n\r\n**Describe the current behavior**\r\n\r\nI am trying to extend the [Micro Speech example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech) by adding more keywords such as 'yes, no, stop, go, up, down' etc. I followed the instructions for  training and converting the model to tflite format from the [Micro Speech GitHub page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model). However, when I add my newly trained and converted 'tiny_conv_micro_features_model_data.cc' file to my project, compiling and running it on the Sparkfun Microcontroller gives the following error :\r\n\r\n '_Bad input tensor parameters in model_' . \r\n\r\n**More detail and analysis on the above problem**\r\n\r\nI successfully compiled and ran the Micro Speech example on SparkFun Edge Development Board using [AI on a microcontroller with TensorFlow Lite and SparkFun Edge](https://codelabs.developers.google.com/codelabs/sparkfun-tensorflow/#0) tutorial.\r\n\r\nNext, I trained the model for recognizing more (\"yes,no,up,down\")  keywords giving the following commands:\r\n\r\n> bazel run -c opt --copt=-mavx2 --copt=-mfma --verbose_failures --host_force_python=PY2 tensorflow/examples/speech_commands:train -- \\\r\n--model_architecture=tiny_conv --window_stride=20 --preprocess=micro \\\r\n--wanted_words=\"yes,no,up,down\" --silence_percentage=25 --unknown_percentage=25 --quantize=1\r\n\r\nThen I added the newly created 'tiny_conv_micro_features_model_data.cc' file to the [micro_speech_test project](https://drive.google.com/file/d/1cawEQAkqquK_SO4crReDYqf_v7yAwOY8/view) in order to verify that the newly trained model works correctly. Compiling the project and running the binary gives the following error: \r\n\r\n![image](https://user-images.githubusercontent.com/36483321/60624860-310df200-9d9b-11e9-8fef-83cab23fc777.png)\r\n\r\nThis shows that interpreter requires an input tensor of shape: (1, 49, 43, 1), but the interpreter receives tensor of shape (1, 49, 40, 1) from the newly trained model. \r\n\r\nIf the model is trained on input tensor of shape  (1, 49, 43, 1) as per the the [ spectrogram ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#calculating-the-input-to-the-neural-network) of (49x43) pixels, how does the model interpreter of the micro speech project receives wrong shaped input tensor? \r\n\r\nIs it because we select input shape as (1, 49, 40, 1), while converting '.pb' file to '.tfilte' file by running the bazel command:\r\n\r\n> bazel run tensorflow/lite/toco:toco -- \\\r\n--input_file=/tmp/tiny_conv.pb --output_file=/tmp/tiny_conv.tflite \\\r\n--input_shapes=1,49,40,1 --input_arrays=Reshape_1 --output_arrays='labels_softmax' \\\r\n--inference_type=QUANTIZED_UINT8 --mean_values=0 --std_values=9.8077\r\n\r\nas per the instruction on [GitHub page](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#creating-your-own-model) If this is the reason why is the input shape been selected as (1,49,40,1) and not (1,49,43,1).\r\n\r\nI also tried the above command by changing the input_shapes argument as --input_shapes=1,49,43,1. But this gives the following error:\r\n\r\n![image](https://user-images.githubusercontent.com/36483321/60626163-c8c10f80-9d9e-11e9-9bff-b49f9a92cedb.png)\r\n\r\n\r\nTo verify that whether this error occurs due to adding more keywords to the wanted words argument (such as 'yes, no, up, down'), I trained the model only for 'yes,no' keywords. Still the same error and problem persisted as above.\r\n\r\nI have not made any changes to the TensorFlow or micro speech example source files for the above experimentation. I just replaced the pre-existing 'tiny_conv_micro_features_model_data.cc' file by the new 'tiny_conv_micro_features_model_data.cc' file of the newly trained model for verifying my work.\r\n\r\nKindly guide me to solve my above problem.\r\n\r\nThanks.", "comments": ["Hi,\r\n\r\nCan someone ( @petewarden @gadagashwini ) guide me to the solution of the above problem? It would be of great help.\r\n\r\nThank you.", "The input shape (1,49,40,1) is right which means there are bugs in their code. 'tflite for microcontrollers' is on progress so you should update the source code. I had the same error before but a week ago I re-builded 'micro_speech' from the source and the problem was solved!", "Hi,\r\n\r\n@tuanphan09 Thank you for helping me out. It solved my problem and now I am able to successfully compile and deploy my model to the microcontroller.", "@saumpsh What was your solution? I'm having the same problem...", "Hi,\r\n\r\nI followed the advice from @tuanphan09 to solve my problem. Just download TensorFlow repository in the TF docker container. Then follow these [steps ](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/micro_speech#use-your-local-machine)to train your model. It should work. \r\n\r\nHowever, the TensorFlow lite for microcontroller developers are using Python scripts to train the model for the microcontroller. Previous they used Bazel build system to train the model. I have not tried training the model using Python scripts. So can not comment on it. The bazel build worked for me. But python scripts should work if you have 'tf-nightly-gpu==1.15.0.dev20190729' version installed as per the above GitHub page.", "Thanks for the info. I installed locally nightly build instead of standard 1.14.0 and got it working.\r\n\r\ntf-estimator-nightly 1.14.0.dev2019072901\r\ntf-nightly           1.15.0.dev20190729\r\n\r\nIt think nightly builds have some TF operations which does not exist in 1.14.0\r\n"]}, {"number": 30375, "title": "[INTEL_MKL] Parallelizing scatter update operation", "body": "This PR parallelizes scatter update op using TF thread pool.\r\n\r\nBenchmark results from tensorflow/core/kernels/scatter_op_test.cc\r\n\r\nSingle Threaded\r\nBenchmark                 Time(ns) Iterations\r\n---------------------------------------------\r\nBM_ScatterAddInt32/1      11742060        100    85.2M items/s\r\nBM_ScatterAddInt32/10     23648080        100    422.9M items/s\r\nBM_ScatterAddInt32/64     62754140        100    1019.9M items/s\r\nBM_ScatterAddInt32/256   187718030        100    1363.7M items/s\r\nBM_ScatterAddInt32/1024  662454890        100    1545.8M items/s\r\n\r\nTF_NUM_INTRAOP_THREADS=13\r\nBenchmark                 Time(ns) Iterations\r\n---------------------------------------------\r\nBM_ScatterAddInt32/1       2277507        282    439.1M items/s\r\nBM_ScatterAddInt32/10      3805265        181    2627.9M items/s\r\nBM_ScatterAddInt32/64     11039150        100    5797.5M items/s\r\nBM_ScatterAddInt32/256    42545230        100    6017.1M items/s\r\nBM_ScatterAddInt32/1024  139565500        100    7337.1M items/s\r\n\r\nWe are seeing about 5x improvements from the benchmark results on Skylake CPU's", "comments": ["Thanks @penpornk . Addressed the changes requested.", "@Srini511 can you please check build failures ?", "@rthadur , I did a full build and also all the 4 root cause targets at my end. They seem to be passing. Any suggestions on how to reproduce this?", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30375) for more info**.\n\n<!-- need_author_cla -->", "@penpornk , I accepted your suggestion and somehow, the cla check is broken :-)", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30375) for more info**.\n\n<!-- cla_yes -->", "We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for all the commit author(s) or Co-authors.  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA ([login here](https://cla.developers.google.com/) to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again. If the bot doesn't comment, it means it doesn't think anything has changed.\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30375) for more info**.\n\n<!-- need_author_cla -->", "A Googler has manually verified that the CLAs look good.\n\n(Googler, please make sure the reason for overriding the CLA status is clearly documented in these comments.)\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30375) for more info**.\n\n<!-- cla_yes -->", "12 tests passed and only `Ubuntu Python2` and `Ubuntu Python3` are failing. Both of them don't have links to test logs so I'm going to rerun the tests again.", "@penpornk I think all the required tests have passed! ", "@Srini511 Yay!\r\n@rthadur I think this PR is ready to pull. :) Only `Windows Bazel GPU` is failing this time, and it used to pass last time we ran the tests (without code changes).", "@penpornk done.", "@rthadur Thank you! :)", "Thanks @penpornk and @rthadur", "Hi @penpornk is there a problem with the PR? wondering why it is reverted.", "@Srini511 Sorry about that. It was causing third_party/tensorflow/contrib/metrics:metric_ops_large_test to fail so I had to revert. Could you please help take a look? Thank you!\r\n\r\nHere is the error log (line numbers might not match):\r\n```\r\nTraceback (most recent call last):   File \"absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n     yield   \r\nFile \"absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n     testMethod()   \r\nFile \"tensorflow/contrib/metrics/python/ops/metric_ops_large_test.py\", line 64, in testLargeCase\r\n     self.assertNear(got_value, expected_value, 1.0)   \r\nFile \"tensorflow/python/framework/test_util.py\", line 1095, in decorated\r\n     return f(*args, **kwds)   \r\nFile \"tensorflow/python/framework/test_util.py\", line 2195, in assertNear\r\n     (f1, f2, err, \" (%s)\" % msg if msg is not None else \"\"))   \r\nFile \"<embedded stdlib>/unittest/case.py\", line 422, in assertTrue\r\n     raise self.failureException(msg) AssertionError: False is not true : 291006464.000000 != 297795584.000000 +/- 1.000000\r\n```", "Hi @penpornk, I cannot find that test since it is in contrib directory. Is it an internal test? \r\n ", "Found the test. Will look into it and let you know. Thanks!"]}, {"number": 30374, "title": "[XLA:GPU][ROCm] instantiate GPU transfer manager for both NVPTX and AMDGPU", "body": "Follow-up PR for #30323 ", "comments": []}, {"number": 30373, "title": "add -lm to mlir-tblgen linkopts", "body": "Fixes the following build error:\r\n\r\n```\r\nCouldn't build file external/local_config_mlir/mlir-tblgen: Linking of rule '@local_config_mlir//:mlir-tblgen' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command\r\n...\r\n/usr/bin/ld: bazel-out/k8-opt/bin/external/llvm/libsupport.a(APInt.o): undefined reference to symbol 'round@@GLIBC_2.2.5'\r\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\r\ncollect2: error: ld returned 1 exit status\r\n```", "comments": []}, {"number": 30372, "title": "Poor Feature/Example serialization performance", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): unknown 2.0.0-beta1\r\n- Python version: 3.7.2\r\n- CUDA/cuDNN version: 10.0/7.5\r\n- GPU model and memory: GeForce GTX Titan X, 12GB\r\n\r\n**Describe the current behavior**\r\nThe rate of serializing examples is unreasonably slow. In particular, performance is slow for `Feature`s that are a sequence of values. I have a dataset with numerous fields, including two that are represented by short, fixed length, 1-dimensional `Tensor`s. When excluding those two features, serialization happens in a slow but manageable amount of time. Adding either of those two features causes it to take many times as long.\r\n\r\nNotably, when mapping a `Dataset` to a function that performs serialization, increasing the number of threads does not significantly impact performance and the CPU remains mostly idle. I do see marked performance improvements and higher CPU usage when parallelizing other mapped functions, so it could be that there is some kind of global bottleneck for this operation.\r\n\r\n**Describe the expected behavior**\r\nRecords should be serialized in a reasonable amount of time.\r\n\r\n**Code to reproduce the issue**\r\nThe following code serializes 10000 records in about 6s on a particular machine. Note that if I replace the mapped function with one that simply returns a constant, it takes less than 1s to complete, so the serialization is the problem.\r\n\r\n    def make_example_1(*data_list):\r\n        feature_dict = {\r\n            'a':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[0]])),\r\n            'b':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[1]])),\r\n            'c':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[2]])),\r\n            'd':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[3]])),\r\n            'e':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[4]])),\r\n            'f':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[5]])),\r\n            'g':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[6]])),\r\n            'h':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[7]])),\r\n            'i':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[8]])),\r\n            'j':tf.train.Feature(int64_list=tf.train.Int64List(value=[data_list[9]]))}\r\n        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\r\n        return example.SerializeToString()\r\n    \r\n    def make_example_1_wrap(*input):\r\n        data = input[0]\r\n        return tf.py_function(\r\n            make_example_1,\r\n            (\r\n                data['a'],data['b'],data['c'],data['d'],data['e'],\r\n                data['f'],data['g'],data['h'],data['i'],data['j']),\r\n            Tout=tf.string)\r\n        \r\n    def feature_test_1(num_threads=None):\r\n        source = {\r\n            'a':tf.constant(0),\r\n            'b':tf.constant(1),\r\n            'c':tf.constant(2),\r\n            'd':tf.constant(3),\r\n            'e':tf.constant(4),\r\n            'f':tf.constant(5),\r\n            'g':tf.constant(6),\r\n            'h':tf.constant(7),\r\n            'i':tf.constant(8),\r\n            'j':tf.constant(9)}\r\n        ds = tf.data.Dataset.from_tensors(source).repeat(10000)\r\n        ds = ds.map(make_example_1_wrap, num_threads)\r\n        it = iter(ds)\r\n        for x in it:\r\n            pass\r\n\r\nThe following example uses about the same input data size as the previous one, but uses 2 features of length 5 instead of 10 features of length 1. The execution time increases to 17s, highlighting the problem with sequences as `Int64List`s.\r\n\r\n    def make_example_2(*data_list):\r\n        feature_dict = {\r\n            'a':tf.train.Feature(int64_list=tf.train.Int64List(value=data_list[0])),\r\n            'b':tf.train.Feature(int64_list=tf.train.Int64List(value=data_list[1]))}\r\n        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\r\n        return example.SerializeToString()\r\n    \r\n    def make_example_2_wrap(*input):\r\n        data = input[0]\r\n        return tf.py_function(\r\n            make_example_2,\r\n            (data['a'], data['b']),\r\n            Tout=tf.string)\r\n    \r\n    def feature_test_2(num_threads=None):\r\n        x = {\r\n            'a':tf.constant([0,1,2,3,4]),\r\n            'b':tf.constant([5,6,7,8,9])}\r\n        ds = tf.data.Dataset.from_tensors(x).repeat(10000)\r\n        ds = ds.map(make_example_2_wrap, num_threads)\r\n        it = iter(ds)\r\n        for x in it:\r\n            pass\r\n\r\n**Other info / logs**\r\nA possibly related performance issue is mentioned by many users in #16933, although that issue was closed over a year ago due to inactivity.", "comments": ["Yes, same issue as mentioned in other issues like #16933. To write something performant we've used `tf.serialize_tensor` with `tf.data.experimental.TFRecordWriter()` as mentioned briefly eg in the recent [load images](https://www.tensorflow.org/tutorials/load_data/images) tutorial. This is quite cumbersome to use compared to Examples and Features. ", "@novog ,\r\nCan you please confirm if using the APIs and referring the Tutorial mentioned by @areeh has given you expected performance.", "OK, thanks. If using `Example` isn't viable for the moment, then I suppose that will do.\r\n\r\nIn my case, I have not a single tensor, but a number of tensors corresponding to named features of differing types, so I can't just use `tf.io.serialize_tensor`. I could serialize them individually, put them in a dictionary, serialize the dictionary (using something like python's `json`), then pass those strings to `TFRecordWriter`. Let me know if you have a cleaner solution. Otherwise, I'll try that and post a follow-up on the relative performance.", "Yes, it is much faster using `tf.io.serialize_tensor` manually rather than using `Feature`/`Example`. The following code completes in about 3.5s instead of 17s:\r\n\r\n    def make_example_2(*data_list):\r\n        feature_dict = {\r\n            'a':tf.io.serialize_tensor(data_list[0]).numpy(),\r\n            'b':tf.io.serialize_tensor(data_list[1]).numpy()}\r\n        serialized = pickle.dumps(feature_dict)\r\n        return serialized\r\n\r\nGiven that mapping to a function that simply returns a constant string takes 2.5s, the time added by serialization decreased from about 14.5s to 1s.\r\n\r\nAs to the status of this issue, while there is a sort of workaround, I would consider the defect itself to remain: `Feature`/`Example` perform unreasonably poorly.", "@novog Yes and I don't know of a way to have behaviour more or less equivalent to `Feature`/`Example` with `serialize_tensor`\r\n\r\nedit: for the record, the use-case I'm thinking of is similar to @novog with named (potentially heterogeneous) dicts of tensors written, read and processed as a single unit in terms of operations such as shuffling, interleaving reads from shards, etc", "I don't think we can meaningfully speed up protocol buffer serialization in TF. I suggest you follow this bug up with one against the maintaners of protocol buffers themselves.\r\n\r\nEither that or use, as suggested above, another serialization format.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30372\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30372\">No</a>\n", "I tried [both approaches](https://stackoverflow.com/questions/62174662/slow-serialization-of-ndarray-to-tfrecord) and frankly `serialize_tensor` performed even worse. Ideas?", "> Yes, it is much faster using `tf.io.serialize_tensor` manually rather than using `Feature`/`Example`. The following code completes in about 3.5s instead of 17s:\r\n> \r\n> ```\r\n> def make_example_2(*data_list):\r\n>     feature_dict = {\r\n>         'a':tf.io.serialize_tensor(data_list[0]).numpy(),\r\n>         'b':tf.io.serialize_tensor(data_list[1]).numpy()}\r\n>     serialized = pickle.dumps(feature_dict)\r\n>     return serialized\r\n> ```\r\n> \r\n> Given that mapping to a function that simply returns a constant string takes 2.5s, the time added by serialization decreased from about 14.5s to 1s.\r\n> \r\n> As to the status of this issue, while there is a sort of workaround, I would consider the defect itself to remain: `Feature`/`Example` perform unreasonably poorly.\r\n\r\nHello @novog , I tried this idea, It's much faster than tf.train.Feature. But I can't make the read pipeline work.\r\nWhen I call tf.io.parse_tensor, it's failed with the error message \r\n2021-07-03 23:35:01.655594: W tensorflow/core/framework/op_kernel.cc:1755] Invalid argument: ValueError: Tensor conversion requested dtype string for Tensor with dtype int32: <tf.Tensor: shape=(9, 9, 395), dtype=int32, numpy=\r\n\r\ncould you please show me the code how to read from the make_example_2 ?", "@xykong If I understand your question correctly, you are trying to use `parse_tensor` to transform the serialized string that we created back to a tensor. Instead, you would need to call `pickle.loads` on the string after reading it in from disk. Doing so will yield the `feature_dict` object that we had serialized.\r\n\r\nIn any case, I believe that the general movement has been toward using Keras functionality in place of things like Feature/FeatureColumn/Example, so you might want to check out how Keras preprocessing layers and data generators work and consider them as an option."]}, {"number": 30371, "title": "Bad inference on mobilenet quantized graph", "body": "Hi! I'm having some trouble about quantization on ssd_mobilenet_v1, doing inferences on raspberry Pi 3. I have trained this model using 2007voc dataset with the following comand:\r\n\r\n```bash\r\npython /models/research/object_detection/legacy/train.py \\\r\n\t\t--train_dir $TRAIN_DIR \\\r\n\t\t--pipeline_config_path \"$TRAIN_DIR/pipeline.config\"\r\n```\r\nAnd params '{\"batch_size\":8,\"learning_rate\":0.003}'.\r\nTo generate both frozen and quantized graph, i use the following comand:\r\n\r\n```bash\r\n/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n\t\t--in_graph=\"$TRAIN_DIR/frozen_inference_graph.pb\" \\\r\n\t\t--out_graph=\"$TRAIN_DIR/quantized_graph.pb\" \\\r\n\t\t--inputs='image_tensor' \\\r\n\t\t--outputs=MobilenetV1/Predictions/Reshape_1 \\\r\n\t\t--transforms='\r\n\t\tquantize_weights'\r\n```\r\n\r\nDoing inferences using just the quantized graph, the result is really good. The model predict almost everything, with good bouding boxes. But, when i do inferences using the quantized graph, the bouding boxes look like this:\r\n![my2](https://user-images.githubusercontent.com/23393117/60616312-8c45e180-9da7-11e9-9328-77d62e71b144.png)\r\n\r\nI know the inference time is not equals when we compare both models. And i also know the mAP is note the same, but it is suposed to be that bad?\r\n\r\n### Process information\r\n- **The main process you can find [here](https://medium.com/nanonets/how-to-easily-detect-objects-with-deep-learning-on-raspberrypi-225f29635c74)**\r\n- **I use for train:**\r\n- Linux 16.04\r\n- Core i7-4770s\r\n- 16GB Ram\r\n- **I use for Inference:**\r\n- Raspberry Pi3\r\n\r\n", "comments": ["Can you please confirm the bad inference is on frozen_inference_graph.pb or quantized_graph.pb?.Also ,please let us know did you convert .pb model to Lite to see the inference on raspberry Pi 3. I .", "The bad inference happened on quantized_grap.pb. And i did not convert to lite. I used quantized_graph.pb to make inferences on raspberry using this [code](https://github.com/NanoNets/TF-OD-Pi-Test/blob/master/ObjectDetectionPredict.py). There is a way to convert to lite and run on raspberry? Anyway, i do not think that gonna solve my problem, once the bad inferences happened alredy on .pb quantized graph", "@joaolcaas Could you please let us know if this issue still persists ? If it is resolved then please feel free to move this issue to close status ?Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30371\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30371\">No</a>\n"]}, {"number": 30370, "title": "Enable Use of Cudnn APIs for Backward Filter Grouped Convolutions", "body": "In cudnn_conv_rewriter, enabling cudnn for backprop filter requires a reshape of the input shape such that it is compatible with cudnn. This reshape was missing which caused an internal error when simply trying to use cudnn backprop APIs.\r\n\r\nThe input had been reshaped in tf2xla bridge in order to use cudnn forward conv for computing gradients in case of grouped convolutions. Now that cudnn 7.6 has support for efficient backprop for grouped convolutions, there is no need to use cudnn fwd conv APIs for backprop.\r\n\r\nAdded unit-test for this change.", "comments": ["> Looks like this PR breaks tests in\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/tests/conv_depthwise_test.cc\r\n> \r\n> Please have a look. Let me know if you have issues reproducing the test breakages.\r\n\r\nHi Thomas.... I investigated the test failures. Now that I have enabled cudnn backward filter group convs, it looks like `MatchBackwardFilter` is incorrectly matching a forward depthwise convolution to backward filter group convolution. This check https://github.com/tensorflow/tensorflow/blob/b645595011b6a808f7186cd2fd6c4e0f21a5e281/tensorflow/compiler/xla/service/gpu/cudnn_conv_rewriter.cc#L151 fails since the test case has a window dilation even though it is a fwd convolution, which in my opinion, is quite possible. It was passing earlier since by default any conv op with feature_group_count > 1 was returning `no_match_result`. I am not sure if I am missing something but `(input_batch_dim == output_batch_dim)` seems to be a sufficient condition to check whether a visited conv is a fwd convolution or not. Please correct me if I am wrong. I have formulated some checks to explicitly isolate depthwise fwd conv from backward filter conv, but if removing `!window_util::HasWindowDilation(conv->window())` in the matcher makes sense, the additional checks that I have formulated might be redundant. Please let me know what you think. ", "> In general, I think you should take a look at\r\n> tensorflow/compiler/xla/service/convolution_group_converter.cc\r\n> If you want to map backward filter convolutions to cudnn calls, you need to remove this pass from nvptx_compiler and then handle the batch_group_count parameter, and still return no_match for feature_group_count > 1.\r\n> You can see what the bridge does for depthwise convolutions here:\r\n> tensorflow/compiler/tf2xla/kernels/conv_op_helpers.cc\r\n\r\nHi Adrian... I made some changes to the tf2xla bridge to  remap all depthwise backward filter convs to use feature_group_count instead of batch_group_count and use cudnn group conv BackwardFilter APIs. This was my first pass at handling depthwise backward filter and it seems to work fine. We had found that mobilenet with xla enabled runs slower than TF-native. With this change , switching backward filter to use cudnn group conv, xla performs better than TF-native. `convolution_group_converter` is now redundant in `nvptx_compiler` with this change. However, I understand that the other way to do this would be to remove `convolution_group_converter`in `nvptx_compiler`, add a new pass in `service/gpu` to handle batch_group convs and rewrite them to use feature_group. Which of these two would be the way to go? Making changes to the bridge might affect other backends but I don't have the complete information to make that judgment. Also, according to your last comment, I am not clear on why we would want to  \"still return no_match for feature_group_count > 1\" if we can transform the convs to correctly use feature_groups in a separate pass. As per my understanding we would subsequently want match these convs to backward filter conv and thunk to cudnn.  Please correct me if my understanding is wrong. ", "> Hi Adrian... I made some changes to the tf2xla bridge to remap all depthwise backward filter convs to use feature_group_count instead of batch_group_count and use cudnn group conv BackwardFilter APIs. This was my first pass at handling depthwise backward filter and it seems to work fine. We had found that mobilenet with xla enabled runs slower than TF-native. With this change , switching backward filter to use cudnn group conv, xla performs better than TF-native. `convolution_group_converter` is now redundant in `nvptx_compiler` with this change. However, I understand that the other way to do this would be to remove `convolution_group_converter`in `nvptx_compiler`, add a new pass in `service/gpu` to handle batch_group convs and rewrite them to use feature_group. Which of these two would be the way to go? Making changes to the bridge might affect other backends but I don't have the complete information to make that judgment. Also, according to your last comment, I am not clear on why we would want to \"still return no_match for feature_group_count > 1\" if we can transform the convs to correctly use feature_groups in a separate pass. As per my understanding we would subsequently want match these convs to backward filter conv and thunk to cudnn. Please correct me if my understanding is wrong.\r\n\r\nI haven't seen your changes to the bridge in this pull request, but I would expect that this would move it back towards an earlier state that was intentionally changed to introduce the batch_group_count parameter in order to be able to avoid a reshape. As far as I know, the TPU backend prefers not to have reshapes because they are not necessarily cheap.\r\nMy suggestion was meant as not changing the bridge, just look at what it does and then pattern match that. This means when you pattern match, you would never match a feature_group_count > 1 to a backward filter convolution, but instead match a batch_group_count > 1 to a backward filter convolution. Sorry for not making this clear enough :-(\r\nApart from that, if you actually want to pattern match this, I think you need to remove the ConvolutionFeatureGroupConverter pass from nvptx_compiler, otherwise your pattern matching will never trigger because that pass runs earlier than the CudnnConvRewriter pass. The pass is still needed for the TPU and CPU backends though.", "Hi @akuegel .. not sure why this PR got closed :(", "@akuegel anything wrong with this PR?", "> > Hi Adrian... I made some changes to the tf2xla bridge to remap all depthwise backward filter convs to use feature_group_count instead of batch_group_count and use cudnn group conv BackwardFilter APIs. This was my first pass at handling depthwise backward filter and it seems to work fine. We had found that mobilenet with xla enabled runs slower than TF-native. With this change , switching backward filter to use cudnn group conv, xla performs better than TF-native. `convolution_group_converter` is now redundant in `nvptx_compiler` with this change. However, I understand that the other way to do this would be to remove `convolution_group_converter`in `nvptx_compiler`, add a new pass in `service/gpu` to handle batch_group convs and rewrite them to use feature_group. Which of these two would be the way to go? Making changes to the bridge might affect other backends but I don't have the complete information to make that judgment. Also, according to your last comment, I am not clear on why we would want to \"still return no_match for feature_group_count > 1\" if we can transform the convs to correctly use feature_groups in a separate pass. As per my understanding we would subsequently want match these convs to backward filter conv and thunk to cudnn. Please correct me if my understanding is wrong.\r\n> \r\n> I haven't seen your changes to the bridge in this pull request, but I would expect that this would move it back towards an earlier state that was intentionally changed to introduce the batch_group_count parameter in order to be able to avoid a reshape. As far as I know, the TPU backend prefers not to have reshapes because they are not necessarily cheap.\r\n> My suggestion was meant as not changing the bridge, just look at what it does and then pattern match that. This means when you pattern match, you would never match a feature_group_count > 1 to a backward filter convolution, but instead match a batch_group_count > 1 to a backward filter convolution. Sorry for not making this clear enough :-(\r\n> Apart from that, if you actually want to pattern match this, I think you need to remove the ConvolutionFeatureGroupConverter pass from nvptx_compiler, otherwise your pattern matching will never trigger because that pass runs earlier than the CudnnConvRewriter pass. The pass is still needed for the TPU and CPU backends though.\r\n\r\nHey Adrian...thanks for making this so clear \ud83d\ude42. I didn't push my changes to this PR since I suspected that  a pass specific for  GPU backend would be ideal rather than changing the bridge. I made the bridge change to investigate how use of feature_groups rather than batch_groups would impact performance. I did have to add a reshape but even with that it turns out using feature_groups and thunking to cudnn improves the performance in mobilenet (which is predominantly depthwise convs) by arnd ~20 %. I will work on this but put it in a separate PR. This current PR would work without explicitly handling depthwise conv's BackwardFilter since it is being mapped to fwd convs (feature_grp = 1) anyway. I have separated the BackwardInput case [here](https://github.com/tensorflow/tensorflow/pull/30775). I have also made changes to handle test failures.", "Sorry, I accidentally clicked the wrong button and closed this request. I thought it meant I am finished with my review comment.\r\nIn general I am not sure if the pull request as-is adds anything. As you have seen by now, anything with feature_group_count > 1 is not a real backward filter convolution (at least it was not generated as such on the tensorflow level), and with your latest change you added one more check to make sure we don't accidentally recognize something as backward filter convolution. Does your new code still trigger, and then for which convolutions?", "The latest check I added makes sure that it accidentally doesn't match a depthwise forward convolution to a backward conv. The tests that were failing were depthwise fwd convs but with window dilation. Since an existing [check](https://github.com/tensorflow/tensorflow/blob/b645595011b6a808f7186cd2fd6c4e0f21a5e281/tensorflow/compiler/xla/service/gpu/cudnn_conv_rewriter.cc#L151) requires a fwd convolution to not have window dilation, the testcase falsely matches the fwd conv to a backward filter conv. IMO this additional check of `!window_util::HasWindowDilation(conv->window())` [here](https://github.com/tensorflow/tensorflow/blob/b645595011b6a808f7186cd2fd6c4e0f21a5e281/tensorflow/compiler/xla/service/gpu/cudnn_conv_rewriter.cc#L151) is most likely misleading (since fwd convs can have dilations). But I might be missing something. Since I haven't been able to investigate the complete scope of removing this additional condition (`!window_util::HasWindowDilation(conv->window())`), I added another check which isolates and identifies depthwise fwd convolutions. This additional check addresses the test failures. ", "As such, this PR is to make use of cudnn backward APIs for grouped backward convolutions as they have become efficient post cudnn 7.6. There were some comments in the existing code that suggested that cudnn can be used for backward conv by simply removing the checks. However, on doing so xla threw an internal error when matching BackwardFilter. This PR enables cudnn for backward filter and fixes the  internal error. We came across this while investigating performance of ResneXt 101. Also, as I mentioned earlier, I found that mobilenet's performance improves when using cudnn FilterBackprop APIs for feature_grps > 1. ", "> As such, this PR is to make use of cudnn backward APIs for grouped backward convolutions as they have become efficient post cudnn 7.6. There were some comments in the existing code that suggested that cudnn can be used for backward conv by simply removing the checks. However, on doing so xla threw an internal error when matching BackwardFilter. This PR enables cudnn for backward filter and fixes the internal error. We came across this while investigating performance of ResneXt 101. Also, as I mentioned earlier, I found that mobilenet's performance improves when using cudnn FilterBackprop APIs for feature_grps > 1.\r\n\r\nThe comment only refers to depthwise backward input convolution. And I have tested now that we still get regressions when we enable it. I suspect it is due to the additional reshape that we need here. I haven't looked at the generated HLO yet, but maybe there is an option for optimizing the reshape away.\r\nRegarding backward filter convolution: as I tried to explain, the tf2xla bridge transforms depthwise backward filter convolutions to XLA convolutions with batch_group_count > 1, but feature_group_count = 1. So if you add code to try to recognize something as backward filter convolution that has feature_group_count > 1, you are likely introducing a bug here, because feature_group_count > 1 is only generated by the bridge for depthwise forward convolution and depthwise backward input convolution. You had a test failure that you then fixed by also handling one more case as forward convolution. I am interested if you would still recognize something as backward filter convolution that has feature_group_count > 1, and if yes, what convolution it is, and if you get the correct results.\r\nThere is a small chance that for some convolutions we have the choice whether to handle them as backward filter convolutions even though they are a backward input or a forward convolution at tensorflow level, but I think the right way to enable using cudnn for depthwise backward filter convolution is to implement what I suggested before, and abandon the attempt in this pull request. ", "> The comment only refers to depthwise backward input convolution. And I have tested now that we still get regressions when we enable it. I suspect it is due to the additional reshape that we need here. I haven't looked at the generated HLO yet, but maybe there is an option for optimizing the reshape away.\r\n\r\nIn that case we can still continue using forward convolutions for backward input convs for feature_group_count > 1 and investigate what impact the reshape has in order to enable cudnn backward input API.\r\n\r\n> Regarding backward filter convolution: as I tried to explain, the tf2xla bridge transforms depthwise backward filter convolutions to XLA convolutions with batch_group_count > 1, but feature_group_count = 1. So if you add code to try to recognize something as backward filter convolution that has feature_group_count > 1, you are likely introducing a bug here, because feature_group_count > 1 is only generated by the bridge for depthwise forward convolution and depthwise backward input convolution. You had a test failure that you then fixed by also handling one more case as forward convolution. I am interested if you would still recognize something as backward filter convolution that has feature_group_count > 1, and if yes, what convolution it is, and if you get the correct results.\r\n\r\nAs per my understanding, there are two cases (not completely unrelated) that tf2xla bridge handles for backward filter when feature_grps > 1:\r\n1)  When feature_groups > 1 but the conv is NOT depthwise - In this case, the xla conv op it creates has feature_group_count > 1 but batch_group_count = 1. The backward filter operation  is done using a fwd conv as described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_grad_ops.h). The input also gets transformed [here](https://github.com/tensorflow/tensorflow/blob/e1f6d9e43de2227e3adf038f538cd88cae37ff06/tensorflow/compiler/tf2xla/kernels/conv_op_helpers.cc#L516).\r\n2) When feature_group_count > 1 and the conv IS depthwise - In this case, the xla conv op has feature_group_count = 1 but batch_group_count > 1. Batch_group convolutions are leveraged to implement backward filter for depthwise convs. Note that, unlike in case (1),  the input is not transformed here. Only the dims are swaped as required by the algorithm (same as case (1))\r\n\r\nMy change here only handles case (1). The backward filter convs that get matched have feature_group_count > 1 but are not depthwise. This is so because, as it stand now, no depthwise filter backprop will have feature_group_count > 1 and hence my change won't even be triggered. They will still be computed using forward convs. So in short, to answer your question, yes, backward filter convolution that have feature_group_count > 1 will be correctly matched but only if they are NOT depthwise. Backward filters which are not depthwise have feature_group_count > 1. `cudnn_conv_rewriter.cc` only tries to match those and not depthwise backward filter since, as you rightly said, they don't have feature_group_count > 1.\r\n\r\n> There is a small chance that for some convolutions we have the choice whether to handle them as backward filter convolutions even though they are a backward input or a forward convolution at tensorflow level, but I think the right way to enable using cudnn for depthwise backward filter convolution is to implement what I suggested before, and abandon the attempt in this pull request.\r\n\r\nYou are right...we should handle depthwise backward filter convs through a different pass but we still need this PR. This so because this PR will handle (as in enable use of cudnn) all backward filters with feature_group_count >1 that are not depthwise. I plan to add a pass to identify specifically depthwise backward filter conv (only these will have batch_group > 1 and feature_group = 1 based on what the bridge does) and replace them with convs with equivalent convs with feature_group_count > 1 and batch_group =1 so that now `cudnn_conv_rewriter.cc` matches them what I described in case (1) i.e, regular backward filter with feature_group > 1. So my plan is to have another PR to handle depthwise backward filter (i.e, write a pass to map them to regular backward grouped filter convs) but this PR in itself is fine and necessary to handle all other cases. I hope I haven't rambled too much :P . Please let me know if this makes sense or if I am missing something.\r\n\r\n\r\n", "Thanks for the explanation and linking to the bridge code line. You are right, there are cases with feature_group_count > 1 which are backward filter convolutions. Probably this happens for the more general case of grouped convolutions (not depthwise). Now I see why this PR is needed in addition to what I suggested. Thanks for working on this :)", "@thomasjoerg I have addressed the test failures...Can you please take a look :) ", "> @thomasjoerg I have addressed the test failures...Can you please take a look :)\r\n\r\nIt turns out there are still two test failures in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/conv_ops_test.py\r\nConv2DTest.testInputGradientGroupConv \r\nConv2DTest.testFilterGradientGroupConv\r\nThe latter should definitely work, this is not a case of matching the wrong thing. I already asked before if you have a specific convolution where you have verified that the result is correct which makes use of the cudnn call to grouped backward filter convolution. I haven't thought myself about what is needed, but it looks like just doing a reshape is not enough.\r\nCan you please try to debug this? Conv2DTest.testFilterGradientGroupConv should definitely be passing by using your new code.\r\nEdit: I forgot to say, when running the test, make sure you have cuda enabled. I am not sure how that works in the open source build, but internally we use --config=cuda for that. Also, you need the _xla_gpu suffix after the test target name.", "> > @thomasjoerg I have addressed the test failures...Can you please take a look :)\r\n> \r\n> It turns out there are still two test failures in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/conv_ops_test.py\r\n> Conv2DTest.testInputGradientGroupConv\r\n> Conv2DTest.testFilterGradientGroupConv\r\n> The latter should definitely work, this is not a case of matching the wrong thing. I already asked before if you have a specific convolution where you have verified that the result is correct which makes use of the cudnn call to grouped backward filter convolution. I haven't thought myself about what is needed, but it looks like just doing a reshape is not enough.\r\n> Can you please try to debug this? Conv2DTest.testFilterGradientGroupConv should definitely be passing by using your new code.\r\n> Edit: I forgot to say, when running the test, make sure you have cuda enabled. I am not sure how that works in the open source build, but internally we use --config=cuda for that. Also, you need the _xla_gpu suffix after the test target name.\r\n\r\nThanks Adrian for pointing this out :). Indeed I was missing a transpose due to the way that bridge handles this. I tried to follow the existing `MatchBackwardInput` in `cudnn_conv_rewriter` but turns out the existing code in `MatchBackwardInput` also has a similar bug. I fixed the issue and will push the changes soon. In the meantime, are there any other tests that I shud be running to qualify?  Are all other tests  passing? The CI looks clean. I am not sure which checks show test failures :\\ ", "@akuegel I pushed the changes...please take a look.. I ran all the tests in kernel_tests and they are clean. There were some failures (some sporadic) but were unrelated to my changes and independent of whether xla is enabled. All conv tests are clean.", "> Thanks Adrian for pointing this out :). Indeed I was missing a transpose due to the way that bridge handles this. I tried to follow the existing `MatchBackwardInput` in `cudnn_conv_rewriter` but turns out the existing code in `MatchBackwardInput` also has a similar bug. I fixed the issue and will push the changes soon. In the meantime, are there any other tests that I shud be running to qualify? Are all other tests passing? The CI looks clean. I am not sure which checks show test failures :\\\r\n\r\nYes, I think those were the only failing tests. If those pass now, it is a very good sign and things should be ok. MatchBackwardInput might have the same bug, but I thought it should work for depthwise convolution. It doesn't handle the grouped convolution case which probably requires something more along the lines what you did here. But I also locally debugged that for the grouped convolution backward input, we would never patter match it right now even when removing the early return (at least not for the test cases that we have).", "> I am hopeful that this time it will work.\r\n\r\n:) Hopefully covered all the bases", "@akuegel I see an `import/copybara` failure...not sure what it is about...Do I need any changes on my end?", "> @akuegel I see an `import/copybara` failure...not sure what it is about...Do I need any changes on my end?\r\n\r\nI actually don't know. Maybe it is because there exists still something from the previous import attempt? I deleted that changelist now, but I don't know how to retrigger  another import.", "Ok, a team mate showed me how to import it manually. I am doing this now."]}, {"number": 30369, "title": "Fixing missed saving of \"interpolation\" parameter in UpSampling2D layer", "body": "**Bug description**: UpSampling2D layers in a model restored by \"tf.keras.models.load_model\" are initialized with interpolation=\"nearest\" even if in original model UpSampling2D layers have interpolation=\"bilinear\".", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30369) for more info**.\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F30369) for more info**.\n\n<!-- ok -->"]}, {"number": 30368, "title": "redundant code in tutorial; allowing bool to be passed in dropout", "body": "", "comments": ["> Seems to me that the doc string for Model needs to be updated. The following lines and the code do not match\r\n> \r\n> ```\r\n>   If you subclass `Model`, you can optionally have\t  If you subclass `Model`, you can optionally have\r\n>   a `training` argument (boolean) in `call`, which you can use to specify\t  a `training` argument (boolean) in `call`, which you can use to specify\r\n>   a different behavior in training and inference:\t  a different behavior in training and inference:\r\n> ```\r\n\r\nDiscussed with @omalleyt12 who worked on training arg related changes recently. Having training arg in this use case is still valid as users can explicitly pass training=False. Would like to keep the doc in the file as is.", "@pavithrasv @omalleyt12 \r\nRight, the users can pass training=False. In fact, the transformer example (https://www.tensorflow.org/beta/tutorials/text/transformer#encoder_and_decoder) is passing training=False as a simple test (`sample_encoder_layer_output = sample_encoder_layer(\r\n    tf.random.uniform((64, 43, 512)), False, None)` this is in the second line of the second code snippet under https://www.tensorflow.org/beta/tutorials/text/transformer#encoder_and_decoder).\r\n\r\nYet the dropout layer (tensorflow/python/keras/layers/core.py line 154) is only checking if training is None. If we change that to if not training, that call can be used for both the None/not None case and the True/False case.", "\r\n\r\n\r\n\r\n> @pavithrasv @omalleyt12\r\n> Right, the users can pass training=False. In fact, the transformer example (https://www.tensorflow.org/beta/tutorials/text/transformer#encoder_and_decoder) is passing training=False as a simple test (`sample_encoder_layer_output = sample_encoder_layer( tf.random.uniform((64, 43, 512)), False, None)` this is in the second line of the second code snippet under https://www.tensorflow.org/beta/tutorials/text/transformer#encoder_and_decoder).\r\n> \r\n> Yet the dropout layer (tensorflow/python/keras/layers/core.py line 154) is only checking if training is None. If we change that to if not training, that call can be used for both the None/not None case and the True/False case.\r\n\r\n@seanccho The `training is None` check is done to substitute in the learning_phase() Tensor. If a user passes in `False` explicitly, this substitution should not be done. Also, symbolic Tensors can not be part of a boolean expression, and since `training` can be a Tensor, this has the potential to throw errors\r\n\r\nExample (assuming 1.x):\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.ones((10, 10))\r\nif x:  # will error out\r\n  print('hey')\r\n```\r\n", "Thanks for the clarification!\r\n```\r\nimport tensorflow as tf\r\nx = tf.constant(True) # T/F flag\r\nif x:\r\n  print('hey')\r\n```\r\nyields a TypeError which says to Use `if t is not None:` instead of `if t:` in 1.14."]}, {"number": 30367, "title": "tf.test.is_gpu_available() raises error instead of returning False", "body": "\r\n\r\n**System information**\r\n\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\nv1.12.1-5259-ge703239 2.0.0-dev20190629\r\n\r\n\r\nnvcc --version\r\n```\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Sat_Aug_25_21:08:01_CDT_2018\r\nCuda compilation tools, release 10.0, V10.0.130\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nThe code snippet below raises an error instead\r\nof printing something\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nprint(\"tf version {}\".format(tf.__version__))\r\nif tf.test.is_gpu_available():\r\n    print(tf.test.gpu_device_name())\r\nelse:\r\n    print(\"TF cannot find GPU\")\r\n```\r\nProduces\r\n```\r\ntf version 2.0.0-dev20190629\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-8-b9f80ad1b5e1> in <module>\r\n     10 from tensorflow import keras\r\n     11 print(\"tf version {}\".format(tf.__version__))\r\n---> 12 if tf.test.is_gpu_available():\r\n     13     print(tf.test.gpu_device_name())\r\n     14 else:\r\n\r\n~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/test_util.py in is_gpu_available(cuda_only, min_cuda_compute_capability)\r\n   1388 \r\n   1389   try:\r\n-> 1390     for local_device in device_lib.list_local_devices():\r\n   1391       if local_device.device_type == \"GPU\":\r\n   1392         if (min_cuda_compute_capability is None or\r\n\r\n~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/client/device_lib.py in list_local_devices(session_config)\r\n     39   return [\r\n     40       _convert(s)\r\n---> 41       for s in pywrap_tensorflow.list_devices(session_config=session_config)\r\n     42   ]\r\n\r\n~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py in list_devices(session_config)\r\n   2203     return ListDevicesWithSessionConfig(session_config.SerializeToString())\r\n   2204   else:\r\n-> 2205     return ListDevices()\r\n   2206 \r\n   2207 \r\n\r\nInternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable\r\n\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should print \"GeForce RTX 2080 with Max-Q Design\"\r\n(as PyTorch does).\r\n\r\n**Code to reproduce the issue**\r\n\r\nSee above\r\n", "comments": ["@murphyk I executed the code on Colab with Tensorflow-gpu 2.0.0.dev20190703. Can you test once and let us know is this still an issue. Thanks!", "On my personal ubuntu 16 laptop, python 3.7. with cuda 10.0, pytorch 1.0,\ntf 2.0 and jax installed,  I executed the script below\nand only got errors with TF\n\n\n nvidia-smi\nThu Jul  4 07:51:10 2019\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.74       Driver Version: 418.74       CUDA Version: 10.1\n  |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr.\nECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute\nM. |\n|===============================+======================+======================|\n|   0  GeForce RTX 208...  On   | 00000000:01:00.0 Off |\n N/A |\n| N/A   34C    P8     5W /  N/A |    486MiB /  7952MiB |      4%\n Default |\n+-------------------------------+----------------------+----------------------+\n\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU\nMemory |\n|  GPU       PID   Type   Process name                             Usage\n   |\n|=============================================================================|\n|    0      1449      G   /usr/lib/xorg/Xorg\n181MiB |\n|    0      1642      G   /usr/bin/gnome-shell\n106MiB |\n|    0      2039      G   ...quest-channel-token=4691931010505848148\n 80MiB |\n|    0      3514      C   /home/murphyk/miniconda3/bin/python\n 111MiB |\n+-----------------------------------------------------------------------------+\n\n\nimport jax\nimport jax.numpy as np\nfrom jax import grad, jacfwd, jacrev, jit, vmap\nfrom jax.experimental import optimizers\nprint(\"jax version {}\".format(jax.__version__))\nfrom jax.lib import xla_bridge\nprint(\"jax backend {}\".format(xla_bridge.get_backend().platform))\n\nimport torch\nimport torchvision\nprint(\"torch version {}\".format(torch.__version__))\nif torch.cuda.is_available():\n    print(torch.cuda.get_device_name(0))\n    print(\"current device {}\".format(torch.cuda.current_device()))\nelse:\n    print(\"Torch cannot find GPU\")\n\nimport tensorflow as tf\nfrom tensorflow import keras\nprint(\"tf version {}\".format(tf.__version__))\nif tf.test.is_gpu_available():\n    print(tf.test.gpu_device_name())\nelse:\n    print(\"TF cannot find GPU\")\n\n\n\njax version 0.1.39\njax backend gpu\ntorch version 1.1.0\nGeForce RTX 2080 with Max-Q Design\ncurrent device 0\ntf version 2.0.0-dev20190629\n\n---------------------------------------------------------------------------InternalError\n                            Traceback (most recent call\nlast)<ipython-input-4-b432327d0c41> in <module>     40 from tensorflow\nimport keras     41 print(\"tf version {}\".format(tf.__version__))--->\n42 if tf.test.is_gpu_available():     43\nprint(tf.test.gpu_device_name())     44 else:\n~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/test_util.py\nin is_gpu_available(cuda_only, min_cuda_compute_capability)   1388\n1389   try:-> 1390     for local_device in\ndevice_lib.list_local_devices():   1391       if\nlocal_device.device_type == \"GPU\":   1392         if\n(min_cuda_compute_capability is None or\n~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/client/device_lib.py\nin list_local_devices(session_config)     39   return [     40\n_convert(s)---> 41       for s in\npywrap_tensorflow.list_devices(session_config=session_config)     42\n]\n~/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\nin list_devices(session_config)   2203     return\nListDevicesWithSessionConfig(session_config.SerializeToString())\n2204   else:-> 2205     return ListDevices()   2206    2207\nInternalError: CUDA runtime implicit initialization on GPU:0 failed.\nStatus: unknown error\n\n\n\n\n\nOn Thu, Jul 4, 2019 at 3:09 AM gadagashwini <notifications@github.com>\nwrote:\n\n> @murphyk <https://github.com/murphyk> I executed the code on Colab with\n> Tensorflow-gpu 2.0.0.dev20190703. Can you test once and let us know is this\n> still an issue. Thanks!\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/30367?email_source=notifications&email_token=ABDK6EARNDUPJAIWQMZXN2LP5XD55A5CNFSM4H5IJX32YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZG655Q#issuecomment-508423926>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABDK6EH5SL5O3GOC2UKXWMLP5XD55ANCNFSM4H5IJX3Q>\n> .\n>\n", "@murphyk I think this is not related to `Tensorflow` and it is more related to your GPU. \r\nAre you running any other codes in parallel (when you ran `tf.test.is_gpu_available()` ) that are holding `GPU:0` as busy? \r\nYour error says\r\n`InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable`. Thanks!", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30367\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30367\">No</a>\n"]}, {"number": 30366, "title": "get_variable style tf.Variable initialization", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 1.x\r\n- Are you willing to contribute it (Yes/No): No\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrently, tf.Variable requires passing in an initial value for the variable. It's often much more useful to pass in a shape and initializer. It'd be great if there was a convenience function that made this easy to do. tf.get_variable did this, but it requires a namescope to be specified, which is no longer desirable.\r\n**Will this change the current api? How?**\r\nBarely - it will simply add an additional way to initialize variables.\r\n**Who will benefit with this feature?**\r\nAlmost everyone using TF.\r\n**Any Other info.**\r\n", "comments": ["@tomhennigan I think you wanted this for sonnet as well, right?", "Hey @jbuckman, if you're using Keras you can do this inside `Layer`s via [`add_weight`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#add_weight) which has almost the same signature as `tf.get_variable`.\r\n\r\nYou can approximate the TF1 API in two lines with the following:\r\n\r\n```python\r\n>>> def get_variable(name, shape, dtype=tf.float32, initializer=tf.initializers.GlorotNormal()):\r\n...   return tf.Variable(initializer(shape, dtype), name=name)\r\n\r\n>>> w = get_variable(\"w\", [])\r\n<tf.Variable 'w:0' shape=() dtype=float32, numpy=-0.15932713>\r\n```\r\n\r\nWe did discuss this for [Sonnet 2](https://github.com/deepmind/sonnet/tree/v2), but we found that there was a source of confusion since `get_variable` had both creation and re-use (and we only wanted creation). We've also found that for most layers using the `tf.Variable` API directly leads to pretty simple code (example: https://github.com/deepmind/sonnet/blob/v2/sonnet/src/linear.py#L73-L80).\r\n\r\nI think currently we're happy enough without `get_variable` in TF2.", "@jbuckman,\r\nAs per my understanding, [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable) now offers the functionality that you are looking for. Can you please confirm if it is so, so that we can close this issue? Thanks!"]}]