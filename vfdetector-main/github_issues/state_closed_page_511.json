[{"number": 38420, "title": "predict result with SavedModel are not same in Python Api and Java Api", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- OS Platform and Distribution (windows 7): \r\n- TensorFlow installed from (binary): \r\n- TensorFlow version (cpu, 1.15.0): \r\n- Python version: - 3.6.3\r\n\r\n\r\n**Describe the current behavior**\r\n I saved model with Estimator.export_saved_model()\r\n\r\n**I load the model with Python:**\r\n\r\n`\r\nfrom tensorflow.contrib import predictor\r\nself.model = predictor.from_saved_model(self.mode_dir)\r\n\r\n\\\\# predict \r\na_input_ids, a_input_mask, a_segment_ids = self._build_input(a_input)\r\noutput = self.model({'a_input_ids': [a_input_ids],\r\n                             'a_input_mask': [a_input_mask],\r\n                             'a_segment_ids': [a_segment_ids]})\r\n a_output = output['a_output_layer']\r\n`\r\na_output = [[-0.05960074  0.03045687 -0.20487925  0.36802548  0.07898629 -0.35250664\r\n   0.21251363 -0.23284832 -0.30972436 -0.20010747 -0.00487598  0.48967522\r\n   0.1831991  -0.28579575  0.15075627  0.2821794  -0.02628851 -0.05371238\r\n   0.06514908 -0.38573033 -0.34205046  0.3108538  -0.01758813  0.59596956\r\n   0.5169708  -0.46524945 -0.6804516  -0.32393196  0.36948654 -0.46160206\r\n  -0.15634336  0.44929808 -0.39321676 -0.18401513 -0.3726705   0.19476992\r\n   0.33169916 -0.11876976 -0.36055735  0.19275247  0.12676252  0.10232886\r\n   0.63154477 -0.07467962 -0.17044203 -0.47212833  0.26961723 -0.33468968\r\n   0.22710937  0.05272907 -0.6149754  -0.02799183  0.10492884  0.23291017\r\n  -0.20572647 -0.13610545 -0.05362191  0.44776174  0.4095006  -0.43816873\r\n   0.22285426  0.33557323  0.31537503  0.07024186 -0.38216737 -0.12280162\r\n  -0.27534372 -0.41657594 -0.05565406 -0.33100575 -0.29913923  0.00283101\r\n   0.10702493 -0.31459734 -0.2403451   0.42180565 -0.03365724  0.3264306\r\n   0.5190079   0.21016245 -0.3...\r\n\r\n\r\n**I load the model with Java:**\r\n\r\n`\r\nSavedModelBundle bundle = SavedModelBundle.load(exportDir, \"serve\");\r\n\\\\# predict :\r\nThreeTuple<long[][], long[][], long[][]> input = buildInput(sentenceList);\r\n\r\nTensor<?> aInputIdsTensor = Tensor.create(input.param1);\r\nTensor<?> aInputMaskTensor = Tensor.create(input.param2);\r\nTensor<?> aSegmentIdsTensor = Tensor.create(input.param3);\r\nList<Tensor<?>> tensors = this.bundle.session().runner()\r\n                .feed(\"a_input_ids\", aInputIdsTensor)\r\n                .feed(\"a_input_mask\", aInputMaskTensor)\r\n                .feed(\"a_segment_ids\", aSegmentIdsTensor)\r\n                .fetch(\"a_output\").run();\r\nTensor<?> result = tensors.get(0);\r\nfloat[][] outResult = new float[1][dim];\r\n result.copyTo(outResult);\r\nfloat[] a_output = outResult[0]\r\n`\r\na_output = [[-0.018384377, -0.06955972, 0.051764473, 0.015454082, 0.06722302, -0.07839473, 0.02793679, -0.072589695, -0.051289488, -0.039070662, 0.049129114, 0.11508791, 0.0076964055, -0.042025223, 0.05569571, 0.05739251, -0.04230939, -0.05749864, 0.10970076, -0.15183078, -0.08809995, 0.07375819, 0.08044808, 0.12184837, 0.043990605, -0.12923256, -0.056834757, 0.056434825, 0.033050016, -0.022836037, -0.09641873, 0.029169578, -0.0059487675, -0.084053494, -0.095500425, -0.009507669, 0.032067284, -0.026453126, -0.070464775, 0.058229186, 0.016397119, 0.0129444385, 0.07648615, -0.014742567, -0.01920672, -0.10167458, -0.040589973, -0.037671003, -0.02273454, ...\r\n\r\nI use the same version of Tensorflow, same SavedModel, same vocab,same everything but the code language\u3002Apparently, the two result above is not same.\r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nexpected same predict result.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hmm, it's hard to say from the results alone. Is there some randomization being applied in the signature (e.g. dropout)? Are you able to post code that produced the SavedModel? ", "> Hmm, it's hard to say from the results alone. Is there some randomization being applied in the signature (e.g. dropout)? Are you able to post code that produced the SavedModel?\r\n\r\nThere haven't any randomization , I post my simplified code below: \r\n1. first i set output in model_fn():\r\n\r\n`  \r\n\r\n\r\n    def model_fn():\r\n       (a_output_layer) = create_model_a(albert_config, is_training, a_input_ids, a_input_mask,\r\n                     a_segment_ids, use_one_hot_embeddings)\r\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(mode=mode,predictions={\"a_output_layer\": a_output_layer}, scaffold_fn=scaffold_fn)\r\n\r\n   return output_spc\r\n\r\n`\r\n\r\n2.  create placeholder for input, and build feather and receive_tensors:\r\n\r\n` \r\n\r\n     def serving_input_receiver_fn():\r\n\r\n                a_input_ids = tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.max_seq_length], name='a_input_ids')\r\n                a_input_mask = tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.max_seq_length], name='a_input_mask')\r\n                a_segment_ids = tf.placeholder(dtype=tf.int64, shape=[None, FLAGS.max_seq_length], name='a_segment_ids')\r\n                receive_tensors = {'a_input_ids': a_input_ids, 'a_input_mask': a_input_mask, 'a_segment_ids': a_segment_ids}\r\n                features = {'a_input_ids': a_input_ids, 'a_input_mask': a_input_mask, 'a_segment_ids': a_segment_ids}\r\n       return tf.estimator.export.ServingInputReceiver(features, receive_tensors)\r\n`\r\n\r\n3. export SavedModel:\r\n\r\n`\r\n     estimator._export_to_tpu = False\r\n     estimator.export_saved_model(Flags.export_dir, serving_input_receiver_fn)\r\n`\r\n\r\n4. than I got result as below:\r\n-1585645721\r\n--variables\r\n-----variables.data-00000-of-00001\r\n-----variables.index\r\n--saved_model.pb\r\n\r\nI think its normal and correct way to use SavedModel, but I don't knwon why I got different result with Python Api and Java Api when reload model and predict. Please help me.", "Is the memory limited in jvm?", "i also have the same question.\r\nthe same pb file,when load it by python,result is right; if load by java,result is wrong;\r\nthe java code of load pb is used by many model modules,so it is the right;\r\nany one met the same question??", "Hi There,\n\n We are checking to see if you still need help on this, as you are using an older version of tensorflow which is officially considered end of life . We recommend that you upgrade to the latest 2.x version and let us know if the issue still persists in newer versions. Please open a new issue for any help you need against 2.x, and we will get you the right help. \n\n This issue will be closed automatically 7 days from now. If you still need help with this issue, please provide us with more information.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38420\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38420\">No</a>\n"]}, {"number": 38419, "title": "tf.python.keras.layers.preprocessing.index_lookup.IndexLookup cannot be saved and loaded", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\nPlease check this Colab using tf-nightly.\r\nAlso reproduced in 2.2.0rc2\r\nhttps://colab.research.google.com/drive/1QFmqgiGLsHN3shnEuhakAWLrqjT0nD6B\r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): \r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): \r\n- Python version: - Bazel\r\nversion (if compiling from source):\r\n- GCC/Compiler version (if compiling from\r\nsource): \r\n- CUDA/cuDNN version: - GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nAfter saving and loading the model the output of the layer is different.\r\n\r\n**Describe the expected behavior**\r\nThe output should be the same\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\nhttps://colab.research.google.com/drive/1QFmqgiGLsHN3shnEuhakAWLrqjT0nD6B\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Was able to reproduce the issue.Please find the gist [here](https://colab.research.google.com/gist/saikumarchalla/1192a5fd6c38d16bedfec6f2b7c91b54/untitled17.ipynb)", "@saikumarchalla Thank you for the feedback!\r\nAutoKeras is pending on this issue for the next release.\r\nSo it would be good to have an estimated timeline of resolving this issue,\r\nso that our team can better coordinate to figure out a solution on our side.", "Hi @haifeng-jin - because IndexLookup isn't officially exported yet, we need to treat it as a custom symbol. Can you try adding custom_objects={\"IndexLookup\": preprocessing.index_lookup.IndexLookup} to your load_model call? This worked for me at head in nightly.", "@markomernick @k-w-w \r\nIt works! Thank you for the help.\r\nI am closing this issue now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38419\">No</a>\n"]}, {"number": 38418, "title": "save_weight/load_weight", "body": "python 3.7/tensorflow 2.1\r\n\r\nWhen I setup inception resnet v2 model and do training/validation, the accuracy is very high, already reaches 97%, while when I do save_weight, and load_weight to do testing, seems classification doesn't work properly(Cannot classified). Who can tell me what happens?\r\n\r\nHyper-parameter settings:\r\nAdam learning rate = 0.0001\r\nAdam decay =1e-4\r\nL1_REGULIZER(kernel_regularizer) = 0.01\r\nL2_REGULIZER(activity_regularizer) = 0.01\r\nCLASS_NUM = 3\r\n\r\nTest Result\r\nAll Images classified to 0 class\r\n\r\nMain Function\r\n`from __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nimport math\r\nimport os\r\nimport datetime\r\nimport sys\r\n\r\n# User defined packages\r\nfrom configuration import IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS, \\\r\n    EPOCHS, BATCH_SIZE, save_model_root_dir, log_root_dir,  GLOBAL_LEARNING_RATE, \\\r\n    WEIGHT_DECAY, THRESHOLD\r\nfrom prepare_data import generate_datasets, load_and_preprocess_image\r\nfrom models import mobilenet_v1, mobilenet_v2, mobilenet_v3_large, mobilenet_v3_small, \\\r\n    efficientnet, resnext, inception_v4, inception_resnet_v1, inception_resnet_v2, \\\r\n    se_resnet, squeezenet, densenet, shufflenet_v2, resnet\r\nfrom models.model_selection import get_model\r\n\r\n\r\ndef print_model_summary(network):\r\n    network.build(input_shape=(None, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))\r\n    network.summary()\r\n\r\n\r\ndef process_features(features, data_augmentation):\r\n    image_raw = features['image_raw'].numpy()\r\n    image_tensor_list = []\r\n    for image in image_raw:\r\n        image_tensor = load_and_preprocess_image(image, data_augmentation=data_augmentation)\r\n        image_tensor_list.append(image_tensor)\r\n    images = tf.stack(image_tensor_list, axis=0)\r\n    labels = features['label'].numpy()\r\n\r\n    return images, labels\r\n\r\ndef folder_preparation(job_id, product_id):\r\n    # Genearte log file path and precreate log file header\r\n    log_dir = log_root_dir + job_id + \"/\" + product_id  + \"/\"\r\n    if not os.path.exists(log_dir):\r\n        os.makedirs(log_dir)\r\n    file = open(log_dir +\"training_result_step\" + \".log\", \"w\")\r\n    file.write(\"type\\t\")\r\n    file.write(\"timestamp\\t\")\r\n    file.write(\"epoch\\t\")\r\n    file.write(\"step\\t\")\r\n    file.write(\"train_accuracy\\t\")\r\n    file.write(\"predict_labels\\t\")\r\n    file.write(\"actual_labels\\n\")\r\n    file.close()\r\n    \r\n    file = open(log_dir +\"training_result\" + \".log\",\"w\")\r\n    file.write(\"timestamp\\t\")\r\n    file.write(\"epoch\\t\")\r\n    file.write(\"valid accuracy\\n\")\r\n    file.close()\r\n            \r\n    # Generate save model path\r\n    save_model_dir = save_model_root_dir + job_id + \"/\" + product_id + \"/\"\r\n    if not os.path.exists(save_model_dir):\r\n        os.makedirs(save_model_dir)\r\n        \r\n    return log_dir, save_model_dir\r\n\r\ndef main(argv):\r\n    # Need the user to provide system argv for job_id and product_id, it is prepared for frontend calling\r\n    if len(argv) < 2 or len(argv) > 3:\r\n        print(\"ERROR: Format error, refer to the usage: python test.py job_id product_id\")\r\n    elif not argv[1].isdigit():\r\n        print(\"ERROR: Format error, job_id must be in int format\")\r\n    elif not argv[1].isalnum():\r\n        print(\"ERROR: Format error, product_id must be consistent by character or number, without special character\")\r\n    else:\r\n        print(\"INFO: Start training model \" + datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")) \r\n        # GPU settings\r\n        gpus = tf.config.list_physical_devices(\"GPU\")\r\n        if gpus:\r\n            for gpu in gpus:\r\n                tf.config.experimental.set_memory_growth(gpu, True)\r\n\r\n        # Folder generate for log file and model saving\r\n        log_dir, save_model_dir = folder_preparation(argv[1], argv[2])                \r\n             \r\n        # get the dataset\r\n        train_dataset, valid_dataset, test_dataset, train_count, valid_count, test_count = generate_datasets()\r\n\r\n        # create model\r\n        model = get_model()\r\n        print_model_summary(network=model)\r\n    \r\n        # Setup target for validation dataset accuracy, only when the valid_accuracy reachs the threshold the weight can be saved\r\n        threshold = THRESHOLD\r\n\r\n        # define loss calculation\r\n        loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\r\n    \r\n        # Tried RMSprop for optimizer, the result is not so good, finetune the optimizer to Adam or Momentum\r\n        #optimizer = tf.keras.optimizers.RMSprop(learning_rate = GLOBAL_LEARNING_RATE,\r\n        #                                        momentum = MOMENTUM,\r\n        #                                        name = 'rms_optimizer')\r\n        optimizer = tf.keras.optimizers.Adam(lr = GLOBAL_LEARNING_RATE, decay = WEIGHT_DECAY, name = 'adam_optimizer')\r\n\r\n        # Define training KPI\r\n        train_loss = tf.keras.metrics.Mean(name='train_loss')\r\n        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\r\n\r\n        # Define valid KPI\r\n        valid_loss = tf.keras.metrics.Mean(name='valid_loss')\r\n        valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\r\n\r\n        # @tf.function\r\n        def train(image_batch, label_batch):\r\n            with tf.GradientTape() as tape:\r\n                predictions = model(image_batch, training=True)\r\n                loss = loss_object(y_true=label_batch, y_pred=predictions)\r\n            gradients = tape.gradient(loss, model.trainable_variables)\r\n            optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\r\n\r\n            train_loss.update_state(values=loss)\r\n            train_accuracy.update_state(y_true=label_batch, y_pred=predictions)\r\n        \r\n            return predictions.numpy(), tf.math.argmax(predictions, axis =1).numpy()\r\n\r\n        # @tf.function\r\n        def valid(image_batch, label_batch):\r\n            predictions = model(image_batch, training=True)\r\n            v_loss = loss_object(label_batch, predictions)\r\n\r\n            valid_loss.update_state(values=v_loss)\r\n            valid_accuracy.update_state(y_true=label_batch, y_pred=predictions)\r\n        \r\n            return tf.math.argmax(predictions, axis =1).numpy()\r\n\r\n        # start training\r\n        for epoch in range(EPOCHS):\r\n            train_step = 0\r\n            #valid_step = 0\r\n            for features in train_dataset:\r\n                train_step += 1\r\n                images, labels = process_features(features, data_augmentation=False)\r\n                predictions, predict_labels = train(images, labels)\r\n                \r\n                # Print the info on the screen for developer to monitor training detail\r\n                print(\"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}, softmax(logits):{}, \"\r\n                      \"predict_label:{}, target_label:{}\".format(epoch,\r\n                                                                EPOCHS,\r\n                                                                train_step,\r\n                                                                math.ceil(train_count / BATCH_SIZE),\r\n                                                                train_loss.result().numpy(),\r\n                                                                train_accuracy.result().numpy(),\r\n                                                                predictions,\r\n                                                                predict_labels,\r\n                                                                labels))\r\n                \r\n                # Record information into the log file\r\n                file = open(log_dir +\"training_result_step\" + \".log\", \"a\")\r\n                file.write(\"train\\t\")\r\n                file.write(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \"\\t\")\r\n                file.write(str(epoch) + \"\\t\")\r\n                file.write(str(train_step) + \"\\t\")\r\n                file.write(str(train_accuracy.result().numpy()) + \"\\t\")\r\n                file.write(str(predict_labels) + \"\\t\")\r\n                file.write(str(labels) + \"\\n\")\r\n                file.close()\r\n\r\n            for features in valid_dataset:\r\n                #valid_step += 1\r\n                valid_images, valid_labels = process_features(features, data_augmentation=False)\r\n                predict_labels = valid(valid_images, valid_labels)\r\n                \r\n                #file = open(log_dir +\"training_result_step\" + \".log\", \"a\")\r\n                #file.write(\"validation\\t\")\r\n                #file.write(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \"\\t\")\r\n                #file.write(str(epoch) + \"\\t\")\r\n                #file.write(str(valid_step) + \"\\t\")\r\n                #file.write(str(valid_accuracy.result().numpy()) + \"\\t\")\r\n                #file.write(str(predict_labels) + \"\\t\")\r\n                #file.write(str(labels) + \"\\n\")\r\n                #file.close()\r\n                \r\n            # Print the info on the screen for developer to monitor validation result\r\n            print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\r\n                  \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch,\r\n                                                                      EPOCHS,\r\n                                                                      train_loss.result().numpy(),\r\n                                                                      train_accuracy.result().numpy(),\r\n                                                                      valid_loss.result().numpy(),\r\n                                                                      valid_accuracy.result().numpy()))\r\n            # Create log file in txt format, easy for pandas to analysis and for best model selection\r\n            file = open(log_dir +\"training_result\" + \".log\",\"a\")\r\n            file.write(datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + \"\\t\")\r\n            file.write(str(epoch) + \"\\t\")\r\n            file.write(str(valid_accuracy.result().numpy()) + \"\\n\")\r\n            file.close()\r\n            \r\n            valid_accuracy_result = valid_accuracy.result().numpy()\r\n        \r\n            train_loss.reset_states()\r\n            train_accuracy.reset_states()\r\n            valid_loss.reset_states()\r\n            valid_accuracy.reset_states()\r\n            \r\n            # Save the weights for evaluation and prediction only when the valid accuracy is higher than threshold and best ever result\r\n            if valid_accuracy_result >= threshold:\r\n                #model.save_weights(filepath=save_model_dir + str(epoch) + \"/model\", save_format='tf')\r\n                model.save_weights(filepath=save_model_dir+\"model\", save_format='tf')\r\n                # model._set_inputs(inputs=tf.random.normal(shape=(1, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS)))\r\n                # tf.keras.models.save_model(model, save_model_dir + str(epoch), save_format='tf')\r\n                \r\n                # Threshold update\r\n                threshold = valid_accuracy_result\r\n            \r\n        # save the whole model\r\n        # tf.saved_model.save(model, save_model_dir)\r\n\r\n        # convert to tensorflow lite format\r\n        # model._set_inputs(inputs=tf.random.normal(shape=(1, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS)))\r\n        # converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n        # tflite_model = converter.convert()\r\n        # open(\"converted_model.tflite\", \"wb\").write(tflite_model)\r\n    \r\n\r\nif __name__ == '__main__':\r\n    main(sys.argv)\r\n    \r\n\r\n`\r\n\r\nModel Definition\r\n`import tensorflow as tf\r\nfrom models.inception_modules import Stem, ReductionA, BasicConv2D, Conv2DLinear\r\nfrom configuration import NUM_CLASSES, DROPOUT_RATIO, L1_REGULIZER, L2_REGULIZER\r\n\r\n\r\nclass InceptionResNetA(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(InceptionResNetA, self).__init__()\r\n        self.b1_conv = BasicConv2D(filters=32,\r\n                                   kernel_size=(1, 1),\r\n                                   strides=1,\r\n                                   padding=\"same\")\r\n        self.b2_conv1 = BasicConv2D(filters=32,\r\n                                    kernel_size=(1, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b2_conv2 = BasicConv2D(filters=32,\r\n                                    kernel_size=(3, 3),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b3_conv1 = BasicConv2D(filters=32,\r\n                                    kernel_size=(1, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b3_conv2 = BasicConv2D(filters=48,\r\n                                    kernel_size=(3, 3),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b3_conv3 = BasicConv2D(filters=64,\r\n                                    kernel_size=(3, 3),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.conv = Conv2DLinear(filters=384,\r\n                                 kernel_size=(1, 1),\r\n                                 strides=1,\r\n                                 padding=\"same\")\r\n\r\n    def call(self, inputs, training=None, **kwargs):\r\n        b1 = self.b1_conv(inputs, training=training)\r\n        b2 = self.b2_conv1(inputs, training=training)\r\n        b2 = self.b2_conv2(b2, training=training)\r\n        b3 = self.b3_conv1(inputs, training=training)\r\n        b3 = self.b3_conv2(b3, training=training)\r\n        b3 = self.b3_conv3(b3, training=training)\r\n\r\n        x = tf.concat(values=[b1, b2, b3], axis=-1)\r\n        x = self.conv(x, training=training)\r\n\r\n        output = tf.keras.layers.add([x, inputs])\r\n        return tf.nn.relu(output)\r\n\r\n\r\nclass InceptionResNetB(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(InceptionResNetB, self).__init__()\r\n        self.b1_conv = BasicConv2D(filters=192,\r\n                                   kernel_size=(1, 1),\r\n                                   strides=1,\r\n                                   padding=\"same\")\r\n        self.b2_conv1 = BasicConv2D(filters=128,\r\n                                    kernel_size=(1, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b2_conv2 = BasicConv2D(filters=160,\r\n                                    kernel_size=(1, 7),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b2_conv3 = BasicConv2D(filters=192,\r\n                                    kernel_size=(7, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.conv = Conv2DLinear(filters=1152,\r\n                                 kernel_size=(1, 1),\r\n                                 strides=1,\r\n                                 padding=\"same\")\r\n\r\n    def call(self, inputs, training=None, **kwargs):\r\n        b1 = self.b1_conv(inputs, training=training)\r\n        b2 = self.b2_conv1(inputs, training=training)\r\n        b2 = self.b2_conv2(b2, training=training)\r\n        b2 = self.b2_conv3(b2, training=training)\r\n\r\n        x = tf.concat(values=[b1, b2], axis=-1)\r\n        x = self.conv(x, training=training)\r\n\r\n        output = tf.keras.layers.add([x, inputs])\r\n\r\n        return tf.nn.relu(output)\r\n\r\n\r\nclass InceptionResNetC(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(InceptionResNetC, self).__init__()\r\n        self.b1_conv = BasicConv2D(filters=192,\r\n                                   kernel_size=(1, 1),\r\n                                   strides=1,\r\n                                   padding=\"same\")\r\n        self.b2_conv1 = BasicConv2D(filters=192,\r\n                                    kernel_size=(1, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b2_conv2 = BasicConv2D(filters=224,\r\n                                    kernel_size=(1, 3),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b2_conv3 = BasicConv2D(filters=256,\r\n                                    kernel_size=(3, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.conv = Conv2DLinear(filters=2144,\r\n                                 kernel_size=(1, 1),\r\n                                 strides=1,\r\n                                 padding=\"same\")\r\n\r\n    def call(self, inputs, training=None, **kwargs):\r\n        b1 = self.b1_conv(inputs, training=training)\r\n        b2 = self.b2_conv1(inputs, training=training)\r\n        b2 = self.b2_conv2(b2, training=training)\r\n        b2 = self.b2_conv3(b2, training=training)\r\n\r\n        x = tf.concat(values=[b1, b2], axis=-1)\r\n        x = self.conv(x, training=training)\r\n\r\n        output = tf.keras.layers.add([x, inputs])\r\n\r\n        return tf.nn.relu(output)\r\n\r\n\r\nclass ReductionB(tf.keras.layers.Layer):\r\n    def __init__(self):\r\n        super(ReductionB, self).__init__()\r\n        self.b1_maxpool = tf.keras.layers.MaxPool2D(pool_size=(3, 3),\r\n                                                    strides=2,\r\n                                                    padding=\"valid\")\r\n        self.b2_conv1 = BasicConv2D(filters=256,\r\n                                    kernel_size=(1, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b2_conv2 = BasicConv2D(filters=384,\r\n                                    kernel_size=(3, 3),\r\n                                    strides=2,\r\n                                    padding=\"valid\")\r\n        self.b3_conv1 = BasicConv2D(filters=256,\r\n                                    kernel_size=(1, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b3_conv2 = BasicConv2D(filters=288,\r\n                                    kernel_size=(3, 3),\r\n                                    strides=2,\r\n                                    padding=\"valid\")\r\n        self.b4_conv1 = BasicConv2D(filters=256,\r\n                                    kernel_size=(1, 1),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b4_conv2 = BasicConv2D(filters=288,\r\n                                    kernel_size=(3, 3),\r\n                                    strides=1,\r\n                                    padding=\"same\")\r\n        self.b4_conv3 = BasicConv2D(filters=320,\r\n                                    kernel_size=(3, 3),\r\n                                    strides=2,\r\n                                    padding=\"valid\")\r\n\r\n    def call(self, inputs, training=None, **kwargs):\r\n        b1 = self.b1_maxpool(inputs)\r\n\r\n        b2 = self.b2_conv1(inputs, training=training)\r\n        b2 = self.b2_conv2(b2, training=training)\r\n\r\n        b3 = self.b3_conv1(inputs, training=training)\r\n        b3 = self.b3_conv2(b3, training=training)\r\n\r\n        b4 = self.b4_conv1(inputs, training=training)\r\n        b4 = self.b4_conv2(b4, training=training)\r\n        b4 = self.b4_conv3(b4, training=training)\r\n\r\n        return tf.concat(values=[b1, b2, b3, b4], axis=-1)\r\n\r\n\r\ndef build_inception_resnet_a(n):\r\n    block = tf.keras.Sequential()\r\n    for _ in range(n):\r\n        block.add(InceptionResNetA())\r\n    return block\r\n\r\n\r\ndef build_inception_resnet_b(n):\r\n    block = tf.keras.Sequential()\r\n    for _ in range(n):\r\n        block.add(InceptionResNetB())\r\n    return block\r\n\r\n\r\ndef build_inception_resnet_c(n):\r\n    block = tf.keras.Sequential()\r\n    for _ in range(n):\r\n        block.add(InceptionResNetC())\r\n    return block\r\n\r\n\r\nclass InceptionResNetV2(tf.keras.Model):\r\n    def __init__(self):\r\n        super(InceptionResNetV2, self).__init__()\r\n        self.stem = Stem()\r\n        self.inception_resnet_a = build_inception_resnet_a(5)\r\n        self.reduction_a = ReductionA(k=256, l=256, m=384, n=384)\r\n        self.inception_resnet_b = build_inception_resnet_b(10)\r\n        self.reduction_b = ReductionB()\r\n        self.inception_resnet_c = build_inception_resnet_c(5)\r\n        self.avgpool = tf.keras.layers.AveragePooling2D(pool_size=(8, 8))\r\n        self.dropout = tf.keras.layers.Dropout(rate=DROPOUT_RATIO)\r\n        self.flat = tf.keras.layers.Flatten()\r\n        self.fc = tf.keras.layers.Dense(units=NUM_CLASSES,\r\n                                        activation=tf.keras.activations.softmax,\r\n                                        kernel_regularizer=tf.keras.regularizers.l1(L1_REGULIZER),\r\n                                        activity_regularizer=tf.keras.regularizers.l2(L2_REGULIZER)\r\n                                       )\r\n\r\n    def call(self, inputs, training=None, mask=None):\r\n        x = self.stem(inputs, training=training)\r\n        x = self.inception_resnet_a(x, training=training)\r\n        x = self.reduction_a(x, training=training)\r\n        x = self.inception_resnet_b(x, training=training)\r\n        x = self.reduction_b(x, training=training)\r\n        x = self.inception_resnet_c(x, training=training)\r\n        x = self.avgpool(x)\r\n        x = self.dropout(x, training=training)\r\n        x = self.flat(x)\r\n        x = self.fc(x)\r\n\r\n        return x`", "comments": ["@rogeryuchao \r\n\r\nCan you please provide colab link or simple standalone code with proper indentation to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@rogeryuchao \r\n\r\nAny update on this issue please. Thanks!", "@ravikyram\r\nHi, sorry for the late reply, please kindly help to check the follow repo for the code framework.\r\nBecause of the china gov policy, I cannot launch colab so sorry for the inconvenience.\r\nThank you~\r\n\r\n`git@github.com:rogeryuchao/general_cnn.git`\r\n", "Will it be possible to share the simple standalone code with proper indentation to reproduce the issue reported here.It helps us in localizing the issue faster.Thanks!", "I cannot provide the image I use to you, because it is huge files. But I can let you know the steps:\r\n\r\n1. Put the `class_name[folder_name]/image_files.jpgs` into `original_data_set` folder\r\n2. In shell, Run `python split_dataset.py`\r\n3. In shell, Run `python to_tfrecord.py`\r\n4. In configuration.py, change `THRESHOLD` function to 0.0\r\n5. In configuration.py, best to change `save_every_n_epoch` to 1\r\n6. In shell, Run `python train.py 1 ABCD`\r\n7. Wait for the model to finish 1 epoch and save the weight under `saved_model/1/ABCD` folder\r\n8. In shell, Run `python evaluate.py 1 ABCD` to check whether the weight works or not(**_In my case, it doesn't work_**)\r\n\r\np.s. If you want to check the model definition, please refer to `model `folder\r\n\r\n[general_cnn-master.zip](https://github.com/tensorflow/tensorflow/files/4516269/general_cnn-master.zip)\r\n", "@rogeryuchao This is not Build/Installation or Bug/Performance issue. Please post this kind of support questions at [Stackoverflow](https://stackoverflow.com/questions/tagged/tensorflow). There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks!\r\n\r\nPlease post this issue in Stackoverflow and provide a simple standalone code. Either community and/or us will respond to you and resolve this support issue. Thanks! "]}, {"number": 38417, "title": "r2.2-rc3 cherry-pick request: Fix a bug that profile XLA gpu crashes OOM.", "body": "Remove data redundancy by remove xla expression in trunk annotation.\r\nDon't create device tracer when there's no GPU in the system.", "comments": ["Thanks!"]}, {"number": 38416, "title": "tf.keras: Model parameters suddenly updated to 'nan' during back propagation when training", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): \r\n- CUDA/cuDNN version: 10.1/7.6.5\r\n- GPU model and memory: Nvidia 1080Ti\r\n\r\n**Describe the current behavior**\r\nI was trying to train a small net similar to PNet of MTCNN and I wrote a custom loss to test if it is working. During the training, after several epochs the model weights and loss became 'nan'. \r\nI test the training procedure one epoch by one and found that the last epoch that gives a normal loss (0.0814), also outputs a model with all parameters of 'nan'. Thus I think when giving a normal loss, the backward propagation has something wrong and gives the model a 'nan' update.\r\n\r\nWhat I have done to rule out some other possibilities:\r\n(1) Check & clean the data: \r\nMy data set is:\r\nX: images of shape (12, 12, 3);\r\nY: label, box regression coords & 6-landmark regression coords concatenated together of shape (17, ).\r\nFor the label, it could be 1, -1, 0, -2 where only labels 1 and 0 will participate in calculating the custom loss I wrote myself.\r\nFor the roi & landmark coords, they all belong to [-1, 1].\r\nFor the image data, it will be processed as: (x - 127.5) / 128. before being sent into the training stream. \r\nI tried both the TFRecords dataflow & numpy array as the input for training.\r\n\r\n(2) Add BatchNormalization layer, add L2-Norm to the weights, use Xavier initialization and pick a smaller learning rate (from 0.001 to 0.0001) to avoid problems like gradient exploding. \r\n\r\n(3) Replace the custom loss I wrote myself with 'mse'.\r\n\r\nAll the three changes made did not fix the 'nan' loss thing.\r\n\r\n**Describe the expected behavior**\r\nThe training procedure should work well.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n```\r\ndef pnet_train1(train_with_landmark = False):\r\n    \r\n    X = Input(shape = (12, 12, 3), name = 'Pnet_input')\r\n    \r\n    M = Conv2D(10, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv1')(X)\r\n    M = PReLU(shared_axes = [1, 2], name = 'Pnet_prelu1')(M)\r\n    M = MaxPooling2D(pool_size = 2, name = 'Pnet_maxpool1')(M) # default 'pool_size' is 2!!! \r\n    \r\n    M = Conv2D(16, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv2')(M)\r\n    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu2')(M)\r\n    \r\n    M = Conv2D(32, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv3')(M)\r\n    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu3')(M)\r\n    \r\n    Classifier_conv = Conv2D(1, 1, activation = 'sigmoid', name = 'Pnet_classifier_conv', kernel_initializer = glorot_normal)(M)\r\n    Bbox_regressor_conv = Conv2D(4, 1, name = 'Pnet_bbox_regressor_conv', kernel_initializer = glorot_normal)(M)\r\n    Landmark_regressor_conv = Conv2D(12, 1, name = 'Pnet_landmark_regressor_conv', kernel_initializer = glorot_normal)(M)\r\n    \r\n    Classifier = Reshape((1, ), name = 'Pnet_classifier')(Classifier_conv)\r\n    Bbox_regressor = Reshape((4, ), name = 'Pnet_bbox_regressor')(Bbox_regressor_conv) \r\n    if train_with_landmark: \r\n        Landmark_regressor = Reshape((12, ), name = 'Pnet_landmark_regressor')(Landmark_regressor_conv)\r\n        Pnet_output = Concatenate()([Classifier, Bbox_regressor, Landmark_regressor]) \r\n        model = Model(X, Pnet_output) \r\n    else:\r\n        Pnet_output = Concatenate()([Classifier, Bbox_regressor])\r\n        model = Model(X, Pnet_output)\r\n    \r\n    return model\r\n\r\ndef pnet_train2(train_with_landmark = False):\r\n\r\n    X = Input(shape = (12, 12, 3), name = 'Pnet_input')\r\n\r\n    M = Conv2D(10, 3, strides = 1, padding = 'valid', use_bias = False, kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv1')(X)\r\n    M = BatchNormalization(axis = -1, name = 'Pnet_bn1')(M)\r\n    M = PReLU(shared_axes = [1, 2], name = 'Pnet_prelu1')(M)\r\n    M = MaxPooling2D(pool_size = 2, name = 'Pnet_maxpool1')(M) # default 'pool_size' is 2!!! \r\n\r\n    M = Conv2D(16, 3, strides = 1, padding = 'valid', use_bias = False, kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv2')(M)\r\n    M = BatchNormalization(axis = -1, name = 'Pnet_bn2')(M)\r\n    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu2')(M)\r\n\r\n    M = Conv2D(32, 3, strides = 1, padding = 'valid', use_bias = False, kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv3')(M)\r\n    M = BatchNormalization(axis = -1, name = 'Pnet_bn3')(M)\r\n    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu3')(M)\r\n\r\n    Classifier_conv = Conv2D(1, 1, activation = 'sigmoid', name = 'Pnet_classifier_conv', kernel_initializer = glorot_normal)(M)\r\n    Bbox_regressor_conv = Conv2D(4, 1, name = 'Pnet_bbox_regressor_conv', kernel_initializer = glorot_normal)(M)\r\n    Landmark_regressor_conv = Conv2D(12, 1, name = 'Pnet_landmark_regressor_conv', kernel_initializer = glorot_normal)(M)\r\n\r\n    Classifier = Reshape((1, ), name = 'Pnet_classifier')(Classifier_conv)\r\n    Bbox_regressor = Reshape((4, ), name = 'Pnet_bbox_regressor')(Bbox_regressor_conv) \r\n    if train_with_landmark: \r\n        Landmark_regressor = Reshape((12, ), name = 'Pnet_landmark_regressor')(Landmark_regressor_conv)\r\n        Pnet_output = Concatenate()([Classifier, Bbox_regressor, Landmark_regressor]) \r\n        model = Model(X, Pnet_output) \r\n    else:\r\n        Pnet_output = Concatenate()([Classifier, Bbox_regressor])\r\n        model = Model(X, Pnet_output)\r\n\r\n    return model\r\n\r\n# Here just check the the first classify loss. \r\ndef custom_loss(y_true, y_pred):\r\n        \r\n    zero_index = K.zeros_like(y_true[:, 0]) \r\n    ones_index = K.ones_like(y_true[:, 0]) \r\n    \r\n    labels = y_true[:, 0] \r\n    class_preds = y_pred[:, 0] \r\n    bi_crossentropy_loss = -labels * K.log(class_preds) - (1 - labels) * K.log(1 - class_preds) \r\n    \r\n    classify_valid_index = tf.where(K.less(y_true[:, 0], 0), zero_index, ones_index) \r\n    classify_keep_num = K.cast(tf.cast(tf.reduce_sum(classify_valid_index), tf.float32) * 0.7, dtype = tf.int32) \r\n    \r\n    classify_loss_sum = bi_crossentropy_loss * tf.cast(classify_valid_index, bi_crossentropy_loss.dtype) \r\n    classify_loss_sum_filtered, _ = tf.nn.top_k(classify_loss_sum, k = classify_keep_num) \r\n    classify_loss = tf.where(K.equal(classify_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(classify_loss_sum_filtered)) \r\n    \r\n    loss = classify_loss \r\n    \r\n    return loss\r\n```\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n![1](https://user-images.githubusercontent.com/31966022/78958654-4d557100-7b1b-11ea-8f5b-52406fb76111.png)", "comments": ["@TMaysGGS,\r\nIn order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with the dataset you are using . Thanks!", "> @TMaysGGS,\r\n> In order to expedite the trouble-shooting process, could you please provide the complete code to reproduce the issue reported here along with the dataset you are using . Thanks!\r\n\r\nHi, @amahendrakar , \r\nThanks for your reply. The complete version of my codes is the training pnet part of a Keras ver. MTCNN in my repo [MTCNN-Keras](https://github.com/TMaysGGS/MTCNN-Keras). \r\n\r\nThe main codes are below:\r\n```\r\nimport os\r\nimport sys\r\nimport argparse\r\nimport tensorflow as tf\r\nimport tensorflow.python.keras.backend as K\r\nfrom tensorflow.python.keras.optimizers import SGD \r\nfrom tensorflow.python.keras.optimizer_v2.adam import Adam\r\n\r\nsys.path.append(r'../')\r\nfrom MTCNN_models import pnet_train1, rnet, onet\r\n\r\ndef main(args):\r\n    \r\n    IMG_SIZE = args.IMG_SIZE\r\n    if args.USE_PRETRAINED_MODEL == 0:\r\n        USE_PRETRAINED_MODEL = False\r\n    else:\r\n        USE_PRETRAINED_MODEL = True\r\n    BATCH_SIZE = 1792\r\n    EPOCHS = 100\r\n    DATA_COMPOSE_RATIO = [1 / 7., 3 / 7., 1 / 7., 2 / 7.]\r\n    SAMPLE_KEEP_RATIO = 0.7\r\n    if args.OPTIMIZER == 'sgd':\r\n        OPTIMIZER = SGD\r\n    else:\r\n        OPTIMIZER = Adam\r\n    if IMG_SIZE == 12:\r\n        print('Training PNet')\r\n        loss_weights = [1., 0.5, 0.5]\r\n        model = pnet_train1(train_with_landmark = not args.TRAIN_WITHOUT_LANDMARK)\r\n        model.summary()\r\n        if USE_PRETRAINED_MODEL:\r\n            model.load_weights('../Models/PNet_train.h5')\r\n    elif IMG_SIZE == 24:\r\n        print('Training RNet')\r\n        loss_weights = [1., 0.5, 0.5]\r\n        model = rnet(training = True, train_with_landmark = not args.TRAIN_WITHOUT_LANDMARK)\r\n        if USE_PRETRAINED_MODEL:\r\n            model.load_weights('../Models/RNet_train.h5')\r\n    elif IMG_SIZE == 48:\r\n        print('Training ONet')\r\n        loss_weights = [1., 0.5, 1.]\r\n        model = onet(training = True, train_with_landmark = not args.TRAIN_WITHOUT_LANDMARK)\r\n        if USE_PRETRAINED_MODEL:\r\n            model.load_weights('../Models/ONet_train.h5')\r\n    else:\r\n        raise Exception(\"IMG_SIZE must be one of 12, 24 and 48. \")\r\n    \r\n    TFRECORDS_DIR = os.path.join(r'../Data', str(IMG_SIZE))\r\n    POS_TFRECORDS_PATH_LIST = []\r\n    NEG_TFRECORDS_PATH_LIST = []\r\n    PART_TFRECORDS_PATH_LIST = []\r\n    LANDMARK_TFRECORDS_PATH_LIST = []\r\n    for file_name in os.listdir(TFRECORDS_DIR):\r\n        if len(file_name) > 9 and file_name[-9: ] == '.tfrecord':\r\n            if 'pos' in file_name:\r\n                POS_TFRECORDS_PATH_LIST.append(os.path.join(TFRECORDS_DIR, file_name))\r\n            elif 'neg' in file_name:\r\n                NEG_TFRECORDS_PATH_LIST.append(os.path.join(TFRECORDS_DIR, file_name))\r\n            elif 'part' in file_name:\r\n                PART_TFRECORDS_PATH_LIST.append(os.path.join(TFRECORDS_DIR, file_name))\r\n            elif 'landmark' in file_name:\r\n                LANDMARK_TFRECORDS_PATH_LIST.append(os.path.join(TFRECORDS_DIR, file_name))\r\n    \r\n    raw_pos_dataset = tf.data.TFRecordDataset(POS_TFRECORDS_PATH_LIST)\r\n    raw_neg_dataset = tf.data.TFRecordDataset(NEG_TFRECORDS_PATH_LIST)\r\n    raw_part_dataset = tf.data.TFRecordDataset(PART_TFRECORDS_PATH_LIST)\r\n    raw_landmark_dataset = tf.data.TFRecordDataset(LANDMARK_TFRECORDS_PATH_LIST)\r\n    \r\n    image_feature_description = {\r\n        'height': tf.io.FixedLenFeature([], tf.int64),\r\n        'width': tf.io.FixedLenFeature([], tf.int64),\r\n        'depth': tf.io.FixedLenFeature([], tf.int64),\r\n        'info': tf.io.FixedLenFeature([17], tf.float32),\r\n        'image_raw': tf.io.FixedLenFeature([], tf.string),\r\n        }\r\n    \r\n    def _read_tfrecord(serialized_example):\r\n        \r\n        example = tf.io.parse_single_example(serialized_example, image_feature_description)\r\n        \r\n        img = tf.image.decode_jpeg(example['image_raw'], channels = 3) # RGB rather than BGR!!! \r\n        img = (tf.cast(img, tf.float32) - 127.5) / 128.\r\n        img_shape = [example['height'], example['width'], example['depth']]\r\n        img = tf.reshape(img, img_shape)\r\n        \r\n        info = example['info']\r\n        \r\n        return img, info\r\n    \r\n    parsed_pos_dataset = raw_pos_dataset.map(_read_tfrecord)\r\n    parsed_neg_dataset = raw_neg_dataset.map(_read_tfrecord)\r\n    parsed_part_dataset = raw_part_dataset.map(_read_tfrecord)\r\n    parsed_landmark_dataset = raw_landmark_dataset.map(_read_tfrecord)\r\n    \r\n    ds = tf.data.experimental.sample_from_datasets([parsed_pos_dataset.repeat(), \r\n                                                    parsed_neg_dataset.repeat(), \r\n                                                    parsed_part_dataset.repeat(), \r\n                                                    parsed_landmark_dataset.repeat()], DATA_COMPOSE_RATIO)\r\n    \r\n    ds = ds.batch(BATCH_SIZE)\r\n    ds = ds.prefetch(28672)\r\n    \r\n    '''Building custom loss/cost function'''\r\n    def custom_loss(y_true, y_pred, loss_weights = loss_weights):\r\n        \r\n        zero_index = K.zeros_like(y_true[:, 0]) \r\n        ones_index = K.ones_like(y_true[:, 0]) \r\n        \r\n        labels = y_true[:, 0] \r\n        class_preds = y_pred[:, 0] \r\n        bi_crossentropy_loss = -labels * K.log(class_preds) - (1 - labels) * K.log(1 - class_preds) \r\n        \r\n        classify_valid_index = tf.where(K.less(y_true[:, 0], 0), zero_index, ones_index) \r\n        classify_keep_num = K.cast(tf.cast(tf.reduce_sum(classify_valid_index), tf.float32) * SAMPLE_KEEP_RATIO, dtype = tf.int32) \r\n        \r\n        classify_loss_sum = bi_crossentropy_loss * tf.cast(classify_valid_index, bi_crossentropy_loss.dtype) \r\n        classify_loss_sum_filtered, _ = tf.nn.top_k(classify_loss_sum, k = classify_keep_num) \r\n        classify_loss = tf.where(K.equal(classify_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(classify_loss_sum_filtered)) \r\n        \r\n        rois = y_true[:, 1: 5] \r\n        roi_preds = y_pred[:, 1: 5] \r\n        roi_raw_mean_square_error = K.sum(K.square(rois - roi_preds), axis = 1)\r\n        \r\n        roi_valid_index = tf.where(K.equal(K.abs(y_true[:, 0]), 1), ones_index, zero_index) \r\n        roi_keep_num = K.cast(tf.reduce_sum(roi_valid_index), dtype = tf.int32) \r\n        \r\n        roi_valid_mean_square_error = roi_raw_mean_square_error * tf.cast(roi_valid_index, roi_raw_mean_square_error.dtype)\r\n        roi_filtered_mean_square_error, _ = tf.nn.top_k(roi_valid_mean_square_error, k = roi_keep_num) \r\n        roi_loss = tf.where(K.equal(roi_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(roi_filtered_mean_square_error)) \r\n        \r\n        pts = y_true[:, 5: 17] \r\n        pt_preds = y_pred[:, 5: 17] \r\n        pts_raw_mean_square_error  = K.sum(K.square(pts - pt_preds), axis = 1) # mse \r\n        \r\n        pts_valid_index = tf.where(K.equal(y_true[:, 0], -2), ones_index, zero_index) \r\n        pts_keep_num = K.cast(tf.reduce_sum(pts_valid_index), dtype = tf.int32) \r\n        \r\n        pts_valid_mean_square_error = pts_raw_mean_square_error * tf.cast(pts_valid_index, tf.float32) \r\n        pts_filtered_mean_square_error, _ = tf.nn.top_k(pts_valid_mean_square_error, k = pts_keep_num) \r\n        pts_loss = tf.where(K.equal(pts_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(pts_filtered_mean_square_error)) \r\n        \r\n        loss = classify_loss * loss_weights[0] + roi_loss * loss_weights[1] + pts_loss * loss_weights[2]\r\n        \r\n        return loss \r\n        \r\n    '''Training'''\r\n    lr = args.LEARNING_RATE\r\n    model.compile(optimizer = OPTIMIZER(lr = lr), loss = custom_loss)\r\n    model.fit(ds, steps_per_epoch = 1636, epochs = EPOCHS, validation_data = ds, validation_steps = 1636)\r\n    model.save(r'../Models/PNet_trained_without_lm.h5')\r\n\r\ndef parse_arguments(argv):\r\n    \r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--IMG_SIZE', type = int, help = 'The input image size', default = 0)\r\n    parser.add_argument('--USE_PRETRAINED_MODEL', type = int, help = 'Use pretrained model or not', default = 0)\r\n    parser.add_argument('--TRAIN_WITHOUT_LANDMARK', action = 'store_false', help = 'Train model with landmark regression or not')\r\n    parser.add_argument('--OPTIMIZER', type = str, help = 'The training optimizer', default = 'adam')\r\n    parser.add_argument('--LEARNING_RATE', type = float, help = 'The initial learning rate', default = 0.001)\r\n    \r\n    return parser.parse_args(argv)\r\n\r\nif __name__ == '__main__':\r\n    \r\n    main(parse_arguments(sys.argv[1:]))\r\n```\r\nIf you need the complete steps, the steps are: \r\n1. Use Data_Augmentation/00.1_WIDER_FACE_annotation_intergration.py or Data_Augmentation/00.1_wider_face_add_lm_annotation_integration.py to generate training data annotation for face detection; use Data_Augmentation/00.1_CelebA_data_marking_chin.py to generate training data annotation for landmark regression. \r\n2. Use Data_Augmentation/00.2_CelebA_annotation_integration.py to integrate the extra chin point annotation for landmark regression. \r\n3. Use Data_Augmentation/01_WIDER_FACE_Pnet_data_aug.py or Data_Augmentation/01_WIDER_FACE_add_lm_annotation_Pnet_data_aug.py to generate augmented images for face detection.\r\n4. Use Data_Augmentation/01_CelebA_landmark_data_gen.py to generate augmented images for landmark regression. \r\n5. Use Data_Augmentation/02_TFRecords_generation.py to generate TFRecords for training. \r\n6. Use Training/Training_with_TFRecords_v2.py to train PNet and the error occurs during the training.\r\n\r\nIf you need extra information for this, please inform me and I will reply ASAP. Thanks.\r\n", "@TMaysGGS,\r\nSorry for the delayed response. Is it possible for you to provide a minimal code sample to replicate the issue reported here? Thanks!", "@amahendrakar ,\r\nBelow is the minimal code to reproduce the issue:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.python.keras.backend as K\r\nfrom tensorflow.python.keras.optimizer_v2.adam import Adam\r\nfrom tensorflow.keras.layers import Input, Conv2D, PReLU, MaxPooling2D, Reshape, Concatenate\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.initializers import glorot_normal\r\nfrom tensorflow.keras.regularizers import l2\r\n\r\nSAMPLE_KEEP_RATIO = 0.7\r\nLOSS_WEIGHTS = [1., 0.5, 0.5]\r\n\r\n# Create model\r\ndef pnet(train_with_landmark = False):\r\n    \r\n    X = Input(shape = (12, 12, 3), name = 'Pnet_input')\r\n    \r\n    M = Conv2D(10, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv1')(X)\r\n    M = PReLU(shared_axes = [1, 2], name = 'Pnet_prelu1')(M)\r\n    M = MaxPooling2D(pool_size = 2, name = 'Pnet_maxpool1')(M) \r\n    \r\n    M = Conv2D(16, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv2')(M)\r\n    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu2')(M)\r\n    \r\n    M = Conv2D(32, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal, kernel_regularizer = l2(0.00001), name = 'Pnet_conv3')(M)\r\n    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu3')(M)\r\n    \r\n    Classifier_conv = Conv2D(1, 1, activation = 'sigmoid', name = 'Pnet_classifier_conv', kernel_initializer = glorot_normal)(M)\r\n    Bbox_regressor_conv = Conv2D(4, 1, name = 'Pnet_bbox_regressor_conv', kernel_initializer = glorot_normal)(M)\r\n    Landmark_regressor_conv = Conv2D(12, 1, name = 'Pnet_landmark_regressor_conv', kernel_initializer = glorot_normal)(M)\r\n    \r\n    Classifier = Reshape((1, ), name = 'Pnet_classifier')(Classifier_conv)\r\n    Bbox_regressor = Reshape((4, ), name = 'Pnet_bbox_regressor')(Bbox_regressor_conv) \r\n    if train_with_landmark: \r\n        Landmark_regressor = Reshape((12, ), name = 'Pnet_landmark_regressor')(Landmark_regressor_conv)\r\n        Pnet_output = Concatenate()([Classifier, Bbox_regressor, Landmark_regressor]) \r\n        model = Model(X, Pnet_output) \r\n    else:\r\n        Pnet_output = Concatenate()([Classifier, Bbox_regressor])\r\n        model = Model(X, Pnet_output)\r\n    \r\n    return model\r\n\r\nmodel = pnet(train_with_landmark = True)\r\n\r\n# Create data\r\nx1 = np.random.rand(1792, 12, 12, 3)\r\ny1 = np.ones((1792, 17), dtype = np.float32) * (-1)\r\ny1[: 256, 0] = 1\r\ny1[256: 1024, 0] = 0\r\ny1[1024: 1280, 0] = -1\r\ny1[1280: , 0] = -2\r\nfor i in range(1792):\r\n    if y1[i][0] == 1 or y1[i][0] == -1:\r\n        y1[i][1: 5] = np.random.rand(4, )\r\n    elif y1[i][0] == -2:\r\n        y1[i][5: ] = np.random.rand(12, )\r\n\r\n# Create loss\r\ndef custom_loss(y_true, y_pred, loss_weights = LOSS_WEIGHTS):\r\n    \r\n    zero_index = K.zeros_like(y_true[:, 0]) \r\n    ones_index = K.ones_like(y_true[:, 0]) \r\n    \r\n    # Classifier\r\n    labels = y_true[:, 0] \r\n    class_preds = y_pred[:, 0] \r\n    bi_crossentropy_loss = -labels * K.log(class_preds) - (1 - labels) * K.log(1 - class_preds) \r\n    \r\n    classify_valid_index = tf.where(K.less(y_true[:, 0], 0), zero_index, ones_index) \r\n    classify_keep_num = K.cast(tf.cast(tf.reduce_sum(classify_valid_index), tf.float32) * SAMPLE_KEEP_RATIO, dtype = tf.int32) \r\n    # For classification problem, only pick 70% of the valid samples. \r\n    \r\n    classify_loss_sum = bi_crossentropy_loss * tf.cast(classify_valid_index, bi_crossentropy_loss.dtype) \r\n    classify_loss_sum_filtered, _ = tf.nn.top_k(classify_loss_sum, k = classify_keep_num) \r\n    classify_loss = tf.where(K.equal(classify_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(classify_loss_sum_filtered)) \r\n\r\n    loss = classify_loss * loss_weights[0] # + roi_loss * loss_weights[1] + pts_loss * loss_weights[2]\r\n    \r\n    return loss \r\n    \r\n# Train\r\nmodel.compile(optimizer = Adam(lr = 0.001), loss = custom_loss)\r\nmodel.fit(x1, y1, batch_size = 896, epochs = 200, shuffle = True)\r\nmodel.save(r'../Models/PNet_train.h5')\r\n```", "@TMaysGGS,\r\nI did not observe any nan values while running the code with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/9cf66344fe1c93248508ec3f011bf455/38416-2-1.ipynb) and [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/2254b7122f8a08ca71aa6f898c0b8eca/38416-2-2.ipynb). \r\nHowever, the `model.save` line throws an error stating `TypeError: get_config() missing 1 required positional argument: 'self'`. Please find the attached gist. Thanks!", "> @TMaysGGS,\r\n> I did not observe any nan values while running the code with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/9cf66344fe1c93248508ec3f011bf455/38416-2-1.ipynb) and [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/2254b7122f8a08ca71aa6f898c0b8eca/38416-2-2.ipynb).\r\n> However, the `model.save` line throws an error stating `TypeError: get_config() missing 1 required positional argument: 'self'`. Please find the attached gist. Thanks!\r\n\r\n@amahendrakar ,\r\nThanks for your reply. I re-ran my codes and found the 'nan' loss occurred on epoch 345. Please change the line `model.fit(x1, y1, batch_size = 896, epochs = 200, shuffle = True)` to `model.fit(x1, y1, batch_size = 896, epochs = 400, shuffle = True)` and the 'nan' loss should occur when the loss is reduced to around 0.0178.\r\nAnd I also slightly modified a little on the initialization of the model so the 'model.save' error will not occur. Please use the following code:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.python.keras.backend as K\r\nfrom tensorflow.python.keras.optimizer_v2.adam import Adam\r\nfrom tensorflow.keras.layers import Input, Conv2D, PReLU, MaxPooling2D, Reshape, Concatenate\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.initializers import glorot_normal\r\nfrom tensorflow.keras.regularizers import l2\r\n\r\nSAMPLE_KEEP_RATIO = 0.7\r\nLOSS_WEIGHTS = [1., 0.5, 0.5]\r\n\r\n# Create model\r\ndef pnet(train_with_landmark = False):\r\n    \r\n    X = Input(shape = (12, 12, 3), name = 'Pnet_input')\r\n    \r\n    M = Conv2D(10, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal(), kernel_regularizer = l2(0.00001), name = 'Pnet_conv1')(X)\r\n    M = PReLU(shared_axes = [1, 2], name = 'Pnet_prelu1')(M)\r\n    M = MaxPooling2D(pool_size = 2, name = 'Pnet_maxpool1')(M) \r\n    \r\n    M = Conv2D(16, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal(), kernel_regularizer = l2(0.00001), name = 'Pnet_conv2')(M)\r\n    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu2')(M)\r\n    \r\n    M = Conv2D(32, 3, strides = 1, padding = 'valid', kernel_initializer = glorot_normal(), kernel_regularizer = l2(0.00001), name = 'Pnet_conv3')(M)\r\n    M = PReLU(shared_axes= [1, 2], name = 'Pnet_prelu3')(M)\r\n    \r\n    Classifier_conv = Conv2D(1, 1, activation = 'sigmoid', name = 'Pnet_classifier_conv', kernel_initializer = glorot_normal())(M)\r\n    Bbox_regressor_conv = Conv2D(4, 1, name = 'Pnet_bbox_regressor_conv', kernel_initializer = glorot_normal())(M)\r\n    Landmark_regressor_conv = Conv2D(12, 1, name = 'Pnet_landmark_regressor_conv', kernel_initializer = glorot_normal())(M)\r\n    \r\n    Classifier = Reshape((1, ), name = 'Pnet_classifier')(Classifier_conv)\r\n    Bbox_regressor = Reshape((4, ), name = 'Pnet_bbox_regressor')(Bbox_regressor_conv) \r\n    if train_with_landmark: \r\n        Landmark_regressor = Reshape((12, ), name = 'Pnet_landmark_regressor')(Landmark_regressor_conv)\r\n        Pnet_output = Concatenate()([Classifier, Bbox_regressor, Landmark_regressor]) \r\n        model = Model(X, Pnet_output) \r\n    else:\r\n        Pnet_output = Concatenate()([Classifier, Bbox_regressor])\r\n        model = Model(X, Pnet_output)\r\n    \r\n    return model\r\n\r\nmodel = pnet(train_with_landmark = True)\r\n\r\n# Create data\r\nx1 = np.random.rand(1792, 12, 12, 3)\r\ny1 = np.ones((1792, 17), dtype = np.float32) * (-1)\r\ny1[: 256, 0] = 1\r\ny1[256: 1024, 0] = 0\r\ny1[1024: 1280, 0] = -1\r\ny1[1280: , 0] = -2\r\nfor i in range(1792):\r\n    if y1[i][0] == 1 or y1[i][0] == -1:\r\n        y1[i][1: 5] = np.random.rand(4, )\r\n    elif y1[i][0] == -2:\r\n        y1[i][5: ] = np.random.rand(12, )\r\n\r\n# Create loss\r\ndef custom_loss(y_true, y_pred, loss_weights = LOSS_WEIGHTS):\r\n    \r\n    zero_index = K.zeros_like(y_true[:, 0]) \r\n    ones_index = K.ones_like(y_true[:, 0]) \r\n    \r\n    # Classifier\r\n    labels = y_true[:, 0] \r\n    class_preds = y_pred[:, 0] \r\n    bi_crossentropy_loss = -labels * K.log(class_preds) - (1 - labels) * K.log(1 - class_preds) \r\n    \r\n    classify_valid_index = tf.where(K.less(y_true[:, 0], 0), zero_index, ones_index) \r\n    classify_keep_num = K.cast(tf.cast(tf.reduce_sum(classify_valid_index), tf.float32) * SAMPLE_KEEP_RATIO, dtype = tf.int32) \r\n    # For classification problem, only pick 70% of the valid samples. \r\n    \r\n    classify_loss_sum = bi_crossentropy_loss * tf.cast(classify_valid_index, bi_crossentropy_loss.dtype) \r\n    classify_loss_sum_filtered, _ = tf.nn.top_k(classify_loss_sum, k = classify_keep_num) \r\n    classify_loss = tf.where(K.equal(classify_keep_num, 0), tf.constant(0, dtype = tf.float32), K.mean(classify_loss_sum_filtered)) \r\n\r\n    loss = classify_loss * loss_weights[0] # + roi_loss * loss_weights[1] + pts_loss * loss_weights[2]\r\n    \r\n    return loss \r\n    \r\n# Train\r\nmodel.compile(optimizer = Adam(lr = 0.001), loss = custom_loss)\r\nmodel.fit(x1, y1, batch_size = 896, epochs = 400, shuffle = True)\r\nmodel.save(r'PNet_train.h5')\r\n```", "@TMaysGGS,\r\nI was able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/80bcebe4d22acdf01bb8c5ae07da630d/38416.ipynb).\r\n\r\nThe issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/df7727f839692cf57c2d3a153d10abdd/38416-tf-nightly.ipynb), as I was able to run the code without any issues. Please find the attached gist. Thanks!", "> @TMaysGGS,\r\n> I was able to reproduce the issue with [TF v2.2.0-rc3](https://colab.research.google.com/gist/amahendrakar/80bcebe4d22acdf01bb8c5ae07da630d/38416.ipynb).\r\n> \r\n> The issue seems to be fixed with the latest [TF-nightly](https://colab.research.google.com/gist/amahendrakar/df7727f839692cf57c2d3a153d10abdd/38416-tf-nightly.ipynb), as I was able to run the code without any issues. Please find the attached gist. Thanks!\r\n\r\n@amahendrakar ,\r\nI just tried the latest TF-nightly installed by using the command 'pip install tf-nightly'. This issue still happens and this time it happens randomly in some epoch. I tried training several times and please increase the total epochs to 1200 so that this problem would always happen.\r\n", "Was able to reproduce the issue with latest TF-nightly too i.e. TF v2.2.0-dev20200427. Please find the gist [here](https://colab.research.google.com/gist/amahendrakar/b2b3b5450d0005a16662cf5afa609e6e/38416-tf-nightly.ipynb). Thanks!", "This is likely a numerical stability issue in the way the model is set up. At this stage, this is better suited for Stack Overflow, as there is a larger community that can help debug there.\r\n\r\nYou can try using run_eagerly=True in your model to walk through step by step and layer by layer to see which layer is causing the issue. \r\n\r\nIf you can create a smaller example that narrows down the issue to a bug, please reopen an issue here.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38416\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38416\">No</a>\n"]}, {"number": 38415, "title": "In v2.1.0, conversion of \"tensorflow.python.framework.ops.Tensor\" to array ", "body": "I have used **tf.nn.softmax_cross_entropy_with_logits(logits,labels)** , which as an output gives a tensor of type \"tensorflow.python.framework.ops.Tensor\". \r\nI need to convert it into numpy array which by any means i am not able to do.\r\nI have even tried to create tf.Session() but that to not working in this case.\r\nPlease help me out in it. \r\n \r\n", "comments": ["@shivamdubey7466, Can you provide the complete code to reproduce the issue. Thanks!", "Hi, I hadn't initialized the global_variable_initializer() . The Mistake was from my end. Thanks for your support @gadagashwini"]}, {"number": 38414, "title": "Resource exhausted:  MemoryError: Unable to allocate", "body": "Hi, I'm getting a `ResourceExhaustedError` in the middle of a training and I'm assuming that shouldn't be possible. I.e., either it happens in the very first iteration, showing me my GPU doesn't have enough memory to train my model or it works until the end.\r\nIn my case, it ran fine for 46 epochs (batch size = 8, image size = (720, 1280)), then I got the error.\r\nI'm running the code from a Jupyter notebook.\r\n\r\nAny help is appreciated, thank you.\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Windows 10\r\n\r\n- TensorFlow installed from (source or\r\nbinary): intalled from pip inside Anaconda environment\r\n\r\n- TensorFlow version (use command below): 2.2.0-dev20200401\r\n\r\n- Python version: 3.7.6\r\n\r\n- CUDA/cuDNN version:\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2019 NVIDIA Corporation\r\nBuilt on Sun_Jul_28_19:12:52_Pacific_Daylight_Time_2019\r\nCuda compilation tools, release 10.1, V10.1.243\r\n\r\n- GPU model and memory: GeForce RTX 2080 Ti\r\n\r\n**Describe the current behavior**\r\nGetting a `ResourceExhaustedError` in the middle of the training (46 epochs in).\r\n\r\n**Describe the expected behavior**\r\nExpected to finish the training normally, or not work from the very first epoch.\r\n\r\n**Standalone code to reproduce the issue** \r\nSince it's not that small, I'm attaching a file.\r\nFeel free to update my report with the code inline if more appropriate.\r\n[minimal_breaking_code.txt](https://github.com/tensorflow/tensorflow/files/4477650/minimal_breaking_code.txt)\r\n\r\nThose are the entry variables:\r\n![2020-04-14 17_33_59-Window](https://user-images.githubusercontent.com/764094/79271798-c8e65380-7e76-11ea-8c4f-1986d62e935b.png)\r\n\r\nAlso, the images are 1080x1920.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\n---------------------------------------------------------------------------\r\nResourceExhaustedError                    Traceback (most recent call last)\r\nC:/Users/Gamer/Dropbox/Projetos/MLPython/Imports.py in <module>\r\n     31             'balanced_training_data': False,\r\n     32             'data_augmentation': True,\r\n---> 33             'training_data_samples': None\r\n     34         }\r\n     35     }\r\n\r\n~\\Dropbox\\Projetos\\Ecotrace\\NewImages\\Model.py in run_experiment(data_path, images_data_path, training_data, balanced_training_data, validation_data, parameters)\r\n    338             keras.callbacks.ModelCheckpoint(filepath = last_model_filepath, save_best_only = True),\r\n    339             keras.callbacks.ModelCheckpoint(filepath = best_model_filepath, save_best_only = True),\r\n--> 340             keras.callbacks.CSVLogger(experiment_folder + '/history.csv')\r\n    341         ]\r\n    342     )\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _method_wrapper(self, *args, **kwargs)\r\n     69   def _method_wrapper(self, *args, **kwargs):\r\n     70     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 71       return method(self, *args, **kwargs)\r\n     72 \r\n     73     # Running inside `run_distribute_coordinator` already.\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    940               workers=workers,\r\n    941               use_multiprocessing=use_multiprocessing,\r\n--> 942               return_dict=True)\r\n    943           val_logs = {'val_' + name: val for name, val in val_logs.items()}\r\n    944           epoch_logs.update(val_logs)\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in _method_wrapper(self, *args, **kwargs)\r\n     69   def _method_wrapper(self, *args, **kwargs):\r\n     70     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 71       return method(self, *args, **kwargs)\r\n     72 \r\n     73     # Running inside `run_distribute_coordinator` already.\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\r\n   1171             with trace.Trace('TraceContext', graph_type='test', step_num=step):\r\n   1172               callbacks.on_test_batch_begin(step)\r\n-> 1173               tmp_logs = test_function(iterator)\r\n   1174               if data_handler.should_sync:\r\n   1175                 context.async_wait()\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\r\n    606         xla_context.Exit()\r\n    607     else:\r\n--> 608       result = self._call(*args, **kwds)\r\n    609 \r\n    610     if tracing_count == self._get_tracing_count():\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\r\n    644       # In this case we have not created variables on the first call. So we can\r\n    645       # run the first trace but we should fail if variables are created.\r\n--> 646       results = self._stateful_fn(*args, **kwds)\r\n    647       if self._created_variables:\r\n    648         raise ValueError(\"Creating variables on a non-first call to a function\"\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in __call__(self, *args, **kwargs)\r\n   2418     with self._lock:\r\n   2419       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n-> 2420     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2421 \r\n   2422   @property\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _filtered_call(self, args, kwargs)\r\n   1663          if isinstance(t, (ops.Tensor,\r\n   1664                            resource_variable_ops.BaseResourceVariable))),\r\n-> 1665         self.captured_inputs)\r\n   1666 \r\n   1667   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\r\n   1744       # No tape is watching; skip to running the function.\r\n   1745       return self._build_call_outputs(self._inference_function.call(\r\n-> 1746           ctx, args, cancellation_manager=cancellation_manager))\r\n   1747     forward_backward = self._select_forward_and_backward_functions(\r\n   1748         args,\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\eager\\function.py in call(self, ctx, args, cancellation_manager)\r\n    596               inputs=args,\r\n    597               attrs=attrs,\r\n--> 598               ctx=ctx)\r\n    599         else:\r\n    600           outputs = execute.execute_with_cancellation(\r\n\r\n~\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     58     ctx.ensure_initialized()\r\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n---> 60                                         inputs, attrs, num_outputs)\r\n     61   except core._NotOkStatusException as e:\r\n     62     if name is not None:\r\n\r\nResourceExhaustedError: 2 root error(s) found.\r\n  (0) Resource exhausted:  MemoryError: Unable to allocate 10.5 MiB for an array with shape (720, 1280, 3) and data type float32\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 243, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 784, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\", line 816, in wrapped_generator\r\n    for data in generator_fn():\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 1022, in get\r\n    six.reraise(*sys.exc_info())\r\n\r\n  File \"C:\\Users\\Gamer\\AppData\\Roaming\\Python\\Python37\\site-packages\\six.py\", line 693, in reraise\r\n    raise value\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 998, in get\r\n    inputs = self.queue.get(block=True).get()\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\multiprocessing\\pool.py\", line 657, in get\r\n    raise self._value\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\multiprocessing\\pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 932, in next_sample\r\n    return six.next(_SHARED_SEQUENCES[uid])\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 104, in __next__\r\n    return self.next(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 116, in next\r\n    return self._get_batches_of_transformed_samples(index_array)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 231, in _get_batches_of_transformed_samples\r\n    x = img_to_array(img, data_format=self.data_format)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\", line 299, in img_to_array\r\n    x = np.asarray(img, dtype=dtype)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 85, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n\r\nMemoryError: Unable to allocate 10.5 MiB for an array with shape (720, 1280, 3) and data type float32\r\n\r\n\r\n\t [[{{node PyFunc}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[IteratorGetNext]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n  (1) Resource exhausted:  MemoryError: Unable to allocate 10.5 MiB for an array with shape (720, 1280, 3) and data type float32\r\nTraceback (most recent call last):\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 243, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 309, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 784, in generator_py_func\r\n    values = next(generator_state.get_iterator(iterator_id))\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\", line 816, in wrapped_generator\r\n    for data in generator_fn():\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 1022, in get\r\n    six.reraise(*sys.exc_info())\r\n\r\n  File \"C:\\Users\\Gamer\\AppData\\Roaming\\Python\\Python37\\site-packages\\six.py\", line 693, in reraise\r\n    raise value\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 998, in get\r\n    inputs = self.queue.get(block=True).get()\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\multiprocessing\\pool.py\", line 657, in get\r\n    raise self._value\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\multiprocessing\\pool.py\", line 121, in worker\r\n    result = (True, func(*args, **kwds))\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\data_utils.py\", line 932, in next_sample\r\n    return six.next(_SHARED_SEQUENCES[uid])\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 104, in __next__\r\n    return self.next(*args, **kwargs)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 116, in next\r\n    return self._get_batches_of_transformed_samples(index_array)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\", line 231, in _get_batches_of_transformed_samples\r\n    x = img_to_array(img, data_format=self.data_format)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\", line 299, in img_to_array\r\n    x = np.asarray(img, dtype=dtype)\r\n\r\n  File \"C:\\Users\\Gamer\\Anaconda3\\envs\\TensorFlow-nightly\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 85, in asarray\r\n    return array(a, dtype, copy=False, order=order)\r\n\r\nMemoryError: Unable to allocate 10.5 MiB for an array with shape (720, 1280, 3) and data type float32\r\n\r\n\r\n\t [[{{node PyFunc}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[IteratorGetNext]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[IteratorGetNext/_4]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_test_function_16419]\r\n\r\nFunction call stack:\r\ntest_function -> test_function\r\n", "comments": ["@rodrigoruiz In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!", "@saikumarchalla I edited the initial report adding the code.\r\nNot sure how to remove the `stat:awaiting response` label.", "@rodrigoruiz Could you please share the data as it helps us to reproduce the issue.", "@saikumarchalla unfortunately I can\u2019t since it\u2019s  another company\u2019s data, but I imagine the same problem will occur with any set of imagens of the same size. That\u2019s why I shared a print of the data variables along with image sizes.", "@saikumarchalla Also, I don't think it's a performance issue, since it works for a few epochs, I think it's more like a memory leak.", "@saikumarchalla any update on this?", "@rodrigoruiz  Apologies for the delay in response. Memory error indicates that the process has consumed all of the ram memory. You may want to reduce the batch size/image size and try again.\r\n", "@ymodak To elaborate, there are two things that makes me think that is not the issue:\r\n1) this same network used to work in Keras with TensorFlow 1.\r\n2) the memory runs or after about 45 epoch. If the batch or network size was the problem, it should fail at the very beginning of training.", "We need a minimum repro example to validate this behavior and troubleshoot further. Perhaps if you can build a toy example to reproduce this behavior will be much appreciated. Thanks for the issue.", "@ymodak the minimum repro example is attached as a txt file in the issue (minus the images, because I'm not allowed to provide those, but as I said before, any image of the same size should do).", "@rodrigoruiz Could you help to format the repro code into colab with some synthetic input data? This will help a lot to debug the issue. Thanks!", "I am having the same problem", "I'm having the same problem with TF 2.2. \r\nThe error happened after about 120 epoches. \r\n\r\nMy input size is 640 x 640 x 3. \r\nAnd I use `tf.distribute.MirroredStrategy( cross_device_ops=tf.distribute.HierarchicalCopyAllReduce() )` strategy to parallelly train the model with two GPUs. \r\n\r\nWhen training, the used RAM by the python process increases slowly from about 1.5G to 7.3G. However, the total used RAM increases from about 5G to more than 24G (I have total 32G RAM, and I did not run any other programs when training), and finally ends up with the MemoryError. \r\nWhen I close or restart the python process, the used RAM go back to available again. \r\n\r\nupdate:\r\nI fixed this by training the model with manual epoch iteration. In each epoch, I added `tf.keras.backend.clear_session`.\r\nnow the used memory doesnot increase any more. \r\nThe problem is definitely in model.fit function. ", "@rodrigoruiz If the issue still persists, can you please provide with some synthetic input data as requested in [issuecomment-642836301](https://github.com/tensorflow/tensorflow/issues/38414#issuecomment-642836301) or else can you please close the issue?", "Since there is no activity, closing the issue. Please reopen it if you the issue persists..", "I'm having the same problem with TF 2.4.0rc3\r\nI can reproduce this problem on my PC for sure. \r\n\r\nAnaconda python 3.7\r\nWindows 10\r\nGPU RTX 3090 24GB\r\nTotal memory 96GB\r\nTensorflow 2.4.0rc3 and CUDA11 and cuDNN 8.0.2\r\n\r\nWhen the problem occurred, I had 70GB of RAM and 5GB of video memory left on my system. But \"Unable to allocate 735. MiB\".\r\n\r\nI exported a subset of the code and data used to reproduce the problem.\r\nFor code and data, please see: https://github.com/liasece/tf-38414\r\n\r\nTo hide this problem, simply reduce frozen_batch_size from 64 to 32, i.e., reduce the batch size.\r\n\r\n@ymodak @geetachavan1 "]}, {"number": 38413, "title": "r2.1 windows libtensorflow build failed", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10 (1909) x64\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version: r2.1\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 0.29.1\r\n- GCC/Compiler version (if compiling from source): MSVC 2019 Build Tool\r\n- CUDA/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\nI can't build tensorflow r2.1 as libtensorflow.\r\nWhen I build source code it always failed with ProtoCompile ERROR. (Exit -1073741795)\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nFirst, installed msys and MSVC 2019 build tool. (Described in https://tensorflow.google.cn/install/source_windows)\r\n\r\nAnd then, run bazel with following command.\r\n.\\bazel.exe build --config=opt //tensorflow/tools/lib_package:libtensorflow\r\n\r\n**Build Error**\r\nERROR: D:/work/tensorflow/source/tensorflow/tensorflow/core/BUILD:2045:1: ProtoCompile tensorflow/core/protobuf/autotuning.pb.h failed (Exit -1073741795)\r\nTarget //tensorflow/tools/lib_package:libtensorflow failed to build\r\nINFO: Elapsed time: 6.115s, Critical Path: 3.44s\r\nINFO: 2 processes: 2 local.\r\nFAILED: Build did NOT complete successfully", "comments": ["[Latest reasons](https://github.com/tensorflow/tensorflow/issues/39905#issuecomment-641588284) why nothing happens for last 6 months for Windows DLL for TF2.0, TF2.1 and TF2.2", "I have already finished build.\r\nThank you.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38413\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38413\">No</a>\n"]}, {"number": 38412, "title": "shape_invariants in tf.while_loop", "body": "**System information** \r\nOS Platform and Distribution : macOS Catalina 10.15.3\r\n\r\nTensorFlow installed from : binary\r\n\r\nTensorFlow version : 1.15.2\r\n\r\nPython version: 3.7.3\r\n\r\nSo basically i have \r\n`matrix = tf.constant([[[16,15,16,15,87],[3,4,3,4,87]],[[ 3,4,3,41,87],[0,0,0,0,0]]]) `\r\n\r\n`b = tf.constant([[[1],[2]],[[1],[1]]])`\r\n\r\nI want to \r\n\r\nI want vector c such that\r\n\r\n[ 16 15 16 15 87] is repeated [1] times ,\r\n[ 3 4 3 4 87] is repeated [2] times ,\r\n[ 3 4 14 95 87] is repeated [2] times,\r\n[ 3 7 9 250 87] is repated [2] times and so on ............\r\n\r\nI am writing following code \r\n\r\n`import tensorflow as tf`\r\n\r\n`tf.enable_eager_execution()`\r\n\r\n`def repeatOperation2(t1,t2):`\r\n  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`xShape = tf.shape(t1)[0]`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`reshaped  = tf.reshape(t1, [1, xShape])`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`repeat = tf.repeat(reshaped, repeats=t2, axis=0)`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`ta = tf.TensorArray(dtype=tf.int32, size=1)`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`ta.write(0, repeat)`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`ta_final_result = ta.stack()`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`return ta_final_result`\r\n\r\n`def repeatOperation1(t1,t2):`\r\n   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`matrix_rows = tf.shape(t1)[0]`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`ta = tf.TensorArray(dtype=tf.int32, size=10)`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`i0 = tf.constant(0)`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`init_state = (i0, ta)`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`condition = lambda i, _: i < matrix_rows`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`body = lambda i, ta: (i + 1, ta.write(i, repeatOperation2(t1[i,:],t2[i,:])))`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`n, ta_final = tf.while_loop(condition, body,init_state,shape_invariants=[i0.get_shape(), tf.TensorShape((None,None,5))])`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`ta_final_result = ta_final.stack()`\r\n    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`return ta_final_result`\r\n\r\n\r\n`matrix = tf.constant([[[16,15,16,15,87],[3,4,3,4,87]],[[ 3,4,3,41,87],[0,0,0,0,0]]]) `\r\n`b = tf.constant([[[1],[2]],[[1],[1]]])`\r\n\r\n`matrix_rows = tf.shape(matrix)[0]`\r\n`ta = tf.TensorArray(dtype=tf.int32, size=matrix_rows)`\r\n`init_state = (0, ta)`\r\n`condition = lambda i, _: i < matrix_rows`\r\n`body = lambda i, ta: (i + 1, ta.write(i, repeatOperation1(matrix[i,:,:],b[i,:,:])))`\r\n`n, ta_final = tf.while_loop(condition, body, init_state)`\r\n`ta_final_result = ta_final.stack()`\r\n`print(ta_final_result)`\r\n\r\n\r\n\r\nbasically for each iteration multiplication i am using `shape_invariants=[i0.get_shape(), tf.TensorShape((None,None,5))])` since repeating matrix tensor equal to b tensor the number in first two dimension changes  but i am getting below error \r\n\r\nError:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf_2.py\", line 52, in <module>\r\n    n, ta_final = tf.while_loop(condition, body, init_state)\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2714, in while_loop\r\n    loop_vars = body(*loop_vars)\r\n  File \"tf_2.py\", line 51, in <lambda>\r\n    body = lambda i, ta: (i + 1, ta.write(i, repeatOperation1(matrix[i,:,:],b[i,:,:])))\r\n  File \"tf_2.py\", line 37, in repeatOperation1\r\n    n, ta_final = tf.while_loop(condition, body, init_state,shape_invariants=[i0.get_shape(), tf.TensorShape((None,None,5))])\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2714, in while_loop\r\n    loop_vars = body(*loop_vars)\r\n  File \"tf_2.py\", line 36, in <lambda>\r\n    body = lambda i, ta: (i + 1, ta.write(i, repeatOperation2(t1[i,:],t2[i,:])))\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 1084, in write\r\n    return self._implementation.write(index, value, name=name)\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 815, in write\r\n    self._write(index, value)\r\n  File \"<path>/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 805, in _write\r\n    (value.shape, self._element_shape))\r\nValueError: Incompatible shape for value ((1, 2, 5)), expected ((1, 1, 5))", "comments": ["We can close this ticket , solved in https://github.com/tensorflow/tensorflow/issues/38375", "> We can close this ticket , solved in #38375\r\n\r\nClosing the issue as per user's comment."]}, {"number": 38411, "title": "Tensorflow-gpu not interacting with gpu after fresh install", "body": "\r\n**System information**\r\n- Windows 10\r\n- pip install\r\n- TensorFlow-gpu==1.15.2\r\n-python 3.7.6\r\n- using conda and trying to run through spyder and ubuntu for windows\r\n- Bazel version (if compiling from source): idk\r\n- GCC/Compiler version (if compiling from source): idk\r\n- CUDA/cuDNN version: cuda 10.1 cuDNN 7.6.5 driver 436.3\r\n- GPU model and memory: geforce 1660 ti\r\n\r\ntensorflow can't see my gpu's I just need it to see it. I am trying to run magenta with gpu and I have been travelling in circles. I did have tensorflow working with gpu support and now I have messed with it too much that it must be broken. I have done a full uninstall of conda and then installed it all again. \r\nnvcc works and nvidia-smi works so I have it. But it isn't showing up in tensorflow-gpu. \r\n\r\nfrom running:\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import device_lib\r\nprint(device_lib.list_local_devices())\r\nprint(tf.test.is_gpu_available(\r\n    cuda_only=False,\r\n    min_cuda_compute_capability=None\r\n))\r\n\r\nimport tensorflow as tf\r\nprint(tf.test.gpu_device_name())\r\n\r\n[name: \"/device:CPU:0\"\r\ndevice_type: \"CPU\"\r\nmemory_limit: 268435456\r\nlocality {\r\n}\r\nincarnation: 172585563774038155\r\n]\r\nFalse\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 436.30       Driver Version: 436.30       CUDA Version: 10.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 166... WDDM  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   56C    P0    24W /  N/A |    614MiB /  6144MiB |      6%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n", "comments": ["tf.test.is_built_with_cuda() returns true as well.", "@Scathos, \r\nDid you add CUDA and cuDNN path,Please follow the instructions mentioned in the [Tensorflow](https://www.tensorflow.org/install/gpu#windows_setup) website. Thanks", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38411\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38411\">No</a>\n"]}, {"number": 38410, "title": "tf.test.is_gpu_available is deprecated", "body": "", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F38410) for more info**.\n\n<!-- need_sender_cla -->", "Check out this pull request on&nbsp; <a href=\"https://app.reviewnb.com/tensorflow/tensorflow/pull/38410\"><img align=\"absmiddle\"  alt=\"ReviewNB\" height=\"28\" class=\"BotMessageButtonImage\" src=\"https://raw.githubusercontent.com/ReviewNB/support/master/images/button_reviewnb.png\"/></a> \n\n You'll be able to see Jupyter notebook diff and discuss changes. Powered by <a href='https://www.reviewnb.com'>ReviewNB</a>.", "@urlocal12 can you please sign CLA ? ", "I'm going to go ahead and close this PR, because it seems to have stalled. If you're still interested in pursing this (and responding to my comments), please feel free to reopen!"]}, {"number": 38408, "title": "Cherrypick libjpeg-turbo version update", "body": "PiperOrigin-RevId: 305742301\r\nChange-Id: I50968c0868f70a5009f018d3132cba77c0900158", "comments": []}, {"number": 38407, "title": "Cherrypick libjpeg-turbo version update", "body": "PiperOrigin-RevId: 305742301\r\nChange-Id: I50968c0868f70a5009f018d3132cba77c0900158", "comments": []}, {"number": 38406, "title": "Cherrypick libjpeg-turbo version update", "body": "PiperOrigin-RevId: 305742301\r\nChange-Id: I50968c0868f70a5009f018d3132cba77c0900158", "comments": []}, {"number": 38405, "title": "Cherrypick libjpeg-turbo version update", "body": "PiperOrigin-RevId: 305742301\r\nChange-Id: I50968c0868f70a5009f018d3132cba77c0900158", "comments": []}, {"number": 38404, "title": "CollectiveBcastRecv error for Tensorflow 2.0 Distributed training with MultiWorkerMirrored strategy using estimator API ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Red Hat Enterprise Linux Server release 7.4 (Maipo)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: \r\n- TensorFlow installed from (source or\r\nbinary): - TensorFlow version (use command below): 2.0\r\n- Python version: 3.7 - Bazel \r\nversion (if compiling from source): 0.26.1\r\n- GCC/Compiler version (if compiling from\r\nsource): GCC 7.3.1\r\n- CUDA/cuDNN version: - GPU model and memory: 10.1\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nI'm migrating a model for TF 1.14 to TF 2. I used the\u00a0tf_upgrade_v2\u00a0script to change the model.\u00a0 The upgraded model used `tf.compat.v1.get_variable()`\u00a0 and and ` tf.compat.v1.train.AdagradOptimizer()`.\u00a0It successfully\u00a0ran on\u00a0TF 2.0 using parameter\u00a0server strategy. We used the estimator api for the distributed training. But when I changed the strategy to multi-worker\u00a0mirrored strategy, it didn't work for me.\u00a0There is some issue with\u00a0Adagrad Initializer. Please see the error below. Are those calls compatible (`tf.compat.v1.get_variable()`\u00a0 and tf.compat.v1.train.AdagradOptimize()) with multi-worker mirrored\u00a0strategy using\u00a0estimator API?\r\n**Describe the expected behavior**\r\nIt should work by changing the strategy to multi-worker\u00a0mirrored strategy in estimator RunConfig.\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\nAPI calls:\r\n\r\nx = tf.compat.v1.get_variable(\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=x,\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initializer=tf.random.truncated_normal([tensor_len, num_classes],\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0stddev=1.0 / math.sqrt(float(tensor_len))),\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 regularizer=tf.keras.regularizers.l2(l2_reg_weight)\r\n\r\n\u00a0optimizer = tf.compat.v1.train.AdagradOptimizer(0.01)\r\n\r\n\r\nError Log:\r\n\r\n2020-04-09 01:57:51.499271: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: RecvBufResponse returned 2408 bytes where to_tensor expected 8082020-04-09 01:57:51.499305: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: RecvBufResponse returned 808 bytes where to_tensor expected 24082020-04-09 01:57:51.499272: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: RecvBufResponse returned 2408 bytes where to_tensor expected 8082020-04-09 01:57:51.499371: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: RecvBufResponse returned 808 bytes where to_tensor expected 24082020-04-09 01:57:51.499365: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Internal: RecvBufResponse returned 2408 bytes where to_tensor expected 808\t [[{{node memberFeatures_geoRegion_weights/Adagrad/Initializer/CollectiveBcastRecv}}]]2020-04-09 01:57:51.500466: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Cancelled: [_Derived_]CancelledAdditional GRPC error information:{\"created\":\"@1586397471.499604492\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"Cancelled\",\"grpc_status\":1}2020-04-09 01:57:51.501131: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: [_Derived_]RecvBufResponse returned 2408 bytes where to_tensor expected 808\t [[{{node memberFeatures_geoRegion_weights/Adagrad/Initializer/CollectiveBcastRecv}}]]2020-04-09 01:57:51.501135: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: [_Derived_]RecvBufResponse returned 2408 bytes where to_tensor expected 808\t [[{{node memberFeatures_geoRegion_weights/Adagrad/Initializer/CollectiveBcastRecv}}]]2020-04-09 01:57:51.501170: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: [_Derived_]RecvBufResponse returned 2408 bytes where to_tensor expected 808\t [[{{node memberFeatures_geoRegion_weights/Adagrad/Initializer/CollectiveBcastRecv}}]]2020-04-09 01:57:51.501175: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: [_Derived_]RecvBufResponse returned 2408 bytes where to_tensor expected 808\t [[{{node memberFeatures_geoRegion_weights/Adagrad/Initializer/CollectiveBcastRecv}}]]2020-04-09 01:57:51.501192: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: [_Derived_]RecvBufResponse returned 2408 bytes where to_tensor expected 808\t [[{{node memberFeatures_geoRegion_weights/Adagrad/Initializer/CollectiveBcastRecv}}]]2020-04-09 01:57:51.501223: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at collective_ops.cc:365 : Internal: [_Derived_]RecvBufResponse returned 2408 bytes where to_tensor expected 808\t [[{{node memberFeatures_geoRegion_weights/Adagrad/Initializer/CollectiveBcastRecv}}]]Traceback (most recent call last): File \"<>/site-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call return fn(*args) File \"<>/site-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn target_list, run_metadata) File \"<>/site-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun run_metadata)tensorflow.python.framework.errors_impl.InternalError: From /job:worker/replica:0/task:1:RecvBufResponse returned 2408 bytes where to_tensor expected 808\t [[{{node memberFeatures_geoRegion_weights/Adagrad/Initializer/CollectiveBcastRecv}}]]During handling of the above exception, another exception occurred:\r\n\r\n", "comments": ["Estimator executes model_fn in graph mode. Unfortunately, it didn't able handle python `dict` properly in graph mode for multiworker mirrored strategy. I solved it by maintaining the consistent feature ordering using `OrderedDict`.\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38404\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38404\">No</a>\n", "@arde171 what was the resolution. Below scripts runs without callback, when I enable callbacks I get the size mismatch error.\r\n\r\nos.environ['TF_CONFIG'] = json.dumps(tf_config)\r\n\r\nBUFFER_SIZE = 100\r\nBATCH_SIZE = 8\r\n\r\ndef make_datasets_unbatched():\r\n  #Scaling MNIST data from (0, 255] to (0., 1.]\r\n  def scale(image, label):\r\n    image = tf.cast(image, tf.float16)\r\n    image /= 255\r\n    return image, label\r\n\r\n  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n  return datasets['train'].map(scale, num_parallel_calls=tf.data.experimental.AUTOTUNE).cache().repeat()\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\n\r\nprint(\"Created strategy \" , strategy)\r\nNUM_WORKERS = strategy.num_replicas_in_sync\r\nprint(\"Number of workers \" ,NUM_WORKERS )\r\nGLOBAL_BATCH_SIZE = 64 * NUM_WORKERS\r\n\r\nprint(\"Creating datasets inside scope...\")\r\ntrain_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\r\nprint(\"Created datasets \" , train_datasets)\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA  # AutoShardPolicy.OFF can work too.\r\ntrain_datasets_no_auto_shard = train_datasets.with_options(options)\r\n\r\n\r\ndef build_and_compile_cnn_model():\r\n  model = tf.keras.Sequential([\r\n      tf.keras.layers.Flatten(input_shape=(28,28,1)),\r\n      tf.keras.layers.Dense(256, activation='relu'),\r\n      tf.keras.layers.Dropout(0.5),\r\n      tf.keras.layers.Dense(10, activation='softmax')\r\n  ])\r\n  model.compile(\r\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n      optimizer=tf.keras.optimizers.Adam(),\r\n      metrics=['accuracy'])\r\n  return model\r\n\r\ncallbacks = []\r\n\r\nif my_task_index == 0:\r\n  callbacks = [tf.keras.callbacks.TensorBoard(log_dir='logs')]\r\n  #callbacks = [tf.keras.callbacks.TensorBoard(log_dir='logs'), tf.keras.callbacks.ModelCheckpoint(filepath=\"models\", save_best_only=False)]\r\n\r\n\r\nwith strategy.scope():\r\n  print(\"Creating model inside scope...\")\r\n  model = build_and_compile_cnn_model()\r\n  print(model.summary())\r\n\r\n\r\n\r\nprint(\"Starting to fit model....\")\r\nverbose = 1 if my_task_index == 0 else 0\r\nmodel.fit(x=train_datasets_no_auto_shard, epochs=10, steps_per_epoch=25, callbacks = callbacks , verbose=verbose)\r\nprint(\"Model fit completed\")\r\n\r\n\r\npi@RpiCluster4:/nfs_share/tfslurm/tensorflow_on_slurm/examples/cifar-10 $ srun -N3 python3 main_2.py -vv\r\n2020-04-26 02:03:19.993404: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> RpiCluster1:2222, 1 -> localhost:2222, 2 -> RpiCluster3:2222}\r\n2020-04-26 02:03:19.994945: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222\r\n2020-04-26 02:03:20.049228: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> RpiCluster2:2222, 2 -> RpiCluster3:2222}\r\n2020-04-26 02:03:20.049956: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222\r\n2020-04-26 02:03:20.061103: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> RpiCluster1:2222, 1 -> RpiCluster2:2222, 2 -> localhost:2222}\r\n2020-04-26 02:03:20.061870: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_fn` is not passed in. The `worker_fn` will be used if an \"evaluator\" task exists in the cluster.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:`eval_strategy` is not passed in. No distribution strategy will be used for evaluation.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\nWARNING:tensorflow:ModelCheckpoint callback is not provided. Workers will need to restart training if any fails.\r\n2020-04-26 02:03:25.637066: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:398 : Internal: RecvBufResponse returned 8 bytes where to_tensor expected 4\r\n2020-04-26 02:03:25.636609: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at collective_ops.cc:398 : Internal: RecvBufResponse returned 8 bytes where to_tensor expected 4\r\n2.1.0\r\nFound Cluster spec  {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222', 'RpiCluster3:2222']}\r\nFound TFConfig {'cluster': {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222', 'RpiCluster3:2222']}, 'task': {'type': 'worker', 'index': 2}}\r\nCreated strategy  <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x7659a390>\r\nNumber of workers  3\r\nCreating datasets inside scope...\r\nCreated datasets  <DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.float16, tf.int64)>\r\nCreating model inside scope...\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nflatten (Flatten)            (None, 784)               0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 256)               200960\r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 256)               0\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                2570\r\n=================================================================\r\nTotal params: 203,530\r\nTrainable params: 203,530\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\nStarting to fit model....\r\n2.1.0\r\nFound Cluster spec  {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222', 'RpiCluster3:2222']}\r\nFound TFConfig {'cluster': {'worker': ['RpiCluster1:2222', 'RpiCluster2:2222', 'RpiCluster3:2222']}, 'task': {'type': 'worker', 'index': 1}}\r\nCreated strategy  <tensorflow.python.distribute.collective_all_reduce_strategy.CollectiveAllReduceStrategy object at 0x76657330>\r\nNumber of workers  3\r\nCreating datasets inside scope...\r\nCreated datasets  <DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.float16, tf.int64)>\r\nCreating model inside scope...\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\nflatten (Flatten)            (None, 784)               0\r\n_________________________________________________________________\r\ndense (Dense)                (None, 256)               200960\r\n_________________________________________________________________\r\ndropout (Dropout)            (None, 256)               0\r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 10)                2570\r\n=================================================================\r\nTotal params: 203,530\r\nTrainable params: 203,530\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\nStarting to fit model....\r\nTraceback (most recent call last):\r\n  File \"main_2.py\", line 75, in <module>\r\n    model.fit(x=train_datasets_no_auto_shard, epochs=10, steps_per_epoch=25, callbacks = callbacks , verbose=verbose)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\nTraceback (most recent call last):\r\n  File \"main_2.py\", line 75, in <module>\r\n    model.fit(x=train_datasets_no_auto_shard, epochs=10, steps_per_epoch=25, callbacks = callbacks , verbose=verbose)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\r\n    use_multiprocessing=use_multiprocessing)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 790, in fit\r\n    *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 777, in wrapper\r\n    mode=dc.CoordinatorMode.INDEPENDENT_WORKER)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 853, in run_distribute_coordinator\r\n    task_id, session_config, rpc_layer)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_coordinator.py\", line 360, in _run_single_worker\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n    return worker_fn(strategy)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_distributed.py\", line 772, in _worker_fn\r\n    return method(model, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 342, in fit\r\n    total_epochs=epochs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\r\n    batch_outs = execution_function(iterator)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\r\n    distributed_function(input_fn))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 568, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 615, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 694, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 497, in _initialize\r\n    *args, **kwds))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2389, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 201, in _call_for_each_replica\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 439, in wrapped_fn\r\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 85, in distributed_function\r\n    per_replica_function, args=args)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 763, in experimental_run_v2\r\n    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1819, in call_for_each_replica\r\n    return self._call_for_each_replica(fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 694, in _call_for_each_replica\r\n    fn, args, kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 917, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 201, in _call_for_each_replica\r\n    coord.join(threads)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 273, in _process_single_batch\r\n    model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 433, in apply_gradients\r\n    self._create_hypers()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 655, in _create_hypers\r\n    aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 817, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/pi/.local/lib/python3.7/site-packages/six.py\", line 703, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/coordinator.py\", line 297, in stop_on_exception\r\n    yield\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 917, in run\r\n    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 292, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 433, in train_on_batch\r\n    output_loss_metrics=model._output_loss_metrics)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/shared_variable_creator.py\", line 69, in create_new_variable\r\n    v = next_creator(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 312, in train_on_batch\r\n    output_loss_metrics=output_loss_metrics))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\", line 273, in _process_single_batch\r\n    model.optimizer.apply_gradients(zip(grads, trainable_weights))\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 433, in apply_gradients\r\n    self._create_hypers()\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 655, in _create_hypers\r\n    aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\", line 817, in add_weight\r\n    aggregation=aggregation)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1330, in creator_with_resource_vars\r\n    return self._create_variable(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 548, in _create_variable\r\n    values.SyncOnReadVariable, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/values.py\", line 1034, in create_mirrored_variable\r\n    value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 540, in _real_mirrored_creator\r\n    v = next_creator(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/tracking/base.py\", line 744, in _add_variable_with_custom_getter\r\n    **kwargs_for_getter)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\", line 142, in make_variable\r\n    shape=variable_shape if variable_shape else None)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\r\n    shape=shape)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/shared_variable_creator.py\", line 69, in create_new_variable\r\n    v = next_creator(*args, **kwargs)\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 485, in variable_capturing_scope\r\n    lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\", line 1330, in creator_with_resource_vars\r\n    return self._create_variable(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 548, in _create_variable\r\n    values.SyncOnReadVariable, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/values.py\", line 1034, in create_mirrored_variable\r\n    value_list = real_mirrored_creator(devices, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py\", line 540, in _real_mirrored_creator\r\n    v = next_creator(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 65, in getter\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return captured_getter(captured_previous, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 485, in variable_capturing_scope\r\n    lifted_initializer_graph=lifted_initializer_graph, **kwds)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 178, in __init__\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 383, in initial_value_fn\r\n    collective_instance_key)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/collective_ops.py\", line 176, in broadcast_recv\r\n    communication_hint=communication_hint.lower())\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_collective_ops.py\", line 57, in collective_bcast_recv\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/def_function.py\", line 178, in __init__\r\n    initial_value() if init_from_fn else initial_value,\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/distribute/collective_all_reduce_strategy.py\", line 383, in initial_value_fn\r\n    collective_instance_key)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/collective_ops.py\", line 176, in broadcast_recv\r\n    communication_hint=communication_hint.lower())\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_collective_ops.py\", line 57, in collective_bcast_recv\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: RecvBufResponse returned 8 bytes where to_tensor expected 4 [Op:CollectiveBcastRecv]\r\ntensorflow.python.framework.errors_impl.InternalError: RecvBufResponse returned 8 bytes where to_tensor expected 4 [Op:CollectiveBcastRecv]\r\n2020-04-26 02:03:26.982004: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.\r\n2020-04-26 02:03:26.983840: W tensorflow/core/common_runtime/eager/context.cc:349] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown."]}, {"number": 38403, "title": "model.reset_states() does not work for bidirectional-RNNs in tf.keras", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): YES\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): Ubuntu 18.04 LTS\r\n- TensorFlow installed from (source or\r\nbinary): binary\r\n- TensorFlow version (use command below):  \r\nTF 2.1\r\nand\r\ntf-nightly==2.2.0.dev20200407\r\n(both have bug around this issue, but different issues)\r\n\r\n- Python version:  3.7.4\r\n\r\n- CUDA/cuDNN version: 10.1, 7.6.5\r\n\r\n- GPU model and memory: 2080ti, 11GB.  Bug is on both CPU/GPU.\r\n\r\n**Describe the current behavior**\r\n\r\nmodel.reset_states() does not work for bidirectional, stateful recurrent layers (bidi-RNNs).\r\n\r\nTF 2.1: model.reset_states() does nothing for stateful bidi-RNNs.\r\ntf-nightly: calling model.reset_states() for stateful bidi-RNNs causes a crash\r\n\r\n**Describe the expected behavior**\r\nThis was reported as a bug in TF 2.0 -- [model.reset_states() does nothing for bidi-RNNs](https://github.com/tensorflow/tensorflow/issues/34055).  I thought this was fixed in tf-nightly at that time, but has returned in TF 2.1.  \r\n\r\nmodel.reset_states() for standard RNNs changed in TF 2.1. has the following behavior:\r\n\r\n* if model is stateful with NO initial state input: resets state to zero\r\n* if model is stateful with initial state input: resets state to state input\r\n* otherwise the state is carried over form the last call.\r\n\r\nThus the **expected behavior** for stateful bidi-RNNs is:\r\n\r\n* if model is stateful with NO initial state input: resets fwd and bwd state to zero\r\n* if model is stateful with initial state input: resets fwd state to fwd state input and  resets bwd state to bwd state input\r\n* otherwise the fwd state and bwd state are carried over form the last call (as is done in stateful bidi-RNNs).  \r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nCode to show this behavior with no state-inputs:\r\n```python\r\nimport os\r\nos.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\r\nos.environ['CUDA_VISIBLE_DEVICES']=''\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional\r\nfrom tensorflow.keras.models import Model\r\n\r\nREC = LSTM\r\n\r\nsequence_length = 3\r\nfeature_dim = 1\r\nfeatures_in = Input(batch_shape=(1, sequence_length, feature_dim)) \r\n\r\nrnn_out = Bidirectional( REC(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in)\r\nstateless_model = Model(inputs=[features_in], outputs=[rnn_out])\r\n\r\nstateful_rnn_out = Bidirectional( REC(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in)\r\nstateful_model = Model(inputs=features_in, outputs=stateful_rnn_out)\r\n\r\nstateful_model.set_weights( stateless_model.get_weights() )\r\n\r\nx_in = np.random.normal(0,10,sequence_length)\r\nx_in = x_in.reshape( (1, sequence_length, feature_dim) )\r\n\r\ndef print_bidi_out(non_stateful_out, stateful_out):\r\n\tfb = ['FWD::', 'BWD::']\r\n\r\n\tfor i in range(2):\r\n\t\tprint(fb[i])\r\n\t\tprint(f'non_stateful: {non_stateful_out.T[i]}')\r\n\t\tprint(f'stateful: {stateful_out.T[i]}')\r\n\t\tprint(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')\r\n\r\n\r\nnon_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))\r\nstateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))\r\nprint_bidi_out(non_stateful_out, stateful_out)\r\n\r\nnon_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))\r\nstateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))\r\nprint_bidi_out(non_stateful_out, stateful_out)\r\n\r\nprint('\\n** RESETING STATES in STATEFUL MODEL **\\n')\r\nstateful_model.reset_states()\r\nnon_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))\r\nstateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))\r\nprint_bidi_out(non_stateful_out, stateful_out)\r\n```\r\n\r\nCode to demo with initial-state inputs:\r\n```python\r\nimport os\r\nos.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\r\nos.environ['CUDA_VISIBLE_DEVICES']=''\r\n\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional\r\nfrom tensorflow.keras.models import Model\r\n\r\nREC = LSTM\r\n\r\nsequence_length = 3\r\nfeature_dim = 1\r\nfeatures_in = Input(batch_shape=(1, sequence_length, feature_dim)) \r\nstate_h_fwd_in = Input(batch_shape=(1, 1))\r\nstate_h_bwd_in = Input(batch_shape=(1, 1))\r\nstate_c_fwd_in = Input(batch_shape=(1, 1))\r\nstate_c_bwd_in = Input(batch_shape=(1, 1))\r\n\r\nfour_state_shape = [state_h_fwd_in, state_c_fwd_in, state_h_bwd_in, state_c_bwd_in]\r\ntwo_state_shape = [state_h_fwd_in, state_h_bwd_in]\r\n\r\nif REC == LSTM:\r\n    rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=four_state_shape)\r\n    stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=four_state_shape)\r\n    rnn_inputs = [features_in, state_h_fwd_in, state_c_fwd_in, state_h_bwd_in, state_c_bwd_in]\r\nelse:\r\n    if REC == SimpleRNN:\r\n        rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=two_state_shape)\r\n        stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=two_state_shape)\r\n    else:\r\n        rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=two_state_shape)\r\n        stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=two_state_shape)\r\n    rnn_inputs = [features_in, state_h_fwd_in, state_h_bwd_in]\r\n\r\nstateless_model = Model(inputs=rnn_inputs, outputs=rnn_out)\r\nstateful_model = Model(inputs=rnn_inputs, outputs=stateful_rnn_out)\r\n\r\n\r\n# toy_weights = [np.asarray([[ 1.0]], dtype=np.float32), np.asarray([[0.5 ]], dtype=np.float32), np.asarray([[ -1.0 ]], dtype=np.float32), np.asarray([[ -0.5 ]], dtype=np.float32)]\r\n# stateless_model.set_weights(toy_weights)\r\n# stateful_model.set_weights(toy_weights)\r\n\r\nstateful_model.set_weights( stateless_model.get_weights() )\r\n\r\nstateful_model.save('temp_stateful.h5')\r\nstateless_model.save('temp_stateless.h5')\r\n\r\nx_in = np.random.normal(0,10,sequence_length)\r\nx_in = np.asarray([1,0,0])\r\nx_in = x_in.reshape( (1, sequence_length, feature_dim) )\r\n\r\nfwd_initial_h = np.asarray(2.75).reshape(1,1)\r\nfwd_initial_c = np.asarray(1.3).reshape(1,1)\r\nbwd_initial_h = np.asarray(-2.0).reshape(1,1)\r\nbwd_initial_c = np.asarray(-1.2).reshape(1,1)\r\n\r\n# fwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)\r\n# fwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)\r\n# bwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)\r\n# fwd_initial_c = np.asarray(np.random.normal(0,10)).reshape(1,1)\r\n# bwd_initial_c = np.asarray(np.random.normal(0,10)).reshape(1,1)\r\n\r\nif REC == LSTM:\r\n    rnn_input = [x_in, fwd_initial_h, fwd_initial_c, bwd_initial_h, bwd_initial_c]\r\nelse:\r\n    rnn_input = [x_in, fwd_initial_h, bwd_initial_h] \r\n    \r\n\r\ndef print_bidi_out(non_stateful_out, stateful_out):\r\n\tfb = ['FWD::', 'BWD::']\r\n\r\n\tfor i in range(2):\r\n\t\tprint(fb[i])\r\n\t\tprint(f'non_stateful: {non_stateful_out.T[i]}')\r\n\t\tprint(f'stateful: {stateful_out.T[i]}')\r\n\t\tprint(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')\r\n\r\nnon_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))\r\nstateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))\r\nprint_bidi_out(non_stateful_out, stateful_out)\r\n\r\nnon_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))\r\nstateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))\r\nprint_bidi_out(non_stateful_out, stateful_out)\r\n\r\nprint('\\n** RESETING STATES in STATEFUL MODEL **\\n')\r\nstateful_model.reset_states()\r\nnon_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))\r\nstateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))\r\nprint_bidi_out(non_stateful_out, stateful_out)\r\n```\r\n\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nsample output for a SimpleRNN with input states -- using TF 2.1:\r\n\r\n```\r\nFWD::\r\nnon_stateful: [7.375   3.6875  1.84375]\r\nstateful: [7.375   3.6875  1.84375]\r\ndelta: [0. 0. 0.]\r\nBWD::\r\nnon_stateful: [ 11.5 -25.   50. ]\r\nstateful: [ 11.5 -25.   50. ]\r\ndelta: [0. 0. 0.]\r\nFWD::\r\nnon_stateful: [7.375   3.6875  1.84375]\r\nstateful: [1.921875   0.9609375  0.48046875]\r\ndelta: [-5.453125  -2.7265625 -1.3632812]\r\nBWD::\r\nnon_stateful: [ 11.5 -25.   50. ]\r\nstateful: [-2.4375  2.875  -5.75  ]\r\ndelta: [-13.9375  27.875  -55.75  ]\r\n\r\n** RESETING STATES in STATEFUL MODEL **\r\n\r\nFWD::\r\nnon_stateful: [7.375   3.6875  1.84375]\r\nstateful: [1.2402344 0.6201172 0.3100586]\r\ndelta: [-6.1347656 -3.0673828 -1.5336914]\r\nBWD::\r\nnon_stateful: [ 11.5 -25.   50. ]\r\nstateful: [-0.6953125 -0.609375   1.21875  ]\r\ndelta: [-12.1953125  24.390625  -48.78125  ]\r\n```\r\n\r\nCrash when using 4/7 tf-nightly:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"temp_bidi_state_in.py\", line 89, in <module>\r\n    stateful_model.reset_states()\r\n  File \"/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\", line 473, in reset_states\r\n    layer.reset_states()\r\n  File \"/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/wrappers.py\", line 676, in reset_states\r\n    self.forward_layer.reset_states()\r\n  File \"/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\", line 903, in reset_states\r\n    spec_shape = nest.flatten(self.input_spec[0])[0].shape\r\nAttributeError: 'NoneType' object has no attribute 'shape'\r\n```\r\n\r\n", "comments": ["Works without issues with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/4dcad65983174e0f482251cf99f534d0/38403.ipynb). Was able to reproduce the issue with [TF v2.2.0-rc2](https://colab.research.google.com/gist/amahendrakar/230e360bfceb18658b1c800e4a7fd6f6/38403-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/65a2ce796d53b8b649918ee5bbf36cb3/38403-tf-nightly.ipynb). Please find the attached gist. Thanks!", "> Works without issues with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/4dcad65983174e0f482251cf99f534d0/38403.ipynb). Was able to reproduce the issue with [TF v2.2.0-rc2](https://colab.research.google.com/gist/amahendrakar/230e360bfceb18658b1c800e4a7fd6f6/38403-2-2.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/65a2ce796d53b8b649918ee5bbf36cb3/38403-tf-nightly.ipynb). Please find the attached gist. Thanks!\r\n\r\nFor TF 2.1, it is not crashing, but it is not resetting the states in the stateful bidi models.  The results after the state-reset should be the same for the stateful and nonstateful models.  The only case that is working correctly is the when there is no initial-state-input and TF 2.2rc or tf-nightly is used.  This is consistent with my original post.  Sorry, it is a bit detailed, but here is teh summary:\r\n\r\n* TF 2.1: state reset for stateful models does nothing.  Should resets the state to zero if no initial state is input or back to the input initial state if it is provided (same as uni-directional RNNs).\r\n* TF 2.2rc and tf-nightly: behavior is correct when no initial state is provided, but reset_state crashes when initial-state is input to stateful models.\r\n\r\nFixing the 2.2/nightly behavior would be a great fix as 2.2 is on the way..  Thanks!!", "Thanks for reporting the issue. Will take a closer look.", "@keithchugg I think this was resolved in recent `tf-nightly` by @qlzh727 . I checked it with `tf-nightly` and cannot reproduce the issue. [Here](https://colab.research.google.com/gist/jvishnuvardhan/be5bbce7844f13ffc44fda50d392e2fb/38403-tf-nightly.ipynb) is the gist for your reference. \r\n\r\nPlease verify once and close the issue, if this was resolved for you. Thanks!", "Oh, thanks for the notice, I think this should be fixed already. I just forgot to close the github issue.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38403\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38403\">No</a>\n"]}, {"number": 38402, "title": "Smart Reply AAR example for tensorflow lite won't build ", "body": "I have not changed any code, and followed the procedure as per: https://github.com/tensorflow/examples/blob/master/lite/examples/smart_reply/android/how-to-build.md\r\n\r\nI tried to build on Windows, and Linux (16.04) and I get the following error from bazel: \r\nLinux: \r\namsha@amsha-linux:/local/mnt/workspace/workspace/tensorflowliteExamples/smartReply/examples/lite/examples/smart_reply/android/app$ bazel build libs/cc:smartreply_runtime_aar\r\nWARNING: Output base '/usr2/amsha/.cache/bazel/_bazel_amsha/09e97388c3884f6fff92e89b26f572b5' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nWARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nERROR: /local/mnt/workspace/workspace/tensorflowliteExamples/smartReply/examples/lite/examples/smart_reply/android/app/libs/cc/BUILD:163:1: //libs/cc:smartreply_runtime_aar_dummy_app_for_so: no such attribute 'aapt_version' in 'android_binary' rule\r\nERROR: error loading package 'libs/cc': Package 'libs/cc' contains errors\r\nINFO: Elapsed time: 0.401s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded)\r\n\r\nWindows:\r\nWARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found\r\nERROR: C:/workspace/bazelbuilds/smart_reply/android/app/libs/cc/BUILD:163:1: //libs/cc:smartreply_runtime_aar_dummy_app_for_so: no such attribute 'aapt_version' in 'android_binary' rule\r\nERROR: error loading package 'libs/cc': Package 'libs/cc' contains errors\r\nINFO: Elapsed time: 88.009s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded)\r\n\r\n", "comments": ["I am not too familiar with the bazel build system, however in:\r\nhttps://github.com/tensorflow/examples/blob/master/lite/examples/smart_reply/android/app/WORKSPACE\r\n\r\nthe WORKSPACE file makes a reference to:\r\nhttps://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/316e6133888bfc39fb860a4f1a31cfcbae485aef.tar.gz\r\n\r\nwhich gives a 404, rather then a tar.gz", "Hi Amit, \r\n\r\nLet me take a look at this and come back to you.\r\n\r\nIn general, if you just want to use gradle to build the app, you may download prebuilt AAR and forget about bazel (it is a [optional step](https://github.com/tensorflow/examples/blob/master/lite/examples/smart_reply/android/how-to-build.md#optional-how-to-build-aar-package-from-source-code)). That is done by gradle.", "Hi Amit,\r\n\r\nThe problem is from bazel, the build tool. The latest bazel removes this attribute \"aapt_version\". I guess you must be using bazel >= 2.0. \r\n\r\nAs TensorFlow has a minimum version requirement of 0.19, it is [this] (https://github.com/tensorflow/tensorflow/commit/c5dce1fd008d7e13ea8b735b80281867a38c9aef) that causes the breaking change with the bazel, while our dependency didn't update it.\r\n\r\n**A quick solution:** If you build custom ops with current C++ code, you can use bazel **1.x.x** (from [release](https://github.com/bazelbuild/bazel/releases), or https://bazel.build). Again, unless you want to build your own custom ops AAR from source, this step is optional. Gradle or directly using Android Studio can build Android app with prebuilt AAR. Hope it could help!\r\n\r\nAt the same time, we will figure out dependency update from our side. :-)\r\n", "Okay, I will try again with an old version of bazel. \r\n\r\nI have edited the C++ code so I can pass in which device to use (nnapi, gpu, cpu, number of threads, etc), and I have added a few instrumented tests. So, that would require me to rebuild the AAR.\r\n\r\nI have some related questions.\r\n\r\nIn android studio the default build tool for jni/native apps is CMAKE, and android studio has tooling such that, I can step into the C++ code, use watchlists, etc.\r\n\r\nWhat is the recommended (your own personal), work flow for making these types of native apps. \r\nDo you write the C++ in android studio, or VS Code, or something else. Are you building from commandline (using bazel)? How do you debug your native code in your workflow? Do you use a fully featured debugger?\r\n\r\nThere are a couple external dependencies in this project, tf lite, RE2, flatbuffers. I was in the middle of rebuilding this app in CMAKE. But then I realized the .a I made for RE2 was not made for arm64 devices. (I am a bit new to cmake and c++ in general)\r\n\r\nDo you recommend using bazel for such tasks? I believe I will just bite the bullet and transition to bazel since its the build tool of tensorflow lite (and I see many other google/android github projects using it). Do you have a recommended environment for writing bazel code, or do you do it in a regular text editor and just check for errors by running it. \r\n\r\nThen on the topic of custom ops. \r\n\r\nThere are two ways that I see of writing custom ops. You write the ops and then register them in your C++ code. Or, if you are doing everything in java/kotlin you can put your code in a specific file location and build a custom tflite .aar.\r\n\r\nHowever, in this project, I am unsure how the custom ops are being registered. I understand \"RegisterSelectedOps\" is related, but I cant find the documentation for it. \r\n\r\nI see that \"RegisterSelectedOps\" should be overridden, but where is that happening? I suspect it has to do with the build system, and how the .aar is constructed. In that regard, I am sure I will figure it out as my bazel skills increase, but if you have any incite for me, that would be appreciated. \r\n\r\nAgain, thank you for your time, and answering my questions, and any additional advice you have.", "Fresh projects with no code changes:\r\n\r\nI used bazel 1.2.1 on windows, and I get some error related to com_google_protobuf.\r\nIs this project supported on windows?\r\n\r\nOn ubuntu I had 3.0.0 installed, I then used \"sudo apt-get --purge remove bazel\" to uninstall.\r\nThen followed:\r\nhttps://docs.bazel.build/versions/master/install-ubuntu.html\r\nto install bazel-1.0.0, and then I renamed it to bazel in usr/bin/ \r\nand got a error related to \"'libs/cc/testdata': BUILD file not found in any of the following directories.\"\r\nWhich is true, there is no build file there, there is a build file located in the parent directory. \r\n\r\nI will learn bazel, to better debug these issues. Its a bit inconvenient dealing with an older version of bazel. \r\n\r\nFull error messages: \r\nOn Windows:\r\nC:\\workspace\\SmartReplyRebuild\\CleanSmartReply\\examples\\lite\\examples\\smart_reply\\android\\app>bazel version\r\nBuild label: 1.2.1\r\nBuild target: bazel-out/x64_windows-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Nov 26 15:24:17 2019 (1574781857)\r\nBuild timestamp: 1574781857\r\nBuild timestamp as int: 1574781857\r\n\r\nC:\\workspace\\SmartReplyRebuild\\CleanSmartReply\\examples\\lite\\examples\\smart_reply\\android\\app>bazel build libs/cc:smartreply_runtime_aar\r\nINFO: Call stack for the definition of repository 'com_google_protobuf' which is a tf_http_archive (rule definition at C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl:121:19):\r\n - C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/tensorflow/workspace.bzl:434:5\r\n - C:/workspace/smartreplyrebuild/cleansmartreply/examples/lite/examples/smart_reply/android/app/WORKSPACE:39:1\r\nINFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\r\nIf the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 101\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nINFO: Call stack for the definition of repository 'remotejdk11_win' which is a http_archive (rule definition at C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/utils.bzl:205:9\r\n - /DEFAULT.WORKSPACE.SUFFIX:249:1\r\nINFO: Call stack for the definition of repository 'remote_java_tools_windows' which is a http_archive (rule definition at C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/utils.bzl:205:9\r\n - /DEFAULT.WORKSPACE.SUFFIX:270:1\r\nINFO: Call stack for the definition of repository 'android_tools' which is a http_archive (rule definition at C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - /DEFAULT.WORKSPACE.SUFFIX:343:1\r\nERROR: Analysis of target '//libs/cc:smartreply_runtime_aar' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 101\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nINFO: Elapsed time: 6.725s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n    currently loading: @org_tensorflow//tensorflow\r\n\r\n\r\non ubuntu:\r\n\r\namsha@amsha-linux:/local/mnt/workspace/workspace/tfliteExample/SmartReply/examples/lite/examples/smart_reply/android/app$ bazel build libs/cc:smartreply_runtime_aar\r\nWARNING: Output base '/usr2/amsha/.cache/bazel/_bazel_amsha/a55405c8a948a2ec30cda52a5973c4a9' is on NFS. This may lead to surprising failures and undetermined behavior.\r\nINFO: Writing tracer profile to '/usr2/amsha/.cache/bazel/_bazel_amsha/a55405c8a948a2ec30cda52a5973c4a9/command.profile.gz'\r\nERROR: /local/mnt/workspace/workspace/tfliteExample/SmartReply/examples/lite/examples/smart_reply/android/app/libs/cc/BUILD:17:1: no such package 'libs/cc/testdata': BUILD file not found in any of the following directories.\r\n - /local/mnt/workspace/workspace/tfliteExample/SmartReply/examples/lite/examples/smart_reply/android/app/libs/cc/testdata and referenced by '//libs/cc:smartreply_ops'\r\nERROR: Analysis of target '//libs/cc:smartreply_runtime_aar' failed; build aborted: no such package 'libs/cc/testdata': BUILD file not found in any of the following directories.\r\n - /local/mnt/workspace/workspace/tfliteExample/SmartReply/examples/lite/examples/smart_reply/android/app/libs/cc/testdata\r\nINFO: Elapsed time: 0.333s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (1 packages loaded, 0 targets configured)\r\n    currently loading: @org_tensorflow//tensorflow/lite/schema\r\namsha@amsha-linux:/local/mnt/workspace/workspace/tfliteExample/SmartReply/examples/lite/examples/smart_reply/android/app$\r\n\r\n\r\n\r\n", "Hi Amit,\r\n\r\nFor your Ubuntu case, I find this too, since libs/cc/testdata/BUILD is mistakenly removed. Here is the content:\r\n```# libs/cc/testdata/BUILD\r\npackage(\r\n    default_visibility = [\"//visibility:public\"],\r\n    licenses = [\"notice\"],  # Apache 2.0\r\n)\r\n\r\nexports_files(glob([\"*\"]))\r\n\r\nfilegroup(\r\n    name = \"testdata\",\r\n    srcs = glob([\r\n        \"**/*\",\r\n    ])\r\n)\r\n```\r\n\r\n(Requires [step 1](https://github.com/tensorflow/examples/blob/master/lite/examples/smart_reply/android/how-to-build.md#step-1-download-pre-built-aar-package-containing-custom-ops) to download smartreply.tflite to that testdata folder)\r\n\r\nIt fixes the problem from my side.\r\n\r\nFYI, a patch is ongoing to fix this and update dependencies for the latest version of bazel. We will release it soon. :-)", "Okay, I will add the missing project files... and see if that works...\r\n\r\nAny further comments on my previous questions would be appreciated.", "Bazel is the open source build tool by Google, which supports cross platforms and language compilation. The BUILD of Bazel resolves external dependencies from other libraries. TensorFlow uses this, so if your changes need to depends on deep changes or customization related, it is recommended. (FYI, we are planning to provide more examples with other build system, in particular for those example applications in this repo.)\r\n\r\nThere is a tutorial [Building Android Apps with Bazel](https://codelabs.developers.google.com/codelabs/bazel-android-intro/index.html?index=..%2F..index#0). However, we provide gradle for the example apps is mainly because it is more common for Android development.\r\n\r\nTests guard the correctness of each component, so whatever text editor or IDEs should be fine. It is free for you to choose different tools for each part and put them together. My suggestion is that you may use Bazel to custom ops and lower-level interface to build AAR, and use gradle to create Android app. Our c/c++ interface contains the richest utilities for customization.\r\n", "Fresh Build:\r\n\r\nOn Ubuntu: works\r\nOn Windows: com_google_protobuf related error\r\n\r\nfull error:\r\nC:\\workspace\\SmartReplyRebuild\\CleanSmartReply\\examples\\lite\\examples\\smart_reply\\android\\app>bazel build libs/cc:smartreply_runtime_aar\r\nINFO: Call stack for the definition of repository 'com_google_protobuf' which is a tf_http_archive (rule definition at C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl:121:19):\r\n - C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/tensorflow/workspace.bzl:434:5\r\n - C:/workspace/smartreplyrebuild/cleansmartreply/examples/lite/examples/smart_reply/android/app/WORKSPACE:39:1\r\nINFO: Repository 'com_google_protobuf' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'b9e92f9af8819bbbc514e2902aec860415b70209f31dfc8c4fa72515a5df9d59' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/protocolbuffers/protobuf/archive/310ba5ee72661c081129eb878c1bbcec936b20f0.tar.gz\r\nIf the definition of 'com_google_protobuf' was updated, verify that the hashes were also updated.\r\nERROR: An error occurred during the fetch of repository 'com_google_protobuf':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 101\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nINFO: Call stack for the definition of repository 'remote_java_tools_windows' which is a http_archive (rule definition at C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/utils.bzl:205:9\r\n - /DEFAULT.WORKSPACE.SUFFIX:270:1\r\nINFO: Call stack for the definition of repository 'remotejdk11_win' which is a http_archive (rule definition at C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/utils.bzl:205:9\r\n - /DEFAULT.WORKSPACE.SUFFIX:249:1\r\nINFO: Call stack for the definition of repository 'android_tools' which is a http_archive (rule definition at C:/users/amsha/_bazel_amsha/sqsdm6y6/external/bazel_tools/tools/build_defs/repo/http.bzl:292:16):\r\n - /DEFAULT.WORKSPACE.SUFFIX:343:1\r\nERROR: Analysis of target '//libs/cc:smartreply_runtime_aar' failed; build aborted: no such package '@com_google_protobuf//': Traceback (most recent call last):\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 101\r\n                _apply_patch(ctx, <1 more arguments>)\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 67, in _apply_patch\r\n                _wrap_bash_cmd(ctx, <1 more arguments>)\r\n        File \"C:/users/amsha/_bazel_amsha/sqsdm6y6/external/org_tensorflow/third_party/repo.bzl\", line 28, in _wrap_bash_cmd\r\n                fail(<1 more arguments>)\r\nBAZEL_SH environment variable is not set\r\nINFO: Elapsed time: 6.728s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)\r\n    currently loading: @org_tensorflow//tensorflow\r\n", "In the readme, it should be noted that it doesn't work on windows, and that an old version of bazel is needed, presently.\r\n\r\nAlso, yes I have done the bazel tutorial on the getting started page... and I understand the concept of tests. Thanks for the advice...\r\n\r\nAnd, I guess I will just have to read the codebase to figure out the rich C++ feature set...", "Hi Amit, \r\n\r\nWe updated the custom ops AAR [dependencies](https://github.com/tensorflow/examples/commit/9afaa3dd04c4c83635f723dee5e328aeaf418e3b#diff-4c4d100d288346383c90bbbe0d61dc07R37), and polished the [guidance](https://github.com/tensorflow/examples/blob/master/lite/examples/smart_reply/android/how-to-build.md#optional-how-to-build-aar-package-from-source-code). You may follow it to build your own AAR. Please let us know if you have any question. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38402\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38402\">No</a>\n", "Hi Tian,\r\nWhy is the .aar that I built twice the size of the default .aar (I have not included any of my additional code).\r\n\r\nI also tried the '-c opt' option as specified:\r\n\r\n\"bazel build -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a cc:smartreply_runtime_aar\"\r\n\r\nwhich is also wrong, it should be:\r\nbazel build -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a libs/cc:smartreply_runtime_aar\r\n\r\nThank you for the time.\r\n\r\n"]}, {"number": 38401, "title": "Update libjpeg-turbo 2.0.0 => 2.0.4", "body": "This PR updates libjpeg-turbo from 2.0.0 (July 2018) to the latest 2.0.4 (Dec 2019),\r\nas there are quite a few CVE related vulnerabilities\r\n(https://cve.mitre.org/cgi-bin/cvekey.cgi?keyword=libjpeg) since then.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@mihaimaruseac I updated the PR to always use the full line (with `1` at the end) for replacement rule in bazel:\r\n```\r\n\"#cmakedefine HAVE_STDLIB_H 1\": \"#define HAVE_STDLIB_H 1\",\r\n```\r\n\r\nI think this will make the rule clear (and helps in future maintenance). Please take a look."]}, {"number": 38400, "title": "[tf-2.2] XNNPack doesn't compile on AArch64", "body": "**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspberry Pi 4 or cross-compiling on Ubuntu 18.04 using #38399\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: r2.2 branch \r\n- Python version: 3.6\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 2.0.0\r\n- GCC/Compiler version (if compiling from source): Arm gcc 9.2.1\r\n\r\n**Describe the problem**\r\nCurrently XNNPack on the TensorFlow 2.2 branch doesn't compile on AArch64. When trying to compile the TFLite benchmark binary for the Raspberry PI 4 64-bit, compilation fails with:\r\n```\r\nERROR: /root/.cache/bazel/_bazel_root/ba9f8c20da37904a26a4b29ac8536dec/external/XNNPACK/BUILD.bazel:1643:1: C++ compilation of rule '@XNNPACK//:neonfp16arith_ukernels' failed (Exit 1)\r\nexternal/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c: In function 'xnn_f16_spmm_ukernel_32x1__neonfp16arith_unroll2':\r\nexternal/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c:219:59: error: incompatible type for argument 1 of 'vreinterpret_f32_f16'\r\n  219 |             const float16x4_t va01 = vreinterpret_f32_f16(vld1_dup_f32(__builtin_assume_aligned(a, 1)));\r\n      |                                                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n      |                                                           |\r\n      |                                                           float32x2_t\r\nIn file included from external/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c:12:\r\n/root/.cache/bazel/_bazel_root/ba9f8c20da37904a26a4b29ac8536dec/external/aarch64_compiler/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/include/arm_neon.h:4176:35: note: expected 'float16x4_t' but argument is of type 'float32x2_t'\r\n 4176 | vreinterpret_f32_f16 (float16x4_t __a)\r\n      |                       ~~~~~~~~~~~~^~~\r\nexternal/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c:227:76: error: incompatible type for argument 1 of 'vreinterpret_f16_f32'\r\n  227 |         vst1_lane_f32(__builtin_assume_aligned(c, 1), vreinterpret_f16_f32(vout01), 0);\r\n      |                                                                            ^~~~~~\r\n      |                                                                            |\r\n      |                                                                            float16x4_t\r\nIn file included from external/XNNPACK/src/f16-spmm/gen/32x1-neonfp16arith-unroll2.c:12:\r\n/root/.cache/bazel/_bazel_root/ba9f8c20da37904a26a4b29ac8536dec/external/aarch64_compiler/bin/../lib/gcc/aarch64-none-linux-gnu/9.2.1/include/arm_neon.h:4022:35: note: expected 'float32x2_t' but argument is of type 'float16x4_t'\r\n 4022 | vreinterpret_f16_f32 (float32x2_t __a)\r\n      |                       ~~~~~~~~~~~~^~~\r\nTarget //tensorflow/lite/tools/benchmark:benchmark_model failed to build\r\n```\r\nI believe this problem was fixed in https://github.com/google/XNNPACK/commit/d21fdcb4af35a8764333c3bb24fa54c55bb5db52 and tensorflow master compiles without a problem.\r\n\r\nSince AArch64 was supported in TF 2.1, @Maratyszcza do you think it would be possible to backport the fixes to the 2.2 release branch?\r\n\r\nIt probably will be enough to cherry-pick 06ca7fc73ca96d9a468d771273be377ed4cc6ed2 and cd59b293b5b7f9f337ba37becdb850e1278ac0c0 in order to upgrade XNNPack, but not sure if this would have any side effects.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nThis can be reproduced on a normal linux system by using the cross-compile toolchain from PR #38399 and compiling the TFLite benchmark binary using:\r\n```\r\nbazelisk build -c opt tensorflow/lite/tools/benchmark:benchmark_model \\\r\n    --cpu=aarch64 --crosstool_top=@local_config_arm_compiler//:toolchain\r\n```", "comments": ["Doesn't #38131 solve it already?", "Generally, cherry-picking commits which update XNNPACK/pthreadpool/cpuinfo is safe. XNNPACK maintains backward-compatible API for TensorFlow Lite.", "> Doesn't #38131 solve it already?\r\n\r\n#38131 solved part of the problem, but unfortunately it still failed later in the compilation due to the gcc error mentioned above.", "You may try to add `--copt=-flax-vector-conversions` to Bazel command", "Thanks for the help\r\n\r\n> You may try to add `--copt=-flax-vector-conversions` to Bazel command\r\n\r\nUnfortunately this doesn't seem to help, I still get the same error.", "#38436 Fixes the issues for us, though not sure if there is a more elegant solution.", "#38436 has been merged, so this issue is resolved. Thanks for the help and the fast review.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38400\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38400\">No</a>\n", "Hi @Maratyszcza ,\r\n\r\nI installed tf-nightly 2.4 by pip. Would XNNPACK complied by default? How can I check it? \r\n\r\nThank you"]}, {"number": 38399, "title": "Add aarch64 cross compile toolchain for TFLite", "body": "This PR extends the current Arm toolchain by adding an `aarch64` bazel toolchain based on [Arm GCC 9.2](https://developer.arm.com/tools-and-software/open-source-software/developer-tools/gnu-toolchain/gnu-a/downloads). This e.g. allows to cross compile TFLite with `bazel` for the Raspberry Pi 4 running on a 64-bit OS.\r\n\r\nThis has can be tested by compiling the TFLite benchmark binary on Linux by running:\r\n```\r\nbazelisk build -c opt tensorflow/lite/tools/benchmark:benchmark_model \\\r\n    --cpu=aarch64 --crosstool_top=@local_config_arm_compiler//:toolchain\r\n```", "comments": ["Hi, @lgeiger \r\nInterestingly, I was making the same change for TFLite toolchain. In my experiment, GCC 9.2 requires glibc 2.30 which is not yet popular. Didn't you have difficult in running output binary?\r\n", "We used an ARM gcc 8.3 based toolchain on a Rapberry Pi 4 before. When making the PR I upgraded to 9.2 and re-ran some test in QEMU sucessfully, speaking of glibc I am not 100% sure if I re-tested on an actual device though. I'll double check tomorrow and will get back to you.", "Any chance glibc-2.17 for manylinux2014 wheel spec be supported? The front end could be gcc-7/8/9 thanks to RH\u2019s devtoolset.", "@lgeiger my RPI3 has libc-2.28.so even after running \"apt-get update libc6\". I'll wait for your test result. thanks!\r\n\r\n@byronyi I'm not familiar with manylinux2014 and RH\u2019s devtoolset. Do you want to have a toolchain which supports glibc-2.17? ", "> @byronyi I'm not familiar with manylinux2014 and RH\u2019s devtoolset. Do you want to have a toolchain which supports glibc-2.17?\r\n\r\ndevtoolset-X is a back port of gcc-X to older version of RHEL/CentOS (so also older versions of glibc, libstdcxx, etc.). \r\n\r\nFor example, devtoolset-7 on CentOS 6 provides gcc-7.3 using glibc-2.12, and devtoolset-8 on CentOS 7 provides gcc-8.2 using glibc-2.17. Python packaging on Linux has followed these standards.\r\n\r\n| OS version | glibc version | Python wheel standard |\r\n|:---:|:---:|:---:|\r\n| CentOS 5 | glibc-2.5 | [manylinux1](https://www.python.org/dev/peps/pep-0513/) |\r\n| CentOS 6 | glibc-2.12 | [manyinux2010](https://www.python.org/dev/peps/pep-0571/) |\r\n| CentOS 7 | glibc-2.17 | [manylinux2014](https://www.python.org/dev/peps/pep-0599/) |\r\n\r\nTensorFlow Python package is built with [devtoolset-7](https://github.com/tensorflow/tensorflow/commit/2afcda57062b5ebb94c35a2bbd3581c9b8bc7393) and glibc-2.12 [starting from TF 1.15](https://www.tensorflow.org/install/source#cpu). \r\n\r\nHowever, manylinux2014 is the first packaging standard that considers multi-arch support. IBM has supplied their [ppc64le TF builds](https://github.com/tensorflow/build/blob/master/images/ppc64le/gpu/Dockerfile.manylinux_2014.cuda10_1) using the manylinux2014 standard. I am wondering if we could do the same for aarch64.\r\n\r\n", "> Didn't you have difficult in running output binary?\r\n\r\nI just double checked and I am able to build the model benchmark binary with this toolchaing and run it on the Raspberry Pi 4.\r\nI forgot to mention earlier that although it has a 64-bit CPU the default Raspbian OS is a 32-bit OS, so I opted to use a custom 64-bit OS which apparently comes with a newer version of glibc. I'm currently using [Manjaro](https://manjaro.org/download/#raspberry-pi-4-xfce).", "@lgeiger thanks for the confirmation. So it's not a Rasbian release. Good to know about manjaro.\r\n\r\n@byronyi  thanks for the information. We need a cross toolchain for aarch64 which relies on glibc-2.17 but I can't find it. I've checked older versions of Linaro toolchains but failed. https://releases.linaro.org/components/toolchain/binaries/\r\nWhich target OS are you using?  Can't you upgrade your target OS?", "> @lgeiger thanks for the confirmation. So it's not a Rasbian release. Good to know about manjaro.\r\n> \r\n> @byronyi thanks for the information. We need a cross toolchain for aarch64 which relies on glibc-2.17 but I can't find it. I've checked older versions of Linaro toolchains but failed. https://releases.linaro.org/components/toolchain/binaries/\r\n> Which target OS are you using? Can't you upgrade your target OS?\r\n\r\nAs I said, manylinux2014 is a Python packaging spec, and it roughly translates to CentOS 7. Being compatible to this spec means the binary could be installed/distributed using pip on PyPI.\r\n\r\ncc @perfinion", "I took a look at devtoolset and it does have aarch64 support: http://mirror.aktkn.sg/centos-altarch/7/sclo/aarch64/rh/devtoolset-7/"]}, {"number": 38398, "title": "[compiler] Correct dims_rhs constraint", "body": "Correct dims_rhs constraint", "comments": ["@joker-eph thanks for the review. Could you kindly point me to the existing coverage/example for this (type of) functionality ?", "Here is the current test https://github.com/tensorflow/tensorflow/blob/316054e5c1d71285970951dc097db4d45f1e1567/tensorflow/compiler/mlir/tensorflow/tests/einsum.mlir", "@joker-eph not familiar with the mlir format. Added a test cases (5D), please advise. thank you."]}, {"number": 38397, "title": "In Tensorflow Lite, Instance norm takes a lot of time", "body": "**System information**\r\n- OS Platform and Distribution: window10\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (or github SHA if from source):1.15.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\n```\r\n2020-04-09 23:44:06.327454: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\n2020-04-09 23:44:06.620150: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)\r\n2020-04-09 23:44:06.631412: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2020-04-09 23:44:06.870610: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-04-09 23:44:06.879972: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 295 nodes (-88), 348 edges (-90), time = 80.397ms.\r\n2020-04-09 23:44:06.889699: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 295 nodes (0), 348 edges (0), time = 26.95ms.\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nI'm trying to run a [Fast Style Transfer](https://github.com/lengstrom/fast-style-transfer) model in my Android phone. I find when I use batchnorms in this network, it takes very little time (40ms), but when I use instance norms, it takes a lot of time (215ms). \r\n\r\nThe Batch norm mentioned here is `tf.layers.batch_normalization()`. The CPU of my Android phone is snapdragon 845, and in TensorFlow Lite I use the GPU mode (GpuDelegate). \r\n\r\n[Image Transform network](https://github.com/lengstrom/fast-style-transfer/blob/master/src/transform.py):\r\n```\r\ndef net(image):\r\n    conv1 = _conv_layer(image, 32, 9, 1)\r\n    conv2 = _conv_layer(conv1, 64, 3, 2)\r\n    conv3 = _conv_layer(conv2, 128, 3, 2)\r\n    resid1 = _residual_block(conv3, 3)\r\n    resid2 = _residual_block(resid1, 3)\r\n    resid3 = _residual_block(resid2, 3)\r\n    resid4 = _residual_block(resid3, 3)\r\n    resid5 = _residual_block(resid4, 3)\r\n    conv_t1 = _conv_tranpose_layer(resid5, 64, 3, 2)\r\n    conv_t2 = _conv_tranpose_layer(conv_t1, 32, 3, 2)\r\n    conv_t3 = _conv_layer(conv_t2, 3, 9, 1, relu=False)\r\n    preds = tf.add(tf.nn.tanh(conv_t3) * 150,255./2, name=\"output\")\r\n    return preds\r\n```\r\nInstance norm:\r\n```\r\ndef _instance_norm(net, train=True):\r\n    batch, rows, cols, channels = [i.value for i in net.get_shape()]\r\n    var_shape = [channels]\r\n    mu, sigma_sq = tf.nn.moments(net, [1, 2], keep_dims=True)\r\n    shift = tf.Variable(tf.zeros(var_shape))\r\n    scale = tf.Variable(tf.ones(var_shape))\r\n    epsilon = 1e-3\r\n    normalized = (net-mu)*tf.rsqrt((sigma_sq + epsilon))\r\n    return scale * normalized + shift\r\n```\r\nI used the following version of TensorFlow Lite:\r\n```\r\nimplementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'\r\nimplementation 'org.tensorflow:tensorflow-lite-support:0.0.0-nightly'\r\n```\r\n\r\n**Any other info / logs**\r\nWhen I use the Instance norm, the model requires an average of 215ms in CPU mode and 205ms in GPU mode (stylize a image of 128*128 pixels). I'm sure the GPU is working, but it seems like the Instance norm is running on the CPU, so there's no obvious time decrease. How can I improve the speed of the instance norm in TensorFlow Lite\uff1f\r\n", "comments": ["[model_batchnorm](https://drive.google.com/file/d/1lJQCnTCNypn57M31DtktRJ0YaBLAlUkP/view?usp=sharing)\r\n[model_instance_norm](https://drive.google.com/file/d/1TOdpWbOTTsVLVUMWROS_Sde5rKBXCP38/view?usp=sharing)\r\n[model_without_norm](https://drive.google.com/file/d/1l4c0MGoEKN_m177tcodHdx_rNTzVYtIY/view?usp=sharing)\r\n\r\ninput shape: 1\\*128\\*128\\*3, tf.float32\r\noutput shape: 1\\*128\\*128\\*3, tf.float32 \r\n\r\nmy test results (num_thread=4):\r\n| model | CPU | GPU |\r\n|  :----: |  :----: | :----: |\r\n| model_instance_norm | 215.7ms | 205.1ms |\r\n| model_batch_norm | 169.6ms | 40.6ms |\r\n| model_without_norm | 167.7ms | 41.4ms |\r\n\r\nThese tflite files were generated by this code:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef pb2tflite(pb_file, tflite_file):\r\n    convert = tf.lite.TFLiteConverter.from_frozen_graph(pb_file,\r\n                                                        input_arrays=[\"input\"],\r\n                                                        output_arrays=[\"output\"])\r\n    convert.target_spec.supported_types = [tf.float32]\r\n    tflite_model = convert.convert()\r\n    open(tflite_file, \"wb\").write(tflite_model)\r\n\r\npb2tflite(pb_file=\"model.pb\", tflite_file=\"model.tflite\")\r\n```", "There are some unsupported ops of GPU delegate in the instance norm graph.\r\nhttps://www.tensorflow.org/lite/performance/gpu_advanced#supported_ops", "> There are some unsupported ops of GPU delegate in the instance norm graph.\r\n> https://www.tensorflow.org/lite/performance/gpu_advanced#supported_ops\r\n\r\nI investigated the difference between Batch norm and Instance norm, I found that the Batch norm layer will calculate the moving mean and moving variance when training, but use the learned moving value when testing, so the `tf.nn.moments` was only used at training time in BN layer. I think is the reason why Batch norm is faster than Instance norm. I'm not sure this idea is right.", "In mobile environment, to use GPU acceleration fully, the ops in the graph should be supported by the TensorFlow Lite GPU delegate. The instance norm based TFLite graph contains some unsupported ops of GPU delegate. Like your performance experiments, the executions of many parts of the graph will be fall back to CPU calculations.", "I solved this problem using other framework.", "That list of supported ops is bit outdated. You can find the actual list from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/common/model_builder.cc#L2447\r\n\r\nI've tested the model_instance_norm and it works well with GPU delegate.\r\nIn Pixel 4, GPU takes 15ms while CPU takes 263ms.\r\nIt might be worth to try it with recent TFLite versions.", "> That list of supported ops is bit outdated. You can find the actual list from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/gpu/common/model_builder.cc#L2447\r\n> \r\n> I've tested the model_instance_norm and it works well with GPU delegate.\r\n> In Pixel 4, GPU takes 15ms while CPU takes 263ms.\r\n> It might be worth to try it with recent TFLite versions.\r\n\r\nWhich version do you use?", "> Which version do you use?\r\n\r\nI used master branch.\r\n"]}, {"number": 38396, "title": "Keras `model_from_json` ignores distribution strategy", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): CentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if\r\nthe issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or\r\nbinary): Binary\r\n - TensorFlow version (use command below): \r\n- Python version: 2.0.1\r\n - Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from\r\nsource):  N/A\r\n- CUDA/cuDNN version: 10.0\r\n- GPU model and memory: Tesla V100 (affects multiple sizes)\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nWhen loading a model using `tf.keras.models.model_from_json`, TensorFlow ignores the distribution strategy.  If you try to train or predict with such a loaded model with a `MirroredStrategy`, for example, it will still only use a single GPU.\r\n\r\n**Describe the expected behavior**\r\n\r\n`tf.keras.models.model_form_json` should use the provided strategy scope and distribute accordingly.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\nFirst we'll make a silly model, here with a strategy and verify it uses it properly.  How we create it here, however, is irrelevant to the strategy under which we load it later.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n## silly model of sufficient size to notice spike in memory\r\nwith tf.distribute.MirroredStrategy().scope():\r\n    m = tf.keras.models.Sequential()\r\n    m.add(tf.keras.layers.Conv2D(256,5, input_shape=(256,256,1)))\r\n    m.add(tf.keras.layers.Conv2D(256,5))\r\n    m.add(tf.keras.layers.Conv2D(256,5))\r\n    m.add(tf.keras.layers.Conv2D(256,5))\r\n    m.add(tf.keras.layers.Conv2D(256,5))\r\n    m.add(tf.keras.layers.Conv2D(256,5))\r\n    m.add(tf.keras.layers.Conv2D(256,5))\r\n    m.compile(optimizer='sgd', loss='mse')\r\n\r\n## save model to json\r\nwith open('foo_model.json','w') as f:\r\n    print(m.to_json(), file=f)\r\n\r\n## test that we're using multiple GPUs\r\ndata = np.random.random(size=(1024,256,256,1))\r\ndata = np.array(data,dtype=np.float32)\r\n\r\nans = m.predict(data) # observe multiple GPUs spike in memory and utilization\r\n```\r\n\r\nNow, we load the saved model, using MirroredStrategy, but we find that TensorFlow ignores this strategy.  It seems to load the models initially onto all available GPUs (taking up a nominal amount of memory), but then only does computations on a single GPU.  The data never makes it to multiple GPUs, slowing down computations and causing memory problems.\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndata = np.random.random(size=(1024,256,256,1))\r\ndata = np.array(data,dtype=np.float32)\r\n\r\nwith open('foo_model.json','r') as f:\r\n    J = f.read()\r\n\r\n# use a strategy, see it be ignored\r\nwith tf.distribute.MirroredStrategy().scope():\r\n    m = tf.keras.models.model_from_json(J) # see modest spike in memory as seems to load onto multiple gpu\r\n\r\nans = m.predict(data) # only a single GPU receives data\r\n```\r\n\r\nI understand that loading from a TF-saved model is supposed to work.  Because of other TensorFlow bugs, however, I have to save my models in the portable JSON format.  I believe this more or less rebuilds the whole model and reloads weights.  That should mean that the strategy gets applied properly, but we see that's not the case.  Is `model_from_json` overwriting or otherwise ignoring the strategy somewhere?\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["Hi Anjali, can you take a look? The model appears to have a distribution strategy (`m2._distribution_strategy` = mirrored stratetgy), and the variables are mirrored, so I'm not sure why the model isn't utilizing the GPU.", "@k-w-w I think `m2._distribution_strategy` *doesn't* get set correctly is the problem.  It's set fine in the initial model creation.  If you check after loading, however, it's `None`.\r\n\r\nBut you gave me an idea for a workaround that I think resolves this:\r\n\r\n```python\r\nstrat = tf.distribute.MirroredStrategy()\r\nwith strat.scope():\r\n    m = tf.keras.models.model_from_json(J)\r\n\r\nif m._distribution_strategy is None:\r\n    m._distribution_strategy = strat\r\n    m._distributed_function_cache = {}\r\n    m._distributed_model_cahce = {}\r\n```\r\n\r\nA quick test with this showed multiple GPUs being used.  This should be fine for my purposes.\r\n\r\nThe underlying issue, I suspect, is the Keras deserialization isn't aware of distribution strategies.  I poked around in the code base but it wasn't clear the fix might need to go.  Maybe [here](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/utils/generic_utils.py#L281)", "Hmm, this might be a bug with an earlier version of Tensorflow. I tried running it in colab (which is currently on version 2.2.0-rc3), and the `_distribution_strategy` attribute is set correctly. Can you try updating and checking again?", "Unfortunately I don't have administrative privileges where I run TF, so I'm unable to try this with a different version.  Are you saying this is already fixed in 2.2.0?", "I'm not able to test the GPU utilization but I believe it should be.", "Hi @dvbuntu, just ran a quick test on GCP and it seems that both GPUs are utilized. Have you had a chance to try this out with a more recent version of TF?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38396\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38396\">No</a>\n"]}, {"number": 38395, "title": "Generalize compiling libtensorflow-lite.a for arm32", "body": "**System information**\r\n- TensorFlow version (you are using):N/A\r\n- Are you willing to contribute it (Yes/No): Yes (maybe if I can figure it out haha)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nRight now cross compiling `libtensorflow-lite.a` will be either for [arm64](https://www.tensorflow.org/lite/guide/build_arm64) (which works great, thanks) and [rpi](https://www.tensorflow.org/lite/guide/build_rpi). I understand the build for rpi will mostly be for arm32 bits, why not make that more generalized for any arm32 system?\r\nAlso, with rpi4 in the market, we are running into issues where processor can support armv8 (64 bits) but the official OS is yet to support 64 bits. What happens when users start installing 64bits OS and realized that their rpi build isn't working?\r\n\r\n**Will this change the current api? How?**\r\nNo changes to the api at all as far as I'm concern, the tutorial may just be more generalized as a user.\r\n\r\n**Who will benefit with this feature?**\r\nAnybody with a 32bits arm machine :)\r\n\r\nPlease correct me if I understand this incorrectly or if this issue has been raised before!\r\nThanks\r\n", "comments": ["I've just checked in ARM crossbuild toolchains (https://github.com/tensorflow/tensorflow/commit/4961f18733ca3967198393abf419e14476b4a85c)  for Bazel build.\r\n\r\nYou can build armhf, aarch64 targets with the following commands.\r\n\r\n```sh\r\n$ bazel build --config=elinux_armhf  //tensorflow/lite/tools/benchmark:benchmark_model \r\n$ bazel build --config=elinux_aarch64  //tensorflow/lite/tools/benchmark:benchmark_model \r\n```", "@terryheo sorry, I didn't stated my full intention. The problem is that I'm trying to build tflite natively on different platform, without needing to first cross compile the `lib-tensorflowlite.a` first. The problem with this approach is that `bazel` doesn't support arm platform. So I'm working with cmake instead and I'm using your [makefile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/Makefile) which leads to this `rpi_makefile.inc` from [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/make/Makefile#L244).\r\nI see that the included makefile above has 'armv7l' and a `TARGET_ARCH`, so I guess my concerns is that why not make this a `arm32_makefile` instead of rpi_makefile?\r\n\r\nHere is my specific use case:\r\nLet say I wanted to add tflite as an ExternalProject in cmake:\r\n\r\n```\r\nExternalProject_Add(tflite\r\n    GIT_REPOSITORY https://github.com/tensorflow/tensorflow\r\n    PREFIX \"tensorflow\"\r\n    CONFIGURE_COMMAND ./tensorflow/lite/tools/make/download_dependencies.sh\r\n    BUILD_IN_SOURCE 1\r\n    BUILD_COMMAND ${TFLITE_BUILD_COMMAND}\r\n${CMAKE_BINARY_DIR}/tensorflow/src/tf/tensorflow/lite/tools/make/gen/${TF_INSTALL_PREFIX}/lib/libtensorflow-lite.a ${CMAKE_BINARY_DIR}/\r\n)\r\n```\r\nThen I need to set the `${TFLITE_BUILD_COMMAND}` variable, that's the variable that I'm talking about and here is how I'm setting it:\r\n\r\n```\r\n# GET CPU Architechture\r\nif(${CMAKE_SYSTEM_PROCESSOR} STREQUAL \"x86_64\")\r\n    set(EDGE_ARCH \"k8\")\r\n    set(TFLITE_BUILD_COMMAND make -j8 BUILD_WITH_NNAPI=false -C ${CMAKE_BINARY_DIR}/tensorflow/src/tflite -f tensorflow/lite/tools/make/Makefile lib)\r\n    set(TF_INSTALL_PREFIX \"linux_x86_64\")\r\nelseif(${CMAKE_SYSTEM_PROCESSOR} STREQUAL \"aarch64\")\r\n    set(EDGE_ARCH \"${CMAKE_SYSTEM_PROCESSOR}\")\r\n    set(TF_INSTALL_PREFIX \"generic-aarch64_armv8-a\")\r\n    set(CROSS_PREFIX \"aarch64-linux-gnu-\")\r\n    set(TFLITE_BUILD_COMMAND make -j4 TARGET=generic-aarch64 TARGET_ARCH=armv8-a -C ${CMAKE_BINARY_DIR}/tensorflow/src/tflite -f tensorflow/lite/tools/make/Makefile CC=${CROSS_PREFIX}g++ CXX=${CROSS_PREFIX}g++ AR=${CROSS_PREFIX}ar CFLAGS=-fpermissive lib)\r\nelseif(${CMAKE_SYSTEM_PROCESSOR} STREQUAL \"armv7l\")\r\n    message(\"We are going to assume that this build is for the rpi\")\r\n    set(EDGE_ARCH \"${CMAKE_SYSTEM_PROCESSOR}\")\r\n    set(TF_INSTALL_PREFIX \"rpi_armv7l\")\r\n    set(CROSS_PREFIX \"arm-linux-gnueabihf-\")\r\n    set(TFLITE_BUILD_COMMAND make -j4 TARGET=rpi TARGET_ARCH=armv7l -C ${CMAKE_BINARY_DIR}/tensorflow/src/tflite -f tensorflow/lite/tools/make/Makefile CC=${CROSS_PREFIX}g++ CXX=${CROSS_PREFIX}g++ AR=${CROSS_PREFIX}ar CFLAGS=-fpermissive lib)\r\nelse()\r\n\tmessage(FATAL_ERROR \"Not implemented to build for: ${CMAKE_SYSTEM_PROCESSOR}\")\r\nendif()\r\n```\r\n\r\n**The only problem** is that in the case of \"armv7l\" arch, there is no way to just specify TARGET=arm32 because we don't have an arm32_makefile.inc file.\r\n\r\nThis is not a huge issue, and I image the rpi makefile should just works but why not generalizing it?\r\n\r\nOn top of that, the rpi_makefile.inc is technically not correct, because it doesn't account the cases where the pi4 is using a 64 bits OS (aarch64)."]}, {"number": 38394, "title": "how to know the GPU's num and specify with tf ?", "body": "hi,dear\r\nhaven't seen the methed of tf codes,\r\nSo I just want to solve the problem.\r\nCould you please help me ?\r\n\r\nthx", "comments": ["@ucas010 \r\nplease follow https://www.tensorflow.org/guide/gpu to know gpu's available", "@ucas010\r\nplease update as per above comment", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 38393, "title": "[C++] set_visible_device_list raise RegisterAlreadyLocked when creating a session", "body": "**System information** \r\n- Have I written custom code (as opposed to using a stock\r\nexample script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g.,\r\nLinux Ubuntu 16.04): ubuntu 18.04\r\n- TensorFlow installed from (source or\r\nbinary): source\r\n- TensorFlow version (use command below): 1.15.2 or 2.2.0\r\n- GCC/Compiler version (if compiling from\r\nsource): 7.5.0\r\n- CUDA/cuDNN version: cuda 10.0 or 10.1\r\n- GPU model and memory: Nvidia 1080 ti\r\n\r\n**Describe the current behavior**\r\n\r\nThe function NewSession raise an exception when setting `set_visible_device_list`:\r\n```\r\n2020-04-09 14:06:36.112937: F tensorflow/core/framework/op.cc:214] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: No attr with name '0' for input 'constants'; in OpDef: name: \"XlaLaunch\" input_arg { name: \"constants\" description: \"0\" type_attr: \"0\" number_attr: \"0\" type_list_attr: \"Tconstants\" } input_arg { name: \"args\" description: \"0\" type_attr: \"0\" number_attr: \"0\" type_list_attr: \"Targs\" } input_arg { name: \"resources\" description: \"0\" type: DT_RESOURCE type_attr: \"0\" number_attr: \"Nresources\" type_list_attr: \"0\" } output_arg { name: \"results\" description: \"0\" type_attr: \"0\" number_attr: \"0\" type_list_attr: \"Tresults\" } attr { name: \"Tconstants\" type: \"list(type)\" description: \"0\" has_minimum: true } attr { name: \"Targs\" type: \"list(type)\" description: \"0\" has_minimum: true } attr { name: \"Nresources\" type: \"int\" description: \"0\" has_minimum: true } attr { name: \"Tresults\" type: \"list(type)\" description: \"0\" has_minimum: true } attr { name: \"function\" type: \"func\" description: \"0\" } summary: \"XLA Launch Op. For use by the XLA JIT only.\" description: \"0\" is_stateful: true\r\nAborted (core dumped)\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThis should not raise. It was working in tf 1.13.2. But since 1.15.2 and 2.2.0 it doesn't work.\r\n\r\n**Standalone code to reproduce the issue** \r\n\r\n```c++\r\n#include \"tensorflow/core/public/session.h\"\r\n#include \"tensorflow/core/public/session_options.h\"\r\n\r\nint main()\r\n{\r\n    tensorflow::SessionOptions session_options;\r\n    session_options.config.mutable_gpu_options()->set_visible_device_list(\"0\");\r\n\r\n    auto session = tensorflow::NewSession(session_options);\r\n}\r\n```\r\n\r\nYou can use `floopcz/tensorflow_cc:ubuntu-cuda-2.2.0` (you will need to install protobuf.so it seems) and compile with \r\n```\r\ng++ new_session.cpp -I/usr/local/include/tensorflow/bazel-bin/tensorflow/include -I/usr/local/include/eigen3 -I/usr/local/include/tensorflow/bazel-bin/tensorflow/include/src/ -ltensorflow_cc -lprotobuf\r\n```\r\n\r\nI can't thus load a saved model using `LoadSavedModel` (which call `NewSession`) on a specific gpu.", "comments": ["In 1.13.2 I had a build without monolithic config, but because of a boringssl conflict I had to build in monolithic for the newer versions. \r\nIt seems to be related to https://github.com/tensorflow/tensorflow/issues/1888#issuecomment-369407156", "@maingoh TF 2.2 final version has released. Is this still an issue with latest TF version?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38393\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38393\">No</a>\n", "Yes, This issue still exist in TensorFlow v2.2.0", "This issue still exists in Tensorflow v2.4.0 as well. Please reopen.", "This issue still exists in Tensorflow v2.4.1 as well. Please reopen."]}, {"number": 38392, "title": "Quantization give float32 weights(instead of int8)  on TF 2.2.0-rc2", "body": "**System information**\r\n- Google colab\r\n- TF version: 2.2.0-rc2\r\n\r\n**Problem summary**\r\nRunning the `hello_world` application for TFLite for Microcontrolllers. A model is generated successfully. The model is supposed to be quantized but the weights of all layers are `float32` and not `int8`. I am unable to generate a truly quantized model with `int8` weights. \r\nGetting the same problem in TF 2.1.0\r\n\r\n**Command used to run the converter or code if you\u2019re using the Python API**\r\n\r\n```\r\nhttps://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb\r\n```\r\n\r\n\r\n**Also, please include a link to the saved model or GraphDef**\r\n\r\n```\r\nhttp://s000.tinyupload.com/?file_id=25815880594812659996\r\n```\r\n\r\n**Failure details**\r\nThe mode is supposed to be quantized. However, the weights of the model are `float32`\r\nHow can I obtain a truly quantized model? \r\n\r\n", "comments": ["Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/c4a971855359c6d268ad6c818fd17601/38392-2-1.ipynb), [TF v2.2.0rc2](https://colab.research.google.com/gist/amahendrakar/49d0957ae877a81a8b6feb0c40d95c76/38392.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/b5c2024a9ff99baa576a1ba97c583e04/38392-tf-nightly.ipynb). Please find the attached gist. Thanks!\r\n\r\n![Screenshot 2020-04-13 at 11 55 51 AM](https://user-images.githubusercontent.com/57165142/79099033-dfaa7e80-7d80-11ea-80d6-ebd96549da9e.png)\r\n", "@gggekov,\r\nCould you please check [this comment](https://github.com/tensorflow/tensorflow/issues/38104#issuecomment-607560564) from a similar issue and let us know if it works? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> @gggekov,\r\n> Could you please check [this comment](https://github.com/tensorflow/tensorflow/issues/38104#issuecomment-607560564) from a similar issue and let us know if it works? Thanks!\r\n\r\nAny updates regarding this issue? Thanks!\r\n", "@amahendrakar \r\nApologies for the delayed reply. Yes, I confirm appling tensor bigger than 1024 solves the issue. \r\n\r\nBest,\r\nGeorge"]}, {"number": 38391, "title": "[TF 2.1] Jacobian / Hessian with Conv1D layers", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using): 2.1.0\r\n- Are you willing to contribute it (Yes/No): Yes\r\n\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCan only compute 2nd derivatives (Jacobian of Gradient) with convolutional layers (Dense works fine) when experimental_use_pfor is set to False, while for Dense layers this is not necessary and seems to be faster (hard to compare though). Here is an executable code building a model and computing Hessian as Jacobian of Gradient as discussed here [[TF 2.0] tf.hessians #29781](https://github.com/tensorflow/tensorflow/issues/29781#issuecomment-611312435)\r\n\r\n**Executable example**\r\n\r\n```\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nloss_object = tf.keras.losses.MeanSquaredError()\r\ndef loss_function(real, pred):\r\n    return tf.reduce_mean(loss_object(real, pred))\r\n\r\nx_train = tf.random.normal((10,20,1))\r\n\r\ninp = tf.keras.layers.Input(shape=x_train.shape[1:])\r\nx1 = tf.keras.layers.Conv1D(32, 3, activation=\"relu\", padding='same')(inp)\r\nx1 = tf.keras.layers.Conv1D(1,3,activation=\"relu\",padding=\"same\")(x1)\r\ncnn = tf.keras.models.Model(inp, x1)\r\ncnn.compile(loss=\"mse\",optimizer=\"sgd\")\r\n\r\n# hessian computation via jacobian\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    prediction = cnn(x_train)\r\n    loss = loss_function(x_train,prediction)\r\n    grads = tape.gradient(loss, cnn.trainable_variables)\r\n\r\nhessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]\r\ndel tape\r\n```\r\n\r\nproduces \r\n\r\n```\r\n2.1.0\r\nWARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\r\nERROR:tensorflow:Got error while pfor was converting op name: \"loop_body/Conv2D\"\r\nop: \"Conv2D\"\r\ninput: \"loop_body/Shape_3/17537\"\r\ninput: \"loop_body/Reshape_3\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"data_format\"\r\n  value {\r\n    s: \"NHWC\"\r\n  }\r\n}\r\nattr {\r\n  key: \"dilations\"\r\n  value {\r\n    list {\r\n      i: 1\r\n      i: 1\r\n      i: 1\r\n      i: 1\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"explicit_paddings\"\r\n  value {\r\n    list {\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"padding\"\r\n  value {\r\n    s: \"SAME\"\r\n  }\r\n}\r\nattr {\r\n  key: \"strides\"\r\n  value {\r\n    list {\r\n      i: 1\r\n      i: 1\r\n      i: 1\r\n      i: 1\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"use_cudnn_on_gpu\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\nwith inputs (<tf.Tensor 'loop_body/Shape_3/17537:0' shape=(10, 1, 20, 1) dtype=float32>, <tf.Tensor 'loop_body/Reshape_3:0' shape=(1, 3, 1, 32) dtype=float32>)\r\n, converted inputs [WrappedTensor(t=<tf.Tensor 'loop_body/Conv2D/pfor/Tile:0' shape=(96, 10, 1, 20, 1) dtype=float32>, is_stacked=True, is_sparse_stacked=False), WrappedTensor(t=<tf.Tensor 'loop_body/Reshape_3/pfor/Reshape:0' shape=(96, 1, 3, 1, 32) dtype=float32>, is_stacked=True, is_sparse_stacked=False)]\r\nInput \"filter\" of op \"Conv2D\" expected to be loop invariant\r\nHere are the pfor conversion stack traces:\r\nERROR:tensorflow:name: \"loop_body/Conv2D\"\r\nop: \"Conv2D\"\r\ninput: \"loop_body/Shape_3/17537\"\r\ninput: \"loop_body/Reshape_3\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"data_format\"\r\n  value {\r\n    s: \"NHWC\"\r\n  }\r\n}\r\nattr {\r\n  key: \"dilations\"\r\n  value {\r\n    list {\r\n      i: 1\r\n      i: 1\r\n      i: 1\r\n      i: 1\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"explicit_paddings\"\r\n  value {\r\n    list {\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"padding\"\r\n  value {\r\n    s: \"SAME\"\r\n  }\r\n}\r\nattr {\r\n  key: \"strides\"\r\n  value {\r\n    list {\r\n      i: 1\r\n      i: 1\r\n      i: 1\r\n      i: 1\r\n    }\r\n  }\r\n}\r\nattr {\r\n  key: \"use_cudnn_on_gpu\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\n\r\ncreated at:\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\r\n    app.launch_new_instance()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\r\n    app.start()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 583, in start\r\n    self.io_loop.start()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 153, in start\r\n    self.asyncio_loop.run_forever()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/asyncio/base_events.py\", line 538, in run_forever\r\n    self._run_once()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/asyncio/base_events.py\", line 1782, in _run_once\r\n    handle._run()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/asyncio/events.py\", line 88, in _run\r\n    self._context.run(self._callback, *self._args)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\r\n    lambda f: self._run_callback(functools.partial(callback, future))\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\r\n    ret = callback()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\r\n    self.run()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\r\n    yielded = self.gen.send(value)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\r\n    yield gen.maybe_future(dispatch(*args))\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\r\n    yield gen.maybe_future(handler(stream, idents, msg))\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\r\n    user_expressions, allow_stdin,\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\r\n    yielded = next(result)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\r\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\r\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2858, in run_cell\r\n    raw_cell, store_history, silent, shell_futures)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\r\n    return runner(coro)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\r\n    coro.send(None)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3063, in run_cell_async\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\r\n    if (await self.run_code(code, result,  async_=asy)):\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n    File \"<ipython-input-32-9fcc71f67cfa>\", line 22, in <module>\r\n    hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]\r\n    File \"<ipython-input-32-9fcc71f67cfa>\", line 22, in <listcomp>\r\n    hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1113, in jacobian\r\n    parallel_iterations=parallel_iterations)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py\", line 189, in pfor\r\n    return f()\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2362, in __call__\r\n    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2703, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\", line 2593, in _create_graph_function\r\n    capture_by_value=self._capture_by_value),\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 978, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 964, in wrapper\r\n    user_requested=True,\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py\", line 183, in f\r\n    return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py\", line 224, in _pfor_impl\r\n    loop_fn_outputs = loop_fn(loop_var)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1103, in loop_fn\r\n    unconnected_gradients=unconnected_gradients)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 1029, in gradient\r\n    unconnected_gradients=unconnected_gradients)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\", line 77, in imperative_grad\r\n    compat.as_str(unconnected_gradients.value))\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\", line 141, in _gradient_function\r\n    return grad_fn(mock_op, *out_grads)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_grad.py\", line 92, in _Conv2DBackpropFilterGrad\r\n    data_format=op.get_attr(\"data_format\").decode())\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\", line 969, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 742, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\", line 595, in _create_op_internal\r\n    compute_device)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3322, in _create_op_internal\r\n    op_def=op_def)\r\n    File \"/home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1756, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)\r\n   1112         output = pfor_ops.pfor(loop_fn, target_size,\r\n-> 1113                                parallel_iterations=parallel_iterations)\r\n   1114       except ValueError as err:\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)\r\n    188     f = function.defun(f)\r\n--> 189   return f()\r\n    190 \r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2361     with self._lock:\r\n-> 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n   2704       self._function_cache.primary[cache_key] = graph_function\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),\r\n   2594         self._function_attributes,\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n\r\nValueError: in converted code:\r\n\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:183 f  *\r\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:256 _pfor_impl\r\n        outputs.append(converter.convert(loop_fn_output))\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1280 convert\r\n        output = self._convert_helper(y)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1482 _convert_helper\r\n        six.reraise(e.__class__, e, sys.exc_info()[2])\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/six.py:703 reraise\r\n        raise value\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1466 _convert_helper\r\n        new_outputs = converter(pfor_inputs)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:902 _f\r\n        return converter(pfor_input, self.op_type, *self._args, **self._kw_args)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1592 _convert_flatten_batch\r\n        inputs = _inputs_with_flattening(pfor_input, dims)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1576 _inputs_with_flattening\r\n        inp = pfor_input.unstacked_input(i)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:776 unstacked_input\r\n        input_name, op_type))\r\n\r\n    ValueError: Input \"filter\" of op \"Conv2D\" expected to be loop invariant\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-32-9fcc71f67cfa> in <module>\r\n     20     grads = tape.gradient(loss, cnn.trainable_variables)\r\n     21 \r\n---> 22 hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]\r\n     23 del tape\r\n\r\n<ipython-input-32-9fcc71f67cfa> in <listcomp>(.0)\r\n     20     grads = tape.gradient(loss, cnn.trainable_variables)\r\n     21 \r\n---> 22 hessians = [tape.jacobian(grad, cnn.trainable_variables) for grad in grads]\r\n     23 del tape\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)\r\n   1119                 \"jacobian computation. Vectorization can be disabled by setting\"\r\n   1120                 \" experimental_use_pfor to False.\"),\r\n-> 1121             sys.exc_info()[2])\r\n   1122     else:\r\n   1123       if context.executing_eagerly() and not self._persistent:\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/six.py in reraise(tp, value, tb)\r\n    700                 value = tp()\r\n    701             if value.__traceback__ is not tb:\r\n--> 702                 raise value.with_traceback(tb)\r\n    703             raise value\r\n    704         finally:\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in jacobian(self, target, sources, unconnected_gradients, parallel_iterations, experimental_use_pfor)\r\n   1111       try:\r\n   1112         output = pfor_ops.pfor(loop_fn, target_size,\r\n-> 1113                                parallel_iterations=parallel_iterations)\r\n   1114       except ValueError as err:\r\n   1115         six.reraise(\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py in pfor(loop_fn, iters, parallel_iterations)\r\n    187   if context.executing_eagerly() or _is_under_xla_context():\r\n    188     f = function.defun(f)\r\n--> 189   return f()\r\n    190 \r\n    191 \r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\r\n   2360     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\r\n   2361     with self._lock:\r\n-> 2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\r\n   2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\r\n   2364 \r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)\r\n   2701 \r\n   2702       self._function_cache.missed.add(call_context_key)\r\n-> 2703       graph_function = self._create_graph_function(args, kwargs)\r\n   2704       self._function_cache.primary[cache_key] = graph_function\r\n   2705       return graph_function, args, kwargs\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\r\n   2591             arg_names=arg_names,\r\n   2592             override_flat_arg_shapes=override_flat_arg_shapes,\r\n-> 2593             capture_by_value=self._capture_by_value),\r\n   2594         self._function_attributes,\r\n   2595         # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\r\n    976                                           converted_func)\r\n    977 \r\n--> 978       func_outputs = python_func(*func_args, **func_kwargs)\r\n    979 \r\n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n\r\n~/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)\r\n    966           except Exception as e:  # pylint:disable=broad-except\r\n    967             if hasattr(e, \"ag_error_metadata\"):\r\n--> 968               raise e.ag_error_metadata.to_exception(e)\r\n    969             else:\r\n    970               raise\r\n\r\nValueError: in converted code:\r\n\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:183 f  *\r\n        return _pfor_impl(loop_fn, iters, parallel_iterations=parallel_iterations)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/control_flow_ops.py:256 _pfor_impl\r\n        outputs.append(converter.convert(loop_fn_output))\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1280 convert\r\n        output = self._convert_helper(y)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1482 _convert_helper\r\n        six.reraise(e.__class__, e, sys.exc_info()[2])\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/six.py:703 reraise\r\n        raise value\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1466 _convert_helper\r\n        new_outputs = converter(pfor_inputs)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:902 _f\r\n        return converter(pfor_input, self.op_type, *self._args, **self._kw_args)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1592 _convert_flatten_batch\r\n        inputs = _inputs_with_flattening(pfor_input, dims)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:1576 _inputs_with_flattening\r\n        inp = pfor_input.unstacked_input(i)\r\n    /home/kkottmann/anaconda3/envs/tftwo/lib/python3.7/site-packages/tensorflow_core/python/ops/parallel_for/pfor.py:776 unstacked_input\r\n        input_name, op_type))\r\n\r\n    ValueError: Input \"filter\" of op \"Conv2D\" expected to be loop invariant\r\n\r\nEncountered an exception while vectorizing the jacobian computation. Vectorization can be disabled by setting experimental_use_pfor to False.\r\n```\r\n", "comments": ["Hi, just wanted to ask whether this was the right way to give attention to this and also whether it can be expected that `experimental_use_pfor = True` for convolutional layers will be implemented in some future version?", "@Qottmann Is there any actionable item like a PR?\r\n\r\nPlease note that Keras development moved to another repository to focus entirely on only keras. Could you please repost this issue on [keras-team/keras repo](https://github.com/keras-team/keras/issues). Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 38389, "title": "Wrong x_lerp computation in \"ResizeBilinearKernel_faster\" and \"ResizeBilinearKernel\"", "body": "This is a bug on compute the **x_lerp** in CUDA version of resize_bilinear. See the code in \r\n[ResizeBilinearKernel_faster](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc#L62) and [ResizeBilinearKernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc#L141). \r\n\r\nThe computation of **x_lerp** is different from **y_lerp** in cuda version, which should regarded the symmetric operation on different axis.  When computing the grad, the **x_lerp** has been computed correctly, see code [ResizeBilinearGradKernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op_gpu.cu.cc#L199)\r\n\r\nBesides, its also different from the cpu version of resize_bilinear, see code in [ResizeGradCore](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/resize_bilinear_op.cc#L339).\r\n\r\n**System information** \r\nUbuntu 18.04\r\nstandard TF 2.1 (installed from pip)\r\n\r\n**Describe the current behavior**\r\n`const float x_lerp = in_x - left_x_index;`\r\n\r\n**Describe the expected behavior**\r\n`const float x_lerp = in_x - floorf(in_x);`\r\n\r\nThe bug comes from the incorrect computation of x_lerp (as described above). And the code link is also listed above (The links on ResizeBilinearKernel and ResizeBilinearKernel_faster)\r\n\r\n**Standalone code to reproduce the issue**\r\nThe bug comes from the CUDA code, which is found by reading the code myself.", "comments": ["@PCSCI \r\n\r\nPlease, let us know which tensorflow version you are using? Also, request you to fill [issue template.](https://github.com/tensorflow/tensorflow/issues/new/choose)\r\n\r\nAlso, please share simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!", "> Also, please share simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Thanks!\r\n\r\nI have updated the description. Besides, I think my previous description is also simple and straightforward, as I have put the code links, which referring to the line I described.  The bug is contained in the standard distribution of TF 2.1 (w.r.t to the code release), and I could not provide the code to reproduce it, since I find the bug by reading the tensorflow code.", "@PCSCI \r\n\r\nIs this still an issue?\r\n\r\nCould you please check if you are facing the same error with TF v2.4.1/nightly as well? Thanks!", "One master `const int left_x_index = in_x > 0.0 ? floorf(in_x) : 0;` so I think that we could close this\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/3fd3ae1fbb10961dd1aa6805280674c781fd4609/tensorflow/core/kernels/image/resize_bilinear_op_gpu.cu.cc#L60-L63", "Is this still an issue @Saduf2019 @bhack \u3002\r\n\r\nAlthough for in_x > 0.0, the computation of x_lerp and y_lerp are the same. But for in_x < 0, the computation formula is different. This kind of difference is weird during application of reisze kernels.", "Mhh.. Can you extend the test to cover `in_x < 0 case`? \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/resize_bilinear_op_test.cc", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Mhh.. Can you extend the test to cover `in_x < 0 case`?\r\n> https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/image/resize_bilinear_op_test.cc\r\n\r\nIt seems difficult for me to add the test cases in resize_bilinear_op_test.cc, as I don't have the environment available to build the tensorflow gpu version now. ", "@PCSCI If the diff it is not too large to be developed you could use directly the CI tests with a PR", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "@sanjoy What do you think?", "The inconsistency between the computation of `x_lerp` and `y_lerp` will never result in incorrect output. The calculations of both values is below:\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/c78f78dc015b8b75fd7ba5e0186899f755eeb8ef/tensorflow/core/kernels/image/resize_bilinear_op_gpu.cu.cc#L131-L142\r\n\r\n`x_lerp` is currently set to `in_x - left_x_index`, while the issue suggests changing it to `in_x - floorf(in_x)` instead, to be more consistent with `y_lerp`. However, making such a change will never change the output. To see why, note making such a change only affects the value of `x_lerp` when `in_x <= 0`. In such a case, `left_x_index` and `right_x_index` will both be zero, so so the left edge and right edge of the box will be the same. However, this means that `x_lerp` is ignored, since `x_lerp` is only used to extrapolate a value between the left and right edges, and this extrapolation has no effect when the two edges are equal. Therefore, changing `x_lerp` from `in_x - left_x_index` to `in_x - floorf(in_x)` has no functional impact.\r\n\r\nStill, making such a change would make the code more consistent and easier to understand, so I'll make the change.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38389\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38389\">No</a>\n"]}]