[{"number": 43999, "title": "Why is loading resnet50 keras model in TF 1.15 on TPU does not work?", "body": "### System information\r\n\r\n-   **Have I written custom code ( No , I am using tf.keras.application.resnet50 ):\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 20.04)**:\r\n-   **TensorFlow version (1.15)**:\r\n-   **Python version**: (3.7)\r\n-   **TPU *: V2 using TPU estimator API\r\n\r\n\r\n###\r\n         I am trying to initialize resnet50 as backbone for a model in TF 1.15, and the model is run on google TPU V2. My code is this:\r\n\r\n\r\n### Source code \r\n\r\nimport tensorflow.keras as keras\r\nfrom MBCONV import MBConvBlock\r\nimport yaml\r\nimport tensorflow.keras.backend as K\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\ntf.compat.v1.enable_eager_execution()\r\n\r\ndef model():\r\n\r\n    \r\n    mode=resnet_backbone() // error is in this fuction \r\n\r\n    // adding some more layers etc \r\n\r\n    return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op, host_call=host_call,\r\n                                           predictions={\"emb\": embeddings_layer})\r\n    \r\n\r\ndef resent_backbone():\r\n \r\n    backbone_model=tf.keras.applications.ResNet50(include_top=False, weights='imagenet',pooling=None)\r\n    return backbone_model\r\n\r\n\r\ndef main(unused_argv):\r\n\r\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(FLAGS.tpu)\r\n\r\n    run_config = tf.contrib.tpu.RunConfig(\r\n        model_dir=FLAGS.model_dir,\r\n        cluster=tpu_cluster_resolver,\r\n        session_config=tf.ConfigProto(\r\n            allow_soft_placement=True, log_device_placement=True),\r\n        tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations),\r\n    )\r\n\r\n    classifier = tf.contrib.tpu.TPUEstimator(\r\n        model_fn=model,\r\n        use_tpu=FLAGS.use_tpu,\r\n        train_batch_size=FLAGS.batch_size,\r\n        eval_batch_size=FLAGS.batch_size,\r\n        predict_batch_size=FLAGS.batch_size,\r\n        config=run_config,\r\n        params={\r\n\r\n            \"use_tpu\": FLAGS.use_tpu,\r\n        })\r\n\r\n    classifier.train(\r\n        input_fn=lambda params: train_input_fn(params[\"batch_size\"]),\r\n        # input_fn=lambda params: train_input_fn(params[\"batch_size\"]),\r\n        max_steps=FLAGS.train_steps)\r\n\r\n\r\ndef build_prediction_network():\r\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\r\n        FLAGS.tpu)\r\n\r\n    run_config = tf.contrib.tpu.RunConfig(\r\n        model_dir=FLAGS.model_dir,\r\n        cluster=tpu_cluster_resolver,\r\n        session_config=tf.ConfigProto(\r\n            allow_soft_placement=True, log_device_placement=True),\r\n        tpu_config=tf.contrib.tpu.TPUConfig(FLAGS.iterations),\r\n    )\r\n    classifier = tf.contrib.tpu.TPUEstimator(\r\n        model_fn=model,\r\n        use_tpu=FLAGS.use_tpu,\r\n        train_batch_size=FLAGS.batch_size,\r\n        eval_batch_size=FLAGS.batch_size,\r\n        predict_batch_size=FLAGS.batch_size,\r\n        config=run_config,\r\n        params={\r\n\r\n            \"use_tpu\": FLAGS.use_tpu, })\r\n    return classifier\r\n\r\n\r\n\r\n\r\n\r\n### I get the following errors\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nOperation of type Placeholder (input_1) is not supported on the TPU. Execution will fail if this op is used in the graph. \r\n\r\ntensorflow.python.framework.errors_impl.InaccessibleTensorError: Operation 'VarIsInitializedOp' has been marked as not fetchable. Typically this happens when it is defined in another function or code block. Use return values,explicit Python locals or TensorFlow collections to access it.\r\n\r\n\r\n", "comments": ["@usama-ahmedkhan \r\nPlease provide with complete stand alone indented code to replicate the issue faced or if possible share a colab gist with the error reported.\r\nPlease note there is no official support for 1.x tf, could you try on 2.x and let us know if you face any issues.", "yes , I have edited , not possible to work in tf 2.x right now , because of most code is written in the tf 1.x. If i put @tf.function on top of def resent_backbone: , then this error goes away and following error comes:\r\n\r\n\r\n ValueError: Cannot use 'conv1_conv/kernel/Initializer/random_uniform' as input to 'conv1_conv/kernel/Assign' because 'conv1_conv/kernel/Initializer/random_uniform' is in a while loop. See info log for more details.", "@usama-ahmedkhan\r\nPlease provide with complete stand alone indented code to replicate the issue faced or if possible share a colab gist with the error reported.", "I updated it , can you give a guess to what could be the issue , it's just a simple 1 line of code initialization with no input to it .\r\n\r\nFrom what I have searched , following is the closest guess to the error I am getting , it is not possible to fetch the result of an op created inside the while loop's body, because the body might execute 0 or more times, based on the loop condition. To get a value out of the loop, you need to return it from the body function (as one of the loop variables), and its final value after all the iterations will be returned from tf.while_loop(). But this happens inside tf code , not externally , because i am only running a single line code for initialization of a model.\r\nInside tf.keras implementation code , while loading weights into variable , it checks if there are uninitialized variables and there the error comes , the Varisinitializedop is used there  .\r\n", "In TF 1.x, there are subtle differences between the graph representation of models implemented in Keras (e.g. tf.keras.applications...)  and models implemented in standard TensorFlow.\r\n\r\nIf you must stick with TF 1, I would highly suggest starting from the [reference ResNet model](https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_main.py) that works well on TPUs, but I would highly suggest moving to TF 2.x + Keras which is a lot easier to use. [Here](https://github.com/tensorflow/models/tree/master/official/vision/image_classification) is an equivalent reference ResNet model that works well on CPU, GPU and TPUs.\r\n\r\n", "thanks for your reply , and yes I am now using reference ResNet model you suggested , it is easy  and flexible to use that resnet model.", "@usama-ahmedkhan \r\nPlease move the issue to closed status if resolved."]}, {"number": 43998, "title": "Update BatchNormalization documentation", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue: \r\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\r\n\r\n## Description of issue (what needs changing):\r\n\r\n### Clear description\r\nThe document should mention that the axis can take in a list of integers, not just an integer. I tested it and it is already implemented in the TensorFlow. I was not aware of it and used reshape and transpose, which is inefficient.\r\n\r\n### Correct links\r\n\r\nIs the link to the source code correct?\r\n\r\n### Parameters defined\r\n\r\nAre all parameters defined and formatted correctly?\r\n\r\nThe axis can be a list of integers.\r\n\r\n### Returns defined\r\n\r\nAre return values defined?\r\n\r\n### Raises listed and defined\r\n\r\nAre the errors defined? For example,\r\nhttps://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises\r\n\r\n### Usage example\r\n\r\nIs there a usage example?\r\n\r\nSee the API guide: https://www.tensorflow.org/community/contribute/docs_ref\r\non how to write testable usage examples.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs,\r\ndocs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["@vermouth1992 \r\nCould you please explain the issue faced.", "There is no practical issue I am facing. However, the documentation is misleading. For example, if we want the batch normalization layer to normalize over multiple dimensions, the following code is working\r\n```python\r\nimport tensorflow as tf\r\nlayer = tf.keras.layers.BatchNormalization(axis=[0, 2])\r\na = layer(tf.random.normal([3, 100, 10]))\r\n```\r\nHowever, the documentation says that the axis argument must be an integer, which confuses me at the beginning. So I have to use a reshape + transpose to implement the same functionality, which is ineffective.", "Thanks for your issue. The docs are now updated.\r\nhttps://github.com/tensorflow/tensorflow/blob/9318f787e6d33adabc5c1b18c6663d1432f646f9/tensorflow/python/keras/layers/normalization.py#L83"]}, {"number": 43997, "title": "Why TF loss function gives different results than Pytorch?", "body": "I'm trying to figure out why TF binary_cross_entropy gives different results than Pytorch's binary_cross_entropy.\r\n\r\nhttps://colab.research.google.com/drive/1hzFEI05_hKNRV4gE1FFDumd1ygN3LVsQ?usp=sharing\r\n\r\nAnyone?", "comments": ["@chrismaliszewski \r\n\r\nPlease, use` tf.keras.losses.BinaryCrossentropy`as loss function so you can see almost same results with TF and Pytorch.\r\nPlease, find the gist [here](https://colab.research.google.com/gist/ravikyram/90d4e8364e43be17ebf449a61826ebaf/untitled452.ipynb) for your reference.Thanks!", "Thank you, @ravikyram. "]}, {"number": 43996, "title": "Build options", "body": "is it possible to build a lite version from the source code? I only need Cuda support and an Intel processor. As I see during the build there is a compilation for arm including many other things. What commands can I use to restrict build options that I don't need?\r\n", "comments": ["@Expert73 \r\nPlease check [this link](https://www.hatthieves.es/2020/09/23/tensorflow-2-0-y-redes-neuronales/) for reference.\r\nPlease follow instructions on [this link](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html) for cuda.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 43994, "title": "How to use custom tensorflow op for CUDA?", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version: 2.3\r\n- Python version: 3.7\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:10.1 7.6.5\r\n- GPU model and memory: GTX 1060 6G\r\n\r\n**Describe the problem**\r\nRecently I am researching deformable convolution, because i can't find a tf2.3 version to use, i use a tensorflow python api build, it is too slow. I want to learn how to use a cuda version to get better performance.\r\nthanks for advice.\r\n", "comments": ["@WzwzzZ \r\n\r\nPlease, refer this [tutorial](https://www.tensorflow.org/guide/create_op) and see if it helps you.\r\n\r\nThis question is better asked on StackOverflow since it is not a bug or feature request. There is also a larger community that reads questions there and provide better and faster support for such issues. Thanks!", "Thanks, I'll try it.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43994\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43994\">No</a>\n"]}, {"number": 43993, "title": "How to print the tensor values in tensorflow 2.x version?", "body": "I have a small issue, in tensorflow 2.x version, we don't need session to run the graph.\r\n\r\nCurrently I have a tensor object, calculated by tf.image.ssim(), but I have no way to print the value in the tensor. \r\n\r\nMost of the tutorials I found is about keras models, but what is the way to print tensor values in simple tensor?\r\n\r\n", "comments": ["Refer to the documentation of tf.print [here](https://www.tensorflow.org/api_docs/python/tf/print)", "> Refer to the documentation of tf.print [here](https://www.tensorflow.org/api_docs/python/tf/print)\r\n\r\nThanks, it helps. However, do you know how to convert tensor to array in tensorflow 2.x version?", "You can use .numpy() method of tensorflow to convert into a numpy array . [Here](https://stackoverflow.com/questions/34097281/how-can-i-convert-a-tensor-into-a-numpy-array-in-tensorflow) is stackoverlfow link to your question ", "> You can use .numpy() method of tensorflow to convert into a numpy array . [Here](https://stackoverflow.com/questions/34097281/how-can-i-convert-a-tensor-into-a-numpy-array-in-tensorflow) is stackoverlfow link to your question\r\n\r\nThank you! I'd tried this function before, but encounter some error. Now it's solved!"]}, {"number": 43992, "title": "Passing structs by value makes C API hard to wrap", "body": "The API has a number of functions that pass `TF_Output` argument by value. For example:\r\n\r\n```c\r\nTF_CAPI_EXPORT extern int TF_GraphGetTensorNumDims(TF_Graph* graph, TF_Output output, TF_Status* status);\r\n```\r\n\r\nThis creates problems with different calling conventions. In my case, I'm trying to call these functions from Go through the syscall API instead of cgo and have no idea how to pass the struct correctly. I realize that existing functions cannot be changed, but would suggest deprecating them and adding new ones that only use struct pointers.", "comments": ["Thanks for the suggestion. As you mentioned, it is not possible to change these APIs now but we are working on new APIs for building `tf.function` in TF2 which will have pointers for tensors."]}, {"number": 43991, "title": "Bump junit from 4.11 to 4.13.1.", "body": "Fixes [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250)", "comments": []}, {"number": 43990, "title": "Bump junit from 4.11 to 4.13.1.", "body": "Fixes [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250)", "comments": []}, {"number": 43989, "title": "Bump junit from 4.11 to 4.13.1.", "body": "Fixes [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250)", "comments": []}, {"number": 43988, "title": "Bump junit from 4.11 to 4.13.1.", "body": "Fixes [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250)", "comments": []}, {"number": 43987, "title": "Bump junit from 4.11 to 4.13.1.", "body": "Fixes [CVE-2020-15250](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-15250)", "comments": []}, {"number": 43986, "title": "installing tflite-model-maker encounters errors in regards to tf-nightly==2.4.0.dev20200810", "body": "**System information**\r\n- OS Platform and Distribution (e.g., windows):\r\n- TensorFlow installed from both source and binary: tried on both\r\n- TensorFlow version:  source 2.2.0 , binary 2.0.0\r\n- Python version: 3.7.4\r\n- Installed using virtualenv? pip? conda?: pip\r\n- Bazel version (if compiling from source): 3.2.0\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nI am trying to pip install tflite-model-maker, (tried from the source too) but I keep getting this error:\r\nERROR: No matching distribution found for tf-nightly==2.4.0.dev2.4.0.dev20201012\r\nIt seems that there is no version of tf-nightly 2.4.0.dev2.4.0.dev20201012\r\nI tried to install another version of tf-nightly but that also encounters other error\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 228, in _main\r\n    status = self.run(options, args)\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 182, in wrapper\r\n    return func(self, options, args)\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 324, in run\r\n    reqs, check_supported_wheels=not options.target_dir\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 183, in resolve\r\n    discovered_reqs.extend(self._resolve_one(requirement_set, req))\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 388, in _resolve_one\r\n    abstract_dist = self._get_abstract_dist_for(req_to_install)\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 340, in _get_abstract_dist_for\r\n    abstract_dist = self.preparer.prepare_linked_requirement(req)\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in prepare_linked_requirement\r\n    hashes=self._get_linked_req_hashes(req)\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 259, in unpack_url\r\n    hashes=hashes,\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 130, in get_http_url\r\n    link, downloader, temp_dir.path, hashes\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 282, in _download_http_url\r\n    for chunk in download.chunks:\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 168, in iter\r\n    for x in it:\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 88, in response_chunks\r\n    decode_content=False,\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\r\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"c:\\users\\chingool\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 442, in _error_catcher\r\n    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\r\npip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\r\n\r\nI feel like I will never be able to install tflite-model-maker, though it looks to be very useful.\r\nIs there any other alternative to that?\r\n", "comments": ["Nightly versions change everyday and all the nightly versions are not saved in PyPi only the latest ones.\r\n`tf-nightly==2.4.0.dev20200810` tries to install nightly package built on 08/10/2020 which is not available.\r\nCan you try;\r\n```python\r\npip install tflite-model-maker-nightly\r\n```\r\nThis worked for me.\r\nFor installing latest TF try,\r\n```python\r\npip install tf-nightly\r\n```\r\n", "I tried the nightly build too. Didn't work. \r\nHowever later I realized that tf-nightly 2.4.0.dev2.4.0.dev20201012 version has no wheel for python3.7, upgrading to 3.8 and running it again solved the problem.\r\nHowever now I get another error:\r\nAttributeError: module 'tensorflow_lite_support.metadata.metadata_schema_py_generated' has no attribute 'ModelMetadataT'", "Yes nightly wheels get updated daily and are not saved in Pypi.\r\nHow are you hitting?\r\n```python\r\nAttributeError: module 'tensorflow_lite_support.metadata.metadata_schema_py_generated' has no attribute 'ModelMetadataT'\r\n```\r\nDo you have a repro code to share?", "yeah I didn't know that. I am quite new to Python. Things were different in Java world.\r\n\r\n> Do you have a repro code to share?\r\nYes, it just happens at the last line of example, which is exporting. As a matter of fact I just raised an issue here:\r\nhttps://github.com/tensorflow/tensorflow/issues/44062", "I will close this issue since the original topic is solved. We can continue the discussion on the other thread you created. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43986\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43986\">No</a>\n"]}, {"number": 43985, "title": "TF to TF Lite Int 8 resulting in error: \"Quantization not yet supported for op: %\"", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes (a mix)**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): \r\n- TensorFlow installed from (source or binary): **Source**\r\n- TensorFlow version (use command below): **2.3**\r\n- Python version: **3**\r\n\r\n**Describe the current behavior**\r\nConversion to TF Lite Float 16 works and model runs well\r\nConversion to TF Lite Int 8 does NOT convert \r\n\r\n**Describe the expected behavior**\r\nConversion to TF Lite Int 8 works\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n`    \r\n# TFLite model export\r\n    try:\r\n        print('\\nStarting TFLite export with TensorFlow %s...' % tf.__version__)\r\n        if opt.no_tfl_detect:\r\n            print(\"Don't export Detect module\")\r\n            m.training = True\r\n            keras_model = keras.Model(inputs=inputs, outputs=tf_model.predict(inputs))\r\n\r\n        # fp16 TFLite model export\r\n        converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n        converter.target_spec.supported_types = [tf.float16]\r\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\r\n        converter.allow_custom_ops = False\r\n        converter.experimental_new_converter = True\r\n        tflite_model = converter.convert()\r\n        f = opt.weights.replace('.pt', '.tflite')  # filename\r\n        open(f, \"wb\").write(tflite_model)\r\n        print('\\nTFLite export success, saved as %s' % f)\r\n\r\n        # int8 TFLite model export\r\n        if opt.tfl_int8:\r\n\r\n            dataset = LoadImages(opt.source, img_size=opt.img_size, auto=False)\r\n                \r\n            def representative_data_gen():\r\n                n = 0\r\n                for path, img, im0s, vid_cap in dataset:\r\n                    # Get sample input data as a numpy array in a method of your choosing.\r\n                    n += 1\r\n                    input = np.transpose(img, [1, 2, 0])\r\n                    input = np.expand_dims(input, axis=0).astype(np.float32)\r\n                    input /= 255.0\r\n                    yield [input]\r\n                    if n >= opt.ncalib:\r\n                        break\r\n\r\n            converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\r\n            # This enables quantization\r\n            converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n            # This sets the representative dataset for quantization\r\n            converter.representative_dataset = representative_data_gen\r\n            # This ensures that if any ops can't be quantized, the converter throws an error\r\n            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n            # For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\r\n            converter.target_spec.supported_types = [tf.int8]\r\n            # These set the input and output tensors to uint8 (added in r2.3)\r\n            converter.inference_input_type = tf.uint8\r\n            converter.inference_output_type = tf.uint8\r\n            tflite_model = converter.convert()\r\n\r\n            with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:\r\n              f.write(tflite_model)\r\n\r\n`\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n[logs.pdf](https://github.com/tensorflow/tensorflow/files/5373243/logs.pdf)\r\n", "comments": ["@amahendrakar Hey Abhilash, let me know if you need any more info! Thanks! ", "The error looks strange. Could you try it again with tf-nightly?\r\n```\r\nTFLite export failure: Quantization not yet supported for op: %\r\nTraceback (most recent call last):\r\n File \"yolov5/models/tf.py\", line 568, in <module>\r\n tflite_model = converter.convert()\r\n File \"/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 831, in convert\r\n self).convert(graph_def, input_tensors, output_tensors)\r\n File \"/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 638, in convert\r\n result = self._calibrate_quantize_model(result, **flags)\r\n File \"/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/lite.py\", line 452, in\r\n_calibrate_quantize_model\r\n inference_output_type, allow_float, activations_type)\r\n File \"/opt/conda/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line\r\n98, in calibrate_and_quantize\r\n np.dtype(activations_type.as_numpy_dtype()).num)\r\nRuntimeError: Quantization not yet supported for op: %\r\n```", "@patcombe,\r\nOn running the code, I am facing an error stating `NameError: name 'opt' is not defined`. Please find the gist of it [here](https://colab.research.google.com/gist/amahendrakar/d943bd3f2e75d20a7d6112ea01b36efd/43985.ipynb).\r\n\r\nIn order to reproduce the issue reported, could you please provide the complete code and the dataset you are using. \r\n\r\nAlso, as @terryheo suggested please try running the code with the latest TF-nightly and check if it works. Thanks!", "@terryheo @amahendrakar using tf-nightly seemed to solve it! Thanks! ", "@patcombe, \r\nThank you for the update. Marking the issue as closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43985\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43985\">No</a>\n"]}, {"number": 43984, "title": "TF to TF Lite Int 8 resulting in error: ", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below):\r\n- Python version:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\n**Describe the expected behavior**\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\n**Other info / logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n", "comments": ["@patcombe \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced]", "Looks like he filed another issue #43985", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43984\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43984\">No</a>\n"]}, {"number": 43983, "title": "remote devices in cluster not visible in TF 2", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.7\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3\r\n- Python version: 3.6.12\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: -\r\n\r\n**Describe the current behavior**\r\nI am not able to simply put a tensor on a specific device in a TF cluster.\r\n\r\nThis example works for TF1.14:\r\n```python\r\n# try_worker_tensor_tf1.py\r\nimport json\r\nimport os\r\nimport time\r\nimport sys\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.distribute.cluster_resolver import TFConfigClusterResolver\r\n\r\nid_ = int(sys.argv[1])\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': ['localhost:8081', 'localhost:8082']\r\n    },\r\n    'task': {\r\n        'type': 'worker',\r\n        'index': id_\r\n    }\r\n})\r\nresolver = TFConfigClusterResolver()\r\ncluster = resolver.cluster_spec()\r\n\r\nwith tf.device(\"/job:worker/task:0\"):\r\n    v1 = tf.constant(1)\r\n\r\nexperimental_config = tf.ConfigProto.Experimental(\r\n    share_cluster_devices_in_session=False,\r\n    share_session_state_in_clusterspec_propagation=False)\r\nconfig = tf.ConfigProto(experimental=experimental_config)\r\nserver = tf.train.Server(\r\n    cluster, job_name=\"worker\", task_index=id_, config=config)\r\nsess = tf.Session(target=server.target, config=config)\r\nprint(sess.run(v1))\r\ntime.sleep(5)\r\n```\r\n```console\r\n> python try_worker_tensor_tf1.py 0&\r\n> python try_worker_tensor_tf1.py 1&\r\n```\r\noutputs\r\n```console\r\n1\r\n1\r\n```\r\n\r\nBut this example in TF2.3 does not work:\r\n```python\r\nimport json\r\nimport os\r\nimport time\r\nimport sys\r\n\r\nimport tensorflow as tf\r\n\r\nid_ = int(sys.argv[1])\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': ['localhost:8081', 'localhost:8082']\r\n    },\r\n    'task': {\r\n        'type': 'worker',\r\n        'index': id_\r\n    }\r\n})\r\n# Initializes tf server internally.\r\ndist = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n\r\nprint('Devices', tf.config.list_logical_devices())\r\n\r\nwith tf.device(\"/job:worker/task:0\"):\r\n    v1 = tf.constant(1)\r\n\r\nprint(v1.numpy())\r\ntime.sleep(5)\r\n```\r\n\r\n```console\r\npython try_worker_tensor_tf2.py 0\r\n```\r\noutputs:\r\n```console\r\n2020-10-13 16:56:49.658744: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:8081, 1 -> localhost:8082}\r\n2020-10-13 16:56:49.658987: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:8081\r\nDevices [LogicalDevice(name='/job:worker/replica:0/task:0/device:CPU:0', device_type='CPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:XLA_CPU:0', device_type='XLA_CPU')]\r\n1\r\n```\r\nwithout blocking on the other server to start. In a separate terminal in parallel:\r\n```console\r\npython try_worker_tensor_tf2.py 1\r\n```\r\noutputs:\r\n```console\r\n2020-10-13 17:00:11.617132: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:8082\r\nDevices [LogicalDevice(name='/job:worker/replica:0/task:1/device:CPU:0', device_type='CPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:XLA_CPU:0', device_type='XLA_CPU')]\r\nTraceback (most recent call last):\r\n  File \"try_worker_tensor_tf2.py\", line 25, in <module>\r\n    v1 = tf.constant(1)\r\n  File \"/Users/fgranqvist/opt/anaconda3/envs/py36-tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 264, in constant\r\n    allow_broadcast=True)\r\n  File \"/Users/fgranqvist/opt/anaconda3/envs/py36-tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 275, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n  File \"/Users/fgranqvist/opt/anaconda3/envs/py36-tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 300, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n  File \"/Users/fgranqvist/opt/anaconda3/envs/py36-tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\", line 98, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: /job:worker/replica:0/task:0/device:CPU:0 unknown device.\r\n```\r\n\r\n**Describe the expected behavior**\r\nI am expecting the TF2 example to also output `1` for both processes.\r\n\r\nThe reason why I am trying to do this is because I need to use `tf.queue.FIFOQueue` in the same way as `v1` above (as a distributed queue). One worker in the cluster enqueues items and all workers dequeues items and processes it.", "comments": ["Hi @grananqvist the examples don't look exactly equivalent to me. I'm wondering if in TF1 were you also using `MultiWorkerMirroredStrategy`? Also, have you tried an if/else block like the example under the `task_id` attribute in the `TFConfigClusterResolver` [docs](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver)?\r\n\r\n```\r\nif cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\r\n    # do something\r\n```", "I am using `MultiWorkerMirroredStrategy` in tf2 because the TF servers are initialized internally, and done so when initializing a distribution strategy (from here it seems https://github.com/tensorflow/tensorflow/blob/28a88cc4f7aef7ec2efe75b1ea2d0a7bdfae3a17/tensorflow/python/eager/context.py#L665 ). I can't initialize like in TF1 because there is no session.\r\n\r\n> ```\r\n> if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\r\n>    # do something\r\n> ```\r\n\r\nThis is a better example which works in TF1. As you can see, the problem is initializing a variable on one device, and reach it from all.\r\n\r\n```python\r\n# This example works in TF1. How to reproduce it to TF2?\r\nimport json\r\nimport os\r\nimport time\r\nimport sys\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.distribute.cluster_resolver import TFConfigClusterResolver\r\n\r\nid_ = int(sys.argv[1])\r\n\r\nos.environ['TF_CONFIG'] = json.dumps({\r\n    'cluster': {\r\n        'worker': ['localhost:8081', 'localhost:8082']\r\n    },\r\n    'task': {\r\n        'type': 'worker',\r\n        'index': id_\r\n    }\r\n})\r\nresolver = TFConfigClusterResolver()\r\ncluster = resolver.cluster_spec()\r\n\r\nexperimental_config = tf.ConfigProto.Experimental(\r\n    share_cluster_devices_in_session=False,\r\n    share_session_state_in_clusterspec_propagation=False)\r\nconfig = tf.ConfigProto(experimental=experimental_config)\r\nserver = tf.train.Server(cluster,\r\n                         job_name=\"worker\",\r\n                         task_index=id_, config=config)\r\nsess = tf.Session(target=server.target, config=config)\r\n\r\nwith tf.device(\"/job:worker/task:0\"):\r\n    queue = tf.queue.FIFOQueue(10, dtypes=(tf.int32,), shared_name='my_queue')\r\nif id_ == 0:\r\n    sess.run(queue.enqueue_many(np.arange(10)))\r\n    sess.run(queue.close())\r\n    time.sleep(10)\r\nelse:\r\n    while True:\r\n        try:\r\n            # Should print 1..10\r\n            print(sess.run(queue.dequeue()))\r\n        except tf.errors.OutOfRangeError:\r\n            print('done')\r\n            break\r\n```\r\n\r\n```\r\n#terminal 1: puts 10 numbers in the queue\r\npython try_fed_queue_tf1.py 0\r\n#terminal 2: pulls 10 numbers from the queue\r\npython try_fed_queue_tf1.py 1", "How to reproduce the above for TF2? If a distribution strategy is indeed required to initialize the server in the backend, and it is a feature that distribution strategies hide remote logical devices by default, how do I prevent this?", "Just to clarify, your use of `MultiWorkerMirroredStrategy` is not because you want to use the strategy to do synchronous training, but rather, you're using `MultiWorkerMirroredStrategy` to initialize the cluster of workers?", "In this example yes. \r\n(In production we still use MultiWorkerMirroredStrategy to train with the cluster and tf.queue for other asynchronous pulling mechanisms in TF1.)", "Hi @grananqvist, sorry for the late response here. But did you try my suggestion in an earlier comment to use\r\n\r\n```\r\nif cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\r\n    # do something\r\n```\r\n\r\nDid that work for you?", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "> Hi @grananqvist, sorry for the late response here. But did you try my suggestion in an earlier comment to use\r\n> \r\n> ```\r\n> if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\r\n>     # do something\r\n> ```\r\n> \r\n> Did that work for you?\r\n\r\nI'm not sure how that is supposed to work. \r\n\r\n```python\r\nif cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\r\n    v1 = tf.constant(1)\r\n\r\n# On all workers, get value of v1 variable from worker 0. worker != 0 also needs a reference to this. Will not work.\r\nprint(v1.numpy())\r\n```\r\n\r\nIf this is not  possible in TF2, i.e. reach a variable placed on another worker in the cluster, then how is `tf.queue.FIFOQueue` supposed to be used in TF2 with multiple workers?", "`tf.queue.FIFOQueue` doesn't work across workers with `MultiWorkerMirroredStrategy`. Note that it's not TF2 specific.  `MultiWorkerMirroredStrategy` is designed that each worker only sees its local devices. \r\n\r\nIt is still possible to do what you describe in #1 with TF2. You can start a `tf.distribute.Server` on each worker, and connects to the cluster from a client by `tf.config.experimental_connect_to_cluster`. But you cannot use `MultiWorkerMirroredStrategy` together with it, and you need to handle distributed training on your own. ", "Ok, thanks for the clarification! Then I will assume everything is working as intended and will close this for now.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43983\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43983\">No</a>\n"]}, {"number": 43982, "title": "AttributeError: module 'tensorflow._api.v2.sets' has no attribute 'set_intersection'", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\nHello there,\r\nI must admit that I am quite new to this, so please excuse me if I raise a trivial problem.\r\nI have installed tensorflow and tried to implement Mask R-CNN just to make sure that everything runs smoothly before I try it on my own images. The programme and all necessary modules appears run without a problem until it is confronted by an error. I have copied all the commands and the error below. I would be appreciated if you could help.\r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nimport os\r\nimport sys\r\nimport random\r\nimport math\r\nimport numpy as np\r\nimport skimage.io\r\nimport matplotlib\r\nimport matplotlib.pyplot as plt\r\n\r\n# Root directory of the project\r\nROOT_DIR = os.path.abspath(\"../\")\r\n\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\n\r\n# Import Mask RCNN\r\nsys.path.append(ROOT_DIR)  # To find local version of the library\r\nfrom mrcnn import utils\r\nimport mrcnn.model as modellib\r\nfrom mrcnn import visualize\r\n# Import COCO config\r\nsys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\r\nimport coco\r\n\r\n%matplotlib inline\r\n\r\n# Directory to save logs and trained model\r\nMODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\r\n\r\n# Local path to trained weights file\r\nCOCO_MODEL_PATH = os.path.join('', \"mask_rcnn_coco.h5\")\r\n\r\n# Download COCO trained weights from Releases if needed\r\nif not os.path.exists(COCO_MODEL_PATH):\r\n    utils.download_trained_weights(COCO_MODEL_PATH)\r\n\r\n# Directory of images to run detection on\r\nIMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\r\n\r\nclass InferenceConfig(coco.CocoConfig):\r\n    # Set batch size to 1 since we'll be running inference on\r\n    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\r\n    GPU_COUNT = 1\r\n    IMAGES_PER_GPU = 1\r\n\r\nconfig = InferenceConfig()\r\nconfig.display()\r\n\r\n# Create model object in inference mode.\r\nmodel = modellib.MaskRCNN(mode=\"inference\", model_dir='mask_rcnn_coco.hy', config=config)\r\n\r\n# Load weights trained on MS-COCO\r\nmodel.load_weights('mask_rcnn_coco.h5', by_name=True)\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-a2b43923c698> in <module>\r\n     48 \r\n     49 # Create model object in inference mode.\r\n---> 50 model = modellib.MaskRCNN(mode=\"inference\", model_dir='mask_rcnn_coco.hy', config=config)\r\n     51 \r\n     52 # Load weights trained on MS-COCO\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in __init__(self, mode, config, model_dir)\r\n   1835         self.model_dir = model_dir\r\n   1836         self.set_log_dir()\r\n-> 1837         self.keras_model = self.build(mode=mode, config=config)\r\n   1838 \r\n   1839     def build(self, mode, config):\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in build(self, mode, config)\r\n   2041             # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in\r\n   2042             # normalized coordinates\r\n-> 2043             detections = DetectionLayer(config, name=\"mrcnn_detection\")(\r\n   2044                 [rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])\r\n   2045 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/backend/tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)\r\n     73         if _SYMBOLIC_SCOPE.value:\r\n     74             with get_graph().as_default():\r\n---> 75                 return func(*args, **kwargs)\r\n     76         else:\r\n     77             return func(*args, **kwargs)\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/keras/engine/base_layer.py in __call__(self, inputs, **kwargs)\r\n    487             # Actually call the layer,\r\n    488             # collecting output(s), mask(s), and shape(s).\r\n--> 489             output = self.call(inputs, **kwargs)\r\n    490             output_mask = self.compute_mask(inputs, previous_mask)\r\n    491 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in call(self, inputs)\r\n    808 \r\n    809         # Run detection refinement graph on each item in the batch\r\n--> 810         detections_batch = utils.batch_slice(\r\n    811             [rois, mrcnn_class, mrcnn_bbox, window],\r\n    812             lambda x, y, w, z: refine_detections_graph(x, y, w, z, self.config),\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/utils.py in batch_slice(inputs, graph_fn, batch_size, names)\r\n    818     for i in range(batch_size):\r\n    819         inputs_slice = [x[i] for x in inputs]\r\n--> 820         output_slice = graph_fn(*inputs_slice)\r\n    821         if not isinstance(output_slice, (tuple, list)):\r\n    822             output_slice = [output_slice]\r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in <lambda>(x, y, w, z)\r\n    810         detections_batch = utils.batch_slice(\r\n    811             [rois, mrcnn_class, mrcnn_bbox, window],\r\n--> 812             lambda x, y, w, z: refine_detections_graph(x, y, w, z, self.config),\r\n    813             self.config.IMAGES_PER_GPU)\r\n    814 \r\n\r\n/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/mask_rcnn-2.1-py3.8.egg/mrcnn/model.py in refine_detections_graph(rois, probs, deltas, window, config)\r\n    718     if config.DETECTION_MIN_CONFIDENCE:\r\n    719         conf_keep = tf.where(class_scores >= config.DETECTION_MIN_CONFIDENCE)[:, 0]\r\n--> 720         keep = tf.sets.set_intersection(tf.expand_dims(keep, 0),\r\n    721                                         tf.expand_dims(conf_keep, 0))\r\n    722         keep = tf.sparse_tensor_to_dense(keep)[0]\r\n\r\nAttributeError: module 'tensorflow._api.v2.sets' has no attribute 'set_intersection' \r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["@SalahRahimi \r\nPlease refer to these to issues and let us know if it helps resolve:#30092 [link](https://stackoverflow.com/questions/60581677/attributeerror-module-tensorflow-core-api-v2-config-has-no-attribute-expe)", "@Saduf2019 \r\nthank you for your response. I couldn't find any solution in the links you have provided. Would you please mind to provide me with more details? just in case if I am missing something. ", "@SalahRahimi,\r\nIn order to expedite the trouble-shooting process, could you please provide the following details\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version:\r\n- Python version:\r\n- Installed using virtualenv? pip? conda?:\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n", "Also, on running the code snippet you have provided, I am facing an error stating `ModuleNotFoundError: No module named 'mrcnn'`.\r\n\r\nCould you please provide the complete code to reproduce the issue reported here along with the dataset you are using? Thanks!", "@amahendrakar \r\nThe main issue here was the tensor flow version. mrcnn doesn't work with tensorflow.2.0, therefore I had to remove this version and re-install version 1.15. Note that it is not possible to install version 1.15 in Python3.8, so I switched to Python 3.7, which allowed me to install tensorflow 1.15. Now, this problem is resolved and I have been able to run the programme.\r\n\r\n", "@SalahRahimi,\r\nThank you for the update. Closing the issue as it is resolved, please feel free to re-open if necessary.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43982\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43982\">No</a>\n", "In case anyone encounters this issue and has tensorflow 2.0 but doesn't want to change the versions of tensorflow and python, there is another easier solution:\r\n\r\nYou can either change from:\r\n**tf.sets.set_intersection(...)**\r\nto:\r\n**tf.compat.v1.sets.set_intersection(...)**\r\n\r\nand anywhere else necessary when receiving these kind of errors, or:\r\n\r\nsimply when importing tensorflow you can change:\r\n**import tensorflow as tf**\r\nto:\r\n**import tensorflow.compat.v1 as tf**\r\n\r\n"]}, {"number": 43981, "title": "[Intel MKL] Fixing mkl_eager_op_rewrite_test error", "body": "This PR fixes mkl_eager_op_rewrite_test error by releasing the eager context correctly.", "comments": []}, {"number": 43980, "title": "Official libtensorflow links", "body": "Add official GCS libtensorflow links.", "comments": []}, {"number": 43979, "title": "ERROR: An error occurred during the fetch of repository 'local_config_cc':", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows 10 \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):     source \r\n- TensorFlow version:   2.3\r\n- Python version: 3.8.5\r\n- Bazel version (if compiling from source):    3.6.0\r\n- GCC/Compiler version (if compiling from source):   MSVC\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the problem**\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\nRepository rule cc_autoconf defined at:\r\n  C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/cc_configure.bzl:143:30: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_cc':\r\n   Traceback (most recent call last):\r\n        File \"C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/cc_configure.bzl\", line 120, column 36, in cc_autoconf_impl\r\n                configure_windows_toolchain(repository_ctx)\r\n        File \"C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 783, column 35, in configure_windows_toolchain\r\n                msvc_vars_x64 = _get_msvc_vars(repository_ctx, paths, \"x64\")\r\n        File \"C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 631, column 32, in _get_msvc_vars\r\n                env = setup_vc_env_vars(repository_ctx, vc_path)\r\n        File \"C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 387, column 24, in setup_vc_env_vars\r\n                _check_env_vars(env_map, cmd, expected = envvars)\r\n        File \"C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/windows_cc_configure.bzl\", line 393, column 32, in _check_env_vars\r\n                auto_configure_fail(\r\n        File \"C:/users/10184590/_bazel_10184590/z5zmmi7q/external/bazel_tools/tools/cpp/lib_cc_configure.bzl\", line 112, column 9, in auto_configure_fail\r\n                fail(\"\\n%sAuto-Configuration Error:%s %s\\n\" % (red, no_color, msg))\r\nError in fail:\r\nAuto-Configuration Error: Setting up VC environment variables failed, WINDOWSSDKDIR is not set by the following command:\r\n    \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Auxiliary\\Build\\VCVARSALL.BAT\" amd64  -vcvars_ver=14.27.29110\r\nINFO: Repository icu instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule third_party_http_archive defined at:\r\n  F:/git/tensorflow/tensorflow/third_party/repo.bzl:216:43: in <toplevel>\r\nINFO: Repository 'icu' used the following cache hits instead of downloading the corresponding file.\r\n * Hash 'dfc62618aa4bd3ca14a3df548cd65fe393155edd213e49c39f3a30ccd618fc27' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/unicode-org/icu/archive/release-64-2.zip\r\nIf the definition of 'icu' was updated, verify that the hashes were also updated.\r\nERROR: F:/git/tensorflow/tensorflow/tensorflow/tools/build_info/BUILD:9:10: //tensorflow/tools/build_info:gen_build_info depends on @local_config_cc//:cc-compiler-x64_windows in repository @local_config_cc which failed to fetch. no such package '@local_config_cc//':\r\nAuto-Configuration Error: Setting up VC environment variables failed, WINDOWSSDKDIR is not set by the following command:\r\n    \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Auxiliary\\Build\\VCVARSALL.BAT\" amd64  -vcvars_ver=14.27.29110\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed\r\nINFO: Elapsed time: 31.340s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (201 packages loaded, 3847 targets configured)\r\n    Fetching C:/users/10184590/_bazel_10184590/z5zmmi7q/external/icu; Extracting C:/users/10184590/_bazel_10184590/z5zmmi7q/external/icu/release-64-2.zip 24s\r\n    Fetching @llvm-project; fetching 15s\r\n    Fetching @boringssl; fetching 14s\r\n    Fetching .../_bazel_10184590/z5zmmi7q/external/llvm-project; Extracting C:/users/10184590/_bazel_10184590/z5zmmi7q/external/llvm-project/a2291a58bf1c860d026581fee6fe96019dc25440.tar.gz 14s\r\n    Fetching ...184590/_bazel_10184590/z5zmmi7q/external/boringssl; Extracting C:/users/10184590/_bazel_10184590/z5zmmi7q/external/boringssl/80ca9f9f6ece29ab132cce4cf807a9465a18cfac.tar.gz 14s\r\n\r\n\r\n", "comments": ["@li744831579 \r\nPlease refer to the bazel build related [issue](https://github.com/tensorflow/tensorflow/issues/23719) with same error and let us know if it helps.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43979\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43979\">No</a>\n"]}, {"number": 43978, "title": "Cannot save mixed precision model when using activity regularizer", "body": "<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),\r\nwe only address code/doc bugs, performance issues, feature requests and\r\nbuild/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: Python 3.6.9\r\n- Bazel version (if compiling from source): -\r\n- GCC/Compiler version (if compiling from source): -\r\n- CUDA/cuDNN version: 10.1/7.6.5.32-1+cuda10.1\r\n- GPU model and memory: RTX 2080Ti 11Gb\r\n\r\n**Describe the current behavior**\r\nWhen using activity_regularizer in a subclassed keras model, it is not possible to save the model if mixed_float16 policy is set.\r\nIf mixed_float16 is not set, the example below does work.\r\n\r\n**Describe the expected behavior**\r\nThe example below should save model.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n``` python:\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.mixed_precision import experimental as mixed_precision\r\n\r\n\r\npolicy = mixed_precision.Policy('mixed_float16')\r\nmixed_precision.set_policy(policy)\r\n\r\n\r\nclass TestModel(tf.keras.models.Model):\r\n    def __init__(self, **kwargs):\r\n        super(TestModel, self).__init__(**kwargs)\r\n        self.conv_0 = tf.keras.layers.Conv2D(1, (1, 1), activity_regularizer='l1')\r\n    \r\n    def call(self, x):\r\n        return self.conv_0(x)\r\n    \r\n    def get_config(self):\r\n        return {}\r\n\r\n\r\nmodel = TestModel()\r\n_ = model(np.zeros((1, 16, 16, 3)))\r\nmodel.save(\"test_model\")\r\n```\r\n\r\n**Other info / logs** \r\n```\r\nValueError: Python inputs incompatible with input_signature:\r\n  inputs: (\r\n    Tensor(\"StatefulPartitionedCall:0\", shape=(None, 16, 16, 1), dtype=float16))\r\n  input_signature: (\r\n    TensorSpec(shape=<unknown>, dtype=tf.float32, name=None))\r\n```\r\n", "comments": ["@ManuelWiese \r\n\r\nI have made small changes to your code and i am not seeing any issue. Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/1754042ea2ebf9446f560cc6254d5945/untitled454.ipynb).Please, verify once and let us know if it solves your question.Please, refer this [tutorial](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/Policy#how_to_use_mixed_precision_in_a_keras_model_2) and see if it helps you.Thanks!", "@ravikyram \r\nThanks for your reply! I looked through your code and as far as i understand the mixed precision policy is not set. Without mixed precision the example works perfectly.\r\n\r\nWhen using \"tf.keras.mixed_precision.experimental.set_policy('mixed_float16')\", as shown in the tutorial, in your colab it does not work anymore.", "I can reproduce the issue when . [Here](https://colab.research.google.com/gist/jvishnuvardhan/d17691aa4682b20d939bf6e89521d816/untitled58.ipynb) is the gist for our reference. \r\n\r\nNote: There is no error when `activity_regularizer='l1'` is removed from the following line\r\n`self.conv_0 = tf.keras.layers.Conv2D(1, (1, 1), activity_regularizer='l1')`", "Thank you for the very short and clean example to reproduce! I will have a fix soon.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43978\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43978\">No</a>\n", "Thank you for the fix. It does work in tf-nightly now!"]}, {"number": 43976, "title": "run-time error", "body": "internal/modules/cjs/loader.js:1122\r\n  return process.dlopen(module, path.toNamespacedPath(filename));\r\n", "comments": ["@X-swordx \r\nWe see that the issue template has not been filled, could you please do so as it helps us analyse the issue [tf version, steps followed before you ran into this error or stand alone code to reproduce the issue faced], if possible share a colab gist with the error reported.", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43976\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43976\">No</a>\n"]}, {"number": 43975, "title": "DOC Add None as possible in input_shape", "body": "Update docs to explicitly mention that `None` can be passed in `input_shape`.", "comments": []}, {"number": 43974, "title": "about RuntimeError: Use `_distributed_apply()` instead of `apply_gradients()` in a cross-replica context.", "body": "the tf version is 1.14\r\nwhen i use mirrored_strategy = tf.distribute.MirroredStrategy(), there are a error:\r\n    self.opt_op = self.optimizer.minimize(self.loss)\r\n  File \"miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 413, in minimize\r\n    name=name)\r\n  File \"miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 564, in apply_gradients\r\n    raise RuntimeError(\"Use `_distributed_apply()` instead of \"\r\nRuntimeError: Use `_distributed_apply()` instead of `apply_gradients()` in a cross-replica context.\r\n\r\nhow can i fix it?", "comments": ["@xhjcxxl \r\n\r\nRequest you to share colab link or simple standalone code to reproduce the issue in our environment.It helps us in localizing the issue faster.Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Has anyone found the solution to this error? Trying to use a TPU."]}, {"number": 43973, "title": "Update array_ops.py", "body": "minor edit on comment example", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!` and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F43973) for more info**.\n\n<!-- need_sender_cla -->", "We do not support minor edits, to prevent Hacktoberfest spam."]}, {"number": 43972, "title": "Error when bazel-building tensorflow from source", "body": "**System information**\r\n- OS Platform and Distribution : Ubuntu 18.04.5 LTS\r\n- TensorFlow installed from: source\r\n- TensorFlow version: TF 2.1\r\n- Python version: Python 3.7.5\r\n- Installed using: venv\r\n- Bazel version (if compiling from source): bazel 3.1.0\r\n- GCC/Compiler version (if compiling from source): gcc 7.5.0\r\n- CUDA/cuDNN version: -\r\n- GPU model and memory: - \r\n\r\n\r\nDear all,\r\nI am facing a problem when building tensorflow using bazel.\r\nAs a first remark, I have to say that I tried to install TF using pip; it worked but unfortunately my CPU does not support the AVX instruction set, so I couldn't really import TF after installation. I read here: [https://github.com/tensorflow/tensorflow/issues/30114](url) that I should build it from source, which is not working. Here's what I did and the error which prompts on the bash:\r\n\r\n1) I git-cloned:\r\n\r\ngit clone https://github.com/tensorflow/tensorflow.git\r\ncd tensorflow\r\n\r\n2) I run the configuration:\r\n\r\n(venv) fabio@fabio-HP-620:~/tensorflow$ ./configure\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /home/fabio/venv/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /home/fabio/venv/lib/python3.7/site-packages\r\n  /home/fabio/root-6.20.00_builddir/lib\r\n  /usr/local/lib/python3.7/dist-packages\r\n  /home/fabio/fastjet-install/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=mkl_aarch64    # Build with oneDNN support for Aarch64.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n\r\n3) I built:\r\nbazel build //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=192\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/fabio/venv/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/home/fabio/venv/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:short_logs in file /home/fabio/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file /home/fabio/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/fabio/tensorflow/.bazelrc: --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/fabio/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/fabio/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Repository local_config_python instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule python_configure defined at:\r\n  /home/fabio/tensorflow/third_party/py/python_configure.bzl:294:20: in <toplevel>\r\nERROR: An error occurred during the fetch of repository 'local_config_python':\r\n   Traceback (most recent call last):\r\n        File \"/home/fabio/tensorflow/third_party/py/python_configure.bzl\", line 267\r\n                _create_local_python_repository(<1 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/py/python_configure.bzl\", line 214, in _create_local_python_repository\r\n                _symlink_genrule_for_dir(<4 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/py/python_configure.bzl\", line 66, in _symlink_genrule_for_dir\r\n                \"\\n\".join(<1 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/py/python_configure.bzl\", line 66, in \"\\n\".join\r\n                read_dir(repository_ctx, <1 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/remote_config/common.bzl\", line 101, in read_dir\r\n                execute(repository_ctx, <2 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nfind: \u2018/usr/include/python3.7m\u2019: No such file or directory\r\nINFO: Repository aws-c-event-stream instantiated at:\r\n  no stack (--record_rule_instantiation_callstack not enabled)\r\nRepository rule third_party_http_archive defined at:\r\n  /home/fabio/tensorflow/third_party/repo.bzl:216:28: in <toplevel>\r\nERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Traceback (most recent call last):\r\n        File \"/home/fabio/tensorflow/third_party/py/python_configure.bzl\", line 267\r\n                _create_local_python_repository(<1 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/py/python_configure.bzl\", line 214, in _create_local_python_repository\r\n                _symlink_genrule_for_dir(<4 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/py/python_configure.bzl\", line 66, in _symlink_genrule_for_dir\r\n                \"\\n\".join(<1 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/py/python_configure.bzl\", line 66, in \"\\n\".join\r\n                read_dir(repository_ctx, <1 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/remote_config/common.bzl\", line 101, in read_dir\r\n                execute(repository_ctx, <2 more arguments>)\r\n        File \"/home/fabio/tensorflow/third_party/remote_config/common.bzl\", line 208, in execute\r\n                fail(<1 more arguments>)\r\nRepository command failed\r\nfind: \u2018/usr/include/python3.7m\u2019: No such file or directory\r\nINFO: Elapsed time: 20.364s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (70 packages loaded, 37 targets configured)\r\n\r\nI don't really know how to solve it. Thanks for your help, it's really appreciated.\r\n\r\nBest,\r\nFabio.", "comments": ["@fiemmi,\r\nPlease take a look at the [tested build configurations](https://www.tensorflow.org/install/source#cpu) and check if you are facing the same issue with Bazel v0.27.1 as well?\r\n\r\nAlso, make sure you have installed the pip package dependencies as mentioned [here](https://www.tensorflow.org/install/source#setup_for_linux_and_macos). Thanks!\r\n", " @amahendrakar \r\nThe pip depedencies are correctly installed. I could not install Bazel v0.27.1 (sudo apt install bazel-0.27.1 didn't find any package) but I tried to build TF 2.3.0, which uses Bazel 3.1.0 (i.e., the one I currently have) as a building tool. The error is still present.\r\n\r\nCheers,\r\nF.", "from the message, it seems\r\n```\r\nfind: \u2018/usr/include/python3.7m\u2019: No such file or directory\r\n```\r\nis the culprit.\r\n\r\n`apt-get install python3.7-dev` should resolve the issue.", "@freedomtan, @amahendrakar,\r\nthanks for your feedback, I had indeed installed python3-dev but not python3.7-dev. Once I did install it, the build went a bit further, but still coulnd't be completed. This time the error is:\r\n\r\n```\r\nERROR: /home/fabio/tensorflow/tensorflow/core/kernels/BUILD:6127:1: C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\n```\r\n\r\nI can give the full standard output content which was prompted by the build, if you believe it can help.\r\n\r\nThanks.", "> I can give the full standard output content which was prompted by the build, if you believe it can help.\r\n\r\n@fiemmi,\r\nYes please, that would be helpful. Could you please provide the exact sequence of commands / steps that you executed the second time along with the complete error log? Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Dear @amahendrakar,\r\n\r\napologies for the late reply. Concering the full sequence of commands/steps, I cannot really give it here, since I am not able to reproduce the error: if I try to build TF again, the laptop becomes super laggy and gets stuck in the end. Sorry for that. On the other hand, I have the full standard output from last time I could build:\r\n\r\n(venv) fabio@fabio-HP-620:~/tensorflow$ bazel build //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=192\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/fabio/venv/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.7/dist-packages --python_path=/home/fabio/venv/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/fabio/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/fabio/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/fabio/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/fabio/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: /home/fabio/tensorflow/tensorflow/core/BUILD:1749:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: /home/fabio/tensorflow/tensorflow/core/BUILD:2161:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation\r\nWARNING: /home/fabio/tensorflow/tensorflow/core/BUILD:1774:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_headers_for_pybind: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: /home/fabio/tensorflow/tensorflow/python/BUILD:4662:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/fabio/tensorflow/tensorflow/python/BUILD:115:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (380 packages loaded, 30793 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /home/fabio/.cache/bazel/_bazel_fabio/808e53958f3088fba3170445aa37ec42/sandbox\r\nINFO: From ProtoCompile tensorflow/compiler/xla/xla_data.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/example.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/feature.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor_shape.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/resource_handle.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/types.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/example/example_parser_configuration.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/lib/core/error_codes.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/bfc_memory_map.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/cluster.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/control_flow.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/config.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/data/experimental/snapshot.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tensorflow_server.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/debug.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/device_filters.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/debug_event.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/device_properties.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/graph_debug_info.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/named_tensor.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/meta_graph.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/queue_runner.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/remote_tensor_handle.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/rewriter_config.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saved_model.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saver.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/saved_object_graph.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/struct.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/trackable_object_graph.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tensor_bundle.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/transport_options.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/verifier_config.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/summary.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/event.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor_slice.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/versions.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/saved_tensor_slice.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/memmapped_file_system.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/protobuf/xplane.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/util/test_log.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/graph.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/allocation_description.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/cost_graph.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/api_def.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/variable.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/kernel_def.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/attr_value.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/step_stats.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/device_attributes.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/log_memory.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/function.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/remote_fused_graph_execute_info.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/reader_base.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/tensor_description.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/op_def.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/node_def.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/error_codes.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/framework/graph_transfer_info.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/profiler_options.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/grappler/costs/op_performance_data.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/stream_executor/dnn.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/conv_autotuning.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/autotuning.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From Compiling external/snappy/snappy-stubs-internal.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Compiling external/snappy/snappy-sinksource.cc:\r\ncc1plus: warning: command line option '-Wno-implicit-function-declaration' is valid for C/ObjC but not for C++\r\nINFO: From Executing genrule @llvm-project//mlir:LinalgNamedStructuredOpsIncGen__gen_impl_genrule:\r\nbazel-out/host/bin/external/llvm-project/mlir/mlir-linalg-ods-gen -gen-impl external/llvm-project/mlir/include/mlir/Dialect/Linalg/IR/LinalgNamedStructuredOpsSpec.tc -o bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/Dialect/Linalg/IR/LinalgNamedStructuredOps.cpp.inc\r\nINFO: From Executing genrule @llvm-project//mlir:LinalgNamedStructuredOpsIncGen__gen_ods_decl_genrule:\r\nbazel-out/host/bin/external/llvm-project/mlir/mlir-linalg-ods-gen -gen-ods-decl external/llvm-project/mlir/include/mlir/Dialect/Linalg/IR/LinalgNamedStructuredOpsSpec.tc -o bazel-out/k8-opt/bin/external/llvm-project/mlir/include/mlir/Dialect/Linalg/IR/LinalgNamedStructuredOps.td\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tpu/topology.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tpu/dynamic_padding.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tpu/compile_metadata.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/compiler/xla/service/hlo.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/compiler/xla/xla.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/compiler/tf2xla/tf2xla.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tpu/optimization_parameters.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tpu/tpu_embedding_output_layout.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/tpu/tpu_embedding_configuration.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/compiler/jit/xla_activity.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/compiler/xla/service/hlo_profile_printer_data.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/compiler/xla/service/hlo_execution_profile_data.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/compiler/tf2xla/host_compute_metadata.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/kernels/boosted_trees/boosted_trees.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/data/dataset.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/master.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/worker.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/profile.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/tfprof_log.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/debug/debug_service.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/tfprof_options.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/debug/debugger_event_metadata.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/profiler/tfprof_output.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/eager_service.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/python/framework/cpp_shape_inference.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/protobuf/replay_log.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/lite/toco/toco_flags.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/lite/toco/logging/toco_conversion_log.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/lite/toco/model_flags.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/compiler/mlir/lite/quantization/quantization_info.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/lite/toco/types.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/data/service/common.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\ntensorflow/core/data/service/common.proto:6:1: warning: Import tensorflow/core/framework/types.proto but not used.\r\nINFO: From ProtoCompile tensorflow/core/data/service/dispatcher.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nINFO: From ProtoCompile tensorflow/core/data/service/worker.pb.h:\r\nbazel-out/k8-opt/bin/external/com_google_protobuf/src: warning: directory does not exist.\r\nERROR: /home/fabio/tensorflow/tensorflow/core/kernels/BUILD:6127:1: C++ compilation of rule '//tensorflow/core/kernels:training_ops' failed (Exit 4)\r\ngcc: internal compiler error: Killed (program cc1plus)\r\nPlease submit a full bug report,\r\nwith preprocessed source if appropriate.\r\nSee <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 10392.185s, Critical Path: 3849.97s\r\nINFO: 2679 processes: 2679 local.\r\nFAILED: Build did NOT complete successfully\r\n", "Dear @amahendrakar, all,\r\n\r\nI have some updates. While the previous error, for what I understand, seemed to come from my laptop lacking computing resources and getting stuck, yesterday I managed in the end to have the build running without lag, and I got the following. I report the full steps that led to the error. Many thanks, your help is really appreciated.\r\n\r\n```\r\n(venv) fabio@fabio-HP-620:~$ bazel --version\r\nbazel 3.6.0\r\n```\r\n\r\n```\r\n(venv) fabio@fabio-HP-620:~$ pip --version\r\npip 20.2.4 from /home/fabio/venv/lib/python3.7/site-packages/pip (python 3.7)\r\n\r\n```\r\n```\r\n(venv) fabio@fabio-HP-620:~$ python3 --version\r\nPython 3.7.5\r\n```\r\n\r\n```\r\n(venv) fabio@fabio-HP-620:~$ gcc --version\r\ngcc (Ubuntu 7.5.0-3ubuntu1 ~18.04) 7.5.0\r\nCopyright (C) 2017 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n```\r\n\r\n```\r\n(venv) fabio@fabio-HP-620:~$ pip install -U pip numpy wheel\r\nCollecting pip\r\n  Downloading pip-20.2.4-py2.py3-none-any.whl (1.5 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.5 MB 662 kB/s \r\nCollecting numpy\r\n  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\r\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.5 MB 109 kB/s \r\nRequirement already up-to-date: wheel in ./venv/lib/python3.7/site-packages (0.35.1)\r\nInstalling collected packages: pip, numpy\r\n  Attempting uninstall: pip                                                                                                                                                                     \r\n    Found existing installation: pip 20.2.3                                                                                                                                                     \r\n    Uninstalling pip-20.2.3:\r\n      Successfully uninstalled pip-20.2.3\r\n  Attempting uninstall: numpy\r\n    Found existing installation: numpy 1.18.5\r\n    Uninstalling numpy-1.18.5:\r\n      Successfully uninstalled numpy-1.18.5\r\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\r\n\r\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.                                                                     \r\n\r\nkeras-applications 1.0.8 requires h5py, which is not installed.                                                                                                                                 \r\nSuccessfully installed numpy-1.19.2 pip-20.2.4\r\n\r\n(venv) fabio@fabio-HP-620:~$ pip install h5py\r\nCollecting h5py\r\n  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\r\nRequirement already satisfied: six in ./venv/lib/python3.7/site-packages (from h5py) (1.15.0)\r\nRequirement already satisfied: numpy>=1.7 in ./venv/lib/python3.7/site-packages (from h5py) (1.19.2)\r\nInstalling collected packages: h5py\r\nSuccessfully installed h5py-2.10.0\r\n\r\n(venv) fabio@fabio-HP-620:~$ pip install -U pip numpy wheel\r\nRequirement already up-to-date: pip in ./venv/lib/python3.7/site-packages (20.2.4)\r\nRequirement already up-to-date: numpy in ./venv/lib/python3.7/site-packages (1.19.2)\r\nRequirement already up-to-date: wheel in ./venv/lib/python3.7/site-packages (0.35.1)\r\n```\r\n\r\n\r\n\r\n\r\n```\r\n(venv) fabio@fabio-HP-620:~$ pip install -U keras_preprocessing --no-deps\r\nRequirement already up-to-date: keras_preprocessing in ./venv/lib/python3.7/site-packages (1.1.2)\r\n\r\n(venv) fabio@fabio-HP-620:~$ git clone https://github.com/tensorflow/tensorflow.git\r\nCloning into 'tensorflow'...\r\nremote: Enumerating objects: 95, done.\r\nremote: Counting objects: 100% (95/95), done.\r\nremote: Compressing objects: 100% (83/83), done.\r\nremote: Total 1026153 (delta 25), reused 33 (delta 12), pack-reused 1026058\r\nRicezione degli oggetti: 100% (1026153/1026153), 612.95 MiB | 1.65 MiB/s, done.\r\nRisoluzione dei delta: 100% (836642/836642), done.\r\nChecking out files: 100% (23610/23610), done.\r\n\r\n(venv) fabio@fabio-HP-620:~$ cd tensorflow/\r\n\r\n(venv) fabio@fabio-HP-620:~/tensorflow$ git checkout r2.3\r\nChecking out files: 100% (9147/9147), done.\r\nBranch 'r2.3' set up to track remote branch 'r2.3' from 'origin'.\r\nSwitched to a new branch 'r2.3'\r\n\r\n(venv) fabio@fabio-HP-620:~/tensorflow$ ./configure\r\nYou have bazel 3.1.0 installed.\r\nPlease specify the location of python. [Default is /home/fabio/venv/bin/python3]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /home/fabio/root-6.20.00_builddir/lib\r\n  /usr/lib/python3/dist-packages\r\n  /home/fabio/venv/lib/python3.7/site-packages\r\n  /usr/local/lib/python3.7/dist-packages\r\n  /home/fabio/fastjet-install/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/home/fabio/root-6.20.00_builddir/lib]\r\n/usr/local/lib/python3.7/dist-packages\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n\r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: n\r\nNo CUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to download a fresh release of clang? (Experimental) [y/N]: n\r\nClang will not be downloaded.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native -Wno-sign-compare]: n\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See .bazelrc for more details.\r\n        --config=mkl            # Build with MKL support.\r\n        --config=monolithic     # Config for mostly static monolithic build.\r\n        --config=ngraph         # Build with Intel nGraph support.\r\n        --config=numa           # Build with NUMA support.\r\n        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.\r\n        --config=v2             # Build TensorFlow 2.x instead of 1.x.\r\nPreconfigured Bazel build configs to DISABLE default on features:\r\n        --config=noaws          # Disable AWS S3 filesystem support.\r\n        --config=nogcp          # Disable GCP support.\r\n        --config=nohdfs         # Disable HDFS support.\r\n        --config=nonccl         # Disable NVIDIA NCCL support.\r\nConfiguration finished\r\n```\r\n\r\n\r\n\r\n\r\n```\r\n\r\n\r\n(venv) fabio@fabio-HP-620:~/tensorflow$ bazel build //tensorflow/tools/pip_package:build_pip_package\r\nStarting local Bazel server and connecting to it...\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=192\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.bazelrc:\r\n  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2\r\nINFO: Reading rc options for 'build' from /home/fabio/tensorflow/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=/home/fabio/venv/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.7/dist-packages --python_path=/home/fabio/venv/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0\r\nINFO: Found applicable config definition build:v2 in file /home/fabio/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:xla in file /home/fabio/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true\r\nINFO: Found applicable config definition build:linux in file /home/fabio/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels\r\nINFO: Found applicable config definition build:dynamic_kernels in file /home/fabio/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nWARNING: /home/fabio/tensorflow/tensorflow/core/BUILD:1749:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: /home/fabio/tensorflow/tensorflow/core/BUILD:2161:1: in linkstatic attribute of cc_library rule //tensorflow/core:framework_internal: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'tf_cuda_library', the error might have been caused by the macro implementation\r\nWARNING: /home/fabio/tensorflow/tensorflow/core/BUILD:1774:1: in linkstatic attribute of cc_library rule //tensorflow/core:lib_headers_for_pybind: setting 'linkstatic=1' is recommended if there are no object files. Since this rule was created by the macro 'cc_library', the error might have been caused by the macro implementation\r\nWARNING: /home/fabio/tensorflow/tensorflow/python/BUILD:4662:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nWARNING: /home/fabio/tensorflow/tensorflow/python/BUILD:115:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.\r\nINFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (380 packages loaded, 30793 targets configured).\r\nINFO: Found 1 target...\r\nINFO: Deleting stale sandbox base /home/fabio/.cache/bazel/_bazel_fabio/808e53958f3088fba3170445aa37ec42/sandbox\r\nINFO: From Compiling external/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:\r\nIn file included from external/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:206:0:\r\nbazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/X86/X86GenDAGISel.inc: In member function 'virtual bool {anonymous}::X86DAGToDAGISel::CheckNodePredicate(llvm::SDNode*, unsigned int) const':\r\nbazel-out/k8-opt/bin/external/llvm-project/llvm/lib/Target/X86/X86GenDAGISel.inc:287817:0: note: -Wmisleading-indentation is disabled from this point onwards, since column-tracking was disabled due to the size of the code/headers                                                                                                                                                           \r\n return true;                                                                                                                                                                                   \r\n \r\nERROR: /home/fabio/tensorflow/tensorflow/python/BUILD:501:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (Exit 1)\r\ntensorflow/python/lib/core/bfloat16.cc: In function 'bool tensorflow::{anonymous}::Initialize()':\r\ntensorflow/python/lib/core/bfloat16.cc:664:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [6], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:668:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [10], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:671:77: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [5], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n   if (!register_ufunc(\"less\", CompareUFunc<Bfloat16LtFunctor>, compare_types)) {\r\n                                                                             ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:675:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [8], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:679:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [11], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\ntensorflow/python/lib/core/bfloat16.cc:683:36: error: no match for call to '(tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>) (const char [14], <unresolved overloaded function type>, const std::array<int, 3>&)'\r\n                       compare_types)) {\r\n                                    ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note: candidate: tensorflow::{anonymous}::Initialize()::<lambda(const char*, PyUFuncGenericFunction, const std::array<int, 3>&)>\r\n                             const std::array<int, 3>& types) {\r\n                                                            ^\r\ntensorflow/python/lib/core/bfloat16.cc:638:60: note:   no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void (*)(char**, const long int*, const long int*, void*)}'\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 4996.847s, Critical Path: 105.03s\r\nINFO: 1068 processes: 1068 local.\r\nFAILED: Build did NOT complete successfully\r\n```", "Dear @amahendrakar, all,\r\n\r\nafter browsing the forum I found that the last error was faced by other people previously, e.g., in #40654.\r\nI followed the advices given there, i.e., I downgraded numpy, and I managed to complete the build successfully.\r\n\r\nThanks a lot for your help, I guess this thread can be closed.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43972\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43972\">No</a>\n"]}, {"number": 43971, "title": "Bump junit from 4.11 to 4.13.1 in /tensorflow/java/maven/tensorflow-hadoop", "body": "Bumps [junit](https://github.com/junit-team/junit4) from 4.11 to 4.13.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/junit-team/junit4/releases\">junit's releases</a>.</em></p>\n<blockquote>\n<h2>JUnit 4.13.1</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md\">release notes</a> for details.</p>\n<h2>JUnit 4.13</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md\">release notes</a> for details.</p>\n<h2>JUnit 4.13 RC 2</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.13 RC 1</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.13 Beta 3</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.13 Beta 2</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.13 Beta 1</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.12</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md\">release notes</a> for details.</p>\n<h2>JUnit 4.12 Beta 3</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md\">release notes</a> for details.</p>\n<h2>JUnit 4.12 Beta 2</h2>\n<p>No release notes provided.</p>\n<h2>JUnit 4.12 Beta 1</h2>\n<p>No release notes provided.</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66\"><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2\"><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24\"><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567\"><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9\"><code>543905d</code></a> Use separate line for annotation in Javadoc</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944\"><code>510e906</code></a> Add sub headlines to class Javadoc</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae\"><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec\"><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=\"https://github-redirect.dependabot.com/junit-team/junit4/issues/1671\">#1671</a>)</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646\"><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=\"https://github-redirect.dependabot.com/junit-team/junit4/issues/1672\">#1672</a>)</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8\"><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=\"https://github-redirect.dependabot.com/junit-team/junit4/issues/1660\">#1660</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/junit-team/junit4/compare/r4.11...r4.13.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.11&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/tensorflow/tensorflow/network/alerts).\n\n</details>", "comments": []}, {"number": 43970, "title": "Bump junit from 4.11 to 4.13.1 in /tensorflow/java/maven/spark-tensorflow-connector", "body": "Bumps [junit](https://github.com/junit-team/junit4) from 4.11 to 4.13.1.\n<details>\n<summary>Release notes</summary>\n<p><em>Sourced from <a href=\"https://github.com/junit-team/junit4/releases\">junit's releases</a>.</em></p>\n<blockquote>\n<h2>JUnit 4.13.1</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md\">release notes</a> for details.</p>\n<h2>JUnit 4.13</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md\">release notes</a> for details.</p>\n<h2>JUnit 4.13 RC 2</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.13 RC 1</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.13 Beta 3</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.13 Beta 2</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.13 Beta 1</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit4/wiki/4.13-Release-Notes\">release notes</a> for details.</p>\n<h2>JUnit 4.12</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md\">release notes</a> for details.</p>\n<h2>JUnit 4.12 Beta 3</h2>\n<p>Please refer to the <a href=\"https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md\">release notes</a> for details.</p>\n<h2>JUnit 4.12 Beta 2</h2>\n<p>No release notes provided.</p>\n<h2>JUnit 4.12 Beta 1</h2>\n<p>No release notes provided.</p>\n</blockquote>\n</details>\n<details>\n<summary>Commits</summary>\n<ul>\n<li><a href=\"https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66\"><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2\"><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24\"><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567\"><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9\"><code>543905d</code></a> Use separate line for annotation in Javadoc</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944\"><code>510e906</code></a> Add sub headlines to class Javadoc</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae\"><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec\"><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=\"https://github-redirect.dependabot.com/junit-team/junit4/issues/1671\">#1671</a>)</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646\"><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=\"https://github-redirect.dependabot.com/junit-team/junit4/issues/1672\">#1672</a>)</li>\n<li><a href=\"https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8\"><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=\"https://github-redirect.dependabot.com/junit-team/junit4/issues/1660\">#1660</a>)</li>\n<li>Additional commits viewable in <a href=\"https://github.com/junit-team/junit4/compare/r4.11...r4.13.1\">compare view</a></li>\n</ul>\n</details>\n<br />\n\n\n[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.11&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates)\n\nDependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.\n\n[//]: # (dependabot-automerge-start)\n[//]: # (dependabot-automerge-end)\n\n---\n\n<details>\n<summary>Dependabot commands and options</summary>\n<br />\n\nYou can trigger Dependabot actions by commenting on this PR:\n- `@dependabot rebase` will rebase this PR\n- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it\n- `@dependabot merge` will merge this PR after your CI passes on it\n- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it\n- `@dependabot cancel merge` will cancel a previously requested merge and block automerging\n- `@dependabot reopen` will reopen this PR if it is closed\n- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually\n- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)\n- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language\n- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language\n- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language\n- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language\n\nYou can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/tensorflow/tensorflow/network/alerts).\n\n</details>", "comments": []}, {"number": 43968, "title": "Adam Optimizer not working in migration of custom model", "body": "I'm performing a migration to my custom model from TensorFlow 1.x to TensorFlow 2.x, so parts of my custom model (Class Model) are wrapped in a function that decorated with tf.function so before I use session.run with feed_dict, but now I use a function that decorated with tf.function, the problem is that the accuracy is small and the loss is big, so I implement Adam optimizer inside the decorated function with tf.function (def new_function), but it shows error creating variable on non-first call, however, it runs when I declare Adam optimizer in forecast function(def forecast) where I call the Model to predict, but it doesn't affect the accuracy that should be increasing and the loss should be minimized, it just remains the same like without using adam optimizer but slower. Is something missing? or how should be the syntax written in the code and where should it be placed?\r\n\r\nnote : Adam are implemented with 3 lines of code with comment in def forecast, see the difference of speed with or without Adam, and why the accuracy is not increasing and loss is also still big.\r\n\r\nhttps://colab.research.google.com/drive/1VpahdrELXkMk29670yxLksOJMoR7uHKa\r\nthis is the colab pushed file\r\n\r\nkindly need help for this problem\r\n", "comments": ["I have tried in colab with TF nightly version(`2.4.0-dev20201012`) and was able to reproduce the issue.Please, find the gist [here](https://colab.research.google.com/gist/ravikyram/bb7af0c799ad201833cd5e22aa4ad25d/untitled446.ipynb).Thanks!", "Hey sir @ravikyram thanks for the respond, is it because it is still using lstmcell and multirnn from tensorflow.compat? bcs that is the only thing that shows and when i use lstmcell and stackedrnn from tf.keras it is error bcs the state_is_tuple must set to False, and i still dont understand what is the issue with adam optimizer in def forecast not giving any effect in the accuracy and loss of training. Thank You for the help, still waiting others \ud83d\udc4d ", "Apologies for the delay in response. This question is better asked on [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a bug or feature request. There is also a larger community that reads questions there. The response chances highly increase if you can come up with minimal code repro.\r\nThanks!\r\n", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43968\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43968\">No</a>\n"]}, {"number": 43967, "title": "could tf.feature_column.shared_embeddings support sequence_categorical_column_with_* in the future?", "body": "\r\n", "comments": ["@hazardwayne Perhaps you may want to raise a feature request for this issue. This will help to track the status and have a dedicated thread to discuss the functionality. Make sure to elaborate your proposal as well using this [feature template](https://github.com/tensorflow/tensorflow/issues/new?labels=type%3Afeature&template=30-feature-request.md). Thanks!\r\n", "Thanks for your help"]}]