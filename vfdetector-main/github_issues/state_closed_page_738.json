[{"number": 31431, "title": "Python 3.7 on Windows.", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version (you are using):\r\nTried many.\r\n\r\n- Are you willing to contribute it (Yes/No):\r\nYes\r\n\r\n\r\n**Describe the feature and the current behavior/state.**\r\nMake Tensorflow work for python 3.7 on Windows.\r\n\r\n**Will this change the current api? How?**\r\nI don't know.\r\n\r\n**Who will benefit with this feature?**\r\nUsers and developers.\r\n\r\n**Any Other info.**\r\nIt's a huge issue, dlib has same problem.", "comments": ["Hi,\r\nThe latest builds are normally compatible with python 3.7, whatever the (non-exotic) OS. Could you please be a little more specific as to which version of TF you are targetting, which installation method you have tried and what problems (including error stacks) you have encountered?", "I am just targeting\r\npip install tensorflow\r\n\r\nI have a repo with over 100 users, so many people are using windows. \r\nhttps://github.com/instabotai/\r\n\r\nWhen you enter \r\npip install tensorflow\r\n\r\nIn python 3.7 it is not recognized by pip.. So \"nothing\" happens\".. \r\n\r\nAlso if i look in here:\r\nhttps://www.tensorflow.org/install/pip#package-location\r\n\r\nI can see tensorflow pip support linux python 3.7 but not windows python 3.7.", "There should be python3.7 for Windows too, we are building the pips.", "> There should be python3.7 for Windows too, we are building the pips.\r\n\r\nThanks for your answer, then i will stick with tensorflow.. \r\n\r\nAre we talking a week before pip release, month or more ? \r\n\r\nBecause if it's more than i week, i will make a Docker option for my users with issues win/py3.7..\r\n\r\nThanks for answering me :) ", "I'll be looking to see why they don't get released for 1.14.1, this should probably land next week or the one after that.\r\n\r\nKeep also an eye on 2.0, there will be release candidates and if pip is missing please ping on here so we can look at why.", "Anything new?", "They will be provided with the new releases. There should be soon a release candidate 0 (RC0) for the next version and that one should be have the needed pips.", "something new ?", "There hasn't been a new release yet, but hopefully we'll have 2.0 RC0 early next week", "Great, people have been looking for this for years ;) ", "TF 2.0 RC0 is out, and I see a Python 3.7 link for each os at https://pypi.org/project/tensorflow/2.0.0rc0/#files\r\n\r\nPlease test it and let us know if there are still issues with python 3.7 on Windows", "pip install tensorflow==2.0.0rc0\r\nis still not working for 3.7 on Windows. Has anybody been able to get this to work without downgrading yet?", "it works on some windows system, it's all about the cpu :/", "@ebaj What version of pip you use? What CPU? What compiler? Which version of Windows? Using WSL?", "@ebaj, also please post output of `pip debug --verbose`", "I'm having the same issues. Tried tensorflow tensorflow-gpu and all pip upgrades...\r\nWindows Server 2019 with newest c++ on a Google Virtual Engine with Intel Xeon 2.2\r\n\r\nhere my pip debug --verbose:\r\n```\r\npip version: pip 19.2.3 from c:\\users\\kitesurf\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\pip (python 3.7)\r\nsys.version: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 21:26:53) [MSC v.1916 32 bit (Intel)]\r\nsys.executable: c:\\users\\kitesurf\\appdata\\local\\programs\\python\\python37-32\\python.exe\r\nsys.getdefaultencoding: utf-8\r\nsys.getfilesystemencoding: utf-8\r\nlocale.getpreferredencoding: cp1252\r\nsys.platform: win32\r\nsys.implementation:\r\n  name: cpython\r\nConfig variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect\r\nConfig variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect\r\nCompatible tags: 14\r\n  cp37-cp37m-win32\r\n  cp37-none-win32\r\n  py3-none-win32\r\n  cp37-none-any\r\n  cp3-none-any\r\n  py37-none-any\r\n  py3-none-any\r\n  py36-none-any\r\n  py35-none-any\r\n  py34-none-any\r\n  py33-none-any\r\n  py32-none-any\r\n  py31-none-any\r\n  py30-none-any\r\n```", "Yeah, I have seen many report unable to use TensorFlow on Windows Servers. ", "@kitesurf Our windows tag is `cp37-cp37m-win_amd64`, but you have `cp37-cp37m-win32`. That is, we only release for 64 bits but your CPU is 32 bits, hence we don't have a pip for you.\r\n\r\nYou can try building from source", "> @kitesurf Our windows tag is `cp37-cp37m-win_amd64`, but you have `cp37-cp37m-win32`. That is, we only release for 64 bits but your CPU is 32 bits, hence we don't have a pip for you.\r\n> \r\n> You can try building from source\r\n\r\nWhy not isn't it easy to implement or are you guys jumping on the 64bit should be a standard thing for 2.0?\r\n\r\nRemember many users are still using tensorflow for servers, most servers are 32bit. ", "Our support matrix is already extremely large, supporting 32 bits on x86 CPUs will almost double it and this is very untractable.", "I know what you mean, but people want to use a package, all users easy can install. There is no point in making your app support TensorFlow, and then telling all your users which use 32 how to compile from source. ", "@mihaimaruseac Is it really the CPU, or actually the type of Python installed?\r\nMy physical CPU is a 64-bit Intel but I was running 32-bit Python, as many users do, so platform.architecture was showing up as 32 bit, which is where I think the mismatch arose.\r\nAfter testing with 64-bit Python 3.7.4, tensorflow installed successfully with pip.\r\n\r\nI believe the \"latest version\" download from Python defaults to the 32-bit installer so I imagine many people will be in this situation, if this is indeed the source of the issue.  \r\nIn addition, [this thread on stackflow](https://stackoverflow.com/questions/3117626/should-i-use-python-32bit-or-python-64bit) highlights why many people even on 64-bit machines still choose to run 32-bit Python.\r\n\r\nWith TensorFlow's well-earned popularity, I hope the resources will exist to support this userbase.", "Yes it works with python version 64 bit. You are correct on all your statements. \r\nMost user by default gets confused, because more either download 32 bit because it's the default python version, or they use a VM or like my server a 32 bit CPU. I own a repo with hundreds of users and I get daily issues from people who need support for this issue. This issue is a must, it destroys it for people who want to use it and not spending hours daily because of TensorFlow issues users has. \r\n\r\nI don't think the devs want this supported, they are devs not product creators. This would most likely not happen, they have a \"dev\" roadmap. They will wait 2 years until 64 bit becomes standard for python, they build for the \"future\". \r\n\r\nI know many devs who does not understand how to compile tensorflow from source, so the normal users don't have a chance. \r\n\r\nI am thinking about creating a package for all package mangers for my app, just to fix this problem. \r\n\r\n### **Huge Issues**\r\n- Pip installer does not work with 32 bit python.\r\n- Pip installer does also not work with 32 bit because of above issue.\r\n- Python downloads 32 bit by default **BIG PROBLEM**", "> Most user by default gets confused, because more either download 32 bit because it's the default python version, or they use a VM or like my server a 32 bit CPU.\r\n\r\nYou know 64 bits processors have been the norm for ten years, right? And I don't know where you find your python downloads, but 64 bits _is_ the default.\r\n\r\n> I don't think the devs want this supported, they are devs not product creators.\r\n\r\nSkip the newspeak pal, and have a little respect for tensorflow devs. You can pretty easily compile a 32 bits version if you need; they already make an impressive job supporting a large number of 64 bits system, and you cannot blame them for putting their efforts into fixing bugs, adding functionalities and enhancing existing ones rather than avoiding you the not-so-tremendous effort to have to compile the package once in a while.\r\n\r\n> I am thinking about creating a package for all package mangers for my app, just to fix this problem.\r\n\r\nMakes sense ; you have a use-case problem, make a use-case solution. That being said, instead of adding a package manager layer, I would intuitively recommend just packing (pre-compiled) dependencies with your app (e.g. with pex or docker). You know, like a dev deploying their tool might do.", "You are talking like I have the problem, yet it is my hundreds of users who has the problem. That you are trying to say \"do this, do that\". You don't understand the larger issue on scale.\r\n\r\nWhen python is 32 bit by default, and you are talking about norms. Maybe you should follow the norm for python, so python users can install this package, instead of talking about what others do.\r\n\r\n50% of all python users has issues installing TensorFlow, but that is just the norm for years. Maybe you should fix that? \r\n\r\nBut if you guys don't want to fix it, I will most likely just fix it myself, my users does not care about the norm, but just it works, maybe you should try that to. \r\n\r\nI even have a docker, so because you don't want to support 32 bit your solution is, for users to go into the bios, enable Hyper-V go back install the docker and then install the package. haha\r\n\r\nAlso docker does not work on these version of windows:\r\nWindows 10 Home; Windows 10 Mobile; Windows 10 Mobile Enterprise, because they does not support Hyper-V.\r\n\r\nWhich is 50% of the windows market\r\n\r\nI will look into pex, sorry for rant i have just seen many post about this issue for years and i get over 10 messages from people daily, who asks me how to install TensorFlow, because of 32bit. \r\n\r\nIt's really frustrating to create an app and then TensorFlow fills 50% of the user support and 80% runs away because of Tensorflow installation issues.\r\n\r\n", "I'm disputing the 50% statistics, the 80% numbers, referencing a 10 years old Stack Overflow issue and many other anectdata in this thread.\r\n\r\nHowever, this is getting offtopic and the main issue seems to be solved: python3.7 pips for Windows are now released.\r\n\r\nFor 32 bits architecture support, I created a feature request: #32315. Closing this one", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31431\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31431\">No</a>\n", "https://github.com/tensorflow/tensorflow/issues/32627"]}, {"number": 31430, "title": "cherrypick to fix the Linux CPU py2 pip builds", "body": "\u2026g and testing TensorFlow using pip_new.sh.\r\n\r\nPiperOrigin-RevId: 262165125", "comments": []}, {"number": 31429, "title": "SIGABRT on `tf.image.encode_png` with empty tensor", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): tf-nightly-2.0-preview==2.0.0.dev20190807\r\n- TensorFlow version (use command below): v1.12.1-8193-ge7d48dc 2.0.0-dev20190807\r\n- Python version: 3.6.6\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: None\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\nCalling `tf.image.encode_png` kills the process with SIGABRT if you pass\r\na tensor that has no elements.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should never SIGABRT.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nimage = tf.cast(tf.tile([[[0, 0, 0]]], [0, 0, 1]), tf.uint8)\r\n# Or: = tf.cast(tf.reshape([], [0, 0, 3]))\r\ntry:\r\n  tf.print(image)\r\n  tf.print(tf.shape(image))\r\n  tf.image.encode_png(image)\r\nfinally:\r\n  print(\"We never get here!\")\r\n```\r\n\r\n```\r\n$ TF_CPP_MIN_LOG_LEVEL=1 python test.py\r\n[]\r\n[0 0 3]\r\n2019-08-07 16:32:12.864303: F tensorflow/core/lib/png/png_io.cc:347] 'image' Must be non NULL\r\nAborted\r\n$ echo $?\r\n134\r\n```\r\n\r\n**Other info / logs**\r\n\r\nThis also affects `tf.summary.image`.\r\n", "comments": ["Issue replicating with tf-nightly-2.0-preview, please find the [gist](https://colab.sandbox.google.com/gist/oanush/3e90e50f2c711e628cf859a71d55dafa/31429.ipynb) of colab.Thanks", "Beginner here. Would like to work on this issue. Any leads?", "@Soniyanayak51 check `tensorflow/core/kernels/encode_png_op.cc`, in `Compute`, just before the `png::WriteImageToBuffer` gets called, `image.flat<..>.data()` should be checked. It currently returns nullptr which then causes the abort.", "Thanks for the reply. On it", "@oanush: Not sure what you mean. Your Colab is not publicly visible, and\r\nI can reproduce this in a fresh virtualenv:\r\n\r\n```\r\n$ cd \"$(mktemp -d)\"\r\n$ virtualenv -q -p python3.6 ./ve\r\n$ . ./ve/bin/activate\r\n(ve) $ pip install -q tf-nightly-2.0-preview==2.0.0.dev20190807\r\n(ve) $ python\r\nPython 3.6.7 (default, Oct 21 2018, 08:08:16) \r\n[GCC 8.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import tensorflow as tf\r\n>>> tf.__version__\r\n'2.0.0-dev20190807'\r\n>>> image = tf.cast(tf.tile([[[0, 0, 0]]], [0, 0, 1]), tf.uint8)\r\n>>> tf.print(image)\r\n[]\r\n>>> tf.print(tf.shape(image))\r\n[0 0 3]\r\n>>> tf.image.encode_png(image)\r\n2019-08-08 10:28:38.860641: F tensorflow/core/lib/png/png_io.cc:347] 'image' Must be non NULL\r\nAborted\r\n(ve) $ echo $?\r\n134\r\n```\r\n", "@mihaimaruseac \r\nSorry, I am stuck with running tests here. Any leads on how to test code easily before raising a PR?", "I would build a pip package, install it in a virtualenv and test with the code to reproduce posted at the beginning of the issue.\r\n\r\nSee https://www.tensorflow.org/install/source for how to build the pip package.\r\n\r\nThen, if that works, we can continue making this either a Python integration test or converting it to a C++ test.", "Added a PR #33220 for the fix. /cc @mihaimaruseac please take a look.", "Was able to reproduce the issue with [TF v2.1](https://colab.research.google.com/gist/amahendrakar/855918e07fcb96fe091964a7b0c543be/2-1-template.ipynb) and [TF-nightly](https://colab.research.google.com/gist/amahendrakar/343256a8d0c24e08a95b050b90bc5942/tf-nightly.ipynb#scrollTo=ieAW-NK5iqpf) i.e. v2.2.0-dev20200327. Session crashes in both the cases. Please find the attached gist. Thanks!", "@amahendrakar there is no attached gist", "@mihaimaruseac,\r\nTF 2.1 - https://colab.research.google.com/gist/amahendrakar/855918e07fcb96fe091964a7b0c543be/2-1-template.ipynb\r\n\r\nTF-nightly - https://colab.research.google.com/gist/amahendrakar/343256a8d0c24e08a95b050b90bc5942/tf-nightly.ipynb#scrollTo=ieAW-NK5iqpf\r\n\r\nHere are the direct links. Could you please check if these work? Thanks!\r\n", "\r\nWas able to reproduce the issue with TF v2.3 and [TF-nightly ](https://colab.research.google.com/gist/Saduf2019/007bcd1a40306bb9c57096a9c7bd1e8c/untitled359.ipynb)", "@wchargin I tried to reproduce the issue in TF 2.5 and the session is not crashing  but facing different error. Please check the gist [here](https://colab.research.google.com/gist/saikumarchalla/74fcac3ed929e5823b6400ea886e556e/untitled78.ipynb).Thanks!", "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31429\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31429\">No</a>\n", "The error in the recent gist is the supposed error to prevent the crash. This is now fixed."]}, {"number": 31428, "title": "Constant Folding: Only apply size limit if size of tensor is increasing", "body": "[This recent commit](https://github.com/tensorflow/tensorflow/commit/56488bea5c1258fc89207944f1aa5d3f4e32a495#diff-f3c968af33d813270dacefabc01b6a73) accidently reverted my improvements from #24478\r\n\r\nThis PR adds now ensures that CreateNodeDef and IsConstantFoldable follow the same logic regarding max size.\r\n\r\nI also added a unit test to make sure such a regression doesn't happen in the future.", "comments": []}, {"number": 31427, "title": "Document AssertNextDataset op", "body": "This PR adds documentation to the API file for the `AssertNextDataset` op, which is part of `tf.data`.", "comments": []}, {"number": 31426, "title": "Optimizer other than GradientDescent throwing errors on first run", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colag\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 1.14\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\n- GPU model and memory:\r\n\r\n\r\n\r\n**Describe the current behavior**\r\nOn running the train method, it is throwing error despite initializing all variables before the optimize step. \r\n\r\n**Describe the expected behavior**\r\nIt should not throw errors and initialize all variables.\r\n**Code to reproduce the issue**\r\nColab [gist](https://colab.research.google.com/gist/dsgupta/bb90037cb95573f610ff2d974883286d/test.ipynb)\r\n\r\n", "comments": ["You need to initialize the variables inside the session. For instance,\r\n```python\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    # rest of your code \r\n```\r\nSee https://www.tensorflow.org/guide/variables#initializing_variables", "@ymodak  \r\nDoesn't the following take care of that?\r\n```\r\nwith tf.Session() as sess:\r\n    sess.run([var_init, iterator_init])\r\n```", "You need to slightly tweak the ```fetches``` arg in ```session.run```\r\nThe ```fetches``` argument may be a single graph element, or an arbitrarily nested list, tuple.\r\nSee https://www.tensorflow.org/api_docs/python/tf/Session#run\r\nYou need to pass elements ```var_init, iterator_init``` as a tuple perhaps.\r\n```python\r\nmy_tuple = (var_init, iterator_init)\r\nwith tf.Session() as sess:\r\n    sess.run(my_tuple)\r\n```\r\nOutput\r\n```python\r\n0.7025812\r\n5.704461e-10\r\n1.5058428e-13\r\n2.5943065e-16\r\n1.4000413e-18\r\n1.699697e-20\r\n3.7725005e-22\r\n1.3543658e-23\r\n7.2010655e-25\r\n5.316337e-26\r\n```", "I think it's because global_variables() are read by global_variables_initializer before they are created by the train_step.\r\n\r\nThis works:\r\n```python\r\nwith mirrored_strategy.scope():\r\n  input_iterator = dist_dataset.make_initializable_iterator()\r\n  iterator_init = input_iterator.initialize()\r\n  loss = train_step(input_iterator.get_next())\r\n  var_init = tf.global_variables_initializer()  \r\n  with tf.Session() as sess:\r\n    sess.run([var_init, iterator_init])\r\n    for _ in range(10):\r\n      print(sess.run(loss))\r\n```\r\n\r\nThanks for the colab.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31426\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31426\">No</a>\n"]}, {"number": 31425, "title": "Replace #31411 with 0ccb3675", "body": "Based on https://github.com/tensorflow/tensorflow/pull/31411#issuecomment-519282298, 0ccb3675 is the better fix for the issue cherry-picked in #31411", "comments": []}, {"number": 31424, "title": "Cherry-picks for 1.14 with manylinux toolchain", "body": "", "comments": []}, {"number": 31423, "title": "Update release notes for TensorFlow 1.14.1", "body": "This PR is intentionally incomplete. One of the Release Owners for 1.14.1\nneeds to fill in the internal release notes for this version before the PR gets\nsubmitted. Click on the :pencil2: icon in the header for `RELEASE.md` under\n\"Files Changed\" above.", "comments": ["This was already done in the past, recreated now due to tool restart, please ignore"]}, {"number": 31422, "title": "r2.0-rc0 cherry-pick request: Fix FlushQuantileSummaries Op", "body": "Without this change TF.Transform compatibility with TF 2.0 will be significantly delayed and hindered. This new op is necessary for quantiles to function without tf.contrib.", "comments": []}, {"number": 31421, "title": "[TF 2.0] Issues Serializing SavedModel Serving Default Signature", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX + Linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\r\n- TensorFlow installed from (source or binary): `pipenv install --pre tensorflow==2.0.0-beta1 --python=3.6.8`\r\n- TensorFlow version (use command below): `2.0.0-beta1`\r\n- Python version: `3.6.8`\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n**Describe the current behavior**\r\n\r\nI want to use a `SavedModel` to run batch inference in `Spark`. A few libraries implement this functionality, such as [Databrick's Spark Deep Learning](https://github.com/databricks/spark-deep-learning), however, they are implemented using `TF 1.13.1` or lower. I am using `TF 2` to train and save my models, so I would like to use the same version for deploying my model. \r\n\r\nTo use an object in PySpark it must be serializable, but I am getting a pickle issue when trying to use a `SavedModel`'s `signatures['serving_default]` concrete function. \r\n\r\nPreviously, this call was serializable\r\n```python\r\n# TensorFlow 1.X\r\noutputs = session.run(f(placeholder), feed_dict={placeholder: input})\r\n``` \r\n\r\nNow, this is giving issues\r\n```python\r\n# TensorFlow 2.0\r\noutputs = f(input)\r\n```\r\n\r\nThis behavior can be reproduced without using `Spark`. You only need to try `pickle.dump` the `signatures['serving_default']` concrete function.\r\n\r\n**Describe the expected behavior**\r\n\r\nI expect both the `TF 1.X` and `TF 2.0` ways of predicting from a `SavedModel` to be serializable\r\n\r\n```python\r\n# TensorFlow 1.X\r\noutputs = session.run(f(placeholder), feed_dict={placeholder: input})\r\n# TensorFlow 2.0\r\noutputs = f(input)\r\n```\r\n\r\nLet me know if I am approaching this problem the wrong way! Any help is appreciated. \r\n\r\n**Code to reproduce the issue**\r\n\r\n```python\r\n\r\n# Example\r\nimport os\r\nimport pickle\r\n\r\nfrom pathlib import Path\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\n\r\n\r\n# Create a simple model\r\ninputs = tf.keras.Input(shape=(784,), name='img')\r\n\r\nx = layers.Dense(64, activation='relu')(inputs)\r\nx = layers.Dense(64, activation='relu')(x)\r\noutputs = layers.Dense(10, activation='softmax')(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')\r\n\r\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\r\n\r\n# Compile\r\nmodel.compile(loss='sparse_categorical_crossentropy',\r\n              optimizer=tf.keras.optimizers.RMSprop(),\r\n              metrics=['accuracy'])\r\n\r\n# Train\r\nhistory = model.fit(x_train, y_train,\r\n                    batch_size=64,\r\n                    epochs=5,\r\n                    validation_split=0.2)\r\n\r\n# Save to SavedModel \r\noutput_dir = \"/tmp/workdir/v1/\"\r\noutput_directory = Path(output_dir)\r\noutput_directory.mkdir(parents=True, exist_ok=True)\r\n\r\nmodel_save_path = os.path.join(output_dir,'model')\r\n\r\nmodel.save(tf_model_save_path, save_format=\"tf\")\r\n\r\n\r\n# Load the SavedModel\r\n\r\nsaved_model = tf.saved_model.load(model_save_path, tags=['serve'])\r\n\r\n# Get the 'predict' concrete function\r\ninfer = saved_model.signatures['serving_default']\r\n\r\npickle_file = os.path.join(output_dir, 'serialized')\r\n\r\n# Try serialize the function \r\nwith open(pickle_file, 'wb') as f:\r\n    pickle.dump(infer, f)\r\n\r\n# Expect error \r\n# _pickle.PickleError: can't pickle repeated message fields, convert to list first\r\n```\r\n\r\n**Other info / logs**\r\n````\r\n_pickle.PickleError: can't pickle repeated message fields, convert to list first\r\n````", "comments": ["I have tried in Jupyter notebook with TF version 2.0 beta1 and was able to reproduce the issue.Please, find the gist in attachment.Thanks!\r\n[savedmodel.ipynb.tar.gz](https://github.com/tensorflow/tensorflow/files/3480677/savedmodel.ipynb.tar.gz)\r\n", "I'm not sure I see the relation between pickling an object and Spark. Spark directly uses the SavedModel by calling `tf.saved_model.loader`, without re-serializing it to another format (from an admittedly brief look at the library - let met know if I missed something). Can you clarify?", "@k-w-w Thanks for taking a look at this.\r\n\r\nWhen you create a [user defined function](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) in Spark, it serializes (pickles) the function in order to distribute it across workers. In this case, I want to create a [user defined function](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) that predicts on each row with a SavedModel, but the method for predicting in `TF 2.0`\r\n\r\n```python\r\n# Get the 'predict' concrete function\r\ninfer = saved_model.signatures['serving_default']\r\n```\r\n\r\nis no longer serializable. Whereas, \r\n\r\n```python\r\noutputs = session.run(f(placeholder), feed_dict={placeholder: input})\r\n```\r\n\r\nwas serializable, so it worked with [user defined functions](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) \r\n\r\nLet me know if that makes sense! \r\n", "That makes sense, thank you for the explanation!\r\n\r\n`infer` is a ConcreteFunction which is an object defined in Tensorflow, which isn't compatible with pickle. You can file a separate issue requesting this feature, or submit a PR to add this functionality. \r\n\r\nA possible workaround is to create a regular python function that loads the SavedModel and calls the default signature function, and register that as a user-defined function.\r\n\r\nI'll close this issue because it is not a bug, but please do file the feature request or try the workaround suggested!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31421\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31421\">No</a>\n", "@k-w-w \r\nThanks! I'll make a feature request if this workaround doesn't work \ud83d\udc4d \r\n", "@k-w-w Workaround worked \ud83d\ude04 \r\n\r\nFor all those that run into this issue, read the guide below!\r\n\r\n# Guide to Deploying a TF 2.0 SavedModel on Spark using PySparK\r\n\r\n\r\n## Load Dependencies \r\n```python\r\n\r\nimport tensorflow as tf\r\n\r\nfrom pyspark import SparkFiles\r\nfrom pyspark.sql.functions import udf\r\nimport pyspark.sql.types as T\r\nfrom pyspark.sql import Row\r\n\r\nprint(tf.__version__)\r\n```\r\n\r\n## Fetch SavedModel from S3/GCS and Distribute to Nodes\r\n\r\n```python\r\nS3_PREFIX = \"s3://\"\r\n\r\nMODEL_BUCKET = \"my-models-bucket\"\r\nMODEL_PATH = \"path/to/my/model/dir\"\r\nMODEL_NAME = \"model\"\r\n\r\nS3_MODEL = f\"{S3_PREFIX}{MODEL_BUCKET}/{MODEL_PATH}/{MODEL_NAME}\"\r\n\r\nprint(\"Fetching model\", S3_MODEL)\r\n\r\n# Add model to all workers\r\nspark.sparkContext.addFile(S3_MODEL, recursive=True)\r\n```\r\n\r\n## Create the Input Dataframe\r\n\r\n```python\r\n# In this example, the SavedModel has the following format:\r\n\r\n# inputs = tf.keras.Input(shape=(784,), name='img')\r\n# x = layers.Dense(64, activation='relu')(inputs)\r\n# x = layers.Dense(64, activation='relu')(x)\r\n# outputs = layers.Dense(10, activation='softmax')(x)\r\n# model = tf.keras.Model(inputs=inputs, # outputs=outputs, name='mnist_model')\r\n\r\n(_, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\r\nx_test = x_test.reshape(10000, 784).astype('float32') / 255\r\n\r\nrows = list(map(lambda n: Row(img=[n.tolist()]), x_test))\r\n\r\nschema = T.StructType([T.StructField('img',T.ArrayType(T.ArrayType(T.FloatType())))])\r\n\r\ninput_df = spark.createDataFrame(rows, schema=schema)\r\n```\r\n\r\n## Memoize Retrieval of the Saved Model\r\n\r\n```python\r\n# Simple memoization helper with a single cache key\r\ndef compute_once(f):\r\n    K = '0'\r\n    cache = {}\r\n    \r\n    def wrapper(x):\r\n        # Set on first call\r\n        if K not in cache:\r\n            cache[K] = f(x)\r\n        \r\n        return cache[K]\r\n\r\n    return wrapper\r\n    \r\n\r\ndef load_model(model_name):\r\n    # Models are saved under the SparkFiles root directory\r\n    root_dir = SparkFiles.getRootDirectory()\r\n    export_dir = f\"{root_dir}/{model_name}\"\r\n    \r\n    return tf.saved_model.load(export_dir, tags=['serve'])\r\n    \r\n\r\n# Only load the model once per worker!\r\n# The reduced disk IO makes prediction much faster\r\nmemo_model_load = compute_once(load_model)\r\n\r\ndef get_model_prediction(model_name, input):\r\n    \"\"\"\r\n    Note: \r\n        TF session is scoped to where the model is loaded.\r\n        All calls to the model's ConcreteFunciton must be in the same scope as\r\n        the loaded model (i.e in the same function!)\r\n        \r\n        If not, TF will throw errors for undefined/ variables\r\n    \"\"\"\r\n    # Load the predict function (from disk or cache)\r\n    m = memo_model_load(model_name)\r\n    \r\n    # Save the predict signature\r\n    pred_func = m.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\r\n    \r\n    return pred_func(input)\r\n\r\n```\r\n\r\n## Create the Predict UDF\r\n\r\n```python\r\n# Decorator with return type of UDF\r\n@udf(\"array<array<float>>\")\r\ndef infer(data):\r\n    # Cast the input to a Tensor\r\n    input_data = tf.constant(data)\r\n    \r\n    # Returns a dict of the form { TENSOR_NAME: Tensor }\r\n    outputs = get_model_prediction(MODEL_NAME, input_data)\r\n\r\n    # Assuming we have a single output\r\n    output_tensor = list(outputs.values())[0]\r\n    \r\n    # Convert back to regular python\r\n    output_value = output_tensor.numpy().tolist()\r\n    \r\n    return output_value\r\n```\r\n\r\n## Infer on the Dataset \ud83c\udf89\r\n\r\n```python\r\npredictions_df = input_df.withColumn(\"predictions\", infer(\"img\"))\r\n\r\n# All done :) \r\npredictions_df.show(vertical=True)\r\n```\r\n"]}, {"number": 31420, "title": "Cannot install TF wheel file for Python3 due to functools32 dependency", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 (Linux v5.1.0)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version: b28755fbd2f33baaff2ce703f456513f087f8e76 (`master`)\r\n- Python version: 3.5.2\r\n- Installed using virtualenv? pip? conda?: pip3 19.2.1\r\n- Bazel version (if compiling from source): 0.28.1\r\n- GCC/Compiler version (if compiling from source): 5.4.0\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n\r\n\r\n\r\n**Describe the problem**\r\nSince commit 82c46d74d9ece4f6b5832f27a7ae580d95e1a312, `pip3` refuses to install the tensorflow wheel file due to the missing `functools32` dependency. Since I'm running Python3, I cannot install `functools32` since it is a backport for Python2.\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n```\r\n# Inside working copy of tensorflow\r\n$ bazel build //tensorflow/tools/pip_package:build_pip_package\r\n...\r\n$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /data/Packages/\r\nINFO: Elapsed time: 61.754s, Critical Path: 33.82s\r\nINFO: 533 processes: 533 local.\r\nINFO: Build completed successfully, 753 total actions\r\nWed Aug 7 12:47:45 CDT 2019 : === Preparing sources in dir: /tmp/tmp.jkwGwPI847\r\n...\r\nWed Aug 7 12:49:09 CDT 2019 : === Building wheel\r\nwarning: no files found matching '*.pyd' under directory '*'\r\nwarning: no files found matching '*.pd' under directory '*'\r\nwarning: no files found matching '*.dylib' under directory '*'\r\nwarning: no files found matching '*.dll' under directory '*'\r\nwarning: no files found matching '*.lib' under directory '*'\r\nwarning: no files found matching '*.h' under directory 'tensorflow/include/tensorflow'\r\nwarning: no files found matching '*' under directory 'tensorflow/include/third_party'\r\nWed Aug 7 12:49:40 CDT 2019 : === Output wheel file is in: /data/Packages/\r\n\r\n$ pip3 install --upgrade /data/Packages/tensorflow-1.14.0-cp27-cp27mu-linux_x86_64.whl\r\nProcessing /data/Packages/tensorflow-1.14.0-cp27-cp27mu-linux_x86_64.whl\r\nCollecting protobuf>=3.6.1 (from tensorflow==1.14.0)\r\n  Using cached https://files.pythonhosted.org/packages/22/cb/8ca68af4233c09f2dd9833f3b9c6d8e706da2d33988dfb3732a777e15e4b/protobuf-3.9.1-cp35-cp35m-manylinux1_x86_64.whl\r\nCollecting gast>=0.2.0 (from tensorflow==1.14.0)\r\nCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow==1.14.0)\r\n  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\r\nCollecting keras-applications>=1.0.8 (from tensorflow==1.14.0)\r\n  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\r\nCollecting termcolor>=1.1.0 (from tensorflow==1.14.0)\r\nCollecting opt-einsum>=2.3.2 (from tensorflow==1.14.0)\r\nCollecting tensorboard<1.15.0,>=1.14.0 (from tensorflow==1.14.0)\r\n  Using cached https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl\r\nCollecting functools32>=3.2.3 (from tensorflow==1.14.0)\r\n  Using cached https://files.pythonhosted.org/packages/c5/60/6ac26ad05857c601308d8fb9e87fa36d0ebf889423f47c3502ef034365db/functools32-3.2.3-2.tar.gz\r\n    ERROR: Command errored out with exit status 1:\r\n     command: /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-hmfnihxv/functools32/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-hmfnihxv/functools32/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base pip-egg-info\r\n         cwd: /tmp/pip-install-hmfnihxv/functools32/\r\n    Complete output (1 lines):\r\n    This backport is for Python 2.7 only.\r\n    ----------------------------------------\r\nERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\r\n```\r\n\r\n**Any other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n", "comments": ["It looks like this package was built in Python 2.7 mode:\r\n\r\n$ pip3 install --upgrade /data/Packages/tensorflow-1.14.0-**cp27-cp27**mu-linux_x86_64.whl\r\n\r\nWhen you ran `./configure` prior to your first call to Bazel, Ubuntu 16 may have defaulted to Python 2, so the package was built in Python 2 mode. You may have to change the Python path in another call to `./configure`.\r\n", "@angersson Thanks. That makes sense. I attached my [`.tf_configure.bazelrc`](https://github.com/tensorflow/tensorflow/files/3487671/tf_configure.bazelrc.txt) and [`.bazelrc`](https://github.com/tensorflow/tensorflow/files/3487665/bazelrc.txt)\r\n used throughout the build. I thought I pointed `configure` to the Python 3 executable and library (3.5). Did I miss something?\r\n\r\n```\r\n# .tf_configure.bazelrc\r\nbuild --action_env PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\nbuild --action_env PYTHON_LIB_PATH=\"/usr/lib/python3.5\"\r\nbuild --python_path=\"/usr/bin/python3\"\r\nbuild:xla --define with_xla_support=true\r\nbuild --config=xla\r\nbuild --action_env TF_NEED_OPENCL_SYCL=\"0\"\r\nbuild --action_env TF_NEED_ROCM=\"0\"\r\nbuild --action_env TF_NEED_CUDA=\"0\"\r\nbuild --action_env TF_DOWNLOAD_CLANG=\"0\"\r\nbuild:opt --copt=-march=native\r\nbuild:opt --copt=-Wno-sign-compare\r\nbuild:opt --host_copt=-march=native\r\nbuild:opt --define with_default_optimizations=true\r\nbuild:v2 --define=tf_api_version=2\r\ntest --flaky_test_attempts=3\r\ntest --test_size_filters=small,medium\r\ntest --test_tag_filters=-benchmark-test,-no_oss,-oss_serial\r\ntest --build_tag_filters=-benchmark-test,-no_oss\r\ntest --test_tag_filters=-gpu\r\ntest --build_tag_filters=-gpu\r\nbuild --action_env TF_CONFIGURE_IOS=\"0\"\r\n```\r\n[bazelrc.txt](https://github.com/tensorflow/tensorflow/files/3487670/bazelrc.txt)\r\n[tf_configure.bazelrc.txt](https://github.com/tensorflow/tensorflow/files/3487671/tf_configure.bazelrc.txt)\r\n", "I found the culprit. I started a new working copy, ran `configure`, and noticed it generated `tools/python_bin_path.sh`, which I was missing. Lesson is re-run `configure` after a `git pull`.\r\n```\r\n# tools/python_bin_path.sh\r\nexport PYTHON_BIN_PATH=\"/usr/bin/python3\"\r\n```\r\n\r\nThe wheel builder produced `tensorflow-1.14.0-cp35-cp35m-linux_x86_64.whl`, which indicates Python 3.5. The new wheel installs without issues, so closing this issue."]}, {"number": 31419, "title": "Memory leakage when converting to tensor", "body": "`\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nfor i in range(5000):\r\n    print(i)\r\n    array = np.random.random((1024, 1024))\r\n    tf.convert_to_tensor(array, dtype=tf.float32)\r\n`\r\n\r\nTensorflow version is 1.14.0, Numpy version is 1.17.0, python version is 3.6.8\r\nThe process is killed when i ~= 2400 on my machine\r\nThe command \"watch -d free -m\" shows that memory decreases over time until it gets close to zero, then crashes\r\n\r\nI did not find a way to free the memory from the unreferenced tensors\r\n\r\nBest,\r\nBeno\u00eet", "comments": ["Was able to reproduce the error in [google colab](https://colab.sandbox.google.com/gist/gowtham-kp/7671d9665e30bb8e90366ffd164eef40/untitled73.ipynb). ", "@benoitkoenig You are overloading the graph with several variables. In order to clear graph, I added a line to the end of your code `tf.reset_default_graph()` which makes it run for 5000 times without any issue. Please check the full code. Here is the [gist](https://colab.sandbox.google.com/gist/jvishnuvardhan/9be183857b7741d93a6246f3ce839970/tf_31419.ipynb)\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nfor i in range(5000):\r\n  print(i)\r\n  array = np.random.random((1024, 1024))\r\n  tf.convert_to_tensor(array, dtype=tf.float32)\r\n  tf.reset_default_graph()\r\n```", "@jvishnuvardhan thank you for your answer\r\nThat being said, I actually use this code inside a generator, which I call using fit_generator.\r\nIf I call reset_default_graph, then my model is lost, too. To put it simple, consider the following code:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\noutside_tensor = tf.convert_to_tensor(2)\r\nfor i in range(5000):\r\n  print(i)\r\n  array = np.random.random((1024, 1024))\r\n  tf.convert_to_tensor(array, dtype=tf.float32)\r\nprint(outside_tensor)\r\n```\r\nThe problem I am facing is equivalent to keeping the outside_tensor in the graph in order to print it at the end. In that case, reseting the whole graph won't do. Is there any way to clear one specific tensor from the graph? Here is my specific code:\r\n```\r\ndef training_generator(graph):\r\n    for (img, true_mask) in list:\r\n        with graph.as_default():\r\n            image = tf.convert_to_tensor(img, dtype=tf.float32)\r\n            true_mask = tf.convert_to_tensor(true_mask, dtype=tf.float32)\r\n        yield ([image], [true_mask])\r\n\r\ngraph = tf.compat.v1.get_default_graph()\r\nmodel = get_model()\r\nunet.compile(optimizer=Adam(lr=learning_rate), loss=calculate_loss)\r\ngen = training_generator(graph)\r\nunet.fit_generator(gen, steps_per_epoch=steps_per_epoch, epochs=epochs)\r\n```\r\nCalling tf.reset_default_graph or not using graph.as_default inside the generator both result in a \"Invalid input graph.\" error\r\n\r\nThanks,\r\nBeno\u00eet", "@benoitkoenig Sorry for the delay in my response. There were lot of improvements between TF1.14 and TF1.15. Can you please check with TF1.15.0 which was released recently and let us know how it progresses. Thanks!", "Hi!\r\n\r\nSorry for not getting back here, I'm having issues installing tensorflow 1.15 on my machine, I will get back to you as soon as this is done\r\n\r\nSo you know, the test simply consists in running the following code:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\noutside_tensor = tf.convert_to_tensor(2)\r\nfor i in range(5000):\r\n  print(i)\r\n  array = np.random.random((1024, 1024))\r\n  tf.convert_to_tensor(array, dtype=tf.float32)\r\nprint(outside_tensor)\r\n```\r\nAnd making sure no memory leakage is observed (it does with tensorflow1.14)\r\n\r\nBeno\u00eet", "@benoitkoenig Is this still an issue? I suspect there are no more updates to TF1.x unless there is any security related issues. Can you please try TF2.x and let us know how it progresses.\r\n\r\nI used recent `tf-nightly` and I cannot reproduce the issue. Please check it with `tf-nightly` and let us know whether you have any issues. Please check the [gist here](https://colab.research.google.com/gist/jvishnuvardhan/67b2107cdf865108f19f35d3858b5118/31419.ipynb). Thanks!\r\n\r\nPlease close the issue If this was already resolved for you. Thanks!", "@benoitkoenig Can you please check my last response? Thanks!", "Hi @jvishnuvardhan \r\nI updated pip (v20.0.2), ran `pip install tensorflow` which installed version 2.1.0, ran the following code:\r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\nprint('\\n\\n\\n\\n')\r\nprint(tf.__version__)\r\nprint('\\n\\n\\n\\n')\r\n\r\noutside_tensor = tf.convert_to_tensor(2)\r\nfor i in range(5000):\r\n  print(i)\r\n  array = np.random.random((1024, 1024))\r\n  tf.convert_to_tensor(array, dtype=tf.float32)\r\nprint(outside_tensor)\r\n`\r\nAnd got the error: AttributeError: module 'tensorflow' has no attribute 'compat', which is unrelated and has already been pointed out in other threads\r\n\r\nI then uninstalled tensorflow and ran `pip install tf-nightly`, got tf-nightly-2.2.0.dev20200402, ran the same code. This time it executed but ran in the same issue as mentioned in the thread, and my python script got Killed after ~1700 iterations when memory hit 0. tf.__version__ in this scenario is 2.2.0-dev20200402\r\n\r\nRegarding you gist, when I execute it, the fields \"RAM\" and \"Disk\" seem stable, however execution stops at the 57th iteration\r\n\r\nLet me know if I can help you any further,\r\nBeno\u00eet\r\n\r\n", "I suspect the `compat` error is coming from elsewhere? The snippet that you included should run fine in TF 2.\r\n\r\nIn TF 1, whenever you call convert_to_tensor, a new constant is created in the graph. These constants are permanent. Removing them them is not easy and in general you want to avoid creating too many anyway. In TF 2, this is not a problem because the execution model has radically changed and it's more intuitive. But in TF 1, you should consider using a single `tf.random.normal([1024, 1024])`, or if you need to inject external data, `tf.Placehlder` of `tf.py_function`.", "Can confirm that there isn't any memory increase with TF2 (as per Dan's last comment). Closing issue for now. Please let me know if you run into problems.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31419\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31419\">No</a>\n"]}, {"number": 31418, "title": "Cherrypick for fixing the tests timing out.", "body": "", "comments": []}, {"number": 31417, "title": "libnvinfer-dev : Depends: libnvinfer5 (= 5.0.2-1+cuda10.0) but 5.1.5-1+cuda10.1 is to be installed", "body": "can someone tell me how to change the libvnfer to the right one ?\r\nlibnvinfer-dev : Depends: libnvinfer5 (= 5.0.2-1+cuda10.0) but 5.1.5-1+cuda10.1 is to be installed\r\nE: Unable to correct problems, you have held broken packages.", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!\r\n", "i use ubuntu 16.04\r\nand the way i install it : https://www.tensorflow.org/install/gpu\r\ni follow the Ubuntu 16.04 (CUDA 10) installation", "@movie3105 Can you please elaborate the issue with context and commands or code if any. Thanks!", "@movie3105 \r\n```\r\nsudo apt-get install -y --no-install-recommends libnvinfer5=5.0.2-1+cuda10.0 libnvinfer-dev=5.0.2-1+cuda10.0\r\n```\r\n\r\nUPDATE: I think `5.0.2-1` version is depreciated and I can't find it in https://developer.download.nvidia.cn/compute/machine-learning/repos/ubuntu1604/x86_64/\r\n\r\nYou can change it to `5.1.2-1`.", "@yifeif Can you please take a look? Thanks!", "I believe this has been resolved. Thanks @dc3671!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31417\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31417\">No</a>\n", "I have the same issue, but for version6 (6.0.1.5) with CUDA 10.1 on Ubuntu18.04 x86_64:\r\nsudo dpkg -i nv-tensorrt-repo-ubuntu1804-cuda10.1-trt6.0.1.5-ga-20190913_1-1_amd64.deb\r\nsudo apt-key add /var/nv-tensorrt-repo-cuda10.1-trt6.0.1.5-ga-20190913/7fa2af80.pub\r\nsudo apt-get update\r\nsudo apt-get install tensorrt\r\n\r\nThe final line returns the following:\r\n\r\nThe following packages have unmet dependencies:\r\n tensorrt : Depends: libnvinfer6 (= 6.0.1-1+cuda10.1) but 6.0.1-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-plugin6 (= 6.0.1-1+cuda10.1) but 6.0.1-1+cuda10.2 is to be installed\r\n            Depends: libnvparsers6 (= 6.0.1-1+cuda10.1) but 6.0.1-1+cuda10.2 is to be installed\r\n            Depends: libnvonnxparsers6 (= 6.0.1-1+cuda10.1) but 6.0.1-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-bin (= 6.0.1-1+cuda10.1) but it is not going to be installed\r\n            Depends: libnvinfer-dev (= 6.0.1-1+cuda10.1) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-plugin-dev (= 6.0.1-1+cuda10.1) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvparsers-dev (= 6.0.1-1+cuda10.1) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvonnxparsers-dev (= 6.0.1-1+cuda10.1) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-samples (= 6.0.1-1+cuda10.1) but it is not going to be installed\r\n            Depends: libnvinfer-doc (= 6.0.1-1+cuda10.1) but it is not going to be installed\r\nE: Unable to correct problems, you have held broken packages.\r\n\r\nI presume it is unrelated, but, the sudo apt-get update call returns a bunch of warnings similar to the following (where my .../sources.list.d/cuda.list file contains only the text \"deb http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 /\"):\r\n\r\n\"W: Target Packages (Packages) is configured multiple times in /etc/apt/sources.list:52 and /etc/apt/sources.list.d/cuda.list:1\r\nW: Target Translations (en_US) is configured multiple times in /etc/apt/sources.list:52 and /etc/apt/sources.list.d/cuda.list:1\"\r\n\r\nEtc., etc.\r\n\r\nNote that, unlike the case above, this version (libnvinfer 6.0.1-1+cuda10.1) is still posted at the repository, so that doesn't seem to be the issue.\r\n\r\n\r\n\r\n", "Don't know why every one is getting this error \r\n```tensorrt : Depends: libnvinfer7 (= 7.0.0-1+cuda10.0) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-plugin7 (= 7.0.0-1+cuda10.0) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvparsers7 (= 7.0.0-1+cuda10.0) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvonnxparsers7 (= 7.0.0-1+cuda10.0) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-bin (= 7.0.0-1+cuda10.0) but it is not going to be installed\r\n            Depends: libnvinfer-dev (= 7.0.0-1+cuda10.0) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-plugin-dev (= 7.0.0-1+cuda10.0) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvparsers-dev (= 7.0.0-1+cuda10.0) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvonnxparsers-dev (= 7.0.0-1+cuda10.0) but 7.0.0-1+cuda10.2 is to be installed\r\n            Depends: libnvinfer-samples (= 7.0.0-1+cuda10.0) but it is not going to be installed\r\n            Depends: libnvinfer-doc (= 7.0.0-1+cuda10.0) but it is not going to be installed\r\n``` \r\nI solved it by this command \r\n`sudo apt-get install -y --no-install-recommends libnvinfer7=7.0.0-1+cuda10.0 libnvinfer-dev=7.0.0-1+cuda10.0\r\n`\r\n but got another error `/usr/bin/ld: cannot find -lnvcaffe_parser`\r\n\r\nfor your's try this one `sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 libnvinfer-dev=6.0.1-1+cuda10.1`\r\n"]}, {"number": 31416, "title": "Bazel 0.26.1", "body": "Since build VMs use Bazel 0.26.1 now, we need to cherry-pick one commit with changes to `ci_sanity.sh` and also add one extra flag to `bazel` commands.\r\n\r\nthis should make CI sanity builds pass or at least progress.", "comments": []}, {"number": 31415, "title": "tools.graph_transforms.TransformGraph has no docs or example of usage", "body": "Thank you for submitting a TensorFlow documentation issue. Per our GitHub\r\npolicy, we only address code/doc bugs, performance issues, feature requests, and\r\nbuild/installation issues on GitHub.\r\n\r\nThe TensorFlow docs are open source! To get involved, read the documentation\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs\r\n\r\n## URL(s) with the issue:\r\n\r\nthe provided wrapper function has no docs\r\n\r\n## Description of issue (what needs changing):\r\n\r\nprovide a docs such\r\nexamples of usage:\r\nTransformGraph( graph.as_graph_def(), [], [], ['remove_nodes(op=loss/init)']) ...\r\n\r\n### Clear description\r\n\r\nI wanna to use this method has a way to do specifics editions and graph redefinitions while building the model, from inside python, withou having to go to command line.\r\n\r\n### Parameters defined\r\n\r\nI think how to use the parameters are exactly the problem, the README.md from the repository gives bazel example, but it dosnt work as it should in the wrapper\r\n### Usage example\r\n\r\nthis is my first try, i could not get the desirable result (strip init op from graph):\r\n\r\n```python\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n\r\n  with tf.variable_scope('signal_in'):\r\n    signal_in = tf.placeholder(tf.float32, shape=(10,40,2,1))\r\n\r\n  with tf.variable_scope('dascope1'):\r\n    conv_linear = tf.keras.layers.Conv2D( 8, (8,2), padding='valid', name='conv_linear', use_bias=True, kernel_initializer=tf.initializers.lecun_normal(seed=137), bias_initializer=tf.initializers.lecun_normal(seed=137) )(signal_in)\r\n  \r\n  with tf.variable_scope('softmax'):\r\n    logits = tf.contrib.layers.fully_connected(conv_linear, 2, activation_fn=None, normalizer_fn=None, normalizer_params=None, weights_initializer=tf.initializers.lecun_normal(seed=731), weights_regularizer=None, biases_initializer=tf.initializers.lecun_normal(seed=777), biases_regularizer=None, reuse=None, variables_collections=None, outputs_collections=None, trainable=True, scope='logit')\r\n    softmax = tf.nn.softmax(logits,axis=0)            \r\n    \r\n  with tf.variable_scope('loss'):\r\n    l_vec = tf.placeholder(tf.float32, shape=(10,2))\r\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0)(l_vec, softmax)         \r\n    minimize_op = tf.train.AdamOptimizer(learning_rate=0.05).minimize(loss)\r\n    tf.global_variables_initializer()\r\n```\r\nthen:\r\n\r\n```python\r\ngraphdef = tf.tools.graph_transforms.TransformGraph( graph.as_graph_def(), [], [], ['remove_nodes(op=loss/init)'])\r\n\r\nwith tf.Graph().as_default() as g:  \r\n  tf.import_graph_def(graphdef,name = '')\r\n  for op in g.get_operations():\r\n    if op.name.split('/')[-1] == 'init':\r\n      print('True')\r\n``` \r\n\r\nreturns True\r\nSo how to use this wrapper ? note that init op dosnt have any input output but only dependency arrows as input.\r\n\r\n### Request visuals, if applicable\r\n\r\nAre there currently visuals? If not, will it clarify the content?\r\n\r\n### Submit a pull request?\r\n\r\nWaiting for instructions of the community about the use of this function.\r\n\r\nAre you planning to also submit a pull request to fix the issue? See the docs\r\ncontributor guide: https://www.tensorflow.org/community/contribute/docs and the\r\ndocs style guide: https://www.tensorflow.org/community/contribute/docs_style\r\n", "comments": ["Please fill the issue [template](https://github.com/keras-team/keras/issues/new?template=a--tensorflow-backend-users.md). Could you update them if they are relevant in your case, or leave them as N/A? Along with the template, please provide a standalone code to reproduce the issue. Thanks!", "@Uiuran Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "There's an extensive \"readme\" in the source directory: \r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms\r\n\r\nBut we can't add this to tensorflow.org as it is not considered an implementation detail and not part of the public API."]}, {"number": 31414, "title": "ImportError: DLL load failed: %1 is not a valid Win32 application.", "body": "<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>\r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Windows 10):\r\n- TensorFlow installed from (source or binary): Binary\r\n- TensorFlow version: 1.13.1\r\n- Python version: 3.6.8\r\n- Installed using virtualenv? pip? conda?: conda \r\n- CUDA/cuDNN version:  10.0.130/ 7.6.0\r\n- GPU model and memory:  Running on laptop to demo a conda environment build so not using GPU, 16 GB memory \r\n\r\n**Describe the problem**\r\n\r\nAfter building my conda environment using the environment.yml from a known working system, I encounter a \"ImportError: DLL load failed: %1 is not a valid Win32 application.\" issue which occurs when the script attempts to import pywrap_tensorflow (see traceback below). This is strange because both the origin computer (from which the environment was copied) and the source computer are 64-bit and tensorflow is certainly not a 32-bit application. The error seems to be coming from the imp module. Has anyone else encountered a similar issue? \r\n\r\n\r\n**Provide the exact sequence of commands / steps that you executed before running into the problem**\r\n\r\nconda env create -n <envname> -f  <Environment.yml>\r\npython <main.py>\r\n\r\n\r\n**Any other info / logs**\r\n\r\nTraceback (most recent call last):\r\n  File \"hyperparam_grid_search.py\", line 10, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Users\\mmusil\\.conda\\envs\\AutoTune5\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\r\n  File \"C:\\Users\\mmusil\\.conda\\envs\\AutoTune5\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Users\\mmusil\\.conda\\envs\\AutoTune5\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 74, in <module>\r\n    raise ImportError(msg).pywrap_tensorflow_internal import *\r\nImportError: Traceback (most recent call last):ib\\site-packages\\tens  File \"C:\\Users\\mmusil\\.conda\\envs\\AutoTune5\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *s\\tens  File \"C:\\Users\\mmusil\\.conda\\envs\\AutoTune5\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()l', fp, pathna  File \"C:\\Users\\mmusil\\.conda\\envs\\AutoTune5\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper_module\r\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\\mmusil\\.conda\\envs\\AutoTune5\\lib\\imp.py\", line 343,  File \"C:\\Users\\mmusil\\.conda\\envs\\AutoTune5\\lib\\imp.py\", line 243, in load_moduled(spec)\r\n    return load_dynamic(name, filename, file)id Win32 application.\r\n  File \"C:\\Users\\mmusil\\.conda\\envs\\AutoTune5\\lib\\imp.py\", line 343, in load_dynamic\r\n    return _load(spec)ive TensorFlow runtime.\r\nImportError: DLL load failed: %1 is not a valid Win32 application.\r\n\r\n", "comments": ["Can you try with 1.13.2 please?", "> Can you try with 1.13.2 please?\r\n\r\nI can't find a 1.13.2 version available for Conda anywhere. And installing via pip results in another series of 'DLL module load failure' errors. ", "@mmusil25 I tried on my system its working fine.\r\n`pip install tensorflow==1.13.2` . \r\nCan you provide the new error log. Thanks!", "@gadagashwini \r\n\r\nSo I did a fresh install of my environment using pip and virtualenv and encountered a similar \"DLL Module failed to load error\". I then installed tensorflow 1.13.2 and it seems to be working just fine. Thanks! "]}, {"number": 31413, "title": "//tensorflow/contrib/metrics:metric_ops_test fails with Assertion error", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04 s390x\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.15\r\n- Bazel version (if compiling from source): 0.24.1\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\r\n- CUDA/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\npython tensorflow/contrib/metrics/python/ops/metric_ops_test.py\r\n```\r\n======================================================================\r\nFAIL: testWithMultipleUpdates (__main__.AucWithConfidenceIntervalsTest)\r\ntestWithMultipleUpdates (__main__.AucWithConfidenceIntervalsTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py\", line 37, in testPartExecutor\r\n    yield\r\n  File \"/usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py\", line 162, in run\r\n    testMethod()\r\n  File \"tensorflow/contrib/metrics/python/ops/metric_ops_test.py\", line 2313, in testWithMultipleUpdates\r\n    batch_size = 50\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 1073, in decorated\r\n    return f(*args, **kwds)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 2303, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 2272, in _assertAllCloseRecursive\r\n    (path_str, path_str, msg)))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py\", line 2207, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=\"\\n\".join(msgs), equal_nan=True)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py\", line 1501, in assert_allclose\r\n    verbose=verbose, header=header, equal_nan=equal_nan)\r\n  File \"/usr/local/lib/python2.7/dist-packages/numpy/testing/_private/utils.py\", line 827, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError:\r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\nMismatched value: a is different from b.\r\nnot close lhs = 0.753345469611\r\nnot close rhs = 0.753343403339\r\nnot close dif = 2.0662712803e-06\r\nnot close tol = 1.75334340334e-06\r\ndtype = float64, shape = ()\r\nMismatch: 100%\r\nMax absolute difference: 2.06627128e-06\r\nMax relative difference: 2.74280132e-06\r\n x: array(0.753345)\r\n y: array(0.753343, dtype=float32)\r\n```\r\n\r\n\r\n\r\n**Describe the expected behavior**\r\nThe test should pass on s390x.\r\n", "comments": ["Hi,\r\nAs seen above, the test fails due to very small difference in `auc.auc.eval()`.\r\nWe have tried changing the `scale` value from `0.2` to `0.23` while calculating `noise` as below:-\r\n```diff\r\ndiff --git a/tensorflow/contrib/metrics/python/ops/metric_ops_test.py b/tensorflow/contrib/metrics/python/ops/metric_ops_test.py\r\nindex aec07241e7..3ece21da96 100644\r\n--- a/tensorflow/contrib/metrics/python/ops/metric_ops_test.py\r\n+++ b/tensorflow/contrib/metrics/python/ops/metric_ops_test.py\r\n@@ -2325,7 +2325,7 @@ class AucWithConfidenceIntervalsTest(test.TestCase):\r\n       sess.run(variables.local_variables_initializer())\r\n       for _ in xrange(num_batches):\r\n         new_labels = np.random.randint(0, 2, size=batch_size)\r\n-        noise = np.random.normal(0.0, scale=0.2, size=batch_size)\r\n+        noise = np.random.normal(0.0, scale=0.23, size=batch_size)\r\n         new_predictions = 0.4 + 0.2 * new_labels + noise\r\n         labels = np.concatenate([labels, new_labels])\r\n         predictions = np.concatenate([predictions, new_predictions])\r\n```\r\nThe test passes with the above change on s390x as well as x86.\r\nCould you please comment if this change is appropriate?", "@ymodak @martinwicke Can you please comment on this?", "This seems fine to me, although this \"fix\" passes only accidentally, I think.\r\n\r\nI will note that the implied imprecision on BE is troubling, do we understand why this is? ", "My concern is that something on s390x is lowering expectations for\nprecision all over the place. Do we understand why that is? I'd rather\navoid increasing test tolerances. I'd also like to avoid fixes like the one\nyou propose, which make this test extremely brittle (I suppose using\nscale=0.23 just hit a particularly lucky combination of numbers which\ndoesn't every hit the worst case).\n\nYou can send a PR with your proposal -- this is a fairly harmless fix. But\nI will warn that this might break again at any time, and I'd feel much more\ncomfortable if we understood why numerics on s390x behave differently.\n", "@martinwicke Thanks for the reply.\r\nWe are still debugging on the issue of why exactly the numerics on s390x behave differently and also we are checking if its an issue with NumPy (similar to [12963](https://github.com/tensorflow/tensorflow/pull/12963) and [14017](https://github.com/tensorflow/tensorflow/issues/14017)).\r\n\r\nCould you please help us out, where exactly the value of `auc` gets calculated and how?\r\n\r\n", "This seems to pertain to contrib/metrics, which are deprecated. The\ncomputation you are interested in probably happens here:\nhttps://www.tensorflow.org/api_docs/python/tf/contrib/metrics/streaming_auc\n", "@martinwicke Thanks for the direction.\r\nAfter debugging further, we were able to figure out that, for this particular test, the `auc` value gets calculated using placement values [here](https://github.com/tensorflow/tensorflow/blob/87989f69597d6b2d60de8f112e1e3cea23be7298/tensorflow/contrib/metrics/python/ops/metric_ops.py#L1230).\r\n\r\nSo, We tried printing the values of `auc`, `lower` and `upper` using below:-\r\n```\r\n(Pdb) l\r\n1340          math_ops.reduce_sum(weights_0 * (1. - placement_values_0)) /\r\n1341          (total_0 + _EPSILON))\r\n1342      auc_1 = (\r\n1343          math_ops.reduce_sum(weights_1 * (placement_values_1)) /\r\n1344          (total_1 + _EPSILON))\r\n1345 ->   auc = array_ops.where(math_ops.less(total_0, total_1), auc_1, auc_0)\r\n1346\r\n1347      # Calculate variance and standard error using the placement values.\r\n1348      var_0 = (\r\n1349          math_ops.reduce_sum(\r\n1350              weights_0 * math_ops.square(1. - placement_values_0 - auc_0)) /\r\n(Pdb) from tensorflow.python.ops import variables\r\n(Pdb) sess.run(variables.local_variables_initializer())\r\n(Pdb) sess.run(auc)\r\n```\r\n\r\nHowever, I encountered the following error:-\r\n```\r\n*** FailedPreconditionError: Attempting to use uninitialized value Variable\r\n         [[node Variable/read (defined at /usr/local/lib/python2.7/dist-packages/absl/third_party/unittest3_backport/case.py:162) ]]\r\n```\r\n\r\nIn fact, this error is displayed, whenever we try to display the values of other tensors( such as `placement_values_0` ,`placement_values_1`, `lower`, `upper`) too.\r\n\r\nCould you please help me in finding out what exact values of these tensors are, as we might be able to find the exact reason why the values are getting computed differently.", "@martinwicke Any Insights on the above issue? \r\nI have even tried initializing these values using `initialized_value()`, but still I encounter the same issue. ", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31413\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31413\">No</a>\n", "Closing this issue since, contrib folder is removed from master.", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31413\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31413\">No</a>\n"]}, {"number": 31411, "title": "Resolve output shape issue with conv dilation by moving op creation i\u2026", "body": "\u2026nto layer.call.\r\n\r\nPiperOrigin-RevId: 254997784\r\n\r\nCherry-picking to r1.14", "comments": ["@robieta I recall you had a CL that dealt with the performance issues caused by this change. Should that be merged in as well?", "Indeed, moving the op creation from build to call causes a performance hit from recreating the class each time. If we instead cherrypick https://github.com/tensorflow/tensorflow/commit/0ccb3675d69cf1c277a59c17ef59ab3ba9b1a36c then we can revert this commit and still have proper shape inference.", "Will send a new PR in a few minutes"]}, {"number": 31410, "title": "MirrorStrategy() fills up memory of GPU that is not selected for training in TF 2.0.", "body": "I am using `tf.distribute.MirroredStrategy()` in order to train a `tf.keras` model. I have 2 T4 GPUs from Google available. I want  to train my model consisting LSTM layer on only one GPU i.e. `/gpu:1`. So, in order to select it, I define `devices = [\"/gpu:1\"]` inside `MirrorStrategy()` as suggested [here](https://www.tensorflow.org/guide/distribute_strategy#mirroredstrategy), now when I run the file for training, the memory of first GPU(/gpu:0) is also filling up completely however, during training only GPU (/gpu:1) is being utilized completely. My question is why is this happening since I only want to utilize GPU (/gpu:1)? In this case, first GPU is useless.\r\nHowever, if I select the first GPU(/gpu:0) for training then it uses only 112MB of memory of the second GPU(/gpu:1).\r\n```\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\r\n| N/A   60C    P0    28W /  70W |  14449MiB / 15079MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |\r\n| N/A   75C    P0    74W /  70W |  14517MiB / 15079MiB |     95%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     20576      C   python                                     14439MiB |\r\n|    1     20576      C   python                                     14507MiB |\r\n+-----------------------------------------------------------------------------+", "comments": ["@rishabhsahrawat are you executing any other op before you declare the distributed strategy ?\r\nis this still happening if you declare distribute strategy before anything else?\r\nIt would be easier to comment on this if you could provide a reproducible code snippet! ", "Hi @srihari-humbarwadi , thank you for your questions. No, I am not declaring any op as far as I know. I am only creating dataset for training and testing. \r\nYes, it is still happening even if I define distribute strategy right after defining modules at the top of the file.\r\nIf you use the code from this [tutorial](https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches) after defining the model inside the `MirrorStrategy()` like \r\n````\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices = ['/gpu:1'])# for using second GPU\r\n\r\nwith mirrored_strategy.scope():\r\n  model = tf.keras.Sequential()\r\n  model.add(tf.keras.layers.Embedding(17179, 64))\r\n  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation= 'tanh', recurrent_activation= 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias= True)))\r\n  # One or more dense layers.\r\n  # Edit the list in the `for` line to experiment with layer sizes.\r\n  for units in [64, 64]:\r\n    model.add(tf.keras.layers.Dense(units, activation='relu'))\r\n\r\n  # Output layer. The first argument is the number of labels.\r\n  model.add(tf.keras.layers.Dense(3, activation='softmax'))\r\n  model.compile(optimizer='adam',\r\n                loss='sparse_categorical_crossentropy',\r\n                metrics=['accuracy'])\r\n````\r\n, you will be able to see this behaviour. You can do it on your own just to be sure otherwise, here is the full script.\r\n````\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport os\r\nDIRECTORY_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\r\nFILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\r\n\r\nfor name in FILE_NAMES:\r\n  text_dir = tf.keras.utils.get_file(name, origin=DIRECTORY_URL+name)\r\n  \r\nparent_dir = os.path.dirname(text_dir)\r\n\r\nprint parent_dir\r\ndef labeler(example, index):\r\n  return example, tf.cast(index, tf.int64)  \r\n\r\nlabeled_data_sets = []\r\n\r\nfor i, file_name in enumerate(FILE_NAMES):\r\n  lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))\r\n  labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))\r\n  labeled_data_sets.append(labeled_dataset)\r\n\r\nBUFFER_SIZE = 50000\r\nBATCH_SIZE = 64\r\nTAKE_SIZE = 5000\r\nall_labeled_data = labeled_data_sets[0]\r\nfor labeled_dataset in labeled_data_sets[1:]:\r\n  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\r\n  \r\nall_labeled_data = all_labeled_data.shuffle(\r\n    BUFFER_SIZE, reshuffle_each_iteration=False)\r\n\r\nfor ex in all_labeled_data.take(1):\r\n  print(ex)\r\n\r\n\r\ntokenizer = tfds.features.text.Tokenizer()\r\n\r\nvocabulary_set = set()\r\nfor text_tensor, _ in all_labeled_data:\r\n  some_tokens = tokenizer.tokenize(text_tensor.numpy())\r\n  vocabulary_set.update(some_tokens)\r\n\r\nvocab_size = len(vocabulary_set)\r\nprint 'coab size: ',vocab_size\r\n\r\nencoder = tfds.features.text.TokenTextEncoder(vocabulary_set)\r\nexample_text = next(iter(all_labeled_data))[0].numpy()\r\nprint(example_text)\r\nencoded_example = encoder.encode(example_text)\r\nprint(encoded_example)\r\n\r\ndef encode(text_tensor, label):\r\n  encoded_text = encoder.encode(text_tensor.numpy())\r\n  #encoded_text = encoded_text[:10]\r\n  return encoded_text, label\r\n\r\ndef encode_map_fn(text, label):\r\n  return tf.py_function(encode, inp=[text, label], Tout=(tf.int64, tf.int64))\r\n\r\nall_encoded_data = all_labeled_data.map(encode_map_fn)\r\n\r\ntrain_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\r\ntrain_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]))\r\n#train_data= train_data.batch(64)\r\ntest_data = all_encoded_data.take(TAKE_SIZE)\r\n\r\ntest_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=([-1],[]), drop_remainder = False)\r\n#test_data = test_data.batch(64)\r\nsample_text, sample_labels = next(iter(test_data))\r\nprint '**Test Samples**'\r\nprint sample_text[0], sample_labels[0]\r\n\r\nvocab_size += 1\r\n###Distributed Strategy###\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices = ['/gpu:1'])\r\nwith mirrored_strategy.scope():\r\n  model = tf.keras.Sequential()\r\n  model.add(tf.keras.layers.Embedding(vocab_size, 64))\r\n  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation= 'tanh', recurrent_activation= 'sigmoid', recurrent_dropout = 0, unroll = False, use_bias= True)))\r\n  # One or more dense layers.\r\n  # Edit the list in the `for` line to experiment with layer sizes.\r\n  for units in [64, 64]:\r\n    model.add(tf.keras.layers.Dense(units, activation='relu'))\r\n\r\n  # Output layer. The first argument is the number of labels.\r\n  model.add(tf.keras.layers.Dense(3, activation='softmax'))\r\n  model.compile(optimizer='adam',\r\n                loss='sparse_categorical_crossentropy',\r\n                metrics=['accuracy'])\r\n###Strategy  end###\r\nsteps = 5000/64\r\n\r\nprint steps\r\n\r\nmodel.fit(train_data, epochs=10, validation_steps = steps, validation_data=test_data)\r\n````\r\nThank you,\r\nRishabh Sahrawat", "The distribute strategy declaration should be done before you use any tf operations (like tf.data, tf.keras etc). Please post what happens when you declare this at the start of your code.\r\n```\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices = ['/gpu:1'])\r\n```", "I already mentioned it in my last comment. But another try is here.\r\n\r\n> Yes, it is still happening even if I define distribute strategy right after defining modules at the top of the file.\r\n\r\n````\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport os\r\nmirrored_strategy = tf.distribute.MirroredStrategy(devices = ['/gpu:1'])\r\n--rest is same like script in last comment (of course after removing the above line from it)--\r\n````\r\nThe result is still the same, it is using both GPUs for memory but utilising second.\r\n````\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |\r\n| N/A   44C    P0    26W /  70W |  14449MiB / 15079MiB |      2%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |\r\n| N/A   43C    P0    31W /  70W |  14827MiB / 15079MiB |     40%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0     16200      C   python                                     14439MiB |\r\n|    1     16200      C   python                                     14817MiB |\r\n+-----------------------------------------------------------------------------+\r\n````", "Just add the tf.data ops under the distribute strategy scope", "Unfortunately, it still has the same behaviour. \r\n\r\nI tried this exact code, on Google Colab with GPU and the memory it takes there is only 661MiB but when I run it on the two GPUs from Google, it fills up the whole memory. My TF versions are same Beta1. I also tried on Latest nightly build. I request if someone can use this exact code for running on two GPUs and see the memory usage.", "I am surprised that setting the scope:gpu1 at the very top doesn't work as well.\r\n\r\nCould you try running with the environmental variable CUDA_VISIBLE_DEVICES=1 (see more info http://stackoverflow.com/q/37893755), please?", "Hi @isaprykin , thank you for your suggestion. I tried it and can confirm this works however one must not define `devices` parameter of the strategy otherwise it will throw error. I think this makes this parameter useless since the way to choose a GPU doesn't depend on `devices` parameter. Also I would request you to add `os.environ[\"CUDA_VISIBLE_DEVICES\"]` this in the documentation.", "I am facing a weird situation. When I train the same model on one GPU, it takes 45 secs to process 100 batches during first epoch, but when I use both GPUs for training, it takes 61 secs for the same however it should train faster on more number fo GPUs. Do you guys think there is a reason for that? @isaprykin @jvishnuvardhan @oanush ", "Are both GPUs the same?\r\n\r\nProfiling tutorial is here: https://www.tensorflow.org/tensorboard/r2/tensorboard_profiling_keras", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31410\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31410\">No</a>\n", "Yes, both GPUs are same Tesla T4s from Google. I also tried using 2 K80s but it throws OOM errors then. \r\nI already tried using `tf.data.prefetch()` but it doesn't show any big improvement, with the use of more number fo GPUs it gets slower in training."]}, {"number": 31409, "title": "More flexible fftshift and ifftshift", "body": "This fixes the issue raised at the end of the thread in issue #26989 .\r\nThat is, if we want to have `fftshift` and `ifftshift` accept None dimensions. This happens when building a keras model (sequential or functionnal API), using `Lambda` layers (i.e. `Lambda(fftshift, arguments={'axes': [1, 2]})(inputs)`, where `inputs` has shape `(None, None, None, 1)`).\r\n\r\nI don't know if this requires unit testing.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31409) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31409) for more info**.\n\n<!-- ok -->", "Can one of the admins verify this patch?", "@rryan Sorry about that last push, I had to correct the linting (4 vs 2 spaces haha). I ran the linting according to https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md and reran the unit tests with bazel (all passing).\r\n\r\nI guess you just need to re-approve to relaunch the CI.", "RE the failing CI, I don't really think it's mine to blame. Does it need to be force re-run?\r\nAnd I can't see the logs for Ubuntu Sanity (404)."]}, {"number": 31408, "title": "Value of sqrt(2) is calculated incorrectly and inconsistently", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.1 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): not known\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source): not known\r\n- CUDA/cuDNN version: 10.0/7.6.0\r\n- GPU model and memory: Tesla K-80 12gb\r\n\r\n**Describe the current behavior**\r\nI just computed sqrt(2) 100 times in google colab. The values are computed inconsistently, and a bit incorrectly, in a single tensor.\r\n\r\n**Describe the expected behavior**\r\nsqrt(2) should be computed the same way every time.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\n\r\nsqrt2s = tf.sqrt(tf.constant([2.0]*100, dtype=tf.float32))\r\nprint(sqrt2s)\r\nprint(sqrt2s[0].numpy())\r\nprint(sqrt2s[-1].numpy())\r\nprint(sqrt2s[0].numpy() == sqrt2s[-1].numpy())\r\n```\r\nThe results:\r\n```\r\ntf.Tensor(\r\n[1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134\r\n 1.4142134 1.4142134 1.4142134 1.4142134 1.4142134 1.4142135 1.4142135\r\n 1.4142135 1.4142135], shape=(100,), dtype=float32)\r\n1.4142134\r\n1.4142135\r\nFalse\r\n```\r\n**Other info / logs**\r\nThe notebook is here:\r\nhttps://colab.research.google.com/drive/1PQp2KtFUoaUEHETrIUne-J1Cu2blg2cG", "comments": ["Also confirmed on TF2.0-beta.", "@rmlarsen is this the scalar vs vectorized path difference we saw in the other bug?", "I tested in GPU, there is no such problem.\r\nHowever, this problem occurs when I use CPU (9900K).\r\n\r\nI switch off GPU using `export CUDA_VISIBLE_DEVICES=''`\r\n\r\nSystem information\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 LTS\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\nTensorFlow installed from (source or binary): pip3 install tensorflow-gpu==1.14\r\nTensorFlow version (use command below): 1.14.0\r\nPython version: 3.6.8\r\nBazel version (if compiling from source):\r\nGCC/Compiler version (if compiling from source): not known\r\nCUDA/cuDNN version: 9.0/7.6.1\r\nGPU model and memory: RTX 2080Ti\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "If we use `dtype=tf.float16`, there is no such problem.", "This issue is still replicating with Tf2.1 and tf-nightly==2.2.0.dev20200313.\r\nPlease find the gist [here](https://colab.sandbox.google.com/gist/gadagashwini/a0f428a577715b5da0277410c610da51/untitled455.ipynb). Thanks", " Both numbers are correct to within 1 ulp (unit in last place). The inconsistency is due to the difference between the vectorized and scalar path in the underlying Eigen code. In short, please don't rely on exact equality of floating point numbers.\r\n\r\nClosing this issue as it was the intended behavior. Please feel free to reopen the issue if you still have a concern. Thanks!", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31408\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31408\">No</a>\n", "@saikumarchalla Thanks. \r\nA similar issue is #31619. It seems running the above code using GPU has no such issues. \r\n\r\nWhat the meaning of \"the difference between the vectorized and scalar path in the underlying Eigen code\"? Is there any reference? I want to learn more about this. Thanks. "]}, {"number": 31407, "title": "What is the correct  way to compile a 'so' file without \"undefined reference to...\" error?", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  5.2.5-arch1-1-ARCH\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): 0.27.0- (@non-git)\r\n- GCC/Compiler version (if compiling from source): 9.1.0\r\n- CUDA/cuDNN version: cuda: 10.1 ,cudnn: 7.6\r\n- GPU model and memory: 16G\r\n\r\nI'm using the **tfcompile** to convert my trained model into object files. **Everything works fine if I compile it as** `cc_binary` with **Bazel**.\r\n**As I must wrap all the tf-relative codes into a single part**, I compiled the converted .o files with `cc_library` and got a so file indeed.But when it was linked as a third lib in non-bazel env it issued like this:\r\n```\r\n/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `xla::ExecutableRunOptions::set_intra_op_thread_pool(Eigen::ThreadPoolDevice const*)'\r\n/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `__xla_cpu_runtime_EigenConvF32'\r\n/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `tensorflow::XlaCompiledCpuFunction::~XlaCompiledCpuFunction()'\r\n/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `__xla_cpu_runtime_EigenMatMulF32'\r\n/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `typeinfo for tensorflow::XlaCompiledCpuFunction'\r\n/usr/bin/ld: ./libcaptchaLib.so: undefined reference to `tensorflow::XlaCompiledCpuFunction::Run()'\r\n```\r\nI also tried to **add these missing objs into a static lib and passed it to the linker hand by hand.\r\nIt works but really horrible because of the linkage chain**.\r\nIs there a novel way to this case?\r\nhere is my **BUID** file:\r\n```\r\ncc_library(\r\n    name = \"captchaLib\",\r\n    srcs = [\r\n    \t \"captcha_run.cc\",\r\n\t \"captcha.h\",\r\n\t \"captcha_model.o\",\r\n\t \"captcha_helper.o\",\r\n\t ],\r\n    copts=[\"-fPIC\",\"-std=c++11\",\"-O3\"],\r\n    deps = [\r\n       \"//tensorflow/compiler/xla:shape_util\",\r\n         \"//tensorflow/compiler/tf2xla:xla_compiled_cpu_function\",\r\n         \"//tensorflow/core:framework_lite\",\r\n        \"//third_party/eigen3\",\r\n       \"@com_google_absl//absl/strings\",\r\n      ] + [\r\n            \"//tensorflow/compiler/xla/service/cpu:runtime_conv2d\",\r\n            \"//tensorflow/compiler/xla/service/cpu:runtime_key_value_sort\",\r\n            \"//tensorflow/compiler/xla/service/cpu:runtime_matmul\",\r\n            \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_conv2d\",\r\n            \"//tensorflow/compiler/xla/service/cpu:runtime_single_threaded_matmul\",\r\n\t    ],\r\n    linkstatic=False,\r\n)\r\n```", "comments": ["This is not really a TensorFlow issue, more like a Bazel one and the best way to ask would be on StackOverflow or Bazel repo.\r\n\r\nThe undefined reference comes from an object not being present at the moment you get the error (link time or run time)."]}, {"number": 31406, "title": "Using another model despite of provided in android sample", "body": "@ry @jmhodges \r\n\r\nI have replaced the tensorflow lite (Detect.tflite) model in the android sample.\r\n\r\nI ended up with several errors on building the project.\r\n\r\nOne of them is \r\n\r\n` java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 1080000 bytes and a ByteBuffer with 270000 bytes.`\r\n\r\nAfter some try and error I ended up with another error mentioned below\r\n\r\n `java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [1, 1917, 4] and a Java object with shape [1, 3].`\r\n\r\nPlease look it into asap so can move forward with the sample.\r\n\r\n\r\n\r\n", "comments": ["Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?\r\n\r\nMake sure you also include the code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue [template](https://github.com/tensorflow/tensorflow/issues/new/choose).\r\n\r\nWe ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!", "@ravikyram \r\n\r\nI have tried running the Demo on my samsung A50.\r\n\r\nSimply I have replaced the detect.tfite with my .tflite file and label file with my file, and ended up with above errors.\r\n\r\nPlease assist.\r\n ", "> Simply I have replaced the detect.tfite with my .tflite file and label file with my file\r\n\r\nYou can't simply drop in an arbitrary model and expect the sample to function without any modifications. In particular, depending on your model, the input/output tensors will differ, and you'll need to adjust your usage of the TFLite Interpreter accordingly. Without knowing any additional details about your model, it's impossible to say what is going wrong here, but it's pretty clear that your new model has fundamentally different input shapes and output types than what are used in the sample. \r\n\r\nHave a look at [this guide](https://www.tensorflow.org/lite/guide/faq#how_do_i_inspect_a_tflite_file) for how to inspect your graph to determine the format of inputs/outputs you should provide."]}, {"number": 31405, "title": "Edited comments for test script.", "body": "Fixed grammar to me same over comments.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here with `@googlebot I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the Google project maintainer to [go/cla#troubleshoot](http://go/cla#troubleshoot) ([Public version](https://opensource.google.com/docs/cla/#troubleshoot)).\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31405) for more info**.\n\n<!-- need_sender_cla -->", "@googlebot I signed it!", "CLAs look good, thanks!\n\n\u2139\ufe0f **Googlers: [Go here](https://goto.google.com/prinfo/https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensorflow%2Fpull%2F31405) for more info**.\n\n<!-- ok -->"]}, {"number": 31404, "title": "Does the forward propagation of tensorflow code really use GPUs?", "body": "https://stackoverflow.com/questions/57372076/does-the-forward-propagation-of-tensorflow-code-really-use-gpus", "comments": ["@ZesenChen ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Thanks!", "> @ZesenChen ,\r\n> Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.Thanks!\r\n\r\nThe tensorflow version is 1.12 with python=3.5, the GPU is K20, but I don't know the operating system because I run it on the ML Platform of my company. \r\nI think the reason for this may be that the time spent on slicing and merging data is longer than that spent on forward propagation for one batch. But I am not sure.", "@ZesenChen ,\r\nWhen tried executing the code given, error `NameError: name 'timesteps' is not define` was faced.Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "Are you satisfied with the resolution of your issue?\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31404\">Yes</a>\n<a href=\"https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31404\">No</a>\n"]}, {"number": 31403, "title": "Upgrade python version in official tensorflow docker image", "body": "Using docker image from tensorflow official website\r\n[https://www.tensorflow.org/install#run-a-tensorflow-container](url) \r\nInstalls python2.7 default, which is planned to be deprecated near in future.\r\n\r\n> DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\r\n\r\nThis issue is  request to provide updated python version with docker image.", "comments": ["@rabinandan \r\nPlease, go through the below link and see if it helps you.Thanks!\r\nhttps://github.com/tensorflow/tensorflow/issues/30057\r\n\r\n", "Yes, It helped me. \r\nThank You. ", "Are you satisfied with the resolution of your issue?<br> [Yes](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31403)<br> [No](https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31403)\r\n"]}, {"number": 31402, "title": "Support for Unix/Linux netpbm (.pgm/.ppm) image formats", "body": "As we are dealing with very large problems with millions of images, cropping small images\r\nfrom hundreds of thousands of large images and then transforming the ROIs to .jpg or .png for tensorflow only, is a real resource problem. See .pdf of article on: arXiv:1904.08421\r\nThis is especially true because this is a dynamic environment with new large images coming in all the time such that any form of prepackaging data sets becomes a stumbling block in resource space. It would make a tremendous difference if TF were able to handle raw Unix/Linux NetPBM images directly (binary: .pbm, grey: .pgm, color: .ppm). See: https://sourceforge.net/projects/netpbm/  (600 package downloads per month).\r\nOn the web, proposals are made to handle NetPBM conversion in Python, but that does not appear to be the most efficient path. With our problem scale, the communication, I/O and conversion overhead is much more noticeable than in the well-known closed-data set TF examples. ", "comments": ["While this would be a good addition, I think this would have to come from the community via a chain of PRs.", "OK, I see. C++ versions for I/O in this format exist: (https://people.sc.fsu.edu/~jburkardt/cpp_src/pbmb_io/pbmb_io.html)\r\nThe three formats are very simple, that is why they won't disappear. It should be doable\r\nbut I don't know how complex the I/O section in TF has been organized: C++ gives humans\r\na lot of tempting space to generate complexity :).\r\nI'll look in my group for volunteers to get this started.\r\nPS our peak load was 1.3 billion files on a single fs mount point.", "I might get some cycles next year if there's no takers by then", "Dear Mihai,\n\nwhat do you mean with 'cycles'? (I am not a GitHub expert user)\nI suppose it is positive for the suggestion (?)\n\nBest regards!\nLambert\n________________________________\nFrom: Mihai Maruseac <notifications@github.com>\nSent: Thursday, August 8, 2019 4:37 PM\nTo: tensorflow/tensorflow <tensorflow@noreply.github.com>\nCc: GrHound <lrb_schomaker@hotmail.com>; Author <author@noreply.github.com>\nSubject: Re: [tensorflow/tensorflow] Support for Unix/Linux netpbm (.pgm/.ppm) image formats (#31402)\n\n\nI might get some cycles next year if there's no takers by then\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://github.com/tensorflow/tensorflow/issues/31402?email_source=notifications&email_token=AISWNU5OGAZXKVT2TYH6KATQDQVTDA5CNFSM4IJ55ILKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD33ZUAA#issuecomment-519543296>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AISWNU355FY2LDJWQA2U3SDQDQVTDANCNFSM4IJ55ILA>.\n", "This should be in the [tensorflow/io](http://github.com/tensorflow/io/) repository ", "@GrHound I added a PR tensorflow/io#599 in  [tensorflow/io](http://github.com/tensorflow/io/)  repo for PBM(PGM/PPM) support. You can take a look if you are interested."]}, {"number": 31401, "title": "How can I use the tensorflow.python.profiler.model_analyzer under the estimator framework", "body": "I use the estiamtor framework and I want to run profiler within it.\r\nfor the feature of estimator, I just write the profiler code like following:\r\n```\r\nwith tf.contrib.tfprof.ProfileContext(<my config>) as pctx:\r\n    pctx.add_auto_profiling(<my profiler>)\r\n    estimator.train(<my params>)\r\n```\r\nLuckly, It's work! I can get the profiler and show it on the profiler ui [(profiler ui)](https://github.com/tensorflow/profiler-ui)\r\nThen, I want to use the model_analyzer just like following(just add after the train ):\r\n```\r\nfrom tensorflow.python.profiler import model_analyzer\r\nmodel_analyzer(options=model_analyzer.ALL_ADVICE)\r\n```\r\nhere is a tip that: No RunMeta.\r\nI know in the normal pipline the runmeta will produce by the profiler, but under estimator, do not need the runmeta.\r\nI would be very grateful for any reply.\r\n", "comments": ["@SakuraGit ,\r\nPlease provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version.\r\nIn order to expedite the trouble-shooting process, please provide complete code snippet to reproduce the issue reported here. Thanks", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!"]}]