[{"number": 12765, "title": "Three questions about ChiefSessionCreator?", "body": "* There are two parameters of [ChiefSessionCreator][1]: `checkpoint_dir` and `checkpoint_filename_with_path`,\r\nwhat difference between them?\r\n\r\n* And ChiefSessionCreator has a parameter [scaffold][2] which has a parameter `saver`, does it mean ChiefSessionCreator uses scaffold.saver to save and restore variables?\r\n\r\n* And scaffold also has a parameter `init_fn`, and it can be used like \r\ninit_fn=[assign_from_checkpoint_fn][3], assign_from_checkpoint_fn will new a saver with the parameter `var_list` of assign_from_checkpoint_fn. So here comes another question, ChiefSessionCreator will use scaffold.saver to restore variables or init_fn?\r\n\r\nTo answer the three questions above, you may need read the source code, but I am not capable to do it, so I need your help. Thanks.\r\n\r\n  [1]: https://www.tensorflow.org/versions/master/api_docs/python/tf/train/ChiefSessionCreator\r\n  [2]: https://www.tensorflow.org/versions/master/api_docs/python/tf/train/Scaffold\r\n  [3]: https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/assign_from_checkpoint_fn", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "It has been asked on stackoverflow, but no response."]}, {"number": 12764, "title": "the side &deep model   is not good compare with deep model and wide model ", "body": "these days ,I'm learning the wide & deep model ,and run the  wide_n_deep_tutorial.py, so the anwser looks like this:\r\n\r\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=deep\r\nTraining data is downloaded to /tmp/tmpFB4dsd\r\nTest data is downloaded to /tmp/tmpomj5Pi\r\n2017-09-02 20:03:15.713609: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\r\n your machine and could speed up CPU computations.\r\n2017-09-02 20:03:15.713884: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\r\n your machine and could speed up CPU computations.\r\n2017-09-02 20:03:15.714050: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\r\nur machine and could speed up CPU computations.\r\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\r\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\r\nmodel directory = /tmp/tmpWei9wK\r\naccuracy: 0.850071\r\naccuracy_baseline: 0.763774\r\nauc: 0.894038\r\nauc_precision_recall: 0.743199\r\naverage_loss: 0.393638\r\nglobal_step: 2000\r\nlabel/mean: 0.236226\r\nloss: 39.3179\r\nprediction/mean: 0.242167\r\n\r\n\r\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py  --model_type=wide\r\nTraining data is downloaded to /tmp/tmpFJdWft\r\nTest data is downloaded to /tmp/tmpjB5nm7\r\n2017-09-02 20:01:09.197612: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\r\n your machine and could speed up CPU computations.\r\n2017-09-02 20:01:09.197906: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\r\n your machine and could speed up CPU computations.\r\n2017-09-02 20:01:09.198072: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\r\nur machine and could speed up CPU computations.\r\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\r\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\r\nmodel directory = /tmp/tmpuPHsDx\r\naccuracy: 0.835391\r\naccuracy_baseline: 0.763774\r\nauc: 0.882763\r\nauc_precision_recall: 0.694257\r\naverage_loss: 0.352975\r\nglobal_step: 2000\r\nlabel/mean: 0.236226\r\nloss: 35.2563\r\nprediction/mean: 0.240918\r\n\r\n\r\nXXT@apptruexxnet:~$ python wide_n_deep_tutorial.py\r\nTraining data is downloaded to /tmp/tmpDdWc_T\r\nTest data is downloaded to /tmp/tmpFF0PZJ\r\n2017-09-02 20:00:08.334742: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on\r\n your machine and could speed up CPU computations.\r\n2017-09-02 20:00:08.335105: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on\r\n your machine and could speed up CPU computations.\r\n2017-09-02 20:00:08.335273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on yo\r\nur machine and could speed up CPU computations.\r\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\r\nWARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\r\nmodel directory = /tmp/tmpq2M7SE\r\naccuracy: 0.820834\r\naccuracy_baseline: 0.763774\r\nauc: 0.850518\r\nauc_precision_recall: 0.676198\r\naverage_loss: 0.424271\r\nglobal_step: 2000\r\nlabel/mean: 0.236226\r\nloss: 42.3776\r\nprediction/mean: 0.256489\r\n\r\nI don't know why looks likes this, so someone can help me? thank you.", "comments": ["This is not a bug?", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!", "alright,thanks."]}, {"number": 12763, "title": "fix typo", "body": "fix the meaning of num_units in BasicRnncell", "comments": ["Can one of the admins verify this patch?", "@meettyj, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @drpngx and @martinwicke to be potential reviewers.", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->", "I signed it", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 12762, "title": "Removing extra space", "body": "", "comments": ["@printdhruv, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @wangqr and @yongtang to be potential reviewers.", "Can one of the admins verify this patch?", "Yifei, is that space intentional?", "Thanks for the PR @printdhruv. The space here is intentional. I will close this PR for now."]}, {"number": 12760, "title": "typo in config.py (line 688)", "body": "Currently is:\r\n\r\n```\r\ncudnn_path_from_ldconfig = re.search('.*libcudnn.so .* => (.*)',\r\n```\r\n\r\nshould be:\r\n\r\n```\r\ncudnn_path_from_ldconfig = re.search('.*libcudnn.so.* => (.*)',\r\n```\r\n\r\n(note the extra space in the current version)\r\n", "comments": ["Opened issue at [Request](https://github.com/tensorflow/tensorflow/pull/12762)", "Thanks! Marking contributions welcome for now pending the PR.", "The space here is intentional. \r\nIt is supposed to get the link without the cudnn version number:\r\n\t`libcudnn.so (libc6,x86-64) => /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudnn.so`\r\n@DeliciousHair was it causing any issue for you?", "It was, the config script kept erroring on me when building from master until I removed that space (hence mentioning it). Wouldn't you want/need the version number though, since the script does in fact ask about it as well?", "Do you mind posting the error you are seeing? \r\nThe script follows the regex match with this check:\r\n`if os.path.exists('%s.%s' % (cudnn_path_from_ldconfig, tf_cudnn_version)):`, which is why it was looking for the link for libcudnn.so instead of libcudnn.so.<tf_cudnn_version>.", "happy to, but will not be around the computer for about 16 hours.", "Closing this, as I cannot seem to reproduce the original problem, that really did require me to remove the space to fix.\r\n\r\nMy only guess is that I have rebooted the machine between then and now to get the cuda drivers to actually work as opposed to being just installed, and that seems to have fixed the problem somehow as well.\r\n\r\nSorry about that!"]}, {"number": 12759, "title": "Fix issue of `name` used in `build_default_serving_input_fn`", "body": "This fix tries to address the issue raised in #12755 where the name used in `build_default_serving_input_fn` is not a valid variable scope:\r\n```\r\n$ python -c \"import tensorflow as tf\r\nf = {'feature': tf.placeholder(name='feature', shape=[32], dtype=tf.float32) }\r\nserving_input = tf.contrib.learn.utils.input_fn_utils.build_default_serving_input_fn(f)\r\nserving_input()\"\r\n...\r\nValueError: 'feature:0' is not a valid scope name\r\n```\r\n\r\nIn `build_default_serving_input_fn`, the name of the tensor is used directly which consists of the op name and the value index:\r\n```\r\nclass Tensor(_TensorLike):\r\n....\r\n  @property\r\n  def name(self):\r\n    \"\"\"The string name of this tensor.\"\"\"\r\n    if not self._op.name:\r\n      raise ValueError(\"Operation was not named: %s\" % self._op)\r\n    return \"%s:%d\" % (self._op.name, self._value_index)\r\n```\r\n\r\nThis fix fixes this issue by using the name of the tensor's op (not tensor): `t.op.name` instead, to avoid the \":0\" (value index) at the end.\r\n\r\nThis fix fixes #12755.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["@yongtang, thanks for your PR! By analyzing the history of the files in this pull request, we identified @davidsoergel and @tensorflower-gardener to be potential reviewers.", "Can one of the admins verify this patch?", "Jenkins, test this please.", "Probably should add a simple unit test for build_default_serving_input_fn that mimics what I did to repro this?  ", "Yes, such a test would be great.", "Thanks @slacy @martinwicke . The PR has been updated with test cases added. Please take a look.", "Updated the PR to cover `tf.estimator.export.build_raw_serving_input_receiver_fn` as well.", "@tensorflow-jenkins test this please", "@davidsoergel, @martinwicke does the change look good to you?", "Jenkins, test this please. (master got stuck)\r\n\r\n@davidsoergel @martinwicke any luck with this?", "@martinwicke ping", "In one of my legacy project using google **seq2seq**, I use **Tensorflow 1.3**\r\nIn [train.py](https://github.com/google/seq2seq/blob/master/bin/train.py), could see that `Experiment` based structure is used. To existing code, I additionally added `export_strategies` parameters and tried to export. Faced the same error as in [Issue 12775](https://github.com/tensorflow/tensorflow/issues/12755). Referring to this merge request, I manually changed the respective files and exported successfully.\r\n\r\nBut, I cannot even load the exported graph with these simple lines,\r\n```\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n  tf.saved_model.loader.load(sess, [\"serve\"], export_dir)\r\n```\r\nThe error says,\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n<ipython-input-1-a5f6a641c0c9> in <module>()\r\n      4 \r\n      5 with tf.Session(graph=tf.Graph()) as sess:\r\n----> 6   tf.saved_model.loader.load(sess, [\"serve\"], export_dir)\r\n\r\n/pyenv/local/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.pyc in load(sess, tags, export_dir, **saver_kwargs)\r\n    214 \r\n    215     # Build a saver by importing the meta graph def to load.\r\n--> 216     saver = tf_saver.import_meta_graph(meta_graph_def_to_load, **saver_kwargs)\r\n    217 \r\n    218     if saver:\r\n\r\n/pyenv/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)\r\n   1696                                       clear_devices=clear_devices,\r\n   1697                                       import_scope=import_scope,\r\n-> 1698                                       **kwargs)\r\n   1699   if meta_graph_def.HasField(\"saver_def\"):\r\n   1700     return Saver(saver_def=meta_graph_def.saver_def, name=import_scope)\r\n\r\n/pyenv/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.pyc in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\r\n    690           for value in field.value:\r\n    691             col_op = graph.as_graph_element(\r\n--> 692                 ops.prepend_name_scope(value, scope_to_prepend_to_names))\r\n    693             graph.add_to_collection(key, col_op)\r\n    694         elif kind == \"int64_list\":\r\n\r\n/pyenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in as_graph_element(self, obj, allow_tensor, allow_operation)\r\n   2706 \r\n   2707     with self._lock:\r\n-> 2708       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n   2709 \r\n   2710   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):\r\n\r\n/pyenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\r\n   2766         if name not in self._nodes_by_name:\r\n   2767           raise KeyError(\"The name %s refers to an Operation not in the \"\r\n-> 2768                          \"graph.\" % repr(name))\r\n   2769         return self._nodes_by_name[name]\r\n   2770 \r\n\r\nKeyError: \"The name 'hash_table' refers to an Operation not in the graph.\"\r\n```\r\nDoes this occur because of my manual changes that may not be compatible with other part? Possibly, should I change any other part of code? or that change has nothing to do and is a different error?", "I don't think the issue you cited is related, but it's impossible to tell\nwithout knowing what you exactly did to the graph when you edited the\nfiles. If you modified some the Graph it is quite likely you created an\ninconsistent state.\n\nOn Tue, Feb 26, 2019 at 5:07 AM Sathyamoorthy R <notifications@github.com>\nwrote:\n\n> In one of my legacy project using google *seq2seq*, I use *Tensorflow 1.3*\n> In train.py <https://github.com/google/seq2seq/blob/master/bin/train.py>,\n> could see that Experiment based structure is used. To existing code, I\n> additionally added export_strategies parameters and tried to export.\n> Faced the same error as in Issue 12775\n> <https://github.com/tensorflow/tensorflow/issues/12755>. Referring to\n> this merge request, I manually changed the respective files and exported\n> successfully.\n>\n> But, I cannot even load the exported graph with these simple lines,\n>\n> with tf.Session(graph=tf.Graph()) as sess:\n>   tf.saved_model.loader.load(sess, [\"serve\"], export_dir)\n>\n> The error says,\n>\n> ---------------------------------------------------------------------------\n> KeyError                                  Traceback (most recent call last)\n> <ipython-input-1-a5f6a641c0c9> in <module>()\n>       4\n>       5 with tf.Session(graph=tf.Graph()) as sess:\n> ----> 6   tf.saved_model.loader.load(sess, [\"serve\"], export_dir)\n>\n> /pyenv/local/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.pyc in load(sess, tags, export_dir, **saver_kwargs)\n>     214\n>     215     # Build a saver by importing the meta graph def to load.\n> --> 216     saver = tf_saver.import_meta_graph(meta_graph_def_to_load, **saver_kwargs)\n>     217\n>     218     if saver:\n>\n> /pyenv/local/lib/python2.7/site-packages/tensorflow/python/training/saver.pyc in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)\n>    1696                                       clear_devices=clear_devices,\n>    1697                                       import_scope=import_scope,\n> -> 1698                                       **kwargs)\n>    1699   if meta_graph_def.HasField(\"saver_def\"):\n>    1700     return Saver(saver_def=meta_graph_def.saver_def, name=import_scope)\n>\n> /pyenv/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.pyc in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)\n>     690           for value in field.value:\n>     691             col_op = graph.as_graph_element(\n> --> 692                 ops.prepend_name_scope(value, scope_to_prepend_to_names))\n>     693             graph.add_to_collection(key, col_op)\n>     694         elif kind == \"int64_list\":\n>\n> /pyenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in as_graph_element(self, obj, allow_tensor, allow_operation)\n>    2706\n>    2707     with self._lock:\n> -> 2708       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n>    2709\n>    2710   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):\n>\n> /pyenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\n>    2766         if name not in self._nodes_by_name:\n>    2767           raise KeyError(\"The name %s refers to an Operation not in the \"\n> -> 2768                          \"graph.\" % repr(name))\n>    2769         return self._nodes_by_name[name]\n>    2770\n>\n> KeyError: \"The name 'hash_table' refers to an Operation not in the graph.\"\n>\n> Does this occur because of my manual changes that may not be compatible\n> with other part? Possibly, should I change any other part of code? or that\n> change has nothing to do and is a different error?\n>\n> \u2014\n> You are receiving this because you modified the open/close state.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/12759#issuecomment-467429299>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAjO_bA9JOZCRY9weCxsTNYKkWe2fVMfks5vRTGagaJpZM4PKuL5>\n> .\n>\n", "@martinwicke \r\nI tried to export the model (**SavedModel** format) using the following code in [existing file](https://github.com/google/seq2seq/blob/master/bin/train.py),\r\n\r\n```\r\n  source_tokens_ph = tf.placeholder(dtype=tf.string, shape=(1, None))\r\n  source_len_ph = tf.placeholder(dtype=tf.int32, shape=(1,))\r\n\r\n  features_serve = {\r\n    \"source_tokens\": source_tokens_ph,\r\n    \"source_len\": source_len_ph\r\n  }\r\n\r\n  experiment = PatchedExperiment(\r\n  ...\r\n\r\n  export_strategies = [saved_model_export_utils.make_export_strategy(serving_input_fn = build_default_serving_input_fn(features_serve))]\r\n  )\r\n```\r\nI faced an error as in this [issue](https://github.com/tensorflow/tensorflow/issues/12755).\r\nUpon referring to this [merge request](https://github.com/tensorflow/tensorflow/pull/12759/files), I edited the following files in my local tensorflow library,\r\n\r\n1. tensorflow/contrib/learn/python/learn/utils/input_fn_utils.py\r\n2. tensorflow/python/estimator/export/export.py\r\n\r\nIt helped me to successfully export the model.\r\n\r\nHowever, the simple code to load that exported model fails with the previously mentioned error,\r\n```\r\nwith tf.Session(graph=tf.Graph()) as sess:\r\n  tf.saved_model.loader.load(sess, [\"serve\"], export_dir)\r\n```\r\nAlso, leaving out the exported graph, I tried this with the model files directory,\r\n\r\n```\r\nwith tf.Session() as sess:\r\n    saver = tf.train.import_meta_graph(checkpoint + '.meta')\r\n```\r\nThis also fails with the similar error,\r\n\r\n> KeyError: \"The name 'hash_table' refers to an Operation not in the graph.\"\r\n\r\nCould not find what's messed up!\r\n\r\nTo further say, the code in this [file](https://github.com/google/seq2seq/blob/master/bin/train.py) uses **MonitoredTrainingSession**\r\n\r\nAlso, if you can help, I used the exported model for Tensorflow Serving and facing [this problem](https://stackoverflow.com/questions/54899188/error-model-att-seq2seq-minimum0-is-both-fed-and-fetched)(may be not related to this)", "Upon analyzing further,\r\nI tried to print `self._nodes_by_name` from [here](https://github.com/tensorflow/tensorflow/blob/31f770fa9fa8e32606fca7333349ce5c722fc199/tensorflow/python/framework/ops.py#L3709)\r\n\r\n**Part of Output:**\r\n```\r\nu'model/att_seq2seq/hash_table': <tf.Operation 'model/att_seq2seq/hash_table' type=HashTableV2>\r\nu'model/att_seq2seq/hash_table_1': <tf.Operation 'model/att_seq2seq/hash_table_1' type=HashTableV2>, \r\nu'model/att_seq2seq/hash_table_3': <tf.Operation 'model/att_seq2seq/hash_table_3' type=HashTableV2>, \r\nu'model/att_seq2seq/hash_table_2': <tf.Operation 'model/att_seq2seq/hash_table_2' type=HashTableV2>,\r\nu'model/att_seq2seq/hash_table_5': <tf.Operation 'model/att_seq2seq/hash_table_5' type=HashTableV2>,\r\nu'model/att_seq2seq/hash_table_4': <tf.Operation 'model/att_seq2seq/hash_table_4' type=HashTableV2>\r\n```\r\nAnd, I tried to print `meta_graph_def.collection_def.items()` from [here](https://github.com/tensorflow/tensorflow/blob/31f770fa9fa8e32606fca7333349ce5c722fc199/tensorflow/python/framework/meta_graph.py#L859)\r\n\r\n**Part of Output:**\r\n```\r\n(u'vocab_tables_values', node_list {\r\n  value: \"hash_table\"\r\n  value: \"hash_table_5\"\r\n  value: \"hash_table_1\"\r\n  value: \"hash_table_2\"\r\n  value: \"hash_table_4\"\r\n  value: \"hash_table_3\"\r\n})\r\n```\r\nThe node_list's(**2nd Output**) values are validated against **1st Output** by seeing [here](https://github.com/tensorflow/tensorflow/blob/31f770fa9fa8e32606fca7333349ce5c722fc199/tensorflow/python/framework/ops.py#L3709)\r\n\r\nThat seems to raise an error.\r\n\r\nSearching the seq2seq library for **vocab_tables** gives some hint to reach [here](https://github.com/google/seq2seq/blob/7f485894d412e8d81ce0e07977831865e44309ce/seq2seq/models/seq2seq_model.py#L214)\r\n\r\nAlso, this seems to deal with tensorflow's [name_scope](https://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow)\r\n\r\nHope this could help towards the possible solution.\r\nKindly ping if more details are needed.\r\n\r\nAgain, to stress, it uses **Tensorflow 1.3**\r\n\r\nAny help is appreciated. Thanks in advance"]}, {"number": 12758, "title": "[Feature request]: Hyper Parameters optimizer", "body": "Hi,\r\n\r\nDo you plan to add a code in order to optimize automatically the hyper-parameters of deep learning algorithm ? (number of hidden layers, value of the learning rate, type of down-sampling etc).\r\n\r\nBest,\r\n", "comments": ["It seems like a paper on NIPS 2016, which used Reinforcement Learning to learn the network structure, but it cost a lot of sources.", "Yes, it is a state of the art method but it could be really useful to be able to build with this kind of computation block in order to obtain better deep learning model. \r\n\r\nIt consisted in doing machine learning on the machine learning hyper parameters.", "This is a fairly vague feature request since there are so many hyper-parameter optimization methods. The most commonly\u00a0used one is the learning rate optimization. You launch several runs in parallel, with different learning rates in increments of 1.5, and see which one works better\r\n\r\nAt Google 1.5 years ago there were a few hyper-parameter optimizers implemented for TensorFlow, and I can't recall any of them being too practical. Couple of issues I remember\r\n\r\n1. to run hyper-parameter candidates in parallel needs extra infrastructure setup. Imagine your model trains with 50 replicas to train, a hyper-parameter optimizer needs to launch x copies of 50 replicas.\r\n\r\n2. hyper-parameter optimizers tend to have their own hyper-parameters that they are sensitive too. For instance, you have to decide how long to train your model with new parameters. Shrinking the learning rate is known to increase accuracy in short term, but will harm performance in the long term. So setting the number of epochs too small will point your search in the wrong direction, while setting it too large will take too long to be practical.\r\n\r\nI think it would make sense to request a specific hyper-parameter optimizer that works well for a specific application -- ie, maybe a specific algorithm that is known to achieve good results on for standard image recognition tasks.", "I agree with @yaroslavvb. Automating this sort of thing is a broad problem space that can be tackled in a variety of ways. We're open to feature requests in this area, but it helps when they're as specific as possible. Anyone with ideas like that, which could help TensorFlow improve, please feel free to open a new issue."]}, {"number": 12757, "title": "[Feature Request]: Resize netwoks on the fly", "body": "Hi,\r\n\r\nDo you plan to change the present paradigm about tensorflow neural network that make the networks size to be static after using run ?\r\n\r\nIs it plan to be able to resize on the fly the network shape (number of kernels, type of activation function, type of down sampling) during the training for instance.\r\n\r\nThanks.\r\n", "comments": ["I can't speak authoritatively on this subject, but I'm not aware of any such plans at the moment. I also can't say for certain whether or not it would be possible.\r\n\r\nI might be able to help you better if you shared a specific example of something you want to do with TensorFlow, but currently can't do, unless the paradigm changes.", "This issue is automatically closed due to lack of activity. Please re-open if this is still an issue for you. Thanks!"]}, {"number": 12756, "title": "Use `keepdims` and maintain backward compatible for `keep_dims`", "body": "This fix tries to address the issue raised in #6815 where\r\nboth `keepdims` and `keep_dims` were used with inconsistency.\r\n\r\nThis fix changes related api to `keepdims` while at the same\r\ntime maintain backward compatible for `keep_dims` so that\r\nuse will not be impacted.\r\n\r\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "comments": ["Can one of the admins verify this patch?", "Thanks @martinwicke for the review. The PR has been updated with `@deprecated_args` used.", "Any reason to choose `keepdims` instead of `keep_dims`? The latter seems like the better choice to me.", "@drpngx Not knowing enough history about the code base though I would assume the reason of having `keepdims` instead of `keep_dims` is to align with `numpy`? e.g., \r\n\r\nhttps://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.amin.html#numpy.amin\r\n\r\nAt the moment `keras` backend use `keepdims` while the rest of tensorflow mostly use `keep_dims`.", "Thank you @yongtang , that makes sense, `keepdims` seems good then.", "So, sorry to be dense, but does that mean that current code will issue a warning when `tf.reduce_sum(.., keep_dims=True)` will issue a warning but and behave like `tf.reduce_sum(..., keepdims=False)` (the default)?\r\n\r\nDon't we want to keep **both** and override `keep_dims` with `keepdims` if both are specified?", "Thanks @drpngx. The current behavior of this PR is to show a warning in doc string (from the annotation `@deprecated_args`), and accept either `keep_dims` or `keepdims` in the call.\r\n\r\nBoth `keep_dims` or `keepdims` should work.\r\n\r\nHowever, in case both `keep_dims` and `keepdims` are specified in the same call, then an error will be thrown.", "Oh, just saw the code. Sounds good.", "Jenkins, test this please.", "The PR has been updated with \r\n```\r\n    bazel-bin/tensorflow/tools/api/tests/api_compatibility_test \\\r\n              --update_goldens True\r\n```\r\nso that `//tensorflow/tools/api/tests:api_compatibility_test` could pass.", "Jenkins, test this please.", "The `Linux CPU Tests Makefile` test might be unrelated I think?", "this one seems like a download error\r\n\r\nJenkins, test this please.", "Violates PEP guidelines, https://github.com/tensorflow/tensorflow/issues/6815#issuecomment-355110926\r\n\r\nI vote for keeping `keep_dims`. There needs to be a strong compelling reason to change. Or support both and get rid of the annoying warnings."]}, {"number": 12755, "title": "build_default_serving_input_fn() uses a name that is not a valid variable scope. ", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04 LTS \r\n- **TensorFlow installed from (source or binary)**: SOURCE (pip install tensorflow) \r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: Python 2.7.6\r\n- **Bazel version (if compiling from source)**: N/A \r\n- **CUDA/cuDNN version**: N/A \r\n- **GPU model and memory**: N/A \r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nSee this short example:\r\n\r\n$ python -c \"import tensorflow as tf \r\nf = {'feature': tf.placeholder(name='feature', shape=[32], dtype=tf.float32) } \r\nserving_input = tf.contrib.learn.utils.input_fn_utils.build_default_serving_input_fn(f)\r\nserving_input()\"\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 4, in <module>\r\n  File \"/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/utils/input_fn_utils.py\", line 112, in input_fn\r\n    name=t.name)\r\n  File \"/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1548, in placeholder\r\n    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)\r\n  File \"/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 2094, in _placeholder\r\n    name=name)\r\n  File \"/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 374, in apply_op\r\n    with g.as_default(), ops.name_scope(name) as scope:\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 4522, in name_scope\r\n    with g.as_default(), g.name_scope(n) as scope:\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\r\n    return self.gen.next()\r\n  File \"/usr/local/google/home/slacy/src/tf_clean/env/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3172, in name_scope\r\n    raise ValueError(\"'%s' is not a valid scope name\" % name)\r\nValueError: 'feature:0' is not a valid scope name\r\n\r\nThe issue is that in input_fn_utils.py here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/utils/input_fn_utils.py#L112\r\n\r\nThe attribute \"t.name\" is passed to array_ops.placeholder(), and t.name will always have a \":N\" suffix, and \":\" is not a valid variable scope name, so this function fals. \r\n\r\n### Source code / logs\r\n\r\nSee above. ", "comments": ["I think the issue could be fixed by using tensor's op name (`t.op.name`) instead. Created a PR #12759 for the fix.", "Apparently that function was moved into core as [`tf.estimator.export.build_raw_serving_input_receiver_fn`](https://www.tensorflow.org/api_docs/python/tf/estimator/export/build_raw_serving_input_receiver_fn). Does that function work for you?", "@jart @slacy The `tf.estimator.export.build_raw_serving_input_receiver_fn` was using:\r\n```\r\nt.name.split(':')[0]\r\n```\r\nso it is fine although I think it might make sense to change it to\r\n```\r\nt.op.name\r\n```\r\n as well. This will make it easy to maintain later.\r\n\r\nI updated the PR #12759 and also added the test case to cover `tf.estimator.export.build_raw_serving_input_receiver_fn`  as well. Please take a look."]}, {"number": 12754, "title": "Issue with tf.py_func, tf.Tensor.set_shape and tf.Queues", "body": "This works fine:\r\n```\r\nimport tensorflow as tf\r\nx = tf.py_func(lambda: 42, [], tf.int64)\r\nx = reshape(x, [1])\r\nsess = tf.Session()\r\ny = tf.train.shuffle_batch([x], 32, 64, 32)\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\nsess.run(y)\r\n```\r\n\r\nBut simply setting the shape with `tf.Tensor.set_shape` instead of reshaping does not work:\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.py_func(lambda: 42, [], tf.int64)\r\nx.set_shape([1])\r\nsess = tf.Session()\r\ny = tf.train.shuffle_batch([x], 32, 64, 32)\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)  # Fails here.\r\nsess.run(y)\r\n\r\n2017-09-01 20:46:12.585027: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Shape mismatch in tuple component 0. Expected [1], got []\r\n2017-09-01 20:46:12.585264: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Shape mismatch in tuple component 0. Expected [1], got []\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>, Shape mismatch in tuple component 0. Expected [1], got []\r\n\t [[Node: shuffle_batch/random_shuffle_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](shuffle_batch/random_shuffle_queue, PyFunc)]]\r\n```\r\n\r\nEven though this works fine:\r\n\r\n```\r\nimport tensorflow as tf\r\nx = tf.py_func(lambda: 42, [], tf.int64)\r\nx.set_shape([1])\r\nsess.run(x)\r\n```", "comments": ["I think set_shape is just a hint to the shape inference code to fill in unknown shapes, and doesn't reshape the underlying Tensor. You should just use reshape here (which you report works). Closing for now, but please reopen if I misunderstood the issue."]}, {"number": 12753, "title": "tf.image.resize_bilinear has nearest neighbor gradients when downscaling", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.2.0-5-g435cdfc 1.2.1\r\n\r\n- **Python version**: \r\n3.5.2\r\n\r\n### Describe the problem\r\nWhen using `tf.image.resize_bilinear` to downscale, the gradients of the output w.r.t. the input is the same as what we get from nearest neighbors. For instance, using the code below you can confirm that the gradients are being sparsely backpropagated to specific pixels of the input image.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\nif __name__ == '__main__':\r\n    session = tf.Session()\r\n\r\n    x = tf.placeholder(tf.float32, shape=[None, 16, 16, 3])\r\n    x_low = tf.image.resize_bilinear(x, (4, 4))\r\n    loss = tf.reduce_sum(x_low)\r\n    loss_grad = tf.gradients(loss, x)\r\n\r\n    grad = session.run(loss_grad, {x: np.ones([1, 16, 16, 3], dtype=np.float32)})[0]\r\n    grad = grad[0, ...].transpose(2, 0, 1)\r\n    print(grad)  # should not be sparse, but it is\r\n\r\n    session.close()\r\n```\r\n\r\nThe output is:\r\n```\r\n[[[ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\r\n\r\n [[ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\r\n\r\n [[ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\r\n```", "comments": ["@andrehentz could you take a look at this resize_bilinear issue?", "@saeta, did you ever look at the gradients. @ankitkv, what did you want it to do? As an aside, you may not want to use a bilinear downsample filter, something like a box area filter (take the destination pixel size and make it into a box that overlaps all pixels on the source image). There's no primitive in tensorflow that does this to my knowledge.", "> @saeta, did you ever look at the gradients. @ankitkv, what did you want it to do? As an aside, you may not want to use a bilinear downsample filter, something like a box area filter (take the destination pixel size and make it into a box that overlaps all pixels on the source image). There's no primitive in tensorflow that does this to my knowledge.\r\n\r\n@aselle Thank you. Your comment really helped.\r\n\r\nI was expecting the tensorflow bilinear downsampling behavior to be similar to that of pillow's, but apparently it's not. For me the [resize_area](https://www.tensorflow.org/api_docs/python/tf/image/resize_area) op seems a better choice.", "Nagging Assignee @saeta: It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Are you satisfied with the resolution of your issue?\r\n[Yes](https://goo.gl/forms/Oe0tEvODFRoI2gJF3)\r\n[No](https://goo.gl/forms/fUjzOfrtkFbrOT8d2)"]}, {"number": 12752, "title": "Avoid gcc7 memcpy build error by updating BoringSSL", "body": "Tensorflow produces this build error with gcc7 due to ambiguous memcpy inlining in all versions of BoringSSL prior to https://github.com/google/boringssl/commit/17cf2cb1d226b0ba2401304242df7ddd3b6f1ff2  on Dec 12, 2016:\r\n\r\n```\r\nIn file included from /usr/include/string.h:639:0,\r\n                 from external/boringssl/src/crypto/asn1/a_bitstr.c:59:\r\nIn function 'memcpy',\r\n    inlined from 'i2c_ASN1_BIT_STRING' at external/boringssl/src/crypto/asn1/a_bitstr.c:118:5:\r\n/usr/include/bits/string3.h:53:10: error: '__builtin_memcpy': specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [-Werror=stringop-overflow=]\r\n   return __builtin___memcpy_chk (__dest, __src, __len, __bos0 (__dest));\r\n          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\ncc1: all warnings being treated as errors\r\n```\r\n\r\n\r\nUpdating BoringSSL snapshot from 7-11-16 to 7-7-17 fixes this.\r\n\r\nJuly 7 snapshot was picked because this is the stable build that Apollo uses:\r\nhttps://github.com/ApolloAuto/apollo/pull/185/files\r\n\r\n\r\nReliance on older snapshots of BoringSSL with this same memcpy bug is also breaking GRPC and Tensorflow Serving on modern gcc toolchains in the same way:\r\n\r\nhttps://github.com/grpc/grpc/issues/10843\r\nhttps://github.com/grpc/grpc/issues/11765\r\nhttps://github.com/tensorflow/serving/issues/475\r\n\r\n\r\nNote: This build error is ubiquitous since bazel's primary workspace.bzl downloads and builds BoringSSL regardless of whether it is required (i.e., even when Google Cloud Storage support is disabled and when SSL support is not enabled).", "comments": ["Can one of the admins verify this patch?", "@louiehelm, thanks for your PR! By analyzing the history of the files in this pull request, we identified @jart, @tensorflower-gardener and @kirilg to be potential reviewers.", "Jenkins, test this please.", "Jenkins, test this please."]}, {"number": 12751, "title": " bazel build tensorflow/python/tools:optimize_for_inference failed", "body": "$ bazel build tensorflow/python/tools:optimize_for_inference\r\nERROR: /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/core/BUILD:1416:1: no such target '//tensorflow/tools/git:gen/spec.json': target 'gen/spec.json' not declared in package 'tensorflow/tools/git' defined by /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/core/BUILD:1416:1: no such target '//tensorflow/tools/git:gen/head': target 'gen/head' not declared in package 'tensorflow/tools/git' defined by /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/core/BUILD:1416:1: no such target '//tensorflow/tools/git:gen/branch_ref': target 'gen/branch_ref' not declared in package 'tensorflow/tools/git' defined by /Users/andylin/Desktop/gitwork/tensorflow/tensorflow/tools/git/BUILD and referenced by '//tensorflow/core:version_info_gen'.\r\nERROR: Analysis of target '//tensorflow/python/tools:optimize_for_inference' failed; build aborted.\r\nINFO: Elapsed time: 0.875s", "comments": ["What I can tell from  the error messages is you did not run `configure` script before trying to build.\r\nPlease make sure to read our install from sources guide if you would like to build any part of tensorflow from sources.\r\nhttps://www.tensorflow.org/install/install_sources", "@gunan Thanks."]}, {"number": 12750, "title": "Android-Tensorflow model loading issue with SavedModelBundle.load()", "body": "Loading the model in Android give below error:\r\n\r\nFATAL EXCEPTION: main\r\n                  Process: tensorflow.lgsi.com.posapplication, PID: 516\r\n                  java.lang.RuntimeException: Unable to start activity ComponentInfo{tensorflow.lgsi.com.posapplication/tensorflow.lgsi.com.posapplication.MainActivity}: java.lang.U**nsupportedOperationException**: **Loading a SavedModel is not supported in Android. File a bug at https://github.com/tensorflow/tensorflow/issues if this feature is important to you**\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2727)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2788)\r\n                      at android.app.ActivityThread.-wrap12(ActivityThread.java)\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1504)\r\n                      at android.os.Handler.dispatchMessage(Handler.java:102)\r\n                      at android.os.Looper.loop(Looper.java:154)\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6248)\r\n                      at java.lang.reflect.Method.invoke(Native Method)\r\n                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:872)\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:762)\r\n                   Caused by: java.lang.UnsupportedOperationException: Loading a SavedModel is not supported in Android. File a bug at https://github.com/tensorflow/tensorflow/issues if this feature is important to you\r\n                      at org.tensorflow.SavedModelBundle.load(Native Method)\r\n                      at org.tensorflow.SavedModelBundle.load(SavedModelBundle.java:38)\r\n                      at tensorflow.com.posapplication.tagger.PosTagger.<init>(PosTagger.java:23)\r\n                      at tensorflow.lgsi.com.posapplication.tagger.PosTagger.getInsPosTagger(PosTagger.java:30)\r\n                      at tensorflow.com.posapplication.MainActivity.onCreate(MainActivity.java:56)\r\n                      at android.app.Activity.performCreate(Activity.java:6757)\r\n                      at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1119)\r\n                      at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2680)\r\n                      at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2788)\u00a0\r\n                      at android.app.ActivityThread.-wrap12(ActivityThread.java)\u00a0\r\n                      at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1504)\u00a0\r\n                      at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\r\n                      at android.os.Looper.loop(Looper.java:154)\u00a0\r\n                      at android.app.ActivityThread.main(ActivityThread.java:6248)\u00a0\r\n                      at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n                      at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:872)\u00a0\r\n                      at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:762)\u00a0\r\nI/art: Starting\r\n\r\n**Model is saved in Python by using below API:**\r\nbuilder = tf.saved_model.builder.SavedModelBuilder(r'./tmp/model')\r\nbuilder.add_meta_graph_and_variables(session, [tf.saved_model.tag_constants.SERVING])\r\nbuilder.save(True) \r\n\r\n**Model is loading in Andoid using below api:**\r\ninferenceInterface = new TensorFlowInferenceInterface(context.getAssets(), MODEL_FILE);", "comments": ["hey guys,\r\nim facing the same issue. Is savedmodel the recommended way to export from tensorflow ? OTOH, I do see a [commit](https://github.com/tensorflow/tensorflow/commit/ca6b88eb95b089010a1b970e0de7398195b5bcca) about adding this support to android.\r\n\r\nWe are seeing too many options here:\r\n1. freezing - https://github.com/tensorflow/tensorflow/issues/12319  \r\n2. savedmodelbuilder - https://github.com/tensorflow/tensorflow/issues/12750 (which doesnt look to load on android)  \r\n3. convert_variables_to_constants - https://github.com/tensorflow/models/issues/38 (not sure if this saves with all the values)  \r\n4. tf.train.export_meta_graph - https://www.tensorflow.org/api_guides/python/meta_graph (is this the same as freezing) ?  \r\n5. optimize_for_inference - https://github.com/thtrieu/darkflow/issues/286 . However, they wrote their own save function and graph function  \r\n6. https://www.tensorflow.org/performance/xla/tfcompile  \r\n7. Go directly to Keras - http://blog.stratospark.com/creating-a-deep-learning-ios-app-with-keras-and-tensorflow.html\r\n\r\nPretty puzzled on what is the production-ready way to do this.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "The recommended way to load a model is through the freeze_graph process. There's some more detail here: https://www.tensorflow.org/mobile/prepare_models", "@petewarden thanks for the detailed note on the page. Could you add how to do this when using Google's own Cloud ML for training ? because the bazel thing is extremly tricky on CloudML.\r\n\r\nAdditionally, it would help to have this example on https://github.com/GoogleCloudPlatform/cloudml-samples \r\nthanks!"]}, {"number": 12749, "title": "Android- Tensorflow model loading issue", "body": "I have saved model using tf.train.Saver.save() and then freeze the graph bu using below method:\r\ndef freezeModel(modelName):\r\n    input_graph_path = modelName + '.pb'\r\n    checkpoint_path = './' + modelName + '.ckpt'\r\n    input_saver_def_path = \"\"\r\n    input_binary = True\r\n    output_node_names = \"x_indices,x_values,x_dense_shape,unary_scores,transition_params,train_op\"\r\n    restore_op_name = \"save/restore_all\"\r\n    filename_tensor_name = \"save/Const:0\"\r\n    output_frozen_graph_name = 'frozen_' + modelName + '.pb'\r\n   \r\n    clear_devices = True\r\n\r\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,input_binary, checkpoint_path, output_node_names,restore_op_name, filename_tensor_name,output_frozen_graph_name, clear_devices, \"\")\r\n    #freeze_graph.freeze_graph(input_saved_model_dir='Model/', saved_model_tags='serve')\r\n    return\r\n\r\n\r\nThen loading the modelName.pb file in Android using inference api as below:\r\nTensorFlowInferenceInterface inferenceInterface = new **TensorFlowInferenceInterface**(context.getAssets(), MODEL_FILE);\r\n\r\nI am getting **below error**:\r\nCaused by: **java.io.IOException: Not a valid TensorFlow Graph serialization: Value for attr 'T' of int64 is not in the list of allowed values: float, int32**\r\n; NodeDef: Less_1 = Less[T=DT_INT64](ToInt64_2, ToInt64_3); Op<name=Less; signature=x:T, y:T -> z:bool; attr=T:type,allowed=[DT_FLOAT, DT_INT32]>\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.loadGraph(TensorFlowInferenceInterface.java:439)\r\nat org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:98)\r\n\r\n\r\n **Here is the code using to create the graph:**\r\nx_indices_t= tf.placeholder(tf.int64, shape=(None, 3), name=\"x_indices\")\r\nx_values_t= tf.placeholder(tf.float32, shape=(None), name=\"x_values\")\r\nx_dense_shape_t= tf.placeholder(tf.int64, shape=(3), name=\"x_dense_shape\")#[len(X_train), max_word_count, feature_count]\r\nshape_unary_score_t = tf.placeholder(dtype=tf.int64, shape=(None, None, None), name=\"shape_unary_score\")#[num_examples_t, max_word_count, num_tags]\r\nshape_t = tf.shape(shape_unary_score_t)\r\n\r\nx_t = tf.SparseTensor(indices=x_indices, values=x_values, dense_shape=x_dense_shape)\r\n\r\n\r\nTo resolve above error i changed the dtype of indices and shapes to int32 then tf.SparseTensor() is givving below error:\r\nFile \"CRF_POS_Trainer_Tensor - Training.py\", line 323, in <module>\r\n    x_t = tf.SparseTensor(indices=x_indices_t, values=x_values_t, dense_shape=x_dense_shape_t)\r\n  File \"/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/sparse_tensor.py\", line 119, in _init_\r\n    indices, name=\"indices\", dtype=dtypes.int64)\r\n  File \"/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 676, in convert_to_tensor\r\n    as_ref=False)\r\n  File \"/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 741, in internal_convert_to_tensor\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n  File \"/home/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 614, in _TensorTensorConversionFunction\r\n    % (dtype.name, t.dtype.name, str(t)))\r\nValueError: Tensor conversion requested dtype int64 for Tensor with dtype int32: 'Tensor(\"x_indices:0\", shape=(?, 3), dtype=int32)'\r\n        \r\n\r\nSo how to i resolve issue to create SparseTensor with int32 data.\r\n\r\n**Tensorflow Version: 1.3.0**", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 12748, "title": "ImportError: DLL load failed @ VS2012 / PTVS 2.1.1 / Windows 7", "body": "## System information\r\n-OS Platform: Windows 7\r\n-TensorFlow version: tensorflow-gpu==1.3.0\r\n-CUDA/cuDNN version: 8.0/6.0\r\n-GPU model and memory: GeForce GTX 750\r\n-IDE: Visual Studio 2012/PTVS 2.1.1/virtualenv==15.1.0\r\n\r\n## Problem\r\nI installed tensorflow-gpu in \"virtualenv\".\r\nUnder the following environment, it works well.\r\n    @Command prompt: activate \"virtualenv\" > start python > import tensorflow : \r\nHowever. it does not work well under the following environment.\r\n    @VS2012: Python Environments=\"virtualenv\" > I try to run \"main.py\" which has only 1 line; \"import tensorflow\": Then, the error mentioned below occurs!\r\n**How can I use \"tensorflow-gpu\" @VS2012 ?**\r\n\r\n## Log\r\nTraceback (most recent call last):\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: \uc9c0\uc815\ub41c \ubaa8\ub4c8\uc744 \ucc3e\uc744 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<debug input>\", line 1, in <module>\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 49, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 52, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 18, in swig_import_helper\r\n    return importlib.import_module(mname)\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 969, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 958, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 666, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\r\n  File \"<frozen importlib._bootstrap_external>\", line 914, in create_module\r\n  File \"<frozen importlib._bootstrap>\", line 222, in _call_with_frames_removed\r\nImportError: DLL load failed: \uc9c0\uc815\ub41c \ubaa8\ub4c8\uc744 \ucc3e\uc744 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 41, in <module>\r\n    from tensorflow.python.pywrap_tensorflow_internal import *\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 21, in <module>\r\n    _pywrap_tensorflow_internal = swig_import_helper()\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 20, in swig_import_helper\r\n    return importlib.import_module('_pywrap_tensorflow_internal')\r\n  File \"D:\\WORK_online3\\pyToolbox_Packages\\virtualenv\\venv35-64\\lib\\importlib\\__init__.py\", line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\nImportError: No module named '_pywrap_tensorflow_internal'\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n\r\n\r\n\r\n", "comments": ["We build the TensorFlow release with Visual Studio 2015. It's possible that your 2012-based setup lacks MSVCP140.dll. See [this Stack Overflow answer](https://stackoverflow.com/a/44296533) for more details.", "After running your script \"https://gist.github.com/ee5dbcfdd045fa48a27d56664411d41c.git\", \r\nI can use \"tensorflow-gpu\" successfully. I appreciate your quick reply.\r\n", "Great! I will close for now but please reopen if the issue isn't resolved."]}, {"number": 12747, "title": "How to build a full C++ API libtensorflow_cc.so for Android?", "body": "### Describe the problem\r\nWhen I use \r\n```\r\nbazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures\r\n```\r\nto compile the full C++ API libtensorflow_cc.so for Android, but It can not work.\r\n\r\n\r\n### Source code / logs\r\n```\r\nC++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1): arm-linux-androideabi-gcc failed: error executing command \r\n  (exec env - \\\r\n    PATH=/home/mlin2/.cargo/bin:/home/mlin2/bin:/home/mlin2/anaconda2/bin:/usr/lib64/qt-3.3/bin:/home/mlin2/perl5/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/go/bin:/opt/gopath/bin:/home/mlin2/software/android-ndk-r12b/:/home/mlin2/.fzf/bin:/home/mlin2/.local/bin:/home/mlin2/bin \\\r\n    PWD=/proc/self/cwd \\\r\n    PYTHON_BIN_PATH=/home/mlin2/anaconda2/bin/python \\\r\n    PYTHON_LIB_PATH=/home/mlin2/anaconda2/lib/python2.7/site-packages \\\r\n    TF_NEED_CUDA=0 \\\r\n    TF_NEED_OPENCL=0 \\\r\n  external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -fstack-protector-strong -fpic -ffunction-sections -funwind-tables -no-canonical-prefixes -fno-canonical-system-headers '-march=armv7-a' '-mfpu=vfpv3-d16' '-mfloat-abi=softfp' -mthumb -Os -g -DNDEBUG -MD -MF bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.d '-frandom-seed=bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o' -iquote external/highwayhash -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/highwayhash -iquote external/bazel_tools -iquote bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 '--sysroot=external/androidndk/ndk/platforms/android-14/arch-arm' -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/libs/armeabi-v7a/include -isystem external/androidndk/ndk/sources/cxx-stl/gnu-libstdc++/4.9/include/backward -c external/highwayhash/highwayhash/sip_hash.cc -o bazel-out/arm-linux-androideabi-4.9-v7a-gnu-libstdcpp-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o).\r\nIn file included from external/highwayhash/highwayhash/arch_specific.h:23:0,\r\n                 from external/highwayhash/highwayhash/sip_hash.h:23,\r\n                 from external/highwayhash/highwayhash/sip_hash.cc:15:\r\nexternal/highwayhash/highwayhash/state_helpers.h: In function 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, State*)':\r\nexternal/highwayhash/highwayhash/compiler_specific.h:50:30: error: expected initializer before 'alignas'\r\n #define HH_ALIGNAS(multiple) alignas(multiple)  // C++11\r\n                              ^\r\nexternal/highwayhash/highwayhash/state_helpers.h:49:41: note: in expansion of macro 'HH_ALIGNAS'\r\n   char final_packet[State::kPacketSize] HH_ALIGNAS(32) = {0};\r\n                                         ^\r\nIn file included from external/highwayhash/highwayhash/sip_hash.h:25:0,\r\n                 from external/highwayhash/highwayhash/sip_hash.cc:15:\r\nexternal/highwayhash/highwayhash/state_helpers.h:64:10: error: 'final_packet' was not declared in this scope\r\n   memcpy(final_packet, remaining_bytes, remaining_size - remainder_mod4);\r\n          ^\r\nexternal/highwayhash/highwayhash/state_helpers.h: In function 'void highwayhash::UpdateState(const char*, highwayhash::HH_U64, State*)':\r\nexternal/highwayhash/highwayhash/state_helpers.h:76:76: error: there are no arguments to 'static_assert' that depend on a template parameter, so a declaration of 'static_assert' must be available [-fpermissive]\r\n   static_assert((kPacketSize & (kPacketSize - 1)) == 0, \"Size must be 2^i.\");\r\n                                                                            ^\r\nexternal/highwayhash/highwayhash/state_helpers.h:76:76: note: (if you use '-fpermissive', G++ will accept your code, but allowing the use of an undeclared name is deprecated)\r\nIn file included from external/highwayhash/highwayhash/sip_hash.cc:15:0:\r\nexternal/highwayhash/highwayhash/sip_hash.h: At global scope:\r\nexternal/highwayhash/highwayhash/sip_hash.h:33:9: error: expected nested-name-specifier before 'Key'\r\n   using Key = HH_U64[2];\r\n         ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:36:42: error: 'Key' does not name a type\r\n   explicit HH_INLINE SipHashStateT(const Key& key) {\r\n                                          ^\r\nexternal/highwayhash/highwayhash/sip_hash.h: In constructor 'highwayhash::SipHashStateT<kUpdateIters, kFinalizeIters>::SipHashStateT(const int&)':\r\nexternal/highwayhash/highwayhash/sip_hash.h:37:39: error: invalid types 'const int[int]' for array subscript\r\n     v0 = 0x736f6d6570736575ull ^ key[0];\r\n                                       ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:38:39: error: invalid types 'const int[int]' for array subscript\r\n     v1 = 0x646f72616e646f6dull ^ key[1];\r\n                                       ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:39:39: error: invalid types 'const int[int]' for array subscript\r\n     v2 = 0x6c7967656e657261ull ^ key[0];\r\n                                       ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:40:39: error: invalid types 'const int[int]' for array subscript\r\n     v3 = 0x7465646279746573ull ^ key[1];\r\n                                       ^\r\nexternal/highwayhash/highwayhash/sip_hash.h: At global scope:\r\nexternal/highwayhash/highwayhash/sip_hash.h:104:7: error: expected nested-name-specifier before 'SipHashState'\r\n using SipHashState = SipHashStateT<2, 4>;\r\n       ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:105:7: error: expected nested-name-specifier before 'SipHash13State'\r\n using SipHash13State = SipHashStateT<1, 3>;\r\n       ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:110:29: error: 'SipHashState' was not declared in this scope\r\n HH_INLINE void PaddedUpdate<SipHashState>(const HH_U64 size,\r\n                             ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:113:43: error: 'SipHashState' has not been declared\r\n                                           SipHashState* state) {\r\n                                           ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:110:16: error: template-id 'PaddedUpdate<<expression error> >' for 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, int*)' does not match any template declaration\r\n HH_INLINE void PaddedUpdate<SipHashState>(const HH_U64 size,\r\n                ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:122:29: error: 'SipHash13State' was not declared in this scope\r\n HH_INLINE void PaddedUpdate<SipHash13State>(const HH_U64 size,\r\n                             ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:125:45: error: 'SipHash13State' has not been declared\r\n                                             SipHash13State* state) {\r\n                                             ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:122:16: error: template-id 'PaddedUpdate<<expression error> >' for 'void highwayhash::PaddedUpdate(highwayhash::HH_U64, const char*, highwayhash::HH_U64, int*)' does not match any template declaration\r\n HH_INLINE void PaddedUpdate<SipHash13State>(const HH_U64 size,\r\n                ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:146:39: error: 'SipHashState' does not name a type\r\n static HH_INLINE HH_U64 SipHash(const SipHashState::Key& key, const char* bytes,\r\n                                       ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:146:56: error: expected unqualified-id before '&' token\r\n static HH_INLINE HH_U64 SipHash(const SipHashState::Key& key, const char* bytes,\r\n                                                        ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:146:56: error: expected ')' before '&' token\r\nexternal/highwayhash/highwayhash/sip_hash.h:146:58: error: expected initializer before 'key'\r\n static HH_INLINE HH_U64 SipHash(const SipHashState::Key& key, const char* bytes,\r\n                                                          ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:152:41: error: 'SipHash13State' does not name a type\r\n static HH_INLINE HH_U64 SipHash13(const SipHash13State::Key& key,\r\n                                         ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:152:60: error: expected unqualified-id before '&' token\r\n static HH_INLINE HH_U64 SipHash13(const SipHash13State::Key& key,\r\n                                                            ^\r\nexternal/highwayhash/highwayhash/sip_hash.h:152:60: error: expected ')' before '&' token\r\nexternal/highwayhash/highwayhash/sip_hash.h:152:62: error: expected initializer before 'key'\r\n static HH_INLINE HH_U64 SipHash13(const SipHash13State::Key& key,\r\n                                                              ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:18:20: error: 'highwayhash::SipHash' has not been declared\r\n using highwayhash::SipHash;\r\n                    ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:19:20: error: 'highwayhash::SipHash13' has not been declared\r\n using highwayhash::SipHash13;\r\n                    ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:20:7: error: expected nested-name-specifier before 'Key'\r\n using Key = highwayhash::SipHashState::Key;\r\n       ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:21:7: error: expected nested-name-specifier before 'Key13'\r\n using Key13 = highwayhash::SipHash13State::Key;\r\n       ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc: In function 'highwayhash::HH_U64 SipHashC(const HH_U64*, const char*, highwayhash::HH_U64)':\r\nexternal/highwayhash/highwayhash/sip_hash.cc:26:42: error: ISO C++ forbids declaration of 'type name' with no type [-fpermissive]\r\n   return SipHash(*reinterpret_cast<const Key*>(key), bytes, size);\r\n                                          ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:26:42: error: expected '>' before 'Key'\r\nexternal/highwayhash/highwayhash/sip_hash.cc:26:42: error: expected '(' before 'Key'\r\nexternal/highwayhash/highwayhash/sip_hash.cc:26:42: error: 'Key' was not declared in this scope\r\nexternal/highwayhash/highwayhash/sip_hash.cc:26:46: error: expected primary-expression before '>' token\r\n   return SipHash(*reinterpret_cast<const Key*>(key), bytes, size);\r\n                                              ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc: In function 'highwayhash::HH_U64 SipHash13C(const HH_U64*, const char*, highwayhash::HH_U64)':\r\nexternal/highwayhash/highwayhash/sip_hash.cc:30:44: error: ISO C++ forbids declaration of 'type name' with no type [-fpermissive]\r\n   return SipHash13(*reinterpret_cast<const Key13*>(key), bytes, size);\r\n                                            ^\r\nexternal/highwayhash/highwayhash/sip_hash.cc:30:44: error: expected '>' before 'Key13'\r\nexternal/highwayhash/highwayhash/sip_hash.cc:30:44: error: expected '(' before 'Key13'\r\nexternal/highwayhash/highwayhash/sip_hash.cc:30:44: error: 'Key13' was not declared in this scope\r\nexternal/highwayhash/highwayhash/sip_hash.cc:30:50: error: expected primary-expression before '>' token\r\n   return SipHash13(*reinterpret_cast<const Key13*>(key), bytes, size);\r\n                                                  ^\r\nTarget //tensorflow:libtensorflow_cc.so failed to build\r\nINFO: Elapsed time: 1.812s, Critical Path: 0.14s\r\n```", "comments": ["Please provide details about what platform you are using  (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?  Make sure you also include the exact command if possible to produce  the output included in your test case. If you are unclear what to include  see the issue template displayed in  [the Github new issue template](https://github.com/tensorflow/tensorflow/issues/new).\r\n\r\n We ask for this in the issue submission template, because    it is really difficult to help without that information. Thanks!", "@jart Thank you for your reply, The information as follow:\r\n**Command**: bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures\r\n**Platform**: Linux version 3.10.0-514.10.2.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-11) (GCC) )\r\n**Tensorflow version**: r1.3\r\n**Compile from source**: YES\r\n\r\n", "To summarize the error, I think it's a C++11 issue:\r\n```\r\nexternal/highwayhash/highwayhash/compiler_specific.h:50:30: error: expected initializer before 'alignas'\r\n #define HH_ALIGNAS(multiple) alignas(multiple)  // C++11\r\n```\r\n\r\nCan you double-check and try `--copts=-std=cxx11`?", "@drpngx \r\nI use \"**bazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures --copts=-std=cxx11**\"\r\nBUT:\"**ERROR: Unrecognized option: --copts=-std=cxx11**\"\r\n", "Add options as:\r\n--cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --verbose_failures", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing for now. Feel free to reopen when you have more information.", "I have different error when build tensorflow C++ r1.10 for Android with NDK r15c\r\n\r\nbazel build -c opt //tensorflow:libtensorflow_cc.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures --cxxopt=\"-std=c++11\" --cxxopt=\"-DTENSORFLOW_DISABLE_META\" --verbose_failures\r\n\r\nERROR: /home/rhuang/tensorflow/tensorflow/core/common_runtime/eager/BUILD:215:1: no such package 'util/hash': BUILD file not found on package path and referenced by '//tensorflow/core/common_runtime/eager:attr_builder'\r\nERROR: Analysis of target '//tensorflow:libtensorflow_cc.so' failed; build aborted: no such package 'util/hash': BUILD file not found on package path\r\nINFO: Elapsed time: 3.116s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (60 packages loaded)\r\n\r\nAny idea ? Thanks", "robinqhuang do you find any solution about this error", "> robinqhuang do you find any solution about this error\r\n\r\nNO. I can't build libtensorflow_cc.so for r1.10 either on Mac or Ubuntu 18.04 with NDK r15c.\r\nI am using makefile to build high level lib libtensorflow_infererence.so for Android. It works if I leave a memory leak of session options.", "pleas explain in detail this (It works if I leave a memory leak of session\noptions.). we also have the problem in building\nlibtensorflow_infererence.so for Android .\nplease guid us we are tsuck in our FYP ...\n\n\nOn Mon, 22 Oct 2018 at 20:28, RobinQHuang <notifications@github.com> wrote:\n\n> robinqhuang do you find any solution about this error\n>\n> NO. I can't build libtensorflow_cc.so for r1.10 either on Mac or Ubuntu\n> 18.04 with NDK r15c.\n> I am using makefile to build high level lib libtensorflow_infererence.so\n> for Android. It works if I leave a memory leak of session options.\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/12747#issuecomment-431867077>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AjJ_ob9Bp38Pxr5wZS7ZFEIjZT-M2-fCks5uneQvgaJpZM4PJ3j2>\n> .\n>\n", "Please see the link above you asked the question. It tells u how to build", "@robinqhuang hi, could you kindly give me a link that you mentioned above. It seems no link on the page."]}, {"number": 12746, "title": "Feature Request: Loading weights for layers defined in tf.layers api", "body": "Let's say I define a layer using the `tf.layers` API as shown below:\r\n\r\n```\r\nconv1 = tf.layers.conv2d(input_img, filters=32, \r\n                                     kernel_size=(3,3), \r\n                                     padding='same', \r\n                                     name='Conv1')\r\n```\r\n\r\nNow I can build a whole network defining such layers. Can you please introduce another functionality for the `tf.layers` api so that for each layer we can set the weights in a single line like this:\r\n\r\n`conv1.set_weights(weights)` or something like this `conv1.set_params(param_values)`\r\n\r\nThis would be very very useful.", "comments": ["If you use the Object oriented layers (on master), you can access the variables directly:\r\n\r\n```\r\nconv1 = tf.layers.Conv2d(...)\r\ntf.assign(conv1.kernel, [1,2,3,4]) \r\n```\r\n\r\n(also check out `init_from_checkpoint`)\r\n\r\nWould these work in your use case?", "Correct me if I am wrong, wouldn't it be required to reshape the weights as per the previous layer output shape if I am going to build and set the weights for all the layers in the network? Doing this check for each layer in the net would be a mess. With the feature that I requested for, I think that this check can be included in the method itself where the output of incoming layer is checked, the weights are reshaped accordingly and then assigned to the kernel", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "If you have to reshape weights, they will most likely not work. You either have weights of the correct size, then they can be loaded using init_from_checkpoint, or you don't, in which case, it is highly unlikely to work at all.", "Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Why should I go for a complicated way? The high-level api should come with such methods. Every framework which is a high level api, for example keras or lasagne, comes with such a nice way to set the weights for each layer independently", "All layers in tf.keras come with this ability. You should use them.\r\n\r\nI still believe that there would never be a need to reshape, or if there was, you'd want to know."]}, {"number": 12745, "title": "Distributed tensorflow - Stuck at \"CreateSession still waiting for response from worker\"", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.0 && 1.3\r\n- **Python version**: \r\n2.7\r\n- **CUDA/cuDNN version**:\r\nno\r\n\r\n### Describe the problem\r\nRun the code on k8s with the spec `1 ps + 20 workers`. there are some workers print the log all the time.\r\n```\r\nI tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:12\r\nI tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:17\r\n```\r\nI met two situations:\r\n1. worker 1 wait for worker 12 and 17, but worker 12 and 17 could start training without these logs.\r\n2. all of other works wait for the worker 12 and 17.\r\n\r\nI could telnet the no-response worker within the docker vm in both situations, so it's wired. Is there a bug? \r\nSome have also encountered this problem. see [Stackoverflow](https://stackoverflow.com/search?q=CreateSession+still+waiting+for+response+from+worker).\r\n### Source code / logs\r\n```\r\nimport datetime\r\nimport json\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\nflags.DEFINE_integer(\"max_epochs\", 10000000, \"Number of steps to run trainer.\")\r\nflags.DEFINE_string(\"checkpoint_path\", \"./checkpoint/\",\r\n                    \"The checkpoint directory\")\r\nflags.DEFINE_string(\"output_path\", \"./tensorboard/\",\r\n                    \"indicates training output\")\r\nflags.DEFINE_integer(\"checkpoint_period\", 1,\r\n                     \"Number of epochs to save checkpoint.\")\r\nflags.DEFINE_float(\"learning_rate\", 0.01, \"Initial learning rate.\")\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef main():\r\n    # Create train data\r\n    train_X = np.linspace(-1, 1, 100)\r\n    train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10\r\n    learning_rate = FLAGS.learning_rate\r\n    start_training_time = datetime.datetime.now()\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n    # Exampmle: {\"cluster\": {\"ps\": [\"127.0.0.1:3001\"], \"worker\": [\"127.0.0.1:3002\", \"127.0.0.1:3003\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\r\n    env = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\"))\r\n    task_data = env.get(\"task\", None)\r\n    cluster_spec = env[\"cluster\"]\r\n    task_type = task_data[\"type\"]\r\n    task_index = task_data[\"index\"]\r\n\r\n    cluster = tf.train.ClusterSpec(cluster_spec)\r\n    server = tf.train.Server(cluster,\r\n                             job_name=task_type,\r\n                             task_index=task_index)\r\n\r\n    if task_type == \"ps\":\r\n        server.join()\r\n    elif task_type == \"worker\":\r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:{}/task:{}\".format(task_type, task_index),\r\n                cluster=cluster)):\r\n\r\n            # Define the model\r\n            keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1])\r\n            keys = tf.identity(keys_placeholder)\r\n            X = tf.placeholder(\"float\", shape=[None, 1])\r\n            Y = tf.placeholder(\"float\", shape=[None, 1])\r\n            w = tf.Variable(0.0, name=\"weight\")\r\n            b = tf.Variable(0.0, name=\"bias\")\r\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n            loss = tf.reduce_sum(tf.square(Y - tf.multiply(X, w) - b))\r\n            train_op = optimizer.minimize(loss, global_step=global_step)\r\n            predict_op = tf.multiply(X, w) + b\r\n            tf.summary.scalar(\"loss\", loss)\r\n            summary_op = tf.summary.merge_all()\r\n            init_op = tf.global_variables_initializer()\r\n            saver = tf.train.Saver()\r\n            #saver = tf.train.Saver(sharded=True)\r\n\r\n\r\n            sv = tf.train.Supervisor(is_chief=(task_index == 0),\r\n                                     logdir=FLAGS.checkpoint_path,\r\n                                     init_op=init_op,\r\n                                     #summary_op=summary_op,\r\n                                     summary_op=None,\r\n                                     saver=saver,\r\n                                     global_step=global_step,\r\n                                     save_model_secs=60)\r\n\r\n            try:\r\n                with sv.managed_session(server.target) as sess:\r\n                    print(\"Save tensorboard files into: {}\".format(FLAGS.output_path))\r\n                    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)\r\n\r\n                    print(\"Run training with epoch number: {}\".format(\r\n                        FLAGS.max_epochs))\r\n                    for i in range(FLAGS.max_epochs):\r\n                        for (x, y) in zip(train_X, train_Y):\r\n                            x = np.array([[x]])\r\n                            y = np.array([[y]])\r\n                            sess.run(train_op, feed_dict={X: x, Y: y})\r\n\r\n                        if i % FLAGS.checkpoint_period == 0:\r\n                            x = np.array([[train_X[0]]])\r\n                            y = np.array([[train_Y[0]]])\r\n                            summary_value, loss_value, step = sess.run(\r\n                                [summary_op, loss, global_step],\r\n                                feed_dict={X: x,\r\n                                           Y: y})\r\n                            print(\"Epoch: {}, loss: {}\".format(i, loss_value))\r\n                            if task_index == 0:\r\n                                writer.add_summary(summary_value, step)\r\n\r\n                    writer.close()\r\n\r\n                    end_training_time = datetime.datetime.now()\r\n                    print(\"[{}] End of distributed training.\".format(\r\n                        end_training_time - start_training_time))\r\n\r\n\r\n            except Exception as e:\r\n                print(e)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n", "comments": ["@mrry Could you help to take a look? thanks.", "Does the problem consistently affect workers 12 and 17, and not other workers? If so, is there anything unusual about those workers' network configurations?", "@mrry, thanks for your reply. \r\n\r\nYes, it's consistent between worker 12 and 17, they wait for each other and other workers wait for them. But other workers could connect each other, so it's confused.\r\n\r\nAll workers use the same configuration. use k8s to schedule the network.", "This sounds like the problem is with your network configuration, rather than a bug in TensorFlow. In particular, there seems to be something wrong with the configuration for workers 12 and 17... perhaps something else is listening on the same socket? Is there anything in the logs for those workers? You could try running the servers with environment variable `GRPC_VERBOSITY=DEBUG` and see if there is any extra information.\r\n\r\nAnother possible thing you could try is running with 15 workers (instead of 20) to determine if the VM for worker 12 is always affected (independent of cluster configuration). You could try shuffling the list of network addresses used for the workers, to see if the problem still affects the same hosts. Or you could try connecting directly to the server on worker 12/17 (after setting up the cluster) by creating a `tf.Session(ADDRESS_FOR_WORKER_12_OR_17)` and see if you can run a simple graph.", "Thanks for your detailed answer, I'll have a try.", "@mrry I have the same problem that one worker got stuck at trying to connect to 3 parameter servers (which are apparently running and connected to all other workers). There are other workers on the same machine as the problematic one, and they are all connected to the ps. Also ports are assign to all servers as expected.\r\n\r\nThis behavior happens quite often (especially when num of servers and dataset is large) and the worker number, ps number or the machine number in problem are apparently random.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue? Please update the label and/or status accordingly.", "Closing this issue due to lack of activity. @nolanliou Hopefully things are working now, but feel free to reopen if you can reproduce with a simpler example. @OscarDPan We upgraded to a version of gRPC that fixes a big deadlock bug (that happened more frequently at higher load) in the latest nightly build (will be in TF 1.5), so you might want to try upgrading to see if that fixes the connectivity problems. If it doesn't, please open a new issue with the relevant details.", "I meet the same problem, after several hours debug, i found that because the order of the cluster_spec is incorrect, in other word, the  task_index miss match with the ps/worker list, after i change the order, it fixed....", "I meet the same problem, and my ps server and worker server correct match with ps/worker index. but there also have this problem.  \r\n```\r\nCreateSession still waiting for response from /job/ps/task/0\r\n```\r\ni check ps and worker python process. they all run in correct way. \r\nany idea about this issue\uff1f thanks ", "@mrry I've the same issue! posted a question on stack overflow with the code that I've tried https://stackoverflow.com/questions/51219374/tensorflow-distributed-createsession-still-waiting-for-response-from-worker-j\r\n\r\nI've just tried to run both ps and worker on the same cluster and yet still getting the same message! any idea how to fix/debug that?", "I've the same issue now.Did you solve it?", "Yes, my problem was kinda silly! I ssh to the remote server using `user@example.com` and that what I was using to define the cluster IP address in Tensorflow, but it turned out the I should be only using `example.com` and the problem was solved after that!\r\n\r\nanother thing that could cause the problem is that you make sure the task id matches with the cluster IP address, for simplicity try with one ps and one worker that both are on the same machine and see if it works with you.", "I've the same issue\r\n\r\nchief restart when worker completed will  cause the problem\r\n\r\nany ideas\uff1f", "@mrry I have the following error:\r\nCreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\nIs there any suggestion for this error?", "It's not solved yet. The same issue running a training job in Google Cloud ML using runtimeVersion 1.12. As the number of workers increase the probability of get stuck in this problem increase... I also use a large dataset.\r\n\r\n```\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:3 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:4 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:5 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:7 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:8 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:9 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:10 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:11 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:12 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:13 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:14 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:15 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:16 I  worker-replica-6\r\n worker-replica-6 CreateSession still waiting for response from worker: /job:worker/replica:0/task:17 I  worker-replica-6\r\n```\r\n", "> Yes, my problem was kinda silly! I ssh to the remote server using `user@example.com` and that what I was using to define the cluster IP address in Tensorflow, but it turned out the I should be only using `example.com` and the problem was solved after that!\r\n> \r\n> another thing that could cause the problem is that you make sure the task id matches with the cluster IP address, for simplicity try with one ps and one worker that both are on the same machine and see if it works with you.\r\n\r\nthere are always 2 manchine no matter 8 workers or 6 works, and the IP of them are not fixed.\r\nhave you fixed the issue? thanks a lot.", "> I meet the same problem, after several hours debug, i found that because the order of the cluster_spec is incorrect, in other word, the task_index miss match with the ps/worker list, after i change the order, it fixed....\r\n\r\nCould you elaborate on this? I am getting the same error and I was hoping to fix it", "I met the same problem in kubeflow, due to network problems.\r\nI finally solved this by following steps,\r\n1) fix flannel service, and make sure network between each ps/worker or worker/worker pair is reachable. you can check that by `curl <ip>:<port>` in each tfjob pod;\r\n2) fix dns prolem, and make make sure `curl <tfjob_name>:<port>` between each pair tfjob pods also works. you should check that all dns service and pods are work fine and dns ip you pass to kubelet.service is the same as the cluster ip you get by `kubectl get service -n kube-system|grep dns` ", "@chansonzhang \r\nI use command curl <tfjob_name>:<port> in each pod and it report \"curl: (7) Failed to connect to 192.168.2.53 port 80: Connection refused\",  How to solve this please?", "> @chansonzhang\r\n> I use command curl <tfjob_name>: in each pod and it report \"curl: (7) Failed to connect to 192.168.2.53 port 80: Connection refused\", How to solve this please?\r\n\r\nseems like your flannel service is not ok"]}, {"number": 12744, "title": "a", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n", "comments": ["I am embarrassed by my previous behavior, I am very sorry."]}, {"number": 12743, "title": "Gradients with TensorArray fail after scatter of 0 size", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nThe following fails:\r\n```\r\nimport tensorflow as tf\r\n\r\nx_orig = tf.zeros([3], tf.float32)\r\n\r\nta = tf.TensorArray(tf.float32, size=1, element_shape=[3])\r\nta = ta.write(0, x_orig)\r\nta = ta.scatter(tf.range(0, 0), ta.gather(tf.range(0, 0)))\r\nx = ta.read(0)\r\ng, = tf.gradients(tf.reduce_sum(x), [x_orig])\r\n\r\nsess = tf.Session()\r\nprint sess.run(g)\r\n```\r\nwith this error:\r\n```\r\ntensorflow.python.framework.errors_impl.UnimplementedError: TensorArray has size zero, but element shape <unknown> is not fully defined. Currently only static shapes are supported when packing zero-size TensorArrays.\r\n         [[Node: gradients/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGatherV3 = TensorArrayGatherV3[dtype=DT_FLOAT, element_shape=<unknown>, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](gradients/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/TensorArrayGradV3, range, gradients/TensorArrayScatter/TensorArrayScatterV3_grad/TensorArrayGrad/gradient_flow)]]\r\n```\r\nThe above code works if `ta = ta.scatter(tf.range(0, 0), ta.gather(tf.range(0, 0)))`, which gathers a 0x3 tensor and then scatters it into zero indices in `ta`, is removed.\r\n\r\nThis seems to happen because `_TensorArrayScatterGrad` does not (and cannot?) copy over the `element_size` from the Python `TensorArray` object that `.scatter` was called on.\r\n\r\nI submitted #12742 as a fix, which works for the above snippet..", "comments": ["Assigning to @ebrevdo who is reviewing the related PR.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "That CL is stalled; please feel free to reopen when the CL is resubmitted."]}, {"number": 12742, "title": "In TensorArrayPackOrGatherOp, handle num_indices == 0 better.", "body": "Namely, also consider the shape stored in the TensorArray object, since TensorArrayGatherV3 ops created as part of gradient computation won't have the `element_shape` attr set.\r\n\r\nFixes #12743.", "comments": ["Can one of the admins verify this patch?", "@rshin, thanks for your PR! By analyzing the history of the files in this pull request, we identified @ebrevdo, @tensorflower-gardener and @yuanbyu to be potential reviewers.", "This is supposed to address #12743.", "Thanks! I think the next step is for @ebrevdo to take a look.\r\n\r\nJenkins, test this please.", "@rshin ping", "@rshin, any update?", "Closing as stalled. Please reopen a new PR if there is progress."]}, {"number": 12741, "title": "Export C API symbols of _pywrap_tensorflow_internal.so on OS X", "body": "This follows up #10469 by exporting C API symbols on OS X.\r\n\r\n## How?\r\n\r\nThis PR widens the `.lds` matching pattern to include the C API symbols as they are mangled on OS X.\r\n\r\nBefore this PR the C API symbols are exported in Linux but are hidden in OS X:\r\n```sh\r\n(Linux)$ nm _pywrap_tensorflow_internal.so | grep TF_CloseSession\r\n00000000012d2d40 T TF_CloseSession\r\n00000000010f8e50 t _wrap_TF_CloseSession\r\n\r\n(OS X)$ nm _pywrap_tensorflow_internal.so | grep TF_CloseSession\r\n00000000002be1b0 t _TF_CloseSession\r\n000000000001a0a0 t __Z21_wrap_TF_CloseSessionP7_objectS0_\r\n```\r\nAfter this PR the C API symbols are exported in both Linux and OS X:\r\n```sh\r\n(Linux)$ nm _pywrap_tensorflow_internal.so | grep TF_CloseSession\r\n0000000000fa25f0 T TF_CloseSession\r\n0000000000cf02e0 t _wrap_TF_CloseSession\r\n\r\n(OS X)$ nm _pywrap_tensorflow_internal.so | grep TF_CloseSession\r\n0000000000392880 T _TF_CloseSession\r\n000000000001ae50 t __Z21_wrap_TF_CloseSessionP7_objectS0_\r\n```\r\n(notice the change from `t` to `T`, i.e. static to global)\r\n\r\n## Why?\r\n\r\nThis is important for libraries that use multiple client APIs as described in #7541. My use case is [an R package](https://github.com/nimble-dev/nimble) that uses the Python API through [rstudio/tensorflow](https://github.com/rstudio/tensorflow) and also uses the C API library: I build tf graphs using the Python API and run those graphs from R-wrapped C++ code using the C API. This already works in Linux, and should also work in OS X after this PR.\r\n\r\n## Tested\r\n\r\nSuccessfully loaded [a shared library](https://github.com/nimble-dev/nimble/tree/devel/packages/nimble/inst/CppCode/tensorflow.cpp) in R that uses symbols from `_pywrap_tensorflow_internal.so`:\r\n- [x] Linux\r\n- [x] OS X", "comments": ["Can one of the admins verify this patch?", "@fritzo, thanks for your PR! By analyzing the history of the files in this pull request, we identified @aselle, @tensorflower-gardener and @keveman to be potential reviewers.", "I believe we are going another way, and will provide a separate DLL you can link against to get to the C API, but @allenlavoie to confirm.", "@martinwicke @allenlavoie Can you confirm that #7541 has been resolved, so that both DLLs can be loaded at the same time? If not, could this PR be merged as a temporary workaround? ", "We are planning to split out a shared object, which precompiled op libraries will depend on, and remove RTLD_GLOBAL when loading pywrap_tensorflow. This shared object will not include the C API symbols (the C API relies on several ops, and we'd like to keep ops and kernels out of this new shared object). Both the C API and pywrap_tensorflow will depend on this new shared object, but trying to load both of them will attempt to register ops and kernels twice and throw an error.\r\n\r\nSo this PR is probably your best bet medium-term for adding Python API calls to an otherwise C API usage. Removing RTLD_GLOBAL will mean that you'll need to explicitly link against _pywrap_tensorflow_internal.so for the C API symbols (you don't need to do it now, but I'd recommend it to avoid breaking in the future).", "Thanks, @allenlavoie !", "Jenkins, test this please", "Looks like there are some conflicts. @fritzo do you mind resolve them? Thanks!", "@tensorflow-jenkins test this please", "@yifeif Thanks, I've resolved the conflict. To resolve the conflict I've also added `*` to the `TFE_` symbols exported by #12785 discussed in #12784, so that change will also work in OS X (this is the same issue I had)."]}, {"number": 12740, "title": "[Feature Request]: fold batch norm into convolution weights of graph transform tool ", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.5.1\r\n- **CUDA/cuDNN version**: 8.0, 6.0\r\n- **GPU model and memory**: GTX1080\r\n- **Exact command to reproduce**:\r\n\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=tensorflow_inception_graph.pb \\\r\n--out_graph=optimized_inception_graph.pb \\\r\n--inputs='Mul' \\\r\n--outputs='softmax' \\\r\n--transforms='\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms'\r\n\r\nAfter diving into the generated pb graph, I found that the current fold batch norm ops seems to only fold batch norm into two ops: mul and add, which is not exactly as the readme demonstrated: fold the batch norm into the weights of convolution. If this is the case, it would be better to further merge the mul and add into conv weights.", "comments": ["On a similar note, what about folding [this](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LayerNormBasicLSTMCell) as well? I.e. fold across `array_ops.split`.", "fold_batch_norms/fold_old_batch_norms merges the scale part of a (fused) batch normalization into conv_op only if the bn op is trailing a conv op. Otherwise nothing is did.\r\nAnd the tool run as expected in this case. You can check its implementation.", "It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.", "resolved", "@ydzhang12345  could you pls share how do you resolve the problem?", "ANy Update for that? Iwanna know if you remove the batchnorm how do you finetune the network?", "We manually optimize these in C."]}, {"number": 12739, "title": "TF 1.3 keras TimeDistributed wrapper issue - rnn() got an unexpected keyword argument 'input_length'", "body": "When I use TimeDistributed  wrapper from keras I'm getting unexpected keyword argument 'input_length'\r\n\r\n# **System Info:**\r\n\r\n**Windows 10**\r\n**TF 1.3.0**\r\n**Python 3.5**\r\n\r\n**Code :**\r\n\r\n```python\r\nimport json\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nfrom tensorflow.contrib.keras import layers\r\nfrom tensorflow.contrib.keras.python.keras.layers.wrappers import TimeDistributed\r\nfrom tensorflow.python.estimator.inputs import numpy_io\r\n\r\nMAX_NB_WORDS = 200000\r\nMAX_SEQUENCE_LENGTH = 25\r\nEMBEDDING_DIM = 300\r\nVALIDATION_SPLIT = 0.1\r\nTEST_SPLIT = 0.1\r\nRNG_SEED = 13371447\r\nNB_EPOCHS = 25\r\nDROPOUT = 0.1\r\nBATCH_SIZE = 32\r\n\r\ntf.logging.set_verbosity(tf.logging.INFO)\r\n\r\nQ1_TRAINING_DATA_FILE = 'gen/q1_train.npy'\r\nQ2_TRAINING_DATA_FILE = 'gen/q2_train.npy'\r\nLABEL_TRAINING_DATA_FILE = 'gen/label_train.npy'\r\nNB_WORDS_DATA_FILE = 'gen/nb_words.json'\r\nwith open(NB_WORDS_DATA_FILE, 'r') as f:\r\n    nb_words = json.load(f)['nb_words']\r\n\r\n\r\ndef model_fn(features, labels, mode, params):\r\n    input_data = features['x']\r\n\r\n    Q1_Data = input_data[:, 0]\r\n    Q2_Data = input_data[:, 1]\r\n\r\n    q1 = layers.Embedding(nb_words + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(Q1_Data)\r\n    q1 = tf.contrib.keras.layers.TimeDistributed(layers.Dense(EMBEDDING_DIM, activation='relu'))(q1)\r\n    q1 = layers.Lambda(lambda x: tf.reduce_max(x, axis=1, keep_dims=False))(q1)\r\n\r\n    q2 = layers.Embedding(nb_words + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(Q2_Data)\r\n    q2 = tf.contrib.keras.layers.TimeDistributed(layers.Dense(EMBEDDING_DIM, activation='relu'))(q2)\r\n    q2 = layers.Lambda(lambda x: tf.reduce_max(x, axis=1, keep_dims=False))(q2)\r\n\r\n    merged = layers.concatenate([q1, q2])\r\n    # merged = layers.Flatten()(merged)\r\n    merged = layers.Dense(200, activation='relu')(merged)\r\n    merged = tf.layers.dropout(merged, rate=DROPOUT)\r\n    merged = layers.Dense(200, activation='relu')(merged)\r\n    merged = tf.layers.dropout(merged, rate=DROPOUT)\r\n    merged = layers.Dense(200, activation='relu')(merged)\r\n    merged = tf.layers.dropout(merged, rate=DROPOUT)\r\n    merged = layers.Dense(200, activation='relu')(merged)\r\n    merged = tf.layers.dropout(merged, rate=DROPOUT)\r\n\r\n    predictions = layers.Dense(1)(merged)\r\n\r\n    predictions = tf.reshape(predictions, [-1])\r\n\r\n    train_op = None\r\n    eval_metric_ops = None\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode=mode,\r\n            predictions={\"duplicate\": predictions})\r\n\r\n    loss = tf.losses.sigmoid_cross_entropy(labels, predictions)\r\n    optimizer = tf.train.AdamOptimizer()\r\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\r\n\r\n    eval_metric_ops = {\r\n        \"accuracy\": tf.metrics.accuracy(labels, predictions)\r\n    }\r\n\r\n    print(predictions)\r\n\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)\r\n\r\n\r\nprint('\\n')\r\nprint('Loading Numpy inputs')\r\nq1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))\r\nq2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))\r\nlabels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\r\n\r\nX = np.stack((q1_data, q2_data), axis=1)\r\ny = labels\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\r\n\r\nprint('\\n')\r\nprint('Preparing Numpy Input_Fn for both train and test')\r\ntrain_input_fn = numpy_io.numpy_input_fn(x={'x': X_train}, y=y_train, shuffle=True, batch_size=BATCH_SIZE,\r\n                                         num_epochs=None)\r\n\r\ntest_input_fn = numpy_io.numpy_input_fn(x={'x': X_test}, y=y_test, shuffle=False, batch_size=BATCH_SIZE, num_epochs=1)\r\n\r\nnn = tf.estimator.Estimator(model_fn=model_fn, params=None, model_dir='build/')\r\n\r\nprint('\\n')\r\nprint('Training...............')\r\nnn.train(input_fn=train_input_fn, steps=100)\r\n\r\nprint('\\n')\r\nprint('Training Complete, Evaluating............')\r\nev = nn.evaluate(input_fn=test_input_fn)\r\nprint(\"Loss: %s\" % ev[\"loss\"])\r\nprint(\"Accuracy: %s\" % ev[\"accuracy\"])\r\n\r\n```\r\n\r\n**Exception:**\r\nTraceback (most recent call last):\r\n  File \"D:/PlayGround/Git/Coding2Fun/Blog/DeepLearning/Quora-NLP/model.py\", line 103, in <module>\r\n    nn.train(input_fn=train_input_fn, steps=100)\r\n  File \"D:\\Programs\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 241, in train\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"D:\\Programs\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 630, in _train_model\r\n    model_fn_lib.ModeKeys.TRAIN)\r\n  File \"D:\\Programs\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 615, in _call_model_fn\r\n    model_fn_results = self._model_fn(features=features, **kwargs)\r\n  File \"D:/PlayGround/Git/Coding2Fun/Blog/DeepLearning/Quora-NLP/model.py\", line 38, in model_fn\r\n    q1 = tf.contrib.keras.layers.TimeDistributed(layers.Dense(EMBEDDING_DIM, activation='relu'))(q1)\r\n  File \"D:\\Programs\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\engine\\topology.py\", line 396, in __call__\r\n    output = super(Layer, self).__call__(inputs, **kwargs)\r\n  File \"D:\\Programs\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 450, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"D:\\Programs\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\keras\\python\\keras\\layers\\wrappers.py\", line 208, in call\r\n    unroll=False)\r\n**TypeError: rnn() got an unexpected keyword argument 'input_length'**", "comments": ["@fchollet It looks like a bug fix to [wrappers.py](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/keras/python/keras/layers/wrappers.py) made in https://github.com/tensorflow/tensorflow/commit/fa83a270c943317da6b07a3d093c224be0827bd9 might need to be cherry-picked onto r1.3.", "This has been fixed in the new release."]}, {"number": 12738, "title": "Fix step numbering in Linux install docs", "body": "On this page the numbering for the steps seems to be off:\r\nhttps://www.tensorflow.org/install/install_linux#InstallingVirtualenv", "comments": ["@jeffcarp, thanks for your PR! By analyzing the history of the files in this pull request, we identified @tensorflower-gardener, @av8ramit and @Androbin to be potential reviewers.", "Can one of the admins verify this patch?"]}, {"number": 12737, "title": "Merge pull request #1 from tensorflow/master", "body": "merge", "comments": ["Can one of the admins verify this patch?", "Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed, please reply here (e.g. `I signed it!`) and we'll verify.  Thanks.\n\n---\n\n- If you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n- If your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\n- In order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n<!-- need_sender_cla -->"]}, {"number": 12736, "title": "Tensorflow 1.3 with Python 3.6.2 under Windows 10 64 Bit OS has issue when run tensorflow/tensorflow/examples/image_retraining/label_image.py", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 X64 Enterprise Edition\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**: Anaconda 4.4.0 Python 3.6.2\r\n- **Bazel version (if compiling from source)**: no\r\n- **CUDA/cuDNN version**: No\r\n- **GPU model and memory**: No\r\n- **Exact command to reproduce**:\r\n(tensorflow13) C:\\Users\\James\\Tensorflow\\model-retrain\\tensorflow-for-poets-2\\scripts>python .\\label_image.py --image c:\\Users\\James\\Tensorflow\\sample_img\\Panda001.jpg --graph c:\\Users\\James\\Tensorflow\\model-retrain\\tensorflow-for-poets-2\\scripts\\retrained_graph.pb --labels C:\\Users\\James\\Tensorflow\\model-retrain\\tensorflow-for-poets-2\\scripts\\retrained_labels.txt\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nError Log:\r\n2017-09-01 09:27:46.902115: I C:\\tf_jenkins\\home\\workspace\\nightly-win\\M\\windows\\PY\\36\\tensorflow\\core\\platform\\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2\r\nTraceback (most recent call last):\r\n  File \".\\label_image.py\", line 120, in <module>\r\n    input_operation = graph.get_operation_by_name(input_name);\r\n  File \"C:\\Users\\James\\AppData\\Local\\conda\\conda\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3225, in get_operation_by_name\r\n    return self.as_graph_element(name, allow_tensor=False, allow_operation=True)\r\n  File \"C:\\Users\\James\\AppData\\Local\\conda\\conda\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3097, in as_graph_element\r\n    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n  File \"C:\\Users\\James\\AppData\\Local\\conda\\conda\\envs\\tensorflow13\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3157, in _as_graph_element_locked\r\n    \"graph.\" % repr(name))\r\nKeyError: \"The name 'import/input' refers to an Operation not in the graph.\"\r\n\r\n", "comments": ["I am having this issue as well. I am using WinPython rather than Anaconda. My Tensorflow version is also 1.3 and Python 3.6.2. \r\n\r\n**UPDATE:** My implementation is now working after changing the lines 78-79 in label_image.py from:\r\n\r\n```\r\ninput_layer = \"input\"\r\noutput_layer = \"InceptionV3/Predictions/Reshape_1\"\r\n```\r\n\r\nto:\r\n\r\n```\r\ninput_layer = \"Mul\"\r\noutput_layer = \"final_result\"\r\n```\r\n\r\nI am not sure why they were set to those other values to begin with - as far as I can tell, they are not valid operations.", "Also see resolution in #12815 ([comment](https://github.com/tensorflow/tensorflow/issues/12815#issuecomment-327789980)).", "Hello ,Everyone\r\nIs there anybody who had ever run the code label_image.py in tensorflow/tensorflow/examples/label_image/label_image.py\r\nI have modify it to run on a dataset and read and calssify image one by one,and as the number of images goes,the speed is slower and slower,at first,that's about ten images per second,and when the number of image goes to 1000,the time is about 7s,Incredibly! I guess the matter is memorry leak?!\r\nAnd I find the problem is in the function read_tensor_from_image_file in label_image.py and this part is read and preprocess images, so what's the matter?So I want to know how to speed up? Still how to modify the code so as to making it run for batches ? @drpngx @strategist922 @astewartau ", "I got the same issue as @aureosun got. \r\nAny suggestions would be appreciated.\r\n\r\nThanks", "@astewartau You are my hero! I've been trying to crack this mystery for way too long now and this fixed all my sorrows! ", "Hi,\r\n\r\nI've synced the [examples/label_image](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/label_image.py) script into the [tensorflow-for-poets-2 version](https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/label_image.py) and deleted the duplicate that was in examples/image_retraining. \r\n\r\nCurrently the only difference is the default `input_layer` and `output_layer` names. The `examples` version is set for the inception v3 checkpoint and the codelab version is set for the retrained mobilenet.\r\n\r\nSome of the confusion here is probably caused by slippage between the main version of the tutorial (1.4)  and people using the master branch of the git clone. This is fixed in the [master version](https://www.tensorflow.org/versions/master/tutorials/image_retraining) of the tutorial. A fix is inflight to add a versioned link to the tutorial to help people use the matching version.", "@astewartau Thank you very much for helping me solving the problem! But I am still confusing about the reason inside it.", "Thanks for the input_layer=\"Mul\" hint, saved me. \r\n\r\nI tried to display the node names to find precisely that. I only got some nodes for the picture feeding and similar, nothing for the inception model. Anyone knows how to find them so I am not reliant on random websearches?", "In TensorBoard? double click on boxes to see inside.", "hey, guys, after I retrained Inception-V3 using my own data. when i use label_image.py to test, it comes that my input layer and output layer is not right. I changed them. \r\nbut then another problem comes, anybody knows that:\r\n\r\nInvalidArgumentError: \r\nNodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; \r\nattr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; \r\nattr=padding:string,allowed=[\"SAME\", \"VALID\"]; \r\nattr=data_format:string,default=\"NHWC\",allowed=[\"NHWC\", \"NCHW\"]>; \r\n\r\nNodeDef: import/conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_import/Mul_0_0/_1, import/conv/conv2d_params). \r\n(Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n\r\n[[Node: import/conv/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 2, 2, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](_arg_import/Mul_0_0/_1, import/conv/conv2d_params)]]\r\n\r\nThanks for helping!!!", "Please try using the code from the branch your TF is built from.\r\nLike if you are using 1.5, use the example code from the r1.5 branch.", "@gunan Thank you bro. you said right. my model trained on other tensorflow 1.5 sever machine, but when i tested on my tensorflow 1.5 computer. so it goes wrong. after upgrading tensorflow to 1.5, this problem solved. ", "We me input layer \"Placeholder\" works as well. It would good to know what is the official recommended layer though.. @drpngx @gunan  ?", "This did not work for me is there any other solution for this ?\r\nI changed my code as following \r\n\r\n```\r\n  input_height = 299\r\n  input_width = 299\r\n  input_mean = 0\r\n  input_std = 299\r\n  input_layer = \"Mul\"\r\n  output_layer = \"final_result\"\r\n\r\n  input_name = \"import/\" + input_layer\r\n  output_name = \"import/\" + output_layer\r\n  input_operation = graph.get_operation_by_name(input_name)\r\n  output_operation = graph.get_operation_by_name(output_name)\r\n\r\n```\r\n\r\nPlease suggest me  a solution \r\nThank you\r\n", "i change   input_layer = \"Mul\"\r\nto input_layer = \"Placeholder\"  it work"]}, {"number": 12735, "title": "Magic name directories prevent to embed the mkl library", "body": "For some reasons the build_pip_package script sues the magic directory `solib_k8`. \r\nThat directory does not exists if you compile mkl on a vm machine, where the libraries are added under `_solib_local`. \r\n\r\n```\r\nfabriziomilo@instance-2:~/ml-models/tensorflow$ ag solib_k8\r\ntensorflow/tools/pip_package/build_pip_package.sh\r\n94:      mkdir \"${TMPDIR}/_solib_k8\"\r\n96:                     bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl \\\r\n97:        \"${TMPDIR}/_solib_k8\"\r\n110:      if [ -d bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl ]; then\r\n111:        mkdir \"${TMPDIR}/_solib_k8\"\r\n113:          bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl \\\r\n114:          \"${TMPDIR}/_solib_k8\"\r\n127:      if [ -d bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl ]; then\r\n128:        mkdir \"${TMPDIR}/_solib_k8\"\r\n130:                            bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl \\\r\n131:          \"${TMPDIR}/_solib_k8\"\r\n\r\ntensorflow/tools/pip_package/setup.py\r\n159:matches += ['../' + x for x in find_files('*', '_solib_k8') if '.py' not in x]\r\nfabriziomilo@instance-2:~/ml-models/tensorflow$ find ./bazel-bin/ -iname \\*mkl\\*.so\r\n./bazel-bin/tensorflow/tensorboard/tensorboard.runfiles/org_tensorflow/_solib_local/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl/libmklml_intel.so\r\n./bazel-bin/tensorflow/tools/pip_package/simple_console.runfiles/org_tensorflow/_solib_local/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl/libmklml_intel.so\r\n./bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_local/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl/libmklml_intel.so\r\n./bazel-bin/_solib_local/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uthird_Uparty_Smkl/libmklml_intel.so```", "comments": ["Thanks for the report. I will look into this right away.\r\n\r\nSome more information for me to reproduce the issue on my machine.\r\nAre you using a Cloud VM, docker container or a virtualbox VM on your machine?", "I am using a google cloud vm, standard n1 32 cores, us-east1-d ", "what are the exact commands you used to build?\r\nAlso the OS you installed?\r\nI created an n1-standard-32 instance in us-central1-f, with ubuntu xenial, and I can find the .so files under solib_k8.\r\n\r\n```\r\ngunan@gunan-test-cpu:~/workspace/tensorflow$ ls  bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/\r\n__init__.py  _U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib\r\ngunan@gunan-test-cpu:~/workspace/tensorflow$ ls  bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/_solib_k8/_U_S_Sthird_Uparty_Smkl_Cintel_Ubinary_Ublob___Uexternal_Smkl_Slib\r\n__init__.py  libiomp5.so  libmklml_intel.so\r\n```\r\n\r\n@damienmg based on what do we construct the name of the `solib_k8` directory?", "@gunan I think is [based on the cpu](https://github.com/tensorflow/tensorflow/search?utf8=%E2%9C%93&q=k8). \r\nI did build also with xenial. What env configuration did you use? \r\n\r\nthese are mines:\r\n```\r\nexport TF_NEED_CUDA=1\r\nexport TF_CUDA_VERSION=8.0\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=6.1\r\nexport TF_CUDNN_VERSION=6\r\nexport CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu\r\nexport PYTHON_BIN_PATH=`which python2`\r\nexport TF_NEED_GCP=1\r\nexport TF_NEED_HDFS=0\r\nexport TF_ENABLE_XLA=1\r\nexport TF_NEED_VERBS=1\r\nexport CC_OPT_FLAGS=\"-mavx -msse4.2 -mfpmath=both -DEIGEN_USE_VML\"\r\nexport TF_NEED_MKL=1\r\nexport TF_DOWNLOAD_MKL=1\r\nexport N_JOBS=$($PYTHON_BIN_PATH -c 'import multiprocessing;print(multiprocessing.cpu_count()*2)')\r\n\r\nexport HAS_AVX2=$(grep avx2 /proc/cpuinfo)\r\n\r\nif [ -z $HAS_AVX2 ]; then\r\n  echo \"no avx2 instructions found\";\r\nelse\r\n  export CC_OPT_FLAGS=\"-mavx2 -mfma ${CC_OPT_FLAGS}\"\r\nfi\r\n\r\nyes \"\" | ./configure\r\n```\r\n\r\n\r\n\r\n\r\n", "Yes this is based on the --cpu, /cc @mhlopko for more comment on the solib", "@Mistobaan Could you see if https://github.com/tensorflow/tensorflow/pull/12975 fixes the problem for you? @mhlopko does the PR look like it is a valid solution?", "Yup, it does look like it solves the problem. It's not elegant, but that the bazel problem."]}]