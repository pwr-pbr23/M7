[{"number": 17758, "title": "tensorflow.contrib.mpi import fails even though tensorflow is compiled with mpi", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.9.85-37.55.amzn1.x86_64 (centOS)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**: 4.8.5\r\n- **CUDA/cuDNN version**: 9.0/7.0\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: import tensorflow.contrib.mpi\r\n\r\n### Describe the problem\r\nI have compiled tensorflow from source and used the following configuration - \r\n\r\n```\r\nexport JAVA_HOME=/usr/java/jdk1.8.0_121/\r\nexport TF_NEED_JEMALLOC=1\r\nexport TF_NEED_GCP=0\r\nexport TF_NEED_HDFS=1\r\nexport TF_NEED_S3=1\r\nexport TF_ENABLE_XLA=0\r\nexport TF_NEED_GDR=0\r\nexport TF_NEED_VERBS=0\r\nexport TF_NEED_OPENCL_SYCL=0\r\nexport TF_NEED_COMPUTECPP=0\r\nexport TF_NEED_CUDA=1\r\nexport CUDA_TOOLKIT_PATH=/usr/local/cuda\r\nexport CUDNN_INSTALL_PATH=/usr/local/cuda\r\nexport TF_CUDA_COMPUTE_CAPABILITIES=3.0,3.5,5.2\r\nexport TF_CUDA_CLANG=0\r\nexport TF_NEED_MPI=1\r\nexport MPI_HOME=/usr/local/mpi\r\nexport GCC_HOST_COMPILER_PATH=$(which gcc)\r\nexport CC_OPT_FLAGS=\"-march=native\"\r\nexport TF_NEED_MKL=0\r\nexport TF_DOWNLOAD_MKL=0\r\nexport TF_CUDA_VERSION=\"$(nvcc --version | sed -n 's/^.*release \\(.*\\),.*/\\1/p')\"\r\nexport TF_CUDNN_VERSION=\"$(sed -n 's/^#define CUDNN_MAJOR\\s*\\(.*\\).*/\\1/p' $CUDNN_INSTALL_PATH/include/cudnn.h)\"\r\nexport PYTHON_BIN_PATH=/usr/lib/python/bin/python\r\nexport PYTHON_LIB_PATH=\"$($PYTHON_BIN_PATH -c 'import site; print(site.getsitepackages()[0])')\"\r\n\r\nexport OMPI_SKIP_MPICXX=1\r\nexport CC_OPT_FLAGS=\"-DOMPI_SKIP_MPICXX=1 -march=native\"\r\n```\r\nI installed tf using the wheel file generated after compiling. However, when I try to run `import tensorflow.contrib.mpi` it throws an import error -\r\n\r\n`ImportError: No module named 'tensorflow.mpi'`\r\n", "comments": ["/CC @poxvoculi", "Perhaps @jbedorf has some advice.", "This is the mpi_collectives that fails to import. When I tried this in the past (months ago), I found that bazel was not correctly passing the `TENSORFLOW_USE_MPI`  define to the cc/h files. My fix at that point was just to add a `#define TENSORFLOW_USE_MPI` at the top of the files in this folder (and subfolders): `https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/mpi_collectives` \r\n\r\nYou might want to try that first, if that turns out to be the problem the proper fix would be somewhere in the `bazel` configuration"]}, {"number": 17757, "title": "No OpKernel was registered to support Op 'L2Loss' with these attrs", "body": "Hello everyone!\r\n\r\n I'm having the following error when trying to run an iOS app I've built with tensorflow:\r\n\r\nNon-OK-status: session->Create(graph_def) status: Invalid argument: **No OpKernel was registered to support Op 'L2Loss' with these attrs**.  Registered devices: [CPU], **Registered kernels:\r\n  <no registered kernels>**\r\n\r\n\t [[Node: L2Loss_1 = L2Loss[T=DT_FLOAT](sub_1)]]\r\n\r\n\r\n I've build tensorflow using the ./build_all_ios.sh process.\r\n\r\n From my research I already tried adding the flag -DANDROID_TYPES=__ANDROID_TYPES_FULL__ to the build_all_ios.sh like this:\r\n\r\nTF_CC_FLAGS=\"-O3 -DANDROID_TYPES=__ANDROID_TYPES_FULL__\"\r\n\r\n But I still have the same issue.\r\n\r\n I was having even more errors before but I added a couple .cc files to tf_op_files.txt and it fixed those previous issues.\r\n\r\n Do I have to add more .cc files ?\r\n Also isn't it weird that it says I have no registered kernels ?\r\n\r\n I'm using the latest master.\r\n\r\nThank you", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 59 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17756, "title": "Failed to load the native TensorFlow runtime.", "body": "```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\hari\\Desktop\\tf_object_detection_api\\models\\research\\object_detection\\object_detection_tutorial_CONVERTED.py\", line 16, in <module>\r\n    import tensorflow as tf\r\n  File \"C:\\Python27\\lib\\site-packages\\tensorflow\\__init__.py\", line 24, in <module>\r\n    from tensorflow.python import *\r\n  File \"C:\\Python27\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 72, in <module>\r\n    raise ImportError(msg)\r\nImportError: Traceback (most recent call last):\r\n  File \"C:\\Python27\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 66, in <module>\r\n    from tensorflow.python import pywrap_tensorflow\r\n  File \"C:\\Python27\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 28, in <module>\r\n    _pywrap_tensorflow = swig_import_helper()\r\n  File \"C:\\Python27\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 20, in swig_import_helper\r\n    import _pywrap_tensorflow\r\nImportError: No module named _pywrap_tensorflow\r\n\r\n\r\nFailed to load the native TensorFlow runtime.\r\n\r\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\r\n\r\nfor some common reasons and solutions.  Include the entire stack trace\r\nabove this error message when asking for help.\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We need more information about your system to debug this. Can you provide platform, how you built, etc?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n"]}, {"number": 17755, "title": "crf_decode fails when sequence_length is 0", "body": "Fix for #17746 ", "comments": ["tensorflow/contrib/crf/python/kernel_tests/crf_test.py:287: [C0301(line-too-long), ] Line too long (82/80)", "How should I interpret the current \"GPU CC\" failure? There is no details link as I saw in the past."]}, {"number": 17754, "title": "[bug] segmentation fault happens with nested higher order function", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.11.6\r\n- **TensorFlow installed from (source or binary)**: VirtualEnv\r\n- **TensorFlow version (use command below)**: 1.6\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A (CPU only)\r\n- **GPU model and memory**: N/A (CPU only)\r\n- **Exact command to reproduce**: see the following\r\n\r\n### Describe the problem\r\nsegmentation fault happens when the computation graph contains `scan` with `bidirectional_rnn` embedded. \r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nembed_dim = 10\r\nhidden_dim = 10\r\nnum_class=10\r\n\r\nwords = tf.placeholder(tf.int32, [None, None], name='words')\r\nlength = tf.placeholder(tf.int32, [None], name='length')\r\nlabels = tf.placeholder(tf.int32, (), name='labels')\r\ninit_state = tf.placeholder(tf.float32, [None, hidden_dim * 2], name='initial_state')\r\n\r\ndef make_rnn_cell(): return tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n\r\nwith tf.variable_scope('embeddings'):\r\n    embedding = \\\r\n        tf.get_variable('parameter', shape=(100, embed_dim), dtype=tf.float32, trainable=True)\r\n    embedded  = tf.nn.embedding_lookup(embedding, words, name='lookup')\r\nwith tf.variable_scope('words_lstm'):\r\n    cell_fw = make_rnn_cell()\r\n    cell_bw = make_rnn_cell()\r\n    def step(state, inp):\r\n        data = tf.expand_dims(inp[0], axis=0)\r\n        length = tf.expand_dims(inp[1], axis=0)\r\n        fw_state = tf.split(state[inp[1], :], 2)[0]\r\n        bw_state = tf.split(state[0, :], 2)[1]\r\nonline training (feeding one training example at a time)\r\n        fw_state = tf.expand_dims(fw_state, axis=0)\r\n        bw_state = tf.expand_dims(bw_state, axis=0)\r\n        (outputs_fw, outputs_bw), _ = \\\r\n            tf.nn.bidirectional_dynamic_rnn(\r\n                cell_fw, cell_bw, embedded, sequence_length=length,\r\n                initial_state_fw=fw_state, initial_state_bw=bw_state, dtype=tf.float32\r\n            )\r\n        outputs = tf.squeeze(tf.concat([outputs_fw, outputs_bw], axis=2), axis=[0])\r\n        return outputs\r\n    outputs = tf.scan(step, (embedded, length), initializer=init_state)\r\nwith tf.variable_scope('words_attention'):\r\n    hidden = \\\r\n        tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\r\n    attention = \\\r\n        tf.layers.dense(outputs, units=1, activation=None)\r\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 2, 1])), perm=[0, 2, 1])\r\nsentence_embedding = tf.reduce_sum(outputs * attention, axis=1)\r\nsentence_embedding = tf.expand_dims(sentence_embedding, axis=0)\r\n\r\nwith tf.variable_scope('sentence_lstm'):\r\n    cell_fw = make_rnn_cell()\r\n    cell_bw = make_rnn_cell()\r\n    (outputs_fw, outputs_bw), _ = \\\r\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, sentence_embedding, dtype=tf.float32)\r\noutputs = tf.squeeze(tf.concat([outputs_fw, outputs_bw], axis=2), axis=[0])\r\nwith tf.variable_scope('sentence_attention'):\r\n    hidden = \\\r\n        tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\r\n    attention = \\\r\n        tf.layers.dense(hidden, units=1, activation=None)\r\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention)))\r\noutputs = tf.reduce_sum(outputs * attention, axis=0)\r\noutputs = tf.expand_dims(outputs, axis=0)\r\nlogits = tf.layers.dense(outputs, units=num_class, activation=None)\r\nloss = -tf.log(tf.nn.softmax(logits)[:, labels], name='loss')\r\ntraining_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    words_val = np.random.randint(0, 100, size=(10, 100))\r\n    length_val = np.random.randint(0, 100, size=(10))\r\n    labels_val = np.random.randint(0, num_class, size=())\r\n    init_state_val = np.random.randn(10, hidden_dim * 2)\r\n\r\n    fd = { words : words_val, length : length_val, labels : labels_val, init_state : init_state_val}\r\n    sess.run(training_op, feed_dict=fd)\r\n```\r\n\r\nRunning with TF 1.6, the above code ended up with the following error:\r\n```bash\r\nSegmentation fault: 11\r\n```\r\n\r\nAdditional information,\r\nI ran the program with `dtruss`, the last few lines printed on console before it crashed are as follows,\r\n```bash\r\npsynch_cvsignal(0x7FDED4BC4A68, 0x90000000A00, 0x900)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4A68, 0x90100000A00, 0x900)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4E68, 0x1F0000002000, 0x1F00)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4E68, 0x1F0100002000, 0x1F00)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4E68, 0x200000002100, 0x2000)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4E68, 0x200100002100, 0x2000)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4E68, 0x210000002200, 0x2100)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4E68, 0x210100002200, 0x2100)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4C68, 0x1C0000001D00, 0x1C00)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4C68, 0x1C0100001D00, 0x1C00)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4A68, 0xA0000000B00, 0xA00)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4A68, 0xA0100000B00, 0xA00)\t\t = 0 0\r\npsynch_cvsignal(0x7FDED4BC4D68, 0xD0000000E00, 0xD00)\t\t = 257 0\r\npsynch_cvwait(0x7FDED4BC4D68, 0xD0100000E00, 0xD00)\t\t = 0 0\r\npsynch_cvwait(0x0, 0x0, 0x0)\t\t = 0 0\r\npsynch_cvwait(0x7FDED4BC4F68, 0x1B0100001C00, 0x1B00)\t\t = 0 0\r\npsynch_cvwait(0x7FDED4BC5068, 0x30100000400, 0x300)\t\t = 0 0\r\npsynch_cvwait(0x7FDED4BC5168, 0x20100000300, 0x200)\t\t = 0 0\r\npsynch_cvwait(0x7FDEDCAEFE28, 0x10100000200, 0x100)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4068, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4168, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4268, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4368, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4568, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4468, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4668, 0x100000100, 0x0)\t\t = -1 Err#260\r\npsynch_cvwait(0x7FDED4BC4768, 0x100000100, 0x0)\t\t = -1 Err#260\r\n```", "comments": ["@ebrevdo This seems to be another error related to nested higher order function, although this time I was running a `scan` with `bidirectional_rnn` embedded in the `step` function. Previous reports on `segmentation fault` happening on OSX platform seemed to focus on the erroneous link of cuda library. In my case, I only used the CPU. ", "Can you build TF with bazel build -c dbg, to get a proper stack trace?", "I tried to build TF 1.6 with `bazel -c dbg` on OSX 10.11.6, but failed with the attached error :-(\r\n\r\n```bash\r\nbazel build -c dbg //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n[error.log](https://github.com/tensorflow/tensorflow/files/1823183/error.log)\r\n\r\nusing another box with the same OS X 10.11.6 to build TF with bazel 0.5.4 ended with errors too, although it is different from the above one.\r\n\r\n[error.log](https://github.com/tensorflow/tensorflow/files/1827816/error.log)", "+Andrew Selle <aselle@google.com>\n\nOn Sun, Mar 18, 2018, 3:13 PM Sheng Chen <notifications@github.com> wrote:\n\n> I tried to build TF 1.6 with bazel -c dbg on OSX 10.11.6, but failed with\n> the attached error :-(\n>\n> bazel build -c dbg //tensorflow/tools/pip_package:build_pip_package\n>\n> error.log\n> <https://github.com/tensorflow/tensorflow/files/1823183/error.log>\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17754#issuecomment-374056211>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1bwt0aMy10FvNmiPhOStWOVkcqeks5tftwYgaJpZM4StNkA>\n> .\n>\n", "running in TF 1.7 docker suggests there is a misuse of variable in `bidirectional_rnn`. TF 1.6 does not have an assertion in place to capture the error while building the graph.", "For posterity, can you please paste the full error from the python side in\nTF 1.7?  It's clear there's still a segfault somewhere and we'd like to\nknow what it is so in the future a minor error doesn't bring down the\nentire process.\n\nOn Wed, Mar 21, 2018 at 9:38 AM, Sheng Chen <notifications@github.com>\nwrote:\n\n> Closed #17754 <https://github.com/tensorflow/tensorflow/issues/17754>.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/17754#event-1533684841>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimzRTcC4YZPVXt-JqMchlMJgT-U8uks5tgoIXgaJpZM4StNkA>\n> .\n>\n", "sure - \r\n\r\n```c\r\n2018-03-21 02:46:47.205165: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at strided_slice_op.cc:105 : Invalid argument: slice index 82 of dimension 0 out of bounds.\r\n[I 02:47:15.222 NotebookApp] Saving file at /Untitled.ipynb\r\n2018-03-21 02:49:45.312229: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at strided_slice_op.cc:105 : Invalid argument: slice index 76 of dimension 0 out of bounds.\r\n```\r\n\r\n```python\r\nInvalidArgumentError: assertion failed: [Expected shape for Tensor words_lstm/scan/while/bidirectional_rnn/fw/fw/sequence_length:0 is ] [10] [ but saw shape: ] [1]\r\n\t [[Node: words_lstm/scan/while/bidirectional_rnn/fw/fw/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](words_lstm/scan/while/bidirectional_rnn/fw/fw/All, words_lstm/scan/while/bidirectional_rnn/fw/fw/Assert/Assert/data_0, words_lstm/scan/while/bidirectional_rnn/fw/fw/stack, words_lstm/scan/while/bidirectional_rnn/fw/fw/Assert/Assert/data_2, words_lstm/scan/while/bidirectional_rnn/fw/fw/strided_slice_2/stack)]]\r\n```"]}, {"number": 17753, "title": "Quantize to TFLITE mode:how to change the graph to contain min/max information for relu layer.", "body": "== cat /etc/issue ===============================================\r\nLinux xxxx 4.13.0-36-generic #40~16.04.1-Ubuntu SMP Fri Feb 16 23:25:58 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux xxxxx 4.13.0-36-generic #40~16.04.1-Ubuntu SMP Fri Feb 16 23:25:58 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.1)\r\nnumpydoc (0.7.0)\r\nprotobuf (3.5.2)\r\ntensorflow (1.7.0rc0)\r\ntensorflow-tensorboard (1.5.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.7.0-rc0\r\ntf.GIT_VERSION = b'v1.7.0-rc0-2-ga6f8b22'\r\ntf.COMPILER_VERSION = b'v1.7.0-rc0-2-ga6f8b22'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/user/software_install/TensorRT-3.0.4/lib:/usr/local/cuda/lib64:\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nFri Mar 16 11:28:51 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   38C    P8     9W / 200W |    253MiB /  8110MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    0      1132      G   /usr/lib/xorg/Xorg                           139MiB |\r\n|    0      3053      G   compiz                                       111MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7\r\n\r\n++++++++++Problem+++++++++++\r\nI try to quantize my model. Computation is as follow:\r\npointwise_conv2d(relu(depthwise_conv2d(x)))\r\nThen, I use tf.contrib.quantize.create_eval_graph() to quantize graph. \r\nHowever, there is an error when I use lite.toco_convert() to convert model into tflite.\r\n\r\nI want to konw how to change the graph to contain min/max information for relu layer.\r\n\r\nMSG:\r\nArray conv2_depthwise_relu, which is an input to the Conv operator producing the output array conv2, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.\r\n\r\n[quant.zip](https://github.com/tensorflow/tensorflow/files/1817860/quant.zip)\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17752, "title": "Error:  Executor failed to create kernel. No registered 'Snapshot' OpKernel for GPU devices,while running image label example", "body": "### System information\r\n- OS Platform: Windows10\r\n- TensorFlow installed from source  (GPU support)\r\n- Python version: 3.5\r\n- Compiler version : Microsoft Visual Studio 2015 x64\r\n- CUDA/cuDNN version:CUDA9.0/cuDNN7.0\r\n- GPU: GTX 1070 8G\r\n-CMake settings:  \r\n![image](https://user-images.githubusercontent.com/13750472/37500096-84403974-2902-11e8-8e1d-2a540d3a03b5.png)\r\n\r\n### Problem\r\n  I successfully built tensorflow.lib and tensorflow.dll, then I linked these to my visual studio project.Everything goes well when I run the example code which does matrix multiplication (the example code on:https://joe-antognini.github.io/machine-learning/windows-tf-project)\r\n  But when I tried to run the Label_Image example code(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image/main.cc), I got the error:\r\n![image](https://user-images.githubusercontent.com/13750472/37500237-47decb84-2903-11e8-8bcd-670f4976fb66.png)\r\n`E C:\\tensorflow_gpu\\tensorflow\\tensorflow\\core\\common_runtime\\executor.cc:644] Executor failed to create kernel. Not found: No registered 'Snapshot' OpKernel for GPU devices compatible with node Subtract = Snapshot[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ResizeBilinear)\r\n    .  Registered:  device='CPU'; T in [DT_INT64]device='CPU'; T in [DT_INT32]\r\ndevice='CPU'; T in [DT_UINT16]\r\ndevice='CPU'; T in [DT_INT16]\r\ndevice='CPU'; T in [DT_UINT8]\r\ndevice='CPU'; T in [DT_INT8]\r\ndevice='CPU'; T in [DT_HALF]\r\ndevice='CPU'; T in [DT_BFLOAT16]\r\ndevice='CPU'; T in [DT_FLOAT]\r\ndevice='CPU'; T in [DT_DOUBLE]\r\ndevice='CPU'; T in [DT_COMPLEX64]\r\ndevice='CPU'; T in [DT_COMPLEX128]\r\ndevice='CPU'; T in [DT_BOOL]\r\n     [[Node: Subtract = Snapshot[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ResizeBilinear)]]\r\n2018-03-14 17:13:05.408206: E Image_Label.cpp:385] Not found: No registered 'Snapshot' OpKernel for GPU devices compatible with node Subtract = Snapshot[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ResizeBilinear)\r\n    .  Registered:  device='CPU'; T in [DT_INT64]\r\ndevice='CPU'; T in [DT_INT32]\r\ndevice='CPU'; T in [DT_UINT16]\r\ndevice='CPU'; T in [DT_INT16]\r\ndevice='CPU'; T in [DT_UINT8]\r\ndevice='CPU'; T in [DT_INT8]\r\ndevice='CPU'; T in [DT_HALF]\r\ndevice='CPU'; T in [DT_BFLOAT16]\r\ndevice='CPU'; T in [DT_FLOAT]\r\ndevice='CPU'; T in [DT_DOUBLE]\r\ndevice='CPU'; T in [DT_COMPLEX64]\r\ndevice='CPU'; T in [DT_COMPLEX128]\r\ndevice='CPU'; T in [DT_BOOL]\r\n     [[Node: Subtract = Snapshot[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](ResizeBilinear)]]`\r\n\r\nI found out the code stopped at:\r\n`TF_RETURN_IF_ERROR(session->Run({ inputs }, { output_name }, {}, out_tensors));`\r\nin\r\n`Status ReadTensorFromImageFile(const string& file_name, const int input_height,\r\n    const int input_width, const float input_mean,\r\n    const float input_std,\r\n    std::vector<Tensor>* out_tensors)`\r\n\r\nDoes anyone know how to fix this problem??\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow version\nBazel version\nGPU model and memory\nExact command to reproduce", "@tensorflowbutler \r\nHave I written custom code: no, using example code only\r\nOS Platform and Distribution: Windows10\r\nTensorFlow version: v1.7\r\nBazel version: N/A\r\nGPU model and memory:  nvidia geforce 1070 8G\r\nExact command to reproduce: N/A, the error code is from visual studio project after build and run the example code\r\n\r\nThanks!", "The error says that the SnapshotOp kernel is not available for GPU.  It's a relatively new kernel.  You said you compiled from source, so I believe this kernel should have compiled for GPU.  @rmlarsen do you see anything wrong with the application of this kernel?", "Same problem with Win10 and tf-nightly-gpu installed. After I uninstall the nightly package and reinstall the latest tensorflow and tensorflow-gpu packages, problem solved.", "Same problem here. I'm on Win10 and I've compiled this yesterday (19MAR) using latest sources from https://github.com/tensorflow/tensorflow.git. Built using cmake following instructions from \r\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake.\r\n\r\nThe full error is:\r\n2018-03-20 17:34:41.133280: I D:\\temp\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6522 MB memory) -> physical GPU (device: 0, name: GeForce GTX 880M, pci bus id: 0000:01:00.0, compute capability: 3.0)\r\n2018-03-20 17:34:53.281846: E D:\\temp\\tensorflow\\tensorflow\\tensorflow\\core\\common_runtime\\executor.cc:644] Executor failed to create kernel. Not found: No registered 'Snapshot' OpKernel for GPU devices compatible with node conv1/truncated_normal = Snapshot[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv1/truncated_normal/mul)\r\n", "@btud did you try the uninstall/reinstall suggested by @rancheng ?", "Is this problem only related to the tensorflow branch:master?\r\nHow can I get the lastest tensorflow-gpu packages besides cloning from https://github.com/tensorflow/tensorflow.git and build it with cmake and MCVS 2015 ?\r\n@rancheng Did you get the source code else where?", "@poxvoculi , I tried installing the prebuilt package (hope I understood correctly what @rancheng was suggesting) using \"pip3 install --upgrade tensorflow-gpu\". It seems the default tensorflow-gpu requires the exact 9.0 version of CUDA. I have 9.1 installed - this is what I built against.  So with prebuilt package It fails even for a hello world program - the error is: ImportError: Could not find 'cudart64_90.dll. Not sure if multiple cuda versions can coexist on windows, so I may have to downgrade to 9.0 and then try again. ", "What if I'm trying to build C++ dynamic library?", "@GilesCJ What version of TensorFlow (exact git commit) are you trying to build? @jhseu recently fixed this for the Windows build in d392b1c9ebf131b9ac64ff289d26e43afea21c10, which as far as I can tell is only on the r1.7 branch.\r\n\r\n@yifeif @jhseu Does this fix need to be backported to the master branch?", "Yeah, we usually merge the r1.7 branch back into master sometime near the final release.", "@mrry  I've rebuild master three days ago, and the problem remains. I'll try to build r1.7 today and see how it goes. Is this problem possible related to static library or dynamic library??", "As far as I understand, the problem affects all versions of the GPU build using Windows/CMake since the snapshot op was added.", "@mrry The problem is solved when I build the library with r1.7 branch!! Thank you guys very much!\r\nWill r1.7 merge to the master?? Or I'll have to use r1.7 from now on?"]}, {"number": 17751, "title": "MKL DNN: fix the TF1.6 speed issue by fixing MKL DNN LRN taking the optimum path (#17605)", "body": "* MKL DNN: fix the TF1.6 speed issue by fixing MKL DNN LRN\n\n* fixed typos in the doc for LrnRewrite\nRequesting a pull to tensorflow:r1.7 to fix MKL performance regression\ndescribed in issue #17383.", "comments": ["So there's good news and bad news.\n\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that all authors are ok with their commits being contributed to this project.  Please have them confirm that here in the pull request.\n\n*Note to project maintainer: This is a terminal state, meaning the `cla/google` commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.*\n\n<!-- need_author_consent -->"]}, {"number": 17750, "title": "using finally in tf_record_iterator()", "body": "Hi, this is to handle the case when [generator.throw()](https://www.python.org/dev/peps/pep-0342/#new-generator-method-throw-type-value-none-traceback-none) or [generator.close()](https://www.python.org/dev/peps/pep-0342/#new-generator-method-close) is called on a tfrecord iterator.\r\n\r\nIt will raise an exception like `GeneratorExit` in the generator's stack, and using `finally:` will promptly release the resources in such cases, without relying on garbage collection.\r\n\r\nPlease let me know if I'm missing something.", "comments": []}, {"number": 17749, "title": "android Semantic segmentation ", "body": "if i want to use my code and model network on pc to run it in mobile android for semantic segmentation that is possible ? and how ? \r\nthank you \r\n\r\nSystem information\r\n**Have I written custom code : No\r\n**OS Platform and Distribution : windows 10\r\n**TensorFlow installed from binary\r\n**TensorFlow version :1.6\r\nPython version: 3\r\n**Bazel version *:N/A\r\nCUDA/cuDNN version: cuda 9 - cudnn 7\r\nGPU model and memory: geforce gtx 960 - 4 GB\r\nExact command to reproduce: N/A\r\n", "comments": ["This question is better asked on  [StackOverflow](http://stackoverflow.com/questions/tagged/tensorflow) since it is not a  bug or feature request. There is also a larger community that reads questions there. Thanks!"]}, {"number": 17748, "title": "Inability to get tensorflow output from custom_estimator", "body": "For this costum_estimator, the related documentation on https://www.tensorflow.org/get_started/custom_estimators tells me that I can just input tensorboard --logdir=PATH in the directory and obtain TensorGraph visualisation.  However I do this and I get on the Tensorboard page \"Graph visualisation failed: the graph is empty. Make sure that the graph is passed to the tf.summary.filewriter after the graph is defined\"\r\nAs far as I understood, when using custom estimators it automatically passes the data to TensorBoard. What shall I do? I would like to get the basic graphs as shown at the bottom of the tutorial I linked above.\r\n\r\n`import pandas as pd             \r\nimport numpy as np              \r\nimport matplotlib.pyplot as plt  \r\nimport tensorflow as tf\r\nimport argparse\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for training\"\"\"\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(buffer_size=1000).repeat(count=None).batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\t\r\n\r\ndef eval_input_fn(features, labels, batch_size):\r\n    \"\"\"An input function for evaluation or prediction\"\"\"\r\n    features=dict(features)\r\n    if labels is None:\r\n        # No labels, use only features.\r\n        inputs = features\r\n    else:\r\n        inputs = (features, labels)\r\n\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\r\n\r\n    # Batch the examples\r\n    assert batch_size is not None, \"batch_size must not be None\"\r\n    dataset = dataset.batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\ndef datasetfun(dataframe,split1,split2):\r\n    \r\n    #split1: variable deciding the proportion of test data to training data\r\n    #split2: variable deciding the proportion of test data to training data\r\n\r\n    dataframe = pd.read_csv(\"Supporttry.csv\") # Let's have Pandas load our dataset as a dataframe\r\n    #dataframe = dataframe.drop([\"dzclass\",\"num.co\",\"edu\",\"income\",\"scoma\",\"charges\",\"totcst\",\"totmcst\",\"avtisst\",\"race\",\"meanbp\",\"wblc\",\"hrt\",\"resp\",\"temp\",\"pafi\",\"alb\",\"bili\",\"crea\",\"sod\",\"ph\",\"glucose\",\"bun\",\"urine\",\"adlp\",\"adls\",\"sfdm2\",\"adlsc\"], axis=1) # Remove columns we don't care about\r\n    dataframe = dataframe.drop([\"crea\",\"avtisst\",\"wblc\",\"race\",\"edu\",\"income\",\"charges\",\"totcst\",\"totmcst\",\"pafi\",\"alb\",\"bili\",\"ph\",\"glucose\",\"bun\",\"urine\",\"adlp\",\"adls\",\"sfdm2\"], axis=1)\r\n    train = dataframe[0:split1] # use first 15 rows as training set\r\n    test = dataframe[split1:split2] # keep some as testing\r\n    predictdata = dataframe[split2:] # find the ones to predict\r\n\r\n    train_features, train_labels = train, train.pop(\"death\") #separate features with classification in train set\r\n    test_features, test_labels = test, test.pop(\"death\") #separate features with classification in train set\r\n    predict_features, predict_labels = predictdata, predictdata.pop(\"death\")\r\n\r\n    categorical_column1 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"sex\", vocabulary_list=[\"male\", \"female\"], default_value=0)\r\n    categorical_column2 = tf.feature_column.categorical_column_with_vocabulary_list(key='dzgroup',vocabulary_list=[\"Lung Cancer\",\"Colon Cancer\",\"ARF/MOSF w/Sepsis\",\"MOSF w/Malig\",\"Cirrhosi\",\"CHF\"])\r\n    categorical_column3 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"dzclass\", vocabulary_list=[\"Cancer\", \"ARF/MOSF\",\"COPD/CHF/Cirrhosis\"], default_value=0)\r\n    #categorical_column4 = tf.feature_column.categorical_column_with_vocabulary_list(key=\"race\", vocabulary_list=[\"black\", \"hispanic\",\"White\",\"asian\",\"other\"], default_value=0)\r\n\r\n    my_feature_columns = [\r\n    tf.feature_column.numeric_column(key='age'),\r\n    tf.feature_column.indicator_column(categorical_column1),\r\n    tf.feature_column.numeric_column(key='hospdead'),\r\n    tf.feature_column.numeric_column(key='slos'),\r\n    tf.feature_column.numeric_column(key='d.time'),\r\n    tf.feature_column.indicator_column(categorical_column2),\r\n    tf.feature_column.indicator_column(categorical_column3),\r\n    tf.feature_column.numeric_column(key='scoma'),\r\n    #tf.feature_column.numeric_column(key='avtisst'),\r\n    #tf.feature_column.indicator_column(categorical_column4),\r\n    tf.feature_column.numeric_column(key='meanbp'),\r\n    tf.feature_column.numeric_column(key='hrt'),\r\n    tf.feature_column.numeric_column(key='resp'),\r\n    tf.feature_column.numeric_column(key='temp'),\r\n    #tf.feature_column.numeric_column(key='crea'),\r\n    tf.feature_column.numeric_column(key='sod'),\r\n    tf.feature_column.numeric_column(key='adlsc')]\r\n\r\n    return (train_features, train_labels, test_features, test_labels, predict_features, predict_labels, my_feature_columns)\r\n\r\ndef my_model(features, labels, mode, params):\r\n    \"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"\r\n    # Create three fully connected layers each layer having a dropout\r\n    # probability of 0.1.\r\n    net = tf.feature_column.input_layer(features, params['feature_columns']) #imput layer\r\n    for units in params['hidden_units']: #can change type of layers!!!!\r\n        net = tf.layers.dense(net, units=units, activation=tf.nn.relu) #can change activation function to sigmoid!!!\r\n\r\n    # Compute logits (1 per class).\r\n\r\n    #output layer\r\n    logits = tf.layers.dense(net, params['n_classes'], activation=None)\r\n\r\n    # Compute predictions.\r\n    predicted_classes = tf.argmax(logits, 1)\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        predictions = {\r\n            'class_ids': predicted_classes[:, tf.newaxis],\r\n            'probabilities': tf.nn.softmax(logits),\r\n            'logits': logits,\r\n        }\r\n        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n    # Compute loss.\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n\r\n    # Compute evaluation metrics!!!!!!!\r\n    accuracy = tf.metrics.accuracy(labels=labels,\r\n                                   predictions=predicted_classes,\r\n                                   name='acc_op')\r\n    metrics = {'accuracy': accuracy}\r\n    tf.summary.scalar('accuracy', accuracy[1])\r\n\r\n    if mode == tf.estimator.ModeKeys.EVAL:\r\n        return tf.estimator.EstimatorSpec(\r\n            mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n    # Create training op.\r\n    assert mode == tf.estimator.ModeKeys.TRAIN\r\n    \r\n    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1) #can change optimizer\r\n    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\r\n    \r\n\r\n\r\n\r\n\r\ndef main(m):\r\n\t\r\n\t#classification of death or no death\r\n\r\n    dataframe = pd.read_csv(\"Supporttry.csv\") # Let's have Pandas load our dataset as a dataframe\r\n\r\n    split1=500\r\n    split2=980\r\n\r\n    [train_features, train_labels, test_features, test_labels, predict_features, predict_labels, my_feature_columns] = datasetfun(dataframe,split1,split2)\r\n\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=my_model,\r\n        params={\r\n            'feature_columns': my_feature_columns,\r\n            # Two hidden layers of 10 nodes each.\r\n            'hidden_units': [10, 10],\r\n            # The model must choose between 3 classes.\r\n            'n_classes': 2,\r\n        })\r\n\r\n    # classifier = tf.estimator.DNNClassifier(\r\n    # \tfeature_columns=my_feature_columns,\r\n    # \thidden_units=[10,10,5,6,7],\r\n    # \tn_classes=2)\r\n\r\n    batch_size=100\r\n    train_steps=1000\r\n\r\n    #writer = tf.summary.FileWriter(\"/Users/angelicagrusovin/documents/oxford/MLSP\",graph=tf.get_default_graph())\r\n    #writer.add_graph(sess.graph)\r\n\r\n    classifier.train(input_fn=lambda:train_input_fn(train_features, train_labels, batch_size),steps=train_steps)\r\n\r\n    eval_result = classifier.evaluate(\r\n    input_fn=lambda:eval_input_fn(test_features, test_labels, batch_size))\r\n\r\n    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\r\n\r\n\t#predictions = classifier.predict(\r\n    #input_fn=lambda:eval_input_fn(predict_features,None,batch_size))\r\n\r\n\t# expected=['dead','alive']\r\n\t# STATE=['dead','alive']\r\n\t# print(zip(predictions))\r\n\r\n\t# template = ('\\nPrediction is ({:.1f}%), expected {}')\r\n\r\n\t# for pred_dict in zip(predictions):\r\n\t# \tclass_id = pred_dict[0]['class_ids'][0]\r\n\t# \tprobability = pred_dict[0]['probabilities'][class_id]\r\n\t# \tprint(template.format(100 * probability, STATE[class_id]))\r\n\r\n    \r\n\r\n\r\n\r\n\r\n\t#print(my_feature_columns) \r\n\r\n    return m\r\n\r\nif __name__ == '__main__':\r\n    main(10)`\r\n\r\n\r\n\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 45 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?"]}, {"number": 17747, "title": "Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.6.0\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0.176.2 / cudnn7.0.5\r\n- **GPU model and memory**: GTX 1080 (8GB)\r\n- **Exact command to reproduce**:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nbatch_size = 64\r\nimages = tf.random_normal(shape=[batch_size, 32, 32, 3], dtype=tf.float32)\r\nangles = tf.random_uniform([batch_size], -0.5, 0.5)\r\nimages = tf.contrib.image.rotate(images, angles)\r\n\r\nwith tf.Session() as sess:\r\n    _ = sess.run(images)\r\n```\r\n### Any Idea why this small example produces the following error?\r\n**ERROR:**\r\n*2018-03-16 18:27:24.292665: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION*\r\n*2018-03-16 18:27:24.292700: E C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_INSTRUCTION ::*\r\n*2018-03-16 18:27:24.296409: F C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\35\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1*\r\n\r\n**UPDATE:**\r\nI think issue #17485 is very similar", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "tf.contrib.image.rotate(images, angles) crashed on Win10\uff0cpython3.6\uff0ctensorflow1.6\uff0cCUDA9\uff0ccudnn7.0\uff0c1080Ti\u3002\r\nError Code\uff1afailed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n2018-03-17 22:26:12.688690: F C:\\tf_jenkins\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1", "Same, tf.contrib.image.rotate(images, angles) crashed on Win10, python 3.6, tensorflow 1.4, CUDA 8.0, cuDNN 6, 1080ti.\r\n\r\nError message:\r\n2018-03-30 11:31:04.028220: E C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n2018-03-30 11:31:04.042984: F C:\\tf_jenkins\\home\\workspace\\rel-win\\M\\windows-gpu\\PY\\36\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n\r\nAfter I upgrade to tensorflow 1.7.0, CUDA 9.0, cuDNN 7.\r\nError message become:\r\n2018-03-30 11:55:22.974188: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n2018-03-30 11:55:22.979694: F T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n\r\nWorks well on CPU all the time, only crash when using GPU.\r\n\r\nTry to use tf.contrib.image.angles_to_projective_transforms() and tf.contrib.image.transform(), same error", "Same issue is valid for my environment also; Windows 10 x64, Tensorflow 1.7.0, CUDA 9, cuDNN 7, Python 3.6\r\n\r\n2018-04-29 21:11:23.422100: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n2018-04-29 21:11:23.422100: E T:\\src\\github\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n2018-04-29 21:11:23.422361: F T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1\r\n2018-04-29 21:11:23.422608: F T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:203] Unexpected Event status: 1", "got same issue using tf1.4.0,cuda8.0,cudnn6.0.21. The same code works fine sometimes and reports the error some other time.", "I recreated a fresh conda env with cdnn, tensorflow gpu1.8, and keras gpu 2.1 installed from anaconda. Before that I downgraded my cuda from 9.2 to 9.0", "I also get this issue, in between of the training.\r\n", "@zheng-xq, Do you know about the state of windows CUDA support. Otherwise assign it back and we should assign it to who implemented rotate images.", "same error! \r\n Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS", "same error~ Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n", "Nagging Assignee @zheng-xq: It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "I also have the same error . CUDA_ERROR_ILLEGAL_INSTRUCTION. we just need to know which instruction caused this error as the gpu works fine else and the code works fine if we change the device to be cpu instead.  ", "I've got the same issue running the code above with Windows 10, tensorflow 1.8.0, cudatoolkit 9.0 (anaconda), cudnn 7.1.4 (anaconda) on a GTX 970 (msi), any help would be very appreciated...\r\n\r\n```\r\n>python test_tf_CUDA_ERROR_ILLEGAL_INSTRUCTION.py\r\n2018-10-06 21:06:28.880502: I C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1356] Found device 0 with properties:\r\nname: GeForce GTX 970 major: 5 minor: 2 memoryClockRate(GHz): 1.253\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 4.00GiB freeMemory: 3.30GiB\r\n2018-10-06 21:06:28.892851: I C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1435] Adding visible gpu devices: 0\r\n2018-10-06 21:06:29.561979: I C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-10-06 21:06:29.567773: I C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:929]      0\r\n2018-10-06 21:06:29.574945: I C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:942] 0:   N\r\n2018-10-06 21:06:29.579561: I C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970, pci bus id: 0000:02:00.0, compute capability: 5.2)\r\n2018-10-06 21:06:29.887384: E C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_driver.cc:1110] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_INSTRUCTION ::\r\n2018-10-06 21:06:29.887400: E C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_event.cc:49] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_INSTRUCTION\r\n2018-10-06 21:06:29.898608: F C:\\users\\nwani\\_bazel_nwani\\mmtm6wb6\\execroot\\org_tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_event_mgr.cc:208] Unexpected Event status: 1\r\n```\r\n\r\nThe code to reproduce this is:\r\n```\r\nimport tensorflow as tf\r\n\r\nbatch_size = 64\r\nimages = tf.random_normal(shape=[batch_size, 32, 32, 3], dtype=tf.float32)\r\nangles = tf.random_uniform([batch_size], -0.5, 0.5)\r\nimages = tf.contrib.image.rotate(images, angles)\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\r\n\r\nwith tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\r\n    _ = sess.run(images)\r\n```", "@fsaxen Is this still an issue? @gogobd Can you please test it against latest TensorFlow version and post your findings?", "Closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!\r\n", "I updated to 1.11.0 and the problem is gone. Thank you!", "> same error!\r\n> Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS\r\n\r\nHave you figured it out?\r\n"]}, {"number": 17746, "title": "tf.contrib.crf.crf_decode fails when sequence_length is 0", "body": "These links are similar, \r\n* https://stackoverflow.com/questions/42798518/how-can-i-pass-sequences-of-length-1-to-tf-contrib-crf-in-tensorflow\r\n* #7751\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: ('v1.6.0-0-gd2e24b6', '1.6.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: Build label: 0.10.0\r\n\r\n### Describe the problem\r\n`crf_decode` fails when sequence length == 0. This will happen when I am doing batch evaluation and one sequence is exhausted before the others. \r\n\r\n### Source code / logs\r\n```\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 517, in crf_decode\r\n    fn2=_multi_seq_fn)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/utils.py\", line 209, in smart_cond\r\n    return fn2()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/crf/python/ops/crf.py\", line 490, in _multi_seq_fn\r\n    backpointers, sequence_length - 1, seq_dim=1)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 4145, in reverse_sequence\r\n    seq_dim=seq_dim, batch_dim=batch_dim, name=name)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): seq_lens(90) < 0\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code: See [this commit](https://github.com/tensorflow/tensorflow/pull/17755/commits/5a1cb167d79231219ded100acad7611ba9ef5355) to `crf_test.py`\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: execute unit test in referenced PR w/o the change to `crf.py`", "I only see this problem when batch_size > 1", "Hi @riklopfer! \r\nIt seems you are using older versions(1.x versions) of Tensorflow which is not supported any more. Please visit [addons](https://www.tensorflow.org/addons) page to upgrade your codebase to latest version. Thanks!", "This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you.\n", "Closing as stale. Please reopen if you'd like to work on this further.\n"]}, {"number": 17745, "title": "Cast return value that caused warning", "body": "The member function `num_runs()` returns an `int`, however the return value from `run_total_us_.count()` is an `int64`. Casting the value to an `int` allows for the compiler to know that the intended return value should be an `int`.", "comments": []}, {"number": 17744, "title": "TFSLIM UnknownError Input/Output Error.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: \r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n\r\nI have created tfrecord files and am getting the following error during my training:\r\n\r\nINFO:tensorflow:global step 3338: loss: 0.4686 (2.67 sec/step)\r\nstep=1217\r\nINFO:tensorflow:global step 3339: loss: 0.4468 (2.57 sec/step)\r\nstep=1218\r\nINFO:tensorflow:global step 3340: loss: 0.4400 (2.66 sec/step)\r\nstep=1219\r\nINFO:tensorflow:global step 3341: loss: 0.5029 (2.76 sec/step)\r\nstep=1220\r\nINFO:tensorflow:global step 3342: loss: 0.3761 (43.48 sec/step)\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.UnknownError'>, /content/drive/app/images/train/fmri_train_00000-of-00001.tfrecord; Input/output error\r\n\t [[Node: parallel_read/ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](parallel_read/TFRecordReaderV2, parallel_read/filenames)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1361, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1340, in _run_fn\r\n    target_list, status, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 32, current size 0)\r\n\t [[Node: batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_UINT8, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](batch/fifo_queue, batch/n)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py\", line 990, in managed_session\r\n    yield sess\r\n  File \"train_alz.py\", line 282, in run\r\n    summaries = sess.run(my_summary_op)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.OutOfRangeError: FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 32, current size 0)\r\n\t [[Node: batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_UINT8, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](batch/fifo_queue, batch/n)]]\r\n\r\nCaused by op 'batch', defined at:\r\n  File \"train_alz.py\", line 304, in <module>\r\n    run()\r\n  File \"train_alz.py\", line 184, in run\r\n    images, _, labels = load_batch(dataset, batch_size=batch_size)\r\n  File \"train_alz.py\", line 168, in load_batch\r\n    allow_smaller_final_batch = True)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py\", line 989, in batch\r\n    name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py\", line 761, in _batch\r\n    dequeued = queue.dequeue_up_to(batch_size, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 527, in dequeue_up_to\r\n    self._queue_ref, n=n, component_types=self._dtypes, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 2557, in _queue_dequeue_up_to_v2\r\n    component_types=component_types, timeout_ms=timeout_ms, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3271, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1650, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_3_batch/fifo_queue' is closed and has insufficient elements (requested 32, current size 0)\r\n\t [[Node: batch = QueueDequeueUpToV2[component_types=[DT_FLOAT, DT_UINT8, DT_INT64], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](batch/fifo_queue, batch/n)]]\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_alz.py\", line 304, in <module>\r\n    run()\r\n  File \"train_alz.py\", line 300, in run\r\n    sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\r\n  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py\", line 1000, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/supervisor.py\", line 828, in stop\r\n    ignore_live_threads=ignore_live_threads)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1259, in _single_operation_run\r\n    None)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.UnknownError: /content/drive/app/images/train/fmri_train_00000-of-00001.tfrecord; Input/output error\r\n\t [[Node: parallel_read/ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](parallel_read/TFRecordReaderV2, parallel_read/filenames)]]\r\n\r\nI am a newbie to tensorflow and would really appreciate any help!", "comments": ["Nagging Assignee @rohan100jain: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Can you please elaborate on what you did to exactly get this error? What code you ran etc.?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17743, "title": "Training through an Estimator is much slower than writing a for loop when using a Dataset", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nHappens with stock code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu Server 17.10.1\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\nb'v1.6.0-0-gd2e24b6039' 1.6.0\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\n0.11.0\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc (Ubuntu 6.4.0-8ubuntu1) 6.4.0 20171010\r\n- **CUDA/cuDNN version**:\r\n9.1/7.0.5\r\n- **GPU model and memory**:\r\nnot applicable\r\n- **Exact command to reproduce**:\r\nnot applicable\r\n\r\nI have a dataset that consists of one feature and one label, both are int64, there are 7166 steps of batches consisting of 128 and run for 10 epochs. I have tested this using two approaches, the first uses 100 tfrecord files that are read through the `tf.data.Dataset` API, the second reads the tfrecord data into memory then creates a `tf.data.Dataset` utilizing `from_tensor_slices`.\r\n\r\nObviously the in-memory version runs much faster than the read-from-disk method.\r\n\r\nWhen I train my model using a custom while loop, 10 epochs runs 20 seconds faster than running it using the `tf.estimator.Estimator.train` API. This 20 seconds is a blanket 20 seconds, whether I am running it using the in-memory dataset or the read-from-disk method.\r\n\r\nI thought, the overhead could be from storing checkpoints, etc. so I supplied a `RunConfig` that disables checkpoints and sets the various save/log step counts to very high numbers. This helped, but only a little bit, it went from a blanket 20 seconds to a blanket 17 seconds. 17 seconds doesn't seem like much, but if I am running for a large number of epochs, or if I use a much larger dataset, those seconds can turn into hours.\r\n\r\nIs this something inherent with `tf.estimator.Estimator` or could I be hitting a problem?", "comments": ["Can you reproduce this issue again with tf-nightly? Thanks", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!"]}, {"number": 17742, "title": "Update kernel_methods to fix failing build_docs_test", "body": "", "comments": []}, {"number": 17741, "title": "Fix broken link of performance guide for the `tf.data` API", "body": "As you can see in the [Performance Guide](https://www.tensorflow.org/performance/performance_guide), the below **here** didn't link to the correct place with https://www.tensorflow.org/performance/performance_guide#datasets_performance due to datasets_performance.md is another file instead of an anchor inside current file.\r\n> The tf.data API utilizes C++ multi-threading and has a much lower overhead than the Python-based queue_runner that is limited by Python's multi-threading performance. A detailed performance guide for the tf.data API can be found **here**.\r\n\r\nThis PR is to fix the above broken link of performance guide for the `tf.data` API.", "comments": []}, {"number": 17740, "title": "implement matrix 2-norm", "body": "Implement matrix 2-norm by using tf.svd.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I have signed the CLA.", "CLAs look good, thanks!\n\n<!-- ok -->", "@fumihwh Thanks for the contribution. Could you please update the unit tests in \r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/norm_op_test.py\r\naccordingly?", "@rmlarsen \r\nThanks for review.\r\nI made some changes. I am not sure about test part. Please review again.", "@fumihwh Thanks for the update. I think the minimum is to enable the ord=2 for matrix tests like you did.", "@fumihwh please take a look at the failing tests: \r\n\r\nhttps://source.cloud.google.com/results/invocations/d78fd008-d93a-4253-b95c-800af27c28ce/log", "@rmlarsen \r\nI think the reason is the pollution of axis which is also used outside of if branch.\r\nNow I renamed them and see what happens.", "@rmlarsen Something wrong with testBadOrder test cases.\r\n- doesn't use for loop value (use fixed value)\r\n- adjust test cases\r\n- remove 2 in cases of matrix\r\n- add more test cases of matrix", "@rmlarsen \r\nI passed test locally. Cast svd output to float32 what is an OpKernel registered for reduce_max, to avoid `No OpKernel was registered to support Op 'Max' with these attrs`.\r\nLast one is complex128.", "@rmlarsen I have no idea about internal CI build failed. Any ideas?", "Running the tests again.\r\n", "@rmlarsen \r\nStil failed.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/adagrad_test.runfiles/org_tensorflow/tensorflow/python/training/adagrad_test.py\", line 29, in <module>\r\n    from tensorflow.python.ops import variable_scope\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/adagrad_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 40, in <module>\r\n    from tensorflow.python.ops import init_ops\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/adagrad_test.runfiles/org_tensorflow/tensorflow/python/ops/init_ops.py\", line 42, in <module>\r\n    from tensorflow.python.ops import linalg_ops\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/adagrad_test.runfiles/org_tensorflow/tensorflow/python/ops/linalg_ops.py\", line 27, in <module>\r\n    from tensorflow.python.ops import functional_ops\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/adagrad_test.runfiles/org_tensorflow/tensorflow/python/ops/functional_ops.py\", line 38, in <module>\r\n    from tensorflow.python.ops import variable_scope as vs\r\nImportError: cannot import name variable_scope\r\n```\r\nDo you know anything about this error?\r\nI actually do `from tensorflow.python.ops import functional_ops` in linalg.ops.", "@fumihwh I think you need to add a dependency on functional_ops here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/BUILD#L1975", "Ah, rats, that seems to have introduced a dependency cycle.", "@rmlarsen \r\nSo `functional_ops` conflict with which deps? `linalg_ops_gen`? How to fix this.....", "The cycle is the following\r\n\r\n```\r\nERROR: /tmpfs/src/github/tensorflow/tensorflow/python/BUILD:2594:1: in py_library rule //tensorflow/python:variable_scope: cycle in dependency graph:\r\n    //tensorflow/tools/docker:simple_console\r\n    //tensorflow:tensorflow_py\r\n    //tensorflow/python:python\r\n    //tensorflow/python:no_contrib\r\n    //tensorflow/python:client_testlib\r\n    //tensorflow/python:client\r\n    //tensorflow/python:framework\r\n    //tensorflow/python:function\r\n.-> //tensorflow/python:variable_scope\r\n|   //tensorflow/python:init_ops\r\n|   //tensorflow/python:linalg_ops\r\n|   //tensorflow/python:functional_ops\r\n`-- //tensorflow/python:variable_scope\r\nThis cycle occurred because of a configuration option\r\n```\r\n\r\ncan you try to work around it?", "@rmlarsen Let me try~~~", "@rmlarsen \r\nI've moved functional_ops to linalg_grad. And now, it can build tf.\r\nI am not sure if `ImportError: cannot import name variable_scope` will happen again.\r\n\r\nNow BUILD is following. Could you tell me briefly why it happens if you have time. \r\n```\r\npy_library(\r\n    name = \"linalg_grad\",\r\n    srcs = [\"ops/linalg_grad.py\"],\r\n    srcs_version = \"PY2AND3\",\r\n    deps = [\r\n        \":array_ops\",\r\n        \":control_flow_ops\",\r\n        \":framework_for_generated_wrappers\",\r\n        \":functional_ops\",\r\n        \":linalg_ops\",\r\n        \":math_ops\",\r\n        \"//tensorflow/python/ops/linalg:linalg_impl\",\r\n    ],\r\n)\r\n\r\npy_library(\r\n    name = \"linalg_ops\",\r\n    srcs = [\"ops/linalg_ops.py\"],\r\n    srcs_version = \"PY2AND3\",\r\n    deps = [\r\n        \":array_ops\",\r\n        \":dtypes\",\r\n        \":framework_ops\",\r\n        \":linalg_ops_gen\",\r\n        \":math_ops\",\r\n        \"//third_party/py/numpy\",\r\n    ],\r\n)\r\n```", "@rmlarsen \r\nDepressing..\r\nCI still failing......\r\n\r\nI now begin to understand what happens. Could you tell if I am right?\r\n\r\nHere is one failed test from the early beginning:\r\n```\r\n================================================================================\r\n==================== Test output for //tensorflow/python/kernel_tests:clip_ops_test:\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/clip_ops_test.py\", line 24, in <module>\r\n    from tensorflow.python.platform import test\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py\", line 43, in <module>\r\n    from tensorflow.python.framework import test_util as _test_util\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 57, in <module>\r\n    from tensorflow.python.framework import importer\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/importer.py\", line 32, in <module>\r\n    from tensorflow.python.framework import function\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/function.py\", line 38, in <module>\r\n    from tensorflow.python.ops import variable_scope as vs\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/variable_scope.py\", line 40, in <module>\r\n    from tensorflow.python.ops import init_ops\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/init_ops.py\", line 42, in <module>\r\n    from tensorflow.python.ops import linalg_ops\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/linalg_ops.py\", line 27, in <module>\r\n    from tensorflow.python.ops import functional_ops\r\n  File \"/b/s/w/ir/run/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/clip_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/functional_ops.py\", line 38, in <module>\r\n    from tensorflow.python.ops import variable_scope as vs\r\nImportError: cannot import name variable_scope\r\n```\r\n\r\nfunction.py -> variable_scope.py -> init_ops.py -> linalg_ops.py ->functional_ops.py -> variable_scope.py\r\nSo you suggest to add functional_ops.py as a deps to linalg_ops.\r\nAfter that, variable_scope.py becomes a cycle.\r\n\r\nOK, seems I get the reason.\r\n~~And one question, is there any docker to run this internal CI build thing locally? I don't want to waiting for kokoro anymore......~~", "@rmlarsen Sorry, I've no idea about how to fix this cycle.\r\nAny hint?", "@fumihwh sorry for the delay. I was busy with other tasks.\r\n@gunan the solution seems to be eluding me. Would you mind taking a look?", "I cannot tell from first glance.\r\n@annarev is this, the test failures, potentially related to python api generation?\r\n", "I don't see an easy solution for this cycle, other than moving some functions to a new file.\r\nFor e.g. init_ops.py depends on:\r\nlinalg_ops.qr\r\nlinalg_ops.eye\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/init_ops.py\r\n\r\nlinalg_ops.qr can actually be accessed from gen_linalg_ops instead:\r\nfrom tensorflow.python.ops import gen_linalg_ops\r\ngen_linalg_ops.qr(...)\r\n\r\nlinalg_ops.eye is defined in linalg_ops.py. May be it can be moved to its own file? eye_op.py or linalg_ops_impl.py?\r\n\r\nThen, init_ops.py can just depend on gen_linalg_ops and, say, eye_op.\r\neye_op.eye can be imported in linalg_ops.py, so that any other eye references would still work.", "@annarev thanks for the analysis. \r\n@fumihwh would you be able to implement Anna's suggestions in this PR?", "@rmlarsen Sure.", "@annarev \r\nGet\r\n```\r\n======================================================================\r\nERROR: testInitializerIdentical (__main__.ConvolutionOrthogonal2dInitializerTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/b/s/w/ir/run/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/init_ops_test.py\", line 681, in testInitializerIdentical\r\n    self.assertTrue(identicaltest(self, init1, init2, (3, 3, 10, 10)))\r\n  File \"/b/s/w/ir/run/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/init_ops_test.py\", line 57, in identicaltest\r\n    t1 = init1(shape).eval()\r\n  File \"/b/s/w/ir/run/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/init_ops.py\", line 640, in __call__\r\n    kernel = self._orthogonal_kernel(shape[0], shape[2], shape[3])\r\n  File \"/b/s/w/ir/run/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/init_ops.py\", line 767, in _orthogonal_kernel\r\n    orth = self._orthogonal_matrix(cout)[0:cin, :]\r\n  File \"/b/s/w/ir/run/bazel-out/k8-py3-opt/bin/tensorflow/python/kernel_tests/init_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/init_ops.py\", line 659, in _orthogonal_matrix\r\n    q, r = linalg_ops.qr(a)\r\nNameError: name 'linalg_ops' is not defined\r\n```\r\n\r\nIn init_ops.py, I am using `q, r = gen_linalg_ops.qr(a, full_matrices=False)`, and without dependency of `linalg_ops`.\r\nIt should not look for `linalg_ops.qr` anymore, shouldn't it?\r\nMeanwhile, I am not sure about rewriting `qr = linalg_ops.qr` to `qr = gen_linalg_ops.qr` in linalg_impl.py.\r\nCurrently, it's `qr = linalg_ops.qr`.\r\nMaybe this is the reason?", "It seems like an issue merging with https://github.com/tensorflow/tensorflow/commit/454a22aa29dc2dba355094aabe733cd8419f2788#diff-0a767e3cc15ca06b9fe8b6e63a5b5fe2.\r\n\r\nCan you update base for your branch?", "@annarev Done.", "@annarev \r\nDoesn't work. Same error. But I think we are getting closer to the goal.\r\n\r\nI am not sure why `init_ops.py` try to call `linalg_ops.qr`.\r\nThe only reason is `qr = linalg_ops.qr` in `linalg_impl.py` I think.\r\nWhat do you think?", "That commit (https://github.com/tensorflow/tensorflow/commit/454a22aa29dc2dba355094aabe733cd8419f2788#diff-0a767e3cc15ca06b9fe8b6e63a5b5fe2) added more references to linalg_ops.qr in linalg_ops.py.\r\nCan you update them? Sorry I should have read through the files after last update.", "@annarev Updated.", "@rmlarsen @annarev \r\nAlmost there!\r\nOne problem remained.\r\nWe need to loose the test. `self.assertAllClose(np_norm, tf_norm_val)` default is `rtol=1e-6, atol=1e-6`\r\nIs 1e-5 ok?\r\n\r\nFailed like:\r\n```\r\n======================================================================\r\nFAIL: test_Norm_complex64_2_3_5_5_ord_2_axis_(-2, -1)_False_True (__main__.NormOpTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"T:/src/github/tensorflow/tensorflow/python/kernel_tests/norm_op_test.py\", line 93, in Test\r\n    _CompareNorm(self, matrix)\r\n  File \"T:/src/github/tensorflow/tensorflow/python/kernel_tests/norm_op_test.py\", line 79, in _CompareNorm\r\n    self.assertAllClose(np_norm, tf_norm_val)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 1321, in assertAllClose\r\n    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 1293, in _assertAllCloseRecursive\r\n    path_str))\r\n  File \"C:\\Python35\\lib\\site-packages\\tensorflow\\python\\framework\\test_util.py\", line 1228, in _assertArrayLikeAllClose\r\n    a, b, rtol=rtol, atol=atol, err_msg=msg, equal_nan=True)\r\n  File \"C:\\Python35\\lib\\site-packages\\numpy\\testing\\utils.py\", line 1392, in assert_allclose\r\n    verbose=verbose, header=header)\r\n  File \"C:\\Python35\\lib\\site-packages\\numpy\\testing\\utils.py\", line 739, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-06, atol=1e-06\r\nMismatched value: a is different from b.\r\n(mismatch 16.66666666666667%)\r\n x: array([[ 5.608771,  5.117595,  4.615607],\r\n       [ 4.832485,  4.631978,  4.996054]], dtype=float32)\r\n y: array([[ 5.608775,  5.117597,  4.615607],\r\n       [ 4.832485,  4.631985,  4.996056]], dtype=float32)\r\n```", "@fumihwh cool. Yes, 1e-5 is fine.", "@rmlarsen @annarev \r\nGet only one timeout.\r\n```\r\n//tensorflow/core:common_runtime_ring_reducer_test                      TIMEOUT in 481.2s\r\n  /home/kbuilder/.cache/bazel/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/core/common_runtime_ring_reducer_test/test.log\r\n```\r\n", "This timeout is a known issue, unrelated to your change.", "@rmlarsen @annarev @fumihwh It seems that there are some overlapping merges and the changes of \r\n```diff\r\n-from tensorflow.python.ops import linalg_ops\r\n+from tensorflow.python.ops import linalg_ops_impl\r\n```\r\n\r\ncaused some build failure on master. Created a PR #18664 for the fix. Please take a look.", "> Implement matrix 2-norm by using tf.svd.\r\n\r\ncan u tell me why using svd to implement matrix 2-norm\uff1f"]}, {"number": 17739, "title": "Tensorflow does not close backgroud threads on shutdown", "body": "### System information\r\n- **Have I written custom code**\r\n- **OS Linux Ubuntu 17.10)**\r\n- **TensorFlow installed using this doc https://www.tensorflow.org/install/install_c**:\r\n\r\n### Describe the problem\r\nI see the bug that Tensorflow does not shutdown threads created by TF_NewSession. I do not see any functions in C API, except TF_DeleteSession, which should shutdown threads. But as the example shows TF_DeleteSession does not really close any threads. \r\nYou may say that it happens only on shutdown so it is not an issue, but for the project with leak check, it is important to have clear Valgrind result.\r\n### Source code \r\n```\r\nint main() {\r\n\tTF_Status * status = TF_NewStatus();\r\n\tTF_SessionOptions *opt = TF_NewSessionOptions();\r\n\r\n\tTF_Graph *graph = TF_NewGraph();\r\n\tTF_Session *session = TF_NewSession(graph, opt, status);\r\n\r\n\tTF_CloseSession(session, status);\r\n        TF_DeleteSession(session, status);\r\n        TF_DeleteGraph(graph);\r\n   \tTF_DeleteStatus(status);\r\n   \tTF_DeleteSessionOptions(opt);\r\n}\r\n```\r\n### Steps to reproduce\r\n* `gcc -g -I/usr/local/include -L/usr/local/lib hello_tf.c -ltensorflow -o memtest`\r\n* `valgrind --leak-check=full ./memtest`\r\nAs result valgrind report an error that threads started by tensorflow still running at the end of main.\r\n\r\n### Logs\r\n```\r\n==17499== Memcheck, a memory error detector\r\n==17499== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.\r\n==17499== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info\r\n==17499== Command: ./memtest\r\n==17499== \r\n2018-03-15 16:37:49.446989: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\r\n==17499== \r\n==17499== HEAP SUMMARY:\r\n==17499==     in use at exit: 5,933,322 bytes in 91,227 blocks\r\n==17499==   total heap usage: 330,638 allocs, 239,411 frees, 23,865,452 bytes allocated\r\n==17499== \r\n==17499== 1,280 bytes in 4 blocks are possibly lost in loss record 53,082 of 53,144\r\n==17499==    at 0x4C31B25: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n==17499==    by 0x4013C86: allocate_dtv (dl-tls.c:290)\r\n==17499==    by 0x4013C86: _dl_allocate_tls (dl-tls.c:538)\r\n==17499==    by 0x9099421: allocate_stack (allocatestack.c:597)\r\n==17499==    by 0x9099421: pthread_create@@GLIBC_2.2.5 (pthread_create.c:669)\r\n==17499==    by 0x98CA5E2: std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)()) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.24)\r\n==17499==    by 0x8779F45: tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::function<void ()>) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x875008F: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x875024F: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x8AF9317: tensorflow::LocalDevice::EigenThreadPoolInfo::EigenThreadPoolInfo(tensorflow::SessionOptions const&) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x8AF9487: tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x8B18E11: tensorflow::ThreadPoolDevice::ThreadPoolDevice(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tensorflow::gtl::IntType<tensorflow::Bytes_tag_, long long>, tensorflow::DeviceLocality const&, tensorflow::Allocator*) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x8B19289: tensorflow::ThreadPoolDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<tensorflow::Device*, std::allocator<tensorflow::Device*> >*) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x8AF7979: tensorflow::(anonymous namespace)::GetCPUDevice(tensorflow::Env*) [clone .constprop.149] (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499== \r\n==17499== 1,280 bytes in 4 blocks are possibly lost in loss record 53,083 of 53,144\r\n==17499==    at 0x4C31B25: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)\r\n==17499==    by 0x4013C86: allocate_dtv (dl-tls.c:290)\r\n==17499==    by 0x4013C86: _dl_allocate_tls (dl-tls.c:538)\r\n==17499==    by 0x9099421: allocate_stack (allocatestack.c:597)\r\n==17499==    by 0x9099421: pthread_create@@GLIBC_2.2.5 (pthread_create.c:669)\r\n==17499==    by 0x98CA5E2: std::thread::_M_start_thread(std::shared_ptr<std::thread::_Impl_base>, void (*)()) (in /usr/lib/x86_64-linux-gnu/libstdc++.so.6.0.24)\r\n==17499==    by 0x8779F45: tensorflow::(anonymous namespace)::PosixEnv::StartThread(tensorflow::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::function<void ()>) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x875008F: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x875024F: tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x70BE592: ??? (in /usr/local/lib/libtensorflow.so)\r\n==17499==    by 0x70BFD89: ??? (in /usr/local/lib/libtensorflow.so)\r\n==17499==    by 0x70BFF09: ??? (in /usr/local/lib/libtensorflow.so)\r\n==17499==    by 0x8B107DE: tensorflow::NewSession(tensorflow::SessionOptions const&, tensorflow::Session**) (in /usr/local/lib/libtensorflow_framework.so)\r\n==17499==    by 0x51E1EB3: TF_NewSession (in /usr/local/lib/libtensorflow.so)\r\n==17499== \r\n==17499== LEAK SUMMARY:\r\n==17499==    definitely lost: 0 bytes in 0 blocks\r\n==17499==    indirectly lost: 0 bytes in 0 blocks\r\n==17499==      possibly lost: 2,560 bytes in 8 blocks\r\n==17499==    still reachable: 5,930,762 bytes in 91,219 blocks\r\n\r\n```", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "* OS Platform and Distribution (I have OS in original report)\r\n`Linux apopov-desktop 4.13.0-36-generic #40-Ubuntu SMP Fri Feb 16 20:07:48 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux`\r\n* TensorFlow installed from lib https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.6.0.tar.gz using this doc https://www.tensorflow.org/install/install_c\r\n* TensorFlow version 1.5.0\r\n* Bazel version - I do not use Bazel\r\n* CUDA/cuDNN version - running on CPU and use CPU only lib\r\n* Exact command to reproduce(It is also in original report): \r\n   1. Write code from section `Source code`\r\n   2. Compile it `gcc -g -I/usr/local/include -L/usr/local/lib hello_tf.c -ltensorflow -o memtest`\r\n   3. Run under valgrind `valgrind --leak-check=full ./memtest`\r\n   4. Look to the report\r\n", "Resources like thread-pool and memory allocator are shared between sessions. Shutting down the session would not shut down shared resources because those resources may be reused for future sessions. I guess you want an API to shut down memory allocator/thread-pool, which doesn't exist AFAIK", "I agree with API to shut down memory allocator/thread-pool, if it not exist I think it should be added", "Hi, my program uses TensorFlow C++ API, and address sanitizer has found a memory leak in my program that I believe is related to this issue.\r\n\r\nThe leaked memory found by address sanitizer is allocated by\r\n`tensorflow::thread::ThreadPool::ThreadPool(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int, bool)`, which is called by `TF_NewSession`.\r\n\r\nI hope this issue can be fixed soon. Thanks!", "Kute.  Harramte\n\nOn 11 Jun 2018 14:54, \"trangtv57\" <notifications@github.com> wrote:\n>\n> can anyone update status for this problem? I have same leak memory error\nwhen try to use session to predict.\n>\n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub, or mute the thread.\nKute. Harramte\n", "Have you tried setting `ConfigProto.use_per_session_threads` to `true` when constructing the session?", "It has been 35 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "> Have you tried setting `ConfigProto.use_per_session_threads` to `true` when constructing the session?\r\n\r\n@mrry: in tensorflow 1.12 with parameter server I get this error: tensorflow.python.framework.errors_impl.InvalidArgumentError: Distributed session does not support session thread pool options."]}, {"number": 17738, "title": "Fix folding unfused batchnorm bug", "body": "Folding batch normalization which has parameters like `is_training=False, fuse=False, scale=False` raises an error because its `batch_mean_tensor` is `None`. I suggest using `moving_mean_tensor` instead.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->", "Thanks for the fix!", "Nagging Assignee @suharshs: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly."]}, {"number": 17737, "title": "README.cmd typo.", "body": "Just a typo on README.cmd.", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "CLAs look good, thanks!\n\n<!-- ok -->"]}, {"number": 17736, "title": "worker task crashed  in distributed training if ps task or other worker task not started", "body": "### System information\r\n- **Have I written custom code**: yes\r\n- **OS Platform and Distribution **: CentOS Linux release 7.4.1708\r\n- **TensorFlow installed from **: binary\r\n- **TensorFlow version **: 'v1.6.0-0-gd2e24b6039', '1.6.0'\r\n- **Python version**:  2.7.5\r\n- **CUDA/cuDNN version**:   N/A\r\n- **GPU model and memory**:   N/A\r\n- **Exact command to reproduce**:  N/A\r\n\r\n### Describe the problem\r\nworker task will crashed after 10s, if ps task or other worker task not started. not sure it's a bug or misused the api.\r\n\r\n### Source code / logs\r\n\r\nlogs:\r\n>2018-03-15 19:59:32.309238: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-03-15 19:59:32.312834: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> bjlt-h1269.sy:42557}\r\n2018-03-15 19:59:32.312861: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:37060, 1 -> bjlt-h1270.sy:57571}\r\n2018-03-15 19:59:32.315443: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:37060\r\nVariables initialized ...\r\nWARNING:tensorflow:From ./demo.py:75: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\n2018-03-15 19:59:36.773767: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: OS Error\r\nTraceback (most recent call last):\r\n  File \"./demo.py\", line 173, in <module>\r\n    tf.app.run(main=main)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 126, in run\r\n    _sys.exit(main(argv))\r\n  File \"./demo.py\", line 76, in main\r\n    with sv.prepare_or_wait_for_session(server.target, config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement = True, log_device_placement = True)) as sess:\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 726, in prepare_or_wait_for_session\r\n    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py\", line 281, in prepare_session\r\n    sess.run(init_op, feed_dict=init_feed_dict)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 905, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1137, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1355, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1374, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.UnavailableError: OS Error\r\n\r\nfull code see https://github.com/Qihoo360/XLearning/blob/master/examples/tensorflow/demo.py\r\ncode:\r\n``` python\r\ndef main(_):\r\n  # cluster specification\r\n  FLAGS.task_index = int(os.environ[\"TF_INDEX\"])\r\n  FLAGS.job_name = os.environ[\"TF_ROLE\"]\r\n  cluster_def = json.loads(os.environ[\"TF_CLUSTER_DEF\"])\r\n  cluster = tf.train.ClusterSpec(cluster_def)\r\n\r\n  print(\"ClusterSpec:\", cluster_def)\r\n  print(\"current task id:\", FLAGS.task_index, \" role:\", FLAGS.job_name)\r\n  \r\n  gpu_options = tf.GPUOptions(allow_growth = True)\r\n  server = tf.train.Server(cluster, job_name=FLAGS.job_name,task_index=FLAGS.task_index,config = tf.ConfigProto(gpu_options=gpu_options,allow_soft_placement = True))\r\n  \r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n    # set the train parameters\r\n    learning_rate = FLAGS.learning_rate\r\n    training_epochs = FLAGS.training_epochs\r\n    batch_size = FLAGS.batch_size\r\n    iterData = trainData(FLAGS.data_path, batch_size)\r\n    \r\n    with tf.device(tf.train.replica_device_setter(worker_device=(\"/job:worker/task:%d\"%(FLAGS.task_index)),cluster=cluster)):\r\n      # count the number of updates\r\n      global_step = tf.get_variable('global_step', [],initializer = tf.constant_initializer(0), trainable = False)\r\n      # input \r\n      with tf.name_scope('input'):\r\n        x = tf.placeholder(tf.float32, shape=[None, 100], name=\"x-input\")\r\n        y_ = tf.placeholder(tf.float32, shape=[None, 2], name=\"y-input\")\r\n      # model parameters\r\n      tf.set_random_seed(1)\r\n      with tf.name_scope(\"weights\"):\r\n        W1 = tf.Variable(tf.random_normal([100, 50]))\r\n        W2 = tf.Variable(tf.random_normal([50, 2]))\r\n      # bias\r\n      with tf.name_scope(\"biases\"):\r\n        b1 = tf.Variable(tf.zeros([50]))\r\n        b2 = tf.Variable(tf.zeros([2]))\r\n      # implement model\r\n      with tf.name_scope(\"softmax\"):\r\n        # y is our prediction\r\n        z1 = tf.add(tf.matmul(x,W1),b1)\r\n        a1 = tf.nn.softmax(z1)\r\n        z2 = tf.add(tf.matmul(a1,W2),b2)\r\n        y = tf.nn.softmax(z2)\r\n      # specify cost function\r\n      with tf.name_scope('cross_entropy'):\r\n        # this is our cost\r\n        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n      # specify optimizer\r\n      with tf.name_scope('train'):\r\n        # optimizer is an \"operation\" which we can execute in a session\r\n        grad_op = tf.train.GradientDescentOptimizer(learning_rate)\r\n        train_op = grad_op.minimize(cross_entropy, global_step=global_step)\r\n      # init_op\r\n      tf.summary.scalar('cross_entropy', cross_entropy )\r\n      merged = tf.summary.merge_all()\r\n      init_op = tf.global_variables_initializer()\r\n      saver = tf.train.Saver()\r\n      print(\"Variables initialized ...\")\r\n    sv = tf.train.Supervisor(is_chief = (FLAGS.task_index == 0), global_step = global_step, init_op = init_op)\r\n    with sv.prepare_or_wait_for_session(server.target, config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement = True, log_device_placement = True)) as sess:\r\n      # perform training cycles\r\n      start_time = time.time()\r\n      if(FLAGS.task_index == 0):\r\n        train_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\r\n      for epoch in range(training_epochs):\r\n        # number of batches in one epoch                \r\n        sys.stderr.write(\"reporter progress:%0.4f\\n\"%(float(epoch)/(training_epochs)))\r\n        totalStep = iterData.batchCount()\r\n        for step in range(totalStep):\r\n          iterator_curr = iterData.nextBatch()\r\n          flag = 0\r\n          for iter in iterator_curr:\r\n            if 0 == flag:\r\n                train_x = iter[1].reshape(1,100)\r\n                train_y = oneHot(iter[0]).reshape(1,2)\r\n            else:\r\n                train_x = np.concatenate((train_x, iter[1].reshape(1,100)))\r\n                train_y = np.concatenate((train_y, oneHot(iter[0]).reshape(1,2)))\r\n            flag = 1\r\n          _, summary, cost, gstep = sess.run(\r\n                  [train_op, merged, cross_entropy, global_step],\r\n                  feed_dict={x: train_x, y_: train_y})\r\n          elapsed_time = time.time() - start_time\r\n          start_time = time.time()\r\n          if(FLAGS.task_index == 0):\r\n            train_writer.add_summary(summary, gstep)\r\n          print(\"Step: %d,\" % (gstep),\r\n                \" Epoch: %2d,\" % (epoch),                            \r\n                \" Cost: %.4f,\" % cost,\r\n                \" Time: %3.2fms\" % float(elapsed_time*1000))\r\n        sys.stderr.write(\"reporter progress:%0.4f\\n\"%(float(epoch+1)/(training_epochs)))\r\n  \r\n      print(\"Train Completed.\")\r\n      if(FLAGS.task_index == 0):\r\n        train_writer.close()\r\n        print(\"saving model...\")\r\n        saver.save(sess, FLAGS.save_path+\"/model.ckpt\")\r\n    print(\"done\")       \r\n```\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nBazel version", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "I have the same problem, anyone who can help this?", "It has been 15 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "yes, it is an issue"]}, {"number": 17735, "title": "FEATURE REQUEST : Kindly provide the gradient for tf.assign", "body": "Hi, i am trying to connect a CNN to another custom layer i have created. However the custom layer that i have created requires the variables in my custom layer to be assigned from the outputs of CNN.\r\n\r\nOne may think why i need variables if its assigned from CNN at every iteration. \r\nReason for variable creation is to calculate explicit gradients(via Register gradient and pyFunc) that involves sparse linear equations ,so that i can apply these gradients on my variables so that the error is back propagated all the way into CNN.\r\n\r\nHowever when i call tf.gradients ,all the values are None for CNN. I found a link over stackoverflow stating that gradients are not defined for tf.assign. \r\n\r\nhttps://stackoverflow.com/questions/46984316/how-tf-assign-compute-gradients-in-tensorflow\r\n\r\nCould you please provide gradient for tf.assign so that i can go for an end to end training.\r\n\r\nThanks.\r\n\r\n\r\n### Source code / logs\r\n     with tf.variable_scope(\"xyz\", reuse=tf.AUTO_REUSE) as scope:        \r\n     F = tf.get_variable(\"FXT\",  initializer=''')\r\n     B = tf.get_variable(\"BXT\",  initializer='')   \r\n     lambda_tf = tf.get_variable(\"lamda_tf\",initializer = [100.0],dtype=tf.float32)\r\n\r\n    \r\n    assign_op1 = tf.assign(F,Fx)\r\n    assign_op2 = tf.assign(B,Bx)     \r\n    \r\n    op=custom layer(F,B)\r\n    grads=tf.gradients(op,tf.all_trainables())\r\nHere Fx and Bx are values from CNN\r\n\r\nIn my backprop i have custom gradients that i have explicitly calculated and over-riden with tf.RegisterGradient and gradient overide.\r\nWhen i apply calculate grads i get\r\n[None,None,None............None{None values returned for variables in CNN},Defined Gradients{Defined Gradients returned for variables in Custom Layer}]\r\n ", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "Have I written custom code : Partially yes\r\nOS Platform and Distribution : Ubuntu 16.04\r\nTensorFlow installed from : pip\r\nTensorFlow version : 1.4\r\nBazel version : N/A\r\nCUDA/cuDNN version : 8.0/5.0\r\nGPU model and memory : TeslaK80 , 12 GB\r\nExact command to reproduce : N/A\r\n", "@vrv Can you take a look at this?", "Assigning to Alex, who has more state on the roadmap for everything related to Variables.  Please feel free to re-assign!", "@Raj-08 tf.assign returns \"ref\", ie link to the variable. So if you implemented backprop for it, the result would be zero since state of the variable is functionally independent of other parts of the graph.\r\n\r\nDo you want the gradients to just propagate the right hand side of the assignment? If so, you could differentiate the right-hand side explicitly, and use control dependencies to force assignment to happen on forward pass\r\n\r\n```\r\na = tf.Variable(1)\r\nb = tf.assign(a, stuff)\r\nwith tf.control_dependencies([b]):\r\n  c = tf.identity(stuff)\r\ntf.gradients(c, ...)  # this works\r\n```", "There is no plan to add a gradient to tf.assign because it's not possible in general to connect the uses of the assigned variable with the graph which assigned it.\r\n\r\nInstead, your solution of setting the weights of a network by the output of another is better implemented by not using tf.Variable at all for the computed weights, and just using tensors. We currently make it not too easy to do this using the tf.layers library but you can do this if you subclass the layers in tf.layers and override the build() method.", "@yaroslavvb It worked thanks!!!!\r\n\r\nThere were no errors in graph construction\r\n\r\nBut during the graph execution in backprop i get this error and its tracebacked to-\r\n`tf.gradients(c, ...)`\r\nthen it points to the\r\n`line where i defined the \"stuff\"[as per your comment]`\r\n\r\nERROR:\r\n`InvalidArgumentError (see above for traceback): Input to reshape is a tensor with 16777216 values, but the requested shape has\r\n4096\r\n         [[Node: gradients/truediv_grad/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0\r\n/device:GPU:0\"](gradients/truediv_grad/Sum, gradients/truediv_grad/Shape)]]\r\n`\r\n\r\nDo i need to make some other changes as well?", "@alextp How can apply my gradients to tensors? I tried applying my gradients on tensors and it throws error , i couldn't find any helpful material on stackoverflow.", "@alextp  @yaroslavvb I have checked all my ops in backprop and i am making sure i am passing the gradient values of required shape. Is this an internal problem when creating custom op and gradient?\r\n", "@Raj-08 what do you mean \"apply my gradients to tensors\"? Either you want to store mutable variables as your layer weights (which are initialized from the predictions of another network), in which case you apply your gradients to the variables but you can't backprop through them, or you want your layer weights to be computed tensors, in which case you should apply the gradients not to them but to the network which is producing them, right?\r\n\r\nOr is there another option I'm missing here?", "@yaroslavvb @alextp \r\n\r\nSo my gradients are flowing back right before the CNN(FCN) and then i am seeing an unconnected node on my tensorflow graph visualization. Even though the loss has been decreasing i am not able to find anything meaningful on the final output.\r\n\r\n\r\n![step0001](https://user-images.githubusercontent.com/15856029/37711449-c3f0c416-2d36-11e8-8f43-5b9be2cd3eef.png)\r\n\r\nAre the gradients still flowing through that node?If not how can i connect the node?", "@alextp No you aren't missing anything . \r\n\r\nLet me tell you why i went for creating variables in the first place.\r\n\r\nAt the start i computed the CNNs outputs as tensors and didnt involve any variabe but after creating the graph and calling tf.gradients it was showing None for the custom gradients i had created. \r\nSo i went about creating variables and then after following what @yaroslavvb mentioned i could get the gradients i have created.\r\n\r\nAfter visualizing the tensorflow graph i found that all my gradients were calculated as i intended but now as in the image above we can see that there is an unconnected node in between the gradients and the FCN .\r\n\r\n\r\n\r\n\r\n\r\n", "I'm not sure I understand. What's the unconnected node you see?\r\n\r\nWhen you just compute, on an interactive session tf.gradients on your final loss and your parameters, do you get 0 or None?\r\n\r\nAlso, if you're seeing an error you should share the whole stack trace as that's much easier to debug. The shape error you linked looks like it's a real shape error in your code.", "I am having the same issue for not being able to backprop the gradient through tf.assign. Is there any solution to this ? ", "You can try overriding the assign operator, perhaps something like the following:\r\n\r\n`@tf.custom_gradient`\r\n`def mod_assign(x):`\r\n----`phi = tf.Variable(tf.zeros([1, 4], dtype='float32'), dtype='float32',  trainable=True)`\r\n----` phi = tf.assign(phi,x)`\r\n----`def grad(dy):`\r\n--------`return dy`\r\n----`return phi, grad`", "Thanks !  "]}, {"number": 17734, "title": "tf.flags cannot handle options contain hyphen in the middle.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Cloud ML Engine (maybe Ubuntu)\r\n- **TensorFlow installed from (source or binary)**: Pre installed on Google Cloud ML Engine\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:NA\r\n- **GCC/Compiler version (if compiling from source)**:NA\r\n- **CUDA/cuDNN version**:NA\r\n- **GPU model and memory**:NA\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nAccording to the documentation of Google Cloud ML Engine [1], the ML Engine pass the option `--job-dir` to the python process and the program should be able to handle `--job-dir` option.\r\n\r\nThe snippet shown in the next section can run successfully with TensorFlow 1.4.1 but failed with 1.5.0, 1.6.0, 1.7.0.rc0.\r\n\r\nAs a result, I cannot use runtime version 1.5 on Cloud ML Engine with my package using `tf.flags` to handle option `--job-dir`.\r\nThe support of TensorFlow 1.5.0 on Cloud ML Engine training was released officially just few days before [2].\r\n\r\n\r\n### Source code / logs\r\n\r\nHere is a snippet to reproduce the issue.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.flags.DEFINE_string(\"job-dir\", \"default\", \"job dir\")\r\n\r\ndef main(argv):\r\n    print(tf.__version__)\r\n    print(tf.flags.FLAGS.job_dir)\r\n\r\ntf.app.run()\r\n```\r\n\r\nHere is the backtrace from :\r\n\r\n```\r\n> python snippent.py --job-dir=foo\r\n1.5.0\r\nTraceback (most recent call last):\r\n  File \"snippet.py\", line 9, in <module>\r\n    tf.app.run()\r\n  File \"/Users/chikanaga/.anyenv/envs/pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 124, in run\r\n    _sys.exit(main(argv))\r\n  File \"hoge.py\", line 7, in main\r\n    print(tf.flags.FLAGS.job_dir)\r\n  File \"/Users/chikanaga/.anyenv/envs/pyenv/versions/2.7.12/lib/python2.7/site-packages/tensorflow/python/platform/flags.py\", line 85, in __getattr__\r\n    return wrapped.__getattr__(name)\r\n  File \"/Users/chikanaga/.anyenv/envs/pyenv/versions/2.7.12/lib/python2.7/site-packages/absl/flags/_flagvalues.py\", line 470, in __getattr__\r\n    raise AttributeError(name)\r\nAttributeError: job_dir\r\n```\r\n\r\n1. https://cloud.google.com/ml-engine/docs/training-overview#job_configuration_parameters\r\n2. https://cloud.google.com/ml-engine/docs/runtime-version-list", "comments": ["Thanks for the report. I believe this is rooted in the move to the absl flags library in https://github.com/tensorflow/tensorflow/commit/2652704b576adc16b4d735f651cea1024e88b72e\r\nwhich happened between the release of 1.4.0 and 1.5.0.\r\n\r\nWill look into what should be done with this.\r\n", "Nagging Assignee @asimshankar: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.", "Long story short: In an ideal world, TensorFlow wouldn't be providing a flags library - that is squarely not in its area of expertise :). However, due to historical reasons around the time we created an opensource version of what was being developed within Google, the `tf.flags` library had to be created (this was before `absl-py` existed).\r\n\r\nLong story short, it doesn't seem like investing in `tf.flags` is worthwhile and since it seems that for CloudML using a different library (like `argparse`) suffices.\r\n\r\nSo, I'm going to go ahead an close this issue.\r\n(CC @martinwicke )\r\n\r\nHopefully that sounds reasonable to y'all.\r\nThanks.\r\n"]}, {"number": 17733, "title": "fixed the bug in tf.graph_util.remove_training_nodes when applying to LSTM networks", "body": "This bug damages LSTM network structure and make the model unusable. ", "comments": ["\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project (if not, look below for help). Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\n:memo: **Please visit <https://cla.developers.google.com/> to sign.**\n\nOnce you've signed (or fixed any issues), please reply here (e.g. `I signed it!`) and we'll verify it.\n\n----\n\n#### What to do if you already signed the CLA\n\n##### Individual signers\n\n*   It's possible we don't have your GitHub username or you're using a different email address on your commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n\n##### Corporate signers\n\n*   Your company has a Point of Contact who decides which employees are authorized to participate. Ask your POC to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to `go/cla#troubleshoot`.\n*   The email used to register you as an authorized contributor must be the email used for the Git commit. Check [your existing CLA data](https://cla.developers.google.com/clas) and verify that your [email is set on your git commits](https://help.github.com/articles/setting-your-email-in-git/).\n*   The email used to register you as an authorized contributor must also be [attached to your GitHub account](https://github.com/settings/emails).\n\t\t\n\n<!-- need_sender_cla -->", "I signed it!", "@yongchao-liu You may have to sign it again with the email you used to create the commits and/or your Github email. ", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this PR still valid? Assigning the `stalled` label. Please comment to reassure me that this is still being worked on.", "It has been 43 days that this pull-request has stalled. Please create a new pull-request with the requested changes."]}, {"number": 17732, "title": "contrib/boosted_trees: minor spelling tweaks", "body": "", "comments": ["\r\ntensorflow/contrib/boosted_trees/python/kernel_tests/prediction_ops_test.py:123: [C0301(line-too-long), ] Line too long (81/80)", "@frankchn rebased to match master/HEAD again.  can merge this into #17815 if that would make things simpler.  I would normally rather do a lot of little commits (one for each contrib path) rather than touching a bunch of files like that PR is doing, is there a preferred style?", "@brettkoonce Not really, we have a rotation of who handles PRs such as this, so everyone's preferences would be different. Just do whatever you prefer :)", "Unrelated test failure. Merging."]}, {"number": 17731, "title": "tf.summary.scalar error.  despairing......", "body": "Strange error\r\nWhile i was trying to tf.summary.scalar('content_loss', self.content_loss) there came an error\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'auto_loss': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\r\nSeems strange. However the code can be trained on GPU without the tf.summary.scalar operation", "comments": ["  File \"train_network.py\", line 157, in <module>\r\n    main()\r\n  File \"train_network.py\", line 136, in main\r\n    checkpoint_iterations=options.checkpoint_iterations\r\n  File \"/home/ilab/Downloads/SunAnLan-styletransfer-0e8c2098a5ec/workon/quadtree/fast_style_transfer.py\", line 258, in train\r\n    sess.run(tf.global_variables_initializer())\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'auto_loss': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\r\n\t [[Node: auto_loss = ScalarSummary[T=DT_FLOAT, _device=\"/device:GPU:0\"](auto_loss/tags, div_20)]]\r\n\r\n"]}, {"number": 17730, "title": "freeze_graph.py of tensorflow1.5 without \"--restore_op_name\" function", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:8.0/7.1\r\n- **GPU model and memory**:navidia GTX 1080Ti\r\n- **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nI use freeze_graph.py of tensorflow1.5 to convert model,but i want to use \"--restore_op_name\".\r\nIs there this function for this version or any other method recommended?\r\nThanks!\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nMy convert command as following:\r\npython freeze_graph.py \r\n--input_graph=D:\\mywork\\Re3\\demo\\simple_as_binary.pb --input_binary=True --input_checkpoint=D:\\mywork\\Re3\\demo\\simple.ckpt.data-00000-of-00001 --output_node_names=re3/fc_output/add,re3/lstm1/rnn/while/Exit_3,re3/lstm1/rnn/while/Exit_4,re3/lstm2/rnn/while/Exit_3,re3/lstm2/rnn/while/Exit_4 --restore_op_name=re3/conv1/W_conv,re3/conv1/b_conv,re3/conv1_skip/W_conv,re3/conv1_skip/b_conv,re3/conv1_skip/prelu,re3/conv2/W_conv,re3/conv2/b_conv,re3/conv2_skip/W_conv,re3/conv2_skip/b_conv,re3/conv2_skip/prelu,re3/conv3/W_conv,re3/conv3/b_conv,re3/conv4/W_conv,re3/conv4/b_conv,re3/conv5/W_conv,re3/conv5/b_conv,re3/conv5_skip/W_conv,re3/conv5_skip/b_conv,re3/conv5_skip/prelu,re3/fc6/W_fc,re3/fc6/b_fc,re3/fc_output/W_fc,re3/fc_output/b_fc,re3/lstm1/rnn/LSTM/block_input/biases,re3/lstm1/rnn/LSTM/block_input/weights,re3/lstm1/rnn/LSTM/forget_gate/biases,re3/lstm1/rnn/LSTM/forget_gate/weights,re3/lstm1/rnn/LSTM/input_gate/biases,re3/lstm1/rnn/LSTM/input_gate/weights,re3/lstm1/rnn/LSTM/output_gate/biases,re3/lstm1/rnn/LSTM/output_gate/weights,      re3/lstm2/rnn/LSTM/block_input/biases,re3/lstm2/rnn/LSTM/block_input/weights,re3/lstm2/rnn/LSTM/forget_gate/biases,re3/lstm2/rnn/LSTM/forget_gate/weights,re3/lstm2/rnn/LSTM/input_gate/biases,re3/lstm2/rnn/LSTM/input_gate/weights,re3/lstm2/rnn/LSTM/output_gate/biases,re3/lstm2/rnn/LSTM/output_gate/weights \r\n--output_graph=frozen_model.pb", "comments": ["I have resolved it.use the latest version.it just select nodes which is needed."]}, {"number": 17729, "title": "import frozen graph with batchnorm error", "body": "Error when loading the frozen graph with batchnorm.\r\nIt shows: ValueError: graph_def is invalid at node u'bn_0/cond/ExponentialMovingAverage/AssignMovingAvg/Switch': Input tensor 'bn_0/bn_0/moments/Squeeze/ExponentialMovingAverage:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\r\nIt is same as the question [#3628](https://github.com/tensorflow/tensorflow/issues/3628), and could anyone give me a proper solution for it?\r\n", "comments": ["Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.\nHave I written custom code\nOS Platform and Distribution\nTensorFlow installed from\nTensorFlow version\nBazel version\nCUDA/cuDNN version\nGPU model and memory\nExact command to reproduce", "I'm facing the same problem using freeze_graph.py.\r\n    Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n    OS Platform and Distribution: Linux Ubuntu 16.04\r\n    TensorFlow installed from (source or binary): source - from bazel \r\n    TensorFlow version: release 1.2.1\r\n    Python version: 3.5\r\n    Bazel version (if compiling from source): - 0.5.2\r\n    GCC/Compiler version (if compiling from source): - 5.3.0\r\n    CUDA/cuDNN version: 8/5\r\n    GPU model and memory: GTX 1080Ti\r\nThe errors below is the one I currently face:\r\n\r\nValueError: graph_def is invalid at node 'InceptionV2/InceptionV2/Conv2d_2b_1x1/BatchNorm/AssignMovingAvg': Input tensor 'InceptionV2/Conv2d_2b_1x1/BatchNorm/moving_mean:0' Cannot convert a tensor of type f\r\n", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "Have you guys made any progress on that ? \r\nI'm having similar issue \r\n\r\n> Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n> OS Platform and Distribution: Linux Ubuntu 16.04\r\n> TensorFlow installed from (source or binary): from pip install \r\n> TensorFlow version: tensorflow-gpu 1.7\r\n> Python version: 3.6\r\n> Bazel version (if compiling from source): \r\n> GCC/Compiler version (if compiling from source): \r\n> CUDA/cuDNN version: 9/7\r\n> GPU model and memory: Titan XP\r\n> \r\n\r\nThe errors below is the one I currently face:\r\n\r\n\"graph_def is invalid at node 'block1/conv2d/kernel/Assign': Input tensor 'block1/conv2d/kernel:0' Cannot convert a tensor of type float32 to an input of type float32_ref.\"\r\n", "/CC @petewarden, any ideas what is causing the issues?", "It has been 14 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 29 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "It has been 44 days with no activity and the `awaiting response` label was assigned. Is this still an issue?", "We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks!", "I was trying to convert my frozen graph (.pb model) to . tflite using toco, I got the following error.\r\nValueError: Invalid tensors 'input' were found.\r\n\r\nAny help please ", "I  have the same issue that @Elites2017.  \r\n\r\n", "Me too @ikus and @Elites2017, any progress on this issue?", "@Qryptoc  I update to tensorflow 1.11\r\n\r\n> **pip install tensorflow==1.11.0**\r\n\r\nand use parameter **--input_array=input**\r\n\r\n\r\n> **tflite_convert --output_file=/tmp/foo.tflite --graph_def_file=retrained_graph_docs3.pb\r\n --input_arrays=input\r\n --output_arrays=MobilenetV1/Predictions/Reshape_1**\r\n\r\n\r\n\r\n"]}]